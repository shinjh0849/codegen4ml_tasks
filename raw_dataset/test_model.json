{"snippet": "train_features = train_data[:, 1:]\ntrain_target = train_data[:, 0]\nclf = clf.fit(train_features, train_target)\nscore = clf.score(train_features, train_target)\n\"Mean accuracy of Random Forest: {0}\".format(score)\n", "intent": "Fit the training data and create the decision trees:\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X, y)\n", "intent": "Let's fit a K-neighbors classifier\n"}
{"snippet": "for n_neighbors in [1, 5, 10, 20, 30]:\n    knn = KNeighborsClassifier(n_neighbors)\n    knn.fit(X_train, y_train)\n    print(n_neighbors, knn.score(X_test, y_test))\n", "intent": "Using this, we can ask how this changes as we change the model parameters, in this case the number of neighbors:\n"}
{"snippet": "from keras import models\nfrom keras import layers\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n", "intent": "And here's the Keras implementation, very similar to the MNIST example you saw previously:\n"}
{"snippet": "history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\n", "intent": "Now let's train our network for 20 epochs:\n"}
{"snippet": "dtree = DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "dpt_model = models.Sequential()\ndpt_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\ndpt_model.add(layers.Dropout(0.5))\ndpt_model.add(layers.Dense(16, activation='relu'))\ndpt_model.add(layers.Dropout(0.5))\ndpt_model.add(layers.Dense(1, activation='sigmoid'))\ndpt_model.compile(optimizer='rmsprop',\n                  loss='binary_crossentropy',\n                  metrics=['acc'])\n", "intent": "Let's add two `Dropout` layers in our IMDB network to see how well they do at reducing overfitting:\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n", "intent": "We will be using the same model architecture as before:\n"}
{"snippet": "model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(x_train, y_train,\n                    epochs=10,\n                    batch_size=32,\n                    validation_data=(x_val, y_val))\nmodel.save_weights('pre_trained_glove_model.h5')\n", "intent": "Let's compile our model and train it:\n"}
{"snippet": "from keras.layers import Dense\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n                    epochs=10,\n                    batch_size=128,\n                    validation_split=0.2)\n", "intent": "Let's train a simple recurrent network using an `Embedding` layer and a `SimpleRNN` layer:\n"}
{"snippet": "optimizer = keras.optimizers.RMSprop(lr=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\n", "intent": "Since our targets are one-hot encoded, we will use `categorical_crossentropy` as the loss to train the model:\n"}
{"snippet": "layer_dict = dict([(layer.name, layer) for layer in model.layers])\nloss = K.variable(0.)\nfor layer_name in layer_contributions:\n    coeff = layer_contributions[layer_name]\n    activation = layer_dict[layer_name].output\n    scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n    loss += coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling\n", "intent": "Now let's define a tensor that contains our loss, i.e. the weighted sum of the L2 norm of the activations of the layers listed above.\n"}
{"snippet": "preparation_and_feature_selection_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, k))\n])\n", "intent": "Looking good... Now let's create a new pipeline that runs the previously defined preparation pipeline, and adds top k feature selection:\n"}
{"snippet": "prepare_select_and_predict_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n    ('svm_reg', SVR(**rnd_search.best_params_))\n])\n", "intent": "Question: Try creating a single pipeline that does the full data preparation plus the final prediction.\n"}
{"snippet": "param_grid = [\n        {'preparation__num_pipeline__imputer__strategy': ['mean', 'median', 'most_frequent'],\n         'feature_selection__k': list(range(1, len(feature_importances) + 1))}\n]\ngrid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n                                scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\ngrid_search_prep.fit(housing, housing_labels)\n", "intent": "Question: Automatically explore some preparation options using `GridSearchCV`.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)\n", "intent": "Now let's compare the decision tree model to a random forest.\n"}
{"snippet": "logits = X_valid.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\naccuracy_score = np.mean(y_predict == y_valid)\naccuracy_score\n", "intent": "Let's make predictions for the validation set and check the accuracy score:\n"}
{"snippet": "logits = X_valid.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\naccuracy_score = np.mean(y_predict == y_valid)\naccuracy_score\n", "intent": "Because of the additional $\\ell_2$ penalty, the loss seems greater than earlier, but perhaps this model will perform better? Let's find out:\n"}
{"snippet": "logits = X_test.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\naccuracy_score = np.mean(y_predict == y_test)\naccuracy_score\n", "intent": "And now let's measure the final model's accuracy on the test set:\n"}
{"snippet": "from sklearn.svm import LinearSVR\nlin_svr = LinearSVR(random_state=42)\nlin_svr.fit(X_train_scaled, y_train)\n", "intent": "Let's train a simple `LinearSVR` first:\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(random_state=42)\n", "intent": "*Exercise: Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set.*\n"}
{"snippet": "rnd_clf2 = RandomForestClassifier(random_state=42)\nt0 = time.time()\nrnd_clf2.fit(X_train_reduced, y_train)\nt1 = time.time()\n", "intent": "*Exercise: Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster?*\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing.data, housing.target.reshape(-1, 1))\nprint(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])\n", "intent": "Compare with Scikit-Learn\n"}
{"snippet": "reset_graph()\nn_epochs = 1000\nlearning_rate = 0.01\nX = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\ny = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\ntheta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\ny_pred = tf.matmul(X, theta, name=\"predictions\")\nerror = y_pred - y\nmse = tf.reduce_mean(tf.square(error), name=\"mse\")\n", "intent": "Same as above except for the `gradients = ...` line:\n"}
{"snippet": "with tf.Session() as sess:\n    init.run()\n    print(z.eval())\n    print(sess.run(grads))\n", "intent": "Let's compute the function at $a=0.2$ and $b=0.3$, and the partial derivatives at that point with regards to $a$ and with regards to $b$:\n"}
{"snippet": "kmeans = KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "reset_graph()\n", "intent": "First let's reset the default graph.\n"}
{"snippet": "y_proba = tf.sigmoid(logits)\n", "intent": "In fact, TensorFlow has a nice function `tf.sigmoid()` that we can use to simplify the last line of the previous code:\n"}
{"snippet": "reset_graph()\n", "intent": "Ok, next let's reset the default graph:\n"}
{"snippet": "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n", "intent": "Now we can create the `FileWriter` that we will use to write the TensorBoard logs:\n"}
{"snippet": "reset_graph()\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n", "intent": "Implementing Leaky ReLU in TensorFlow:\n"}
{"snippet": "reset_graph()\nn_inputs = 28 * 28  \nn_hidden1 = 300\nn_hidden2 = 100\nn_outputs = 10\n", "intent": "Let's train a neural network on MNIST using the Leaky ReLU. First let's create the graph:\n"}
{"snippet": "reset_graph()\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n", "intent": "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:\n"}
{"snippet": "def selu(z,\n         scale=1.0507009873554804934193349852946,\n         alpha=1.6732632423543772848170429916717):\n    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))\n", "intent": "Here's a TensorFlow implementation (there will almost certainly be a `tf.nn.selu()` function in future TensorFlow versions):\n"}
{"snippet": "threshold = 1.0\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ngrads_and_vars = optimizer.compute_gradients(loss)\ncapped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n              for grad, var in grads_and_vars]\ntraining_op = optimizer.apply_gradients(capped_gvs)\n", "intent": "Now we apply gradient clipping. For this, we need to get the gradients, use the `clip_by_value()` function to clip them, then apply them:\n"}
{"snippet": "kmeans.fit(df.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./my_model_final.ckpt\")\n", "intent": "Now you can start a session, restore the model's state and continue training on your data:\n"}
{"snippet": "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n", "intent": "Or we could use the graph's `get_tensor_by_name()` method:\n"}
{"snippet": "reset_graph()\nn_inputs = 28 * 28  \nn_hidden1 = 300\nn_outputs = 10\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\ny = tf.placeholder(tf.int64, shape=(None), name=\"y\")\nwith tf.name_scope(\"dnn\"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")\n", "intent": "Let's implement $\\ell_1$ regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity):\n"}
{"snippet": "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\nclipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\nclip_weights2 = tf.assign(weights2, clipped_weights2)\n", "intent": "We can do this as well for the second hidden layer:\n"}
{"snippet": "he_init = tf.contrib.layers.variance_scaling_initializer()\ndef dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None,\n        activation=tf.nn.elu, initializer=he_init):\n    with tf.variable_scope(name, \"dnn\"):\n        for layer in range(n_hidden_layers):\n            inputs = tf.layers.dense(inputs, n_neurons, activation=activation,\n                                     kernel_initializer=initializer,\n                                     name=\"hidden%d\" % (layer + 1))\n        return inputs\n", "intent": "We will need similar DNNs in the next exercises, so let's create a function to build this DNN:\n"}
{"snippet": "dnn_clf = DNNClassifier(random_state=42)\ndnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)\n", "intent": "Let's see if we get the exact same accuracy as earlier using this class (without dropout or batch norm):\n"}
{"snippet": "dnn_clf_bn = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n                           n_neurons=90, random_state=42,\n                           batch_norm_momentum=0.95)\ndnn_clf_bn.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)\n", "intent": "Good, now let's use the exact same model, but this time with batch normalization:\n"}
{"snippet": "hidden5_out = tf.get_default_graph().get_tensor_by_name(\"hidden5_out:0\")\n", "intent": "Let's start by getting a handle on the output of the last frozen layer:\n"}
{"snippet": "dnn_clf_5_to_9 = DNNClassifier(n_hidden_layers=4, random_state=42)\ndnn_clf_5_to_9.fit(X_train2, y_train2, n_epochs=1000, X_valid=X_valid2, y_valid=y_valid2)\n", "intent": "Let's compare that to a DNN trained from scratch:\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS',axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "reset_graph()\nX = tf.placeholder(shape=(None, height, width, channels), dtype=tf.float32)\nconv = tf.layers.conv2d(X, filters=2, kernel_size=7, strides=[2,2],\n                        padding=\"SAME\")\n", "intent": "Using `tf.layers.conv2d()`:\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, INCEPTION_V3_CHECKPOINT_PATH)\n", "intent": "Exercise: Open a session and use the `Saver` to restore the pretrained model checkpoint you downloaded earlier.\n"}
{"snippet": "n_outputs = len(flower_classes)\nwith tf.name_scope(\"new_output_layer\"):\n    flower_logits = tf.layers.dense(prelogits, n_outputs, name=\"flower_logits\")\n    Y_proba = tf.nn.softmax(flower_logits, name=\"Y_proba\")\n", "intent": "Then we can add the final fully connected layer on top of this layer:\n"}
{"snippet": "with tf.device(\"/gpu:0\"):  \n    layer1 = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\nwith tf.device(\"/gpu:1\"):  \n    layer2 = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n", "intent": "Do **NOT** do this:\n"}
{"snippet": "reset_graph()\nn_inputs = 28*28\nX = tf.placeholder(tf.float32, shape=[None, n_inputs])\nhidden1 = tf.nn.elu(tf.matmul(X, W1) + b1)\nhidden2 = tf.nn.elu(tf.matmul(hidden1, W2) + b2)\nhidden3 = tf.nn.elu(tf.matmul(hidden2, W3) + b3)\noutputs = tf.matmul(hidden3, W4) + b4\n", "intent": "Finally, we can create a Stacked Autoencoder by simply reusing the weights and biases from the Autoencoders we just trained:\n"}
{"snippet": "reset_graph()\nn_inputs = 28 * 28\nn_hidden1 = 300\nn_hidden2 = 150  \nn_hidden3 = n_hidden1\nn_outputs = n_inputs\nlearning_rate = 0.01\n", "intent": "Using Gaussian noise:\n"}
{"snippet": "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.sigmoid)\n", "intent": "Note that the coding layer must output values from 0 to 1, which is why we use the sigmoid activation function:\n"}
{"snippet": "logits = tf.layers.dense(hidden1, n_outputs)\noutputs = tf.nn.sigmoid(logits)\nxentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits)\nreconstruction_loss = tf.reduce_mean(xentropy)\n", "intent": "To speed up training, you can normalize the inputs between 0 and 1, and use the cross entropy instead of the MSE for the cost function:\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Let's reset the default graph, in case you re-run this notebook without restarting the kernel:\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "tweak_labels = np.tile(mnist.test.labels[:n_samples], caps2_n_dims * n_steps)\nwith tf.Session() as sess:\n    saver.restore(sess, checkpoint_path)\n    decoder_output_value = sess.run(\n            decoder_output,\n            feed_dict={caps2_output: tweaked_vectors_reshaped,\n                       mask_with_labels: True,\n                       y: tweak_labels})\n", "intent": "Now let's feed these tweaked output vectors to the decoder and get the reconstructions it produces:\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nx1_train = df_train.x1.values.reshape(-1, 1) \nprint('Coefficient (w1)', model.coef_)\nprint('Intercept (w0)', model.intercept_)\n", "intent": "We'll use [sklearn.linear_model.LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n"}
{"snippet": "from keras.models import load_model\nyolo_model = load_model(model_path, compile=False)\nyolo_model.summary()\n", "intent": "Finally, this is the model architecture, as seen by keras.\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dense\nsimple_model = Sequential()\nsimple_model.add(Dense(1, input_dim=4, activation='sigmoid')) \nsimple_model.summary()\n", "intent": "Finally, let's try something simpler.\nLet's create a 1-layer network that can do linear regression.\n"}
{"snippet": "deeper_model = Sequential()\ndeeper_model.add(Dense(256, input_dim=16, activation='relu'))\ndeeper_model.add(Dense(1, activation='sigmoid'))\ndeeper_model.summary()\n", "intent": "How about a 2-layer network to make it a deep neural network?\n"}
{"snippet": "from keras.layers import Flatten, Dense, Dropout\nclassifier = Sequential()\nclassifier.add(Flatten(input_shape=features_train.shape[1:])) \nclassifier.add(Dense(64, activation='relu'))\nclassifier.add(Dropout(0.5))\nclassifier.add(Dense(3, activation='softmax'))\nclassifier.summary()\n", "intent": "Note that this classifier does not have to be related to the architecture of the original network.\nIt simply treats the inputs as opaque features.\n"}
{"snippet": "from sklearn.svm import SVC\nmodel_svc = SVC(gamma='auto')\nscores = cross_validate(model_svc, Z_train, y_train, cv=5,\n                        return_train_score=True,\n                        return_estimator=True)\nscores\n", "intent": "- cross validation\n- learning curve\n"}
{"snippet": "from sklearn.svm import SVC\nmodel_svc = SVC(gamma=0.001, C=10)\nscores = cross_validate(model_svc, Z_train, y_train, cv=5,\n                        return_train_score=True,\n                        return_estimator=True)\nscores\n", "intent": "- cross validation\n- learning curve\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nscores_nb = cross_validate(GaussianNB(), Z_train, y_train, cv=5,\n                           return_train_score=False, return_estimator=True)\nprint('validation scores', scores_nb['test_score'])\n", "intent": "Naive Bayes uses the Bayes Theorem (multiplication if the probabilities of X) to compute the probability of y.\n"}
{"snippet": "knn.fit(X_train,y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(verbose=True, random_state=42)\nsgd_clf.fit(Z_train, y_train)\nscores = sgd_clf.score(Z_test, y_test)\nprint(scores) \n", "intent": "1. SGD + Logistic Regression\n2. SVM with some non-linear kernel\n3. Compare evaluation metrics\n"}
{"snippet": "regressor = linear_model.LinearRegression()\n", "intent": "We pick the `LinearRegression` model to do linear regression of the to-be-selected features and `bathrooms`\n"}
{"snippet": "lof =  LocalOutlierFactor(int(k),n_jobs=-1,p=13,contamination=ratio)\n", "intent": "Since this is a high dimention space, we are not able to visualize it.\n"}
{"snippet": "lof =  LocalOutlierFactor(int(k),n_jobs=-1,p=14,contamination=ratio)\n", "intent": "Since this is a high dimention space, we are not able to visualize it.\n"}
{"snippet": "    model = Sequential()\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Flatten(input_shape = (32,32,3)))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a [dropout](https://keras.io/layers/core/\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, SAVE_FILE)\n    for image_data in resized_images_data:\n        top_3_predictions = sess.run(tf.nn.top_k(softmax, k=3), feed_dict = {x: image_data, keep_prob : 1})\n        print(top_3_predictions)\n", "intent": "**Answer:**\nIt seems that my model is very certain (in fact 100% certain) about its classifications.\n"}
{"snippet": "from datetime import timedelta\nltg30_blobs = goesio.get_ltg_blob_paths(irdt + timedelta(minutes=30), timespan_minutes=15)\nltg30 = goesio.create_ltg_grid(ltg30_blobs, griddef, influence_km)\nmodel = LogisticRegression()\ndf = create_prediction_df(ref, ltg30, griddef)\nx = df.drop(['lat', 'lon', 'ltg'], axis=1)\nmodel = model.fit(x, df['ltg'])\nprint(model.coef_, model.intercept_)\nprint('Model accuracy={}%'.format(100*model.score(x, df['ltg'])))\n", "intent": "How about if we try to predict lightning 30 minutes into the future?\n"}
{"snippet": "a = tf.placeholder(dtype=tf.int32, shape=(None,))  \nb = tf.placeholder(dtype=tf.int32, shape=(None,))\nc = tf.add(a, b)\nwith tf.Session() as sess:\n  result = sess.run(c, feed_dict={\n      a: [3, 4, 5],\n      b: [-1, 2, 3]\n    })\n  print(result)\n", "intent": "<h2> Using a feed_dict </h2>\nSame graph, but without hardcoding inputs at build stage\n"}
{"snippet": "def compute_area(sides):\n  return list_of_areas\nwith tf.Session() as sess:\n  area = compute_area(tf.constant([\n      [5.0, 3.0, 7.1],\n      [2.3, 4.1, 4.8]\n    ]))\n  result = sess.run(area)\n  print(result)\n", "intent": "Extend your code to be able to compute the area for several triangles at once.\nYou should get: [6.278497 4.709139]\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "def custom_estimator(features, labels, mode, params):  \n  input_layer = tf.feature_column.input_layer(features, params['feature_columns'])\n  predictions = tf.layers.dense(input_layer,10,activation=tf.nn.relu)\n  predictions = tf.layers.dense(input_layer,1,activation=None)\n  .....REST AS BEFORE\n", "intent": "Modify the custom_estimator function to be a neural network with one hidden layer, instead of a linear regressor\n"}
{"snippet": "def linear_model(img):\n  X = tf.reshape(img,[-1,HEIGHT*WIDTH]) \n  W = tf.get_variable(\"W\", [HEIGHT*WIDTH,NCLASSES], \n                      initializer = tf.truncated_normal_initializer(stddev=0.1,seed = 1))\n  b = tf.get_variable(\"b\",NCLASSES, initializer = tf.zeros_initializer)\n  ylogits = tf.matmul(X,W)+b\n  return ylogits, NCLASSES\n", "intent": "Let's start with a very simple linear classifier. All our models will have this basic interface -- they will take an image and return logits.\n"}
{"snippet": "def linear_model(img):\n  return ylogits, NCLASSES\n", "intent": "Let's start with a very simple linear classifier. All our models will have this basic interface -- they will take an image and return logits.\n"}
{"snippet": "sess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run([init])\nsaver = tf.train.Saver(max_to_keep=1)\n", "intent": "This resets all neuron weights and biases to initial random values\n"}
{"snippet": "with tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  users_topfeats = {}\n  for ind in range(num_users):\n    top_feats = sess.run(find_user_top_feats(ind))\n    users_topfeats[users[ind]] = list(top_feats)\n", "intent": "We'll create a tensorflow session to compute these values.\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    user_topmovies = {}\n    num_to_recommend = tf.reduce_sum(tf.cast(tf.equal(users_movies, \n                                                      tf.zeros_like(users_movies)), dtype = tf.float32), axis = 1)\n    for ind in range(num_users):\n      top_movies = sess.run(find_user_top_movies(ind, tf.cast(num_to_recommend[ind], dtype = tf.int32)))\n      user_topmovies[users[ind]] = list(top_movies)\n", "intent": "As before, we create a tf.Session to compute the movie recommendations. \n"}
{"snippet": "def try_out():\n  with tf.Session() as sess:\n    fn = read_dataset(tf.estimator.ModeKeys.EVAL, \n                    {'input_path': 'data', 'batch_size': 4, 'nitems': 5668, 'nusers': 82802})\n    feats, _ = fn()\n    print feats['input_rows'].eval()\n    print feats['input_rows'].eval()\ntry_out()\n", "intent": "This code is helpful in developing the input function. You don't need it in production.\n"}
{"snippet": "with tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  users_topfeats = {}\n  for i in range(num_users):\n    top_feats = sess.run(find_user_top_feats(i))\n    users_topfeats[users[i]] = list(top_feats)\n", "intent": "We'll create a tensorflow session to compute these values.\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    user_topmovies = {}\n    num_to_recommend = tf.reduce_sum(tf.cast(tf.equal(users_movies, \n                                                      tf.zeros_like(users_movies)), dtype = tf.float32), axis = 1)\n    for ind in range(num_users):\n      top_movies = sess.run(find_user_top_movies(ind, tf.cast(num_to_recommend[ind], dtype = tf.int32)))\n      user_topmovies[users[ind]] = list(top_movies)         \n", "intent": "As before, we create a tf.Session to compute the movie recommendations. \n"}
{"snippet": "lm.fit(X_train,y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "happyModel.fit(x = X_train, y = Y_train, epochs = 10, batch_size = 20)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model = load_vgg_model(\"pretrained-model/imagenet-vgg-verydeep-19.mat\")\n", "intent": "Next, as explained in part (2), let's load the VGG16 model.\n"}
{"snippet": "inference_model = music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 50)\n", "intent": "Run the cell below to define your inference model. This model is hard coded to generate 50 values.\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nreg = RandomForestRegressor(min_samples_leaf=5)\nreg.fit(train[predictors], train[\"cnt\"])\n", "intent": "By taking the nonlinear predictors into account, the decision tree regressor appears to have much higher accuracy than linear regression.\n"}
{"snippet": "dt.fit(x_train, y_train)\n", "intent": "The scores show that the tree performs well in classification. Therefore, fitting can now be performed.\n"}
{"snippet": "dt = DecisionTreeClassifier(criterion='gini')\n", "intent": "A **Decision Tree** is applied next. **Random Forests** as they can lead to severe overfitting of the model.\n"}
{"snippet": "dt.fit(new_train, new_train_y)\n", "intent": "Accuracy of 1.0 achieved here. The tree is then fitted and explored a little more.\n"}
{"snippet": "from sklearn import linear_model\nmeta_model = linear_model.LinearRegression(n_jobs=10)\nmeta_model.fit(X_train_level2, y_train_level2)\n", "intent": "Fit a linear regression model to the meta-features. Use the same parameters as in the model above.\n"}
{"snippet": "lgb1=LGBMRegressor(n_jobs=8)\nlgb1.fit(X_train,y_train,\n        eval_set=(X_val,y_val),\n        early_stopping_rounds=1,\n        eval_metric=lambda y_t,y_p:('error',score(y_t,y_p),False),\n       )\n", "intent": "lightgbm has early_stopping mechanism by providing validation data\nI get validation score 0.9632\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nspam_detect_model = MultinomialNB().fit(messages_tfidf, messages['label'])\n", "intent": "We'll be using scikit-learn here, choosing the [Naive Bayes](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) classifier to start with:\n"}
{"snippet": "from keras.layers import Dense\nfrom keras.models import Sequential\ndef create_model():\n    model = Sequential()\n    model.add(Dense(6, input_dim=4, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    return model\n", "intent": "Now set up an actual MLP model using Keras:\n"}
{"snippet": "model = ols(\"totalPr ~ duration + nBids + cond + sellerRate + stockPhoto + wheels\", \n            data=mk_df_reduced).fit()\nprint(model.summary())\nprint(\"R-squared is %.4f\" % model.rsquared)\nprint(\"R-squared adjusted is %.4f\" % model.rsquared_adj)\n", "intent": "Now, let's make a regression model with all of these together\n"}
{"snippet": "feature_data_down_two = feature_data_down_one.drop(\"stockPhoto\", axis=1)\nbaseline_performance = 0.73957\nfor feature in feature_data_down_two.columns:\n    feature_data_temp = feature_data_down_two.drop(feature, axis=1)\n    reg_up = linear_model.LinearRegression()\n    reg_up.fit(feature_data_temp, target_data)\n    r2_up = reg_up.score(feature_data_temp, target_data)\n    r2_adj_up = 1 - (1-r2_up)*(len(target_data)-1) / (len(target_data) - feature_data_temp.shape[1]-1)\n    if r2_adj_up > baseline_performance:\n        print(\"We can improve R-squared to %.5f by leaving out %s\" % (r2_adj_up, feature))   \n", "intent": "Again, the model can be improved by leaving out 'stockPhoto'. Let's try one more time.\n"}
{"snippet": "feature_data_down_three = feature_data_down_two.drop(\"nBids\", axis=1)\nbaseline_performance = 0.74006\nfor feature in feature_data_down_three.columns:\n    feature_data_temp = feature_data_down_three.drop(feature, axis=1)\n    reg_up = linear_model.LinearRegression()\n    reg_up.fit(feature_data_temp, target_data)\n    r2_up = reg_up.score(feature_data_temp, target_data)\n    r2_adj_up = 1 - (1-r2_up)*(len(target_data)-1) / (len(target_data) - feature_data_temp.shape[1]-1)\n    if r2_adj_up > baseline_performance:\n        print(\"We can improve R-squared to %.5f by leaving out %s\" % (r2_adj_up, feature))     \n", "intent": "Again, we've improved. So let's take out nBids and try again\n"}
{"snippet": "scores = []\nfor k in n_neighbors:\n    clf = neighbors.KNeighborsClassifier(k)\n    clf.fit(X_train, y_train)\n    scores.append(clf.score(X_test, y_test))\n", "intent": "Now that we have our k-values to try, let's find a good `KNeighborsClassifier`:\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\nmnb.fit(freq_train_dtm, y_train)\nmnb.score(freq_test_dtm, y_test)\n", "intent": "For the frequency model, the [`MultinomialNB`](http://scikit-learn.org/stable/modules/naive_bayes.html\n"}
{"snippet": "tfidf_nb = MultinomialNB()\ntfidf_nb.fit(tfidf_train_dtm, y_train)\ntfidf_nb.score(tfidf_test_dtm, y_test)\n", "intent": "Finally, let's try the same with our **TFIDF** model:\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nazm_mnd = MultinomialNB()\nazm_mnd.fit(train_dtm, y_train)\n", "intent": "Use `sklearn` to build a `MultinomialNB` classifier against your training data.\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(train_dtm, y_train)\n", "intent": "Use `sklearn` to build a `MultinomialNB` classifier against your training data.\n"}
{"snippet": "pipeline.fit(msg_train,label_train)\n", "intent": "Now we can directly pass message text data and the pipeline will do our pre-processing for us! We can treat it as a model/estimator API:\n"}
{"snippet": "km.fit(data)\n", "intent": "Train the classifier on all the data\n"}
{"snippet": "km = KMeans(n_clusters=4,init='k-means++',random_state=1,n_init=10,max_iter=300)\n", "intent": "Initialize a new kmeans classifier\n"}
{"snippet": "kmd = KMeans(n_clusters=20,init='k-means++',random_state=1,n_init=10,max_iter=300)\nkmd.fit(X_digits,Y_digits )\n", "intent": "Try creating a KMeans clusterer with 20 classes (obviously 10 would be ideal, but let's try 20 first).  Fit the model to the digits data.\n"}
{"snippet": "km = KMeans(n_clusters=4, random_state=1)\n", "intent": "Initialize a new kmeans classifier\n"}
{"snippet": "km = KMeans(n_clusters=20)\nkm.fit(X_digits)\n", "intent": "Try creating a KMeans clusterer with 20 classes (obviously 10 would be ideal, but let's try 20 first).  Fit the model to the digits data.\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(32,32,3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "dtc = DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "dtc.fit(X_train, y_train)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "model = KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\n", "intent": "Time to train a model!\n** Import MultinomialNB and create an instance of the estimator and call is nb **\n"}
{"snippet": "from sklearn.svm import SVC\nmodel = SVC()\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "grid = GridSearchCV(estimator=SVC(), param_grid=param_grid, verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), -1, 1))\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "pred, W, b = model(X_train, Y_train, word_to_vec_map)\nprint(pred)\n", "intent": "Run the next cell to train your model and learn the softmax parameters (W,b). \n"}
{"snippet": "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)\n", "intent": "Fit the Keras model on `X_train_indices` and `Y_train_oh`. We will use `epochs = 50` and `batch_size = 32`.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nparameters = { 'C':np.linspace(1e-3,1e3,5)}\nclf = GridSearchCV(SVC(), parameters, scoring=\"roc_auc\")\nclf = clf.fit(x_train, y_train)\nbest_linear_model = clf\n", "intent": "Select the best linear model among $C \\in [\\mathtt{1.e-3}, .., \\mathtt{1.e3}]$.\n"}
{"snippet": "parameters = { 'C':np.linspace(1e-3,1e3,5)}\nlm_tf = GridSearchCV(SVC(), parameters, scoring=\"roc_auc\")\nlm_tf = lm_tf.fit(x_train, y_train)\nbest_linear_model_tf = lm_tf\n", "intent": "Select the best linear model among $C \\in [\\mathtt{1.e-3}, .., \\mathtt{1.e3}]$.\n"}
{"snippet": "def mySigmoid(x):\n    return 1/ (1+np.exp(-x))\nsample=np.random.multivariate_normal(mean=np.array([0,0,0]),cov=np.array([[1,-0.25,0.75],[-0.25,1,0.5],[0.75,0.5,2]]),size=1000)\nmonteCarloApproximation = np.mean(mySigmoid(sample.dot(np.array([2/3,1/6,1/6]))))\nprint(\"MonteCarlo:\",monteCarloApproximation)\nprint(\"FiniteDifferenceHessian:\",approximatedIntegral)\n", "intent": "Compare the results\n"}
{"snippet": "decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(training_X, training_y)\n", "intent": "Construct calssification algorithms and train them.\n"}
{"snippet": "nb.fit(X_train,y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "model = LogisticRegression()\n", "intent": "As a base classifier we will use Logistic Regression. Although it also can procces multiclass data, we will use it as a binary classifier.\n"}
{"snippet": "ovr_classifier = OneVsRestClassifier(clone(model))\novr_classifier.fit(train_X, train_y)\n", "intent": "Apply **ovr** for our model.\n"}
{"snippet": "oc_classifier = OutputCodeClassifier(\n    clone(model),\n    code_size=2.)\noc_classifier.fit(train_X, train_y)\n", "intent": "Apply **Output Coding**.\n"}
{"snippet": "append_categoricals = Pipeline([\n        ('append_cats', DataFrameSelector(attribute_names=one_hot_transformer.encoded_columns))  \n    ])\n", "intent": "Pipeline that simply gets the categorical/encoded columns from the previous transformation (which used `oo-learning`)\n"}
{"snippet": "model = ExtraTreesClassifier(\n                      random_state=42,        \n                     )\n", "intent": "Choose the transformations to tune, below:\n"}
{"snippet": "model = RandomForestClassifier(\n                      random_state=42,        \n                     )\n", "intent": "Choose the transformations to tune, below:\n"}
{"snippet": "model.fit(Xsmall, ysmall, batch_size=500, epochs=40,verbose = 2)\nmodel.save_weights('model_weights/best_RNN_small_textdata_weights.hdf5')\n", "intent": "Now lets fit our model!\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nrfreg = RandomForestRegressor()\nrfreg\n", "intent": "<a id=\"tuning\"></a>\n"}
{"snippet": "k = 4\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(X)\n", "intent": "- Select a number of clusters of your choice based on your visual analysis above.\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\ngrid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "test_acc = []\nfor i in range(1, X_train.shape[0]+1):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    test_acc.append(knn.score(X_test, y_test))\n", "intent": "- Store the test accuracy in a list.\n- Plot the test accuracy vs. the number of neighbors.\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(Xs, y)\n", "intent": "**10.E: Fit a `KNeighborsClassifier` with the best number of neighbors.**\n"}
{"snippet": "lr = LinearRegression()\ntype(lr)\n", "intent": "**Step 2:** \"Instantiate\" the \"estimator.\"\n- \"Estimator\" is scikit-learn's term for \"model.\"\n- \"Instantiate\" means \"make an instance of.\"\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = RandomForestClassifier(n_estimators=50, max_features='sqrt')\nclf = clf.fit(train, target)\n", "intent": "<img src=\"images/random_forest.jpg\" width=\"600\" height=\"400\" />\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlg = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "y = tf.nn.softmax(tf.matmul(x, W) + b)\n", "intent": "$y_{i} = exp^{netinput_{i}}/\\sum_{i} exp^{netinput_{i}} $\n"}
{"snippet": "graph = tf.get_default_graph()\n", "intent": "We already have a default graph that is empty.\n"}
{"snippet": "sess = tf.Session()\nsess.run(input_value)\n", "intent": "If we want to give value to input_value, we have to run the sesseion\n"}
{"snippet": "sess = tf.Session()\n", "intent": "As before, here we'll train the network. Instead of flattening the images though, we can pass them in as 28x28x1 arrays.\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, epochs=40, batch_size=16)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model.fit(X_train, Y_train, epochs = 2, batch_size = 32)\n", "intent": "Run the following cell to train your model on 2 epochs with a batch size of 32. On a CPU it should take you around 5min per epoch. \n"}
{"snippet": "plot_model(model, to_file='model.png')\nSVG(model_to_dot(model).create(prog='dot', format='svg'))\n", "intent": "Finally, run the code below to visualize your ResNet50. You can also download a .png picture of your model by going to \"File -> Open...-> model.png\".\n"}
{"snippet": "classifier = Sequential()\nclassifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\", input_dim=11))\nclassifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\nclassifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\nclassifier.summary()\n", "intent": "Initialize ANN: Define sequence of layers or define graph, here we will use the first way\n"}
{"snippet": "from sklearn.svm import LinearSVR\nlin_svr = LinearSVR()\nlin_svr.fit(X_train_scaled, y_train)\n", "intent": "Let's train a simple `LinearSVR` first:\n"}
{"snippet": "tf.reset_default_graph()\nn_epochs = 1000\nlearning_rate = 0.01\nX = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\ny = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\ntheta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\ny_pred = tf.matmul(X, theta, name=\"predictions\")\nerror = y_pred - y\nmse = tf.reduce_mean(tf.square(error), name=\"mse\")\n", "intent": "Same as above except for the `gradients = ...` line:\n"}
{"snippet": "tf.reset_default_graph()\ndef relu(X):\n    with tf.name_scope(\"relu\"):\n        w_shape = (int(X.get_shape()[1]), 1)                          \n        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")    \n        b = tf.Variable(0.0, name=\"bias\")                             \n        z = tf.add(tf.matmul(X, w), b, name=\"z\")                      \n        return tf.maximum(z, 0., name=\"max\")                          \n", "intent": "Even better using name scopes:\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "First let's reset the default graph.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Ok, next let's reset the default graph:\n"}
{"snippet": "sess = tf.Session()\n", "intent": "As before, here wi'll train the network. Instead of flattening the images though, we can pass them in as 28x28x1 arrays.\n"}
{"snippet": "from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)\n", "intent": "Let's now try decision tree\n"}
{"snippet": "input = model.layers[0].input\noutput = model.layers[-2].output\nshort_model = Model(input, output)\n", "intent": "Remove last layer from the network, use these outputs as new features.\n"}
{"snippet": "a = tf.placeholder(dtype=tf.int32, shape=(None,))  \nb = tf.placeholder(dtype=tf.int32, shape=(None,))\nc = tf.add(a, b)\nwith tf.Session() as sess:\n  result = sess.run(c, feed_dict={\n      a: [3, 4, 5],\n      b: [-1, 2, 3]\n    })\n  print result\n", "intent": "<h2> Using a feed_dict </h2>\nSame graph, but without hardcoding inputs at build stage\n"}
{"snippet": "def linear_model(img):\n  X = tf.reshape(img,[-1,HEIGHT*WIDTH]) \n  W = tf.get_variable(\"W\", [HEIGHT*WIDTH,NCLASSES], \n                      initializer = tf.truncated_normal_initializer(stddev=0.1,seed = 1))\n  b = tf.get_variable(\"b\",NCLASSES, initializer = tf.zeros_initializer)\n  ylogits = tf.matmul(X,W)+b\n  return ylogits, NCLASSES\n", "intent": "A simple low-level matrix multiplication\n"}
{"snippet": "def dnn_model(img, mode):\n  X = tf.reshape(img, [-1, HEIGHT*WIDTH]) \n  h1 = tf.layers.dense(X, 300, activation=tf.nn.relu)\n  h2 = tf.layers.dense(h1,100, activation=tf.nn.relu)\n  h3 = tf.layers.dense(h2, 30, activation=tf.nn.relu)\n  ylogits = tf.layers.dense(h3, NCLASSES, activation=None)\n  return ylogits, NCLASSES\n", "intent": "A 3-hidden layer network that uses tf.layers to reduce the boilerplate\n"}
{"snippet": " def dnn_dropout_model(img, mode):\n  X = tf.reshape(img, [-1, HEIGHT*WIDTH]) \n  h1 = tf.layers.dense(X, 300, activation=tf.nn.relu)\n  h2 = tf.layers.dense(h1,100, activation=tf.nn.relu)\n  h3 = tf.layers.dense(h2, 30, activation=tf.nn.relu)\n  h3d = tf.layers.dropout(h3, rate=0.1, training=(mode == tf.estimator.ModeKeys.TRAIN))\n  ylogits = tf.layers.dense(h3d, NCLASSES, activation=None)\n  return ylogits, NCLASSES\n", "intent": "A 3-hidden layer network with a dropout layer between layer 3 and the output\n"}
{"snippet": "def try_out():\n  with tf.Session() as sess:\n    fn = read_dataset(tf.estimator.ModeKeys.EVAL, \n                    {'input_path': 'data', 'batch_size': 8, 'nitems': 5668, 'nusers': 82802})\n    feats, _ = fn()\n    print feats['input_rows'].eval()\n", "intent": "This code is helpful in developing the input function. You don't need it in production.\n"}
{"snippet": "def try_out():\n  with tf.Session() as sess:\n    fn = read_dataset(tf.estimator.ModeKeys.EVAL, \n                    {'input_path': 'gs://{}/wals/preproc_tft/'.format(BUCKET), \n                     'batch_size': 8, 'nitems': 5668, 'nusers': 82802})\n    feats, _ = fn()\n    print feats['input_rows'].eval()\n", "intent": "This code is helpful in developing the input function. You don't need it in production.\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS', axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "ans = fixed_model(x_var)\n", "intent": "Run the following cell to evaluate the performance of the forward pass running on the CPU:\n"}
{"snippet": "def rand(a, b):\n    return (b-a)*random.random() + a\ndef makeMatrix(I, J, fill=0.0):\n    return np.zeros([I,J])\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\ndef dsigmoid(y):\n    return (y * (1- y))\n", "intent": "This process will eventually result in our own NN class\nWhen we initialize the neural networks, the weights have to be randomly assigned.  \n"}
{"snippet": "a = np.zeros((3,3))\nta = tf.convert_to_tensor(a)\n", "intent": "Input external data into TensorFlow\n"}
{"snippet": "input1 = tf.placeholder(tf.float32)\ninput2 = tf.placeholder(tf.float32)\noutput = tf.mul(input1, input2)\nwith tf.Session() as sess:\n    print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))\n", "intent": "A ** feed_dict ** is a python dictionary mapping from tf.\nplaceholder vars (or their names) to data (numpy arrays, lists, etc.).\n"}
{"snippet": "with tf.variable_scope(\"foo\"):\n    v = tf.get_variable(\"v\", [1])\nassert v.name == \"foo/v:0\"\n", "intent": "** Case 1: reuse set to false **\n- Create and return new variable\n"}
{"snippet": "with tf.variable_scope(\"foo\"):\n    v = tf.get_variable(\"v\", [1])\nwith tf.variable_scope(\"foo\", reuse=True):\n    v1 = tf.get_variable(\"v\", [1])\nassert v1 == v    \n", "intent": "** Case 2: Variable reuse set to true **\n- Search for existing variable with given name. Raise ValueError if none found\n"}
{"snippet": "simay_upp = np.triu(simay,k=1) \ndef sim_art(n):   \n    flat = simay_upp.flatten()\n    indices = np.argpartition(flat, -n)[-n:]\n    indices = indices[np.argsort(-flat[indices])]\n    loc = np.unravel_index(indices, simay.shape)\n    print [docids.keys()[docids.values().index(loc[0][n-1])], loc[0][n-1]]\n    print [docids.keys()[docids.values().index(loc[1][n-1])], loc[1][n-1]]\n", "intent": "<p> From the heatmap above we can know that documentID 16 has more similarity with other documents \n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(solver='lbfgs').fit(X, y);\n", "intent": "Now we can fit the model.\n"}
{"snippet": "from torchvision import datasets, transforms\ntransform = transforms.Compose([transforms.ToTensor(),\n                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                             ])\ntrainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n", "intent": "The same as we saw in part 3, we'll load the MNIST dataset and define our network.\n"}
{"snippet": "images, labels = next(iter(trainloader))\nimg = images[0].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "def test_discriminator(true_count=267009):\n    tf.reset_default_graph()\n    with get_session() as sess:\n        y = discriminator(tf.ones((2, 784)))\n        cur_count = count_params()\n        if cur_count != true_count:\n            print('Incorrect number of parameters in discriminator. {0} instead of {1}. Check your achitecture.'.format(cur_count,true_count))\n        else:\n            print('Correct number of parameters in discriminator.')\ntest_discriminator()\n", "intent": "Test to make sure the number of parameters in the discriminator is correct:\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nrf_exp = RandomForestRegressor(n_estimators= 1000, random_state=42)\nrf_exp.fit(train_features, train_labels);\n", "intent": "The rf_exp uses the same number of decision trees (n_estimators) but is trained on the longer dataset with 3 additional features.\n"}
{"snippet": "rf_small = RandomForestRegressor(n_estimators=10, max_depth = 3, random_state=42)\nrf_small.fit(train_features, train_labels)\ntree_small = rf_small.estimators_[5]\nexport_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\ngraph.write_png('small_tree.png');\n", "intent": "Smaller tree for visualization.\n"}
{"snippet": "V_data = [1., 2., 3.]\nV = torch.Tensor(V_data)\nprint V\nM_data = [[1., 2., 3.], [4., 5., 6]]\nM = torch.Tensor(M_data)\nprint M\nT_data = [[[1.,2.], [3.,4.]],\n          [[5.,6.], [7.,8.]]]\nT = torch.Tensor(T_data)\nprint T\n", "intent": "Tensors can be created from Python lists with the torch.Tensor() function.\n"}
{"snippet": "x = torch.Tensor([ 1., 2., 3. ])\ny = torch.Tensor([ 4., 5., 6. ])\nz = x + y\nprint z\n", "intent": "You can operate on tensors in the ways you would expect.\n"}
{"snippet": "x = torch.randn((2,2))\ny = torch.randn((2,2))\nz = x + y \nvar_x = autograd.Variable( x )\nvar_y = autograd.Variable( y )\nvar_z = var_x + var_y \nprint var_z.grad_fn\nvar_z_data = var_z.data \nnew_var_z = autograd.Variable( var_z_data ) \nprint new_var_z.grad_fn\n", "intent": "Understanding what is going on in the block below is crucial for being a successful programmer in deep learning.\n"}
{"snippet": "for test in test_data:\n    model.zero_grad()\n    preds = model(test, ROOT_ONLY)\n    labels = test.labels[-1:] if ROOT_ONLY else test.labels\n    for pred, label in zip(preds.max(1)[1].data.tolist(), labels):\n        num_node += 1\n        if pred == label:\n            accuracy += 1\nprint(accuracy/num_node * 100)\n", "intent": "In paper, they acheived 80.2 accuracy. \n"}
{"snippet": "gbc.fit(Xtr, ytr)\n", "intent": "[[ go back to the top ]](\n"}
{"snippet": "def get_ll_layers():\n    return [ \n        BatchNormalization(input_shape=(4096,)),\n        Dropout(0.5),\n        Dense(2, activation='softmax') \n        ]\n", "intent": "The functions automate creating a model that trains the last layer from scratch, and then adds those new layers on to the main model.\n"}
{"snippet": "def get_dense_layers():\n    return [\n        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n        Flatten(),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(1000, activation='softmax')\n        ]\n", "intent": "This is our usual Vgg network just covering the dense layers:\n"}
{"snippet": "def test_generator(true_count=1858320):\n    tf.reset_default_graph()\n    with get_session() as sess:\n        y = generator(tf.ones((1, 4)))\n        cur_count = count_params()\n        if cur_count != true_count:\n            print('Incorrect number of parameters in generator. {0} instead of {1}. Check your achitecture.'.format(cur_count,true_count))\n        else:\n            print('Correct number of parameters in generator.')\ntest_generator()\n", "intent": "Test to make sure the number of parameters in the generator is correct:\n"}
{"snippet": "bnl1.set_weights([var0, mu0, mu0, var0])\nbnl4.set_weights([var2, mu2, mu2, var2])\n", "intent": "After inserting the layers, we can set their weights to the variance and mean we just calculated.\n"}
{"snippet": "def fit_model(model, batches, val_batches, nb_epoch=1):\n    model.fit_generator(batches, samples_per_epoch=batches.n, nb_epoch=nb_epoch, \n                        validation_data=val_batches, nb_val_samples=val_batches.n)\n", "intent": "We'll define a simple function for fitting models, just to save a little typing...\n"}
{"snippet": "def embedding_input(name, n_in, n_out):\n    inp = Input(shape=(1,), dtype='int64', name=name)\n    emb = Embedding(n_in, n_out, input_length=1)(inp)\n    return inp, Flatten()(emb)\n", "intent": "Create inputs and embedding outputs for each of our 3 character inputs\n"}
{"snippet": "dense_in = Dense(n_hidden, activation='relu')\n", "intent": "This is the 'green arrow' from our diagram - the layer operation from input to hidden.\n"}
{"snippet": "dense_hidden = Dense(n_hidden, activation='tanh')\n", "intent": "This is the 'orange arrow' from our diagram - the layer operation from hidden to hidden.\n"}
{"snippet": "dense_out = Dense(vocab_size, activation='softmax')\n", "intent": "This is the 'blue arrow' from our diagram - the layer operation from hidden to output.\n"}
{"snippet": "model = Model([c[0] for c in c_ins], c_out)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n", "intent": "So now we can create our model.\n"}
{"snippet": "model=Sequential([\n        Embedding(vocab_size, n_fac, input_length=cs),\n        SimpleRNN(n_hidden, activation='relu', inner_init='identity'),\n        Dense(vocab_size, activation='softmax')\n    ])\n", "intent": "This is nearly exactly equivalent to the RNN we built ourselves in the previous section.\n"}
{"snippet": "model=Sequential([\n        SimpleRNN(n_hidden, return_sequences=True, input_shape=(cs, vocab_size),\n                  activation='relu', inner_init='identity'),\n        TimeDistributed(Dense(vocab_size, activation='softmax')),\n    ])\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam())\n", "intent": "This is the keras version of the theano model that we're about to create.\n"}
{"snippet": "idx = 0\ntarget_y = 6\nX_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\nX_fooling = make_fooling_image(X_tensor[idx:idx+1], target_y, model)\nscores = model(Variable(X_fooling))\nassert target_y == scores.data.max(1)[1][0, 0], 'The model is not fooled!'\n", "intent": "Run the following cell to generate a fooling image:\n"}
{"snippet": "model=Sequential([\n        GRU(n_hidden, return_sequences=True, input_shape=(cs, vocab_size),\n                  activation='relu', inner_init='identity'),\n        TimeDistributed(Dense(vocab_size, activation='softmax')),\n    ])\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam())\n", "intent": "Identical to the last keras rnn, but a GRU!\n"}
{"snippet": "def gate(x, h, W_h, W_x, b_x):\n    return nnet.sigmoid(T.dot(x, W_x) + b_x + T.dot(h, W_h))\n", "intent": "Here's the definition of a gate - it's just a sigmoid applied to the addition of the dot products of the input vectors.\n"}
{"snippet": "model = Model([inp, sz_inp], x)\nmodel.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "When we compile the model, we have to specify all the input layers in an array.\n"}
{"snippet": "model.fit([conv_feat, trn_sizes], trn_labels, batch_size=batch_size, nb_epoch=3, \n             validation_data=([conv_val_feat, val_sizes], val_labels))\n", "intent": "And when we train the model, we have to provide all the input layers' data in an array.\n"}
{"snippet": "vocab = sorted(set(flatten(stories)))\nvocab.insert(0, '<PAD>')\nvocab_size = len(vocab)\n", "intent": "Create vocabulary of corpus and find size, including a padding element.\n"}
{"snippet": "def emb_sent_bow(inp):\n    emb = TimeDistributed(Embedding(vocab_size, emb_dim))(inp)\n    return Lambda(lambda x: K.sum(x, 2))(emb)\n", "intent": "We use <tt>TimeDistributed</tt> here to apply the embedding to every element of the sequence, then the <tt>Lambda</tt> layer adds them up\n"}
{"snippet": "K.set_value(answer.optimizer.lr, 1e-2)\nhist=answer.fit(inps, answers_train, **parms, nb_epoch=4, batch_size=32,\n           validation_data=(val_inps, answers_test))\n", "intent": "And it works extremely well\n"}
{"snippet": "f = Model([inp_story, inp_q], match)\n", "intent": "We can look inside our model to see how it's weighting the sentence embeddings.\n"}
{"snippet": "K.set_value(answer.optimizer.lr, 5e-3)\nhist=answer.fit(inps, answers_train, **parms, nb_epoch=8, batch_size=32,\n           validation_data=(val_inps, answers_test))\n", "intent": "Fitting this model can be tricky.\n"}
{"snippet": "dfDelayData = dfDelayData[dfDelayData.DepDelay.notnull()]\ndfDelayData = dfDelayData.join(pd.get_dummies(dfDelayData['UniqueCarrier'], prefix='carrier'))\ndfDelayData = dfDelayData.join(pd.get_dummies(dfDelayData['DayOfWeek'], prefix='dow'))\nmodel = lm.LogisticRegression()\nfeatures = [i for i in dfDelayData.columns if 'dow_' in i] \n", "intent": "In the section below, I am predicting the probability of delay based upon the time of the day.\n"}
{"snippet": "def conv(x, nf, sz, wd, p):\n    x = Convolution2D(nf, sz, sz, init='he_uniform', border_mode='same', \n                          W_regularizer=l2(wd))(x)\n    return dropout(x,p)\n", "intent": "Convolutional layer:\n* L2 Regularization\n* 'same' border mode returns same width/height\n* Pass output through Dropout\n"}
{"snippet": "model.fit(x_train, y_train, 64, 20, validation_data=(x_test, y_test), **parms)\n", "intent": "This will likely need to run overnight + lr annealing...\n"}
{"snippet": "def meanshift(data):\n    X = np.copy(data)\n    for it in range(5):\n        for i, x in enumerate(X):\n            dist = np.sqrt(((x-X)**2).sum(1))\n            weight = gaussian(dist, 2.5)\n            X[i] = (np.expand_dims(weight,1)*X).sum(0) / weight.sum()\n    return X\n", "intent": "In our implementation, we choose the bandwidth to be 2.5. \nOne easy way to choose bandwidth is to find which bandwidth covers one third of the data.\n"}
{"snippet": "def meanshift(data):\n    X = torch.FloatTensor(np.copy(data))\n    for it in range(5):\n        for i, x in enumerate(X):\n            dist = torch.sqrt((sub(x, X)**2).sum(1))\n            weight = gaussian(dist, 3)\n            num = mul(weight, X).sum(0)\n            X[i] = num / weight.sum()\n    return X\n", "intent": "And the implementation of meanshift is nearly identical too!\n"}
{"snippet": "inp,outp=get_model(arr_hr)\nmodel_hr = Model(inp, outp)\ncopy_weights(top_model.layers, model_hr.layers)\n", "intent": "Since the CNN is fully convolutional, we can use it one images of arbitrary size. Let's try it w/ the high-res as the input.\n"}
{"snippet": "x = torch.Tensor(5, 3); x\n", "intent": "Tensors are similar to numpy's ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n"}
{"snippet": "x = torch.randn(3)\nx = Variable(x, requires_grad = True)\n", "intent": "Because PyTorch is a dynamic computation framework, we can take the gradients of all kinds of interesting computations, even loops!\n"}
{"snippet": "input = Variable(torch.randn(1, 1, 32, 32)).cuda()\nout = net(input); out\n", "intent": "The input to the forward is a `Variable`, and so is the output.\n"}
{"snippet": "outputs = net(Variable(images).cuda())\n_, predicted = torch.max(outputs.data, 1)\n' '.join('%5s'% classes[predicted[j][0]] for j in range(4))\n", "intent": "Okay, now let us see what the neural network thinks these examples above are:\n"}
{"snippet": "X = iris[[\"petal_length\"]]\ny = iris[\"petal_width\"]\nmodel = linear_model.LinearRegression()\nresults = model.fit(X, y)\nprint results.intercept_, results.coef_ \n", "intent": "Now let's use scikit-learn to find the best fit line.\n"}
{"snippet": "def get_contin(feat):\n    name = feat[0][0]\n    inp = Input((1,), name=name+'_in')\n    return inp, Dense(1, name=name+'_d', init=my_init(1.))(inp)\n", "intent": "Helper function for continuous inputs.\n"}
{"snippet": "model = Model([inp, inp_dec], x)\nmodel.compile(Adam(), 'sparse_categorical_crossentropy', metrics=['acc'])\n", "intent": "We can now train, passing in the decoder inputs as well for teacher forcing.\n"}
{"snippet": "ms = MeanShift(bandwidth=bw, bin_seeding=True, min_bin_freq=5)\nms.fit(y_targ)\n", "intent": "This takes some time\n"}
{"snippet": "def relu(x): return Activation('relu')(x)\ndef dropout(x, p): return Dropout(p)(x) if p else x\ndef bn(x): return BatchNormalization(mode=2, axis=-1)(x)\ndef relu_bn(x): return relu(bn(x))\ndef concat(xs): return merge(xs, mode='concat', concat_axis=-1)\n", "intent": "This should all be familiar.\n"}
{"snippet": "happyModel.fit(X_train, Y_train, epochs=40, batch_size=50)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "m = RandomForestRegressor(n_jobs=-1)\nm.fit(df, y)\nm.score(df,y)\n", "intent": "We now have something we can pass to a random forest!\n"}
{"snippet": "def get_oob(df):\n    m = RandomForestRegressor(n_estimators=30, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)\n    x, _ = split_vals(df, n_trn)\n    m.fit(x, y_train)\n    return m.oob_score_\n", "intent": "Let's try removing some of these related features to see if the model can be simplified without impacting the accuracy.\n"}
{"snippet": "df_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(df_trn2, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\nm.fit(X_train, y_train);\n", "intent": "This next analysis will be a little easier if we use the 1-hot encoded categorical variables, so let's load them up again.\n"}
{"snippet": "net = nn.Sequential(\n    nn.Linear(28*28, 100),\n    nn.ReLU(),\n    nn.Linear(100, 100),\n    nn.ReLU(),\n    nn.Linear(100, 10),\n    nn.LogSoftmax()\n).cuda()\n", "intent": "We will begin with the highest level abstraction: using a neural net defined by PyTorch's Sequential class.  \n"}
{"snippet": "model = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary()) \n", "intent": "This means that our best fit line is:\n$$y = a + b x$$\nwhere $a = -0.363075521319$ and $b = 0.41575542$.\nNext let's use `statsmodels`.\n"}
{"snippet": "l = loss(y_pred, Variable(yt).cuda())\nprint(l)\n", "intent": "We can check the loss:\n"}
{"snippet": "xt, yt = next(dl)\ny_pred = net2(Variable(xt).cuda())\n", "intent": "Now, let's make another set of predictions and check if our loss is lower:\n"}
{"snippet": "for t in range(100):\n    xt, yt = next(dl)\n    y_pred = net2(Variable(xt).cuda())\n    l = loss(y_pred, Variable(yt).cuda())\n    if t % 10 == 0:\n        accuracy = np.mean(to_np(y_pred).argmax(axis=1) == to_np(yt))\n        print(\"loss: \", l.data[0], \"\\t accuracy: \", accuracy)\n    optimizer.zero_grad()\n    l.backward()\n    optimizer.step()\n", "intent": "If we run several iterations in a loop, we should see the loss decrease and the accuracy increase with time.\n"}
{"snippet": "net = nn.Sequential(\n    nn.Linear(28*28, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10)\n).cuda()\n", "intent": "We will begin with the highest level abstraction: using a neural net defined by PyTorch's Sequential class.  \n"}
{"snippet": "def meanshift(data):\n    X = np.copy(data)\n    for it in range(5): X = meanshift_iter(X)\n    return X\n", "intent": "By repeating this a few times, we can make the clusters more accurate.\n"}
{"snippet": "def meanshift(data):\n    X = torch.from_numpy(np.copy(data)).cuda()\n    for it in range(5): X = meanshift_iter(X)\n    return X\n", "intent": "And then we'll use the exact same code as before, but first convert our numpy array to a GPU PyTorch tensor.\n"}
{"snippet": "import pymc3 as pm\nwith pm.Model() as model:\n    parameter = pm.Exponential(\"poisson_param\", 1)\n    data_generator = pm.Poisson(\"data_generator\", parameter)\n", "intent": "In PyMC3, we typically handle all the variables we want in our model within the context of the `Model` object.\n"}
{"snippet": "alpha = 1./20.\nlambda_1, lambda_2 = np.random.exponential(scale=1/alpha, size=2)\nprint(lambda_1, lambda_2)\n", "intent": "2\\. Draw $\\lambda_1$ and $\\lambda_2$ from an $\\text{Exp}(\\alpha)$ distribution:\n"}
{"snippet": "import pymc3 as pm\nimport theano.tensor as tt\nfrom theano.tensor.nlinalg import matrix_inverse, diag, matrix_dot\nprior_mu = np.array([x[0] for x in expert_prior_params.values()])\nprior_std = np.array([x[1] for x in expert_prior_params.values()])\ninit = stock_returns.cov()\nwith pm.Model() as model:\n    cov_matrix = pm.WishartBartlett(\"covariance\", np.diag(prior_std**2), 10, testval = init)\n    mu = pm.Normal(\"returns\", mu=prior_mu, sd=1, shape=4)\n", "intent": "And here let's form our basic model:\n"}
{"snippet": "X = iris[[\"petal_length\", \"setosa\", \"versicolor\", \"virginica\"]]\nX = sm.add_constant(X) \ny = iris[\"petal_width\"]\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary()) \n", "intent": "Now we perform a multilinear regression with the dummy variables added.\n"}
{"snippet": "import theano\nimport theano.tensor as T\nstates = T.matrix(\"states[batch,units]\")\nactions = T.ivector(\"action_ids[batch]\")\ncumulative_rewards = T.vector(\"G[batch] = r + gamma*r' + gamma^2*r'' + ...\")\n", "intent": "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n"}
{"snippet": "import tensorflow as tf\ntf.reset_default_graph()\nsess = tf.InteractiveSession()\n", "intent": "Here's a code for an agent that only uses feedforward layers. Please read it carefully: you'll have to extend it later!\n"}
{"snippet": "from agentnet.learning.generic import get_values_for_actions, get_mask_by_eos\nclass llh_trainer:\n    input_sequence = T.imatrix(\"input sequence [batch,time]\")\n    reference_answers = T.imatrix(\"reference translations [batch, time]\")\n    logprobs_seq = <YOUR CODE>\n    crossentropy = - get_values_for_actions(logprobs_seq,reference_answers)\n    mask = get_mask_by_eos(T.eq(reference_answers, out_voc.eos_ix))\n    loss = T.sum(crossentropy * mask)/T.sum(mask)\n    updates = <YOUR CODE>\n    train_step = theano.function([input_sequence,reference_answers], loss, updates=updates)\n", "intent": "Here we define a function that trains our model through maximizing log-likelihood a.k.a. minimizing crossentropy.\n"}
{"snippet": "get_policy = theano.function([observations], probs, allow_input_downcast=True)\ndef act(obs, sample=True):\n    policy = get_policy([obs])[0]\n    if sample:\n        action = int(np.random.choice(n_actions, p=policy))\n    else:\n        action = int(np.argmax(policy))\n    return action, policy\n", "intent": "In this section, we'll define functions that take actions $ a \\sim \\pi_\\theta(a|s) $ and rollouts $ <s_0,a_0,s_1,a_1,s_2,a_2,...s_n,a_n> $.\n"}
{"snippet": "model.fit(X, y)\n", "intent": "Now it is time to apply our model to data.\nThis can be done with the ``fit()`` method of the model:\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier().fit(X, y)\n", "intent": "This process of fitting a decision tree to our data can be done in Scikit-Learn with the ``DecisionTreeClassifier`` estimator:\n"}
{"snippet": "visualize_classifier(DecisionTreeClassifier(), X, y)\n", "intent": "Now we can examine what the decision tree classification looks like:\n"}
{"snippet": "model = grid.best_estimator_\nmodel.fit(X_train, y_train)\n", "intent": "Let's take the best estimator and re-train it on the full dataset:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\nmodel.summary()\n", "intent": "**Q:** _How hard can it be to build a Multi-Layer Fully-Connected Network with keras?_\n**A:** _It is basically the same, just add more layers!_\n"}
{"snippet": "test_models = [LinearRegression(), Ridge(alpha=10), Lasso(alpha=10)]\nscores = [analyze_performance(my_model) for my_model in test_models]\n", "intent": "Let's try a few degrees with a regularized model.\n"}
{"snippet": "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), \n              metrics=['accuracy'])\nnetwork_history = model.fit(X_train, Y_train, batch_size=128, \n                            epochs=2, verbose=1, validation_data=(X_val, Y_val))\n", "intent": "Try increasing the number of epochs (if you're hardware allows to)\n"}
{"snippet": "def sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n", "intent": "First we need to create a sigmoid function.  The code for this is pretty simple.\n"}
{"snippet": "def cost(theta, X, y):\n    theta = np.matrix(theta)\n    X = np.matrix(X)\n    y = np.matrix(y)\n    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))\n    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))\n    return np.sum(first - second) / (len(X))\n", "intent": "Excellent!  Now we need to write the cost function to evaluate a solution.\n"}
{"snippet": "def costReg(theta, X, y, learningRate):\n    theta = np.matrix(theta)\n    X = np.matrix(X)\n    y = np.matrix(y)\n    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))\n    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))\n    reg = (learningRate / 2 * len(X)) * np.sum(np.power(theta[:,1:theta.shape[1]], 2))\n    return np.sum(first - second) / (len(X)) + reg\n", "intent": "Now we need to modify the cost and gradient functions from part 1 to include the regularization term.  First the cost function:\n"}
{"snippet": "svc.fit(data[['X1', 'X2']], data['y'])\nsvc.score(data[['X1', 'X2']], data['y'])\n", "intent": "For the first experiment we'll use C=1 and see how it performs.\n"}
{"snippet": "svc2 = svm.LinearSVC(C=100, loss='hinge', max_iter=1000)\nsvc2.fit(data[['X1', 'X2']], data['y'])\nsvc2.score(data[['X1', 'X2']], data['y'])\n", "intent": "It appears that it mis-classified the outlier.  Let's see what happens with a larger value of C.\n"}
{"snippet": "print(softmax([10, 2, -3]))\n", "intent": "Make sure that this works one vector at a time (and check that the components sum to one):\n"}
{"snippet": "X = np.array([[10, 2, -3],\n              [-1, 5, -20]])\nprint(softmax(X))\n", "intent": "Note that a naive implementation of softmax might not be able process a batch of activations in a single call:\n"}
{"snippet": "def softmax(X):\n    exp = np.exp(X)\n    return exp / np.sum(exp, axis=-1, keepdims=True)\nprint(\"softmax of a single vector:\")\nprint(softmax([10, 2, -3]))\n", "intent": "Here is a way to implement softmax that works both for an individal vector of activations and for a batch of activation vectors at once:\n"}
{"snippet": "y = df[\"Hired\"]\nX = df[features]\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X,y)\n", "intent": "Now actually construct the decision tree:\n"}
{"snippet": "print(np.sum(softmax(X), axis=1))\n", "intent": "The sum of probabilities for each input vector of logits should some to 1:\n"}
{"snippet": "from keras.layers import Input\nfrom keras.models import Model\nx = Input(shape=[1], name='input')\nembedding = embedding_layer(x)\nmodel = Model(inputs=x, outputs=embedding)\n", "intent": "Let's use it as part of a Keras model:\n"}
{"snippet": "from keras.layers import Input\nfrom keras.models import Model\nx = Input(shape=[1], name='input')\nembedding = embedding_layer(x)\nmodel = Model(input=x, output=embedding)\nmodel.output_shape\n", "intent": "Let's use it as part of a Keras model:\n"}
{"snippet": "x = tf.placeholder(tf.float32, shape=[None, 784])\ny_true = tf.placeholder(tf.float32, shape=[None, 10])\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\ny_pred = tf.matmul(x,W) + b\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true)\nloss = tf.reduce_mean(cross_entropy)\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\ncorrect_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y_true,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n", "intent": "- A logistic regression without taking into account the spatiality of the data\n- Very similar to lab01\n"}
{"snippet": "x = tf.placeholder(tf.float32, shape=[None, 784])\ny_true = tf.placeholder(tf.float32, shape=[None, 10])\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\ny_pred = tf.matmul(x,W) + b\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(y_pred, y_true)\nloss = tf.reduce_mean(cross_entropy)\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\ncorrect_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y_true,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n", "intent": "- A logistic regression without taking into account the spatiality of the data\n- Very similar to lab01\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\ntop_model = Sequential()\ntop_model.add(Dense(1, input_dim=n_features, activation='sigmoid'))\ntop_model.compile(optimizer=Adam(lr=1e-4),\n                  loss='binary_crossentropy', metrics=['accuracy'])\ntop_model.fit(features_train, labels_train,\n              validation_split=0.1, verbose=2, epochs=15)\n", "intent": "Let's define the classification model:\n"}
{"snippet": "history = stupid_model.fit(inputs, [out_cls, out_boxes],\n                           batch_size=10, epochs=10)\n", "intent": "Now check whether the loss decreases and eventually if we are able to overfit on these few examples for debugging purpose.\n"}
{"snippet": "history = model.fit(inputs, [out_cls, out_boxes],\n                    batch_size=10, nb_epoch=10)\n", "intent": "Now check whether the loss decreases and eventually if we are able to overfit on these few examples for debugging purpose.\n"}
{"snippet": "from keras.models import Sequential\nmodel = Sequential([SoftmaxMap(input_shape=(w, h, n_classes))])\nmodel.output_shape\n", "intent": "Let's wrap the `SoftmaxMap` class into a test model to process our test data:\n"}
{"snippet": "from sklearn import svm, datasets\nC = 1.0\nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\n", "intent": "Now we'll use linear SVC to partition our graph into clusters:\n"}
{"snippet": "small_train = slice(0, None, 40)\nmodel.fit(X[small_train], y[small_train], validation_split=0.1,\n          batch_size=128, nb_epoch=1)\n", "intent": "Let's train the model for one epoch on a very small subset of the training set to check that it's well defined:\n"}
{"snippet": "pretrained_embedding_layer = Embedding(\n    MAX_NB_WORDS, EMBEDDING_DIM,\n    weights=[embedding_matrix],\n    input_length=MAX_SEQUENCE_LENGTH,\n)\n", "intent": "Build a layer with pre-trained embeddings:\n"}
{"snippet": "simple_seq2seq = load_model(best_model_fname)\n", "intent": "Let's load the best model found on the validation set at the end of training:\n"}
{"snippet": "net = gluon.nn.Sequential()\nwith net.name_scope():\n    net.add(gluon.nn.Conv2D(channels=20, kernel_size=5, activation='relu'))\n    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))            \n    net.add(gluon.nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n    trl = TRL([10, 3, 3, 10], num_outputs)\n    net.add(trl)\n", "intent": "Here, we define a simple Convolution Neural Network in which, instead of having a flattening + fully connected layer for output, we use a TRL:\n"}
{"snippet": "mu = Variable(torch.zeros(1))   \nsigma = Variable(torch.ones(1)) \nx = dist.normal(mu, sigma)      \nprint(x)\n", "intent": "Let's draw a sample from a unit normal distribution:\n"}
{"snippet": "mu = pyro.param(\"mu\", Variable(torch.zeros(1), requires_grad=True))\nprint(mu)\n", "intent": "We can also declare mu as a named parameter:\n"}
{"snippet": "import torch.nn as nn\nz_dim=20\nhidden_dim=100\nnn_decoder = nn.Sequential(\n    nn.Linear(z_dim, hidden_dim), \n    nn.Softplus(), \n    nn.Linear(hidden_dim, 784), \n    nn.Sigmoid()\n)\n", "intent": "First we define our decoder network\n"}
{"snippet": "from pyro.util import ng_zeros, ng_ones \ndef model(x):\n    batch_size=x.size(0)\n    pyro.module(\"decoder\", nn_decoder)  \n    z = pyro.sample(\"z\", dist.normal,   \n                    ng_zeros(batch_size, z_dim), \n                    ng_ones(batch_size, z_dim))\n    bern_prob = nn_decoder(z)          \n    return pyro.sample(\"x\", dist.bernoulli, bern_prob, obs=x) \n", "intent": "Now we can define our generative model conditioned on the observed mini-batch of images `x`:\n"}
{"snippet": "import torchvision.datasets as dset\nimport torchvision.transforms as transforms\nbatch_size=250\ntrans = transforms.ToTensor()\ntrain_set = dset.MNIST(root='./mnist_data', train=True, \n                       transform=trans, download=True)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_set, \n                                           batch_size=batch_size,\n                                           shuffle=True)\n", "intent": "Let's setup a basic training loop. First we setup the data loader:\n"}
{"snippet": "def conv_layer(prev_layer, layer_num, is_training):\n    strides = 2 if layer_num % 3 == 0 else 1\n    conv_layer = tf.layers.conv2d(prev_layer, layer_num*4, 3, strides, 'same', use_bias=True, activation=tf.nn.relu)\n    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n    return conv_layer\n", "intent": "**Alternate solution that uses a bias and ReLU activation function _before_ batch normalization.**\n"}
{"snippet": "def geom_image_prior(x, step=0):\n    p = Variable(torch.Tensor([0.4]))\n    i = pyro.sample('i{}'.format(step), dist.bernoulli, p)\n    if i.data[0] == 1:\n        return x\n    else:\n        x = x + prior_step(step)  \n        return geom_image_prior(x, step + 1)\n", "intent": "Now we can use `prior_step` to define a recursive prior over images:\n"}
{"snippet": "ix = torch.LongTensor([9906, 1879, 5650,  967, 7420, 7240, 2755, 9390,   42, 5584])\nn_images = len(ix)\nexamples_to_viz = X[ix]\n", "intent": "Let's pick some datapoints from the test set to reconstruct and visualize:\n"}
{"snippet": "x = Variable(torch.ones(2, 2), requires_grad=True)\nprint(x)\n", "intent": "Example taken from [this tutorial](http://pytorch.org/tutorials/beginner/former_torchies/autograd_tutorial.html) in the official documentation.\n"}
{"snippet": "proj_t = np.reshape(proj_operator.todense().A, (l//7,l,l,l))\n", "intent": "dimensions: angles (l//7), positions (l), image for each (l x l)\n"}
{"snippet": "regr = linear_model.LinearRegression()\n", "intent": "Let's start by using the sklearn implementation:\n"}
{"snippet": "n_hidden = 128\nrnn = RNN(n_letters, n_hidden, n_categories)\n", "intent": "With our custom `RNN` class defined, we can create a new instance:\n"}
{"snippet": "import random\ndef random_training_pair():                                                                                                               \n    category = random.choice(all_categories)\n    line = random.choice(category_lines[category])\n    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n    line_tensor = Variable(line_to_tensor(line))\n    return category, line, category_tensor, line_tensor\nfor i in range(10):\n    category, line, category_tensor, line_tensor = random_training_pair()\n    print('category =', category, '/ line =', line)\n", "intent": "We will also want a quick way to get a training example (a name and its language):\n"}
{"snippet": "small_hidden_size = 8\nsmall_n_layers = 2\nencoder_test = EncoderRNN(input_lang.n_words, small_hidden_size, small_n_layers)\ndecoder_test = LuongAttnDecoderRNN('general', small_hidden_size, output_lang.n_words, small_n_layers)\nif USE_CUDA:\n    encoder_test.cuda()\n    decoder_test.cuda()\n", "intent": "Create models with a small size (a good idea for eyeball inspection):\n"}
{"snippet": "from sklearn.linear_model import Perceptron\nppn = Perceptron(n_iter=40, eta0=0.1, random_state=1)\nppn.fit(X_train_std, y_train)\n", "intent": "Redefining the `plot_decision_region` function from chapter 2:\n"}
{"snippet": "def conv_layer(prev_layer, layer_num, is_training):\n    strides = 2 if layer_num % 3 == 0 else 1\n    conv_layer = tf.layers.conv2d(prev_layer, layer_num*4, 3, strides, 'same', use_bias=False, activation=tf.nn.relu)\n    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n    return conv_layer\n", "intent": "**Alternate solution that uses a ReLU activation function _before_ normalization, but no bias.**\n"}
{"snippet": "with tf.Session(graph=g2) as sess:\n    try:\n        sess.run(init_op)\n        print('w2:', sess.run(w2))\n    except tf.errors.FailedPreconditionError as e:\n        print(e)\n", "intent": "Error if a variable is not initialized:\n"}
{"snippet": "g2 = tf.Graph()\nwith tf.Session(graph=g2) as sess:\n    new_saver = tf.train.import_meta_graph(\n        './trained-model.meta')\n    new_saver.restore(sess, './trained-model')\n    y_pred = sess.run('y_hat:0', \n                      feed_dict={'tf_x:0' : x_test})\n", "intent": "Restoring the saved model:\n"}
{"snippet": "from sklearn.linear_model import Perceptron\nppn = Perceptron(n_iter=40, eta0=0.1, random_state=0)\nppn.fit(X_train_std, y_train)\n", "intent": "Redefining the `plot_decision_region` function from chapter 2:\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr = lr.fit(X_train_pca, y_train)\n", "intent": "Training logistic regression classifier using the first 2 principal components.\n"}
{"snippet": "import keras.backend as K\nK.set_image_data_format('channels_first')\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, ZeroPadding2D, BatchNormalization, Input\nfrom keras.layers import Conv2DTranspose, Reshape, Activation, Cropping2D, Flatten\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.activations import relu\nfrom keras.initializers import RandomNormal\nconv_init = RandomNormal(0, 0.02)\ngamma_init = RandomNormal(1., 0.02)\n", "intent": "modifed from https://github.com/martinarjovsky/WassersteinGAN \n"}
{"snippet": "def G(z, w=g_weights):\n    h1 = tf.nn.relu(tf.matmul(z, w['w1']) + w['b1'])\n    return tf.sigmoid(tf.matmul(h1, w['out']) + w['b2'])\ndef D(x, w=d_weights):\n    h1 = tf.nn.relu(tf.matmul(x, w['w1']) + w['b1'])\n    return tf.sigmoid(tf.matmul(h1, w['out']) + w['b2'])\n", "intent": "The models were chosen to be very simple, so just an MLP with 1 hidden layer and 1 output layer.\n"}
{"snippet": "with tf.Session() as sess:\n    result = sess.run(neg_x)\n    print(result)\n", "intent": "You need to summon a session so you can launch the negation op:\n"}
{"snippet": "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n    result = sess.run(neg_op)\n    print(result)\n", "intent": "Now let's use a session with a special argument passed in.\n"}
{"snippet": "import tensorflow as tf\nsess = tf.InteractiveSession()\nraw_data = [1., 2., 8., -1., 0., 5.5, 6., 13]\nspikes = tf.Variable([False] * len(raw_data), name='spikes')\nspikes.initializer.run()\n", "intent": "Create an interactive session and initialize a variable:\n"}
{"snippet": "with train_graph.as_default():\n    saver = tf.train.Saver()\nwith tf.Session(graph=train_graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    embed_mat = sess.run(embedding)\n", "intent": "Restore the trained network if you need to:\n"}
{"snippet": "alpha = tf.constant(0.05)\ncurr_value = tf.placeholder(tf.float32)\nprev_avg = tf.Variable(0.)\nupdate_avg = alpha * curr_value + (1 - alpha) * prev_avg\n", "intent": "The moving average is defined as follows:\n"}
{"snippet": "init = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    for i in range(len(raw_data)):\n        summary_str, curr_avg = sess.run([merged, update_avg], feed_dict={curr_value: raw_data[i]})\n        sess.run(tf.assign(prev_avg, curr_avg))\n        print(raw_data[i], curr_avg)\n        writer.add_summary(summary_str, i)\n", "intent": "Time to compute the moving averages. We'll also run the `merged` op to track how the values change:\n"}
{"snippet": "def model(X, w):\n    return tf.multiply(X, w)\n", "intent": "Define the model as `y = w'*x`\n"}
{"snippet": "w = tf.Variable(0.0, name=\"weights\")\n", "intent": "Set up the weights variable\n"}
{"snippet": "y_model = model(X, w)\ncost = tf.reduce_mean(tf.square(Y-y_model))\n", "intent": "Define the cost function as the mean squared error\n"}
{"snippet": "sess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n", "intent": "Initialize all variables\n"}
{"snippet": "def model(X, w):\n    terms = []\n    for i in range(num_coeffs):\n        term = tf.multiply(w[i], tf.pow(X, i))\n        terms.append(term)\n    return tf.add_n(terms)\n", "intent": "Define our polynomial model\n"}
{"snippet": "w = tf.Variable([0.] * num_coeffs, name=\"parameters\")\ny_model = model(X, w)\n", "intent": "Set up the parameter vector to all zero\n"}
{"snippet": "sess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\nfor epoch in range(training_epochs):\n    for (x, y) in zip(trX, trY):\n        sess.run(train_op, feed_dict={X: x, Y: y})\nw_val = sess.run(w)\nprint(w_val)\n", "intent": "Set up the session and run the learning algorithm just as before\n"}
{"snippet": "def discriminator(x, n_units=128, reuse=False, alpha=0.01):\n    with tf.variable_scope('discriminator', reuse=reuse):\n        h1 = tf.layers.dense(x, n_units, activation=None)\n        h1 = tf.maximum(alpha * h1, h1)\n        logits = tf.layers.dense(h1, 1, activation=None)\n        out = tf.sigmoid(logits)\n        return out, logits\n", "intent": "The discriminator network is almost exactly the same as the generator network, except that we're using a sigmoid output layer.\n"}
{"snippet": "learning_rate = 0.001\ntraining_epochs = 1000\nX = tf.placeholder(\"float\")\nY = tf.placeholder(\"float\")\nw = tf.Variable([0., 0.], name=\"parameters\")\n", "intent": "Define the hyper-parameters, placeholders, and variables:\n"}
{"snippet": "y_model = model(X, w)\ncost = tf.reduce_sum(tf.square(Y-y_model))\n", "intent": "Given a model, define the cost function:\n"}
{"snippet": "sess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n", "intent": "Prepare the session:\n"}
{"snippet": "X = tf.placeholder(tf.float32, shape=(None,), name=\"x\")\nY = tf.placeholder(tf.float32, shape=(None,), name=\"y\")\nw = tf.Variable([0., 0.], name=\"parameter\", trainable=True)\ny_model = tf.sigmoid(w[1] * X + w[0])\ncost = tf.reduce_mean(-Y * tf.log(y_model) - (1 - Y) * tf.log(1 - y_model))\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n", "intent": "Define the placeholders, variables, model, cost function, and training op:\n"}
{"snippet": "X1 = tf.placeholder(tf.float32, shape=(None,), name=\"x1\")\nX2 = tf.placeholder(tf.float32, shape=(None,), name=\"x2\")\nY = tf.placeholder(tf.float32, shape=(None,), name=\"y\")\nw = tf.Variable([0., 0., 0.], name=\"w\", trainable=True)\ny_model = tf.sigmoid(-(w[2] * X2 + w[1] * X1 + w[0]))\ncost = tf.reduce_mean(-tf.log(y_model * Y + (1 - y_model) * (1 - Y)))\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n", "intent": "Define placeholders, variables, model, and the training op:\n"}
{"snippet": "x1_boundary, x2_boundary = [], []\nwith tf.Session() as sess:\n    for x1_test in np.linspace(0, 10, 20):\n        for x2_test in np.linspace(0, 10, 20):\n            z = sess.run(tf.sigmoid(-x2_test*w_val[2] - x1_test*w_val[1] - w_val[0]))\n            if abs(z - 0.5) < 0.05:\n                x1_boundary.append(x1_test)\n                x2_boundary.append(x2_test)\n", "intent": "Here's one hacky, but simple, way to figure out the decision boundary of the classifier: \n"}
{"snippet": "train_size, num_features = xs.shape\nX = tf.placeholder(\"float\", shape=[None, num_features])\nY = tf.placeholder(\"float\", shape=[None, num_labels])\nW = tf.Variable(tf.zeros([num_features, num_labels]))\nb = tf.Variable(tf.zeros([num_labels]))\ny_model = tf.nn.softmax(tf.matmul(X, W) + b)\ncost = -tf.reduce_sum(Y * tf.log(y_model))\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\ncorrect_prediction = tf.equal(tf.argmax(y_model, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n", "intent": "Again, define the placeholders, variables, model, and cost function:\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    X, names = get_dataset(sess)\n    centroids = initial_cluster_centroids(X, k)\n    i, converged = 0, False\n    while not converged and i < max_iterations:\n        i += 1\n        Y = assign_cluster(X, centroids)\n        centroids = sess.run(recompute_centroids(X, Y))\n    print(zip(sess.run(Y), names))\n", "intent": "Open a session, obtain a dataset, and cluster the data:\n"}
{"snippet": "x = tf.reshape(raw_data, shape=[-1, 24, 24, 1])\nW = tf.Variable(tf.random_normal([5, 5, 1, 32]))\nb = tf.Variable(tf.random_normal([32]))\nconv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\nconv_with_b = tf.nn.bias_add(conv, b)\nconv_out = tf.nn.relu(conv_with_b)\nk = 2\nmaxpool = tf.nn.max_pool(conv_out, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n", "intent": "Define the TensorFlow ops:\n"}
{"snippet": "from distutils.version import LooseVersion\nimport warnings\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nassert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\nprint('TensorFlow Version: {}'.format(tf.__version__))\nif not tf.test.gpu_device_name():\n    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\nelse:\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n", "intent": "This will check to make sure you have the correct version of TensorFlow and access to a GPU\n"}
{"snippet": "def conv_layer(x, W, b):\n    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n    conv_with_b = tf.nn.bias_add(conv, b)\n    conv_out = tf.nn.relu(conv_with_b)\n    return conv_out\ndef maxpool_layer(conv, k=2):\n    return tf.nn.max_pool(conv, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n", "intent": "Define helper functions for the convolution and maxpool layers:\n"}
{"snippet": "model_op = model()\ncost = tf.reduce_mean(\n    tf.nn.softmax_cross_entropy_with_logits(logits=model_op, labels=y)\n)\ntrain_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\ncorrect_pred = tf.equal(tf.argmax(model_op, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n", "intent": "Here's the cost function to train the classifier.\n"}
{"snippet": "def make_cell(state_dim):\n    return tf.contrib.rnn.LSTMCell(state_dim)\n", "intent": "Now let's make a helper function to create LSTM cells\n"}
{"snippet": "with tf.variable_scope(\"first_cell\") as scope:\n    cell = make_cell(state_dim=10)\n    outputs, states = tf.nn.dynamic_rnn(cell, input_placeholder, dtype=tf.float32)\n", "intent": "Call the function and extract the cell outputs.\n"}
{"snippet": "multi_cell = make_multi_cell(state_dim=10, num_layers=5)\noutputs5, states5 = tf.nn.dynamic_rnn(multi_cell, input_placeholder, dtype=tf.float32)\n", "intent": "Here's the helper function in action:\n"}
{"snippet": "layer   = model.layers[-4] \nweights = layer.get_weights()\nnew_kernel = np.random.normal(size=weights[0].shape)/(GRID_H*GRID_W)\nnew_bias   = np.random.normal(size=weights[1].shape)/(GRID_H*GRID_W)\nlayer.set_weights([new_kernel, new_bias])\n", "intent": "**Randomize weights of the last layer**\n"}
{"snippet": "layer = model.layers[-4] \nweights = layer.get_weights()\nnew_kernel = np.random.normal(size=weights[0].shape)/(GRID_H*GRID_W)\nnew_bias   = np.random.normal(size=weights[1].shape)/(GRID_H*GRID_W)\nlayer.set_weights([new_kernel, new_bias])\n", "intent": "**Randomize weights of the last layer**\n"}
{"snippet": "print(\"operations\")\noperations = [op.name for op in tf.get_default_graph().get_operations()]\nprint(operations)\nprint\nprint(\"variables\")\nvariables = [var.name for var in tf.all_variables()]\nprint(variables)\n", "intent": "Notice that our weights and operations defined in the `l_1` space are saved in the `l_1` directory of the graph.\n"}
{"snippet": "shape = [5, 5, 5]\ntensor = Variable(tl.tensor(rng.random_sample(shape)), requires_grad=True)\n", "intent": "Define a random tensor which we will try to decompose. We wrap our tensors in Variables so we can backpropagate through them:\n"}
{"snippet": "from distutils.version import LooseVersion\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nassert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\nprint('TensorFlow Version: {}'.format(tf.__version__))\n", "intent": "This will check to make sure you have the correct version of TensorFlow\n"}
{"snippet": "net2 = nn.Sequential()\nnet2.add(nn.Dense(128))\nnet2.add(nn.Dense(10))\nnet2.add(CenteredLayer())\n", "intent": "We can also incorporate this layer into a more complicated network, such as by using ``nn.Sequential()``.\n"}
{"snippet": "dense(nd.ones(shape=(2,10)))\n", "intent": "Now we can run through some dummy data.\n"}
{"snippet": "def get_net():\n    net = gluon.nn.Sequential()\n    with net.name_scope():\n        net.add(gluon.nn.Dense(1))\n    net.initialize()\n    return net\n", "intent": "We define a **basic** linear regression model here. This may be modified to achieve better results on Kaggle. \n"}
{"snippet": "def net(X, drop_prob=0.0):\n    h1_linear = nd.dot(X, W1) + b1\n    h1 = relu(h1_linear)\n    h1 = dropout(h1, drop_prob)\n    h2_linear = nd.dot(h1, W2) + b2\n    h2 = relu(h2_linear)\n    h2 = dropout(h2, drop_prob)\n    yhat_linear = nd.dot(h2, W3) + b3\n    return yhat_linear\n", "intent": "Now we're ready to define our model\n"}
{"snippet": "def net(X):\n    h1_linear = nd.dot(X, W1) + b1\n    h1 = relu(h1_linear)\n    h2_linear = nd.dot(h1, W2) + b2\n    h2 = relu(h2_linear)\n    yhat_linear = nd.dot(h2, W3) + b3\n    return yhat_linear\n", "intent": "Now we're ready to define our model\n"}
{"snippet": "net1 = gluon.nn.Sequential()\nwith net1.name_scope():\n    net1.add(gluon.nn.Dense(128, activation=\"relu\"))\n    net1.add(gluon.nn.Dense(64, activation=\"relu\"))\n    net1.add(gluon.nn.Dense(10))\n", "intent": "Now you might remember that up until now, we've defined neural networks (for example, a multilayer perceptron) like this:\n"}
{"snippet": "net1 = gluon.nn.Sequential()\nwith net1.name_scope():\n    net1.add(gluon.nn.Dense(128, activation=\"relu\"))\n    net1.add(gluon.nn.Dense(64, activation=\"relu\"))\n    net1.add(gluon.nn.Dense(10))\n", "intent": "So Sequential is basically a way of throwing together a Block on the fly. Let's revisit the ``Sequential`` version of our multilayer perceptron.\n"}
{"snippet": "net2 = gluon.nn.Sequential()\nwith net2.name_scope():\n    net2.add(gluon.nn.Dense(128, in_units=784, activation=\"relu\"))\n    net2.add(gluon.nn.Dense(64, in_units=128, activation=\"relu\"))\n    net2.add(gluon.nn.Dense(10, in_units=64))\n", "intent": "If we want to specify the shape manually, that's always an option. We accomplish this by using the ``in_units`` argument when adding each layer.\n"}
{"snippet": "ntokens = len(corpus.dictionary)\nmodel = RNNModel(args_model, ntokens, args_emsize, args_nhid,\n                       args_nlayers, args_dropout, args_tied)\nmodel.collect_params().initialize(mx.init.Xavier(), ctx=context)\ntrainer = gluon.Trainer(model.collect_params(), 'sgd',\n                        {'learning_rate': args_lr, 'momentum': 0, 'wd': 0})\nloss = gluon.loss.SoftmaxCrossEntropyLoss()\n", "intent": "We go on to build the model, initialize model parameters, and configure the optimization algorithms for training the RNN model.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: test_x,\n            labels_: test_y}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"./\", one_hot=False)\nwith tf.Session() as sess:\n    saver1 = tf.train.import_meta_graph('./mlp.meta')\n    saver1.restore(sess, save_path='./mlp')\n    test_acc = sess.run('accuracy:0', feed_dict={'features:0': mnist.test.images,\n                                                 'targets:0': mnist.test.labels})\n    print('Test ACC: %.3f' % test_acc)\n", "intent": "**You can restart and the notebook and the following code cells should execute without any additional code dependencies.**\n"}
{"snippet": "history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=2,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\n", "intent": "Now let's train our network for 20 epochs:\n"}
{"snippet": "smaller_model = models.Sequential()\nsmaller_model.add(layers.Dense(4, activation='relu', input_shape=(100,)))\nsmaller_model.add(layers.Dense(4, activation='relu'))\nsmaller_model.add(layers.Dense(1, activation='sigmoid'))\nsmaller_model.compile(optimizer='rmsprop',\n                      loss='binary_crossentropy',\n                      metrics=['acc'])\n", "intent": "Now let's try to replace it with this smaller network:\n"}
{"snippet": "from keras.layers import Dense\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n                    epochs=2,\n                    batch_size=128,\n                    validation_split=0.2)\n", "intent": "Let's train a simple recurrent network using an `Embedding` layer and a `SimpleRNN` layer:\n"}
{"snippet": "dnn_clf = DNNClassifier(random_state=42)\ndnn_clf.fit(X_train1, y_train1, n_epochs=2, X_valid=X_valid1, y_valid=y_valid1)\n", "intent": "Let's see if we get the exact same accuracy as earlier using this class (without dropout or batch norm):\n"}
{"snippet": "dnn_clf_bn = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n                           n_neurons=90, random_state=42,\n                           batch_norm_momentum=0.95)\ndnn_clf_bn.fit(X_train1, y_train1, n_epochs=2, X_valid=X_valid1, y_valid=y_valid1)\n", "intent": "Good, now let's use the exact same model, but this time with batch normalization:\n"}
{"snippet": "dnn_clf_5_to_9 = DNNClassifier(n_hidden_layers=4, random_state=42)\ndnn_clf_5_to_9.fit(X_train2, y_train2, n_epochs=2, X_valid=X_valid2, y_valid=y_valid2)\n", "intent": "Let's compare that to a DNN trained from scratch:\n"}
{"snippet": "import tensorflow as tf\nreset_graph()\nn_inputs = 28 * 28\nn_hidden1 = 300\nn_hidden2 = 150  \nn_hidden3 = n_hidden1\nn_outputs = n_inputs\nlearning_rate = 0.01\n", "intent": "Using Gaussian noise:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\nmodel.summary()\n", "intent": "Take couple of minutes and try to play with the number of layers and the number of parameters in the layers to get the best results. \n"}
{"snippet": "z = model_lr(X_test_lr[:20])\nprint(\"Label    :\", [label.todense().argmax() for label in Y_test_lr[:20]])\nprint(\"Predicted:\", [z[i,:].argmax() for i in range(len(z))])\n", "intent": "Now we can call it like any Python function:\n"}
{"snippet": "autoencoder.fit(x_train_noisy, x_train,\n                epochs=2,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test_noisy, x_test),\n                callbacks=[TensorBoard(log_dir='/tmp/autoencoder_denoise', \n                                       histogram_freq=0, write_graph=False)])\n", "intent": "Let's train the AutoEncoder for `100` epochs\n"}
{"snippet": "k = T.iscalar(\"k\")\nA = T.vector(\"A\")\nresult, updates = scan(fn=lambda prior_result, A: prior_result * A,\n                              outputs_info=T.ones_like(A),\n                              non_sequences=A,\n                              n_steps=k)\nfinal_result = result[-1]\npower = function(inputs=[A,k], outputs=final_result, updates=updates)\n", "intent": "Often we need to loop (for or while operation), e.g. looping over data elements in a batch. In Theano this can be done using the 'scan' operator.\n"}
{"snippet": "import tensorflow as tf\ntf.logging.set_verbosity(tf.logging.ERROR) \nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=2)]\ndnnc = tf.contrib.learn.DNNClassifier(\n  feature_columns=feature_columns,\n  hidden_units=[],\n  n_classes=2)\ndnnc\n", "intent": "**Run the cell below** to define a neural network.\n"}
{"snippet": "import tensorflow as tf\ntf.logging.set_verbosity(tf.logging.ERROR) \nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=2)]\ndnnc = tf.contrib.learn.DNNClassifier(\n  feature_columns=feature_columns,\n  hidden_units=[10, 10, 10],\n  n_classes=2)\ndnnc\n", "intent": "**Run the cell below** to define a neural network.\n"}
{"snippet": "import tensorflow as tf\nx = tf.placeholder(tf.float32, [None, 784]) \nW = tf.Variable(tf.zeros([784, 10])) \nb = tf.Variable(tf.zeros([10])) \ny = tf.nn.softmax(tf.matmul(x, W) + b)\n", "intent": "In TensorFlow, you'll define a \"graph\" of the calculations.\n"}
{"snippet": "init = tf.global_variables()\nsess = tf.Session()\nsess.run(init)\nfor i in range(1000):\n  batch_xs, batch_ys = mnist.train.next_batch(100)\n  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n", "intent": "Randomly selects 100 samples from the 55K images and calculates the gradient. Repeats 1K times.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=256, verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "parameter_pipe ={\n    'count_vector__ngram_range': ((1,1), (1,2), (1,3)), \n    'count_vector__stop_words': ('english', None),\n    'count_vector__max_features': (None, 1000, 2000, 3000),\n    'clf__max_depth': (10, 20, 30, 50, 70, None), \n    'clf__min_samples_leaf': (1, 5, 10, 15)\n}\ngrid_search_pipe = GridSearchCV(estimator = pipe, param_grid = parameter_pipe, scoring = 'accuracy')\ngrid_search_pipe.fit(X, y)\n", "intent": "Let's also perform a grid search on the former pipeline to find the best parameters to be used for CounterVectorizer and DecisionTreeClassifier\n"}
{"snippet": "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nscore = kfold_cv_rmsle(GBoost, final_train_df, y_train)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n", "intent": "Refer [here](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)\n"}
{"snippet": "data = mx.symbol.Variable('data')\nfc1  = mx.symbol.FullyConnected(data = data, name='fc1', num_hidden=128)\nact1 = mx.symbol.Activation(data = fc1, name='relu1', act_type=\"relu\")\nfc2  = mx.symbol.FullyConnected(data = act1, name = 'fc2', num_hidden = 64)\nact2 = mx.symbol.Activation(data = fc2, name='relu2', act_type=\"relu\")\nfc3  = mx.symbol.FullyConnected(data = act2, name='fc3', num_hidden=10)\nmlp  = mx.symbol.SoftmaxOutput(data = fc3, name = 'softmax')\n", "intent": "Now we can start constructing our network:\n"}
{"snippet": "feature_cols = ['TV', 'radio', 'newspaper', 'IsLarge']\nX = data[feature_cols]\ny = data.sales\nlm = LinearRegression()\nlm.fit(X, y)\nmodel_coef = zip(feature_cols, lm.coef_)\nprint('Model Coefficients:')\nfor values in model_coef:\n    print(values)  \n", "intent": "Let's redo the multiple linear regression and include the IsLarge predictor:\n"}
{"snippet": "train['vtype'] = train.vtype.map({'car':0, 'truck':1})\nfeature_cols = ['year', 'miles', 'doors', 'vtype']\nX = train[feature_cols]\ny = train.price\nfrom sklearn.tree import DecisionTreeRegressor\ntreereg = DecisionTreeRegressor(random_state=1)\ntreereg\n", "intent": "Recap: Before every split, this process is repeated for every feature, and the feature and cutpoint that produces the lowest MSE is chosen.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1e9)\nlogreg.fit(X_train, y_train)\nmodel_coef = zip(feature_cols, logreg.coef_[0])\nprint('Model Coefficients:')\nfor values in model_coef:\n    print(values)  \n", "intent": "Confirm that the coefficients make intuitive sense.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=100)\n", "intent": "[KNeighborsClassifier documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n"}
{"snippet": "from sklearn.pipeline import make_pipeline\npipe = make_pipeline(imp, knn)\n", "intent": "[make_pipeline documentation](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)\n"}
{"snippet": "from sklearn.pipeline import Pipeline\npipe = Pipeline([('imputer', imp), ('kneighborsclassifier', knn)])\n", "intent": "[Pipeline documentation](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n"}
{"snippet": "network_def_path = simple_model(28, 28, 1, num_classes)\n", "intent": "Instantiate an instance of the network.\n"}
{"snippet": "for i in range(5):\n    doc = tf_idf[i]\n    print(np.linalg.norm(doc.todense()))\n", "intent": "We can check that the length (Euclidean norm) of each row is now 1.0, as expected.\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "in sklearn\nEstimator = model\n"}
{"snippet": "data = mx.sym.Variable(\"data\")\nfc1 = mx.sym.FullyConnected(data=data, num_hidden=128, name=\"fc1\")\nbn1 = mx.sym.BatchNorm(data=fc1, name=\"bn1\")\nact1 = mx.sym.Activation(data=bn1, name=\"act1\", act_type=\"tanh\")\nfc2 = mx.sym.FullyConnected(data=act1, name=\"fc2\", num_hidden=10)\nsoftmax = mx.sym.Softmax(data=fc2, name=\"softmax\")\nbatch_size = 100\ndata_shape = (batch_size, 784)\nmx.viz.plot_network(softmax, shape={\"data\":data_shape}, node_attrs={\"shape\":'oval',\"fixedsize\":'false'})\n", "intent": "We will use a very simple 1 hidden layer BatchNorm fully connected MNIST network to demo how to use low level API.\nThe network looks like:\n"}
{"snippet": "mean_knn_n2 = KNeighborsClassifier(n_neighbors=1,\n                              weights='uniform')\naccuracy_crossvalidator(X, y, mean_knn_n2, cv_indices)\n", "intent": "---\nAs you can see the mean cross-validated accuracy is very high with 5 neighbors. \nLet's see what it's like when we use only 1 neighbor:\n"}
{"snippet": "X = affair[['age','religious']].values\nknn_uni_n3 = KNeighborsClassifier(n_neighbors=3, weights='uniform')\nknn_uni_n3.fit(X, y)\n", "intent": "---\nYou should choose **2 predictor variables** to predict had affair vs. not\n"}
{"snippet": "knn_uni_n11 = KNeighborsClassifier(n_neighbors=11, weights='uniform')\nknn_uni_n11.fit(X, y)\n", "intent": "---\nUse the same predictor variables and cv folds.\n"}
{"snippet": "def lasso_coefs(X, Y, alphas):\n    coefs = []\n    lasso_reg = Lasso()\n    for a in alphas:\n        lasso_reg.set_params(alpha=a)\n        lasso_reg.fit(X, Y)\n        coefs.append(lasso_reg.coef_)\n    return coefs\nsgd_reg_alphas = sgd_reg_gs_params['param_grid']['alpha']\nlcoefs = lasso_coefs(Xn, y, sgd_reg_alphas)\n", "intent": "---\nHow you choose to examine the results is up to you. Visualizations are always a good idea, but not entirely neccessary (or easy) in this case.\n"}
{"snippet": "slr = SimpleLinearRegression(fit_intercept=True)\nslr.fit_intercept\n", "intent": "Now, if we instantiate the class, it will assign `fit_intercept` to the class attribute `fit_intercept`, like so:\n"}
{"snippet": "lr_pipe.fit(Xtrain, ytrain)\nlr_pipe.score(Xtest, ytest)\n", "intent": "Fit the pipeline with the training data, then score it on the testing data:\n"}
{"snippet": "lda = models.LdaModel(\n    matutils.Sparse2Corpus(X, documents_columns=False),\n    num_topics  =  3,\n    passes      =  20,\n    id2word     =  vocab\n)\n", "intent": "Finally we initialize and assign our model to a variable object!\n"}
{"snippet": "k = \nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(car1)\n", "intent": "Cluster - Choose K based on plot and variables chosen\n"}
{"snippet": "nb = BernoulliNB()\n", "intent": "---\nThe model should only be built (and cross-validated) on the training data.\nCross-validate the score and compare it to baseline.\n"}
{"snippet": "best_estimator = random_search.best_estimator_                        \nh2o_model      = h2o.get_model(best_estimator._final_estimator._id)    \n", "intent": "It is useful to save constructed models to disk and reload them between H2O sessions. Here's how:\n"}
{"snippet": "g = Graph()\ng.set_as_default()\nA = Variable([[10, 20], [30, 40]])\nb = Variable([1, 1])\nx = Placeholder()\ny = matmul(A,x)\nz = add(y,b)\n", "intent": "** Looks like we did it! **\n"}
{"snippet": "def init_weights(shape):\n    init_random_dist = tf.truncated_normal(shape = shape, stddev = 0.1)\n    return tf.Variable(init_random_dist)\n", "intent": "Function to help intialize random weights for fully connected or convolutional layers, we leave the shape attribute as a parameter for this.\n"}
{"snippet": "def init_bias(shape):\n    init_bias_vals = tf.constant(shape =shape, value = 0.1)\n    return tf.Variable(init_bias_vals)\n", "intent": "Same as init_weights, but for the biases\n"}
{"snippet": "fully_connected_layer_after_dropout = tf.nn.dropout(x = fully_connected_layer_1, \n                                                    keep_prob = hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "cell = tf.contrib.rnn.OutputProjectionWrapper(\n    tf.contrib.rnn.BasicRNNCell(num_units = num_neurons, \n                                activation = tf.nn.relu), \n    output_size = num_outputs)\n", "intent": "____\n____\nPlay around with the various cells in this section, compare how they perform against each other.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./checkpoints/rnn_time_series_model\")\n    zero_seq_seed = [0. for i in range(num_time_steps)]\n    for iteration in range(len(ts_data.x_data) - num_time_steps):\n        X_batch = np.array(zero_seq_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n        y_pred = sess.run(outputs, feed_dict={X: X_batch})\n        zero_seq_seed.append(y_pred[0, -1, 0])\n", "intent": "** Note: Can give wacky results sometimes, like exponential growth**\n"}
{"snippet": "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype = tf.float32)\n", "intent": "** Now pass in the cells variable into tf.nn.dynamic_rnn, along with your first placeholder (X)**\n"}
{"snippet": "import helper\nimages, labels = next(iter(trainloader))\nimg = images[0].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits,\n               dim=1)\nhelper.view_classify(img.view(1, 28, 28),\n                     ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "input_size=1 \noutput_size=1\nhidden_dim=32\nn_layers=1\nrnn = RNN(input_size, output_size, hidden_dim, n_layers)\nprint(rnn)\n", "intent": "---\nNext, we'll instantiate an RNN with some specified hyperparameters. Then train it over a series of steps, and see how it performs.\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "intent": "Finetuning the convnet\n----------------------\nLoad a pretrained model and reset final fully connected layer.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, input_dim=x_train1.shape[1]))\nmodel.add(Activation('tanh'))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train1, y_train1, epochs=1000, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.neighbors import KernelDensity\ndef plot_kde(sample, ax, label=''):\n    xmin, xmax = 0.9 * min(sample), 1.1 * max(sample)\n    xgrid = np.linspace(xmin, xmax, 200)\n    kde = KernelDensity(kernel='gaussian').fit(sample[:, None])\n    log_dens = kde.score_samples(xgrid[:, None])\n    ax.plot(xgrid, np.exp(log_dens), label=label)\n", "intent": "We can also approximate the distribution using a kernel density estimator.  I'll use one from Scikit Learn.\n"}
{"snippet": "m1 = KNeighborsClassifier(1).fit(trainx, trainy)\n", "intent": "Compute accuracy on testing sample using both 1-nn and 5-nn classifier\n"}
{"snippet": "clf = LogisticRegression(class_weight='balanced') \n", "intent": "Notice that the resulting labels are imbalanced.\n"}
{"snippet": "pretrained_file = 'taggers/maxent_treebank_pos_tagger/english.pickle'\naccuracy_treebank_pretrained = pos_tagging_pretrained_model(W_treebank_test, Y_treebank_test, pretrained_file)\nprint('Accuracy on Test Set for TreeBank using pretrained model {0:.2f} '.format(accuracy_treebank_pretrained))\n", "intent": "b) Pre-trained Model Using NLTK Max-Entropy for TreeBank Dataset\n"}
{"snippet": "pretrained_file = 'taggers/maxent_brown_pos_tagger/english.pickle'\naccuracy_brown_pretrained = pos_tagging_pretrained_model(W_brown_test, Y_brown_test, pretrained_file)\nprint('Accuracy on Test Set for Brown Corpus using pretrained model {0:.2f} '.format(accuracy_brown_pretrained))\n", "intent": "(e) Pre-trained Model Using NLTK Max-Entropy for TreeBank Dataset\n"}
{"snippet": "size=10000\nclf.fit(X1[:size], y1[:size])\nprint('training OK')\nX1_test, y1_test = transform_to_dataset(test_sentences_X1)\nperformance1_1 = clf.score(X1_test, y1_test)\nprint(\"Accuracy:\", performance1_1)\n", "intent": "* fit the decision tree for a limited amount (size) of training \n* test data and compare with score function on testdata\n"}
{"snippet": "clf_model_b = MultinomialNB()\n", "intent": "* model b - train - [performance measures]\n* model b - validation - [performance measures]\n* model b - test - [performance measures]\n"}
{"snippet": "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=25)\n", "intent": "Train and evaluate\n^^^^^^^^^^^^^^^^^^\nIt should take around 15-25 min on CPU. On GPU though, it takes less than a\nminute.\n"}
{"snippet": "pretrained_file = 'taggers/maxent_treebank_pos_tagger/english.pickle'\naccuracy_treebank_pretrained = pos_tagging_pretrained_model(W_treebank_test, Y_treebank_test, pretrained_file)\nprint('Accuracy on Test Set for TreeBank using pretrained model',accuracy_treebank_pretrained)\n", "intent": "b) Pre-trained Model Using NLTK Max-Entropy for TreeBank Dataset\n"}
{"snippet": "pretrained_file = '/home/debayan/nltk_data/taggers/maxent_treebank_pos_tagger/PY3/english.pickle'\naccuracy_brown_pretrained = pos_tagging_pretrained_model(W_brown_test, Y_brown_test, pretrained_file)\nprint('Accuracy on Test Set for Brown Corpus using pretrained model',accuracy_brown_pretrained)\n", "intent": "(e) Pre-trained Model Using NLTK Max-Entropy for TreeBank Dataset\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier as RFC\nclf_a = RFC(criterion='entropy', random_state=4222)\nmax_size=10000\nclf_a.fit(vec_train_1[:max_size], y1[:max_size])\ntest_classifier(labels=[\"FAKE\",\"REAL\"], title=\"Configuration 1, model a -- train\", Xt=vec_train_1,yt=y1, clf=clf_a)\ncm_1 = test_classifier(labels=[\"FAKE\",\"REAL\"], title=\"Configuration 1, model a -- test\", Xt=vec_test_1,yt=yt1, clf=clf_a)\n", "intent": "* trying a Random Forest classifier \n"}
{"snippet": "from sklearn.neural_network import MLPClassifier\nclf_b = MLPClassifier(hidden_layer_sizes=(100,), random_state=4222)\nclf_b.fit(vec_train_2, y2)\n", "intent": "* trying a MLP as classifier \n"}
{"snippet": "def model_01(X,y,tX,ty, max_size=10000):\n    model01_clf = train_classifier(X,y,MLPClassifier(hidden_layer_sizes=(100,), learning_rate='adaptive'),max_size=max_size)\n    return test_classifier(clf=model01_clf, tX=tX, ty=ty)\n", "intent": "* train and testing english custom POS tagger model:\n"}
{"snippet": "nn = Sequential()\nnn.add(Embedding(max_fatures, embed_dim, input_length = X.shape[1]))\nnn.add(SimpleRNN(units=3))\nnn.add(Dense(2, activation='softmax'))\nnn.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\nprint(nn.summary())\n", "intent": "<h4>Improve the baseline by inserting RNN cells</h4>\n"}
{"snippet": "with graph.as_default():\n    with tf.name_scope('embedding_layer'):\n        embedding = tf.Variable(tf.random_uniform((n_words + 1, embed_size), -1.0, 1.0), name='embedding') \n        embed = tf.nn.embedding_lookup(embedding, inputs_)\n        tf.summary.histogram('embedding', embedding)\n", "intent": "Adding layer to map the ~74K word vocab to size 300 feature array\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_matrix = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n    embeds = tf.nn.embedding_lookup(embedding_matrix, input_data)\n    return embeds\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from distutils.version import LooseVersion\nimport warnings\nimport tensorflow as tf\ntf.reset_default_graph()\nassert LooseVersion(tf.__version__) in [LooseVersion('1.0.0'), LooseVersion('1.0.1')], 'This project requires TensorFlow version 1.0  You are using {}'.format(tf.__version__)\nprint('TensorFlow Version: {}'.format(tf.__version__))\nif not tf.test.gpu_device_name():\n    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\nelse:\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n", "intent": "This will check to make sure you have the correct version of TensorFlow and access to a GPU\n"}
{"snippet": "x = torch.randn(3)\nx = Variable(x, requires_grad=True)\ny = x * 2\nwhile y.data.norm() < 1000:\n    y = y * 2\nprint(y)\n", "intent": "You can do many crazy things with autograd!\n"}
{"snippet": "RF = RandomForestRegressor(n_estimators = 10000, \n                           max_features = 4,     \n                           min_samples_leaf = 5, \n                           oob_score = True)      \nRF.fit(X,y)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n"}
{"snippet": "for i in [X2,X4]:\n    lm1 = smf.ols(formula='y ~ i', data=BostonData).fit()\n    print(lm1.summary())\n", "intent": "Answer: we will most likely use model 2 or 4. If both of these models are significant, then we use model 4. \n"}
{"snippet": "for i in [X12,X12,X14,X15]:\n    lm1 = smf.ols(formula='y ~ i', data=BostonData).fit()\n    print(lm1.summary())\n", "intent": "Answer: if our goal is prediction, we will most like use models 12, 13, 14 or 15. Let's test to see which one is the best!\n"}
{"snippet": "with tf.Session(graph=graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: test_x,\n            labels_: test_y}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embed_values = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1,1), name=\"word_embedding\")\n    embed_lookup = tf.nn.embedding_lookup(embed_values, input_data)\n    return embed_lookup\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X, y)\n", "intent": "Score and plot your predictions.\n"}
{"snippet": "x = Variable(torch.ones(2, 2), requires_grad = True)\nx  \n", "intent": "$X = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1\\end{bmatrix}$, $Z = 2 * (X + 2) ^ 2$, $out = \\bar{Z}$\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embedding,ids = input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf_iris = KNeighborsClassifier()\nclf_iris.fit(X_ir[:,2:], y_ir)\n", "intent": "As the description of the data says petal length and width are the main indicators. Let's build a model based on them and plot the results.\n"}
{"snippet": "outputs = net(Variable(images))\n", "intent": "Okay, now let us see what the neural network thinks these examples above are:\n"}
{"snippet": "poly15 = PolynomialRegression(15)\npoly15.fit(X_train, y_train)\nplot_regr(X_train, y_train, poly15)\n", "intent": "This is much better, so we might try to go to even higher polynomials.\n"}
{"snippet": "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\ngradients = tf.gradients(loss, [w, u, b])\nclipped_gradients, norm = tf.clip_by_global_norm(gradients, .1)\ntrain_step = optimizer.apply_gradients(zip(clipped_gradients, [u,w,b]))\n", "intent": "<span style=\"color:red\">\\** Note this step is different from previous ones </span>\n"}
{"snippet": "def forewardPass(data):\n    x = sigmoid(np.dot(data, backwardB.getWeights().transpose()) + backwardB.getBias())\n    return x\n", "intent": "foreward pass: $\\mathbb{R}^{n \\times 30} \\rightarrow \\mathbb{R}^n$\n"}
{"snippet": "def forewardMNIST(data):\n    hidden = sigmoid(np.dot(data, backwardMNIST.getWeights(0)) + backwardMNIST.getBias(0))\n    out = softmax(np.dot(hidden, backwardMNIST.getWeights(1)) + backwardMNIST.getBias(1))\n    return out\ndef getClass(data):\n    proba = forewardMNIST(data)\n    return np.argmax(proba, axis=1)\n", "intent": " - foreward pass: $\\mathbb{R}^{n \\times 784} \\rightarrow \\mathbb{R}^{n \\times 10}$\n"}
{"snippet": "conv_layer = tf.nn.max_pool(\n    conv_layer,\n    ksize=[1, 2, 2, 1],\n    strides=[1, 2, 2, 1],\n    padding='SAME')\n", "intent": "TensorFlow provides the tf.nn.max_pool() function to apply max pooling to your convolutional layers.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(params=embedding, ids=input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from distutils.version import LooseVersion\nimport warnings\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nassert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\nprint('TensorFlow Version: {}'.format(tf.__version__)) \nif not tf.test.gpu_device_name():\n    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\nelse:\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n", "intent": "This will check to make sure you have the correct version of TensorFlow and access to a GPU\n"}
{"snippet": "model6_entropy = DecisionTreeClassifier(criterion = \"gini\", min_samples_split=3, random_state=100, max_leaf_nodes=None, max_depth=2, min_samples_leaf=8, splitter='best')\nmodel6_entropy.fit(X_train, y_train)\n", "intent": "- Experiment 1 - Adjusting the DecisionTree depth parameter\n"}
{"snippet": "model12_entropy = DecisionTreeClassifier(criterion = \"gini\", min_samples_split=7, random_state=100, max_leaf_nodes=19, max_depth=6, min_samples_leaf=13, splitter='best')\nmodel12_entropy.fit(X_train, y_train)\n", "intent": "- Experiment 3 - Adjusting the min_samples_split and min_samples_leaf parameters\n"}
{"snippet": "correct = 0\ntotal = 0\nfor data in testloader:\n    images, labels = data\n    outputs = net(Variable(images))\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\nprint('Accuracy of the network on the 10000 test images: %d %%' % (\n    100 * correct / total))\n", "intent": "The results seem pretty good.\nLet us look at how the network performs on the whole dataset.\n"}
{"snippet": "a, b = 1.5, 2.5\ne_out = graph(a,b)\nprint(e_out)\n", "intent": "Now, we can call this function to execute the computation graph given some inputs `a,b`:\n"}
{"snippet": "with ShowAndTellModel(train_dir='/content/datalab/img2txt/train',\n                      vocab_file='/content/datalab/img2txt/vocab.yaml') as m:\n    m.show_and_tell('gs://bradley-sample-notebook-data/chopin_vivaldi.jpg')\n    m.show_and_tell('gs://bradley-sample-notebook-data/vivaldi_chopin_tail.jpg')\n    m.show_and_tell('gs://bradley-sample-notebook-data/vivaldi.jpg')  \n", "intent": "For fun, I would give it a try on pictures of my cats!\n"}
{"snippet": "LSTM_SIZE=128\ndef lstm_model(batch_size, train_data, targets, mode):\n", "intent": "Let's try an LSTM based sequential model and see if it can beat DNN.\n"}
{"snippet": "from sklearn import linear_model\nols = linear_model.LinearRegression()\nols.fit(x, y)\n", "intent": "- Create a LinearRegression instance and use `fit()` function to fit the data.\n"}
{"snippet": "ols.fit(X_train, y_train)\nprint \"R^2 for training set:\",\nprint ols.score(X_train, y_train)\nprint '-'*50\nprint \"R^2 for test set:\",\nprint ols.score(X_test, y_test)\n", "intent": "- Do multiple linear regression with new data set.\n- Report the coefficient of determination from the training and testing sets.\n"}
{"snippet": "from sklearn import linear_model\nlogit_1 = linear_model.LogisticRegression()\n", "intent": "- The implementation of logistic regression in scikit-learn can be accessed from class **`LogisticRegression`**.\n"}
{"snippet": "logit_2 = linear_model.LogisticRegression()\nlogit_2.set_params(C=1e4)\nlogit_2.fit(x_tm2, y_tm)\nprint [logit_2.coef_, logit_2.intercept_]\n", "intent": "- Below we see that the boundary is not affected that much by outliers with logistic regression.\n"}
{"snippet": "logit = linear_model.LogisticRegression(C=1e4)\nlogit.fit(scores, decision)\nlogit.score(scores, decision)\n", "intent": "Build a logistic regression model **`logit`** with the data.\n- What's the score?\n"}
{"snippet": "logit.fit(iris_x, iris_y)\n", "intent": "<p><a name=\"ex2\"></a></p>\n- Agian we first create and fit the logistic regression:\n"}
{"snippet": "for data in rand_loader:\n    if torch.cuda.is_available():\n        input_var = Variable(data.cuda())\n    else:\n        input_var = Variable(data)\n    output = model(input_var)\n    print(\"Outside: input size\", input_var.size(),\n          \"output_size\", output.size())\n", "intent": "Run the Model\n-------------\nNow we can see the sizes of input and output tensors.\n"}
{"snippet": "LDA.fit(iris.data, iris.target)\nLDA.score(iris.data, iris.target)\n", "intent": "- Fit a LDA model with all features of iris dataset, what's your overall accuracy?\n"}
{"snippet": "QDA = discriminant_analysis.QuadraticDiscriminantAnalysis()\nQDA.fit(iris.data, iris.target)\n", "intent": "- Create a QDA model and train it with the iris data.\n"}
{"snippet": "from sklearn import naive_bayes\ngnb = naive_bayes.GaussianNB()\ngnb.fit(iris.data, iris.target)\nprint gnb.score(iris.data, iris.target)\n", "intent": "<p><a name=\"gnb-sklearn\"></a></p>\n**Exercise**\nWe will work on the iris dat. Fit a Gaussian Naive Bayes and print out the accuracy:\n"}
{"snippet": "mnb = naive_bayes.MultinomialNB()\nmnb.fit(x, y)\nprint \"The score of multinomial naive bayes is: %.4f\" %mnb.score(x, y)\n", "intent": "- Create a multinomial naive Bayes model and train it with the data above. What is the accuracy of the model?\n"}
{"snippet": "bnb = naive_bayes.BernoulliNB()\nbnb.fit(iris.data, iris.target)\nprint \"The mean accuracy of Bernoulli Naive Bayes is: \" + str(bnb.score(iris.data, iris.target))\n", "intent": "- Train a BNB model with the data. REport the accuracy.\n"}
{"snippet": "ols.fit(x, y)\nprint \"Coefficeints with Entire dataset: \", ols.coef_\nprint \"Bootstrap estimates: \", np.mean(coefs, 0)\n", "intent": "- The coefficients of the model fitting with the whole data set are:\n"}
{"snippet": "from sklearn import linear_model\nridge = linear_model.Ridge()\n", "intent": "Here is an example showing how the coefficients vary as the parameter $\\alpha$ increases.\n"}
{"snippet": "from sklearn import svm\nsvm_model = svm.SVC(kernel ='poly', C = 1e5, degree=1)\n", "intent": "<p><a name=\"svm-sklearn\"></a></p>\n- In order to implement SVM in python, import **svm** module from sklearn library.\n"}
{"snippet": "svm_model.set_params(degree=3)\nsvm_model.fit(iris.data[index, 0:2], iris.target[index])\nplotModel(svm_model, iris.data[index, 0], iris.data[index, 1], iris.target[index])\npl.xlabel('Sepal Length')\npl.ylabel('Sepal Width')\n", "intent": "This time we set *degree=3*, which result in a cubic boundary:\n"}
{"snippet": "x = torch.Tensor(5, 3)\nprint(x)\n", "intent": "Construct a 5x3 matrix, uninitialized:\n"}
{"snippet": "svm_model.set_params(C=1)\nsvm_model.fit(iris.data[:, 2:4], iris.target) \nsvm_model.score(iris.data[:, 2:4], iris.target) \n", "intent": "Below we change to a different constant `C` (reset `C = 1` from `C=1e5` to decrease the effect of penalty)\n"}
{"snippet": "from sklearn import tree\ntree_model = tree.DecisionTreeClassifier()\n", "intent": "The function **tree.DecisionTreeClassifier** in sklearn can be used to implement decision tree.\n"}
{"snippet": "plotModel(tree_model, iris.data[:, 2], iris.data[:, 3], iris.target)\npl.xlabel('Petal Length')\npl.ylabel('Petal Width')\n", "intent": "We visualize the decision boundary of the tree:\n"}
{"snippet": "tree_model.fit(x_train, y_train)\nprint \"The training error is: %.5f\" %(1-tree_model.score(x_train, y_train))\nprint \"The test     error is: %.5f\" %(1-tree_model.score(x_test, y_test))\n", "intent": "Fit on the training data derictly:\n"}
{"snippet": "np.random.seed(1)\nrandomForest.fit(iris.data[:, 2:4], iris.target) \nrandomForest.score(iris.data[:, 2:4], iris.target) \n", "intent": "We use all the observations and the last two features, \"petal length\" and \"petal width\", in the iris data to build a decision-tree.\n"}
{"snippet": "np.random.seed(1)\ngrid_para_forest = [{\"n_estimators\": [10, 50, 100], \"criterion\": [\"gini\", \"entropy\"], \\\n                    \"min_samples_leaf\": range(1, 10), \"min_samples_split\": np.linspace(2, 30, 15)}]\ngrid_search_forest = gs.GridSearchCV(randomForest, grid_para_forest, scoring='accuracy', cv=5)\ngrid_search_forest.fit(x_train, y_train)\n", "intent": "<p><a name=\"3case3\"></a></p>\nAs for decision tree, we also need to decide several parameters by grid search. It might take longer for this one.\n"}
{"snippet": "from sklearn.cluster import KMeans\nkmeans = KMeans()\n", "intent": "<p><a name=\"kmeans\"></a></p>\nImport the class *KMeans* from the module *cluster* in the sklearn library.\n"}
{"snippet": "kmeans.set_params(n_clusters = 2)\nkmeans.fit(x)\n", "intent": "- Set the parameter *n_clusters* to 2, and fit the model.\n"}
{"snippet": "kmeans.fit(iris.data2)\nlabels = kmeans.labels_\nlabels\n", "intent": "- Cluster the data by kmeans algorithm. Print the labels of the data.\n"}
{"snippet": "result = torch.Tensor(5, 3)\ntorch.add(x, y, out=result)\nprint(result)\n", "intent": "Addition: providing an output tensor as argument\n"}
{"snippet": "pca.set_params(n_components = 3)\npca.fit(data)\n", "intent": "Fit a PCA model on this data set.\n"}
{"snippet": "import sklearn.linear_model as lm\nlogit = lm.LogisticRegression()\ny_train2num = [1 if i == 'email' else 0 for i in y_train]\nlogit.fit(x_train2, y_train2num)\n", "intent": "- We can now build a model using the principal components. In this case we'll make a logistic regression.\n"}
{"snippet": "n_estimators=list(range(100,1100,100))\nlearn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\nhyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\ngd=GridSearchCV(estimator=GradientBoostingClassifier(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)\n", "intent": "We will try to increase it with Hyper-Parameter Tuning\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.get_variable(\"embedding\", shape=[vocab_size, embed_dim], dtype=tf.float32)\n    return tf.nn.embedding_lookup(\n        embeddings,\n        input_data\n    )\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "dtree = DecisionTreeClassifier(max_depth=11, min_samples_leaf=1, class_weight={0:1, 1:7})\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "kmeans.fit(colleges.drop('Private', axis = 1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "scaler.fit(secret.drop('TARGET CLASS', axis = 1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "lr = LogisticRegression()\nlr.fit(X_train, y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn import svm\nmethod = svm.SVC(kernel='rbf', C=100.)\nmethod.fit(x_train, t_train)\n", "intent": "We are going to make a first attempt at classification using a Support Vector classifier with radial basis functions.\n"}
{"snippet": "target = Variable(torch.LongTensor([3]))\nloss_fn = nn.CrossEntropyLoss()  \nerr = loss_fn(out, target)\nerr.backward()\nprint(err)\n", "intent": "Define a dummy target label and compute error using a loss function.\n"}
{"snippet": "import networkx as nx\nG = nx.cycle_graph(10)\nA = nx.adjacency_matrix(G)\nprint(A.todense())\n", "intent": "Creating adjacency matrix\n"}
{"snippet": "from sklearn.ensemble import IsolationForest\nisolation_forest = IsolationForest(max_samples=100, random_state=8451)\niso_fit = isolation_forest.fit(X_train)\noutlier_ratings = iso_fit.decision_function(X_train)\noutlier_ratings[:50]\n", "intent": "We'll use a method called Isolation Forest to find outliers.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nmodel = lin_reg.fit(X=X_train, y=y_train)\nprint('Coef: %s' % str(model.coef_))\nprint('Intercept: %s' % str(model.intercept_))\n", "intent": "For the sake of simplicity, let's just do a linear regression.\n"}
{"snippet": "coefficients_1 = train_model(model_1, param_1, X, y)\ncoefficients_2 = train_model(model_2, param_2, X, y)\n", "intent": "And suppose we train the model using a train_model function like this:\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: test_x,\n            labels_: test_y,\n            keep_prob: 1.0}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "valid_rows = data[~data.bmi.isnull()]\ny = valid_rows['bmi']\nvalid_rows1 = valid_rows.drop(['id','bmi'],axis=1)\nvalid_rows1 = sm.add_constant(valid_rows1)\nm = sm.OLS(y,valid_rows1)\nres = m.fit()\nres.summary()\n", "intent": "The distribution is distorted in comparison with original.\n"}
{"snippet": "model = uncertain_gallup_model(gallup_2012)\nprint model.head()\nmodel = model.join(electoral_votes)\n", "intent": "We construct the model by estimating the probabilities:\n"}
{"snippet": "clf.fit(digits.data[:-10], digits.target[:-10])\n", "intent": "The estimator is trained from the learning set using its ``.fit`` method.\n"}
{"snippet": "from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=2, random_state=0).fit(data.T)\nsegmentation = kmeans.labels_.reshape(im.shape[:-1])\n", "intent": "Then we create a ``KMeans`` estimator for two clusters.\n"}
{"snippet": "import torch\na = torch.FloatTensor(5, 7)\n", "intent": "Tensors\n=======\nTensors behave almost exactly the same way in PyTorch as they do in\nTorch.\nCreate a tensor of size (5 x 7) with uninitialized memory:\n"}
{"snippet": "from clipper_admin.deployers import pytorch as pytorch_deployer\npytorch_deployer.deploy_pytorch_model(\n    clipper_conn,\n    name=\"pytorch-model\",\n    version=1, \n    input_type=\"bytes\", \n    func=predict_torch_model,\n    pytorch_model=model,\n)\n", "intent": "> *Once again, Clipper must download this Docker image from the internet, so this may take a minute. Thanks for your patience.*\n"}
{"snippet": "best_model = get_best_model(make_model, trials, metric=\"mean_accuracy\")\n", "intent": "**Exercise**: Now, let's get the best model from your search process, and check its validation accuracy compared to the first model we created above.\n"}
{"snippet": "import torch\nimport torchvision\nfrom torchvision.datasets import FashionMNIST\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\ndata_transform = transforms.ToTensor()\ntest_data = FashionMNIST(root='./data', train=False,\n                                  download=True, transform=data_transform)\nprint('Test data, number of images: ', len(test_data))\n", "intent": "In this cell, we load in just the **test** dataset from the FashionMNIST class.\n"}
{"snippet": "autoencoder = Model(inp_img, out_img)\n", "intent": "We declare the functional model format, passing both inputs and ouputs. \n"}
{"snippet": "def conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\ndef conv_layer(input, shape):\n    W = weight_variable(shape)\n    b = bias_variable([shape[3]])\n    return tf.nn.relu(conv2d(input, W) + b)\n", "intent": "The next step is to define the layers that we will use in our model. \n"}
{"snippet": "sess = tf.Session()\nouts = sess.run(f)\nsess.close()\nprint('Output is {}'.format(outs))\n", "intent": "~~~python \ntf.Session()\n~~~\n"}
{"snippet": "with tf.Session() as sess:\n    outs = sess.run(f)\nprint('Output is {}'.format(outs))\n", "intent": "~~~python \nwith tf.Session()\n~~~\n"}
{"snippet": "with tf.Graph().as_default():\n    c1 = tf.constant(4,dtype=tf.float64,name='c') \n    with tf.name_scope(\"prefix_name\"):\n        c2 = tf.constant(4,dtype=tf.int32,name='c') \n        c3 = tf.constant(4,dtype=tf.float64,name='c')\nprint(c1.name)\nprint(c2.name)\nprint(c3.name)\n", "intent": "Sometimes when dealing with large complicated graphs, we would like to create some node grouping to make it easier to follow and manage. \n"}
{"snippet": "x = tf.Variable(0, name='x')\ninit = tf.global_variables_initializer()\nprint('pre-run variable:\\n{}'.format(x))\nprint('===============================')\nwith tf.Session() as sess:\n    sess.run(init)\n    val = sess.run(x)\n    print('post-run variable:\\n{}'.format(val))\n", "intent": "Variables are Tensor objects that we use to store and update model parameters.\n"}
{"snippet": "V_data = [1., 2., 3.]\nV = torch.Tensor(V_data)\nprint(V)\nM_data = [[1., 2., 3.], [4., 5., 6]]\nM = torch.Tensor(M_data)\nprint(M)\nT_data = [[[1., 2.], [3., 4.]],\n          [[5., 6.], [7., 8.]]]\nT = torch.Tensor(T_data)\nprint(T)\n", "intent": "Creating Tensors\n~~~~~~~~~~~~~~~~\nTensors can be created from Python lists with the torch.Tensor()\nfunction.\n"}
{"snippet": "x = [vocab.toks2idxs('the most'.split())]\nfor n in range(10):\n    x_fw = Variable(torch.from_numpy(np.array(x))).cuda()\n    logits_fw = net.forward(x_fw, 'forward')\n    logits = nn.functional.log_softmax(logits_fw, 2).cpu().data.numpy()\n    logits = logits[0][-1]\n    p = np.exp(logits) / np.sum(np.exp(logits))\n    new_tok_ind = np.argmax(np.random.multinomial(1, p - 1e-9))\n    x[0].append(new_tok_ind)\nprint(vocab.idxs2toks(x[0]))\n", "intent": "Now try to sample from the model forward direction network given short initial phrase like \"the most\"\n"}
{"snippet": "lr.fit(X_train_level2, y_train_level2)\n", "intent": "Fit a linear regression model to the meta-features. Use the same parameters as in the model above.\n"}
{"snippet": "clf = DecisionTreeClassifier(max_depth=1)\nclf.fit(X_train, y_train)\nprint ('Accuracy for a single decision stump: {}'.format(clf.score(X_test, y_test)))\n", "intent": "The datast is really simple and can be solved with a single decision stump.\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators=500, max_depth=4, n_jobs=-1)\nrf.fit(X_train, y_train)\n", "intent": "**Step 1:** first fit a Random Forest to the data. Set `n_estimators` to a high value.\n"}
{"snippet": "class Seq2SeqModel(object):\n    pass\n", "intent": "Let us use TensorFlow building blocks to specify the network architecture.\n"}
{"snippet": "logreg = LogisticRegression(solver='liblinear')\nC_vals = [0.0001, 0.001, 0.01, 0.1, .15, .25, .275, .33, 0.5, .66, 0.75, 1.0, 2.5, 5.0, 10.0, 100.0, 1000.0]\npenalties = ['l1','l2']\ngs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals},\\\n                  verbose=False, cv=15)\ngs.fit(X, y)\n", "intent": "Then will pass those to GridSearchCV\n"}
{"snippet": "model_file_name = \"locally-trained-xgboost-model\"\nbt._Booster.save_model(model_file_name)\n", "intent": "Note that the model file name must satisfy the regular expression pattern: `^[a-zA-Z0-9](-*[a-zA-Z0-9])*;`. The model file also need to tar-zipped. \n"}
{"snippet": "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'}\ncontainer = containers[boto3.Session().region_name]\n", "intent": "This involves creating a SageMaker model from the model file previously uploaded to S3.\n"}
{"snippet": "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.libsvm')).upload_file('train.libsvm')\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.libsvm')).upload_file('validation.libsvm')\n", "intent": "Now we'll copy the file to S3 for Amazon SageMaker's managed training to pickup.\n"}
{"snippet": "x = torch.Tensor([1., 2., 3.])\ny = torch.Tensor([4., 5., 6.])\nz = x + y\nprint(z)\n", "intent": "Operations with Tensors\n~~~~~~~~~~~~~~~~~~~~~~~\nYou can operate on tensors in the ways you would expect.\n"}
{"snippet": "writer = tf.summary.FileWriter(LOGDIR)\nwriter.add_graph(sess.graph)\ntf.summary.histogram('m', m)\ntf.summary.histogram('b', b)\ntf.summary.scalar('loss', loss)\nsummary_op = tf.summary.merge_all()\n", "intent": "Step 5) Set up TensorBoard\n"}
{"snippet": "biases = tf.Variable(tf.zeros([num_classes]))\n", "intent": "The second variable that must be optimized is called `biases` and is defined as a 1-dimensional tensor (or vector) of length `num_classes`.\n"}
{"snippet": "net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n", "intent": "We now do the max-pooling on the output of the convolutional layer. This was also described in more detail in Tutorial \n"}
{"snippet": "net = tf.layers.conv2d(inputs=net, name='layer_conv2', padding='same',\n                       filters=36, kernel_size=5, activation=tf.nn.relu)\n", "intent": "We now add the second convolutional layer which has 36 filters each with 5x5 pixels, and a ReLU activation function again.\n"}
{"snippet": "net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n", "intent": "The output of the second convolutional layer is also max-pooled for down-sampling the images.\n"}
{"snippet": "net = tf.layers.dense(inputs=net, name='layer_fc1',\n                      units=128, activation=tf.nn.relu)\n", "intent": "We can now add fully-connected layers to the neural network. These are called *dense* layers in the Layers API.\n"}
{"snippet": "y_pred = tf.nn.softmax(logits=logits)\n", "intent": "We use the softmax function to 'squash' the outputs so they are between zero and one, and so they sum to one.\n"}
{"snippet": "def new_fc_layer(input,          \n                 num_inputs,     \n                 num_outputs,    \n                 use_relu=True): \n    weights = new_weights(shape=[num_inputs, num_outputs])\n    biases = new_biases(length=num_outputs)\n    layer = tf.matmul(input, weights) + biases\n    if use_relu:\n        layer = tf.nn.relu(layer)\n    return layer\n", "intent": "The following helper-function creates a fully-connected layer.\n"}
{"snippet": "global_step = tf.Variable(initial_value=0,\n                          name='global_step', trainable=False)\n", "intent": "Create a variable for keeping track of the number of optimization iterations performed.\n"}
{"snippet": "x = torch.randn((2, 2))\ny = torch.randn((2, 2))\nz = x + y  \nvar_x = autograd.Variable(x, requires_grad=True)\nvar_y = autograd.Variable(y, requires_grad=True)\nvar_z = var_x + var_y\nprint(var_z.grad_fn)\nvar_z_data = var_z.data  \nnew_var_z = autograd.Variable(var_z_data)\nprint(new_var_z.grad_fn)\n", "intent": "Understanding what is going on in the block below is crucial for being a\nsuccessful programmer in deep learning.\n"}
{"snippet": "net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n", "intent": "After the convolution we do a max-pooling which is also described in Tutorial \n"}
{"snippet": "net = tf.layers.conv2d(inputs=net, name='layer_conv2', padding='same',\n                       filters=36, kernel_size=5, activation=tf.nn.relu)\n", "intent": "Then we make a second convolutional layer, also with max-pooling.\n"}
{"snippet": "net = tf.contrib.layers.flatten(net)\n", "intent": "The output then needs to be flattened so it can be used in fully-connected (aka. dense) layers.\n"}
{"snippet": "net = tf.layers.dense(inputs=net, name='layer_fc1',\n                      units=128, activation=tf.nn.relu)\n", "intent": "We can now add fully-connected (or dense) layers to the neural network.\n"}
{"snippet": "x = tf.get_variable(name=\"x\", shape=[], dtype=tf.float32, initializer=tf.zeros_initializer)\n", "intent": "To define TensorFlow variables, use the `get_variable()` function as follows:\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Useful in Jupyter Notebooks\n"}
{"snippet": "lap_graph = tf.Graph()\nwith lap_graph.as_default():\n    lap_in = tf.placeholder(np.float32, name='lap_in')\n    lap_out = lap_normalize(lap_in)\nshow_graph(lap_graph)\n", "intent": "We did all this in TensorFlow, so it generated a computation graph that we can inspect.\n"}
{"snippet": "l = tf.Variable(\"local_cpu\")\nl.device\n", "intent": "Each Variable is assigned to a specific device.\n"}
{"snippet": "for ps in param_servers:\n    with tf.device(ps):\n        v = tf.Variable(\"my_var\")\nv.device\n", "intent": "We can enforce the assigned device using the `tf.device` context.\n"}
{"snippet": "def negative_log_likelihood(X, y, w):\n    scores = sigmoid(np.dot(X, w))\n    nll = -np.sum(y*np.log(scores+1e-15) + (1-y)*np.log(1-scores+1e-15))\n    print(y)\n    print(nll)\n    return nll\n", "intent": "As defined in Eq. 33\n"}
{"snippet": "import numpy as np\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\nwith tf.Session(\"grpc://clustered-tensorflow-worker0:2222\", graph=graph) as session:\n    result = session.run(final_result, feed_dict={ input_array: np.ones([1000]) }, options=run_options)\n    print(result)\n", "intent": "We can now run the graph \n"}
{"snippet": "with tf.Session() as sess:\n    get_shape = tf.shape([[[1, 2, 3], [1, 2, 3]],\n                          [[2, 4, 6], [2, 4, 6]],\n                          [[3, 6, 9], [3, 6, 9]],\n                          [[4, 8, 12], [4, 8, 12]]])\n    shape = sess.run(get_shape)\n    print(\"Shape of tensor: \" + str(shape))\n", "intent": "You can use the `tf.shape` Operation to get the shape value of `Tensor` objects:\n"}
{"snippet": "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n    with tf.device(\"/gpu:0\"):\n      result = sess.run(product,feed_dict={matrix1: [[3., 3.]], matrix2: [[6.],[6.]]})\n      print result\n", "intent": "Sessions must be closed to release resources.  We may use the 'with' syntax to close sessions automatically when completed.\n"}
{"snippet": "w = tf.Variable(np.random.normal(), name=\"W\")\n", "intent": "We also create a variable for the weights and note that a NumPy array is convertible to a Tensor.\n"}
{"snippet": "W_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\nkeep_prob = tf.placeholder(\"float\")\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n", "intent": "Regularization / Dropout Layer Avoids Overfitting\n"}
{"snippet": "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n", "intent": "Softmax Layer Produces Class Probabilities\n"}
{"snippet": "sess.close()\nops.reset_default_graph()\nfrom tensorflow.models.rnn import rnn_cell, seq2seq\nsess = tf.InteractiveSession()\nseq_length = 5\nbatch_size = 64\nvocab_size = 7\nembedding_dim = 50\nmemory_dim = 100\n", "intent": "In the next example, we demonstrate an autoencoder which learns a lower-dimensional representation of sequential input data.\n"}
{"snippet": "X_batch = [np.random.choice(vocab_size, size=(seq_length,), replace=False)\n           for _ in range(10)]\nX_batch = np.array(X_batch).T\nfeed_dict = {enc_inp[t]: X_batch[t] for t in range(seq_length)}\ndec_outputs_batch = sess.run(dec_outputs, feed_dict)\nprint(X_batch)\n[logits_t.argmax(axis=1) for logits_t in dec_outputs_batch]\ntmp_def = rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\nshow_graph(tmp_def) \n", "intent": "We can now test our lower dimensional autoencoder by passing data through the embedding to determine if the similar input was recovered.\n"}
{"snippet": "NUM_STEPS = 10000\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\n", "intent": "Next, create a session and initialize the graph variables. We'll also set the number of steps we'll train with.\n"}
{"snippet": "def get_gradient(X, y, w, mini_batch_indices, lmbda):\n    n_batch = mini_batch_indices.shape[0]\n    nll_gradient = np.dot(X[mini_batch_indices].T,sigmoid(np.dot(X[mini_batch_indices], w)) - y[mini_batch_indices])\n    ones = np.ones(w.shape)\n    ones[0] = 0\n    reg_gradient = lmbda * ones * w\n    grad = nll_gradient / n_batch + reg_gradient  \n    return grad\n", "intent": "Make sure that you compute the gradient of the loss function $\\mathcal{L}(\\mathbf{w})$ (not simply the NLL!)\n"}
{"snippet": "age = tf.contrib.layers.real_valued_column(\"age\")\neducation_num = tf.contrib.layers.real_valued_column(\"education_num\")\ncapital_gain = tf.contrib.layers.real_valued_column(\"capital_gain\")\ncapital_loss = tf.contrib.layers.real_valued_column(\"capital_loss\")\nhours_per_week = tf.contrib.layers.real_valued_column(\"hours_per_week\")\n", "intent": "Second, configure the real-valued columns.\n"}
{"snippet": "age_buckets = tf.contrib.layers.bucketized_column(age,\n            boundaries=[ 18, 25, 30, 35, 40, 45, 50, 55, 60, 65 ])\neducation_occupation = tf.contrib.layers.crossed_column([education, occupation], hash_bucket_size=int(1e4))\nage_race_occupation = tf.contrib.layers.crossed_column( [age_buckets, race, occupation], hash_bucket_size=int(1e6))\ncountry_occupation = tf.contrib.layers.crossed_column([native_country, occupation], hash_bucket_size=int(1e4))\n", "intent": "We do a few combined features (feature crosses) here. You can add your own to improve on the model!\n"}
{"snippet": "with tf.Session():\n    a = tf.constant(10)\n    b = tf.constant(20)\n    c=a+b\n    print c\n    print(c.eval())\n", "intent": "Put everything in a session, no need to call session, just use .eval()\n"}
{"snippet": "tf.logging.set_verbosity(tf.logging.INFO)\nshutil.rmtree('taxi_trained', ignore_errors=True) \nmodel = tf.contrib.learn.LinearRegressor(\n      feature_columns=make_feature_cols(), model_dir='taxi_trained')\nmodel.fit(input_fn=make_input_fn(df_train), steps=10);\n", "intent": "<h3> Linear Regression with tf.learn Estimators framework </h3>\n"}
{"snippet": "tf.logging.set_verbosity(tf.logging.INFO)\nshutil.rmtree('taxi_trained', ignore_errors=True) \nmodel = tf.contrib.learn.DNNRegressor(hidden_units=[32, 8, 2],\n      feature_columns=make_feature_cols(), model_dir='taxi_trained')\nmodel.fit(input_fn=make_input_fn(df_train), steps=100);\nprint_rmse(model, 'validation', make_input_fn(df_valid))\n", "intent": "<h3> Deep Neural Network regression </h3>\n"}
{"snippet": "tf.logging.set_verbosity(tf.logging.INFO)\nshutil.rmtree('taxi_trained', ignore_errors=True) \nmodel = tf.contrib.learn.LinearRegressor(\n      feature_columns=feature_cols, model_dir='taxi_trained')\nmodel.fit(input_fn=get_train());\n", "intent": "<h2> Create and train the model </h2>\nNote that we no longer have a num_steps variable.  get_train() specifies a num_epochs.\n"}
{"snippet": "INPUT_COLUMNS = [\n    layers.real_valued_column('pickuplon'),\n    layers.real_valued_column('pickuplat'),\n    layers.real_valued_column('dropofflat'),\n    layers.real_valued_column('dropofflon'),\n    layers.real_valued_column('passengers'),\n]\nfeature_cols = INPUT_COLUMNS\n", "intent": "<h2> Create features out of input data </h2>\nFor now, pass these through.  (same as previous lab)\n"}
{"snippet": "forest_params = {'max_depth': range(10, 21),\n                'max_features': range(5, 105, 10)}\nlocally_best_forest = GridSearchCV (RandomForestClassifier(), cv=3, param_grid=forest_params)\nlocally_best_forest.fit(X_train, y_train)\n", "intent": "Train a random forest **(RandomForestClassifier)**. Tune the maximum depth and maximum number of features for each tree using **GridSearchCV**. \n"}
{"snippet": "session = tf.Session()\n", "intent": "TensorFLow Run Session\n======================\n"}
{"snippet": "import numpy as np\nimport theano\nimport theano.tensor as T\nx = T.vector()\ny = T.vector()\nz = x + x\nz = z * y\nf = theano.function([x, y], z)\nf(np.ones((2,)), np.ones((3,)))\n", "intent": "Here is an example of code with a user error. Don't try to find the error by code inspection, check only the error:\n"}
{"snippet": "model = LogisticRegression(penalty='l1', C=22)\nmodel.fit(X, Y)\nmodel.coef_\n", "intent": "Let's check coefficients now.\n"}
{"snippet": "model=Sequential()   \nmodel.add(Dense(units=64,input_dim=4))  \nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(units=32)) \nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(units=3))\nmodel.add(Activation(\"softmax\"))\n", "intent": "But first : let's do a quick install\nhttps://github.com/fchollet/hualos\n& run the app:\n```\npython api.py\n```\n"}
{"snippet": "model_vgg16_conv = VGG16(weights=None, include_top=False)\nmodel_vgg16_conv.summary()\ninput = Input(shape=(32,32,3),name = 'image_input')  \noutput_vgg16_conv = model_vgg16_conv(input)\nx = Flatten(name='flatten')(output_vgg16_conv)\nx = Dense(4096, activation='relu', name='fc1')(x)\nx = Dense(4096, activation='relu', name='fc2')(x)\nx = Dense(10, activation='softmax', name='predictions')(x)\nmy_model = Model(input=input, output=x)  \nmy_model.summary()\n", "intent": "Lets import the model this time\n"}
{"snippet": "model_2 = Sequential()\nmodel_2.add(Dense(500, activation='relu', input_shape=(784,)))\nmodel_2.add(Dropout(0.2))\nmodel_2.add(Dense(500, activation='relu'))\nmodel_2.add(Dropout(0.2))\nmodel_2.add(Dense(10, activation='softmax'))\n", "intent": "[why is training accuracy lower than val](https://keras.io/getting-started/faq/\n"}
{"snippet": "history=[]\nbatch_size=32\nepochs= 20\nfor i in range(epochs):\n    mod=model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n    model.reset_states()\n    history.append(mod.history)\n", "intent": "https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/\n"}
{"snippet": "import statsmodels.api as sm\nimport patsy\ndf['ones'] = np.ones_like(df['DomesticTotalGross'])\ny, X = patsy.dmatrices('DomesticTotalGross ~ ones', data=df, return_type='dataframe')\nmodel = sm.OLS(y, X)\nres = model.fit()\n", "intent": "The results give the mean of all data points for domestic total gross. The residuals are skewed right.\n"}
{"snippet": "y4, X4 = patsy.dmatrices('DomesticTotalGross ~ Budget + rating_R + Runtime + FridayReleaseDay + log_director_credits', data=movies_df2, return_type=\"dataframe\")\nmodel4 = sm.OLS(y4, X4)\nfit4 = model4.fit()\nfit4.summary()\n", "intent": "Model using budget, R rating, runtime, Friday release day dummy variable, and log(director credits).\n"}
{"snippet": "y4, X4 = patsy.dmatrices('log_DomesticTotalGross ~ Budget + Runtime', data=movies_df2, return_type=\"dataframe\")\nmodel4 = sm.OLS(y4, X4)\nfit4 = model4.fit()\nfit4.summary()\n", "intent": "Budget and Runtime features have the highest p-values in the model.\n"}
{"snippet": "y4, X4 = patsy.dmatrices('log_DomesticTotalGross ~ Budget + Runtime*FridayReleaseDay*rating_R ', data=movies_df2, return_type=\"dataframe\")\nmodel4 = sm.OLS(y4, X4)\nfit4 = model4.fit()\nfit4.summary()\n", "intent": "Adding interaction terms.\n"}
{"snippet": "import theano\nfrom theano import tensor as T\nx = T.vector('x')\nW = T.matrix('W')\nb = T.vector('b')\n", "intent": "This notebook contains the code snippets from the slides, so you can execute them and tinker with those examples.\nTo execute a cell: Ctrl-Enter.\n"}
{"snippet": "zip(list(X.columns),list(model.fit(X,Y).coef_[0]))\n", "intent": "Not as good as Logit but better than KNN\n"}
{"snippet": "logreg.fit(X,y).coef_\n", "intent": "KNN and logistic regression are doing slightly better than the always-PG-13 predictor, and all 3 have about 0.5 accuracy.\n"}
{"snippet": "y,X = patsy.dmatrices('damage~type+construction+operation+months', data = data, return_type = 'dataframe')\nmodel0 = sm.GLM(y,X,family = sm.families.Poisson(sm.families.links.log))\nresults0 = model0.fit()\nprint(results0.summary())\n", "intent": "**Challenge Number 1**\n"}
{"snippet": "y2,X2 = patsy.dmatrices('damage~type+construction+operation', data = data, return_type = 'dataframe')\nmodel1 = sm.GLM(y2,X2,family = sm.families.Poisson(sm.families.links.log), offset = np.log(data['months']))\nresults1 = model1.fit()\nprint(results1.summary())\n", "intent": "**Challenge Number 2**\n"}
{"snippet": "ys= ['averageRating', 'tmtopave', 'tmallave','audiencescore', 'tmtop', 'tmall']\ncs= []\nfor tar in ys:\n    y, X = patsy.dmatrices(tar + ' ~ absmeantime + initialRush  + usaGross', data=Data, return_type=\"dataframe\")\n    model = sm.OLS(y, X)\n    fit = model.fit()\n    i=fit.params\n    cs.append([i[0],i[2]])\n", "intent": "Make 6 part graph on initial analysis\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nmodel = Sequential() \nmodel.add(Dense(256, input_shape=(max_words,), activation='elu')) \nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5)) \nmodel.add(Dense(2, activation='softmax'))\n", "intent": "we will be using Dense fully connected NN.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.get_variable(\n        \"EmbeddingMatrix\",\n        shape=[vocab_size, embed_dim],\n        initializer=tf.random_uniform_initializer(-1., 1.)\n    )\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "import theano, theano.tensor as T\nimport lasagne\nfrom lasagne.layers import *\nEMBEDDING_SIZE = 128    \nLSTM_SIZE  = 256        \nATTN_SIZE  = 256        \nFEATURES,HEIGHT,WIDTH = img_codes.shape[1:]\n", "intent": "Since the image encoder CNN is already applied, the only remaining part is to write a sentence decoder.\n"}
{"snippet": "full_dropout = tf.nn.dropout(full_layer, keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "learning_rate = 0.1\nupdates = [\n    (param_i, param_i - learning_rate * grad_i)\n    for param_i, grad_i in zip(params, grads)\n]\nupdate_model = theano.function([x, mask], cost, updates=updates)\nevaluate_model = theano.function([x, mask], cost)\n", "intent": "We can now compile the function that updates the gradients. We also added a function that computes the cost without updating for monitoring purposes.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Tip: When using Jupyter notebook make sure to call tf.reset_default_graph() at the beginning to clear the symbolic graph before defining new nodes.\n"}
{"snippet": "sess = tf.Session() \n", "intent": "Use the `.run()` method on a Session and evaluate tensors and execute operations.\n"}
{"snippet": "model.fit(X_train, y_train, epochs = 50, batch_size= 32)\n", "intent": "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the fit function.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), maxval=1, minval=-1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "rate = 0.001\nmu = 0\nsigma = 0.1\nmodel = LeNetModel(32,32,1,mu_init=mu,sigma_init=sigma)\nlogits = model.build(filter1=6,filter2=16,layer1=120,layer2=84,kernel1=5,kernel2=5)\nlogits2 = LeNet(x)\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\nloss_operation = tf.reduce_mean(cross_entropy)\noptimizer = tf.train.AdamOptimizer(learning_rate = rate)\ntraining_operation = optimizer.minimize(loss_operation)\n", "intent": "Create a training pipeline that uses the model to classify MNIST data.\nYou do not need to modify this section.\n"}
{"snippet": "dTree = DecisionTreeClassifier(criterion=\"entropy\")\ndTree.fit(trainData.ix[:,0:6], trainData.index)\n", "intent": "Vamos ao que interessa...\n"}
{"snippet": "wknn1 = KNeighborsClassifier(n_neighbors=1,weights='uniform')\nwknn1.fit(trfeatures, trlabels)\nwknn1.score(ttfeatures,ttlabels)\n", "intent": "Ou vamos mudar o k para 1:\n"}
{"snippet": "print(\"K \\t Uniform \\t Distance\")\nfor k in range(1,11):\n    UniformKnnClassifier = KNeighborsClassifier(n_neighbors=k,weights='uniform')\n    UniformKnnClassifier.fit(trfeatures, trlabels)\n    uScore = UniformKnnClassifier.score(ttfeatures, ttlabels)\n    DistanceKnnClassifier = KNeighborsClassifier(n_neighbors=k,weights='distance')\n    DistanceKnnClassifier.fit(trfeatures, trlabels)\n    dScore = DistanceKnnClassifier.score(ttfeatures, ttlabels)\n    print k,\"\\t{:f} \\t{:f}\".format(uScore,dScore) \n", "intent": "Verifique no intervalo de k = 1 a 10, qual o melhor valor de k e o melhor tipo de pesso a ser utilizado \n"}
{"snippet": "d_tree = DecisionTreeClassifier(criterion=\"entropy\")\nd_tree.fit(train_data, train_data.index) \n", "intent": "Vamos ao que interessa...\n"}
{"snippet": "lr = .3\nupdates = [(par, par - lr * gra) for par, gra in zip(parameters, gradient)] \nupdate_model = theano.function([x], cost, updates=updates)\nget_cost = theano.function([x], mse)\npredict = theano.function([x], prediction)\nget_hidden = theano.function([x], hidden)\nget_gradient = theano.function([x], gradient)\n", "intent": "We now compile the function that will update the parameters of the model using gradient descent. \n"}
{"snippet": "model = sm.OLS(y, X).fit() \n", "intent": "Note the difference in argument order:\n"}
{"snippet": "model = LogisticRegression()\nmodel = model.fit(X, y)\nmodel.score(X, y)\n", "intent": "Let's go ahead and run logistic regression on the entire data set, and see how accurate it is!\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(X_train, y_train)\nreg.score(X_test, y_test)\n", "intent": "Train (on the training data) and score (on the test data) a logistic regressor that predicts the correct label for the iris data set.\n"}
{"snippet": "visualize_classifier(DecisionTreeClassifier(max_depth=2), X, y)\n", "intent": "Just because we are so familiar with them now, please run a decision tree with very shallow maximum depth. It will do as bad as you think!\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nvisualize_classifier(KNeighborsClassifier(n_neighbors=5), X, y)\n", "intent": "What kind of classifier that you have seen before do you expect to perform pretty well? Try!\n"}
{"snippet": "visualize_classifier(SVC(), X, y)\nvisualize_classifier(AdaBoostClassifier(SVC(), algorithm=\"SAMME\"), X, y)\n", "intent": "Perfection! This must mean a model like that always gets everything right, doesn't it?\n"}
{"snippet": "mlp = MLPClassifier(hidden_layer_sizes=(200, 50, 50), max_iter=100, alpha=1e-4,\n                    solver='sgd', verbose=50, tol=1e-4, random_state=1,\n                    learning_rate_init=.1 )\n", "intent": "Now train a deep network, with three layers. You should be able to get the $R^2$ close to 98%. Good luck!\n"}
{"snippet": "regressor = LinearRegression()\n", "intent": "These models all work as follows. The first thing you need to is to start an instance of the model object:\n"}
{"snippet": "for i in range(1001):\n    mse_train = update_model(data_train)\n    if i % 100 == 0:\n        mse_val = get_cost(data_val)\n        print 'Epoch {}: train mse: {}    validation mse: {}'.format(i, mse_train, mse_val)\n", "intent": "We can now train the network by supplying this function with our data and calling it repeatedly.\n"}
{"snippet": "with graph.as_default():\n    flat = tf.reshape(inception_out, (-1, 8*144))\n    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n    logits = tf.layers.dense(flat, n_classes)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost)\n    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n", "intent": "Now, flatten and pass to the classifier\n"}
{"snippet": "with graph.as_default():\n    flat = tf.reshape(max_pool_4, (-1, 8*144))\n    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n    logits = tf.layers.dense(flat, n_classes)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost)\n    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n", "intent": "Now, flatten and pass to the classifier\n"}
{"snippet": "with graph.as_default():\n    conv1 = tf.layers.conv1d(inputs=inputs_, filters=18, kernel_size=2, strides=1, \n                             padding='same', activation = tf.nn.relu)\n    n_ch = n_channels *2\n", "intent": "Build Convolutional Layer(s)\nQuestions: \n* Should we use a different activation? Like tf.nn.tanh?\n* Should we use pooling? average or max?\n"}
{"snippet": "with graph.as_default():\n    lstm_in = tf.transpose(conv1, [1,0,2]) \n    lstm_in = tf.reshape(lstm_in, [-1, n_ch]) \n    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None) \n    lstm_in = tf.split(lstm_in, seq_len, 0)\n    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n    initial_state = cell.zero_state(batch_size, tf.float32)\n", "intent": "Now, pass to LSTM cells\n"}
{"snippet": "with graph.as_default():\n    lstm_in = tf.transpose(inputs_, [1,0,2]) \n    lstm_in = tf.reshape(lstm_in, [-1, n_channels]) \n    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None) \n    lstm_in = tf.split(lstm_in, seq_len, 0)\n    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    cell = tf.contrib.rnn.MultiRNNCell([lstm] * lstm_layers)\n    initial_state = cell.zero_state(batch_size, tf.float32)\n", "intent": "Construct inputs to LSTM\n"}
{"snippet": "with graph.as_default():\n    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32,\n                                                     initial_state = initial_state)\n    logits = tf.layers.dense(inputs=outputs[-1], units=n_classes, name='logits')\n    labels = tf.one_hot(indices=indices_, depth=n_classes, dtype=logits.dtype)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost) \n    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n", "intent": "Define forward pass, cost function and optimizer:\n"}
{"snippet": "with graph.as_default():\n    lstm_in = tf.transpose(inputs_, [1,0,2]) \n    lstm_in = tf.reshape(lstm_in, [-1, n_channels]) \n    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None) \n    lstm_in = tf.split(lstm_in, seq_len, 0)\n    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n    initial_state = cell.zero_state(batch_size, tf.float32)\n", "intent": "Construct inputs to LSTM\n"}
{"snippet": "with graph.as_default():\n    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32,\n                                                     initial_state = initial_state)\n    logits = tf.layers.dense(outputs[-1], n_classes, name='logits')\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_))\n    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost) \n    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n", "intent": "Define forward pass, cost function and optimizer:\n"}
{"snippet": "lr = LinearRegression(fit_intercept=True)\nlr.fit(X2, y)\nprint lr.intercept_\nprint lr.coef_\nprint 'Estimated function: y = %.2f + %.2fx0 + %.2fx1' %(lr.intercept_, lr.coef_[0], lr.coef_[1])\n", "intent": "Now let's fit a linear model where the input features are (x, x^2).\n"}
{"snippet": "f = theano.function(inputs=[X, W, b],\n                    outputs=output,\n                    updates=updates)\nX_value = np.arange(-3, 3).reshape(3, 2).astype(theano.config.floatX)\nW_value = np.eye(2).astype(theano.config.floatX)\nb_value = np.arange(2).astype(theano.config.floatX)\nprint(f(X_value, W_value, b_value))\n", "intent": "We can now compile our Theano function and see that it gives the expected results.\n"}
{"snippet": "smf.ols(formula = 'sale_price ~ gross_sq_feet', data = data1.ix[15:70]).fit().summary()\n", "intent": "And that's how we produced lecture example\n"}
{"snippet": "lm=smf.ols('speed~age+C(Gender)+C(hour)',data=data2.ix[comind&demoind]).fit()\nlm.summary()\n", "intent": "Now consider hours of the day\n"}
{"snippet": "lm=smf.ols('speed~age+C(Gender)+C(hour)',data=data2.ix[comind&demoind&(data2.wd==1)]).fit()\nlm.summary()\n", "intent": "One can generally recognize five periods in the day - morning (7-10am, day-11am-5pm, evening-6-10pm, late evening-11pm-1am and night-2am-6am)\n"}
{"snippet": "lm=smf.ols('speed~age+C(Gender)+C(wd)+C(hour)',data=data2.ix[comind&demoind]).fit()\nlm.summary()\n", "intent": "Run a regression using both - wd and hour\n"}
{"snippet": "lm=smf.ols('speed~age+C(Gender)+C(wd)+C(hour)+StartDens+EndDens',data=data2.ix[comind&demoind]).fit()\nlm.summary()\n", "intent": "Finally add density parameters for beginning and the end of the trip\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(train_dtm, y_train)\n", "intent": "Let's build the model with Naive Bayes Now\nhttp://scikit-learn.org/stable/modules/naive_bayes.html\n"}
{"snippet": "fit2 = smf.ols('Y ~ X1 + X2 + X3 + X4 + X6', data=x).fit()\nprint fit2.summary()\n", "intent": "remove feature w/ lowest (abs) t score\n"}
{"snippet": "fit4 = smf.ols('Y ~ X1 + X3 + X6', data=x).fit()\nprint fit4.summary()\n", "intent": "--> increasing bias, decreasing variance\n"}
{"snippet": "fit5 = smf.ols('Y ~ X1 + X3', data=x).fit()\nprint fit5.summary()\n", "intent": "$\\rightarrow$ optimal bias-variance point reached\n"}
{"snippet": "f = theano.function(inputs=[M, s],\n                    outputs=output,\n                    updates=updates)\nM_value = np.arange(9).reshape(3, 3).astype(theano.config.floatX)\ns_value = np.zeros((3, ), dtype=theano.config.floatX)\nprint(f(M_value, s_value))\n", "intent": "We can now compile and test the Theano function :\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1, tf.float32))\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "w=tf.Variable(tf.ones((2,2)),name='weights')\n", "intent": "**Notes for `eval()`**:\n`t.eval()` is a shortcut for calling `tf.get_default_session().run(t)`\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\ntree = DecisionTreeClassifier()\nbag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\n                        random_state=1)\nbag.fit(X, y)\nvisualize_classifier(bag, X, y)\n", "intent": "This type of bagging classification can be done manually using Scikit-Learn's ``BaggingClassifier`` meta-estimator, as shown here:\n"}
{"snippet": "text_clf = build_pipeline()\ntext_clf = text_clf.fit(X_train, y_train)\n", "intent": "We should now be ready to feed these vectors into a classifier. \n"}
{"snippet": "text_clf = build_pipeline()\ntext_clf = text_clf.fit(X_train, y_train)\n", "intent": "We should now be ready to feed these vectors into a classifier\n"}
{"snippet": "from sklearn import svm\nsvc = svm.SVC(kernel='rbf')\nsvc.fit(training_data, labels)  \n", "intent": "Instantiate the classifier here:\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\n", "intent": "Import GaussianNB and instantiate one\n"}
{"snippet": "gnb.fit(data,labels)\n", "intent": "Fit the training data:\n"}
{"snippet": "f = theano.function(inputs=[f_init],\n                    outputs=[next_fibonacci_terms, ratios_between_terms],\n                    updates=updates)\nout = f([1, 1])\nprint(out[0])\nprint(out[1])\n", "intent": "Let's compile our Theano function which will take a vector of consecutive values from the Fibonacci sequence and compute the next 10 values :\n"}
{"snippet": "dtree = tree.DecisionTreeClassifier()\ndtree = dtree.fit(features,labels)\ndtree\n", "intent": "Create a Decision Tree and fit the training data\n"}
{"snippet": "from sklearn import svm\nsvr = svm.SVR()\nsvr.fit(X, y) \n", "intent": "Fit the model using Support Vector Regression\n"}
{"snippet": "y_true = X[:, 0]*X[:, 1] + \\\n    torch.tensor(np.log(X[:, 2])).float() + \\\n    (torch.tensor(np.random.rand(5)).float() * X[:, 3:8]).sum(dim=1) + \\\n    torch.tensor(np.random.normal(0, 0.1, 50)).float()\n", "intent": "Use provided $\\textbf{X}$ and $\\textbf{y}$ values to find gradient of RMSE loss with respect to parameters of your network.\n"}
{"snippet": "def get_random_training_sample():\n    cls = random.choice(unique_classes)\n    word = random.choice(examples_by_class[cls])\n    word_tensor = word_to_tensor(word)\n    class_tensor = class_to_tensor(cls)\n    return word, cls, word_tensor, class_tensor\n", "intent": "As you can see the output is a ``<1 x len(unique_classes)>`` Tensor, where\nevery item is the likelihood of that category (higher is more likely).\n"}
{"snippet": "encoder_test = EncoderRNN(10, 10, 2)\nprint(encoder_test)\nencoder_hidden = encoder_test.init_hidden(1)\nword_input = torch.LongTensor([[1, 2, 3]])\nif USE_CUDA:\n    encoder_test.cuda()\n    word_input = word_input.cuda()\nencoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\nencoder_outputs.shape\n", "intent": "To make sure the Encoder and Decoder model are working (and working together) we'll do a quick test with fake word inputs:\n"}
{"snippet": "x = Variable(torch.ones(2,2), requires_grad=True) \nprint(x)\n", "intent": "Example from official documentation\n"}
{"snippet": "x = torch.autograd.Variable(torch.ones(1,1,32,32), requires_grad=True)\nprint(x.size())\nx = x.view(x.size(0), -1)\nprint(x.size())\n", "intent": "Understanding pytorch tensor reshaping\n"}
{"snippet": "def get_model_i(i=0):\n    val_idxs = get_cv_idxs(n, i, val_pct=0.1)\n    tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_basic, max_zoom=1.05)\n    data = ImageClassifierData.from_csv(path, 'train', f'{path}train.csv', bs, tfms, \n                                        val_idxs=val_idxs, suffix='.jpg', continuous=True)\n    learn = ConvLearnerVGG.pretrained(data, ps=0.0, precompute=False)\n    return learn\n", "intent": "Here's code to do cross-validation:\n"}
{"snippet": "base_model = ResNet50(weights='imagenet', include_top=False)\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\npredictions = Dense(1, activation='sigmoid')(x)\n", "intent": "In Keras you have to manually specify the base model and construct on top the layers you want to add.\n"}
{"snippet": "bkm = bikmeans(5, points)\n", "intent": "Perform clustering (k=5)\n"}
{"snippet": "tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_side_on, crop_type=CropType.NO)\nmd = ImageClassifierData.from_csv(PATH, JPEGS, CSV, tfms=tfms)\n", "intent": "From here on it's jus tlike Dogs vs Cats! We have a CSV file containing a bunch of file names, and for each one: the class and bounding box.\n"}
{"snippet": "learn.fit(lrs/5, 1, cycle_len=2)\n", "intent": "Accuracy isn't improving much - since many images have multiple different objects, it's going to be impossible to be that accurate.\n"}
{"snippet": "learn.fit(lr, 1, cycle_len=3, use_clr=(32,5))\n", "intent": "Now we have something that's printing out our Object Detection Loss, Accuracy, and Detection L1:\n"}
{"snippet": "K.set_value(m_final.optimizer.lr, 1e-4)\nm_final.fit([arr_lr, arr_hr], targ, 16, 2, **pars)\n", "intent": "We use learning rate annealing to get a better fit.\n"}
{"snippet": "top_model = Model(inp, outp)\n", "intent": "We're only interested in the trained part of the model, which does the actual upsampling. *the upsampling model*\n"}
{"snippet": "K.set_value(m_sr.optimizer.lr, 1e-4)\nm_sr.fit([arr_lr, arr_hr], targ, 8, 1, **pars)\n", "intent": "We use learning rate annealing to get a better fit.\n"}
{"snippet": "top_model = Model(inp, outp)\n", "intent": "We're only interested in the trained part of the mdoel, which does the actual upsampling.\n"}
{"snippet": "inp = Input((288, 288, 3))\nref_model = Model(inp, ReflectionPadding2D((40, 10))(inp))\nref_model.compile('adam', 'mse')\n", "intent": "Testing the reflection padding layer:\n"}
{"snippet": "batch_size=16\nvgg = Vgg16()\nmodel = vgg.model\nlast_conv_idx = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\nconv_layers = model.layers[:last_conv_idx + 1]\nconv_model = Sequential(conv_layers)\n", "intent": "**---------------------------------- Now the actual work part ----------------------------------**\n"}
{"snippet": "km = kmeans(5, points)\nprint(\"SSE of clustering: \", km['sse'])\n", "intent": "Perform regular K-means clustering and compute SSE\n"}
{"snippet": "model = Sequential ([\n    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, weights=[emb]),\n    Dropout(0.2),\n    graph,\n    Dropout(0.5),\n    Dense(100, activation='relu'),\n    Dropout(0.7),\n    Dense(1, activation='sigmoid')\n    ])\n", "intent": "We then replace the conv/max-pool layer in our original CNN with the concatenated conv layers.\n"}
{"snippet": "model = Sequential([\n    Embedding(vocab_size, 32, input_length=seq_len, mask_zero=True,\n              W_regularizer=l2(1e-6), dropout=0.2),\n    LSTM(100, consume_less='gpu'),\n    Dense(1, activation='sigmoid')])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "This more complex architecture has given us another boost in accuracy.\nWe haven't covered this bit yet!\n"}
{"snippet": "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=1,\n             validation_data=(conv_val_feat, val_labels))\n", "intent": "Now we can train the model as usual, with pre-computed augmented data.\n"}
{"snippet": "dense_out = Dense(vocab_size, activation='softmax') \n", "intent": "This is the 'blue arrow' from our diagram - the layer operation from hidden to hidden.\n"}
{"snippet": "model = Model([c[0] for c in c_ins], c_out)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n", "intent": "So now we can create our model\n"}
{"snippet": "dense_in = Dense(n_hidden, activation='relu')\ndense_out = Dense(vocab_size, activation='softmax', name='output')\n", "intent": "Now our y dataset looks exactly like our x dataset did before, but everything's shifted over by 1 character.\n"}
{"snippet": "model.fit(x_rnn[:mx], y_rnn[:mx], batch_size=bs, nb_epoch=4, shuffle=False)\n", "intent": "The LSTM model takes much longer to run than the regular RNN because it isn't in parallel: each operation has to be run in order.\n"}
{"snippet": "model.fit(oh_x_rnn, oh_y_rnn, batch_size=64, nb_epoch=8)\n", "intent": "The `86` is the onehotted dimension; classes of characters\n"}
{"snippet": "model = Model([inp, sz_inp], x)\nmodel.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "When we compile the mdoel, we have to specify all the input layers in an array\n"}
{"snippet": "from keras.models import load_model\nmodel.save('my_model.h5')\nmodel2 = load_model('my_model.h5')\n", "intent": "It is really important to be able to reload the model after you've been training it for hours on end (usually).\n"}
{"snippet": "model.optimizer.lr = 1e-5\nmodel.fit(conv_feat, [trn_bbox, trn_labels], batch_size=batch_size, nb_epoch=12,\n             validation_data=(conv_val_feat, [val_bbox, val_labels]))\n", "intent": "The above is after 23 epochs. After 35:\n"}
{"snippet": "def sigmoid(x): return 1 / (1 + np.exp(-x))\ndef sigmoid_d(x):\n    output = sigmoid(x)\n    return output * (1 - output)\ndef relu(x): return np.maximum(0., x)\ndef relu_d(x): return (x > 0.) * 1.\n", "intent": "The key step done automatically above by Theano is gradients calculation.\n---\nWill need to define all functions used:\n"}
{"snippet": "model = Sequential([\n            GRU(n_hidden, return_sequences=True, input_shape=(cs, vocab_size),\n                   activation='relu', inner_init='identity'),\n            TimeDistributed(Dense(vocab_size, activation='softmax')),\n        ])\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam())\nmodel.fit(oh_x_rnn, oh_y_rnn, batch_size = 64, nb_epoch=8)\n", "intent": "---\nGated Recurrent Unit\n"}
{"snippet": "latest_weights_filename = None\nfor epoch in range(no_of_epochs):\n    print \"Running epoch %d\" % epoch\n    vgg.fit(batches, val_batches, nb_epoch=1)\n    latest_weights_filename = 'ft%d.h5' % epoch\n    vgg.model.save_weights(results_path + latest_weights_filename)\nprint \"Completed %s fit operations\" % no_of_epochs\n", "intent": "We're also saving our weights after each epoch.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, input_dim=784))\nmodel.add(Activation('relu'))\n", "intent": "You can also simply add layers via the ```.add()``` method:\n"}
{"snippet": "model.add(Dense(2, activation='softmax'))\n", "intent": "Now we add our final Cat vs Dog layer\n"}
{"snippet": "models = [train_model() for m in xrange(6)]\n", "intent": "Trying the above again, this time having h5py installed.\n"}
{"snippet": "VGG = Vgg16()\nVGG.model.pop()\nfor layer in VGG.model.layers: layer.trainable = False\nVGG.model.add(Dense(10, activation='softmax'))\nVGG.model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "NOTE: I'll want a way to clear GPU memory in the future. Right now all I know is restarting the kernel.\n"}
{"snippet": "m = RandomForestRegressor(n_jobs=-1)\nm.fit(df, y)\nm.score(df, y)\n", "intent": "The R^2 score shown below shows the variance (or mean?) of the data. It shows how much the data varies.\n"}
{"snippet": "import keras.backend as K\ndef create_model(layers, activation):\n    model = Sequential()\n    for i, nodes in enumerate(layers):\n        if i==0:\n            model.add(Activation(activation))\n        else:\n    model.compile(optimizer='adadelta', loss='mse')\n    return model\nmodel = KerasRegressor(build_fn=create_model, verbose=0)    \n", "intent": "What is Cross Validation?\nhttps://stats.stackexchange.com/questions/1826/cross-validation-in-plain-english\n"}
{"snippet": "ms = MeanShift()\nms.fit(X)\nlabels = ms.labels_\ncluster_centers = ms.cluster_centers_\nn_clusters = len(np.unique(labels))\n", "intent": "Let's see how Mean Shift does on this example:\n"}
{"snippet": "from keras.layers import Input, Dense\nfrom keras.models import Model\nencoding_dim = 32  \ninput_img = Input(shape=(784,))\nencoded = Dense(encoding_dim, activation='relu')(input_img)\n", "intent": "See [keras autoencoder tutorial](https://blog.keras.io/building-autoencoders-in-keras.html)\n"}
{"snippet": "x = torch.Tensor(5, 3)\nprint(x)\n", "intent": "Construct a 5x3 matrix, unitialized:\n"}
{"snippet": "outputs = net(Variable(images))\n", "intent": "Okay, now let's see what the neural network thinks these examples above are:\n"}
{"snippet": "correct = 0\ntotal = 0\nfor data in testloader:\n    images, labels = data\n    outputs = net(Variable(images))\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\nprint(f'Accuracy of network on 10,000 test images: {np.round(100*correct/total,2)}%')\n", "intent": "Seems pretty good.\nNow to look at how the network performs on the whole dataset.\n"}
{"snippet": "inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n", "intent": "Remember that you'll have to send the inputs and targets at every step to the GPU too:\n"}
{"snippet": "for data in rand_loader:\n    if torch.cuda.is_available():\n        input_var = Variable(data.cuda())\n    else:\n        input_var = Variable(data)\n    output = model(input_var)\n    print(\"Outside: input size\", input_var.size(), \n          \"output_size\", output.size())\n", "intent": "Now we can see the sizes of input and output tensors.\n"}
{"snippet": "log_softmax(xb@weights+bias)[:2]\n", "intent": "The minibatch activations after the Log Softmax and before heading into Negative Log Likelihood:\n"}
{"snippet": "nll(log_softmax(xb@weights+bias), yb)\n", "intent": "The loss value computed via NLL on the Log Softmax activations:\n"}
{"snippet": "model = XGBRegressor()\nmodel.fit(train_x, train_data['cnt'])\n", "intent": "Keep in mind that you still need to `fit` the model. `cross_val_score` only gets you the cross validation score, nothing more.\n"}
{"snippet": "learn.fit(0.15, 50) \n", "intent": "Looks like [the Keras example](https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py\n"}
{"snippet": "temp = Variable(torch.FloatTensor([1,2]))\ntemp.cpu()\n", "intent": "```\nVariable.cpu(self)\nSource:   \n    def cpu(self):\n        return self.type(getattr(torch, type(self.data).__name__))\n```\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "intent": "Load a pretrained model and reset final fully-connected layer\n"}
{"snippet": "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)\n", "intent": "Should take 15-25 min on CPU; < 1 min on GPU.\n"}
{"snippet": "model(x)[0].shape, x.shape\n", "intent": "Is the problem that Y is not flattened when compared to X in BCE?\n"}
{"snippet": "from sklearn.svm import SVC\nSVC().fit(X, y).score(X, y)\n", "intent": "To illustrate this, let's train a Support Vector Machine naively on the digits dataset:\n"}
{"snippet": "svc = SVC(kernel='rbf').fit(X_train, y_train)\ntrain_score = svc.score(X_train, y_train) \ntrain_score\n", "intent": "Let's retrain a new model on the first subset call the **training set**:\n"}
{"snippet": "svc_2 = SVC(kernel='rbf', C=100, gamma=0.001).fit(X_train, y_train)\nsvc_2\n", "intent": "Let's try again with another parameterization:\n"}
{"snippet": "svc = SVC(C=1, gamma=0.0005)\nfor i, train_size in enumerate(train_sizes):\n    cv = ShuffleSplit(n_samples, n_iter=n_iter, train_size=train_size)\n    for j, (train, test) in enumerate(cv):\n        svc.fit(X[train], y[train])\n        train_scores[i, j] = svc.score(X[train], y[train])\n        test_scores[i, j] = svc.score(X[test], y[test])\n", "intent": "We can now loop over training set sizes and CV iterations:\n"}
{"snippet": "model = Sequential()\nmodel.add(Embedding(len(word2num), 50)) \nmodel.add(LSTM(64))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nmodel.summary()\n", "intent": "This method is no different to the method utilised in the sentiment analysis lesson.\n"}
{"snippet": "MultinomialNB()\n", "intent": "The `MultinomialNB` class is a good baseline classifier for text as it's fast and has few parameters to tweak:\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nparameters = {\n    'vec__max_df': [0.8, 1.0],\n    'vec__ngram_range': [(1, 1), (1, 2)],\n    'vec__use_idf': [True, False],\n}\ngs = GridSearchCV(pipeline, parameters, verbose=2, refit=False)\n_ = gs.fit(twenty_train_small.data, twenty_train_small.target)\n", "intent": "For the grid search, the parameters names are prefixed with the name of the pipeline step using \"__\" as a separator:\n"}
{"snippet": "_ = pipeline.fit(twenty_train_small.data, twenty_train_small.target)\n", "intent": "Let's fit a model on the small dataset and collect info on the fitted components:\n"}
{"snippet": "from copy import copy\ndef average_linear_model(models):\n", "intent": "We can now compute the average linear model:\n"}
{"snippet": "reg_trees = ExtraTreesClassifier(n_estimators=50, max_depth=7, min_samples_split=10)\nreg_trees.fit(X_small_train, y_small_train)\nprint(\"Train score: %0.3f\" % reg_trees.score(X_small_train, y_small_train))\nprint(\"Test score: %0.3f\" % reg_trees.score(X_test, y_test))\n", "intent": "More interesting, an ensemble model composed of regularized trees is not underfitting much less than the individual regularized trees:\n"}
{"snippet": "clf_gini = DecisionTreeClassifier(criterion = \"gini\", \n                                  random_state = 100,\n                                  max_depth=10, \n                                  min_samples_leaf=50,\n                                  min_samples_split=50)\nclf_gini.fit(X_train, y_train)\n", "intent": "**Running the model with best parameters obtained from grid search.**\n"}
{"snippet": "clf_gini = DecisionTreeClassifier(criterion = \"gini\", \n                                  random_state = 100,\n                                  max_depth=3, \n                                  min_samples_leaf=50,\n                                  min_samples_split=50)\nclf_gini.fit(X_train, y_train)\nprint(clf_gini.score(X_test,y_test))\n", "intent": "You can see that this tree is too complex to understand. Let's try reducing the max_depth and see how the tree looks.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\n", "intent": "Let's first fit a random forest model with default hyperparameters.\n"}
{"snippet": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nn_folds = 5\nparameters = {'max_depth': range(2, 20, 5)}\nrf = RandomForestClassifier()\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)\n", "intent": "Let's try to find the optimum values for ```max_depth``` and understand how the value of max_depth impacts the overall accuracy of the ensemble.\n"}
{"snippet": "batch_size = 128\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=15, validation_data=(X_test, y_test))\n", "intent": "I may heave cheated and run the following block 3 times. Good thing about Keras is that it remembers the last learning rate and goes from there.\n"}
{"snippet": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nn_folds = 5\nparameters = {'min_samples_leaf': range(100, 400, 50)}\nrf = RandomForestClassifier()\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)\n", "intent": "Let's now check the optimum value for min samples leaf in our case.\n"}
{"snippet": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nn_folds = 5\nparameters = {'min_samples_split': range(200, 500, 50)}\nrf = RandomForestClassifier()\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)\n", "intent": "Let's now look at the performance of the ensemble as we vary min_samples_split.\n"}
{"snippet": "param_grid = {\n    'max_depth': [4,8,10],\n    'min_samples_leaf': range(100, 400, 200),\n    'min_samples_split': range(200, 500, 200),\n    'n_estimators': [100,200, 300], \n    'max_features': [5, 10]\n}\nrf = RandomForestClassifier()\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1,verbose = 1)\n", "intent": "We can now find the optimal hyperparameters using GridSearchCV.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=10,\n                             min_samples_leaf=100, \n                             min_samples_split=200,\n                             max_features=10,\n                             n_estimators=100)\n", "intent": "**Fitting the final model with the best parameters obtained from grid search.**\n"}
{"snippet": "logm2 = sm.GLM(y_train,(sm.add_constant(X_train2)), family = sm.families.Binomial())\nlogm2.fit().summary()\n", "intent": "Now let's run our model again after dropping highly correlated variables\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\n", "intent": " - We expect this to work very well, giving high performance in accuracy\n"}
{"snippet": "crf = sklearn_crfsuite.CRF(\n    algorithm='lbfgs',\n    c1=0.01,\n    c2=0.1,\n    max_iterations=100,\n    all_possible_transitions=True\n)\ncrf.fit(X_train, y_train)\n", "intent": "Let's now fit a CRF with arbitrary hyperparameters. \n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 2))\nnet.add(Linear(2, 4))\nnet.add(LeakyReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\nprint(net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\nprint(net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "model = Sequential()\nmodel.summary()\n", "intent": "This method is no different to the method utilised in the sentiment analysis lesson.\n"}
{"snippet": "x_cnn_sym = T.matrix()\nx_sentence_sym = T.imatrix()\nmask_sym = T.imatrix()\ny_sentence_sym = T.imatrix()\n", "intent": "Define symbolic variables for the various inputs\n"}
{"snippet": "trainingStartTime = time.time()\nlogisticRegressor = linear_model.LogisticRegression(C=0.1, solver='sag', \n                                                    class_weight={1: 0.46, 0: 1.32})\nlogisticRegressor.fit(X, y)\ntrainingDurationInMinutes = (time.time()-trainingStartTime)/60.0\nprint('full training took %.2f minutes' % (trainingDurationInMinutes))\n", "intent": "Train on the full training data\n-------------------------------\n"}
{"snippet": "from os import system\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\ndf['recipe'] = df['title'].map(lambda t: 1 if 'recipe' in unicode(t).lower() else 0)\nX = df[['image_ratio', 'html_ratio', 'recipe', 'label']].dropna()\ny = X['label']\nX.drop('label', axis=1, inplace=True)\nmodel = DecisionTreeClassifier()\nmodel.fit(X, y)\n", "intent": "Let's build a decision tree model to predict the \"evergreen-ness\" of a given website.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nparameters = {\n    'vec__max_df': [0.8, 1.0],\n    'vec__ngrams_range': [(1, 1), (1, 2)],\n    'vec__use_idf': [True, False],\n}\ngs = GridSearchCV(pipeline, parameters, verbose=2, refit=False)\n_ = gs.fit(twenty_train_small.data, twenty_train_small.target)\n", "intent": "For the grid search, the parameters names are prefixed with the name of the pipeline step using \"__\" as a separator:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.01))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "import numpy as np\nwires_cartesian = np.vstack((wires['wire_rho'] * np.cos(wires['wire_phi']),\n                                  wires['wire_rho'] * np.sin(wires['wire_phi']))).T\nfrom sklearn.neighbors import KDTree\nneighbors_tree = KDTree(wires_cartesian)\n", "intent": "Let's write a routine for finding the closest neighbours of a wire.\n"}
{"snippet": "draw_tree(DecisionTreeClassifier(max_depth=2).fit(iris.data, iris.target))\n", "intent": "Decision tree has paramenters (see lecture).\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\ngrid_search_cv = GridSearchCV(RandomForestClassifier(random_state=42), {\n        'criterion': ('gini', 'entropy'),\n        'max_depth': (1, 5, None),\n        'min_samples_split': (2, 3, 4, 5, 10),\n        'n_estimators': (1, 10, 50, 100)}, scoring='roc_auc', n_jobs=2, cv=4)\ngrid_search_cv.fit(line_features, line_labels)\n", "intent": "Tries all parameters combinations. Is expensive.\n"}
{"snippet": "batch_size = 128\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n", "intent": "I may heave cheated and run the following block 3 times. Good thing about Keras is that it remembers the last learning rate and goes from there.\n"}
{"snippet": "def generate_relu(alpha):\n    def relu(x):\n        return T.switch(x > 0, x, alpha * x)\n    return relu\n", "intent": "A smooth approximation to the rectifier is the analytic function\n$f(x) = \\ln(1 + e^x)$\nwhich is called the softplus function.\n"}
{"snippet": "class MyNeuralNetwork(AbstractNeuralNetworkClassifier):\n    def prepare(self):\n        ...\n        def activation(input):\n            ...\n        return activation\n", "intent": "Here is you daon't need to add `b` parameter, this interface does it and includes additional column in `X`\n"}
{"snippet": "x = T.vector() \ny = T.vector()\nalpha = T.scalar()\nbeta = T.scalar()\ncompiled_expr = theano.function([x, y, alpha, beta], \n                        (T.sum(x * (alpha * y)) + T.sum(y * (beta * x))) ** 2)\n", "intent": "* compute $(<x, \\alpha y> + <\\beta x, y>)^2$\n"}
{"snippet": "p_sig = T.nnet.sigmoid(X_.dot(w))\np_bck = 1 - p_sig\n", "intent": "$p_i = sigmoid(\\sum_j X_{ij}w_j)$\n$loss=\\sum y_i \\log{p} + (1-y_i)\\log{(1 - p)}$\n$-loss \\to min$\n"}
{"snippet": "Y_train_binary = (Y_train > 0)\nY_test_binary = (Y_test > 0)\nbinary_tree = DecisionTreeClassifier().fit(X_train, Y_train_binary)\n", "intent": "Too many of them, select according to your task. For binary classification the most common is ROC AUC.\n"}
{"snippet": "plot_decision_surface(BaggingClassifier(\n    base_estimator=LogisticRegression(C=1e3), n_jobs=-1).fit(X_train, Y_train), X_test, Y_test)\n", "intent": "Why does BaggingClassifier not improve the linear regression quality?\n"}
{"snippet": "plot_embedding(Xrp,y)\n", "intent": "Not super helpful to say the least.\n"}
{"snippet": "model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs=5)\n", "intent": "Keras models follow __Scikit-learn__'s interface of fit/predict with some notable extensions. Let's take a tour.\n"}
{"snippet": "model.save_model('catboost_model.dump')\n", "intent": "4) And its always really handy to be able to dump your model to disk (especially if training took some time)\n"}
{"snippet": "show_graph(tf.get_default_graph().as_graph_def())\n", "intent": "Train the graph above:\n"}
{"snippet": "x_t = tf.placeholder('int32',(None,))\nh_t = tf.Variable(np.zeros([1,rnn_num_units],'float32'))\nnext_probs,next_h = rnn_one_step(x_t,h_t)\n", "intent": "Once we've trained our network a bit, let's get to actually generating stuff. All we need is the `rnn_one_step` function you have written above.\n"}
{"snippet": "from sklearn import mixture\ngmm = mixture.GaussianMixture (n_components=10, covariance_type='full')\ngmm.fit(X)\n", "intent": "We try to solve the same problem with another module: [scikit-learn](http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm.html\n"}
{"snippet": "from statsmodels.api import OLS\nimport numpy\ny = data.macron\nX = numpy.ones((len(y), 2))\nX[:,1] = numpy.arange(0,len(y))\nreg = OLS(y,X)\nresults = reg.fit()\nresults.params\n", "intent": "La fonction [detrend](http://statsmodels.sourceforge.net/stable/generated/statsmodels.tsa.tsatools.detrend.html\n"}
{"snippet": "model = LinearRegression(normalize=True)\nprint(model.normalize)\n", "intent": "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated:\n"}
{"snippet": "from sklearn import svm\nclf = svm.SVC(C=5., gamma=0.001)\nclf.fit(X_train_pca, y_train)\n", "intent": "Now we'll perform support-vector-machine classification on this reduced dataset:\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nconfusion(DecisionTreeClassifier(), X_train, X_test, y_train, y_test)\n", "intent": "Quelques exemples pour tester, quelques exemples pour apprendre. C'est peu.\n"}
{"snippet": "model = uncertain_gallup_model(gallup_2012)\nmodel = model.join(electoral_votes)\nmodel.head()\n", "intent": "We construct the model by estimating the probabilities:\n"}
{"snippet": "images, labels = next(iter(trainloader))\nimg = images[13].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "naive_bayes.fit(training_data, y_train)\nbag_mod.fit(training_data, y_train)\nrf_mod.fit(training_data, y_train)\nada_mod.fit(training_data, y_train)\nsvm_mod.fit(training_data, y_train)\n", "intent": "> **Step 1**: Now, fit each of the above models to the appropriate data.  Answer the following question to assure that you fit the models correctly.\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Step 2:** \"Instantiate\" the \"estimator\"\n- \"Estimator\" is scikit-learn's term for model\n- \"Instantiate\" means \"make an instance of\"\n"}
{"snippet": "tree_mod.fit(X_train, y_train)\nrf_mod.fit(X_train, y_train)\nada_mod.fit(X_train, y_train)\nreg_mod.fit(X_train, y_train)\n", "intent": "> **Step 4:** Fit each of your instantiated models on the training data.\n"}
{"snippet": "train_features = train_data[:, 1:]\ntrain_target = train_data[:, 0]\nclf = clf.fit(train_features, train_target)\nscore = clf.score(train_features, train_target)\nprint (\"Mean accuracy of Random Forest = %.2f %%\" % (score*100))\n", "intent": "Fit the training data and create the decision trees:\n"}
{"snippet": "from sklearn import svm\nX = np.array(channel[['x','y']])\ny = np.array(channel[\"Channel\"])\nC = 1.0\nlin_svc = svm.SVC(kernel='linear', C=C).fit(X,y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\npoly9_svc = svm.SVC(kernel='poly', degree=9, C=C).fit(X, y)\n", "intent": "Use Support Vector Machine (SVM), a frontier which best segregates the two classes.\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm_inertia = []\nn = int(input(\"Enter Starting Cluster: \"))\nn1 = int(input(\"Enter Ending Cluster: \"))\nfor i in range(n,n1):\n    km = KMeans(n_clusters=i)\n    km.fit(matrix[matrix.columns[2:]])\n    print (i, km.inertia_)\n    km_inertia.append(km.inertia_)\n", "intent": "**Find optimal K-value**\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=12, init='k-means++', n_init=10)\n", "intent": "Fit the model with the settled number of clusters. \n"}
{"snippet": "model = LogisticRegression()\nmodel = model.fit(X, Y)\nmodel.score(X, Y)\n", "intent": "Let's go ahead and run logistic regression on the entire data set, and see how accurate it is!\n"}
{"snippet": "model_details = model.fit(images_train, class_train,\n                    batch_size = 128,\n                    epochs = NUM_EPOCH, \n                    validation_data= (images_test, class_test),\n                    callbacks=[checkpoint],\n                    verbose=1)\n", "intent": "Fit the model on the data provided\n"}
{"snippet": "from sklearn.linear_model import SGDClassifier\nmy_sgd_model = SGDClassifier(n_iter=100, alpha=0.01)\n", "intent": "- Stochastic Gradient Descent  \nhttp://scikit-learn.org/stable/modules/sgd.html\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal([vocab_size, embed_dim], stddev=0.1), \n                            dtype=tf.float32, name=\"embedding\")\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "knn.fit(X, y)\n", "intent": "**Step 3:** Fit the model with data (aka \"model training\")\n- Model is learning the relationship between X and y\n- Occurs in-place\n"}
{"snippet": "formula = \"HHDelivPurDiff ~ PurRatecardHHImps + Operator\"\nmodel = sm.ols(formula, data=matchedData).fit()\nprint model.summary()\n", "intent": "Linear Regression\n=================\nTry and explain the difference in delivered impressions and purchased impressions through linear regression\n"}
{"snippet": "from sklearn import tree\nfrom sklearn.externals.six import StringIO\ndt = tree.DecisionTreeClassifier()\ndt = dt.fit(X, matchedData.HHBinaryDeliv)\nwith open(\"decision tree.dot\", 'w') as f:\n    f = tree.export_graphviz(dt, out_file=f)\n", "intent": "Decision Tree Output\n====================\n"}
{"snippet": "model = sm.ols(formula = ' sl ~ yd + yr + sx +rk', data=data).fit()\n", "intent": "___\nsex _beta_ changed likely because it is correlated to the other variables\n"}
{"snippet": "model = sm.ols(formula = ' sl ~ yd + yr + sx + rk + sx*rk', data=data).fit()\n", "intent": "___\n**Intercept**: reflects 0 year, female, assistant, 0 years from degree\n"}
{"snippet": "model = LinearRegression()\nX = pd.get_dummies(data.rk)\nmodel.fit(X, data.sl)\n", "intent": "___\nshould drop one of the dummy columns, otherwise will get a nonsense intercept (can't turn all of them off)\n"}
{"snippet": "ridge = Ridge(normalize=True)  \ncoefs = []\nfor a in alphas:\n    ridge.set_params(alpha=a)\n    ridge.fit(X, y)\n    coefs.append(ridge.coef_)\nnp.shape(coefs)\ncoefs[0]\n", "intent": "- Try different values of alpha to compare the affect on the weights\n"}
{"snippet": "ridge = Ridge(normalize=True)  \ncoefs = []\nfor a in alphas:\n    ridge.set_params(alpha=a)\n    ridge.fit(X, y)\n    coefs.append(ridge.coef_)\nnp.shape(coefs)\n", "intent": "- Try different values of alpha to compare the affect on the weights\n"}
{"snippet": "import statsmodels.api as sm\nX = np.array(xs).transpose()\nX = sm.add_constant(X)\nmod = sm.OLS(ys, X)\nres = mod.fit()\nres.summary()\n", "intent": "This generated a fit of $y = 3 x + 1$. Let's see what a linear regression yields.\n"}
{"snippet": "lm = linear_model.LinearRegression()\nX = np.vander(xs, 4)\ny = ys\nprint X, y\n", "intent": "Now we fit a model to the data. If we try to fit a size degree polynomial to the data we should obtain a very overfitted model.\n"}
{"snippet": "grid.fit(X, y)\n", "intent": "- You can set **`n_jobs = -1`** to run computations in parallel (if supported by your computer and OS)\n"}
{"snippet": "from sklearn.cluster import KMeans\nkmeans = KMeans(2)\nkmeans\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "with pm.Model() as logistic_model:\n    pm.glm.glm('bush ~ female + black', df, family=pm.glm.families.Binomial())\n    trace_logistic_model = pm.sample(2000, pm.NUTS(), progressbar=True)\n", "intent": "We'll start by doing a simple model of female and black voters on Bush voteshare:\n"}
{"snippet": "bnb = nb.BernoulliNB()\ngnb = nb.GaussianNB()\nmnb = nb.MultinomialNB()\n", "intent": "Instantiate the Naive Bayes predictor from scikit-learn with the training data. \n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogit = LogisticRegression()\n", "intent": "Define a logistic regression and train it with the feature and target set\n"}
{"snippet": "fit_eval_model(logit)\n", "intent": "Produce the accuracy score of the logistic regression from the test set\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim]))\n    embed = tf.nn.embedding_lookup(embedding,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "x = torch.arange(10, 50, dtype=torch.float).view(4, 10)\nxlen = torch.LongTensor([4, 8, 1, 10])\nm = torch.arange(x.size(1)).unsqueeze(0).expand(x.size())\nmask = xlen.unsqueeze(1).expand(x.size()) <= m\nx[mask] = float('-inf')\nx\n", "intent": "Set unmasked elements to zero.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)\n", "intent": "Now its time to train our model!\n**Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.**\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\ngrid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "rand = RandomizedSearchCV(knn, param_dist, cv=10, scoring='accuracy', n_iter=10, random_state=5)\nrand.fit(X, y)\nrand.grid_scores_\n", "intent": "- **Important:** Specify a continuous distribution (rather than a list of values) for any continous parameters\n"}
{"snippet": "writer = tf.summary.FileWriter(LOGDIR)\nwriter.add_graph(sess.graph)\ntf.summary.histogram('m', m)\ntf.summary.histogram('b', b)\ntf.summary.scalar('loss', loss)\nsummary_op = tf.summary.merge_all()\n", "intent": "Later, we'll get this for free.\n"}
{"snippet": "tf.reset_default_graph()\nsess = tf.Session()\n", "intent": "We will now use a linear model to classify handwritten digits from the MNIST dataset.\n"}
{"snippet": "x, y = get_input_fn(TRAIN_INPUT, 1)()\nwith tf.Session() as s:\n    print(s.run(x))\n    print(s.run(y))\n", "intent": "Testing the input function\n"}
{"snippet": "tf.reset_default_graph()\nhidden1_units = 128\nlearning_rate = 0.005\ninput_dimension = train_data.shape[1]   \noutput_dimension = train_labels.shape[1]  \noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\nimages_column = tf.contrib.layers.real_valued_column(\"images\")\ncanned_model = _\n", "intent": "Instead of defining our own DNN classifier, TensorFlow supplies a number of *canned* estimators that can save a lot of work.\n"}
{"snippet": "logreg = linear_model.LogisticRegression(solver='newton-cg')\nX = cr['Hours Researched'].values.reshape(-1,1)\nY = cr['Hired']\nlogreg.fit(X,Y)\n", "intent": "We call our instance `logreg`, and then feed it our data:\n"}
{"snippet": "lgrg = linear_model.LogisticRegression(solver='newton-cg')\nX = loans[['Credit Score','Loan Request Amount']]\nY = loans['Approval']\nlgrg.fit(X,Y)\n", "intent": "We feed the data to the `linear_model.LogisticRegression()` the same as above, but now our `X` consists of multiple columns.\n"}
{"snippet": "loans_model(650,20)\n", "intent": "The prediction is that the loan will be approved, and we can determine the probability of approval with:\n"}
{"snippet": "ftr = tree.DecisionTreeClassifier()\nftr.fit(fr, fruits['Category'])\n", "intent": "Again, we make an instance of the `DecisionTreeClassifier()` class:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(1000,)))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "import tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n", "intent": "First we start with loading TensorFlow and reseting the computational graph.\n"}
{"snippet": "import xgboost\ngbtree = xgboost.XGBClassifier(n_estimators=300, max_depth=5)\ngbtree.fit(encoded_train, labels_train)\n", "intent": "This time, we use gradient boosted trees as the model, using the [xgboost](https://github.com/dmlc/xgboost) package.\n"}
{"snippet": "import statsmodels.api as sm\nX = np.array(xs).transpose()\nX = sm.add_constant(X)\nmod = sm.OLS(ys, X)\nres = mod.fit()\nprint res.summary()\n", "intent": "This generated a fit of $y = 4 x + 1$. Let's see what a linear regression yields.\n"}
{"snippet": "neigh = KNeighborsClassifier(n_neighbors=3)\n", "intent": "Tell Phyton that we will use kNN Classifier with a k value of 3\n"}
{"snippet": "neigh.fit(X, y) \n", "intent": "Fit the kNN Classifier to the X & y data\n"}
{"snippet": "km = KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "km.fit(df.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "scaler.fit(df.iloc[:,:-1])\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "KNN = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "KNN.fit(X_train,y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Get graph handle with the tf.Session()\n"}
{"snippet": "nb = MultinomialNB()\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "grid=GridSearchCV(SVC(),param_grid,verbose=1)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=1)\nlr.fit(features_train, target_train)\n", "intent": "Let's start with a simple model from sklearn, namely `LogisticRegression`:\n"}
{"snippet": "new_docs = ['He watches basketball and baseball', 'Julie likes to play basketball', 'Jane loves to play baseball']\nnew_term_freq_matrix = tfidf_vectorizer.transform(new_docs)\nprint tfidf_vectorizer.vocabulary_\nprint new_term_freq_matrix.todense()\n", "intent": "That was easy! Next, we can fit new observations into this vocabulary space like so:\n"}
{"snippet": "X = torch.IntTensor(3, 2).fill_(0)\nprint(\"X=>\", X.type())\nY = X.float()\nprint(\"Y=>\", Y.type())\n", "intent": "Q3. Convert the data type of X to float32.\n"}
{"snippet": "X = torch.LongTensor(3, 2).fill_(0)\nprint(\"X=>\", X.type())\nY = X.int()\nprint(\"Y=>\", Y.type())\n", "intent": "Q4. Convert the data type of X to int32.\n"}
{"snippet": "X = torch.arange(1, 22).resize_(3, 7)\nprint(X)\ny = torch.LongTensor([0, 2, 4, 5])\nprint(X.index_select(1, y))\n", "intent": "Q12. Return the columns of indices y.\n"}
{"snippet": "x = torch.Tensor([1, 4])\ny = torch.Tensor([2, 5])\nz = torch.Tensor([3, 6])\nO = torch.stack((x, y, z), 0)\nprint(O)\nassert np.array_equal(O.numpy(), np.stack([x.numpy(), y.numpy(), z.numpy()], 0))\n", "intent": "Q17. Stack x, y, and z vertically.\n"}
{"snippet": "X = torch.Tensor([[1, 2, 3], [4, 5, 6]])\nY = torch.Tensor([[7, 8, 9], [10, 11, 12]])\nZ = torch.cat((X, Y), 0)\nprint(Z)\nassert np.array_equal(Z.numpy(), np.concatenate((X.numpy(), Y.numpy()), 0))\n", "intent": "Q19. Concatenate X and Y along the first dimension.\n"}
{"snippet": "zero_var = tf.Variable(tf.zeros([row_dim, col_dim]))\nones_var = tf.Variable(tf.ones([row_dim, col_dim]))\n", "intent": "Here are variables initialized to contain all zeros or ones.\n"}
{"snippet": "X = torch.Tensor([[0,1,7,0,0],[3,0,0,2,19]])\ny = X.nonzero()\nprint(y)\nassert np.count_nonzero(X.numpy()) == y.size(0)\n", "intent": "Q23. Get the indices of all nonzero elements in X.\n"}
{"snippet": "means = torch.Tensor([1, 2, 3])\nstd = torch.Tensor([0.1, 0., -0.1])\nX = torch.normal(means=means, std=std)\nprint(X)\n", "intent": "Q3. Create a tensor of random numbers drawn from the given mean and standard deviation.\n"}
{"snippet": "means = torch.Tensor([1, 2, 3])\nstd = torch.Tensor([0.1, 0., -0.1])\nX = ...\nprint(X)\n", "intent": "Q3. Create a tensor of random numbers drawn from the given mean and standard deviation.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(X.sum(dim=0, keepdim=True))\n", "intent": "Q5. Sum the elements of X along the first dimension, retaining it.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(X.prod())\n", "intent": "Q6. Return the product of all elements in X.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(X.cumsum(dim=1))\n", "intent": "Q7. Return the cumulative sum of all elements along the second axis in X.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(X.cumprod(dim=1))\n", "intent": "Q8. Return the cumulative product of all elements along the second axis in X.\n"}
{"snippet": "x = torch.Tensor([1., 2., 3.])\ny = x.exp()\nprint(y)\nassert np.array_equal(y.numpy(), np.exp(x.numpy()))\n", "intent": "Q9. Compute $e^x$, element-wise.\n"}
{"snippet": "x = torch.Tensor([1, np.e, np.e**2])\ny = x.log()\nprint(y)\nassert np.allclose(y.numpy(), np.log(x.numpy()))\n", "intent": "Q10. Compute logarithms of x element-wise.\n"}
{"snippet": "zero_similar = tf.Variable(tf.zeros_like(zero_var))\nones_similar = tf.Variable(tf.ones_like(ones_var))\nsess.run(ones_similar.initializer)\nsess.run(zero_similar.initializer)\nprint(sess.run(ones_similar))\nprint(sess.run(zero_similar))\n", "intent": "If the shape of a tensor depends on the shape of another tensor, then we can use the TensorFlow built-in functions `ones_like()` or `zeros_like()`.\n"}
{"snippet": "x = torch.Tensor([1, 2, 3])\ny = torch.Tensor([-1, -2, -3])\nz = y.sub(x)\nprint(z)\nassert np.array_equal(z.numpy(), np.subtract(y.numpy(), x.numpy()))\n", "intent": "Q12. Subtract y from x element-wise.\n"}
{"snippet": "x = torch.Tensor([3, 4, 5])\ny = torch.Tensor([1, 0, -1])\nx_y = x.mul(y)\nprint(x_y)\nassert np.array_equal(x_y.numpy(), np.multiply(x.numpy(), y.numpy()))\n", "intent": "Q13. Multiply x by y element-wise.\n"}
{"snippet": "x = torch.FloatTensor([3., 4., 5.])\ny = torch.FloatTensor([1., 2., 3.])\nz = x.div(y)\nprint(z)\nassert np.allclose(z.numpy(), np.true_divide(x.numpy(), y.numpy()))\n", "intent": "Q14. Divide x by y element-wise.\n"}
{"snippet": "x = torch.Tensor([1, -1])\nnegx = x.neg()\nprint(negx)\nassert np.array_equal(negx.numpy(), np.negative(x.numpy()))\n", "intent": "Q15. Compute numerical negative value of x, element-wise.\n"}
{"snippet": "x = torch.Tensor([1., 2., .2])\ny = x.reciprocal()\nprint(y)\nassert np.array_equal(y.numpy(), np.reciprocal(x.numpy()))\n", "intent": "Q16. Compute the reciprocal of x, element-wise.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = torch.Tensor([[1, 2], [1, 2]])\nX_Y = X.pow(Y)\nprint(X_Y)\nassert np.array_equal(X_Y.numpy(), np.power(X.numpy(), Y.numpy()))\n", "intent": "Q17. Compute $X^Y$, element-wise.\n"}
{"snippet": "x = torch.Tensor([-3, -2, -1, 1, 2, 3])\ny = 2.\nz = x.remainder(y)\nprint(z)\nassert np.array_equal(z.numpy(), np.remainder(x.numpy(), y))\n", "intent": "Q18. Compute the remainder of x / y element-wise.\n"}
{"snippet": "x = torch.Tensor([1., 2.5, -3.2])\ny = x.frac()\nprint(y)\nassert np.allclose(y.numpy(), np.modf(x.numpy())[0])\n", "intent": "Q19. Compute the fractional portion of each element of x.\n"}
{"snippet": "X = torch.Tensor( [[-1, -2, -3], [0, 1, 2]] )\nY = X.eq(0)\nprint(Y)\nassert np.allclose(Y.numpy(), np.equal(X.numpy(), 0))\n", "intent": "Q21. Return 1 if an element of X is 0, otherwise 0.\n"}
{"snippet": "fill_var = tf.Variable(tf.fill([row_dim, col_dim], -1))\nsess.run(fill_var.initializer)\nprint(sess.run(fill_var))\n", "intent": "Here is how we fill a tensor with a constant.\n"}
{"snippet": "x = torch.Tensor([1., 4., 9.])\ny = x.sqrt()\nprint(y)\nassert np.array_equal(y.numpy(), np.sqrt(x.numpy()))\n", "intent": "Q27. Compute square root of x element-wise.\n"}
{"snippet": "x = torch.Tensor([1., 4., 9.])\ny = x.rsqrt()\nprint(y)\nassert np.allclose(y.numpy(), np.reciprocal(np.sqrt(x.numpy())))\n", "intent": "Q28. Compute the reciprocal of the square root of x, element-wise.\n"}
{"snippet": "X = torch.Tensor([[1, -1], [3, -3]])\nY = X.abs()\nprint(Y)\nassert np.array_equal(Y.numpy(), np.abs(X.numpy()))\n", "intent": "Q29. Compute the absolute value of X.\n"}
{"snippet": "x = torch.Tensor([1, 3, 0, -1, -3])\ny = x.sign()\nprint(y)\nassert np.array_equal(y.numpy(), np.sign(x.numpy()))\n", "intent": "Q30. Compute an element-wise indication of the sign of x, element-wise.\n"}
{"snippet": "x = torch.FloatTensor([1.2, 0.7, -1.3, 0.1])\ny = x.sigmoid()\nprint(y)\nassert np.allclose(y.numpy(), 1. / (1 + np.exp(-x.numpy())))\n", "intent": "Q31. Compute the sigmoid of x, elemet-wise.\n"}
{"snippet": "X = torch.Tensor([1,2,3,4])\nY = torch.Tensor([10,10,10,10])\nZ = X.lerp(Y, .9)\nprint(Z)\nassert np.allclose(Z.numpy(), X.numpy() + (Y.numpy()-X.numpy())*.9)\n", "intent": "Q32. Interpolate X and Y linearly with a weight of .9 on Y.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(...)\n", "intent": "Q5. Sum the elements of X along the first dimension, retaining it.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(...)\n", "intent": "Q6. Return the product of all elements in X.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(...)\n", "intent": "Q7. Return the cumulative sum of all elements along the second axis in X.\n"}
{"snippet": "const_var = tf.Variable(tf.constant([8, 6, 7, 5, 3, 0, 9]))\nconst_fill_var = tf.Variable(tf.constant(-1, shape=[row_dim, col_dim]))\nsess.run(const_var.initializer)\nsess.run(const_fill_var.initializer)\nprint(sess.run(const_var))\nprint(sess.run(const_fill_var))\n", "intent": "We can also create a variable from an array or list of constants.\n"}
{"snippet": "x = torch.Tensor([1., 2., 3.])\ny = ...\nprint(y)\n", "intent": "Q9. Compute $e^x$, element-wise.\n"}
{"snippet": "x = torch.Tensor([1, np.e, np.e**2])\ny = ...\nprint(y)\n", "intent": "Q10. Compute logarithms of x element-wise.\n"}
{"snippet": "x = torch.Tensor([1, 2, 3])\ny = torch.Tensor([-1, -2, -3])\nz = ...\nprint(z)\n", "intent": "Q11. Add x and y element-wise.\n"}
{"snippet": "x = torch.Tensor([1, 2, 3])\ny = torch.Tensor([-1, -2, -3])\nz = ...\nprint(z)\n", "intent": "Q12. Subtract y from x element-wise.\n"}
{"snippet": "x = torch.Tensor([3, 4, 5])\ny = torch.Tensor([1, 0, -1])\nx_y = ...\nprint(x_y)\n", "intent": "Q13. Multiply x by y element-wise.\n"}
{"snippet": "x = torch.FloatTensor([3., 4., 5.])\ny = torch.FloatTensor([1., 2., 3.])\nz = ...\nprint(z)\n", "intent": "Q14. Divide x by y element-wise.\n"}
{"snippet": "x = torch.Tensor([1, -1])\nnegx = ...\nprint(negx)\n", "intent": "Q15. Compute numerical negative value of x, element-wise.\n"}
{"snippet": "x = torch.Tensor([1., 2., .2])\ny = ...\nprint(y)\n", "intent": "Q16. Compute the reciprocal of x, element-wise.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = torch.Tensor([[1, 2], [1, 2]])\nX_Y = ...\nprint(X_Y)\n", "intent": "Q17. Compute $X^Y$, element-wise.\n"}
{"snippet": "import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n", "intent": "We introduce how to use placeholders in TensorFlow.\nFirst we import the necessary libraries and reset the graph session.\n"}
{"snippet": "x = torch.Tensor([1., 2.5, -3.2])\ny = ...\nprint(y)\n", "intent": "Q19. Compute the fractional portion of each element of x.\n"}
{"snippet": "X = torch.Tensor( [[-1, -2, -3], [0, 1, 2]] )\nY = ...\nprint(Y)\n", "intent": "Q21. Return 1 if an element of X is 0, otherwise 0.\n"}
{"snippet": "X = torch.Tensor( [[-1, -2, -3], [0, 1, 2]] )\nY = ...\nprint(Y)\n", "intent": "Q22. Return 0 if an element of X is 0, otherwise 1.\n"}
{"snippet": "x = torch.Tensor([1., 4., 9.])\ny = ...\nprint(y)\n", "intent": "Q27. Compute square root of x element-wise.\n"}
{"snippet": "x = torch.Tensor([1., 4., 9.])\ny = ...\nprint(y)\n", "intent": "Q28. Compute the reciprocal of the square root of x, element-wise.\n"}
{"snippet": "X = torch.Tensor([[1, -1], [3, -3]])\nY = ...\nprint(Y)\n", "intent": "Q29. Compute the absolute value of X.\n"}
{"snippet": "x = torch.Tensor([1, 3, 0, -1, -3])\ny = ...\nprint(y)\n", "intent": "Q30. Compute an element-wise indication of the sign of x, element-wise.\n"}
{"snippet": "x = torch.FloatTensor([1.2, 0.7, -1.3, 0.1])\ny = ...\nprint(y)\n", "intent": "Q31. Compute the sigmoid of x, elemet-wise.\n"}
{"snippet": "X = torch.Tensor([1,2,3,4])\nY = torch.Tensor([10,10,10,10])\nZ = ...\nprint(Z)\n", "intent": "Q32. Interpolate X and Y linearly with a weight of .9 on Y.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Start graph session\n"}
{"snippet": "x = torch.Tensor([1, 2])\nY = torch.Tensor([[0, 0], [1, 1]])\nz = x.matmul(Y)\nprint(z)\nassert torch.equal(z, x.unsqueeze(0).mm(Y).squeeze()) is True\n", "intent": "Q2. Compute the product of vector x and matrix Y.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\ny = torch.Tensor([3, 4])\nz = X.matmul(y)\nprint(z)\nassert torch.equal(z, X.mv(y)) is True\n", "intent": "Q3. Compute a matrix-vector product of matrix X and vector y.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = torch.Tensor([[0, 0], [1, 1]])\nZ = X.matmul(Y)\nprint(Z)\nassert torch.equal(Z, X.mm(Y)) is True\n", "intent": "Q4. Compute a matrix multiplication of matrix X and Y.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\ny = torch.Tensor([3, 4])\nm = torch.ones(2)\nprint(m.addmv(X, y))\n", "intent": "Q7.Express the below computation as a single line.<br>\n`m + torch.mv(X, y)`\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = torch.Tensor([[0, 0], [1, 1]])\nM = torch.ones(2, 2)\nZ = M.addmm(X, Y)\nprint(Z)\n", "intent": "Q8.Express the below computation as a single line.<br/>\n`M + torch.mm(X, Y)`\n"}
{"snippet": "X = torch.Tensor([[2, 0, 0], [0, 3, 4], [0, 4, 9]])\ne, v = X.eig(eigenvectors=True)\nprint(\"eigen values=\", e)\nprint(\"eigen vectors=\", v)\n_e, _v = np.linalg.eig(X.numpy())\nassert np.allclose(e.numpy()[:, 0], _e)\n", "intent": "Q14. Compute the eigenvalues and right eigenvectors of X.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = X.inverse()\nprint(Y)\nassert np.allclose(Y.numpy(), np.linalg.inv(X.numpy()))\n", "intent": "Q17. Compute the inverse of X.\n"}
{"snippet": "x = torch.Tensor([1, 2])\ny = torch.Tensor([3, 4])\nz = ...\nprint(z)\n", "intent": "Q1. Compute the inner product of two vectors x and y.\n"}
{"snippet": "x = torch.Tensor([1, 2])\nY = torch.Tensor([[0, 0], [1, 1]])\nz = ...\nprint(z)\n", "intent": "Q2. Compute the product of vector x and matrix Y.\n"}
{"snippet": "import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n", "intent": "This function introduces various ways to create\nmatrices and how to use them in TensorFlow\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = torch.Tensor([[0, 0], [1, 1]])\nZ = ...\nprint(Z)\n", "intent": "Q4. Compute a matrix multiplication of matrix X and Y.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\ny = torch.Tensor([3, 4])\nm = torch.ones(2)\nZ = ...\nprint(Z)\n", "intent": "Q7.Express the below computation as a single line.<br>\n`m + torch.mv(X, y)`\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = torch.Tensor([[0, 0], [1, 1]])\nM = torch.ones(2, 2)\nZ = ...\nprint(Z)\n", "intent": "Q8.Express the below computation as a single line.<br/>\n`M + torch.mm(X, Y)`\n"}
{"snippet": "X = torch.Tensor([[2, 0, 0], [0, 3, 4], [0, 4, 9]])\ne, v = ...\nprint(\"eigen values=\", e)\nprint(\"eigen vectors=\", v)\n", "intent": "Q14. Compute the eigenvalues and right eigenvectors of X.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = ...\nprint(Y)\n", "intent": "Q17. Compute the inverse of X.\n"}
{"snippet": "X = Variable(torch.ones(3, 2))\nprint(X)\n", "intent": "Q0. Create a variable `X` of the size (3, 2), filled with 1's.\n"}
{"snippet": "X = Variable(torch.randn(3, 3))\nY = X.data\nprint(Y)\n", "intent": "Q1. Get the tensor of Variable X.\n"}
{"snippet": "X = Variable(torch.randn(3, 3))\nY = ...\nprint(Y)\n", "intent": "Q1. Get the tensor of Variable X.\n"}
{"snippet": "torch.manual_seed(3)\nx = Variable(Tensor([1, 2, 3]).unsqueeze(1))\nW = Variable(Tensor(3, 3))\nb = Variable(Tensor(3, 1))\ntorch.set_printoptions(precision=1)\nprint(x, W, b)\n", "intent": "The goal is to derive the gradients for W, x, and b for this function:\n$$ f(W, x, b) = \\sum\\sigma(Wx + b) $$\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Start a graph session\n"}
{"snippet": "hist = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test), verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    n_vocab = vocab_size\n    n_embedding = embed_dim \n    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "print('Checking the Training on a Single Batch...')\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(epochs):\n        batch_i = 1\n        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n", "intent": "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches.\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingRegressor\nregressor=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)\nregressor.fit(X_train, y_train)\n", "intent": "9.GradientBoostingRegressor\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\nmodel=GradientBoostingClassifier(learning_rate=0.1, min_samples_split=100,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10)\nperformTimeSeriesCV(X_train, y_train, 10,\n                    GradientBoostingClassifier,\n                    {'min_samples_split':100,'min_samples_leaf':50,'max_depth':8,'max_features':'sqrt','subsample':0.8,'random_state':10},\n                    {'n_estimators':[20,30,40,50,60,70,80],'learning_rate':[0.1]})\n", "intent": "5.2 Fix learning rate and number of estimators for tuning tree-based parameters\n"}
{"snippet": "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nmodel=QuadraticDiscriminantAnalysis()\nmodel.fit(X_train, y_train)\n", "intent": "6 QuadraticDiscriminantAnalysis\n"}
{"snippet": "X = iris[[\"petal_length\"]]\ny = iris[\"petal_width\"]\nmodel = linear_model.LinearRegression()\nresults = model.fit(X, y)\nprint (results.intercept_, results.coef_)\n", "intent": "Now let's use scikit-learn to find the best fit line.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nclfs = {'lr': LogisticRegression(random_state=0),\n        'mlp': MLPClassifier(random_state=0),\n        'dt': DecisionTreeClassifier(random_state=0),\n        'rf': RandomForestClassifier(random_state=0),\n        'svc': SVC(random_state=0)}\n", "intent": "In the dictionary:\n- the key is the acronym of the classifier\n- the value is the classifier (with random_state=0)\n"}
{"snippet": "D = tf.convert_to_tensor(np.array([[1., 2., 3.], [-3., -7., -1.], [0., 5., -2.]]))\nprint(sess.run(D))\n", "intent": "Create matrix from np array:\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=, random_state=0)\n", "intent": "Hint: Consider the number of class labels (in the data) when deciding the value of n_clusters for KMeans\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state=0)\nimportances = \n", "intent": "1. train the random forest classifier\n2. get the feature importances\n3. plot the bar chart of importances (in descending order)\n"}
{"snippet": "from sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.cluster import KMeans\nX_train, X_test, y_train, y_test = your_friends_diligent_work()\nkm = KMeans(n_clusters=3, random_state=0)\nprint(precision_recall_fscore_support(y_test_pred, y_test, average='micro'))\n", "intent": "1. You did not use any other packages\n2. The prediction accuracy on y_test is over 75%\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=0)\n", "intent": "Hint: Consider the number of class labels (in the data) when deciding the value of n_clusters for KMeans\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state=0)\nrfc.fit(X_train_std, y_train)\nimportances = rfc.feature_importances_\n", "intent": "1. train the random forest classifier\n2. get the feature importances\n3. plot the bar chart of importances (in descending order)\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nclfs = {'lr': LogisticRegression(random_state=0),\n        'mlp': MLPClassifier(random_state=0),\n        'dt': DecisionTreeClassifier(random_state=0),\n        'rf': RandomForestClassifier(random_state=0),\n        'svc': SVC(random_state=0, probability=True)}\n", "intent": "In the dictionary:\n- the key is the acronym of the classifier\n- the value is the classifier (with random_state=0)\n"}
{"snippet": "vect.fit(X_train)\n", "intent": "Note : We can also perform tfidf transformation.\n"}
{"snippet": "prediction = dict()\nfrom sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(X_train_df,y_train)\n", "intent": "Generally, Naive Bayes works well on text data. Multinomail Naive bayes is best suited for classification with discrete features. \n"}
{"snippet": "clf.fit(X_scaled, y_scaled)\n", "intent": "Depending on how computationally heavy your algorithm is, this could take a while.\n"}
{"snippet": "print(sess.run(tf.nn.tanh([-1., 0., 1.])))\ny_tanh = sess.run(tf.nn.tanh(x_vals))\n", "intent": "Hyper Tangent activation\n"}
{"snippet": "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)\n", "intent": "Let's now fit the model and run it for one epoch.\n"}
{"snippet": "model = LinearRegression(normalize=True, fit_intercept=False)\nprint model.normalize\nprint model\n", "intent": "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated, and have suitable default values:\n"}
{"snippet": "model = LinearRegression(normalize=True, fit_intercept=False) \nprint model.normalize\nprint model\n", "intent": "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated, and have suitable default values:\n"}
{"snippet": "from sklearn import linear_model\nmodel = linear_model.Perceptron()\nmodel.fit(X, y)\nplot_decision_boundary(model, X, plot_boundary=True)\nscatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)\n", "intent": "The `Perceptron` is a simple algorithm suitable for large scale learning.\n"}
{"snippet": "classifier = svm.SVC(kernel='rbf', gamma=gammas[np.argmax(scores)])\nclassifier.fit(X, y)\nX_test_norm = scaler.transform(X_test)\nprint classifier.score(X_test_norm, y_test)\n", "intent": "Check the score of the best model in the test set.\n"}
{"snippet": "classifier = svm.SVC(kernel='rbf', gamma=gammas[np.argmax(scores)])\nclassifier.fit(U, y)\nX_test_norm = scaler.transform(X_test)\nU_test = pca.transform(X_test_norm)\nprint classifier.score(U_test, y_test)\n", "intent": "Check the score of the best model that uses PCA in the test set.\n"}
{"snippet": "classifier = svm.SVC(kernel='rbf', gamma=gammas[np.argmax(scores)])  \nclassifier.fit(X, y)\nX_test_norm = X_test/255.*2-1 \nprint classifier.score(X_test_norm, y_test)\n", "intent": "Check the score of the best model in the test set.\n"}
{"snippet": "W_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n", "intent": "Now that the image size is reduced to 7x7, we add a [fully-connected layer](https://en.wikipedia.org/wiki/Convolutional_neural_network\n"}
{"snippet": "W_fc2 = weight_variable([1024, labels_count])\nb_fc2 = bias_variable([labels_count])\ny = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n", "intent": "Finally, we add a softmax layer, the same one if we use just a  simple [softmax regression](https://en.wikipedia.org/wiki/Softmax_function).\n"}
{"snippet": "print(sess.run(tf.nn.softsign([-1., 0., 1.])))\ny_softsign = sess.run(tf.nn.softsign(x_vals))\n", "intent": "Softsign activation\n"}
{"snippet": "model_2 = ensemble.RandomForestClassifier(n_estimators=500, max_depth=12, random_state=0)\nmodel_2.fit(X_train2, y_train2)\n", "intent": "Provo il modello sul nuovo training set:\n"}
{"snippet": "model_5 = ensemble.RandomForestClassifier(n_estimators=500, random_state=0)\n", "intent": "Rieseguo il modello sui dati bilanciati:\n"}
{"snippet": "x = Variable(torch.ones(batch_size,1),requires_grad=True)\nout = model(x)\nout = torch.sum(out)\n", "intent": "- Forward_Pre_Hook is called Before Forward\n- Farward_Hook is called After Forward\n"}
{"snippet": "from keras.models import Sequential\nmodel = Sequential()\n", "intent": "Create a sequential model\n"}
{"snippet": "from keras.layers import Dense, Activation\nmodel.add(Dense(64, input_shape=(784, )))\nmodel.add(Activation('relu'))\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n", "intent": "Define its structure.\n"}
{"snippet": "lm.fit(X, bos.PRICE)\nlm.score(X, bos.PRICE)\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "lm = LinearRegression()\nlm.fit(X[['PTRATIO']], bos.PRICE)\n", "intent": "***\nTry fitting a linear regression model using only the 'PTRATIO' (pupil-teacher ratio by town)\nCalculate the mean squared error. \n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,verbose=2)\ngrid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "ols = sklearn.linear_model.LinearRegression()\ncolumns = ['dwelling_type_Condo', 'dwelling_type_Multi-Family', 'dwelling_type_Residential', 'dwelling_type_Unkown']\nols.fit(sac_w_dummies[columns],sac_w_dummies.price)\nzip(columns,ols.coef_)\n", "intent": "If we attempt a linear regression using all 4 dummy variables, there are an infinite number of options for the zero-intercept point. \n"}
{"snippet": "print(sess.run(tf.nn.softplus([-1., 0., 1.])))\ny_softplus = sess.run(tf.nn.softplus(x_vals))\n", "intent": "Softplus activation\n"}
{"snippet": "logreg = LogisticRegression()\nlogreg.fit(train_x, train_y)\n", "intent": "Fit a logistic regression on the training data.\n"}
{"snippet": "k = 5\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(dn1)\n", "intent": "Cluster the Data - We are going to use 5 clusters based off of the above scatterplot\n"}
{"snippet": "kmeans = KMeans(n_clusters=2)\nclusters = kmeans.fit(x)\n", "intent": "Set up the k-means clustering analysis. Use the graph from above to derive \"k\"\n"}
{"snippet": "dbscn = DBSCAN(eps = .5, min_samples = 5).fit(X)  \n", "intent": "Let's set up the DBSCAN from scikit using an epsilon of .5 and a minimum sample number of 5\n"}
{"snippet": "dbscn = DBSCAN(eps = 0.5, min_samples=5).fit(X_standard)\n", "intent": "Let's set up the DBSCAN from scikit using an epsilon of .5 and a minimum sample number of 5\n"}
{"snippet": "import scipy\ndata_dense = data_features.todense()\n", "intent": "Construct your LDA with the pieces above by fitting the model to your feature set and target\n"}
{"snippet": "x_cols = np.array(df)\nks = [n for n in range(2, 11)] \ny = np.empty(0)\nfor k in ks:\n    kmeans = KMeans(n_clusters=k, random_state=41).fit(x_cols)\n    y = np.append(y, kmeans.inertia_)  \n", "intent": "Low value of SS, because we want to minimize the distances to the centroids.\n"}
{"snippet": "fisher_c34 = LinearDiscriminantAnalysis(solver='eigen',n_components=2).fit(x,y_3_4)\nfisher_c34.coef_\n", "intent": "**Fitting Fisher projection by discriminating classes 3 and 4**\n"}
{"snippet": "from keras.utils.np_utils import to_categorical\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nnb_train = 10_000\nmodel.fit(X[:nb_train], to_categorical(y[:nb_train]), validation_split=.1, epochs=5)\n", "intent": "It's time to compile your model and fit it on MNSIT.\n- Compile your model\n- Fit it on MNIST\n"}
{"snippet": "print(sess.run(tf.nn.elu([-1., 0., 1.])))\ny_elu = sess.run(tf.nn.elu(x_vals))\n", "intent": "Exponential linear activation\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Flatten, Dense\nfrom keras.constraints import nonneg\nmodel = Sequential()\nmodel.add(Flatten(input_shape=X.shape[1:]))\nmodel.add(Dense(units=10, activation='sigmoid'))\nmodel.summary()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n", "intent": "The following code creates a multi-label logistic regression classifier to predict images with multiple MNIST digits.\n"}
{"snippet": "model100.fit(X100,y100,batch_size=100,epochs=50)\n", "intent": "- Fit Your RNN Model on `X100` and `Y100`\n- Run your model for `model.fit(..., batch_size=100, epochs=50)`\n"}
{"snippet": "model.fit(X1, Y1, batch_size=1, shuffle=False, epochs=50)\n", "intent": "- Fit your RNN model on `X1` and `Y1`\n- Fit your model with `model.fit(..., batch_size=1, shuffle=False, epochs=50)`\n"}
{"snippet": "model.fit(X100, Y100, batch_size=100, epochs=50)\n", "intent": "- Fit Your RNN Model on `X100` and `Y100`\n- Run your model for `model.fit(..., batch_size=100, epochs=50)`\n"}
{"snippet": "import tensorflow as tf\nX = tf.placeholder(dtype=tf.float32, shape=[1, 13], name='X')\ny = tf.placeholder(dtype=tf.float32, shape=[1, 1], name='y')\nW = tf.Variable(initial_value=tf.zeros(shape=[13, 1]), name='W')\nb = tf.Variable(initial_value=tf.zeros(shape=[1]), name='b')\nz = tf.matmul(X, W, name='z')\ny_pred = tf.add(z, b, name='y_pred')\nr = tf.subtract(y_pred, y, name='r')\nloss = tf.square(r, name='losses')\nX, y, W, b, z, y_pred, r, loss\n", "intent": "Notice here we specify the *shape* of each input tensor as a matrix as opposed to a scalar.\n"}
{"snippet": "A = tf.Variable(tf.random_normal(shape=[1]))\n", "intent": "We now create the variable for our computational graph, `A`.\n"}
{"snippet": "from support import show_graph\nshow_graph(sess.graph)\n", "intent": "1. TensorBoard\n2. Keras Layers\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation=\"relu\", input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes,activation='softmax'))\nmodel.summary()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, batch_size=32,\n          epochs=50,\n          validation_data=(x_test, y_test), \n          verbose=2)\nprint(x_train.shape)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "reg = fit_model(X_train, y_train)\nprint(\"Parameter 'max_depth' is {} for the optimal model.\".format(reg.get_params()['max_depth']))\n", "intent": "* What maximum depth does the optimal model have? How does this result compare to our previous guess?  \n"}
{"snippet": "oob_forest = RandomForestClassifier(oob_score = True, n_estimators = 100)\noob_forest.fit(X, y)\noob_forest.oob_score_\n", "intent": "**Warning:** Out of bag error can only be used with ensemble classifiers\n"}
{"snippet": "def DNN_model(X):\n    Y = X * tf.Variable(tf.ones([]), name=\"dummy1\") \n    Yout = tf.layers.dense(Y, 1, activation=None) \n    return Yout\n", "intent": "<a name=\"assignment1\"></a>\n<div class=\"alert alert-block alert-info\">\n**Assignment \n</div>\n"}
{"snippet": "tf.reset_default_graph() \nfeatures, labels, dataset_init_op, evaldataset_init_op = datasets(NB_EPOCHS)\nYout, loss, eval_metrics, train_op = model_fn(features, labels, linear_model)\n", "intent": "<a name=\"instantiate\"></a>\n"}
{"snippet": "sess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n", "intent": "This resets all neuron weights and biases to initial random values\n"}
{"snippet": "Hzero = np.zeros([BATCHSIZE, RNN_CELLSIZE * N_LAYERS])\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run([init])\n", "intent": "This resets all neuron weights and biases to initial random values\n"}
{"snippet": "predictions = []\nfor i in range(len(x_vals)):\n    x_val = [x_vals[i]]\n    prediction = sess.run(tf.round(tf.sigmoid(my_output)), feed_dict={x_data: x_val})\n    predictions.append(prediction[0])\naccuracy = sum(x==y for x,y in zip(predictions, y_vals))/100.\nprint('Ending Accuracy = ' + str(np.round(accuracy, 2)))\n", "intent": "Now we can also see how well we did at predicting the data by creating an accuracy function and evaluating them on the known targets.\n"}
{"snippet": "def rnn_loop(char_rnn, batch_ix):\n    batch_size, max_length = batch_ix.size()\n    hid_state = char_rnn.initial_state(batch_size)\n    logprobs = []\n    for x_t in batch_ix.transpose(0,1):\n        hid_state, logp_next = char_rnn(x_t, hid_state)  \n        logprobs.append(logp_next)\n    return torch.stack(logprobs, dim=1)\n", "intent": "Once we've defined a single RNN step, we can apply it in a loop to get predictions on each step.\n"}
{"snippet": "background_filter = cb.CatBoostClassifier(iterations=NUM_EPOCH, depth=4, learning_rate=0.1, loss_function='Logloss', custom_loss=['Accuracy'], random_seed=123, logging_level='Silent')\nbackground_filter.fit(X_filter, Y_filter, verbose=False);\n", "intent": "Background filter training\n"}
{"snippet": "class Tensor(object):\n    def __init__(self, data):\n        self.data=data\n    def add(self, more):\n        self.data+=more\n    def __repr__(self):\n        return 'data: '+str(self.data)\n", "intent": "https://keras-cn.readthedocs.io/en/latest/getting_started/functional_API/\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), dtype=tf.float32)\n    embedding = tf.nn.embedding_lookup(embedding, input_data)\n    return embedding\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegressionCV\nmodel_cv = LogisticRegressionCV(10)\nmodel_cv.fit(X_train, y_train)\n", "intent": "(y_true == y_predict).mean()\n"}
{"snippet": "logreg.fit(X_train, y_train)\nzip(feature_cols, logreg.coef_[0])\n", "intent": "Confirm that the coefficients make intuitive sense.\n"}
{"snippet": "model = XGBClassifier()\nmodel.fit(X_train, y_train)\n", "intent": "Let's finally try XGBoost. The hyperparameters are the same, some important ones being ```subsample```, ```learning_rate```, ```max_depth``` etc.\n"}
{"snippet": "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 2000, print_loss = True)\n", "intent": "Now, let's call the function L_layer_model on the dataset we have created.This will take 10-20 mins to run.\n"}
{"snippet": "nn_model.fit(train_set_x, train_set_y, epochs=10, batch_size=10)\n", "intent": "Now, to fit the model on the training input and training target dataset, we run the following command using a minibatch of size 10 and 10 epochs.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "We start a computational graph session.\n"}
{"snippet": "clf = RandomForestClassifier(random_state=42)\nparameters = {'n_estimators':[100, 200, 400, 600],\n              'max_depth':[100, 200, 400, 600],\n              'min_samples_split':[2, 5]}\nscorer = make_scorer(fbeta_score, beta=0.5)\ngrid_obj = GridSearchCV(estimator=clf,\n                        param_grid=parameters,\n                        scoring=scorer)\ngrid_fit = grid_obj.fit(X_t, y_t)\n", "intent": "Grid search is used to find best parameters.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "from keras.callbacks import EarlyStopping\nmy_callbacks = [EarlyStopping(monitor='val_acc', patience=5, mode='max')]\nhist = model.fit(x_train, y_train,\n          batch_size=32,\n          epochs=20,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "images, labels = next(iter(trainloader))\nimg = images[1].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "bag_mod.fit(training_data, y_train)\nrf_mod.fit(training_data, y_train)\nada_mod.fit(training_data, y_train)\nsvm_mod.fit(training_data, y_train)\nprint('fit done')\n", "intent": "> **Step 1**: Now, fit each of the above models to the appropriate data.  Answer the following question to assure that you fit the models correctly.\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\nmodel_GB = GradientBoostingClassifier(learning_rate=0.1,n_estimators=400,random_state = 42)\nmodel_GB.fit(X_train, y_train)\n", "intent": "Used values from Udacity project 1\n"}
{"snippet": "with train_graph.as_default():\n    saver = tf.train.Saver()\nwith tf.Session(graph=train_graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('../../../Data/For_emmbedding/checkpoints'))\n    embed_mat = sess.run(embedding)\n", "intent": "Restore the trained network if you need to:\n"}
{"snippet": "def conv_block(x, num_filters, filter_size, stride=(2,2), mode='same', act=True):\n    x = Convolution2D(num_filters, filter_size, filter_size, subsample=stride, border_mode=mode)(x)\n    x = BatchNormalization(mode=2)(x)\n    return Activation('relu')(x) if act else x\ndef res_block(initial_input, num_filters=64):\n    x = conv_block(initial_input, num_filters, 3, (1,1))\n    x = conv_block(x, num_filters, 3, (1,1), act=False)\n    return merge([x, initial_input], mode='sum')\n", "intent": "ConvBlock  \nResBlock\n"}
{"snippet": "def up_block(x, num_filters, size):\n    x = keras.layers.UpSampling2D()(x)\n    x = Convolution2D(num_filters, size, size, border_mode='same')(x)\n    x = BatchNormalization(mode=2)(x)\n    return Activation('relu')(x)\n", "intent": "Deconvolution / Transposed Conv / Fractionally Strident Convs\n"}
{"snippet": "A = tf.Variable(tf.random_normal(shape=[1]))\nmy_output = tf.multiply(x_data, A)\n", "intent": "We create the one variable in the graph, `A`.  We then create the model operation, which is just the multiplication of the input data and `A`.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim),-1,1))\n    print(\"get_embed input:\",input_data)\n    print(\"get_embed embedding:\",embedding)\n    return tf.nn.embedding_lookup(embedding,input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "import pickle\nA = pickle.load(file('matrix_sparse.pkl', 'r')).todense()\nnum_measurements = np.sum(np.abs(A)/2)\nnum_edges = np.sum(A != 0)/2\nprint('There are %s edges.'%(num_edges))\nprint('There are %s measurements.'%(num_measurements))\n", "intent": "1.\nHow many measurements are in the matrix?\n"}
{"snippet": "import networkx as nx\nG = nx.Graph(A)\nprint('Size of components in A:')\nfor i, component in enumerate(nx.connected_components(G)):\n    print('  Component %s contains %s nodes.'%(i, len(component)))\n", "intent": "2.\nIs the graph connected?\nAlmost but no.\n"}
{"snippet": "X_r = pca.fit(X).transform(X)\nprint(X_r)\n", "intent": "We then fit `X` to get the model and then transform the original data of four variables (`X`) to two variables (components).\n"}
{"snippet": "pca.fit(faces)\n", "intent": "Fit the faces data:\n"}
{"snippet": "model_top = Sequential()\nmodel_top.add(Flatten(input_shape=train_data.shape[1:]))\nmodel_top.add(Dense(256, activation='relu'))\nmodel_top.add(Dropout(0.5))\nmodel_top.add(Dense(1, activation='sigmoid'))\nmodel_top.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n", "intent": "And define and train the custom fully connected neural network :\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    return tf.nn.embedding_lookup(tf.Variable(tf.random_uniform((vocab_size, embed_dim), -0.1, 0.1)), input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "biases = tf.Variable(tf.zeros([num_classes]), name='biases')\nprint biases\n", "intent": "The second variable that must be optimized is called `biases` and is defined as a 1-dimensional tensor (or vector) of length `num_classes`.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded_input = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embedded_input = tf.nn.embedding_lookup(embedded_input, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "ops.reset_default_graph()\nsess = tf.Session()\n", "intent": "------------------\nWe start by resetting the computational graph\n"}
{"snippet": "inp = Input(shape=(SEQ_LEN,))\nemb = Embedding(VOCAB_SIZE, EMBEDDING_LEN_50, input_length=SEQ_LEN)(inp)\nx = Convolution1D(filters=32,kernel_size=4,padding='same')(emb)\nx = MaxPooling1D(pool_size=(2))(x)\nlstm = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(x)\npreds = Dense(2, activation = 'softmax')(lstm)\nconv_lstm_model = Model(inputs=inp, outputs=preds)\nconv_lstm_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\nconv_lstm_model.summary()\n", "intent": "We can also work with convolution and LSTM layers in a model.\n"}
{"snippet": "inp = Input(shape=(SEQ_LEN,))\nemb = Embedding(VOCAB_SIZE, EMBEDDING_LEN_50, input_length=SEQ_LEN)(inp)\nlstm = GRU(100)(emb)\npreds = Dense(2, activation = 'softmax')(lstm)\nsimple_gru_model = Model(inputs=inp, outputs=preds)\nsimple_gru_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\nsimple_gru_model.summary()\n", "intent": "Creating a GRU based model is as simpel as simply replacing the LSTM layer above with the GRU layer\n"}
{"snippet": "def dense_model():\n    model = Sequential()\n    model.add(BatchNormalization(axis=1, input_shape=(28,28,1)))\n    model.add(Flatten())\n    model.add(Dense(1024, activation='relu'))\n    model.add(Dense(10, activation = 'softmax'))\n    model.compile(optimizer = Adam(), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    model.summary()\n    return model\n", "intent": "Now let us create a model which has a single dense layer in it\n"}
{"snippet": "def dense_model_dropout():\n    model = Sequential()\n    model.add(BatchNormalization(axis=1, input_shape=(28,28,1)))\n    model.add(Flatten())\n    model.add(Dense(1024, activation='relu'))\n    model.add(Dropout(0.6))\n    model.add(Dense(10, activation = 'softmax'))\n    model.compile(optimizer = Adam(), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    model.summary()\n    return model\n", "intent": "Since our above model was overfitting, we can add Dropout to it in order to add regularization to our deep neural network\n"}
{"snippet": "outputs = [] \nfor i in [1,2,3]:\n    outputs.append(get_output_layers_of_vgg(i))\nvgg_content = Model(inputs=vgg_inp, outputs=outputs)\n", "intent": "We need to compare thee outputs among some conv layers. Here we are selecting the conv2 layers of blocks 1,2 and 3.\n"}
{"snippet": "K.set_value(super_res_model.optimizer.lr, 1e-4)\nsuper_res_model.fit(x=[arr_lr, arr_hr], y=targ, batch_size=32, epochs=1)\n", "intent": "Running one more epoch ...\n"}
{"snippet": "def copy_weights(from_layers, to_layers):\n    for from_layer,to_layer in zip(from_layers,to_layers):\n        to_layer.set_weights(from_layer.get_weights())\n", "intent": "Now let us copy the weights of our previously trained model, to this new model which takes high res images as input.\n"}
{"snippet": "inp = Input(shape=(g_inp_dim,), name=\"input_G\")\nx = Dense(200, activation='relu')(inp)\nx = Dense(400, activation='relu')(x)\noutp = Dense(784, activation='sigmoid')(x)\nMLP_G = Model(inputs=inp, outputs=outp)\n", "intent": "The generator has 784 as the output shape because we want the generator to create a 28x28 image.\n"}
{"snippet": "MLP_C = Sequential([MLP_G,MLP_D])\nMLP_C.compile(optimizer='adam', loss='binary_crossentropy')\n", "intent": "Now, for the generative flow, we need to combined both G and D in a sequence since the output of G should be fed into D.\n"}
{"snippet": "A = tf.Variable(tf.random_normal(shape=[1,1]))\nmy_output = tf.matmul(x_data, A)\n", "intent": "We create the one variable in the graph, `A`.  We then create the model operation, which is just the multiplication of the input data and `A`.\n"}
{"snippet": "def fit_single_model():\n    new_model = fc_model()\n    new_model.optimizer.lr = 1e-3\n    new_model.fit(train_features_to_dense, train_labels, epochs=10, \n             batch_size=batch_size*4, validation_data=(valid_features_to_dense, valid_labels))\n    new_model.optimizer.lr = 1e-6\n    new_model.fit(train_features_to_dense, train_labels, epochs=15, \n             batch_size=batch_size*4, validation_data=(valid_features_to_dense, valid_labels))\n    return new_model\n", "intent": "Lets create an ensemble to get a further boost to our predictions\n"}
{"snippet": "new_model.optimizer.lr = 1e-6\nnew_model.fit(train_features_to_dense, train_labels, epochs=4, \n             batch_size=batch_size*4, validation_data=(valid_features_to_dense, valid_labels))\n", "intent": "Running for a few more epochs ...\n"}
{"snippet": "new_model.optimizer.lr = 1e-6\nnew_model.fit(train_features_to_dense, train_labels, epochs=4, \n             batch_size=batch_size*8, validation_data=(valid_features_to_dense, valid_labels))\n", "intent": "A few more epochs ...\n"}
{"snippet": "layer_model = Model(inputs=base_vgg_model.input, outputs=conv_layer_output)\n", "intent": "Now we want to snip our VGG model such that given an input image, we can get the outputs of this conv layer.\n"}
{"snippet": "mul_layer_model = Model(base_vgg_model.input, mul_conv_layer_outputs)\n", "intent": "Creating a model which gives the output of the above multiple conv layers.\n"}
{"snippet": "top_model = Model(inp, outp)\n", "intent": "We are only interested in the trained part of the model.\n"}
{"snippet": "hist = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_test, y_test), verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nclf= DecisionTreeClassifier(random_state=1)\nclf.fit(X_train, y_train)\n", "intent": "We start with creating a DecisionTreeClassifier and fit it to your training data.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeding = tf.Variable(tf.random_normal((vocab_size,embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embeding,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Then we start a computational graph session.\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators = 250, bootstrap = True, oob_score = True, max_depth = 50)\nrf.fit(X_train, y_train)\n", "intent": "Fitting a DTC with maximum depth of 6\n"}
{"snippet": "dtc = DecisionTreeClassifier(max_depth=6)\ndtc.fit(X_train, y_train)\n", "intent": "Fitting a DTC with maximum depth of 6\n"}
{"snippet": "model = Sequential()\n", "intent": "** Working on Model **\n"}
{"snippet": "weights = tf.Variable(tf.random_normal([2, 1], 0, 0.1))\nprint weights\n", "intent": "The objective is minimizing L2 loss\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    n_vocab = vocab_size\n    n_embedding = embed_dim \n    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n    embed_input = tf.nn.embedding_lookup(embedding, input_data)\n    return embed_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = sm.Logit(Y_train, X_train)\nresult = model.fit()\nresult.summary2()\n", "intent": "We can see the results by using result.summary2() function.\n"}
{"snippet": "logistic_reg = LogisticRegression()\nlogistic_reg.fit(titanic_data_numeric, titanic_label)\nprint(logistic_reg.score(titanic_data_numeric, titanic_label))\n", "intent": "- Define model\n- Fit titanic_data_numeric, titanic_label into this model\n"}
{"snippet": "happyModel.fit(x = X_train, y = Y_train, epochs=5, batch_size=32)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "def sigmoid(t):\n    return 1/(1 + np.e**-t)\n", "intent": "First, we define the sigmoid function:\n"}
{"snippet": "A = tf.Variable(tf.random_normal(shape=[1, 1]))\nb = tf.Variable(tf.random_normal(shape=[1, 1]))\n", "intent": "We are going to be doing a linear model, so we will need to create two variables, `A` (slope) and `b` (intercept).\n"}
{"snippet": "model = lm.LinearRegression(fit_intercept = True)\nmodel.fit(small_data[[\"sqft\"]], small_data[[\"price\"]])\nprint(\"Intercept:\", model.intercept_[0])\nprint(\"Slope:\", model.coef_[0,0])\n", "intent": "Your friend fits a linear model of sale price on home square footage with an intercept as shown below:\n"}
{"snippet": "mean0 = [-1,-1]  \nmean1 = [1,1] \ncov0 = [[.5, .28], [.28, .5]] \ncov1 = [[1, -.8], [-.8, 1]] \nmixture = util.GaussianMixture(mean0,cov0,mean1,cov1)\nmX,mY = mixture.sample(500,0.5,plot=True)\n", "intent": "*Exercise: Use Sklearn to fit a logistic regression model on the gaussian mixture data.*\n"}
{"snippet": "x_train = [1, 2, 3]\ny_train = [1, 2, 3]\nW = tf.Variable(tf.random_normal([1]), name = 'weight')\nb = tf.Variable(tf.random_normal([1]), name = 'bias')\nhypothesis = x_train * W + b\n", "intent": "$$ H(x) = Wx + b $$\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    emnedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embedded_imput = tf.nn.embedding_lookup(emnedding, input_data)\n    return embedded_imput\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, epochs=10, batch_size=50)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "import torch.nn as nn\ninput = Variable(torch.randn(4, 5), requires_grad=True)\ntarget = Variable(torch.randn(4, 5))\n", "intent": "Question: What is mean squared error? What are the inputs? What's the output?\n"}
{"snippet": "input = Variable(torch.randn(4, 5), requires_grad=True)\ntarget = Variable(torch.LongTensor(4).random_(5))\nprint(input)\nprint(target)\n", "intent": "Question: What is cross entropy loss? What are the inputs? What's the output?\n"}
{"snippet": "import torch.nn as nn\ninput = Variable(torch.randn(4, 5), requires_grad=True)\ntarget = Variable(torch.randn(4, 5))\n", "intent": "Question: What is mean square error? What are the inputs? What's the output?\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embedded = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Start a graph session:\n"}
{"snippet": "fit_1 =  sm.GLS(df.wage, X1).fit()\nfit_2 =  sm.GLS(df.wage, X2).fit()\nfit_3 =  sm.GLS(df.wage, X3).fit()\nfit_4 =  sm.GLS(df.wage, X4).fit()\nfit_5 =  sm.GLS(df.wage, X5).fit()\nsm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1)\n", "intent": "Selecting a suitable degree for the polynomial of age.\n"}
{"snippet": "svm3 = SVC(C=1, kernel='rbf', gamma=2)\nsvm3.fit(X_train, y_train)\n", "intent": "Comparing the ROC curves of two models on train/test data. One model is more flexible than the other.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embed = tf.Variable(tf.random_uniform((vocab_size, embed_dim),\n                                         -1, 1))\n    embedded = tf.nn.embedding_lookup(embed, input_data)\n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "hidden5_out = tf.get_default_graph().get_tensor_by_name(\"hidden5_out:0\")\nprint('done')\n", "intent": "3.\nLet's start by getting a handle on the output of the last frozen layer:\n"}
{"snippet": "dnn_clf_5_to_9 =  DNNClassifier(n_hidden_layers=4, random_state=42)\ndnn_clf_5_to_9.fit(X_train2, y_train2, n_epochs=1000, X_valid=X_valid2, y_valid=y_valid2)\n", "intent": "Let's compare that to a DNN trained from scratch:\n"}
{"snippet": "LR = LinearRegression()\nLR.fit(X_train_scaled, y_train)\n", "intent": "Now train a simple linear regression on scaled data:\n"}
{"snippet": "lr_pca = LinearRegression()\nlr_pca.fit(pca_features_train, y_train)\nplotModelResults(lr_pca,pca_features_train, pca_features_test, plot_intervals=True);\n", "intent": "Now train linear regression on pca features and plot its forecast.\n"}
{"snippet": "ridge.fit(X_train, y_train)\n", "intent": "MAE is ~48.5 which corresponds to ~1.16  error in \n"}
{"snippet": "db2 = DBSCAN(eps=1, min_samples=10)   \ndb2.fit(iris_data_scaled)\ndb2.labels_    \n", "intent": "What happens as we alter either $\\epsilon$ or min_samples?\nLet's first alter min_samples:\n"}
{"snippet": "A = tf.Variable(tf.random_normal(shape=[1,1]))\nmy_output = tf.matmul(x_data, A)\n", "intent": "We create the model variable `A` and the multiplication operation.\n"}
{"snippet": "lasso_all = Lasso().fit(pf_3_data,housing_data[housing_target])\nprint(\"Number of features in the model: \",len(lasso_all.coef_))\nnon_zero_features_mask = np.abs(lasso_all.coef_)>0.0001\nprint(non_zero_features_mask)\nprint(\"Number of non-zero features in the model: \",np.sum(non_zero_features_mask.astype(int)))\nprint(\"Fraction of total features used: \",float(np.sum(non_zero_features_mask.astype(int)))/len(lasso_all.coef_))\n", "intent": "Wow! That worked very well! We now have a much better model! Let's examine how many of the coefficients are non-zero when we fit on the full dataset:\n"}
{"snippet": "scaler.fit(df.drop('Class',axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n                                            hidden_units=[10, 20, 10],\n                                            n_classes=2,\n                                            model_dir=\"./output\")\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "model.fit(data.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "nn = MLPRegressor(max_iter=1000, random_state = seed)\n", "intent": "Define a neural network ([MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n"}
{"snippet": "_, mse_valid_lr = train_model(lr, df_train_X, df_train_y, df_valid_X, df_valid_y)\nprint 'Mean squared error on validation set using linear regression: ', '%.2f' % mse_valid_lr\n", "intent": "Obtain its performance on the validation set. Take a moment to think whether you need to use the original or standardised training / validation sets.\n"}
{"snippet": "mse_train_dt, mse_valid_dt = train_model(dt, df_train_X, df_train_y, df_valid_X, df_valid_y)\nprint 'Mean squared error on validation set using decision tree: ', '%.2f' % mse_valid_dt\n", "intent": "Obtain its performance on the validation set. Take a moment to think whether you need to use the original or standardised training / validation sets.\n"}
{"snippet": "nn_models = [nn1, nn2, nn3, nn4, nn5]\nfor i in range(len(nn_models)):\n    nn = nn_models[i]\n    _, mse_valid_nn = train_model(nn, df_train_X_std, df_train_y, df_valid_X_std, df_valid_y)\n    print 'Mean squared error on validation set using neural network nn' + str(i + 1), ': ', '%.2f' % mse_valid_nn\n", "intent": "Let's obtain the performance of each model on the validation set.\n"}
{"snippet": "model = DecisionTreeRegressor(random_state=seed)\n", "intent": "Run the following to define a new decision tree.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Create a graph session:\n"}
{"snippet": "from pytorchlearning import helper\nimages, labels = next(iter(trainloader))\nimg = images[0].view(1, 784)\nwith torch.no_grad():\n    logps = model(img)\nps = torch.exp(logps)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "t2= torch.Tensor(t)\nprint(t2)\nt3= torch.IntTensor([[1,21],[2,22]])\nprint(t3)b\n", "intent": "Ways of creating a tensor\n"}
{"snippet": "scaler.fit(df.drop(\"TARGET CLASS\",axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "knn = KNeighborsClassifier(5).fit(X_train, y_train)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n"}
{"snippet": "k = range(2, 10)\nparams = {'n_neighbors': k }\ngv = GridSearchCV(\n    estimator = KNeighborsClassifier(),\n    param_grid = params,\n    cv = kf\n)\ngv.fit(X, y)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n"}
{"snippet": "model2 = LogisticRegression()\nmodel2.fit(college[factors], college['admit'])\n", "intent": "Observe that the regularization serves to shrink the coefficient parameters\n"}
{"snippet": "estimator = KMeans(n_clusters=2)\nX = df[[\"x\", \"y\"]]\nestimator.fit(X)\nlabels = estimator.labels_\nprint(labels)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "from gensim.models.ldamodel import LdaModel\nfrom gensim.matutils import Sparse2Corpus\ncorpus = Sparse2Corpus(docs, documents_columns = False)\nnum_topics = 15\nlda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics)\n", "intent": "https://radimrehurek.com/gensim/\n"}
{"snippet": "dt = fit_model(tree.DecisionTreeClassifier(max_depth=3), X, y)\n", "intent": "[Example of a visualization](http://scikit-learn.org/stable/_images/iris.svg)  \n[sklearn reference](http://scikit-learn.org/stable/modules/tree.html)\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Next we start a graph session.\n"}
{"snippet": "knn = KNeighborsClassifier()\nparams = {\"n_neighbors\": range(2,20,2)}\nknn_grid = GridSearchCV(knn, params, n_jobs=-1, verbose=True, cv=cv)\nknn_grid.fit(X,y)\n", "intent": "Using grid search to tune the KNN model to see if it performed better than the logistic regression:\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(X,y)\n", "intent": "Building the model and evaluating mean performance:\n"}
{"snippet": "dt = DecisionTreeClassifier()\nparams = {\"max_depth\": [3,5,7,10,20],\"max_features\":[0.1, 0.3, 0.7, 1], \\\n         \"min_samples_split\":[2,3,5]}\ndt_grid = GridSearchCV(dt, params, n_jobs=-1, verbose=True, cv=cv)\ndt_grid.fit(X,y)\n", "intent": "Using grid search to tune the DT model to see if it performed better than the logistic regression:\n"}
{"snippet": "model.fit(X_train, y_train,\n          validation_data=(X_val, y_val), epochs=5);\n", "intent": "Keras models follow __Scikit-learn__'s interface of fit/predict with some notable extensions. Let's take a tour.\n"}
{"snippet": "class ReLU(Layer):\n    def __init__(self):\n        return np.maximum(0,input)\n    def backward(self,input,grad_output):\n", "intent": "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\n"}
{"snippet": "network = []\nnetwork.append(Dense(X_train.shape[1],100))\nnetwork.append(ReLU())\nnetwork.append(Dense(100,200))\nnetwork.append(ReLU())\nnetwork.append(Dense(200,10))\n", "intent": "We'll define network as a list of layers, each applied on top of previous one. In this setting, computing predictions and training becomes trivial.\n"}
{"snippet": "weights = tf.Variable(dtype= tf.float32, initial_value=tf.zeros([X.shape[1],1]), name=\"weight\")\nb = tf.Variable(dtype= tf.float32, initial_value=0, name=\"bias\")\n", "intent": "__Your code goes here.__ For the training and testing scaffolding to work, please stick to the names in comments.\n"}
{"snippet": "encoder,decoder = build_pca_autoencoder(img_shape,code_size=32)\ninp = L.Input(img_shape)\ncode = encoder(inp)\nreconstruction = decoder(code)\nautoencoder = keras.models.Model(inp,reconstruction)\nautoencoder.compile('adamax','mse')\n", "intent": "Meld them together into one model\n"}
{"snippet": "x_t = tf.placeholder('int32',(1,))\nh_t = tf.Variable(np.zeros([1,rnn_num_units],'float32'))\nnext_probs,next_h = rnn_one_step(x_t,h_t)\n", "intent": "Once we've trained our network a bit, let's get to actually generating stuff. All we need is the `rnn_one_step` function you have written above.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Next we create a graph session\n"}
{"snippet": "ops.reset_default_graph()\nsess = tf.Session()\n", "intent": "$$sigmoid(x) = {1 \\over{1 + e^x}}$$\n$$ReLU(x) = max(0, x)$$\n"}
{"snippet": "for i in range(1001):\n    mse_train = update_model(data_train)\n    if i % 100 == 0:\n        mse_val = get_cost(data_val)\n        print('Epoch {}: train mse: {}    validation mse: {}'.format(i, mse_train, mse_val))\n", "intent": "We can now train the network by supplying this function with our data and calling it repeatedly.\n"}
{"snippet": "input = Variable(torch.randn(1, 1, 32, 32))\nout = net(input)\nprint(out)\n", "intent": "The input to the forward is an ``autograd.Variable``, and so is the output.\n"}
{"snippet": "result = torch.Tensor(5, 3)\ntorch.add(x, y, out=result)\nprint(result)\n", "intent": "Addition: giving an output tensor\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n", "intent": "Finetuning the convnet\n----------------------\nLoad a pretrained model and reset final fully connected layer.\n"}
{"snippet": "print(\"Fitting the classifier to the training set\")\nt0 = time()\nparam_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\nclf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\nclf = clf.fit(X_train_pca, y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(\"Best estimator found by grid search:\")\nprint(clf.best_estimator_)\n", "intent": "Train a SVM classification model\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, input_dim=x_train.shape[1]))\nmodel.add(Activation('softmax'))\nmodel.add(Dense(2))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=500, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "neigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(data, target)\n", "intent": "Set kNN's k value to 3 and fit data\n"}
{"snippet": "sess = tf.Session()\n", "intent": "We create a graph session.\n"}
{"snippet": "_ = linear_regressor.fit(\n    input_fn=training_input_fn,\n    steps=100\n)\n", "intent": "Calling `fit()` on the feature column and targets will train the model.\n"}
{"snippet": "_, adagrad_training_losses, adagrad_validation_losses = train_nn_regression_model(\n    optimizer=tf.train.AdagradOptimizer(learning_rate=0.5),\n    steps=500,\n    batch_size=100,\n    hidden_units=[10, 10],\n    training_examples=normalized_training_examples,\n    training_targets=training_targets,\n    validation_examples=normalized_validation_examples,\n    validation_targets=validation_targets)\n", "intent": "First, let's try Adagrad.\n"}
{"snippet": "_, adam_training_losses, adam_validation_losses = train_nn_regression_model(\n    optimizer=tf.train.AdamOptimizer(learning_rate=0.009),\n    steps=500,\n    batch_size=100,\n    hidden_units=[10, 10],\n    training_examples=normalized_training_examples,\n    training_targets=training_targets,\n    validation_examples=normalized_validation_examples,\n    validation_targets=validation_targets)\n", "intent": "Now let's try Adam.\n"}
{"snippet": "_ = train_linear_classification_model(\n    learning_rate=0.03,\n    steps=1000,\n    batch_size=100,\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\n", "intent": "Here is a set of parameters that should attain roughly 0.9 accuracy.\n"}
{"snippet": "calibration_data = train_model(\n    learning_rate=0.05,\n    steps=500,\n    batch_size=5,\n    input_feature=\"rooms_per_person\")\n", "intent": "To verify that clipping worked, let's train again and print the calibration data once more.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn_params = {\n    'n_neighbors':range(5,20),\n    'weights':['distance','uniform']\n}\nknn_gs = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, verbose=1)\nknn_gs.fit(X_train, y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "lr_params = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\ngs_lrap = GridSearchCV(LogisticRegression(), lr_params, scoring = \"average_precision\", cv=5)\ngs_lrap = gs_lrap.fit(X, y)\nprint('Grid Search Best Score: %.4f' % gs_lrap.best_score_)\nprint('Grid Search Best Parameter for C: ')\nprint gs_lrap.best_params_\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "from sklearn.svm import SVC\nsvc_params = {\n    'C':np.logspace(-5,5),\n    'kernel':['linear']\n}\nsvc_gs = GridSearchCV(SVC(), svc_params, cv=3, verbose=1)\nsvc_gs.fit(X_train, y_train)\n", "intent": "Below is my Support Vector Machine score, it didn't improve.\n"}
{"snippet": "print('Gradient Boost with 1200 trees, learning rate of .9')\ngb3 = GradientBoostingRegressor(learning_rate=.9,n_estimators=1200)\nstage_score_plot(gb3, X_train, y_train, X_test, y_test, .9)\n", "intent": "- Train error goes to zero around 1,200 iterations (this is our optimal parameter then). Next adjust our learning rate.\n"}
{"snippet": "x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nA = tf.Variable(tf.random_normal(shape=[1,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\n", "intent": "We now initialize the placeholders and variables in the model.\n"}
{"snippet": "print('Gradient Boost with 1200 trees, learning rate of .1, max depth of 1')\ngb7 = GradientBoostingRegressor(learning_rate=.1,n_estimators=1200,max_depth=1)\nstage_score_plot(gb7, X_train, y_train, X_test, y_test, .1)\n", "intent": "- Alright, bse MSE around .45. Next, adjust max depth.\n"}
{"snippet": "print('Gradient Boost with 1200 trees, learning rate of .1, max depth of 10')\ngb8 = GradientBoostingRegressor(learning_rate=.1,n_estimators=1200,max_depth=10)\nstage_score_plot(gb8, X_train, y_train, X_test, y_test, .1)\n", "intent": "- It keeps improving, let's try a much higher max depth.\n"}
{"snippet": "random_search_gb_ensemble= RandomizedSearchCV(estimator=GradientBoostingRegressor(),param_distributions=gradient_boost_parameters,cv=3,n_iter=100,verbose=1)\nrandom_search_gb_ensemble.fit(final_predictions_df[['rd', 'myside', 'opptside', 'mypush', 'mypull', 'opptpush', 'opptpull',\n       'mychoice', 'mychoicecard', 'GLM_scaled_train', 'GB_train', 'RF_train',\n       'XGB_train', 'KNN_train']],final_predictions_df['actual_training'])\n", "intent": "- For our GB ensemble, find the best parameters for this training split.\n"}
{"snippet": "def ridge(X, y, d2):\n    return dot(dot(inv(dot(X.T, X) + d2*eye(X.shape[1])), X.T), y)\n", "intent": "The estimator for the ridge regression model is:\n$$\\hat{\\beta}^{ridge} = (X'X + \\alpha I)^{-1}X'y$$\n"}
{"snippet": "for i in [.1,1,10,100]:\n    svm_four = SVC(C=i,gamma=3)\n    svm_four.fit(rbf_df,rbf_df_labels)\n    print(' C = {}, Gamma = 3'.format(i))\n    decision_boundary(svm_four,np.array(rbf_df),np.array(rbf_df_labels))\n", "intent": "> Plot C from [1E-1,1,10,100] holding gamma constant at 3. What do you notice?\n"}
{"snippet": "svm_five = SVC(C=1.0,gamma=250)\nsvm_five.fit(rbf_df,rbf_df_labels)\ndecision_boundary(svm_five,np.array(rbf_df),np.array(rbf_df_labels))\n", "intent": "> This may take a while, but plot gamma at 250. What do you notice?\n"}
{"snippet": "svm_iris = SVC(kernel='rbf').fit(data_iris[:,:2],labels_iris)\n", "intent": "Now, run an SVM on the third and fourth column, and use the function below to plot the boundary. What do you notice?\n"}
{"snippet": "svm_iris = SVC(kernel='linear').fit(data_iris[:,:2],labels_iris)\ndecision_boundary(svm_iris,np.array(data_iris[:,:2]),np.array(labels_iris))\n", "intent": "- Which kernel can you use to correctly classify the last point?\n"}
{"snippet": "svm_iris = SVC(kernel='sigmoid').fit(data_iris[:,:2],labels_iris)\ndecision_boundary(svm_iris,np.array(data_iris[:,:2]),np.array(labels_iris))\n", "intent": "- Poly kernel get the last point correct.\n"}
{"snippet": "x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nA = tf.Variable(tf.random_normal(shape=[1,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n", "intent": "Now we can initialize placeholders, model variables, and model operations.\n"}
{"snippet": "multiNB_tf = MultinomialNB()\n", "intent": "Now fit a new NB with the tf-idf vectorized data\n"}
{"snippet": "from sklearn.svm import SVC\nlb_view = client.load_balanced_view()\nmodel = SVC()\nsvc_params = {\n    'C': np.logspace(-1, 2, 4),\n    'gamma': np.logspace(-4, 0, 5),\n}\nall_parameters, all_tasks = grid_search(\n   lb_view, model, digits_split_filenames, svc_params)\n", "intent": "Let's try on the digits dataset that we splitted previously as memmapable files:\n"}
{"snippet": "from implicit.als import AlternatingLeastSquares\nfrom scipy import sparse\nmodel = AlternatingLeastSquares(factors=20)\nuser_item_train = sparse.csr_matrix(train_data_matrix)\nmodel.fit(user_item_train.T)\nrecommendations = model.recommend(user_normalizer.lookup[10370],user_item_train,\\\n                      N=25,\\\n                filter_items=list(user_item_train[user_normalizer.lookup[125981]].indices)) \nrecommendations = [x[0] for x in recommendations[:20]]\n", "intent": "* There exist many packages that implement this algorithm, for example `implicit`\n"}
{"snippet": "model.fit(x_train, y_train,\n                    batch_size=64,\n                    epochs=30,\n                    verbose=1,\n                    initial_epoch=20,\n                    callbacks=callbacks_list,\n                    validation_data=(x_test, y_test))\n", "intent": "Train for 10 epochs from 20 to 30\n"}
{"snippet": "model.fit(x_train, y_train,\n                    batch_size=64,\n                    epochs=40,\n                    verbose=1,\n                    initial_epoch=30,\n                    callbacks=callbacks_list,\n                    validation_data=(x_test, y_test))\n", "intent": "Train for 10 epochs from 30 to 40\n"}
{"snippet": "model.fit(x_train, y_train,\n                    batch_size=64,\n                    epochs=50,\n                    verbose=1,\n                    initial_epoch=40,\n                    callbacks=callbacks_list,\n                    validation_data=(x_test, y_test))\n", "intent": "Train for 10 epochs from 40 to 50\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(n1, input_dim=n_features, activation='sigmoid'))\nmodel.add(Dense(n2, activation='sigmoid'))\nmodel.add(Dense(n_classes, activation='sigmoid'))\n", "intent": "The layers definitions can be also written in a more compact manner:\n"}
{"snippet": "def tanh(inputs):\n    values = [ np.tanh(x) for x in inputs]\n    return values\ny_tanh = tanh(x)\nline_graph(x, y_tanh, \"x\", \"tanh(x)\")\n", "intent": "- Hyperbolic tangent\n$$\\phi(x) = \\tanh(x)$$\n"}
{"snippet": "def ReLU(inputs):    \n    values = [x if x>0 else 0 for x in inputs]\n    return values\ny_relu = ReLU(x)\nline_graph(x, y_relu, \"x\", \"tanh(x)\")\n", "intent": "- Rectified Linear Unit fuction (ReLU)\n$$\\phi(x) = x I_{x \\ge 0}$$\n"}
{"snippet": "x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nA = tf.Variable(tf.random_normal(shape=[1,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n", "intent": "Same as before, we initialize the placeholders, variables, and model operations.\n"}
{"snippet": "model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))\n", "intent": "For training set ~8000, 20 epochs: \n"}
{"snippet": "model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))\n", "intent": "For training set 10000, 20 epochs: \n"}
{"snippet": "model_8_pretrained_layers = vgg16_model(img_rows, img_cols, channel, num_classes)\nmodel_8_pretrained_layers.summary()\n", "intent": "Let's train 50 epochs of model withou \n"}
{"snippet": "classifier = svm.SVC(gamma=0.0005)\nclassifier.fit( training_data, training_labels )  \n", "intent": "create a classifier (in this case a Support Vector Machine Classifier)\n"}
{"snippet": "with tf.Session() as sess:\n    print \"Addition with variables: %i\" % sess.run(add, feed_dict={a: 2, b: 3})\n    print \"Multiplication with variables: %i\" % sess.run(mul, feed_dict={a: 2, b: 3})\n", "intent": "In this case we need to use the `feed_dict` argument to pass the actual values to our graph.\n"}
{"snippet": "g = tf.get_default_graph()\n[op.name for op in g.get_operations()]\n", "intent": "What does the graph look like for this network?\n"}
{"snippet": "convolved = tf.nn.conv2d(img_4d, z_4d, strides=[1, 1, 1, 1], padding='SAME')\nconvolved\n", "intent": "The convolution operation is performed in tensorflow using the `nn.conv2d` function\n"}
{"snippet": "sess = tf.Session(graph=g)\nsess.close()\n", "intent": "We could also explicitly tell the session which graph we want to manage:\n"}
{"snippet": "convolved = tf.nn.conv2d(img_4d, z_4d,\n                         strides=[1, 1, 1, 1],\n                         padding='SAME')\nconvolved\n", "intent": "The convolution operation is performed in tensorflow using the `nn.conv2d` function\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Start a computational graph session:\n"}
{"snippet": "regr = LinearRegression()\n", "intent": "$$\ny = \\alpha + \\beta_0 x_0 + \\beta_1 x_1 + ...\n$$\n"}
{"snippet": "lm.fit(X,Y)\n", "intent": "Fit the linear model using highway-mpg.\n"}
{"snippet": " lm.fit(Z, df['price'])\n", "intent": "Fit the linear model using the four above-mentioned variables.\n"}
{"snippet": "pipe=Pipeline(Input)\npipe\n", "intent": "we input the list as an argument to the pipeline constructor \n"}
{"snippet": "pipe.fit(Z,y)\n", "intent": "We can normalize the data,  perform a transform and fit the model simultaneously. \n"}
{"snippet": "lm.fit(X, Y)\nlm.score(X, Y)\n", "intent": "Let's calculate the R^2\n"}
{"snippet": "lm.fit(Z, df['price'])\nlm.score(Z, df['price'])\n", "intent": "Let's calculate the R^2\n"}
{"snippet": "lre=LinearRegression()\n", "intent": " We create a Linear Regression object:\n"}
{"snippet": "lre.fit(x_train[['horsepower']],y_train)\n", "intent": "we fit the model using the feature horsepower \n"}
{"snippet": "batch_size = 125\nx_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nA = tf.Variable(tf.random_normal(shape=[1,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n", "intent": "Next we declare the batch size, model placeholders, model variables, and model operations.\n"}
{"snippet": "RigeModel=Ridge(alpha=0.1)\n", "intent": "Let's create a Ridge regression object, setting the regularization parameter to 0.1 \n"}
{"snippet": "RigeModel.fit(x_train_pr,y_train)\n", "intent": " Like regular regression, you can fit the model using the method **fit**.\n"}
{"snippet": "Rsqu_test=[]\nRsqu_train=[]\ndummy1=[]\nALFA=5000*np.array(range(0,10000))\nfor alfa in ALFA:\n    RigeModel=Ridge(alpha=alfa) \n    RigeModel.fit(x_train_pr,y_train)\n    Rsqu_test.append(RigeModel.score(x_test_pr,y_test))\n    Rsqu_train.append(RigeModel.score(x_train_pr,y_train))\n", "intent": " We select the value of Alfa that minimizes the test error, for example, we can use a for loop. \n"}
{"snippet": "RR=Ridge()\nRR\n", "intent": "Create a ridge regions object:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(64, activation='sigmoid', input_shape=(1000,)))\nmodel.add(Dropout(.5))\nmodel.add(Dense(32, activation='sigmoid'))\nmodel.add(Dropout(.8))\nmodel.add(Dense(num_classes, activation='softmax'))\nadam = keras.optimizers.Adam(lr=0.0005)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "history = model.fit(x_train, y_train, epochs=10, batch_size=100, verbose=0, validation_split=.1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "Conv2D(filters, kernel_size, strides, padding, activation='relu', input_shape)\n", "intent": "Then, we can create a convolutional layer by using the following format:\n"}
{"snippet": "Conv2D(64, (2,2), activation='relu')\n", "intent": "If we look up code online, it is also common to see convolutional layers in Keras in this format:\n"}
{"snippet": "MaxPooling2D(pool_size, strides, padding)\n", "intent": "Then, we can create a convolutional layer by using the following format:\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Start a computational graph.\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import MaxPooling2D\nmodel = Sequential()\nmodel.add(MaxPooling2D(pool_size=2, strides=2, input_shape=(100, 100, 15)))\nmodel.summary()\n", "intent": "We can change the arguments in the code below and check how the shape of the max pooling layer changes.\n"}
{"snippet": "from keras.preprocessing.image import ImageDataGenerator\ndatagen_train = ImageDataGenerator(\n    width_shift_range=0.1,  \n    height_shift_range=0.1,  \n    horizontal_flip=True) \ndatagen_train.fit(x_train)\n", "intent": "Feel free to modify this part to use different setting for data [augmentation](https://keras.io/preprocessing/image/).\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nmodel = Sequential()\nmodel.add(Flatten(input_shape = x_train.shape[1:])) \nmodel.add(Dense(1000, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\n", "intent": "The first model we will train is a regular vanilla neural network.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(64, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "import torch\nsample_caption = torch.Tensor(sample_caption).long()\nprint(sample_caption)\n", "intent": "Finally, in **`line 6`**, we convert the list of integers to a PyTorch tensor and cast it to [long type](http://pytorch.org/docs/master/tensors.html\n"}
{"snippet": "with tf.Session() as sess:\n    c = sess.run(1.5*b)\nprint(b)\n", "intent": "Now, let's keep exploring and add some intermediate computations:\n"}
{"snippet": "W2 = tf.get_variable('W2', shape=[1], use_resource=True)\nprint(W2)\n", "intent": "Finally, we can initialize an eager variable using the standard `get_variable` method, by specifying a flag:\n"}
{"snippet": "hid = tf.layers.Dense(units=10, activation=tf.nn.relu)\ndrop = tf.layers.Dropout()\nout = tf.layers.Dense(units=3, activation=None)\ndef nn_model(x, training=False):\n  return out(drop(hid(x), training=training))\n", "intent": "We can stack multiple layers to create more complicated models. For example, a neural network with one hidden layer and dropout in the middle:\n"}
{"snippet": "net = SingleHiddenLayerNetwork()\nopt = tf.train.AdamOptimizer(learning_rate=0.01)\nfor epoch in range(50):\n  for (xb, yb) in tfe.Iterator(train_dataset.shuffle(1000).batch(32)):\n    opt.apply_gradients(loss_grad(net, xb, yb))\n  if epoch % 10 == 0:\n    print(\"Epoch %d: Loss on training set : %f\" %\n              (epoch, loss(net, X_train, y_train).numpy()))\n", "intent": "Using the first syntax, the optimization cycle is now a trivial matter:\n"}
{"snippet": "batch_size = 100\nx_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nA = tf.Variable(tf.random_normal(shape=[2, 1]))\nb = tf.Variable(tf.random_normal(shape=[1, 1]))\n", "intent": "Set model parameters, placeholders, and coefficients.\n"}
{"snippet": "opt.apply_gradients(loss_grad(net, xb, yb), global_step=tf.train.get_or_create_global_step()) \ntf.train.get_global_step()\n", "intent": "Note that the variable is currently set to 0. If we want to update it correctly, we need to provide the global step during the optimization cycle:\n"}
{"snippet": "model.fit(train_images, train_labels, epochs=5)\n", "intent": "Next, we will train the model by using the [fit method](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n"}
{"snippet": "model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(10000,)))\nmodel.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\nmodel.add(tf.keras.layers.Dense(LABEL_DIMENSIONS, activation=tf.nn.softmax))\noptimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])\nmodel.summary()\n", "intent": "Our model is similar to the previous notebook, modified to work for a multiclass classification problem.\n"}
{"snippet": "lreg = LinearRegression()\n", "intent": "Next, we create a LinearRegression object, afterwards, type lm. then press tab to see the list of methods availble on this object.\n"}
{"snippet": "lreg.fit(X_multi,Y_target)\n", "intent": "Finally, we're ready to pass the X and Y using the linear regression object.\n"}
{"snippet": "kn = KNeighborsClassifier(n_neighbors=3,weights='uniform')\nX = affair[['education','religious']]\ny = affair['affair']\nmodel = kn.fit(X,y)\nscore = kn.score(X,y)\nprint score\n", "intent": "---\nYou should choose **2 predictor variables** to predict had affair vs. not\n"}
{"snippet": "penalty = gelr.best_params_['penalty']\nC = gelr.best_params_['C']\nelr = LogisticRegression(penalty = penalty, C=C)\nelr_model = elr.fit(Xen,ye)\n", "intent": "Wow! That was a much better score than I expected.\n"}
{"snippet": "def relu(input):\n    return max(input, 0)\n", "intent": "The 'ReLU' Activation Function outputs the maximum of (input, 0) as a linear number. \n"}
{"snippet": "node_0_input = (input_data * weights['node_0']).sum()\nnode_0_output = relu(node_0_input)\nnode_1_input = (input_data * weights['node_1']).sum()\nnode_1_output = relu(node_1_input)\nhidden_layer_outputs = np.array([node_0_output, node_1_output])\noutput = (hidden_layer_outputs * weights['output']).sum()\nprint(output)\n", "intent": "Rewriting the example above, replacing the tanh with the ReLU\n"}
{"snippet": "batch_size = 350\nx_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nprediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)\nb = tf.Variable(tf.random_normal(shape=[1,batch_size]))\n", "intent": "We declare the batch size (large for SVMs), create the placeholders, and declare the $b$ variable for the SVM model.\n"}
{"snippet": "init = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n", "intent": "Now we have to initialize the Variable in Tensorflow\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim)))\n    embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n", "intent": "Implement two helper function to initialize the RELU Nodes with small positive randomn values\n"}
{"snippet": "def conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                         strides=[1, 2, 2, 1], padding='SAME')\n", "intent": "Implement Convolution and Pooling Settings\n"}
{"snippet": "W_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n", "intent": "We add a 64 feature convolutional layer for each 5x5 patch of input data\n"}
{"snippet": "W_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n", "intent": "Here we add a softmax function to scale the values on each of the 10 classes to a value between 0 and 1.\n"}
{"snippet": "def conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                         strides=[1, 2, 3, 1], padding='SAME')\n", "intent": "Implement Convolution and Pooling Settings\n"}
{"snippet": "def get_weights(n_features, n_labels):\n    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\ndef get_biases(n_labels):\n    return tf.Variable(tf.zeros(n_labels))\ndef linear(input, w, b):\n    return tf.add(tf.matmul(input, w), b)\n", "intent": "Define function to use\n"}
{"snippet": "x = tf.constant(10)\ny = tf.constant(2)\nz = tf.subtract(tf.divide(x,y), tf.cast(tf.constant(1), tf.float64))\nwith tf.Session() as sess:\n    output = sess.run(z)\n    print(output)\n", "intent": "Other example with more operation\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Create a graph session\n"}
{"snippet": "hidden_layer = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\nhidden_layer = tf.nn.relu(hidden_layer)\noutput_layer = tf.add(tf.matmul(hidden_layer, weights['output_layer']), biases['output_layer'])\n", "intent": "* Build Network with tensorflow\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(training_epochs):\n        total_batch = int(mnist.train.num_examples/batch_size)\n        for i in range(total_batch):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            sess.run(optimizer, feed_dict={x:batch_x, y:batch_y })\n", "intent": "* Launch the session\n"}
{"snippet": "tf.reset_default_graph()\nweights = tf.Variable(tf.truncated_normal([2,3]))\nbias = tf.Variable(tf.truncated_normal([3]))\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, save_file)\n    print(\"Weights: \")\n    print(sess.run(weights))\n    print(\"Bias: \\n\") \n    print(sess.run(bias))\n", "intent": "> Dont nedd to call the ***sess.run(tf.global_variables_initializer())***\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1e9)\n", "intent": "Logistic regression can do what we just did:\n"}
{"snippet": "X = df.drop('Class', axis=1)\ny = df['Class']\nscaler.fit(X)\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "kmeans.fit(df.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "clf = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "clf.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid=param_grid, verbose=5)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Start a computational graph session.\n"}
{"snippet": "lr_model = xl.LRModel(task='reg', epoch=10, lr=0.1)\nlr_model.fit(tmp_X_train, tmp_y_train)\n", "intent": "Let's now try the linear method in xlearn\n"}
{"snippet": "lr_model = xl.create_linear()\nlr_model.setTrain(\"trainfm.txt\")\nparam = {'task':'reg', 'lr':0.1, 'epoch': 10}\nlr_model.cv(param)\n", "intent": "At this point I thought I will use these methods and optimize + cross validate, given that the library comes with a convenient `.cv` method\n"}
{"snippet": "knn.fit(X_train,y_train)\n", "intent": "** Ajuste este modelo KNN aos dados de treinamento. **\n"}
{"snippet": "nb.fit(X_train,y_train)\n", "intent": "** Agora ajuste nb usando os dados de treinamento. **\n"}
{"snippet": "nb.fit(X_train, y_train)\n", "intent": "** Agora ajuste nb usando os dados de treinamento. **\n"}
{"snippet": "lm.fit(X_train, y_train)\n", "intent": "** Treine lm nos dados de treinamento. **\n"}
{"snippet": "lm.fit(X_train,y_train)\n", "intent": "** Treine lm nos dados de treinamento. **\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\ngrid.fit(X_train,y_train)\n", "intent": "** Crie um objeto GridSearchCV e ajuste-o aos dados de treinamento. **\n"}
{"snippet": "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']),\\\n    biases['hidden_layer'])\nlayer_1 = tf.nn.relu(layer_1)\nlogits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n", "intent": "<img src=\"images/multi-layer.png\" style=\"height:150px\">\n"}
{"snippet": "x_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)\ny_target = tf.placeholder(shape=[3, None], dtype=tf.float32)\nprediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)\nb = tf.Variable(tf.random_normal(shape=[3,batch_size]))\n", "intent": "Initialize placeholders and create the variables for multiclass SVM\n"}
{"snippet": "kmeans = KMeans(n_clusters=4).fit(df_nonull[features])\ndf_nonull['cluster'] = kmeans.labels_\ncmap = {'0': 'r', '2': 'g', '1': 'b','3':'y' }\ndf_nonull['ccluster'] = df_nonull.cluster.apply(lambda x: cmap[str(x)])\ndf_nonull.plot('Age', 'Pclass', kind='scatter', c=df_nonull.ccluster)\n", "intent": "Observations: Looks like there is an elbow at 4?? What does the plot of 4 clusters look like...\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier()\nforest.fit(xtrain, ytrain)\nprint(forest.score(xtrain, ytrain))\nprint(forest.score(xtest, ytest))\n", "intent": "Interestingly, the training accuracy declined but the test accuracy improved, leading to a better model with less overfititng.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(criterion='gini', class_weight='balanced')\nparam_grid = {'n_estimators' : [40, 50, 60], 'min_samples_split' : [2, 3, 4], \n              'max_depth' : [4, 7, 10]}\nrf_cv = GridSearchCV(rf, param_grid, cv = 5)\nrf_cv.fit(X_train, y_train)\nPrint out the best model\nprint('Best RF Params: {}'.format(rf_cv.best_params_))\nprint('Best RF Score : %f' % rf_cv.best_score_)\n", "intent": "I will us scikit-learn's Random Forest Classifier to build a prediction model for active status.\n"}
{"snippet": "def linear_model(img):\n  X = tf.reshape(img, [-1, HEIGHT*WIDTH]) \n  W = tf.Variable(tf.truncated_normal([HEIGHT*WIDTH, NCLASSES], stddev=0.1))\n  b = tf.Variable(tf.truncated_normal([NCLASSES], stddev=0.1))\n  ylogits = tf.matmul(X, W) + b\n  return ylogits, NCLASSES\n", "intent": "Let's start with a very simple linear classifier. All our models will have this basic interface -- they will take an image and return logits.\n"}
{"snippet": "def linear_model(img, mode):\n  X = tf.reshape(img, [-1, HEIGHT*WIDTH]) \n  W = tf.Variable(tf.truncated_normal([HEIGHT*WIDTH, NCLASSES], stddev=0.1))\n  b = tf.Variable(tf.truncated_normal([NCLASSES], stddev=0.1))\n  ylogits = tf.matmul(X, W) + b\n  return ylogits, NCLASSES\n", "intent": "A simple low-level matrix multiplication\n"}
{"snippet": "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n", "intent": "Without limiting the depth, the DT will be evolved until perfect accuracy.\nBut not really useful &rarr; Over-training\nBetter approach:\n"}
{"snippet": "from sklearn.manifold import Isomap\niso = Isomap(n_components=2)\niso.fit(digits.data)\nXI_2D = iso.transform(digits.data)\n", "intent": "Some digits are nicely isolated, others less so\nThink about it, which digits tend to look similar?\n***\nAlternative method for dimension reduction:\n"}
{"snippet": "xy_pca = pca.fit(xy)\n", "intent": "What does the following code do?\n"}
{"snippet": "model = LinearRegression()\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "import tensorflow as tf\nsess = tf.Session()\n", "intent": "This notebook illustrates how to use the Levenstein distance (edit distance) in TensorFlow.\nGet required libarary and start tensorflow session.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "grid_search = GridSearchCV(SVC(), param_grid, verbose=2)\ngrid_search.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nm = LogisticRegression()\nm.fit(Xtrain, ytrain)\n", "intent": "Create a Machine Learning model using logistic regression and fit it with the training data:\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nm = RandomForestClassifier()\n", "intent": "Let's try a different model: The Random Forest (an **ensemble of decision trees**)\n"}
{"snippet": "m1 = RandomForestClassifier(max_depth=2)\nm2 = RandomForestClassifier(max_depth=3)\nm3 = RandomForestClassifier(max_depth=10)\n", "intent": "Compare how the following parameters affect prediction quality:\n"}
{"snippet": "m.fit(Xtrain, ytrain)\nm.score(Xtrain, ytrain)\n", "intent": "Fit the Random Forest model to the training data yourself and evaluate it on the test set.\n"}
{"snippet": "m = RandomForestClassifier(max_depth=2)\nm.fit(Xtrain, ytrain)\nm.score(Xtrain, ytrain), m.score(Xtest, ytest)\n", "intent": "Limiting the complexity of a model is called **regularization**\n"}
{"snippet": "ad_all_ols = sm.ols(formula=\"Sales ~ TV + Radio + Newspaper\", data=advert).fit()\nad_all_ols.summary()\n", "intent": "**Model:**\n$$\nSales = \\beta_0 + \\beta_1 * TV + \\beta_2*Radio + \\beta_3*Newspaper. \n$$\n"}
{"snippet": "x = tf.Variable(3.0)\ny = tf.Variable(4.0)\nf = x + 2*y*y + 2\ngrads = tf.gradients(f,[x,y])\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer()) \n    print([g.eval() for g in grads])\n", "intent": "TensorFlow can automatically compute the derivative of functions using [```gradients```](https://www.tensorflow.org/api_docs/python/tf/gradients). \n"}
{"snippet": "sess = tf.Session()\n", "intent": "We start a computation graph session.\n"}
{"snippet": "x = tf.Variable(3.0, trainable=True)\ny = tf.Variable(2.0, trainable=True)\nf = x*x + 100*y*y\nopt = tf.train.MomentumOptimizer(learning_rate=1e-2,momentum=.5).minimize(f)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(1000):\n        if i%100 == 0: print(sess.run([x,y,f]))\n        sess.run(opt)\n", "intent": "+ [```MomentumOptimizer```](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer)\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, save_path)\n    X_new_scaled = mnist.test.images\n    Z = logits.eval(feed_dict={X: X_new_scaled})\n    y_pred = np.argmax(Z, axis=1)\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(mnist.test.labels,y_pred))\n", "intent": "We can also print the confusion matrix using [```confusion_matrix```](https://www.tensorflow.org/api_docs/python/tf/confusion_matrix). \n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1)\nlogreg.fit(features_train, target_train)\n", "intent": "Let's start with a simple model from sklearn, namely `LogisticRegression`:\n"}
{"snippet": "def flatten(array):\n    out = []\n    for item in array:\n        out += item\n    return out\n", "intent": "Regroup indices such that the distribution is more less equalized\n"}
{"snippet": "def linear(x, alpha, beta):\n    return f\n", "intent": "In the cell below, define the linear function\n$$f = \\alpha + \\beta\\hspace{1pt}x$$\n"}
{"snippet": "def linear(x, alpha, beta):\n    return alpha + (beta * x)\n", "intent": "In the cell below, define the linear function\n$$f = \\alpha + \\beta\\hspace{1pt}x$$\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=5,\n                           weights='uniform',\n                           p=2,\n                           metric='minkowski')\nknn.fit(X,y)\n", "intent": "Build a logit model and fit\n"}
{"snippet": "model = LogisticRegression()\nmodel.fit(X, y)\n", "intent": "Then let's initialize a logistic regression model:\n"}
{"snippet": "db = DBSCAN(eps=0.5, min_samples=5).fit(x)\ncore_samples = db.core_sample_indices_\nlabels = db.labels_\nprint core_samples\nprint labels\n", "intent": "Next, we'll find the labels calculated by DBSCAN\n"}
{"snippet": "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\nprediction = tf.sigmoid(model_output)\nmy_opt = tf.train.GradientDescentOptimizer(0.001)\ntrain_step = my_opt.minimize(loss)\ninit = tf.global_variables_initializer()\nsess.run(init)\n", "intent": "Now we declare our loss function (which has the sigmoid built in), prediction operations, optimizer, and initialize the variables.\n"}
{"snippet": "session = tf.Session()\nsession.run(tf.global_variables_initializer())\n", "intent": "<h6> Feel free to change the values of \"m\" and \"c\" in future to check how the initial position of line changes </h6>\n"}
{"snippet": "W_fc1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))\nb_fc1 = tf.Variable(tf.constant(0.1, shape=[1024])) \n", "intent": "Composition of the feature map from the last layer (7x7) multiplied by the number of feature maps (64); 1027 outputs to Softmax layer\n"}
{"snippet": "y_CNN= tf.nn.softmax(fc)\ny_CNN\n", "intent": "__softmax__ allows us to interpret the outputs of __fcl4__ as probabilities. So, __y_conv__ is a tensor of probablities.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Lets start with a new session:\n"}
{"snippet": "cells = []\nfor _ in range(num_layers):\n    cell = tf.contrib.rnn.LSTMCell(LSTM_CELL_SIZE)\n    cells.append(cell)\nstacked_lstm = tf.contrib.rnn.MultiRNNCell(cells)\n", "intent": "Lets create the stacked LSTM cell:\n"}
{"snippet": "data = tf.placeholder(tf.float32, [None, None, input_dim])\noutput, state = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)\n", "intent": "Now we can create the RNN:\n"}
{"snippet": "set_rf_samples(6000)\nm = RandomForestRegressor(n_estimators=7000,max_features=0.15,n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\n", "intent": "1) 40 Trees is as good as we get\n"}
{"snippet": "learn.fit(0.005,5)\n", "intent": "0.1 - e-1\n0.01 - e-2\n0.001 - e-3\n"}
{"snippet": "for k in range(1, 30)[::-1]:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(wine_df, y)\n    print(k, knn.score(wine_df, y))\n", "intent": "Using the best features, explore what the best value of `k` is for this dataset. \n"}
{"snippet": "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\nnce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n                                               stddev=1.0 / np.sqrt(embedding_size)))\nnce_biases = tf.Variable(tf.zeros([vocabulary_size]))\nx_inputs = tf.placeholder(tf.int32, shape=[batch_size])\ny_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\nvalid_dataset = tf.constant(valid_examples, dtype=tf.int32)\nembed = tf.nn.embedding_lookup(embeddings, x_inputs)\n", "intent": "Next we define our model and placeholders.\n"}
{"snippet": "knn5 = KNeighborsClassifier(n_neighbors=5, weights='uniform')\nscores = accuracy_crossvalidator(Xs, y, knn5, cv_indices)\n", "intent": "<a id='cv-knn5'></a>\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nclf_dtc = DecisionTreeClassifier()\nclf_dtc.fit(X_train, y_train)\n", "intent": "Let's fit a decision tree classifier.\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nclf_gnb = GaussianNB()\nclf_gnb.fit(X, y)\n", "intent": "Let's fit a Gaussian naive Bayes classifier.\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, verbose = 2)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "dtree = DecisionTreeClassifier()\ndtree.fit(X_train, y_train)\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "reg = LinearRegression()\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "reg.fit(X_train, y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=2,\n         validation_data=(x_test,y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "pipeline = Pipeline(screen=universe)\npipeline.add(\n    mean_reversion_5day_sector_neutral_smoothed(5, universe, sector),\n    'Mean_Reversion_5Day_Sector_Neutral_Smoothed')\nengine.run_pipeline(pipeline, factor_start_date, universe_end_date)\n", "intent": "Let's see what some of the smoothed data looks like.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Next we start a computational graph session.\n"}
{"snippet": "from IPython.display import display\nfrom sklearn.tree import DecisionTreeClassifier\nclf_random_state = 0\nsimple_clf = DecisionTreeClassifier(\n    max_depth=3,\n    criterion='entropy',\n    random_state=clf_random_state)\nsimple_clf.fit(X_train, y_train)\ndisplay(project_helper.plot_tree_classifier(simple_clf, feature_names=features))\nproject_helper.rank_features_by_importance(simple_clf.feature_importances_, features)\n", "intent": "Let's see how a single tree would look using our data.\n"}
{"snippet": "train_score = []\nvalid_score = []\noob_score = []\nfor n_trees in tqdm(n_trees_l, desc='Training Models', unit='Model'):\n    clf = bagging_classifier(n_trees, 0.2, 1.0, clf_parameters)\n    clf.fit(X_train, y_train)\n    train_score.append(clf.score(X_train, y_train.values))\n    valid_score.append(clf.score(X_valid, y_valid.values))\n    oob_score.append(clf.oob_score_)\n", "intent": "With the bagging classifier built, lets train a new model and look at the results.\n"}
{"snippet": "p.show_graph(format='png')\n", "intent": "Note that you can right-click the image and view in a separate window if it's too small.\n"}
{"snippet": "x = np.linspace(min(acc), max(acc))\ne = np.empty((samps,len(x)))\nfor sim_dataset in range(samps):\n    acc = ppc['acc'][sim_dataset,1,:].flatten()\n    ecdf = sm.distributions.empirical_distribution.ECDF(acc)\n    e[sim_dataset,:] = ecdf(x)\n", "intent": "Empirical CDF generated from Posterior\n"}
{"snippet": "import statsmodels.api as sm\nX = np.array(xs).transpose()\nX = sm.add_constant(X)\nmod = sm.OLS(ys, X) \nres = mod.fit()\nprint res.summary()\n", "intent": "This generated a fit of $y = 3 x + 1$. Let's see what a linear regression yields.\n"}
{"snippet": "import sklearn.linear_model as lm\nmodel = lm.LogisticRegression()\nfeatures = ['CRS_DEP_TIME', 'dow_1', 'dow_2', 'dow_3', 'dow_4', 'dow_5', 'dow_6']\nX = df[features]\ny = df['DEP_DEL15']\nmodel.fit(X, y)\n", "intent": "Run a logistic regression model to predict delays:\n"}
{"snippet": "k = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(dfn)\n", "intent": "Cluster the Data - We are going to use 5 clusters based off of the above scatterplot\n"}
{"snippet": "kmeans = KMeans(n_clusters=6)\nclusters = kmeans.fit(X_scale)\n", "intent": "Set up the k-means clustering analysis. Use the graph from above to derive \"k\"\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(500, activation = 'relu', input_shape = (1000,)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation = 'sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer = 'adam',\n              metrics = ['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "logistic_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=tf.cast(log_y_target, tf.float32)))\nprediction = tf.round(tf.sigmoid(model_output))\npredictions_correct = tf.cast(tf.equal(prediction, tf.cast(log_y_target, tf.float32)), tf.float32)\naccuracy = tf.reduce_mean(predictions_correct)\nlogistic_opt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\nlogistic_train_step = logistic_opt.minimize(logistic_loss, var_list=[A, b])\ninit = tf.global_variables_initializer()\nsess.run(init)\n", "intent": "Loss function, Prediction function, Optimization function and variable initializer.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nprint('Accuracy of Logistic regression classifier on training set: {:.2f}'\n     .format(logreg.score(X_train, y_train)))\nprint('Accuracy of Logistic regression classifier on test set: {:.2f}'\n     .format(logreg.score(X_test, y_test)))\n", "intent": "Logistic Regression Model Accuracy\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier().fit(X_train, y_train)\nprint('Accuracy of Random Forest classifier on training set: {:.2f}'\n     .format(clf.score(X_train, y_train)))\nprint('Accuracy of Random Forest classifier on test set: {:.2f}'\n     .format(clf.score(X_test, y_test)))\n", "intent": "Random Forest Model Accuracy\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nprint('Accuracy of K-NN classifier on training set: {:.2f}'\n     .format(knn.score(X_train, y_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'\n     .format(knn.score(X_test, y_test)))\n", "intent": "K Nearest Neighbors Model Accuracy\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nlasso = Lasso()\nalphas = [0.5, 1, 4, 10, 50, 100]\nparam_grid = [\n    {'alpha': alphas}\n]\ngrid_search = GridSearchCV(lasso, param_grid, cv = 5, scoring = 'neg_mean_squared_error')\ngrid_search.fit(housing_prep, y)\n", "intent": "<b>CROSS-VALIDATION / GRIDSEARCH </B>\n"}
{"snippet": "tf.reset_default_graph()\nweights = tf.Variable(tf.truncated_normal([2, 3]))\nbias = tf.Variable(tf.truncated_normal([3]))\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, './model.ckpt')\n    print('Weight:')\n    print(sess.run(weights))\n    print('Bias:')\n    print(sess.run(bias))\n", "intent": "<font size=3>\nNow that the Tensor Variables are saved, let's load them back into a new model.\n"}
{"snippet": "def build_output(lstm_output, in_size, out_size):\n    seq_output = tf.concat(lstm_output, axis=1)\n    x = tf.reshape(seq_output, [-1,in_size]) \n    with tf.variable_scope('softmax'):\n        softmax_w = tf.Variable(tf.truncated_normal([in_size,out_size], stddev=0.1, dtype=tf.float32))\n        softmax_b = tf.Variable(tf.zeros(out_size))\n    logits = tf.matmul(x, softmax_w) + softmax_b\n    out = tf.nn.softmax(logits, name='prediction')\n    return out, logits\n", "intent": "<font size=5>If you have some questions about **LSTM structure**, check discussion here </font> \n"}
{"snippet": "vr_by_f1_features_to_test = []\nvr_by_f1_features_test_results = {}\nfor i, feature in tqdm(enumerate(vr_by_f1_performant_features)):\n    vr_by_f1_features_to_test.append(feature)\n    vr_by_f1_features_test_results[feature] = run_model(LogisticRegression(), 'logit', 100,\n                                                        adult_train_df[vr_by_f1_features_to_test],\n                                                        adult_train_target)\n", "intent": "Add one feature at a time.\n"}
{"snippet": "gspipe.fit(X_train, y_train.ravel())\n", "intent": "This will take a while, and you may see some errors...\n"}
{"snippet": "losses = []\ntest_beta_1 = list(range(-5,5))\ntest_beta_2 = list(range(-5,5))\nfor beta_2 in test_beta_2:\n    for beta_1 in test_beta_1:\n        ptron = Perceptron((beta_1, beta_2), 0.5)\n        losses.append(ptron.loss(xs, ys))\n", "intent": "Let's change our weights and collect the scores.\n"}
{"snippet": "prediction = tf.nn.softmax(model_output)\ntest_prediction = tf.nn.softmax(test_model_output)\ndef get_accuracy(logits, targets):\n    batch_predictions = np.argmax(logits, axis=1)\n    num_correct = np.sum(np.equal(batch_predictions, targets))\n    return(100. * num_correct/batch_predictions.shape[0])\n", "intent": "We also create a prediction and accuracy function for evaluation on the train and test set.\n"}
{"snippet": "sess=tf.Session()\n", "intent": "create a tensorflow session\n"}
{"snippet": "tree=DecisionTreeClassifier()\ntree.fit(X_train,y_train)\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "kmeans=KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS',axis=1))  \n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "KNN=KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "lm=LinearRegression()\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "param_grid={'C':[0.1,1,10,100],'gamma':[1, 0.1, 0.01, 0.001]}\ngrid_model=GridSearchCV(SVC(),param_grid,verbose=2)\n", "intent": "**Create a dictionary called param_grid and fill out some parameters for C and gamma.**\n"}
{"snippet": "grid_model.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embeded = tf.nn.embedding_lookup(embedding, input_data)\n    return embeded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "initial = tf.random_normal(shape) * 0.256\nimage = tf.Variable(initial)\nvgg_net = vgg_network(network_weights, image)\n", "intent": "Make Combined Image\n"}
{"snippet": "model = KMeans(n_clusters=3)\n", "intent": "**Step 3:** Create a `KMeans` model called `model` with `3` clusters.\n"}
{"snippet": "kmeans = KMeans(n_clusters=4)\n", "intent": "**Step 6:** Create an instance of `KMeans` with `4` clusters called `kmeans`.\n"}
{"snippet": "pipeline.fit(samples)\n", "intent": "**Step 3:** Fit the pipeline to the fish measurements `samples`.\n"}
{"snippet": "kmeans = KMeans(n_clusters=14)\n", "intent": "**Step 4:** Create an instance of `KMeans` called `kmeans` with `14` clusters.\n"}
{"snippet": "pipeline = make_pipeline(normalizer, kmeans)\n", "intent": "**Step 5:** Using `make_pipeline()`, create a pipeline called `pipeline` that chains `normalizer` and `kmeans`.\n"}
{"snippet": "pipeline.fit(movements)\n", "intent": "**Step 6:** Fit the pipeline to the `movements` array.\n"}
{"snippet": "model2 = Models.deep_cnn_model(flag_BN=True,flag_STN=True)\nhistory2,evaluate2= train_model_(model2,\"BN_STN_001\",flag_earlystop=False,flag_reduceLR=False, flag_lrsched=False,flag_tensorboard=True)\n", "intent": "* Train Accuracy : 99.54 \n* Valid Accuracy : 99.27\n"}
{"snippet": "model4 = Models.deep_cnn_model(flag_BN=True,flag_STN=True)\nhistory4,evaluate4= train_model_(model4,\"BN_STN_lrsched\",flag_earlystop=False,flag_reduceLR=False, flag_lrsched=True,flag_tensorboard=True)\n", "intent": "lr = (lr*(0.1 ** int(epoch / 4)))\n* Train Accuracy : 99.52 \n* Valid Accuracy : 99.57\n"}
{"snippet": "model5 = Models.deep_cnn_model(flag_BN=True,flag_STN=True)\nhistory5,evaluate5= train_model_(model5,\"BN_STN_lrsched_2\",flag_earlystop=False,flag_reduceLR=False, flag_lrsched=True,flag_tensorboard=True)\n", "intent": " return lr * ( 0.5 ** int(epoch / 5))\n* Train Accuracy = 99.87\n* Valid Accuracy = 99.57\n"}
{"snippet": "lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n                        training_seq_len, vocab_size)\nwith tf.variable_scope(tf.get_variable_scope(), reuse=True):\n    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n                                 training_seq_len, vocab_size, infer_sample=True)\n", "intent": "In order to use the same model (with the same trained variables), we need to share the variable scope between the trained model and the test model.\n"}
{"snippet": "svc_model = model_performance(SVC(probability=True))\n", "intent": "In this section we'll evaluate the performance of a support vector machine.\n"}
{"snippet": "parameters = {'C' : [0.001, 0.1, 1, 10, 100], 'penalty' : ['l1', 'l2']}\nmodel = model_performance(GridSearchCV(LogisticRegression(random_state=42), param_grid=parameters, cv=skf), cv=True)\n", "intent": "Without any optimization, we can predict patients who will convert to AD with 89% accuracy. Now, the classifier will be optimized.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nmodel_eval(RandomForestClassifier())\n", "intent": "Using the out of the box classifier works better on non TD-IDF data.\n"}
{"snippet": "reg = RandomForestRegressor(random_state = 0, max_features = 0.85, min_samples_leaf = 9,n_estimators = 685, n_jobs = -1)\nreg.fit(X_train, y_train)\n", "intent": "Fitting model on training set\n"}
{"snippet": "regressor = LinearRegression(fit_intercept = True)\nregressor.fit(X_train, y_train)\n", "intent": "Fitting model on training set\n"}
{"snippet": "log = model.fit(X_train, y_train,nb_epoch=30, batch_size=32, verbose=2,validation_data=(X_test, y_test))\n", "intent": "Finally, I proceed to train the model with a simple one line command:\n"}
{"snippet": "class DQNAgent(RLDebugger):\n    def __init__(self, observation_space, action_space):\n        self.learning_rate = ??? \n    def build_model(self):\n        model = Sequential()\n        model.add(Dense(???, input_dim=self.state_size, activation=???))\n        model.add(Dense(self.action_size, activation=???))\n        model.compile(loss=???, optimizer=Adam(lr=self.learning_rate))\n        model.summary()\n        return model\n", "intent": "  - Skeleton provided, implemented with [Keras](https://keras.io/)\n"}
{"snippet": "history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\n", "intent": "Train the network for 20 epochs with a batch size of 512. Save the object as `history`.\n"}
{"snippet": "model_a = sm.OLS.from_formula(\"SalePrice ~ scale(OverallQual) + scale(OverallCond) + scale(GrLivArea) + scale(I(GrLivArea**2)) + scale(I(GrLivArea**3)) + scale(KitchenQual) + scale(I(KitchenQual**2)) + scale(I(KitchenQual**3)) + scale(GarageCars) + scale(BsmtQual) + scale(YearBuilt) + C(Neighborhood) + C(MSZoning)\", data=df)\nresult_a = model_a.fit()\nprint(result_a.summary())\n", "intent": "$$ y = OQ + OC + GA + GA^2 + GA^3 + KQ +KQ^2 +KQ^3 + GC + BQ + YB + Category $$\n"}
{"snippet": "lstm_model = LSTM_Model(rnn_size, num_layers, batch_size, learning_rate,\n                        training_seq_len, vocab_size)\nwith tf.variable_scope(tf.get_variable_scope(), reuse=True):\n    test_lstm_model = LSTM_Model(rnn_size,num_layers, batch_size, learning_rate,\n                                 training_seq_len, vocab_size, infer_sample=True)\n", "intent": "Initialize the LSTM Model\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_dim=1000))\nmodel.add(Dropout(.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=200, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "rm , predictions, mse, score = regression_model(['exchange_vol_usd_log'])\nprint_stats('Exchange Volume in USD', mse, score) \nrm , predictions, mse, score = regression_model(['ma_price_7'])\nprint_stats('Seven-day moving average of bitcoin price', mse, score)\nrm , predictions, mse, score = regression_model(['ma_price_3'])\nprint_stats('Three-day moving average of bitcoin price', mse, score)\nrm , predictions, mse, score = regression_model(['price_usd'])\nprint_stats('Daily price of bitcoin', mse, score)\ndraw_scatterplot('price_usd', rm, predictions)\n", "intent": "Try out different models parameters.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nk_range = range(1, 51, 2)\nparam_grid = {'n_neighbors' : k_range}\ngrid = GridSearchCV(knn, param_grid, n_jobs=4)\ngrid.fit(X, y)\nprint grid.grid_scores_\nprint grid.best_score_\nprint grid.best_estimator_\nprint grid.best_params_\n", "intent": "Use grid search to find the optimal value of k.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=4, n_neighbors=7, p=2,\n           weights='uniform')\nknn.fit(X, y)\n", "intent": "With k=7, we get accuracy of 35%. Random accuracy would have been 2.6%.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nparam_grid = {'max_depth': np.arange(3, 10)}\ngrid = GridSearchCV(dtc, param_grid, n_jobs=2)\ngrid.fit(X, y)\nprint grid.grid_scores_\nprint grid.best_score_\nprint grid.best_estimator_\nprint grid.best_params_\n", "intent": "Grid search to optimize depth of the tree.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nk_range = range(1, 51, 2)\nparam_grid = {'n_neighbors' : k_range}\ngrid = GridSearchCV(knn, param_grid, n_jobs=2)\ngrid.fit(X, y)\nprint grid.grid_scores_\nprint grid.best_score_\nprint grid.best_estimator_\nprint grid.best_params_\n", "intent": "Use grid search to find the optimal value of k.\n"}
{"snippet": "clf = SVC()\nclf.fit(X_train, y_train)\n", "intent": "Other classification algorithms: http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n"}
{"snippet": "features = df.drop('Class', axis=1)\nscaler.fit(features)\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Now we start a computational graph session.\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, verbose=3)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)    \n", "intent": "- \"Estimator\" is scikit-learn's term for model\n- \"Instantiate\" means \"make an instance of\"\n"}
{"snippet": "knn.fit(X,y)  \n", "intent": "- Model is learning the relationship between X and y\n- Occurs in-place so need to assign the result to another object\n"}
{"snippet": "images, labels = next(iter(trainloader))\nprint(labels[0])\nimg = images[0].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "model.fit(x_train, y_train,\n          epochs=10, \n          batch_size=32,\n          validation_data = (x_test,y_test),\n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "x1 = tf.Variable(1)\nx.graph is tf.get_default_graph()\n", "intent": "Any node you create is automatically added to the default graph:\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing.data, housing.target.reshape(-1, 1))\nprint(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])\n", "intent": "And with scikit learn\n"}
{"snippet": "c_values = np.arange(0.01,10,0.01)\nlogreg_parameters = {'penalty':['l1','l2'], 'C': c_values}\nmodel_GS1 = GridSearchCV(LogisticRegression(),logreg_parameters,cv=5)\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "svm_clf = SVC(decision_function_shape=\"ovr\")\nsvm_clf.fit(X_train[:7500], np.ravel(y_train[:7500]))\n", "intent": "Now I will test SVC.\n"}
{"snippet": "x_graph_input = tf.placeholder(tf.float32, [None])\ny_graph_input = tf.placeholder(tf.float32, [None])\nm = tf.Variable(tf.random_normal([1], dtype=tf.float32), name='Slope')\noutput = tf.multiply(m, x_graph_input, name='Batch_Multiplication')\nresiduals = output - y_graph_input\nl1_loss = tf.reduce_mean(tf.abs(residuals), name=\"L1_Loss\")\nmy_optim = tf.train.GradientDescentOptimizer(0.01)\ntrain_step = my_optim.minimize(l1_loss)\n", "intent": "Now we declare the placeholders, variables, model operations, loss, and optimization function.\n"}
{"snippet": "tf.reset_default_graph()\nbatch_size = 32\nwith tf.device('/cpu:0'):\n    images, _, _ = data_provider.provide_data('train', batch_size, MNIST_DATA_DIR)\nimgs_to_visualize = tfgan.eval.image_reshaper(images[:20,...], num_cols=10)\nvisualize_digits(imgs_to_visualize)\n", "intent": "<a id='unconditional_input'></a>\n"}
{"snippet": "tf.reset_default_graph()\nbatch_size = 32\nwith tf.device('/cpu:0'):\n    images, one_hot_labels, _ = data_provider.provide_data('train', batch_size, MNIST_DATA_DIR)\nimgs_to_visualize = tfgan.eval.image_reshaper(images[:20,...], num_cols=10)\nvisualize_digits(imgs_to_visualize)\n", "intent": "<a id='conditional_input'></a>\n"}
{"snippet": "tf.reset_default_graph()\nbatch_size = 32\nwith tf.device('/cpu:0'):\n    images, _, _ = data_provider.provide_data('train', batch_size, MNIST_DATA_DIR)\nimgs_to_visualize = tfgan.eval.image_reshaper(images[:20,...], num_cols=10)\nvisualize_digits(imgs_to_visualize)\n", "intent": "<a id='infogan_input'></a>\nThis is the same as the unconditional case (we don't need labels).\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1),dtype=tf.float32)\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def k_means(df):\n    x1=np.array(df[\"t_summer\"])\n    x2=np.array(df[\"t_winter\"])\n    x = np.vstack((x1, x2)).T\n    kmeans=KMeans(init=\"k-means++\").fit(x)\n    l=kmeans.labels_\n    k=set(l)\n    c=kmeans.cluster_centers_\n    return k,c\n", "intent": "Write a function to perform k-means clustering for the given dataset.\nCreate a fucntion k_means()\nReturn\nOptimum value of k\nCluster centers\n"}
{"snippet": "if USE_PRETRAINED:\n    with open('pretrained/xgb_grid_search_eta_50.pkl', 'rb') as f:\n        grid = pickle.load(f)\nelse:      \n    xgb_param_grid = {'eta':[0.5,0.4,0.3,0.2,0.1,0.075,0.05,0.04,0.03]}\n    grid = GridSearchCV(XGBoostRegressor(num_boost_round=50, gamma=0.2, max_depth=8, min_child_weight=6,\n                                         colsample_bytree=0.6, subsample=0.9,nthread=10),\n                        param_grid=xgb_param_grid, cv=5, scoring=mae_scorer)\n    grid.fit(train_x, train_y.values)\n", "intent": "First, we plot different learning rates for a simpler model (50 trees):\n"}
{"snippet": "if USE_PRETRAINED:\n    with open('pretrained/xgb_grid_search_eta_100.pkl', 'rb') as f:\n        grid = pickle.load(f)\nelse:\n    xgb_param_grid = {'eta':[0.5,0.4,0.3,0.2,0.1,0.075,0.05,0.04,0.03]}\n    grid = GridSearchCV(XGBoostRegressor(num_boost_round=100, gamma=0.2, max_depth=8, min_child_weight=6,\n                                         colsample_bytree=0.6, subsample=0.9,nthread=10),\n                        param_grid=xgb_param_grid, cv=5, scoring=mae_scorer)\n    grid.fit(train_x, train_y.values)\n", "intent": "Now, replicate the process for `num_boost_round=100`. We effectively double the amount of trees, let's see how `eta` should be changed.\n"}
{"snippet": "def mlp_model():\n    model = Sequential()\n    model.add(Dense(1024, input_dim=xtr.shape[1]))\n    model.add(Activation('relu'))\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dense(1))\n    model.compile(loss='mae', optimizer='adam')\n    return model\n", "intent": "We initialize a wide3-layer model with 1024-512 units in hidden layers:\n"}
{"snippet": "mlp = mlp_model()\nif USE_PRETRAINED:\n    with open('pretrained/mlp_v4_hist.pkl', 'rb') as f:\n        hist = pickle.load(f)\nelse:\n    sys.stdout = open('mlp_v4_out.txt', 'w')\n    fit = mlp.fit(xtr, ytr, batch_size=128, validation_data=(xte,yte),\n                  nb_epoch=40, verbose=1)\n    hist = fit.history\n", "intent": "We train the model and visualize the results:\n"}
{"snippet": "tf.reset_default_graph()\nx=tf.placeholder(tf.float32,shape=(None,n_steps,n_inputs),name=\"X\")\nbasic_cell=tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\noutputs,states=tf.nn.dynamic_rnn(basic_cell,x,dtype=tf.float32)\ninit=tf.global_variables_initializer()\n", "intent": "* Using Dynamic RNN to let tensorflow take care of stacking and unstacking of the inputs and outputs \n"}
{"snippet": "if USE_PRETRAINED:\n    with open('pretrained/xgb_grid_search_eta_100.pkl', 'rb') as f:\n        grid = pickle.load(f)\nelse:\n    xgb_param_grid = {'eta':[0.5,0.4,0.3,0.2,0.1,0.075,0.05,0.04,0.03]}\n    grid = GridSearchCV(XGBoostRegressor(num_boost_round=100, gamma=0.2, max_depth=8, min_child_weight=6,\n                                         colsample_bytree=0.6, subsample=0.9),\n                        param_grid=xgb_param_grid, cv=5, scoring=mae_scorer)\n    grid.fit(train_x, train_y.values)\n", "intent": "Now, replicate the process for `num_boost_round=100`. We effectively double the amount of trees, let's see how `eta` should be changed.\n"}
{"snippet": "from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = ExtraTreesClassifier(n_estimators=200)\nclf = clf.fit(train, targets)\n", "intent": "Tree-based estimators can be used to compute feature importances, which in turn can be used to discard irrelevant features.\n"}
{"snippet": "model = RandomForestClassifier(n_estimators=100)\nmodel.fit( train_X , train_y )\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))\nplot_model_var_imp(model, train_X, train_y)b\n", "intent": "Try a random forest model by running the cell below. \n"}
{"snippet": "model = SVC()\nmodel.fit( train_X , train_y )\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))\n", "intent": "Try a Support Vector Machines model by running the cell below. \n"}
{"snippet": "model = GradientBoostingClassifier()\nmodel.fit( train_X , train_y )\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))\nplot_model_var_imp(model, train_X, train_y)\n", "intent": "Try a Gradient Boosting Classifier model by running the cell below. \n"}
{"snippet": "model = KNeighborsClassifier(n_neighbors = 3)\nmodel.fit( train_X , train_y )\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))\n", "intent": "Try a k-nearest neighbors model by running the cell below. \n"}
{"snippet": "model = GaussianNB()\nmodel.fit( train_X , train_y )\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))\n", "intent": "Try a Gaussian Naive Bayes model by running the cell below. \n"}
{"snippet": "model = LogisticRegression()\nmodel.fit( train_X , train_y )\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))\n", "intent": "Try a Logistic Regression model by running the cell below. \n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nfrom sklearn.cross_validation import KFold\npredictors = [\"Pclass\", \"Sex\", \"Age\",\"SibSp\", \"Parch\", \"Fare\",\n              \"Embarked\",\"NlengthD\", \"FsizeD\", \"Title\",\"Deck\"]\ntarget=\"Survived\"\nalg = LinearRegression()\nkf = KFold(titanic.shape[0], n_folds=3, random_state=1)\npredictions = []\n", "intent": "*Linear Regression*\n-------------------\n"}
{"snippet": "clf = RandomForestRegressor(max_depth=2, random_state=0)\nltr = PointWiseLTRModel(clf)\nltr._train(train_X, train_y)\n", "intent": "Set `max_depth` roughly to the square root of the number features\n"}
{"snippet": "lm.fit(X, bos.PRICE)\nlm\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "model = SVC()\n", "intent": "Now we create a Support Vector Classification model for the data.\n"}
{"snippet": "model.fit(X_train,Y_train)\n", "intent": "Now we fit our model using the training data set:\n"}
{"snippet": "lm = LinearRegression()\nlm.fit(train_data, train_labels)\n", "intent": "Build a linear regression model for all the points of the complete cases.\n"}
{"snippet": "analyze_single_model(\"pickles/net-base.pickle\")\n", "intent": "This is the same model as above, but run for a maximum 10,000 epochs (it did hit early stopping around 9000).\n"}
{"snippet": "pipeline.fit(X_train,y_train)\n", "intent": "Excellent! Now fit the pipeline just like you would a regular classifier on the training set\n"}
{"snippet": "clf_grid = GridSearchCV(pipeline, param_grid)\nclf_grid.fit(X_train,y_train)\n", "intent": "Great, now instantiate the grid search object and fit it on the training set \n"}
{"snippet": "svm = LinearSVC(C=0.1)\n", "intent": "1) Instantiate an object and set the parameters\n"}
{"snippet": "arr = np.arange(24).reshape(2,3,4)\nprint(\"In NumPy:\", arr.shape,arr,sep=\"\\n\")\nshape_op = tf.shape(arr)\nprint(\"In TensorFlow:\", shape_op, sep=\"\\n\")\nshape = tf.Session().run(shape_op)\nprint(\"Shape of tensor: \" + str(shape))\n", "intent": "You can use the `tf.shape` Operation to get the shape value of `Tensor` objects:\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "**Creating an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "grad_graph = tf.Graph()\nwith grad_graph.as_default():\n    a = tf.Variable(3.0)\n    b = tf.square(a) \n    opt = tf.train.GradientDescentOptimizer(0.05)\n    grads = opt.compute_gradients(b, [a])\n    init = tf.global_variables_initializer()\nwith tf.Session(graph=grad_graph) as session:\n    session.run(init)\n    print(session.run(grads))\n", "intent": "The below code gets the gradient of the `tf.square()` Operation, which we expect to return `2*input`, as the derivative of $x^2$ is $2x$\n"}
{"snippet": "old_alex_graph = tf.Graph()\nwith old_alex_graph.as_default():\n    saver = tf.train.import_meta_graph(\"saved_models/alexnet.meta\")\n", "intent": "And here is how we can bring that saved model back.  We'll play with a \"reloaded\" model more next week\n"}
{"snippet": "with var_graph.as_default(), tf.variable_scope('my_var_scope'):\n    try:\n        w_again = tf.get_variable('w')\n        b_again = tf.get_variable('b')\n    except ValueError as e:\n        print(str(e).splitlines()[0]) \n", "intent": "Note that if we are within the same variable scope, we _must_ set `reuse` to `True`. If we don't, TensorFlow will complain at us:\n"}
{"snippet": "with var_graph.as_default(), tf.variable_scope('my_var_scope') as var_scope:\n    var_scope.reuse_variables()\n    w_again = tf.get_variable('w')\n    b_again = tf.get_variable('b')\nprint(w_again, b_again, sep=\"\\n\")\nprint(w is w_again and b is b_again)  \n", "intent": "As an alternative to passing `reuse` into the `variable_scope` parameter, we can set it after the fact by using the `variable_scope.\n"}
{"snippet": "with var_graph.as_default(), tf.variable_scope('my_var_scope'):\n    curr_scope = tf.get_variable_scope()\n    curr_scope.reuse_variables()\n    w_again = tf.get_variable('w')\n    b_again = tf.get_variable('b')\n", "intent": "You can get the current variable scope with `tf.get_variable_scope()`; similar to `tf.get_default_graph()`:\n"}
{"snippet": "var_graph = tf.Graph()\nwith var_graph.as_default(), tf.variable_scope('my_var_scope') as scope:\n    w_init = tf.truncated_normal_initializer()\n    b_init = tf.zeros_initializer()\n    w = tf.get_variable('w', shape=[10, 10], initializer=w_init)\n    b = tf.get_variable('b', shape=[10], initializer=b_init)\nprint(w,b,sep=\"\\n\")\n", "intent": "Here's the first example we used above:\n"}
{"snippet": "var_graph = tf.Graph()\nwith var_graph.as_default():\n    init1 = tf.truncated_normal_initializer()\n    init2 = tf.zeros_initializer()\n    with tf.variable_scope('var_scope_1', initializer=init1) as var_scope_1:\n        w1 = tf.get_variable('w1', shape=[10, 10])\n        with tf.variable_scope('var_scope_2', initializer=init2) as var_scope_2:\n            w2 = tf.get_variable('w2', shape=[200])\n            w3 = tf.get_variable('w3', shape=[300,10,10])\n", "intent": "These default parameters can be nested, too.\n"}
{"snippet": "h = activation(torch.matmul(features, W1) + B1)\noutput = activation(torch.mm(h, W2) + B2)\nprint(output)\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "import helper\nimages, labels = next(iter(trainloader))\nimg = images[0].view(1, 784)\nwith torch.no_grad():\n    logps = model(img)\nps = torch.exp(logps)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "from pymc3.backends import SQLite\nniter = 2000\nwith pm.Model() as sqlie3_save_demo:\n    p = pm.Beta('p', alpha=2, beta=2)\n    y = pm.Binomial('y', n=n, p=p, observed=heads)\n    db = SQLite('trace.db')\n    trace = pm.sample(niter, trace=db)\n", "intent": "If you are fitting a large complex model that may not fit in memory, you can use the SQLite3 backend to save the trace incremnetally to disk.\n"}
{"snippet": "calibration_data = train_model(\n    learning_rate=0.05,\n    steps=500,\n    batch_size=5,\n    input_feature=\"rooms_per_person\")\n", "intent": "To verify that clipping worked, let's train again and print the calibration data once more:\n"}
{"snippet": "model.pop()\nfor layer in model.layers: layer.trainable=False\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "Re-create and load our modified VGG model with binary dependent (i.e. dogs v cats)\n"}
{"snippet": "def fit_model(model, batches, val_batches, nb_epoch=1):\n    model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=nb_epoch, \n                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n", "intent": "We'll define a simple function for fitting models, just to save a little typing...\n"}
{"snippet": "K.set_value(m_final.optimizer.lr, 1e-4)\nm_final.fit([arr_lr, arr_hr], targ, 16, 2, **parms)\n", "intent": "We use learning rate annealing to get a better fit.\n"}
{"snippet": "top_model = Model(inp, outp)\ntop_model.save_weights(dpath+'top_final.h5')\n", "intent": "We are only interested in the trained part of the model, which does the actual upsampling.\n"}
{"snippet": "linear = LinearRegression()\nlinear.fit(X_train, y_train_hot)\n", "intent": "First, let's do a basic linear regression.\n"}
{"snippet": "from keras.layers import Activation\nmodel.add(Activation('relu'))\n", "intent": "Now let's add an Activation function to our network after our first fully connected layer.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=100)\n", "intent": "Now we have to actually train our model on the data. This is really easy in Keras, in fact it only takes one line of code.\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(..., ...) \n", "intent": "Now let's choose a model. For this we're going ot choose a Gaussian naive Bayes model with model.fit()\n"}
{"snippet": "with pm.Model() as prior_context:\n    sigma = pm.Gamma('gamma', alpha=2.0, beta=1.0)\n    mu = pm.Normal('mu', mu=0, sd=sigma)\n    trace = pm.sample(niter, step=pm.Metropolis())\n", "intent": "Just omit the `observed=` argument.\n"}
{"snippet": "clf = tree.DecisionTreeClassifier(criterion = 'entropy')\nclf.fit(train_features, train_labels)\n", "intent": "Create a basic decision tree which minimizes entropy and fit it to the training data.\n"}
{"snippet": "parameters = {\"min_samples_split\": [2, 10],\n              \"max_depth\": [None, 2, 5, 10],\n              \"min_samples_leaf\": [1, 5, 10],\n              \"max_leaf_nodes\": [None, 5, 10, 20],\n              }\ngridsearch = GridSearchCV(clf, parameters)\ngridsearch.fit(train_features, train_labels)\n", "intent": "We will now use Grid Search to find a good set of hyperparameters which attempt to regualize the tree.\n"}
{"snippet": "svc2 = svm.LinearSVC(C=1000, loss='hinge', max_iter=5000)\nsvc2.fit(data[['X1', 'X2']], data['y'])\nsvc2.score(data[['X1', 'X2']], data['y'])\n", "intent": "It appears that it mis-classified the outlier.  Let's see what happens with a larger value of C.\n"}
{"snippet": "clf = KNeighborsClassifier().fit(X_train, y_train)\n", "intent": "Train and test a standard K Nearest Neighbors classifier on the original data\n"}
{"snippet": "cov = EmpiricalCovariance().fit(X_train)\ncov = cov.covariance_\n", "intent": "To determine how many principle components to use for PCA, let's look at the eigenvalues of the covariance matrix.\n"}
{"snippet": "X_train_pca = pca.transform(X_train)\nclf_pca = KNeighborsClassifier().fit(X_train_pca, y_train)\n", "intent": "Train and test another default K Nearest Neighbors classifier, but now use the data with reduced dimensions.\n"}
{"snippet": "model_yis = model_linear(xis_true, slope_ml, intercept_ml)\nobject_chi2s = 0.5*((yis_noisy - model_yis) / sigma_yis)**2\n", "intent": "Let's visualize the $\\chi^2$ for individual objects\n"}
{"snippet": "mrcnn = model.run_graph([image], [\n    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n    (\"probs\", model.keras_model.get_layer(\"mrcnn_class\").output),\n    (\"deltas\", model.keras_model.get_layer(\"mrcnn_bbox\").output),\n    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n])\n", "intent": "Run the classifier heads on proposals to generate class propbabilities and bounding box regressions.\n"}
{"snippet": "activations = model.run_graph([image], [\n    (\"input_image\",        model.keras_model.get_layer(\"input_image\").output),\n    (\"res2c_out\",          model.keras_model.get_layer(\"res2c_out\").output),\n    (\"res3c_out\",          model.keras_model.get_layer(\"res3c_out\").output),\n    (\"res4w_out\",          model.keras_model.get_layer(\"res4w_out\").output),  \n    (\"rpn_bbox\",           model.keras_model.get_layer(\"rpn_bbox\").output),\n    (\"roi\",                model.keras_model.get_layer(\"ROI\").output),\n])\n", "intent": "In some cases it helps to look at the output from different layers and visualize them to catch issues and odd patterns.\n"}
{"snippet": "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_idx\")\nindexed = indexer.fit(output).transform(output)\n", "intent": "Convert laebl to numeric index\n"}
{"snippet": "activations = model.run_graph([image], [\n    (\"input_image\",        model.keras_model.get_layer(\"input_image\").output),\n    (\"res2c_out\",          model.keras_model.get_layer(\"res2c_out\").output),\n    (\"res3c_out\",          model.keras_model.get_layer(\"res3c_out\").output),\n    (\"rpn_bbox\",           model.keras_model.get_layer(\"rpn_bbox\").output),\n    (\"roi\",                model.keras_model.get_layer(\"ROI\").output),\n])\n", "intent": "In some cases it helps to look at the output from different layers and visualize them to catch issues and odd patterns.\n"}
{"snippet": "model.fit(X_train, Y_train, epochs = 25, batch_size = 32)\n", "intent": "Run the following cell to train your model on 2 epochs with a batch size of 32. On a CPU it should take you around 5min per epoch. \n"}
{"snippet": "testGraph = readGraph(\"input/celegans_metabolic.graph\", Format.METIS)\nprint(\"Loaded graph with\", testGraph.numberOfEdges(), \"edges.\")\n", "intent": "3) Load an actual network that should be analyzed\n"}
{"snippet": "learn.fit(lrs/10,1,wds=wd, cycle_len=5,use_clr=(20,5))\n", "intent": "Training at this image size increases the dice score to 89.3.\n"}
{"snippet": "learn.fit(lrs/10,1,wds=wd, cycle_len=5,use_clr=(20,5))\n", "intent": "Training on full size images brings the dice score to 89.99\n"}
{"snippet": "learn.fit(lr/5,1,wds=wd, cycle_len=5,use_clr=(20,5))\n", "intent": "This brings our dice score to 91.0\n"}
{"snippet": "learner.fit(3e-3, 1, wds=1e-6)\n", "intent": "Now we begin training\n"}
{"snippet": "trn_tfms, val_tfms = tfms_from_model(arch, sz)\nmodel=learn.model\nmodel.eval()\nacts = plot_cnn_visuals(PATH, filename, model, transformer=val_tfms, total_layers=9,\n                    resize=(512,512), animation_interval=250, folder_suffix='_dog1')\n", "intent": "There's a good dog. Now we input the image filename and our model into the plot_cnn_visuals function\n"}
{"snippet": "tfms = tfms_from_model(arch, sz, aug_tfms=augs, crop_type=CropType.NO)\ndata = ImageClassifierData.from_csv(PATH, 'train_crop', f'{PATH}train_int.csv', test_name='test',\n                                      val_idxs=val_idxs, tfms=tfms, bs=bs)\n", "intent": "Here we set up our transformer to do our image augmentation and our dataloader to pass minibatches to the model.\n"}
{"snippet": "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_idx\")\nindexed = indexer.fit(output).transform(output)\n", "intent": "Convert label to numeric index\n"}
{"snippet": "learn.data.valid_ds.ds.classes[np.argmax(to_np(F.softmax(preds[500], dim=0)))]\n", "intent": "The index of the largest value in the vector corresponds to the class prediction\n"}
{"snippet": "class FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048, dropout = 0.2):\n        super().__init__() \n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n    def forward(self, x):\n        x = self.dropout(F.relu(self.linear_1(x)))\n        x = self.linear_2(x)\n        return x\n", "intent": "This is a standard feed forward network that will be used in the encoder and decoder layers.\n"}
{"snippet": "en_vecs = ft.load_model(str((PATH/'wiki.en.bin')))\nfr_vecs = ft.load_model(str((PATH/'wiki.fr.bin')))\n", "intent": "Now we add in fastText word vectors for English and French words in our vocabulary that are represented in fastText\n"}
{"snippet": "learner.fit(1e-3, 1, wds=1e-5, cycle_len=8)\n", "intent": "There doesn't seem to be an obvious effect.\nLets see where we can take this thing\n"}
{"snippet": "kmodel = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "kmodel.fit(X_train,y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "tf.reset_default_graph()\ndef generator(z, reuse=None):\n    with tf.variable_scope('gen', reuse=reuse):\n        alpha = 0.1\n        hidden1 = tf.layers.dense(inputs=z, units=128)\n        hidden1 = tf.maximum(alpha*hidden1, hidden1)\n        hidden2 = tf.layers.dense(inputs=hidden1, units=128)\n        hidden2 = tf.maximum(alpha*hidden2, hidden2)\n        output = tf.layers.dense(hidden2, units=784, activation=tf.nn.tanh)\n        return output\n", "intent": "- takes inputs z\n- applies leaky relu activation function\n- outputs result as it's a generator\n"}
{"snippet": "model2_lasso = lm.Lasso(fit_intercept=False).fit(X, y)\n", "intent": "Fit the model (by default, $\\alpha$ = 1).\n"}
{"snippet": "gs = ms.GridSearchCV(estimator=lm.Lasso(fit_intercept=False),\n                     param_grid={'alpha': np.logspace(-10, 10, 21)},\n                     scoring='neg_mean_squared_error',\n                     cv=ten_fold_cv)\ngs.fit(X, y)\n", "intent": "Determine 'optimal' value of $\\alpha$ using grid search with cross-validation.\n"}
{"snippet": "with pm.Model() as prior_context:\n    sigma = pm.Gamma('sigma', alpha=2.0, beta=1.0)\n    mu = pm.Normal('mu', mu=0, sd=sigma)\n    trace = pm.sample(niter, step=pm.Metropolis())\n", "intent": "Just omit the `observed=` argument.\n"}
{"snippet": "gs = ms.GridSearchCV(estimator=lm.Ridge(fit_intercept=False),\n                     param_grid={'alpha': np.logspace(-10, 10, 21)},\n                     scoring='neg_mean_squared_error',\n                     cv=ten_fold_cv)\ngs.fit(X, y)\n", "intent": "Determine 'optimal' value of $\\alpha$ using grid search with cross-validation.\n"}
{"snippet": "X = whites[['density', 'sulphates']]\ny = whites['quality']\nmodel1 = lm.LinearRegression().fit(X, y)\n", "intent": "Fit a linear regression model for 'quality' using 'density' and 'sulphates' as predictors.\n"}
{"snippet": "gs = ms.GridSearchCV(estimator=lm.Lasso(),\n                     param_grid={'alpha': np.logspace(-10, 10, 21)},\n                     scoring='neg_mean_squared_error',\n                     cv=five_fold_cv)\ngs.fit(X, y)\n", "intent": "Using `GridSearchCV`, tune a linear regression model with LASSO regularisation (include all predictors).\n"}
{"snippet": "Cs = np.logspace(-4, 4, 10)\ngs = ms.GridSearchCV(\n    estimator=lm.LogisticRegression(),\n    param_grid={'C': Cs},\n    scoring='roc_auc',\n    cv=five_fold_cv\n)\ngs.fit(X, y)\n", "intent": "Determine 'optimal' value of `C` by cross-validation using AUC scoring\n(`scikit-learn` uses $L_{2}$ regularisation by default).\n"}
{"snippet": "X = whites.drop(['quality', 'good_quality'], axis=1)\ny = whites.good_quality\nmodel1 = lm.LogisticRegression(C=1e50)\nmodel1.fit(X, y)\n", "intent": "Fit a logistic regression model for 'good_quality' using all predictors.\n"}
{"snippet": "model2 = lm.LogisticRegressionCV(Cs=10, cv=ten_fold_cv, scoring='roc_auc')\nmodel2.fit(X, y)\n", "intent": "Determine 'optimal' value of `C` by cross-validation using AUC scoring and $L_{2}$ regularisation.\n"}
{"snippet": "spls.set_params(\n    pls__n_components=3\n)\nspls.fit(boroughs, feelings)\n", "intent": "Train a PLS regression model with three components.\n"}
{"snippet": "gs = ms.GridSearchCV(\n    estimator=spls,\n    param_grid={\n        'pls__n_components': np.arange(1, 10)\n    },\n    scoring='neg_mean_squared_error',\n    cv=three_fold_cv\n)\ngs.fit(boroughs, feelings)\n", "intent": "Determine 'optimal' number of components.\n"}
{"snippet": "tree1 = tree.DecisionTreeClassifier()\ntree1.fit(X, y)\n", "intent": "Train a decision tree.\n"}
{"snippet": "qw = Normal(mu=tf.Variable(tf.random_normal([p])),\n            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([p]))))\nqb = Normal(mu=tf.Variable(tf.random_normal([1])),\n            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))\n", "intent": "We fit a fully factroized variational model by minimizing the Kullback-Leibler divergence.\n"}
{"snippet": "rf1 = ensemble.RandomForestClassifier(n_estimators=20)\nrf1.fit(X, y)\n", "intent": "Train a random forest with 20 decision trees.\n"}
{"snippet": "X = whites.drop('quality', axis=1)\ny = whites.quality\ntree1 = tree.DecisionTreeRegressor(max_depth=2, min_samples_leaf=50)\ntree1.fit(X, y)\n", "intent": "Train a decision tree for 'quality' limiting the depth to 3, and the minimum number of samples per leaf to 50.\n"}
{"snippet": "rf1 = ensemble.RandomForestRegressor(n_estimators=20)\nrf1.fit(X, y)\n", "intent": "Train a random forest with 20 decision trees.\n"}
{"snippet": "rf2 = ensemble.RandomForestClassifier(n_estimators=20)\nrf2.fit(X_tfidf, y)\n", "intent": "Train a random forest with 20 decision trees.\n"}
{"snippet": "lda = LdaModel(corpus=corpus, num_topics=10, id2word=id2word)\n", "intent": "Fit LDA model with 10 topics.\n"}
{"snippet": "gtb1 = ensemble.GradientBoostingClassifier(n_estimators=20)\ngtb1.fit(X, y)\n", "intent": "Train a gradient tree boosting classifier with 20 decision trees.\n"}
{"snippet": "ssvc.set_params(\n    svc__kernel='linear'\n)\nssvc.fit(X, y)\n", "intent": "Train a support vector classifier with linear (= no) kernel.\n"}
{"snippet": "ssvc.set_params(\n    svc__kernel='rbf'\n)\nssvc.fit(X, y)\n", "intent": "Train using the Radial Basis Function (RBF) kernel.\n"}
{"snippet": "gs = ms.GridSearchCV(\n    estimator=ssvc,\n    param_grid={\n        'svc__C': [1e-15, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n        'svc__kernel': ['linear', 'rbf']\n    },\n    scoring='roc_auc',\n    cv=ten_fold_cv\n)\ngs.fit(X, y)\n", "intent": "Determine 'optimal' kernel and value of `C` by cross-validation using AUC scoring.\n"}
{"snippet": "import sklearn.linear_model\nlinear_model = sklearn.linear_model.LinearRegression()\nlinear_model.fit(x_data.reshape(-1, 1), y_data, sample_weight=weights)\nfit = dict(m=linear_model.coef_[0], b=linear_model.intercept_)\nprint fit\n", "intent": "scikit-learn provides a convenient class for (weighted) linear least squares:\n"}
{"snippet": "nn.add(Dense(input_dim=X.shape[1], units=5, activation='sigmoid'))\n", "intent": "Input layer feeding into hidden layer with 5 neurons (sigmoid activation):\n"}
{"snippet": "nn.add(Dense(units=1, activation='sigmoid'))\n", "intent": "Hidden layer feeding into a single output neuron (sigmoid activation):\n"}
{"snippet": "gs = ms.GridSearchCV(\n    estimator=ssvc,\n    param_grid={\n        'svr__C': [1e-15, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n        'svr__kernel': ['linear', 'rbf']\n    },\n    scoring='neg_mean_squared_error',\n    cv=split\n)\ngs.fit(X, y)\n", "intent": "Determine 'optimal' kernel and value of `C` by cross-validation.\n"}
{"snippet": "regressor = LinearRegression()\nregressor.fit(X_train, y_train)\n", "intent": "Reference: http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\n"}
{"snippet": "humidity_classifier = DecisionTreeClassifier(max_leaf_nodes=10, random_state=0)\nclf = humidity_classifier.fit(X_train, y_train)\nclf\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nFit on Train Set\n<br><br></p>\n"}
{"snippet": "kmeans = KMeans(n_clusters=2)\nclusters = kmeans.fit(X)\n", "intent": "Set up the k-means clustering analysis\n"}
{"snippet": "logreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\nmodel = GridSearchCV(LogisticRegression(),logreg_parameters, cv=5)\nmodel.fit(x,y)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "logreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\nmodel = GridSearchCV(LogisticRegression(),logreg_parameters, scoring = 'average_precision', cv=5)\nmodel.fit(x,y)\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "from sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nX = tb[['1Q15_nSold_x_Price']]\ny = tb['2015_sales']\nlm = LinearRegression()\nlm.fit(X, y)\nlm.coef_ , lm.intercept_\n", "intent": "Using scikit-learn or statsmodels, build the necessary models for your scenario. Evaluate model fit.\n"}
{"snippet": "nsample=2000\nwith pm.Model() as model:\n    start = pm.find_MAP() \n    step = pm.NUTS(scaling=start) \n    trace = pm.sample(nsample, step, start=start, progressbar=True, njobs=4) \nlines = {var:trace[var].mean() for var in trace.varnames}\npm.traceplot(trace, lines=lines)\n", "intent": "Repeat the earlier analysis and learn the variance of the data\n"}
{"snippet": "model_knn = KNeighborsClassifier()\nkNN_grid = {\"n_neighbors\": np.arange(1,11), \"weights\": [\"uniform\", \"distance\"]}\nselector_knn = GridSearchCV(model_knn, kNN_grid, cv = 5)\nselector_knn.fit(X_train2, y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "logreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\ngs = grid_search.GridSearchCV(logit, logreg_parameters, cv=5)\ngs_model = gs.fit(X_train, y_train) \n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nlogreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\nmodel2 = LogisticRegression()\nselector2 = GridSearchCV(model2, logreg_parameters, cv = 5)\nselector2.fit(X_train_best, y_train)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier as kNN\nmodel3 = kNN()\nkNN_grid = {\"n_neighbors\": np.arange(1,11), \"weights\": [\"uniform\", \"distance\"]}\nselector3 = GridSearchCV(model3, kNN_grid, cv = 5)\nselector3.fit(X_train_best, y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "logreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\nmodel4 = LogisticRegression()\nselector4 = GridSearchCV(model4, logreg_parameters, cv = 5, scoring = \"average_precision\")\nselector4.fit(X_train_best, y_train)\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nlogreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\ngs = GridSearchCV(estimator = LogisticRegression(), param_grid = logreg_parameters, cv = 5)\ngs_model = gs.fit(X, y)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn_parameters = {\n    'n_neighbors': np.arange(5, 10),\n    'weights': ['uniform', 'distance']\n}\nknn_gridsearch = GridSearchCV(estimator = KNeighborsClassifier(), param_grid = knn_parameters, cv = 5)\nknn_gridsearch.fit(X, y)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "import statsmodels.api as sm\nX = np.array(xs).transpose()\nX = sm.add_constant(X)\nmod = sm.OLS(ys, X)\nres = mod.fit()\nres.summary()\n", "intent": "This generated a fit of $y = 2 x + 0.2420$. Let's see what a linear regression yields.\n"}
{"snippet": "X = data[[\"petal width (cm)\", \"sepal width (cm)\"]]\ny = target\nkmean_cluster = cluster.KMeans(n_clusters = 3, n_init = 20)\nkmean_cluster.fit(X, y)\n", "intent": "Run the clustering analysis using scikit-learn.\n*Hint*: Estimate the number of clusters, k, based on your visual examination of the distributions.\n"}
{"snippet": "import theano.tensor as T \ndef logp_signoise(yobs, is_outlier, yest_in, sigma_y_in, yest_out, sigma_y_out):\n    pdfs_in = T.exp(-(yobs - yest_in + 1e-4)**2 / (2 * sigma_y_in**2)) \n    pdfs_in /= T.sqrt(2 * np.pi * sigma_y_in**2)\n    logL_in = T.sum(T.log(pdfs_in) * (1 - is_outlier))\n    pdfs_out = T.exp(-(yobs - yest_out + 1e-4)**2 / (2 * (sigma_y_in**2 + sigma_y_out**2))) \n    pdfs_out /= T.sqrt(2 * np.pi * (sigma_y_in**2 + sigma_y_out**2))\n    logL_out = T.sum(T.log(pdfs_out) * is_outlier)\n    return logL_in + logL_out\n", "intent": "Assume the data are drawn from two Gaussians error distribution (one for the function and the other for the outliers)\n"}
{"snippet": "logreg = linear_model.LogisticRegressionCV(cv = 5)\nlogreg.fit(X, y)\n", "intent": "Define a logistic regression and train it with the feature and target set\n"}
{"snippet": "gs_logreg = linear_model.LogisticRegression(C = 2.5, penalty = 'l2', solver = 'liblinear')\ngs_logreg.fit(cat_X,y)\n", "intent": "The best model has C = 100.0 and uses L1 regularization penalty.\n"}
{"snippet": "gs_logreg = linear_model.LogisticRegression(C = 100.0, penalty = 'l1', solver = 'liblinear')\ngs_logreg.fit(X,y)\n", "intent": "The best model has C = 100.0 and uses L1 regularization penalty.\n"}
{"snippet": "formula1 = 'prglngth ~ agepreg'\nmodel = smf.ols(formula1, data=newdf)\nresults = model.fit()\nprint results.summary()\n", "intent": "I started by comparing the dependent variable, pregnancy length, with age pregnant\n"}
{"snippet": "formula1 = 'prglngth ~ race'\nmodel = smf.ols(formula1, data=newdf)\nresults = model.fit()\nprint results.summary()\n", "intent": "I then compared the dependent variable, pregnancy length, with race\n"}
{"snippet": "formula1 = 'prglngth ~ birthord'\nmodel = smf.ols(formula1, data=newdf)\nresults = model.fit()\nprint results.summary()\n", "intent": "I then compared the dependent variable, pregnancy length, with birth order of the child\n"}
{"snippet": "formula1 = 'prglngth ~ basewgt'\nmodel = smf.ols(formula1, data=newdf)\nresults = model.fit()\nprint results.summary()\n", "intent": "I then compared the dependent variable, pregnancy length, with respondents weight\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nforest_model_with_num_words = RandomForestClassifier()\nforest_model_with_num_words.fit(X_train, y_train)\nforest_model_with_num_words.score(X_test, y_test)\n", "intent": "Let's see how good we can do with a Random Forest.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegressionCV\nmodel = LogisticRegressionCV(Cs=[math.e**v for v in range(-5,5)],\n                             multi_class='multinomial',\n                             solver='newton-cg')\nmodel.fit(X_train, y_train)\nprint \"With tuning\", model.score(X_test, y_test)\n", "intent": "As a final step, let's tune the amount of regularization for our logistic regression model (since it seems to be working the best).\n"}
{"snippet": "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('flatten_8').output)\nencoder.summary()\n", "intent": "This will allow us to encode the images and look at what the encoding results in. \n"}
{"snippet": "knn.fit(X, y)\n", "intent": "so now that we have an instance of this classifier, let's use it to train the model\n"}
{"snippet": "logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n", "intent": "> __Note:__ Why 97 and 53? 150 * .35 = 52.5 or rounded to 53\nNow let's use the data and perform the training and testing activities\n"}
{"snippet": "a = tensor(-1.,1)\n", "intent": "Suppose we believe `a = (-1.0,1.0)` then we can compute `y_hat` which is our *prediction* and then compute our error.\n"}
{"snippet": "lm.fit(x_train, y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "dtr = tree.DecisionTreeRegressor(max_depth=4)\n", "intent": "Train a DecisionTreeRegressor.\n"}
{"snippet": "clf = tree.DecisionTreeRegressor(max_depth=2)\nclf = clf.fit(X_train, y_train)\n", "intent": "Train a DecisionTreeRegressor.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32,input_dim = x_train.shape[1]))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(8))\nmodel.add(Dropout(0.5))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(2))\nmodel.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train,y_train,epochs=30,verbose = 0,batch_size = 50)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "results = model.fit()\n", "intent": "Now we actually fit the model. `fit` actually runs the model and returns a model-specific Results class that we can work with.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier( \nknn_clf.fit( \n", "intent": "**Problem 3b**\nTrain a $k=7$ nearest neighbors machine learning model on the Titanic training set.\n"}
{"snippet": "tf.reset_default_graph()\nm1 = tf.constant([[3., 3.]], name='M1')\nm2 = tf.placeholder('float32', shape=...) \nproduct = 10*tf.matmul(m1,m2)\n", "intent": "Instead of the matrix m2 use a placeholder to feed-in values. You must specify the shape of the m2 matrix (rows, columns). \n"}
{"snippet": "sess = tf.Session()\nval_m2 = np.array([[2.],[3.]])\nres = sess.run(fetches=..., feed_dict=...) \nprint(res)\nsess.close()\n", "intent": "Provide the correct feeds (inputs) and fetches (outputs of the computational graph)\n"}
{"snippet": "tf.reset_default_graph()\nm1 = tf.constant([[3., 3.]], name='M1')\nm2 = tf.placeholder('float32', shape=...) \nproduct = 10*tf.matmul(m1,m2)\n", "intent": "Instead of the matrix m2 use a placefolder to feed-in values. You must specify the shape of the m2 matrix (rows, columns). \n"}
{"snippet": "tf.reset_default_graph()\nm1 = tf.constant([[3., 3.]], name='M1')\nm2 = tf.placeholder('float32', shape=(2,1)) \nproduct = 10*tf.matmul(m1,m2)\n", "intent": "Instead of the matrix m2 use a placefolder to feed-in values. You must specify the shape of the m2 matrix (rows, columns). \n"}
{"snippet": "with tf.Session() as sess: \n    sess.run(init_op) \n    res = sess.run(loss, feed_dict={x:x_data, y:y_data, a:, b:}) \n    writer = tf.summary.FileWriter(\"/tmp/linreg\", sess.graph) \n    writer.close()\n    print(\"Loss {} \".format(res))\n", "intent": "Now feed your optimal parameters from above through the tensorflow graph and compare the \"loss\" with the RSS \n"}
{"snippet": "with tf.Session() as sess: \n    sess.run(init_op) \n    res = sess.run(loss, feed_dict={x:x_data, y:y_data, a:1.7, b:1.55}) \n    writer = tf.summary.FileWriter(\"/tmp/linreg\", sess.graph) \n    writer.close()\n    print(\"Loss {} (a=1.7, b=1.55)\".format(res))\n", "intent": "Now feed your optimal parameters from above through the tensorflow graph and compare the \"loss\" with the RSS \n"}
{"snippet": "model_1Dconv_w_d = Sequential()\nks = 5\nmodel_1Dconv_w_d.add(Convolution1D())\nmodel_1Dconv_w_d.add(Convolution1D())\nmodel_1Dconv_w_d.add(Convolution1D())\nmodel_1Dconv_w_d.add(Convolution1D())\nmodel_1Dconv_w_d.add(Dense(1))\nmodel_1Dconv_w_d.add(Lambda(slice, arguments={'slice_length':look_ahead}))\nmodel_1Dconv_w_d.compile(optimizer='adam', loss='mean_squared_error')\nmodel_1Dconv_w_d.summary()\n", "intent": "Here we define a Neural network with 1D convolutions and \"causal\" padding, this time with dilation rate, so we are able to look back longer in time.\n"}
{"snippet": "model_simple_RNN = Sequential()\nmodel_simple_RNN.add(SimpleRNN(12,return_sequences=True,input_shape=(128,1)))\nmodel_simple_RNN.add((Dense(1)))\nmodel_simple_RNN.add(Lambda(slice, arguments={'slice_length':look_ahead}))\nmodel_simple_RNN.summary()\nmodel_simple_RNN.compile(optimizer='adam', loss='mean_squared_error')\n", "intent": "Now, let's use a RNN cell to see if we are able to learn the data generating process. We will use a hidden state size of 12.\n"}
{"snippet": "model_LSTM = Sequential()\nmodel_LSTM.add()\nmodel_LSTM.add((Dense(1)))\nmodel_LSTM.add(Lambda(slice, arguments={'slice_length':look_ahead}))\nmodel_LSTM.summary()\nmodel_LSTM.compile(optimizer='adam', loss='mean_squared_error')\n", "intent": "Let's use a more complex LSTM cell to and see if it works better than the RNN cell,  we again use a hidden state size of 12.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier(n_neighbors=7)\nknn_clf.fit(X, y)\n", "intent": "**Problem 3b**\nTrain a $k=7$ nearest neighbors machine learning model on the Titanic training set.\n"}
{"snippet": "model_LSTM = Sequential()\nmodel_LSTM.add(LSTM(12,return_sequences=True,input_shape=(128,1)))\nmodel_LSTM.add((Dense(1)))\nmodel_LSTM.add(Lambda(slice, arguments={'slice_length':look_ahead}))\nmodel_LSTM.summary()\nmodel_LSTM.compile(optimizer='adam', loss='mean_squared_error')\n", "intent": "Let's use a more complex LSTM cell to and see if it works better than the RNN cell,  we again use a hidden state size of 12.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=300, random_state=36)  \nclassifier.fit(X_train, y_train)\n", "intent": "let's train a radomforest on the bag of words features of the train set\n"}
{"snippet": "mus = np.zeros(shape=(50,100))\nfor i in range(0,50):\n    y_hat = model(x_test)\n    mu = y_hat.mean()\n    mus[i] = mu[:,0]\n", "intent": "Now, we average over many runs of the network. This is the same as taking the mean of the predictive distribution. \n"}
{"snippet": "qw = Normal(mu=tf.Variable(tf.random_normal([D])),\n            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D]))))\nqb = Normal(mu=tf.Variable(tf.random_normal([1])),\n            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))\n", "intent": "Perform variational inference. Define the variational model to be a fully factorized normal across the weights.\n"}
{"snippet": "qw_mu = tf.Variable(tf.random_normal([D]))\nqw_sigma = tf.nn.softplus(tf.Variable(tf.random_normal([D])))\nqb_mu = tf.Variable(tf.random_normal([]) + 10)\nqb_sigma = tf.nn.softplus(tf.Variable(tf.random_normal([])))\nqw = ?\nqb = ?\n", "intent": "Let's define the variational parameters and then the variational distribution. Let's use Gaussians.\n"}
{"snippet": "S = 50\nrs = np.random.RandomState(0)\ninputs = np.linspace(-5, 3, num=400, dtype=np.float32)\nx_in = tf.expand_dims(inputs, 1)\nmus = []\nfor s in range(S):\n    mus += [tf.sigmoid(ed.dot(x_in, ?) + ?)]\nmus = tf.stack(mus)\n", "intent": "We want to sample from the variational distribution. Can you fill in the question marks below?\n"}
{"snippet": "from sklearn.pipeline import make_pipeline\npoly_model = make_pipeline(PolynomialFeatures(7),\n                           LinearRegression())\n", "intent": "**Now, John decides to use the same technique to create a 7th-degree polynomial model for the non-linear data that he generated  earlier **\n"}
{"snippet": "clf = GridSearchCV(SVR(), param_grid=param_grid)\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n", "intent": "This is much faster, but might result in worse hyperparameters and therefore worse results.\n"}
{"snippet": "text_clf = text_clf.fit(X_train, y_train)\n", "intent": "Fitting the model to the train set\n"}
{"snippet": "indexer = StringIndexer(inputCol=\"raw_label\", outputCol=\"label\")\nindexed = indexer.fit(output).transform(output)\n", "intent": "Convert label to numeric index\n"}
{"snippet": "model = svm.SVC(kernel=\"linear\")\n", "intent": "sklearn provides multiple SVM implementations\n"}
{"snippet": "model.fit(X, y)\n", "intent": "After loading and preparing the data and creating an SVM object, the model is now ready to learn.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    params = tf.Variable(tf.random_uniform(shape=(vocab_size, embed_dim), minval=-1, maxval=1))\n    embed = tf.nn.embedding_lookup(params, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def sigmoid(self, z):\n", "intent": "Implement functions to calculate sigmoid and it's derivative\n"}
{"snippet": "def softmax(self, scores):\n", "intent": "Implement the softmax function, cross entropy loss, and its derivative\n"}
{"snippet": "early = EarlyStopping(patience=3)\nmodel.fit(x_train, y_train, batch_size=128, epochs=150, validation_split=.2, verbose = 2, callbacks=[early])\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=1000.0, random_state=0)\nlr.fit(X_train_std, y_train)\nplot_decision_regions(X_combined_std, y_combined,\n                      classifier=lr, test_idx=test_idx,\n                      xlabel = 'petal length [standardized]', \n                      ylabel='petal width [standardized]')\n", "intent": "The code is quite simple.\n"}
{"snippet": "svm = SVC(kernel='rbf', random_state=0, gamma=0.10, C=10.0)\nsvm.fit(X_xor, y_xor)\nplot_decision_regions(X_xor, y_xor,\n                      classifier=svm)\n", "intent": "This is the classification result using a rbf (radial basis function) kernel\nNotice the non-linear decision boundaries\n"}
{"snippet": "import numpy as np\nx = T.matrix(name='x') \nx_sum = T.sum(x, axis=0)\ncalc_sum = theano.function(inputs=[x], outputs=x_sum)\nary = [[1, 2, 3], [1, 2, 3]]\nprint('Column sum:', calc_sum(ary))\nary = np.array(ary, dtype=theano.config.floatX)\nprint('Column sum:', calc_sum(ary))\n", "intent": "This is an example code to work with tensors.\nCreate a $2 \\times 3$ tensor, and calculate its column sum.\n"}
{"snippet": "x = tf.placeholder(tf.float32, shape=(3,3))\ny = tf.matmul(x, x)\ndata = np.random.rand(3, 3)\nwith tf.Session() as s1:\n    print(s1.run(y, feed_dict={x: data})) \n", "intent": "Placeholders are used to feed in data when the data flow graph is run.\n"}
{"snippet": "IrisTree = model.fit(X, Y)\n", "intent": "Once we have an instance of the model we can fit it with the `fit` model by providing the inputs and target:\n"}
{"snippet": "regdict = {}\nfor k in [1, 2, 4, 6, 8, 10, 15]:\n    knnreg = KNeighborsRegressor(n_neighbors=k)\n    knnreg.fit(X_train, y_train)\n    regdict[k] = knnreg \n", "intent": "Lets vary the number of neighbors and see what we get.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nsimp_reg = LinearRegression().fit(xtrain.reshape(-1,1), ytrain)\nbeta0_sreg = simp_reg.intercept_\nbeta1_sreg = simp_reg.coef_[0]\nprint(\"(beta0, beta1) = ({0:8.6f}, {1:8.6f})\".format(beta0_sreg, beta1_sreg))\n", "intent": "To start, let's fit the old classic, linear regression.\n"}
{"snippet": "simp_reg = LinearRegression() \nsimp_reg.fit(xtrain.reshape(-1,1), ytrain) \nbeta0_sreg = simp_reg.intercept_\nbeta1_sreg = simp_reg.coef_[0]\ndfResults['OLS'][:] = [beta0_sreg, beta1_sreg]\ndfResults\n", "intent": "We start with simple linear regression to get the ball rolling.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ndef cv_optimize_ridge(x: np.ndarray, y: np.ndarray, list_of_lambdas: list, n_folds: int =4):\n    est = Ridge()\n    parameters = {'alpha': list_of_lambdas}\n    gs = GridSearchCV(est, param_grid=parameters, cv=n_folds, scoring=\"neg_mean_squared_error\")\n    gs.fit(x, y)\n    return gs\n", "intent": "Here is a helper function that we will use to get the best Ridge regression.\n"}
{"snippet": "from sklearn.linear_model import RidgeCV\nridgeCV_object = RidgeCV(alphas=(1e-8, 1e-4, 1e-2, 1.0, 10.0), cv=5)\nridgeCV_object.fit(Xtrain, ytrain)\nprint(\"Best model searched:\\nalpha = {}\\nintercept = {}\\nbetas = {}, \".format(ridgeCV_object.alpha_,\n                                                                            ridgeCV_object.intercept_,\n                                                                            ridgeCV_object.coef_\n                                                                            )\n     )\n", "intent": "Some sklearn models have built-in, automated cross validation to tune their hyper parameters. \n"}
{"snippet": "red_model = knn_pipeline.fit(x_train.drop('red', axis=1), x_train['red'])\nred_model.score(x_test.drop('red', axis=1), x_test['red'])\n", "intent": "It's easy to run the whole modelling process on new data:\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=24,\n          epochs=36,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "with tf.Session() as sess:\n    output = sess.run(hello_constant)\n    print(output)\n", "intent": "* An environment for running a graph. In charge of allocating the operations to GPU(s) and/or CPU(s).\nContinuing our example:\n"}
{"snippet": "N = df.shape[0]\nX = tf.placeholder(tf.float32, (N, 5))\ny = tf.placeholder(tf.float32, (N, 1))\nW = tf.Variable(tf.random_normal((5,1)))\nb = tf.Variable(tf.random_normal((1,)))\nyhat = tf.matmul(X, W) + b\n", "intent": "Reset the data flow graph.\n"}
{"snippet": "import tensorflow as tf\ndef run():\n    output = None\n    logit_data = [2.0, 1.0, 0.1]\n    logits = tf.placeholder(tf.float32)\n    softmax = tf.nn.softmax(logits)    \n    with tf.Session() as sess:\n        output = sess.run(softmax, feed_dict={logits: logit_data})\n    return output\n", "intent": "That's some elegant Numpy code.\n"}
{"snippet": "model.fit(X_train, y_train,\n          validation_data=(X_val, y_val), epochs=30);\n", "intent": "Keras models follow __Scikit-learn__'s interface of fit/predict with some notable extensions. Let's take a tour.\n"}
{"snippet": "class ReLU(Layer):\n    def __init__(self):\n        return np.maximum(0, input)\n    def backward(self, input, grad_output):\n", "intent": "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\n"}
{"snippet": "small_number = 1e-3\nweights = tf.Variable(initial_value=np.ones([X.shape[1], 1])*small_number,\n                      name=\"weights\", dtype='float32')\nb = tf.Variable(initial_value=small_number, name=\"bias\", dtype='float32')\n", "intent": "__Your code goes here.__ For the training and testing scaffolding to work, please stick to the names in comments.\n"}
{"snippet": "s = reset_tf_session()\nencoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=32)\ninp = L.Input(IMG_SHAPE)\ncode = encoder(inp)\nreconstruction = decoder(code)\nautoencoder = keras.models.Model(inputs=inp, outputs=reconstruction)\nautoencoder.compile(optimizer=\"adamax\", loss='mse')\n", "intent": "Convolutional autoencoder training. This will take **1 hour**. You're aiming at ~0.0056 validation MSE and ~0.0054 training MSE.\n"}
{"snippet": "model = keras.models.Sequential()\nmodel.add(L.InputLayer([None],dtype='int32'))\nmodel.add(L.Embedding(len(all_words),50))\nmodel.add(L.Bidirectional(L.LSTM(64,return_sequences=True), merge_mode='concat'))\nstepwise_dense = L.Dense(len(all_tags),activation='softmax')\nstepwise_dense = L.TimeDistributed(stepwise_dense)\nmodel.add(stepwise_dense)\nmodel.compile('adam','categorical_crossentropy')\nmodel.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n                    callbacks=[EvaluateAccuracy()], epochs=5,)\n", "intent": "**Conclusion:** With an accuracy of 0.95621 it appeared that the model suffered from a higher batch number\n"}
{"snippet": "model = keras.models.Sequential()\nmodel.add(L.InputLayer([None],dtype='int32'))\nmodel.add(L.Embedding(len(all_words),50))\nmodel.add(L.Bidirectional(L.GRU(64,return_sequences=True), merge_mode='concat'))\nstepwise_dense = L.Dense(len(all_tags),activation='softmax')\nstepwise_dense = L.TimeDistributed(stepwise_dense)\nmodel.add(stepwise_dense)\nmodel.compile('adam','categorical_crossentropy')\nmodel.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n                    callbacks=[EvaluateAccuracy()], epochs=5,)\n", "intent": "**Conclusion:** This is marginally better with an accuracy of 0.96562, however, it was timeconsuming\n"}
{"snippet": "model = keras.models.Sequential()\nmodel.add(L.InputLayer([None],dtype='int32'))\nmodel.add(L.Embedding(len(all_words),50))\nmodel.add(L.Bidirectional(L.GRU(128,return_sequences=True), merge_mode='concat'))\nstepwise_dense = L.Dense(len(all_tags),activation='softmax')\nstepwise_dense = L.TimeDistributed(stepwise_dense)\nmodel.add(stepwise_dense)\nmodel.compile('adam','categorical_crossentropy')\nmodel.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n                    callbacks=[EvaluateAccuracy()], epochs=10,)\n", "intent": "**Conclusion:** This is marginally better with an accuracy of 0.96601, however, and was slightly faster than LSTM\n"}
{"snippet": "model = keras.models.Sequential()\nmodel.add(L.InputLayer([None],dtype='int32'))\nmodel.add(L.Embedding(len(all_words),50))\nmodel.add(L.Bidirectional(L.GRU(64,recurrent_dropout=0.2,return_sequences=True), merge_mode='concat'))\nstepwise_dense = L.Dense(len(all_tags),activation='softmax')\nstepwise_dense = L.TimeDistributed(stepwise_dense)\nmodel.add(stepwise_dense)\nmodel.compile('adam','categorical_crossentropy')\nmodel.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n                    callbacks=[EvaluateAccuracy()], epochs=10,)\n", "intent": "**Conclusion:** Oh dear...this was both time-consuming, and it overfitted!\n"}
{"snippet": "from distutils.version import LooseVersion\nimport warnings\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nassert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\nprint('TensorFlow Version: {}'.format(tf.__version__))\n", "intent": "This will check to make sure you have the correct version of TensorFlow and access to a GPU\n"}
{"snippet": "with pm.Model() as logistic_model:\n    pm.glm.GLM.from_formula('income_more_50K ~ age + educ', data=data, family=pm.glm.families.Binomial())\n    map_estimate = pm.find_MAP()\n    print(map_estimate)\n", "intent": "Sumbit MAP estimations of corresponding coefficients:\n"}
{"snippet": "with pm.Model() as logistic_model:\n    pm.glm.GLM.from_formula('income_more_50K ~ sex + age + age_squared + educ + hours', \n                            data=data, \n                            family=pm.glm.families.Binomial())\n    step = pm.NUTS()\n    iter_sample = pm.iter_sample(2 * samples, step, start=map_estimate)\nanim = animation.FuncAnimation(fig, animate, init_func=init,\n                               frames=samples, interval=5, blit=True)\nHTML(anim.to_html5_video())\n", "intent": "Now rerun the animation providing the NUTS sampling method as the step argument.\n"}
{"snippet": "lenet_pow_2_k_4_lr_red_callback_non_aug_high_do_model = lenet_pow_2_k_4(imgs_train.shape[1:], dropout=0.5)\nlenet_pow_2_k_4_lr_red_callback_non_aug_high_do_model = compile_model(lenet_pow_2_k_4_lr_red_callback_non_aug_high_do_model)\nmodel, _ =\\\n    fit_model(lenet_pow_2_k_4_lr_red_callback_non_aug_high_do_model, \n              'lenet_pow_2_k_4_lr_red_callback_non_aug_high_do')\n", "intent": "> **NOTE**: It would be most appropriate to visualize on a test set as we can overfit to the validation set by trying out different architectures\n"}
{"snippet": "tf.reset_default_graph()\nwith tf.Session() as sess:\n    is_training = tf.placeholder(tf.bool, name='is_training')\n    with tf.variable_scope(\"G\") as scope:\n        z = tf.placeholder(tf.float32, [None, Z_DIM], name='z')\n        G = generator(z, is_training)\n    writer = tf.summary.FileWriter('logs', sess.graph)\n    writer.close()\n", "intent": "Tensorboard can be accessed at http://localhost:6060\n"}
{"snippet": "tf.reset_default_graph()\nis_training = tf.placeholder(tf.bool, name='is_training')\nwith tf.variable_scope(\"G\") as scope:\n    z = tf.placeholder(tf.float32, [None, Z_DIM], name='z')\n    G = generator(z, is_training)\nwith tf.variable_scope('D') as scope:\n    images = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS])\n    D_real, D_real_logits = discriminator(images, is_training)\n    scope.reuse_variables()\n    D_fake, D_fake_logits = discriminator(G, is_training)\n", "intent": "Now let's define generator and discriminator.\n"}
{"snippet": "class Seq2SeqModel(object):\n    pass\n", "intent": "**NOTE**: We will just declare the class here, and add functions to it as we progress\n"}
{"snippet": "from sklearn.svm import LinearSVC\nlinsvc2 = LinearSVC()\n", "intent": "6 - Fit a linear SVM to the data for various values of the C parameter, reporting your testing and training accuracies.  Comment on your results.\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nNB = MultinomialNB()\nNB.fit(X_spam_train, y_spam_train)\n", "intent": "I would use a multinomial classifier, since we are dealing with counts:\n"}
{"snippet": "KNN_sonar = KNeighborsClassifier()\nKNN_sonar.fit(X_sonar_train, y_sonar_train)\nprint('Train accuracy: {:.5f}'.format(KNN_sonar.score(X_sonar_train, y_sonar_train)))\nprint('Test accuracy: {:.5f}'.format(KNN_sonar.score(X_sonar_test, y_sonar_test)))\n", "intent": "4 - Fit a KNN classifier using the default settings and report the training and testing accuracies.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    word_embeddings = tf.Variable(tf.random_normal([vocab_size,embed_dim], stddev=0.1))\n    embedding_lookup = tf.nn.embedding_lookup(word_embeddings, input_data)\n    return embedding_lookup\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "for n_features in np.arange(1, 10):\n    treeClass.set_params(max_features=n_features)\n    treeClass.fit(bcw_train, bcw_target_train)\n    print('Training accuracy for {} features: {}'.format(n_features, treeClass.score(bcw_train, bcw_target_train)))\n    print('Testing accuracy for {} features: {}'.format(n_features, treeClass.score(bcw_test, bcw_target_test)))\n    print('\\n')\n", "intent": "3 - Now try different values of the `max_features` parameter and report the training and testing errors.  Comment on your results.\n"}
{"snippet": "treeClass = DecisionTreeClassifier(random_state=59)\nfor min_samples in np.arange(50, 700, 50):\n    treeClass.set_params(min_samples_split=min_samples)\n    treeClass.fit(bcw_train, bcw_target_train)\n    print('Training accuracy for {} samples: {}'.format(min_samples, treeClass.score(bcw_train, bcw_target_train)))\n    print('Testing accuracy for {} samples: {}'.format(min_samples, treeClass.score(bcw_test, bcw_target_test)))\n    print('\\n')\n", "intent": "4 - Now try different settings for the `min_samples_split` parameter.  Comment on your results.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier()\nKNN.fit(bcw_train, bcw_target_train)\nprint(KNN.score(bcw_train, bcw_target_train))\nprint(KNN.score(bcw_test, bcw_target_test))\n", "intent": "6 - Fit two other classification models of your choice to the data and comment on your results.\n"}
{"snippet": "for n_features in np.arange(1, 10):\n    forest.set_params(max_features=n_features)\n    forest.fit(bcw_train, bcw_target_train)\n    print('Training accuracy for {} features: {}'.format(n_features, forest.score(bcw_train, bcw_target_train)))\n    print('Testing accuracy for {} features: {}'.format(n_features, forest.score(bcw_test, bcw_target_test)))\n    print('\\n')\n", "intent": "2 - Now try different values of the `max_features` parameter and report the training and testing errors.  Comment on your results.\n"}
{"snippet": "forest = RandomForestClassifier(random_state=59)\nfor n_estimators in [3, 10, 30, 100, 300, 1000]:\n    forest.set_params(n_estimators=n_estimators)\n    forest.fit(bcw_train, bcw_target_train)\n    print('Training accuracy for {} estimators: {}'.format(n_estimators, forest.score(bcw_train, bcw_target_train)))\n    print('Testing accuracy for {} estimators: {}'.format(n_estimators, forest.score(bcw_test, bcw_target_test)))\n    print('\\n')\n", "intent": "3 - Now try out a few different values of the `n_estimators` parameter.  Comment on your results.\n"}
{"snippet": "forest = RandomForestClassifier(random_state=59)\nfor min_samples in 2**np.arange(1, 10):\n    forest.set_params(min_samples_split=min_samples)\n    forest.fit(bcw_train, bcw_target_train)\n    print('Training accuracy for {} samples split: {}'.format(min_samples, forest.score(bcw_train, bcw_target_train)))\n    print('Testing accuracy for {} samples split: {}'.format(min_samples, forest.score(bcw_test, bcw_target_test)))\n    print('\\n')\n", "intent": "4 - Now try a few different values for the `min_samples_split` parameter.  Then build a final model and comment on your results.\n"}
{"snippet": "forest.set_params(min_samples_split=8, n_estimators=300, max_features=2)\nforest.fit(bcw_train, bcw_target_train)\nprint('Training accuracy: {}'.format(forest.score(bcw_train, bcw_target_train)))\nprint('Testing accuracy: {}'.format(forest.score(bcw_test, bcw_target_test)))\n", "intent": "It seems that values between 4 and 64 give the better generalization, with the best result for 16.\n"}
{"snippet": "forest.set_params(min_samples_split=8, n_estimators=300, max_features=2)\nforest.fit(bcw_train_red, bcw_target_train)\nprint('Training accuracy: {}'.format(forest.score(bcw_train_red, bcw_target_train)))\nprint('Testing accuracy: {}'.format(forest.score(bcw_test_red, bcw_target_test)))\n", "intent": "The result is slightly worse on the training set but the same on the testing set. Let's try the best model from before:\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3)\nkm.fit(iris_train.iloc[:, :4])\nprint(km.labels_)\nprint(km.cluster_centers_)\n", "intent": "1 - Fit the training data using k-means clustering (without labels) using a number a clusters determined by the label values\n"}
{"snippet": "save_model_path = './image_classification'\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(epochs):\n        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_size):\n            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n        print('Epoch {:>2}:  '.format(epoch + 1), end='')\n        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n    saver = tf.train.Saver()\n    save_path = saver.save(sess, save_model_path)\n", "intent": "Now that you have your model built and your hyperparameters defined, let's train it!\n"}
{"snippet": "lr.fit(pm2_regr.dropna(thresh=5).iloc[:, :-1], pm2_regr.dropna(thresh=5).iloc[:, -1])\nlr.score(pm2_regr.dropna(thresh=5).iloc[:, :-1], pm2_regr.dropna(thresh=5).iloc[:, -1])\n", "intent": "Dropping all row with at least 3 NA gets me an error because I have nans in some rows:\n"}
{"snippet": "lr.fit(pm2_enc.iloc[:, :-1], pm2_enc.iloc[:, -1])\nlr.score(pm2_enc.iloc[:, :-1], pm2_enc.iloc[:, -1])\n", "intent": "2 - Perform a multilinear regression, using the classified data, removing the `NA` values.  Comment on your results.\n"}
{"snippet": "gnb.fit(Xtrain_norm, ytrain)\nprint(gnb.score(Xtrain_norm, ytrain))\nprint(gnb.score(Xtest_norm, ytest))\n", "intent": "Normalized dataset:\n"}
{"snippet": "gnb.fit(Xtrain_std, ytrain)\nprint(gnb.score(Xtrain_std, ytrain))\nprint(gnb.score(Xtest_std, ytest))\n", "intent": "Standardized dataset:\n"}
{"snippet": "lda = LinearDiscriminantAnalysis()\nlda.fit(X, y)\n", "intent": "2 - Determine a reasonable number of components for dimensionality reduction on the data.  Comment on your results.\n"}
{"snippet": "lr.fit(Xtrain, ytrain)\nprint('Training accuracy: {:.5f}'.format(lr.score(Xtrain, ytrain)))\nprint('Test accuracy: {:.5f}'.format(lr.score(Xtest, ytest)))\n", "intent": "4 - Perform part (3) again, but this time with the full data set and comment on your results.\n"}
{"snippet": "param_grid = [{'n_neighbors':neigh_values}]\ngs = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, n_jobs=2)\ngs.fit(Xtrain, ytrain)\nprint(gs.best_params_)\nprint(gs.best_score_)\n", "intent": "I'm going to tune each model by itself and then do the ensemble learning with the tuned models:\n"}
{"snippet": "ada = AdaBoostClassifier(n_estimators=50, algorithm='SAMME.R')\nsvm.set_params(probability=True);\n", "intent": "4 - Repeat part (3), but this time using AdaBoost.\n"}
{"snippet": "lr = LogisticRegression()\nboost_lr = AdaBoostClassifier(lr)\n", "intent": "9 - Repeat part (7) using another classification model of your choice, performing either bagging or boosting.  Comment on your results.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "rte = RandomTreesEmbedding()\nrte.fit(X, y)\nX_trans = rte.transform(X)\n", "intent": "2 - Repeat part (1), but this time transforming the data using a Totally Random Trees hash prior to fitting.\n"}
{"snippet": "pipe.set_params(knn__n_neighbors=7)\npipe.fit(Xtrain, ytrain)\npipe.score(Xtest, ytest)\n", "intent": "Seven is the number of neighbors with the best CV score:\n"}
{"snippet": "cols_1 = ['bb', 'bpf', 'double', 'dp', 'e', 'h', 'ha', 'hra', 'ppf', 'ra', 'sb', 'sf', 'triple']\nX_kfold_1 = team_train[cols_1]\ny_kfold_1 = team_train.r.values.reshape(-1, 1)\nOLS_kfold = LinearRegression()\nk_fold(OLS_kfold, X_kfold_1, y_kfold_1, 5)\n", "intent": "All of this is just to verify that all this kfold CV stuff makes sense (and that my functions are somewhat correct):\n"}
{"snippet": "lassocv = LassoCV(cv=10)\nlassocv.fit(X_multvar, y_multvar.ravel())\nprint(lassocv.alpha_)\nprint(lassocv.score(X_multvar, y_multvar))\n", "intent": "$R^2$ is better but RMSE is not, something isn't right...\n"}
{"snippet": "tf.get_default_graph()\n", "intent": "Where is this graph? If you specify nothing, you are working on the \"default\" one.\n"}
{"snippet": "reset_graph()\nx = tf.Variable(3, name=\"x\")   \ny = tf.Variable(4, name=\"y\")\nf = x*x*y + y + 2\n", "intent": "Try one more (in the default Graph).\n"}
{"snippet": "reset_graph()\na = tf.constant(3)\nb = tf.constant(5)\ns = a + b\nwith tf.Session() as sess:\n    result = s.eval()\nprint(result)\n", "intent": ".. but actually inizialisation was unnecessary, e.g. if I do not do it, it still works:\n"}
{"snippet": "reset_graph()\nw = tf.constant(3)\nx = w + 2\ny = x + 5\nz = x * 3\nwith tf.Session() as sess:\n    print(y.eval())  \n    print(z.eval())  \n", "intent": "When you evaluate a node, TF automatically determines the set of nodes that it depends on and it evaluates these nodes first.\n"}
{"snippet": "import numpy as np\nfrom IPython.display import display, HTML\ndef strip_consts(graph_def, max_const_size=32):\n    graph_def = graph_def or tf.get_default_graph()\n    if hasattr(graph_def, 'as_graph_def'):\n        graph_def = graph_def.as_graph_def()\n    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n    display(HTML(iframe))\n", "intent": "Display your graph (not mandatory to understand all code in the next cell.. just run it.. NOTE it works only on Chrome..)\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), mean=0, stddev=0.1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "reset_graph()\nmy_third_graph = tf.get_default_graph()\nx = tf.Variable(3, name=\"x\")\ny = tf.Variable(4, name=\"y\")\nf = x*x*y + y + 2\ninit = tf.global_variables_initializer() \nwith tf.Session() as sess:\n    init.run()\n    result = f.eval()\nprint(result)\n", "intent": "And re-try the same, as a third graph, with global variable initialisation:\n"}
{"snippet": "history = model.fit(x=X_train, y=Y_train, batch_size=15, epochs=50,\n                   validation_data = (X_val, Y_val), shuffle=True)\n", "intent": "Fit the model. Modify the epochs/batch_size as needed. \n"}
{"snippet": "opt = keras.optimizers.SGD(lr=0.05,momentum=.9,nesterov=True,clipnorm=0.5)\nSegModel2.compile(loss=dice_coef,optimizer=opt)\n", "intent": "Now, everything else is just like the previous segmentation model. Let's try it out and see how it works!\n"}
{"snippet": "opt = keras.optimizers.Adam()\nRegModel.compile(loss=loss,optimizer=opt)\n", "intent": "Finally, add an optimizer and compile the model\n"}
{"snippet": "model = Model(inputs=img_input, outputs=x)\n", "intent": "We define our model, define the input(s) and output(s). \n"}
{"snippet": "feature_cols = ['TV', 'Radio', 'Newspaper']\nX = data[feature_cols]\ny = data.Sales\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X, y)\nlm.intercept_, lm.coef_\n", "intent": "Let's redo some of the Statsmodels code above in scikit-learn:\n"}
{"snippet": "feature_cols = ['TV', 'Radio', 'Newspaper', 'IsLarge']\nX = data[feature_cols]\ny = data.Sales\nlm = LinearRegression()\nlm.fit(X, y)\ndict(zip(feature_cols, lm.coef_))\n", "intent": "Let's redo the multiple linear regression and include the **IsLarge** predictor:\n"}
{"snippet": "X_train_2 = sm.add_constant(X_train) \nest1 = sm.OLS(Y_train, X_train_2)\nest12 = est1.fit()\nprint(est12.summary())\n", "intent": "Treinar o novo modelo\n"}
{"snippet": "kmeans.fit(df_user_elo7.values)\nlabels = kmeans.labels_\n", "intent": "Agora podemos encontrar os clusters.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_normal((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "kmeans.fit(X)\n", "intent": "Treine o modelo K-Means.\n"}
{"snippet": "kmeans = KMeans(n_clusters=5)\n", "intent": "Agora podemos iniciar o algoritmo de clustering.\n"}
{"snippet": "kmeans.fit(X)\n", "intent": "Treina o modelo K-Means.\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(column_name='')]\nclassifier = learn.DNNClassifier(n_classes=2, \n                                 hidden_units=[10, 20, 10], \n                                 feature_columns=feature_columns, \n                                 model_dir='./output')\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "kmeans.fit(data.drop(['Unnamed: 0', 'Private'], axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "kmeans = KMeans(n_clusters=4)\nkmeans.fit(data[0])\n", "intent": "Let's now plot the the result of applying KMeans (clusters=4) and original\n"}
{"snippet": "model = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "model.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "model = KNeighborsClassifier(n_neighbors=4)\n", "intent": "Looks like K = 4 could be a good option\n"}
{"snippet": "sess.run(tf.nn.softmax([1., 2.]))\n", "intent": "Btw, this is how I calculated the softmax values in the post:\n"}
{"snippet": "def calculate_mses(alphas, Model):\n    mses = []\n    for a in alphas:\n        model = ...\n        model.fit(...)\n        y_pred = ...\n        mses.append(...)\n    return mses\n", "intent": "Write a function that automates the process of calculating MSE for various alpha (and can be used on different models).\n"}
{"snippet": "lm_fit = first_lm.fit(xsample, ysample)\nprint('The intercept is',lm_fit.intercept_,'and the slope is', lm_fit.coef_[0])\n", "intent": "Then to fit the model to data we do the following:\n"}
{"snippet": "lm_fit = first_lm.fit(xsample,ysample)\nprint('The intercept is',lm_fit.intercept_,'and the slope is', lm_fit.coef_[0])\n", "intent": "Then to fit the model to data we do the following:\n"}
{"snippet": "predictors_all = df_all.loc[:,'WRF+DOMINO':'total_14000']\npredictors_all_const = sm.add_constant(predictors_all)\nest_all = sm.OLS(output, predictors_all_const)\nest_all_fit = est_all.fit()\nprint(est_all_fit.summary())\n", "intent": "Now let's try estimating a model with **all** the predictors embedded:\n"}
{"snippet": "predictors_less = df_all.loc[:,'WRF+DOMINO':'Resident_100']\npredictors_less_const = sm.add_constant(predictors_less)\nest_less = sm.OLS(output, predictors_less_const)\nest_less_fit = est_less.fit()\nprint(est_less_fit.summary())\n", "intent": "Now let's look at what happens if we drop some of the predictors\n"}
{"snippet": "X_all = df_all.loc[:,'WRF+DOMINO':'total_14000']\nX_all_const = sm.add_constant(X_all)\nest_all = sm.OLS(Y, X_all_const)\nresult_all = est_all.fit()\nresult_all.aic\n", "intent": "Now let's try estimating a model with **all** the predictors embedded:\n"}
{"snippet": "X_base = df_all[['WRF+DOMINO', 'Impervious_6000', 'Major_800', 'total_100', 'Major_100', 'Major_200', 'Elevation_truncated_km', 'Distance_to_coast_km', 'Population_800', 'total_800']]\nX_base_const = sm.add_constant(X_base)\nest_base = sm.OLS(Y, X_base_const)\nresults_base = est_base.fit()\nresults_base.aic\n", "intent": "And now a model that is close to (but not exactly the same as) Novotny's\n"}
{"snippet": "m = df_all.loc[:,'Population_800']\nto_add = pd.Series( np.where( m > 0, np.log(m), 0))\nX_base_poplog = X_base.assign(pop_log = to_add.values)\nX_base_poplog_const = sm.add_constant(X_base_poplog)\nest_base_poplog = sm.OLS(Y, X_base_poplog_const)\nresults_base_poplog = est_base_poplog.fit()\nresults_base_poplog.aic\n", "intent": "And now how about taking the log?\n"}
{"snippet": "X_base = df_all[['WRF+DOMINO', 'Impervious_6000', 'Major_800', 'total_100', 'Major_100', 'Major_200', 'Elevation_truncated_km', 'Distance_to_coast_km']]\nX_base_const = sm.add_constant(X_base)\nest_base = sm.OLS(Y, X_base_const)\nresults_base = est_base.fit()\nresults_base.aic\n", "intent": "And now a model that is close to (but not exactly the same as) Novotny's\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\ntrain_cols2 = data.columns[1:-1]\nprint train_cols2\nlm = LogisticRegression(C=1000000000)\ny = data[['admit']]\nx = data[train_cols2]\nlm.fit(x,y)\nprint lm.coef_\nprint lm.intercept_\n", "intent": "Answer: For a 1 point increase in GPA, the odds of being admitted increase by 118% (or, in other words, the odds are 2.18 x)\n"}
{"snippet": "neigh.fit(X, y) \n", "intent": "...and fit the data!\n"}
{"snippet": "a = tf.placeholder(dtype=tf.int32, shape=(None,))  \nb = tf.placeholder(dtype=tf.int32, shape=(None,))\nc = tf.add(a, b)\nwith tf.Session() as sess:\n  result = sess.run(c, feed_dict={\n      a: [3, 4, 5],\n      b: [-1, 2, 3]\n    })\n  print (result)\n", "intent": "<h2> Using a feed_dict </h2>\nSame graph, but without hardcoding inputs at build stage\n"}
{"snippet": "from sklearn.tree import DecisionTreeRegressor\nmodelDTR = MultiOutputRegressor(DecisionTreeRegressor(random_state=0)).fit(X,y)\n", "intent": "Second, let's train a DecisionTreeRegressor and see how it performes \n"}
{"snippet": "vk_session = vk.Session() \nvk_api = vk.API(vk_session)\n", "intent": "Starting new vk session in order to parse data\n"}
{"snippet": "scaler.fit(df.drop(\"Class\",axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "kmean = KMeans(n_clusters=2,n_jobs=-1)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "kmean.fit(df.drop(\"Private\",axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "knn = KNeighborsClassifier(n_jobs=-1,n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "lm.fit(X_train,Y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nclf_dt = DecisionTreeClassifier().fit(scaled_X_train,y_train)              \n", "intent": "Q10. Instantiate and train a DecisionTreeClassifier on the given data\n"}
{"snippet": "X = bikes.loc[:,['season_num', 'is_holiday', 'is_workingday', 'weather', 'temp_celsius',\n       'atemp', 'humidity_percent', 'windspeed_knots']]\ny = bikes.loc[:, 'num_total_users']\nlr_all.fit(X,y)\n", "intent": "- Train the model instance using our new feature matrix $X$ and the same target variable $y$.\n"}
{"snippet": "lr_temp_atemp = LinearRegression()\nX = bikes.loc[:,['temp_celsius', 'atemp']]\nlr_temp_atemp.fit(X,y)\n", "intent": "**Exercise.**\n- Create another `LinearRegression` instance that is fit using `temp_celsius` and `atemp_celsius`, and print the coefficients.\n"}
{"snippet": "lr_atemp = LinearRegression()\nX = bikes.loc[:,['atemp']]\nlr_atemp.fit(X,y)\n", "intent": "- Create another `LinearRegression` instance that is fit using `atemp_celsius` only, and print the coefficients.\n"}
{"snippet": "lr_all = LinearRegression()\n", "intent": "- Make a new instance of the LinearRegression class. Call it lr_all to distinguish it from our last model.\n"}
{"snippet": "lr_all.fit(X, y)\n", "intent": "- Train the model instance using our new feature matrix $X$ and the same target variable $y$.\n"}
{"snippet": "feature_cols = ['temp_celsius', 'atemp_celsius']\nX = bikes.loc[:, feature_cols]\ny = bikes.loc[:, 'num_total_users']\nlr_temp_atemp = LinearRegression()\nlr_temp_atemp.fit(X, y)\nprint(lr_temp_atemp.coef_)\n", "intent": "**Exercise.**\n- Create another `LinearRegression` instance that is fit using `temp_celsius` and `atemp_celsius`, and print the coefficients.\n"}
{"snippet": "feature_cols = ['atemp_celsius']\nX = bikes[feature_cols]\ny = bikes.num_total_users\nlr_atemp = LinearRegression()\nlr_atemp.fit(X, y)\nprint(lr_atemp.coef_)\n", "intent": "- Create another `LinearRegression` instance that is fit using `atemp_celsius` only, and print the coefficients.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=1, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "a_fit = cluster.KMeans(n_clusters=2).fit(a_data)\n", "intent": "For the [KMeans algorithm](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nclf_nb = GaussianNB().fit(scaled_X_train,y_train)                \n", "intent": "Q11. Instantiate and train a GaussianNB on the given data\n"}
{"snippet": "c_fit = cluster.KMeans(n_clusters=2).fit(c_data)\nplot_clusters(c_data, c_fit)\n", "intent": "The fit results look reasonable for (b), although the sharp dividing line between the two clusters looks artificial.\n"}
{"snippet": "fit = compare_sizes(cluster.KMeans(n_clusters=10, random_state=1).fit(df_iso))\n", "intent": "Look for 10 clusters instead of 3 (again using `random_state=1`). One cluster should be clearly different from the others.\n"}
{"snippet": "fit = compare_sizes(cluster.KMeans(n_clusters=10, random_state=123).fit(df_iso))\n", "intent": "Here is one example where the cluster of large sizes disappears:\n"}
{"snippet": "model_selection.cross_validate(\n    svm.LinearSVR(random_state=123),\n    df_iso_normed, e_scaled, cv=3, return_train_score=False)\n", "intent": "Cross validation works the same for classification and regression, but with different score functions:\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\nAdB = AdaBoostClassifier(n_estimators=100)\n", "intent": "Compare to AdaBoost\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators = 100, max_features=1)\n", "intent": "Limit the number of classifiers considered at each step of RF:\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators = 10, max_features=1,oob_score=True)\n", "intent": "Play around with different parameters for the RF\n"}
{"snippet": "lm = LinearRegression(n_jobs=4)\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,verbose=3)\ngrid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor().fit(X_train,y_train)\n", "intent": "Q10. Instantiate any Regressor, such as DecisionTreeRegressor, and train it on the training data.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=128)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n", "intent": "As before, here we'll train the network. Instead of flattening the images though, we can pass them in as 28x28x1 arrays.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_matrix = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -0.1, 0.1))\n    embed = tf.nn.embedding_lookup(embedding_matrix, input_data)\n    return embed\ntest_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "import statsmodels.formula.api as smf\nlm = smf.ols(formula='y ~ X', data=CreditData).fit()\nlm.summary()\n", "intent": "Second Step, find the p-values of your estimates. You have a few variables try to show your p-values along side the names of the variables.\n"}
{"snippet": "RF = RandomForestRegressor(n_estimators = 10000, \n                           max_features = 4,     \n                           min_samples_leaf = 10, \n                           oob_score = True,    \n                           random_state = 1)    \nRF.fit(X,y)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n"}
{"snippet": "lm_stats = smf.ols(formula='sleep ~ age + yrsmarr', data=sleep).fit()\nprint (lm_stats.summary())\n", "intent": "    What are the features with coefficients greater than 0\n---\n"}
{"snippet": "optimal_lasso = LassoCV(n_alphas=300, cv=10, verbose=1)\noptimal_lasso.fit(X_train, y_train)\nprint optimal_lasso.alpha_\n", "intent": "**Now implement a Lasso Regression**\n"}
{"snippet": "l1_ratios = np.linspace(0.01, 1.0, 50)\noptimal_enet = ElasticNetCV(l1_ratio=l1_ratios, n_alphas=300, cv=5, verbose=1)\noptimal_enet.fit(X_train, y_train)\nprint optimal_enet.alpha_\nprint optimal_enet.l1_ratio_\n", "intent": "**Now implement Elastic Net Regression**\n"}
{"snippet": "from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\nparams = {\"max_depth\": [3,5,10,20],\n          \"max_features\": [None, \"auto\"],\n          \"min_samples_leaf\": [1, 3, 5, 7, 10],\n          \"min_samples_split\": [2, 5, 7],\n           \"criterion\" : ['mse']\n         }\nfrom sklearn.grid_search import GridSearchCV\ndtr_gs = GridSearchCV(dtr, params, n_jobs=-1, cv=5, verbose=1)\n", "intent": "---\nInclude a gridsearch \n"}
{"snippet": "model.add(Dense(128, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.25))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.50))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(.25))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "tf.reset_default_graph()\ngraph = tf.get_default_graph()\nsess = tf.Session()\ntensor_a = tf.constant([[4,5,6],[1,3,5],[3,1,3]],shape=[3,3], dtype =tf.float32 ,name='tensor_b')\ntensor_b = tf.constant([[4.,3,5],[12,3,45],[63,41,3]],shape=[3,3], dtype =tf.float32 ,name='tensor_b')\ntensor_matrix_mul = tf.matmul(tensor_a,tensor_b)\n", "intent": "<img src=\"./diagram5g.png\" style=\"width:800px;\">\nmulti dimensional array/matrix\n"}
{"snippet": "tf.reset_default_graph()\ngraph = tf.get_default_graph()\nsess = tf.Session()\ntensor_a = tf.constant([[4,5,6],[1,3,5],[3,1,3]],shape=[3,3], dtype =tf.float32 ,name='tensor_b')\ntensor_b = tf.constant([[4.,3,5],[12,3,45],[63,41,3]],shape=[3,3], dtype =tf.float32 ,name='tensor_b')\ntensor_matrix_mul = tf.matmul(tensor_a,tensor_b)\n", "intent": "<img src=\"./diagram5g.png\" style=\"width:800px;\">\nmultidimensional array/matrix\n"}
{"snippet": "tf.reset_default_graph()\nsess = tf.Session()\ngraph = tf.get_default_graph()\ntensor_a = tf.random_normal(shape=[3,3], mean=0.0,stddev=1.0, dtype=tf.float32,name='tensor_a')\ntensor_b = tf.constant([[4.,3,5],[12,3,45],[.63,.41,3]],shape=[3,3], dtype =tf.float32 ,name='tensor_b')\ntensor_matrix_mul = tf.matmul(tensor_a,tensor_b)\n", "intent": "<img src=\"./diagram5c.png\" style=\"width:800px;\">\ninput_a = tf.constant(4.,shape=[3,2], dtype =tf.float32 ,name='a')\n"}
{"snippet": "def discriminator(x):\n    Dis_h1 = tf.nn.relu(tf.matmul(x, Dis_W1) + Dis_b1)\n    Dis_logits = tf.matmul(Dis_h1, Dis_W2) + Dis_b2\n    Dis_prob = tf.nn.sigmoid(Dis_logits)\n    return Dis_prob, Dis_logits\n", "intent": "This returns a prob result and the logits level that was used to derive that probability\n"}
{"snippet": "def conv_block(x, filters, size, stride=(2,2), mode='same', act=True):\n    x = Convolution2D(filters, size, size, subsample=stride, border_mode=mode)(x)\n    x = BatchNormalization(mode=2)(x)\n    return Activation('relu')(x) if act else x\ndef res_block(ip, nf=64):\n    x = conv_block(ip, nf, 3, (1,1))\n    x = conv_block(x, nf, 3, (1,1), act=False)\n    return merge([x, ip], mode='sum')\n", "intent": "ConvBlock  \nResBlock\n"}
{"snippet": "def deconv_block(x, filters, size, shape, stride=(2,2)):\n    x = Deconvolution2D(filters, size, size, subsample=stride, \n        border_mode='same', output_shape=(None,)+shape)(x)\n    x = BatchNormalization(mode=2)(x)\n    return Activation('relu')(x)\ndef up_block(x, filters, size):\n    x = keras.layers.UpSampling2D()(x)\n    x = Convolution2D(filters, size, size, border_mode='same')(x)\n    x = BatchNormalization(mode=2)(x)\n    return Activation('relu')(x)\n", "intent": "Deconvolution / Transposed Conv / Fractionally Strident Convs\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(8, activation='sigmoid', input_dim=3000))\nmodel.add(Dropout(.3))\nmodel.add(Dense(8, activation='sigmoid'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=100)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "kmeans.fit(Universities.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "print('\\nTraining...')\nmodel.fit(x_train, y_train, epochs=1000, batch_size=50, verbose=0)\nprint('...Done!')\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "knn.fit(X_train,Y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "Now its time to train our model on our training data!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embedded_input = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "X = df[['disp', 'wt']]\nk=3\nkmeans = KMeans(n_clusters=k)\nkmeans.fit(X)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nbagg2 = BaggingClassifier(logreg)\n", "intent": "**Logistic Regression and Bagging**\n"}
{"snippet": "k = 5\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(dn1)\n", "intent": "Cluster the Data to our our target groups. **BONUS**: Perform multiple clustering tests on various indicator variables\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(feature_train, target_train)\n", "intent": "Define a logistic regression and train it with the feature and target set\n"}
{"snippet": "kmeans = cluster.KMeans(n_clusters=3)\nkmeans.fit(data)\n", "intent": "Run the clustering analysis using scikit-learn.\n*Hint*: Estimate the number of clusters, k, based on your visual examination of the distributions.\n"}
{"snippet": "import sklearn.linear_model\nols = sklearn.linear_model.LinearRegression()\ncolumns = ['dwelling_type_Condo', 'dwelling_type_Multi-Family', 'dwelling_type_Residential', 'dwelling_type_Unkown']\nols.fit(sacramento_with_dummies[columns], sacramento.price)\nzip(columns, ols.coef_)\n", "intent": "If we attempt a linear regressionusing all 4 dummy\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1., 1.))\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "dbscan = DBSCAN(eps=0.5, min_samples=5).fit(X_standard)\n", "intent": "Let's set up the DBSCAN from scikit using an epsilon of .5 and a minimum sample number of 5\n"}
{"snippet": "kmeans = KMeans(n_clusters=2)\nkmeans.fit(new_X)\n", "intent": "Set up the k-means clustering analysis. Use the graph from above to derive \"k\"\n"}
{"snippet": "logreg_params = {\n                 'penalty':['l2'],\n                 'C':np.logspace(-5,1,50),\n                 'solver':['liblinear', 'newton-cg', 'lbfgs', 'sag']\n                 }\ngrid = GridSearchCV(logreg, logreg_params, cv=5)\ngrid.fit(X, y)\nprint grid.best_params_\nprint grid.best_score_\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "knc = KNeighborsClassifier()\nknc_params = {'n_neighbors': range(3, 10),\n              'weights': ['uniform', 'distance']}\ngrid = GridSearchCV(knc, knc_params, cv=5)\ngrid.fit(X, y)\nprint grid.best_params_\nprint grid.best_score_\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "grid2 = GridSearchCV(logreg, logreg_params, cv=5, scoring='average_precision')\ngrid2.fit(X, y)\nprint grid2.best_params_\nprint grid2.best_score_\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "import numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nK = 4\nkmeans = KMeans(n_clusters=K).fit(reduced_data)\n", "intent": "There is no clear elbow. ploted values till k = 50. \nTried various options for algo=, and init=\nIts always a near smooth curve. Have taken K=4\n"}
{"snippet": "regr = linear_model.LinearRegression(fit_intercept=True)\nregr.fit(x.reshape((len(x), 1)), y)   \nprint 'Actual: m = 3, b = -2'\nprint 'Fit: m = %.2f, b = %.2f' % (regr.coef_[0], regr.intercept_)\n", "intent": "Of course, `scikit-learn` has linear regression built in, with a simple interface.\n"}
{"snippet": "from pyspark.mllib.regression import LabeledPoint\nlp1 = LabeledPoint(0, dfv)\nlp2 = LabeledPoint(1, sfv)\nlp3 = LabeledPoint(2, Vectors.dense([4.5, 1.0]))\nprint(lp1)\nprint(lp2)\nprint(lp3)\n", "intent": "Then there are labeled feature vectors, represented by the type **`LabeledPoint`**:\n"}
{"snippet": "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat=\"wordcount\")\nspam_RF = RandomForestClassifier(n_estimators=200, criterion='entropy')\nspam_RF.fit(Xtrain,ytrain);\n", "intent": "Let's keep this last classifier and identify which are the misclassified emails.\n"}
{"snippet": "dtc=DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "Xtrain = X[:1000,:]\nytrain = y[:1000]\nXtest = X[1000:,:]\nytest = y[1000:]\ndigits_svc = svm.SVC(kernel='rbf', gamma=1e-3)\ndigits_svc.fit(Xtrain,ytrain);\n", "intent": "Let's identify which are the misclassified images (and find the confusion matrix by the way).\n"}
{"snippet": "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\nspam_nbc = MultinomialNB()\nspam_nbc.fit(Xtrain,ytrain);\n", "intent": "Let's identify which are the misclassified emails (and find the confusion matrix by the way).\n"}
{"snippet": "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\nspam_GP = GaussianProcessClassifier()\nspam_GP.fit(Xtrain.toarray(),ytrain)\n", "intent": "Let's keep this last classifier and identify which are the misclassified emails.\n"}
{"snippet": "from keras.models import Sequential\nmodel = Sequential()\n", "intent": "Let's declare a [feed-forward neural network](https://keras.io/getting-started/sequential-model-guide/).\n"}
{"snippet": "from keras.layers import Activation\nmodel.add(Activation('softmax'))\n", "intent": "Let's indicate that the [activation function](https://keras.io/activations/) for this output layer is a softmax.\n"}
{"snippet": "from keras.utils import plot_model\nplot_model(model, to_file='model.png')\n", "intent": "Let's save a picture of our model.\n"}
{"snippet": "batch_size = 100\ntransform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) ])\ntrain_dataset = dsets.MNIST(root='./data/', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\ntrain_iterator = iter(train_loader)\n", "intent": "As suggested above, we will practise on Mnist database. Basically, we will learn to our computer how to write figures.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Start a TF session (we could actually do that after defining the graph).\n"}
{"snippet": "x = tf.placeholder(tf.float32, shape=[None, 1])\ny_true = tf.placeholder(tf.float32, shape=[None, 1])\nhidden_layer1 = tf.layers.dense(x, units=20, activation=tf.nn.relu)\nhidden_layer2 = tf.layers.dense(hidden_layer1, units=20, activation=tf.nn.relu)\ny_pred = tf.layers.dense(hidden_layer2, units=1)\n", "intent": "Define the neural network. 2 hidden layers with 20 units each, ReLU activation functions.\n"}
{"snippet": "kms=KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\nupdate_weights = optimizer.apply_gradients(zip((grad_W, grad_b),(W,b)))\n", "intent": "We still want to use Tensorflow's `GradientDescentOptimizer` to perform the update operation.\n"}
{"snippet": "iris_dt2 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth=2)\niris_dt2.fit(iris.data, iris.target)\ndisp_iris_tree('iris_dt2',iris_dt2)\n", "intent": "Let's try to limit the depth of the tree to preserve the generalization error.\n"}
{"snippet": "from sklearn.ensemble import AdaBoostClassifier\nboosted_forest = AdaBoostClassifier(tree.DecisionTreeClassifier(criterion='entropy',max_depth=3), n_estimators=100)\nboosted_forest.fit(X,y)\nplot_decision_boundary(boosted_forest,X,y)\nprint(\"Training score:\", boosted_forest.score(X,y))\nprint(\"Testing score: \", boosted_forest.score(Xtest,ytest))\n", "intent": "Scikit-learn provides an [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n"}
{"snippet": "hold_prob = tf.placeholder(tf.float32)\nfull_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "clfdt=DecisionTreeClassifier()\nclfdt, Xtrain, ytrain, Xtest, ytest  = do_classify(clfdt, {\"max_depth\": range(1,10,1)}, dfchurn, colswewant_cont+colswewant_cat, 'Churn?', \"True.\", reuse_split=reuse_split)\n", "intent": "We train a simple decision tree classifier.\n"}
{"snippet": "model = LinearRegression(normalize=True)\nprint model.normalize\n", "intent": "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated:\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB(alpha=0.1)\nclf\n", "intent": "We can now train a classifier, for instance a Multinomial Naive Bayesian classifier which is a fast baseline for text classification tasks:\n"}
{"snippet": "pipeline.fit(twenty_train_subset.data, twenty_train_subset.target)\nprint(\"Train score:\")\nprint(pipeline.score(twenty_train_subset.data, twenty_train_subset.target))\nprint(\"Test score:\")\nprint(pipeline.score(twenty_test_subset.data, twenty_test_subset.target))\n", "intent": "Such a pipeline can then be used to evaluate the performance on the test set:\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\ngrid = GridSearchCV(estimator=pipeline, param_grid=dict(svc__C=[1e-2, 1, 1e2]))\ngrid.fit(X, y)\nprint grid.best_estimator_.named_steps['svc']\n", "intent": "Now we can proceed to run the grid search:\n"}
{"snippet": "kms.fit(df.drop(\"Private\",axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "modelW0 = sm.OLS(y, X)\nresultsW0 = modelW0.fit()\nprint(resultsW0.summary())\n", "intent": "Now let's fit a linear regression model with an intercept. \n"}
{"snippet": "import pymc3 as pm\nwith pm.Model() as model:\n    parameter = pm.Exponential(\"poisson_param\", 1.0)\n    data_generator = pm.Poisson(\"data_generator\", parameter)\n", "intent": "In PyMC3, we typically handle all the variables we want in our model within the context of the `Model` object.\n"}
{"snippet": "from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nreg_DT = DecisionTreeRegressor(random_state=42)\nreg_LR = LinearRegression()\nreg_DT.fit(X_train, y_train)\nreg_LR.fit(X_train, y_train)\n", "intent": "** Ejercicio 7: Importa de la libreria sklearn los modelos elegidos anteriormente y entrenalos**\n"}
{"snippet": "cifar_model = Sequential()\n", "intent": "**(b)** Initialize a Sequential model\n"}
{"snippet": "cifar_model.add(MaxPooling2D(2,2))\n", "intent": "**(f)** Add another ``Conv2D`` layer identical to the others except with 64 filters instead of 32. Add another ``relu`` activation layer.\n"}
{"snippet": "clf = DecisionTreeClassifier(criterion = \"entropy\", \n                                  random_state = 0,\n                                  max_depth=10, \n                                  min_samples_leaf=1\n                                 )\nclf.fit(train_xs, train_ys)\nprint(clf.score(train_xs, train_ys))\nprint(clf.score(validation_xs,validation_ys))\n", "intent": "Why is a single decision tree so prone to overfitting?\n"}
{"snippet": "clf = RandomForestClassifier(criterion='entropy',max_depth=10,min_samples_leaf=1,random_state=0)\nrandom_forest = clf.fit(train_xs, train_ys)\nprint(random_forest.score(train_xs,train_ys))\nprint(random_forest.score(validation_xs,validation_ys))\n", "intent": "How does a random forest classifier prevent overfitting better than a single decision tree?\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(Xtrain, ytrain)\n", "intent": "Now let's choose a model. For this we're going ot choose a Gaussian naive Bayes model with model.fit()\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(784,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])\n", "intent": "Building a network in Keras is incredibly straight forward. Just instantiate a `Sequential` object and then use `.add` to add layers to you network.\n"}
{"snippet": "ss.fit(df.drop(\"TARGET CLASS\",axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "model2 = nn.Sequential()\nmodel2.add(nn.Linear(784, 50), 'Linear1')\nmodel2.add(nn.Sigmoid(50, 50))\nmodel2.add(nn.Linear(50, 10), 'Linear2')\nmodel2.add(nn.CrossEntropyCriterion())\nevaluation_results = main(model2, X_train, y_train, X_test, y_test, X_val, y_val, \n                          learning_rate=1e-3, learning_rate_evolution=None, \n                          n_epoch=10, layers_to_check_gradients = ['Linear1', 'Linear2'])\n", "intent": "1. Linear (784, 50)\n2. Sigmoid (50, 50)\n3. Linear (50, 10)\n4. Cross-Entropy ( )\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\n    return embedding\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('.'))\n    test_accuracy, gotten_wrong_list = evaluate_on_test_data(X_test_norm, y_test)\n    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n", "intent": "* We Run the test Images through convnet and also retrive indexes of images **gotten wrong** by the network.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('.'))\n    predictions = predict_on_custom_data(X_test_custom)\n    print(predictions)\n", "intent": "* Run the images through hte Conv Net Classifier and get predictions\n"}
{"snippet": "top_predictions = tf.nn.top_k(tf.nn.softmax(logits), k=3)\ndef top_predict_on_custom_data(X_data):\n    sess = tf.get_default_session()\n    pred = sess.run(top_predictions, feed_dict={x:X_data, train:-1})\n    return pred\n", "intent": "* Function for getting **top 3 predictions** and thier **softmax** probabilities\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('.'))\n    predictions = top_predict_on_custom_data(X_test_custom)\n", "intent": "* Get **Top 3** predictions for all the images\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(222, activation='relu', input_dim=x_train.shape[1]))\nmodel.add(Dropout(.4))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\nkeras.optimizers.Adagrad(lr=0.1, epsilon=1e-08, decay=0.01)\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=12, batch_size=32, verbose=1, shuffle=True)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "scaler.fit(bank.drop(['Class'], axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "knn=KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "kmeans.fit(X = college.drop(['Private'], axis = 1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS', axis = 1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors = 1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "mnb.fit(msg_train, label_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, verbose = 1)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(64, activation='linear', input_dim=x_train.shape[1]))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=50, verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from keras.layers.normalization import BatchNormalization\nmodel_batch = Sequential()\nmodel_batch.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_batch.add(BatchNormalization())\nmodel_batch.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_batch.add(BatchNormalization())\nmodel_batch.add(Dense(output_dim, activation='softmax'))\nmodel_batch.summary()\n", "intent": "<h2> MLP + Batch-Norm on hidden Layers + AdamOptimizer </2>\n"}
{"snippet": "from keras.layers import Dropout\nmodel_drop = Sequential()\nmodel_drop.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\nmodel_drop.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\nmodel_drop.add(Dense(output_dim, activation='softmax'))\nmodel_drop.summary()\n", "intent": "<h2> 5. MLP + Dropout + AdamOptimizer </h2>\n"}
{"snippet": "lr.fit(X_train,y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "model_sigmoid = Sequential()\nmodel_sigmoid.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))\nmodel_sigmoid.add(Dense(128, activation='sigmoid'))\nmodel_sigmoid.add(Dense(output_dim, activation='softmax'))\nmodel_sigmoid.summary()\nmodel_sigmoid.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_sigmoid.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n", "intent": "<h2>MLP + Sigmoid activation + ADAM </h2>\n"}
{"snippet": "model_relu = Sequential()\nmodel_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu.add(Dense(output_dim, activation='softmax'))\nmodel_relu.summary()\n", "intent": "<h2> MLP + ReLU +SGD </h2>\n"}
{"snippet": "model_relu = Sequential()\nmodel_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu.add(Dense(output_dim, activation='softmax'))\nprint(model_relu.summary())\nmodel_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n", "intent": "<h2> MLP + ReLU + ADAM </h2>\n"}
{"snippet": "from keras.optimizers import Adam,RMSprop,SGD\ndef best_hyperparameters(activ):\n    model = Sequential()\n    model.add(Dense(512, activation=activ, input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\n    model.add(Dense(128, activation=activ, kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\n    model.add(Dense(output_dim, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n    return model\n", "intent": "<h2> Hyper-parameter tuning of Keras models using Sklearn </h2>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>1D ConvNet on Sentiment Analysis</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Encoder-Decoder with Attention</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>FastText with Gensim</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>GloVe-Yelp-Comments-Classification</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>LSTM and GRU on Sentiment Analysis</H1></u></center>\n"}
{"snippet": "grid=GridSearchCV(SVC(),param_grid,verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Word Embeddings on Sentiment Analysis</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Word2Vec-CBOW-Keras</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Word2Vec-Skipgram-Keras</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Developing a Chatbot</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Emotion Recognition with LSTM and Attention</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Neural Machine Translation-Optimization</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Neural Machine Translation with Seq2Seq</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Text Generation</H1></u></center>\n"}
{"snippet": "from keras.models import load_model\nclassifier = load_model('/home/deeplearningcv/DeepLearningCV/Trained Models/simpsons_little_vgg.h5')\n", "intent": "If we just trained our classifer, we an use model instead.\n"}
{"snippet": "x = tf.placeholder(tf.float32)\ntrue_label = tf.placeholder(tf.float32)\nW = tf.Variable([.3], tf.float32)\nb = tf.Variable([-.3], tf.float32)\nlinear_model = W * x + b \nsquared_diff = tf.square(linear_model - true_label)\nloss = tf.reduce_sum(squared_diff)\n", "intent": " - cross-entropy loss, l2 normalization loss, hinge loss, etc\n"}
{"snippet": "clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y, cv_x_responseCoding, cv_y, clf)\n", "intent": "<h3>4.2.2. Testing the model with best hyper paramters</h3>\n"}
{"snippet": "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n", "intent": "<h4>4.3.1.2. Testing the model with best hyper paramters</h4>\n"}
{"snippet": "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n", "intent": "<h4>4.3.2.2. Testing model with best hyper parameters</h4>\n"}
{"snippet": "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42,class_weight='balanced')\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)\n", "intent": "<h3>4.4.2. Testing model with best hyper parameters</h3>\n"}
{"snippet": "clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)\n", "intent": "<h3>4.5.2. Testing model with best hyper parameters (One Hot Encoding)</h3>\n"}
{"snippet": "clf = RandomForestClassifier(max_depth=max_depth[int(best_alpha%4)], n_estimators=alpha[int(best_alpha/4)], criterion='gini', max_features='auto',random_state=42)\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y,cv_x_responseCoding,cv_y, clf)\n", "intent": "<h3>4.5.4. Testing model with best hyper parameters (Response Coding)</h3>\n"}
{"snippet": "X = uscensus[\"population\"].as_matrix()[1:-1].reshape((-1,1))\ny = uscensus[\"relgrowth\"].as_matrix()[1:-1]\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression().fit(X,y)\nm = lr.coef_ [0]\nk_val  = lr.intercept_\nn_val  = -k_val/m\nprint \"N\", n_val\nprint \"k\", k_val\n", "intent": "use a linear regression to fit a line to the data and get $k$, and $N$ which are the intercept points with each axis. recall from above that $m=-k/N$\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    w = tf.get_variable('embedding_weight', shape= [vocab_size, embed_dim])\n    return tf.nn.embedding_lookup(w, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.cluster import DBSCAN\ndbscn = DBSCAN(eps = 3, min_samples = 3)\ndbscn.fit(Xs)\n", "intent": "Remember to pass an `eps` and `min_samples` of your choice.\n"}
{"snippet": "node1 = tf.placeholder(tf.float32)\nnode2 = \nwith tf.Session() as sess:\n    inp = np.random.randn(1,10)\n    print(\"softmax using tensorflow: \\n\", sess.run(node2, {node1:inp}))\n    print(\"softmax using numpy: \\n\", np.exp(inp) / np.sum(np.exp(inp), axis=1))\n", "intent": "  - softmax$(x_i)=\\frac{\\exp^{x_i}}{\\sum_{j=1}^{K}\\exp^{x_i}}$\n"}
{"snippet": "pipe = Pipeline([\n    ('fu', fu),\n    ('lr', LogisticRegression())\n])\n", "intent": "Create a pipeline with two components:\n1. The `FeatureUnion` you set up in the previous step\n2. The `LogisticRegression` class from `sklearn`\n"}
{"snippet": "pipe = Pipeline([\n    ('fu', fu),\n    ('lr', LogisticRegression())\n])\n", "intent": "Create a pipeline with two components:\n1. The `FeatureUnion` you set up in the previous step\n2. The `LogisticRegression` from `sklearn`\n"}
{"snippet": "ridge = Ridge(alpha=optimal_ridge_alpha)\nridge.fit(x_train_ss, y_train)\nridge_train_score = ridge.score(x_train_ss, y_train)\nridge_test_score = ridge.score(x_test_ss, y_test)\nprint ('Train Score with Ridge: ', ridge_train_score)\nprint ('Test Score with Ridge: ', ridge_test_score)\n", "intent": "---\nIs it better than the Linear regression? If so, why might this be?\n"}
{"snippet": "lasso = Lasso(alpha = optimal_lasso_alpha)\nlasso_fit = lasso.fit(x_train_ss, y_train)\nlasso_train_score = lasso.score(x_train_ss, y_train)\nlasso_test_score = lasso.score(x_test_ss, y_test)\nprint ('Train  Score with Lasso: ', lasso_train_score)\nprint ('Test Score with Lasso: ', lasso_test_score)\n", "intent": "---\nIs it better than the Linear regression? Is it better than Ridge? What do the differences in results imply about the issues with the dataset?\n"}
{"snippet": "elastic_net = ElasticNet(alpha=optimal_en_alpha, l1_ratio=optimal_en_l1_ratio)\nelastic_net.fit(x_train_ss, y_train)\nelastic_train_score = elastic_net.score(x_train_ss, y_train)\nelastic_test_score = elastic_net.score(x_test_ss, y_test)\n", "intent": "---\nHow does it compare to the Ridge and Lasso regularized regressions?\n"}
{"snippet": "knn5 = KNeighborsClassifier()\nskf_5_scores = stratified_cross_val(X_stdized.values, y.values.ravel(), np_cv_indices, knn5)\n", "intent": "<a id='cv-knn5'></a>\n"}
{"snippet": "y = admit.admit.values\nX = admit[['gpa']].values\nlinmod = LinearRegression()\nlinmod.fit(X, y)\nprint 'Intercept:', linmod.intercept_\nprint 'Coef(s):', linmod.coef_\n", "intent": "<a id='pred-admit'></a>\n---\nLet's try predicting the `admit` binary indicator using just `gpa` with a Linear Regression to see what goes wrong.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nimport patsy\ny, X = patsy.dmatrices('weight_lb ~ height_in -1', data=baseball, return_type='dataframe')\ny = y.values.ravel()\nlinreg = LinearRegression()\nlinreg.fit(X, y)\nprint 'Intercept:', linreg.intercept_\nprint 'Height coef:', linreg.coef_\n", "intent": "**Construct a linear regression predicting weight from height. Interpret the value of the intercept and the coefficient from this model.**\n"}
{"snippet": "baseball['height_ctr'] = baseball.height_in - baseball.height_in.mean()\ny, X = patsy.dmatrices('weight_lb ~ height_ctr -1', data=baseball, return_type='dataframe')\ny = y.values.ravel()\nlinreg = LinearRegression()\nlinreg.fit(X, y)\nprint 'Intercept:', linreg.intercept_\nprint 'Height centered coef:', linreg.coef_\n", "intent": "**Center the height variable and re-run the regression with the centered height. Interpret the new intercept and coefficient.**\n"}
{"snippet": "x = tf.placeholder(tf.float32)\ntrue_label = tf.placeholder(tf.float32) \nW = tf.Variable([.3], tf.float32)\nb = tf.Variable([-.3], tf.float32)\nlinear_model = W * x + b \nl1_loss = \n", "intent": "  - $y_i$: ground-truth label\n  - $f(x_i)$: predicted label\n  - $L = \\sum_i{|y_i - f(x_i)|}$\n"}
{"snippet": "X = temp_df[['RM','LSTAT']]\ny = temp_df[['MEDV']]\nmlr_model = lm.fit(X,y)\n", "intent": "**Print out the coefficients from this MLR model and interpret them.**\n"}
{"snippet": "studB_days = students['B']['days']\nstudB_mor = students['B']['morale']\nBmod = LinearRegression()\nBmod.fit(studB_days[:, np.newaxis], studB_mor)\n", "intent": "**Now we fit a new model to student B's data:**\n"}
{"snippet": "logreg_cv = LogisticRegressionCV(Cs=100, cv=5, penalty='l1', scoring='accuracy', solver='liblinear')\nlogreg_cv.fit(X_train, y_train)\n", "intent": "**Gridsearch hyperparameters for the training data.**\n"}
{"snippet": "logreg_1 = LogisticRegression(C=best_C['non-crime'], penalty='l1', solver='liblinear', multi_class = 'ovr')\nlogreg_2 = LogisticRegression(C=best_C['non-violent'], penalty='l1', solver='liblinear', multi_class = 'ovr')\nlogreg_3 = LogisticRegression(C=best_C['violent'], penalty='l1', solver='liblinear', multi_class = 'ovr')\nlogreg_1.fit(X_train, y_train)\n", "intent": "**Build three logisitic regression models using the best parameters for each target class.**\n"}
{"snippet": "clf = svm.SVC(kernel='rbf')\ngamma_range = np.logspace(-5, 2, 10)\nC_range = np.logspace(-3, 2, 10)\nparam_grid = dict(gamma=gamma_range, C=C_range)\ngaus_grid = GridSearchCV(clf, param_grid, cv=10, scoring='accuracy', verbose=1)\ngaus_grid.fit(iris_X, iris_y)\n", "intent": "- Gaussian\n- Linear\n- Poly of degree 3\n"}
{"snippet": "k_wine = KMeans(n_clusters=2)\nk_wine.fit(Xs)\n", "intent": "**Fit a KMeans model with K=2 and extract the predicted labels.**\n"}
{"snippet": "from sklearn.ensemble import BaggingClassifier\nclf = DecisionTreeClassifier(max_depth=15)\nbagger = BaggingClassifier(clf, max_samples=1.0)\nvisualize_tree(bagger, X, y, boundaries=False);\n", "intent": "<a id=\"lets-see-it-now-with-our-tree-example\"></a>\n"}
{"snippet": "quote_fit = quote_clf.fit(X_train, y_train)\n", "intent": "Let's try that again, shall we? We call the pipeline just as we would its component parts.\n"}
{"snippet": "feature_set = data.iloc[:, :-1]\ntarget = data.iloc[:, -1]\nclassifier1 = naive_bayes.MultinomialNB().fit(feature_set, target)\n", "intent": "> Check: what do you think is going on with this dataset?\n> Which sklearn NB implementation should we use?\n"}
{"snippet": "node1 = tf.placeholder(tf.float32)\nnode2 = tf.div(tf.exp(node1), tf.reduce_sum(tf.exp(node1)))\nwith tf.Session() as sess:\n    inp = np.random.randn(1,10)\n    print \"softmax using tensorflow: \\n\", sess.run(node2, {node1:inp})\n    print \"softmax using numpy: \\n\", np.exp(inp) / np.sum(np.exp(inp), axis=1)\n", "intent": "  - softmax$(x_i)=\\frac{\\exp^{x_i}}{\\sum_{j=1}^{K}\\exp^{x_i}}$\n"}
{"snippet": "W = tf.Variable(tf.zeros([13, 1]))\nb = tf.Variable(tf.zeros([1]))\n", "intent": "<a id=\"create-weights-to-use-in-our-model\"></a>\n"}
{"snippet": "def multilayer_perceptron(x, weights, biases):\n    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n    return tf.matmul(layer_2, weights['out']) + biases['out']\n", "intent": "**The ReLu function**\n"}
{"snippet": "dbscan = DBSCAN(eps=1.0, n_jobs=-1)\n", "intent": "This is a bad cluster, we are going to try and find another technique\n"}
{"snippet": "from sklearn.linear_model import LogisticRegressionCV\nlogreg = LogisticRegressionCV()\n", "intent": "We now have a train and test set \n"}
{"snippet": "knn5 = KNeighborsClassifier(n_neighbors=5,\n                            weights='uniform')\nscores = accuracy_crossvalidator(Xs, y, knn5, \n                                 cv_indices)\n", "intent": "<a id='cv-knn5'></a>\n"}
{"snippet": "y = admit.admit.values\nX = admit[['gpa']].values\nlinmod = LinearRegression()\nlinmod.fit(X, y)\nprint('Intercept:', linmod.intercept_)\nprint('Coef(s):', linmod.coef_)\n", "intent": "<a id='pred-admit'></a>\n---\nLet's try predicting the `admit` binary indicator using just `gpa` with a Linear Regression to see what goes wrong.\n"}
{"snippet": "lasso = Lasso()\nlasso.fit(X, y)\n", "intent": "Next, we will fit a `Lasso` model predicting `y` with all of the features in `X`:\n"}
{"snippet": "rfe.fit(X, y)\n", "intent": "Next, we fit `rfe` to our `X` and `y` datasets:\n"}
{"snippet": "selectkbest.fit(X, y)\n", "intent": "Now, fit this to the `X` and `y` objects we created before:\n"}
{"snippet": "batch_size = 32\nmodel.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 2)\n", "intent": "- Now we will train the network! Since we don't have too much time, we will train for 5 epochs. Feel free to train for longer :)\n"}
{"snippet": "ag = AgglomerativeClustering(n_clusters=2)\nag.fit(X)\npredicted_labels = ag.labels_\npredicted_labels\n", "intent": "Next we'll ask `AgglomerativeClustering` to return back two clusters for Iris:\n"}
{"snippet": "model1 = Pipeline(modeling_steps)\nmodel1\n", "intent": "Next, we'll instantiate a Pipeline object, passing in the steps:\n"}
{"snippet": "model1.fit(X_train, y_train)\n", "intent": "Finally, we'll fit it to the data we have!\n"}
{"snippet": "petal_width_pipe = make_pipeline(\n    FeatureExtractor('petal_length'),\n    PolynomialFeatures(2, include_bias=False)\n)\n", "intent": "And a second for the `petal_width` feature:\n"}
{"snippet": "fu = make_union(\n    petal_length_pipe,\n    petal_width_pipe\n)\nfu.fit(iris)\nfu.transform(iris)[0:5, :]\n", "intent": "Just like `Pipeline`, `FeatureUnion` also has a function that removes some of the boilerplate code (`make_union()`):\n"}
{"snippet": "petal_width_pipe = make_pipeline(\n    FeatureExtractor('petal_width'),\n    PolynomialFeatures(2, include_bias=False)\n)\n", "intent": "And a second for the `petal_width` feature:\n"}
{"snippet": "random_state = 1\nclf = DecisionTreeClassifier(max_depth=20)\nrng = np.random.RandomState(random_state)\ni = np.arange(len(y))\nrng.shuffle(i)\nvisualize_tree(clf, X[i[:250]], y[i[:250]], boundaries=False,\n               xlim=(X[:, 0].min(), X[:, 0].max()),\n               ylim=(X[:, 1].min(), X[:, 1].max()))\n", "intent": "<a id=\"lets-see-what-the-boundaries-look-like-for-different-seeds\"></a>\n"}
{"snippet": "from sklearn.ensemble import BaggingClassifier\nclf = DecisionTreeClassifier(max_depth=15)\nbagger = BaggingClassifier(clf, max_samples=1.0, n_estimators=20)\nvisualize_tree(bagger, X, y, boundaries=False);\n", "intent": "<a id=\"lets-see-it-now-with-our-tree-example\"></a>\n"}
{"snippet": "lr = LinearRegression()\nlr.fit(X_train, y_train)\nprint(lr.score(X_train, y_train))\n", "intent": "Fit a linear regression to the **training** set (`X_train` and `y_train`) and look at the $R^2$ score **on the training set**\n"}
{"snippet": "start = time()\nnnGrid = GridSearchCV(neighbors.KNeighborsClassifier(), tuned_parameters,cv=5,n_jobs=3)\nnnGrid.fit(features34, activity34)\nprint(str(time() - start)+ \" sec\")\n", "intent": "Nous utilisant maintenant 3 coeurs (il y a en 4 sur mon mac) et nous comparons les temps de calcul :\n"}
{"snippet": "toyregr_skl = linear_model.LinearRegression()\nresults_skl = toyregr_skl.fit(x_train,y_train)\nbeta0_skl = results_skl.intercept_\nbeta1_skl = results_skl.coef_[0]\nprint(\"(beta0, beta1) = (%f, %f)\" %(beta0_skl, beta1_skl))\n", "intent": "Below is the code for sklearn.\n"}
{"snippet": "lol = [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0]\nfitmodel = cv_optimize_ridge(Xtrain, ytrain, lol, n_folds=4)\n", "intent": "> **EXERCISE:** Use the function above to fit the model on the training set with 4-fold cross validation.  Save the fit as the variable `fitmodel`.\n"}
{"snippet": "clf = LogisticRegression(C=100000)\nclf.fit(set1['Xtrain'], set1['ytrain'])\nclf.score(set1['Xtest'], set1['ytest'])\n", "intent": "> YOUR TURN HERE: Carry out an unregularized logistic regression and calculate the score on the `set1` test set.\n"}
{"snippet": "gb_cv.fit(Xtrain, ytrain)\n", "intent": "This will take some time! We've made a smaller grid, but things are still slow\n"}
{"snippet": "from statsmodels.api import OLS\nregr = LinearRegression()\nregr.fit(x_train, y_train)\nprint(regr.coef_)\n", "intent": "We will use the training/testing dataset as before and create our linear regression objects.\n"}
{"snippet": "import statsmodels.api as sm\nX_train_gene_10 = sm.add_constant(X_train[\"Gene 10\"])\nX_test_gene_10 = sm.add_constant(X_test[\"Gene 10\"])\nols = OLS(endog=y_train, exog=X_train_gene_10).fit()\nols.summary()\n", "intent": "Let's use one single predictor variable, `Gene 10` and fit a linear regression model to it. \n"}
{"snippet": "lda = LDA()\nlda.fit(X_train_2, y_train_2)\nqda = QDA()\nqda.fit(X_train_2, y_train_2)\n", "intent": "We can then compare our results to that given by LDA and QDA:\n"}
{"snippet": "knn_2 = KNN(n_neighbors=2)\nknn_2.fit(X_train_2, y_train_2)\nknn_5 = KNN(n_neighbors=5)\nknn_5.fit(X_train_2, y_train_2)\n", "intent": "Finally, we cover the fitting of k-Nearest Neighbors for $k = 2,5$:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=x_train.shape[1]))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(num_classes, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model = LinearRegression(normalize=True)\nprint(model.normalize)\n", "intent": "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated, and have suitable default values:\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, epochs=5, batch_size=32)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model_cluster_1.fit(X_trainW, y_trainW)\nmodel_cluster_2.fit(X_trainM, y_trainM)\n", "intent": "Fit the model and predictions:\n"}
{"snippet": "lm.fit(X_train,y_train) \n", "intent": "**Fit the model on to the instantiated object itself**\n"}
{"snippet": "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\nlearn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\nlearn.clip=25.\nlearn.metrics = [accuracy]\n", "intent": "Lets use the RNN_Learner just as before.\n"}
{"snippet": "cle3 = learn.fit(lr, 1, cycle_len=12, use_clr=(20,10), stepper=Seq2SeqStepper)\n", "intent": "At least 11 epochs as before then it was \"cheating\" by getting the right value.\n"}
{"snippet": "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')  \ncb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\nfit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)\n", "intent": "updating the learning rate with a cosine annealing for the optimizer\n"}
{"snippet": "learn.fit(0.01, 1)\n", "intent": "Now train only the (-4) layer.\n"}
{"snippet": "learn.fit(lrs/5, 1, cycle_len=2)\n", "intent": "Accuracy isn't improving much - since many images have multiple different objects, it's going to be very hard (impossible?) to be more accurate.\n"}
{"snippet": "head_reg4 = nn.Sequential(Flatten(), nn.Linear(25088,4))\nlearn = ConvLearner.pretrained(f_model, md, custom_head=head_reg4)\nlearn.opt_fn = optim.Adam\nlearn.crit = nn.L1Loss()\n", "intent": "Normally the previous layer has $7*7*512=2508$ in ResNet34, so flatten that out into a single vector of length 2508\n"}
{"snippet": "model = LogisticRegression()\nmodel = model.fit(xData, yData)\nmodel.score(xData, yData)\n", "intent": "Let's go ahead and run logistic regression on the entire data set, and see how accurate it is!\n"}
{"snippet": "m = RandomForestRegressor(n_estimators=1, max_depth=1, bootstrap=False)\nm.fit(x_samp, y_samp)\ndraw_tree(m.estimators_[0], x_samp, precision=2)\n", "intent": "Lets do it first using the sckit library to later compare our results.\n"}
{"snippet": "m = RandomForestRegressor(n_estimators=1, max_depth=2, bootstrap=False)\nm.fit(x_samp, y_samp)\ndraw_tree(m.estimators_[0], x_samp, precision=2)\n", "intent": "First (again) do it with the library of sckit to later be able to compare our results.\n"}
{"snippet": "vxmb = Variable(xmb.cuda())\nvxmb\n", "intent": "`Variable(self, /, *args, **kwargs)`  Wraps a tensor and records the operations applied to it. \n(Wrapping a tensor in a variable)\n"}
{"snippet": "xt, yt = next(dl)   \ny_pred = net2(Variable(xt).cuda())  \n", "intent": "First, we will do a **forward pass**, which means computing the predicted y by passing x to the model.\n"}
{"snippet": "m = CharLoopModel(vocab_size, n_fac).cuda() \nopt = optim.Adam(m.parameters(), 1e-2)\n", "intent": "This now is a deeper network, 8 chars vs 2 before. \nAs DNNs get deeper, they become harder to train.\n"}
{"snippet": "learn.fit(lr, 2, cycle_len=1)\n", "intent": "log_preds,y = learn.TTA()\npreds = np.mean(np.exp(log_preds),0)\naccuracy_np(preds,y)\n"}
{"snippet": "m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(x, y);\nm.oob_score_\n", "intent": "Let's now see if we can still predict whether something's in the validation set... \n"}
{"snippet": "m = RandomForestRegressor(n_estimators=1, max_depth=2, bootstrap=False)\nm.fit(x_samp, y_samp)\n", "intent": "First (again) do it with the library of scikit to later be able to compare our results.\n"}
{"snippet": "m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False)\nm.fit(x_samp, y_samp)\n", "intent": "We create a tree ensemble again, this time maximum depth of 3.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    return tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1024, activation='relu', input_dim = 1000))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='sigmoid'))\nmodel.summary()\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics =['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train,y_train, batch_size =16, epochs =5, validation_data=(x_test,y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "import numpy as np\nregr = LinearRegression()\nregr.fit(X_train, y_train)\nprint('Coefficients: \\n', regr.coef_)\n", "intent": "d) Train model on training data, and make predictions on testing data\n"}
{"snippet": "def negative_log_likelihood(X, y, w):\n    scores = sigmoid(np.dot(X, w))\n    nll = -np.sum(y*np.log(scores+1e-15) + (1-y)*np.log(1-scores+1e-15))\n    return nll\n", "intent": "As defined in Eq. 33\n"}
{"snippet": "model = MODEL(out_dim)\n", "intent": "let's create our model, which will take as input a dummy matrix to get a list of output\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X, y)\n", "intent": "**Build scikit-learn model**\n"}
{"snippet": "classifier = LogisticRegression()\n", "intent": "All models in scikit-learn have a very consistent interface.\nFirst, we instantiate the estimator object.\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "This time we set a parameter of the KNeighborsClassifier to tell it we only want to look at one nearest neighbor:\n"}
{"snippet": "knn.fit(X_train, y_train)\n", "intent": "We fit the model with out training data\n"}
{"snippet": "from sklearn import tree\nclf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,min_samples_leaf=5)\nclf = clf.fit(X_train,y_train)\n", "intent": "Fit a decision tree with the data.\n"}
{"snippet": "pca.fit(X_blob)\n", "intent": "Then we fit the PCA model with our data. As PCA is an unsupervised algorithm, there is no output ``y``.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf\n", "intent": "We can now train a classifier, for instance a logistic regression classifier which is a fast baseline for text classification tasks:\n"}
{"snippet": "grid.fit(X_train, y_train)\nprint(grid.best_params_)\nprint(grid.best_score_)\n", "intent": "I installed lightGBM for computing with GPU, check how incredibly faster the computations are:\n"}
{"snippet": "import tensorflow as tf\nA = tf.constant(1234)\nB = tf.constant([123, 456, 789])\nC = tf.constant([[12, 34], [56, 78]])\nwith tf.Session() as sess:\n    output = sess.run(A)\n    print(A)\n    print(B)\n    print(C)\n", "intent": "** Tensorflow - Hello world! **\n"}
{"snippet": "SRN_result = model.fit(X_train, y_train,\n          batch_size=32,\n          epochs=5,\n          validation_data=(X_test, y_test))\n", "intent": "2\\. Fit the model with the training set with 5 epochs and batch size 32.\n"}
{"snippet": "from keras.layers import LSTM\nmodel_L = Sequential()\nmodel_L.add(Embedding(10000, 64))\nmodel_L.add(LSTM(64))\nmodel_L.add(Dense(1, activation='sigmoid'))\nmodel_L.summary()\n", "intent": "i) [2 point] Now built a LSTM model by replacing the simple RNN layter in the above model with a LSTM layer. Print a summary of the LSTM model.\n"}
{"snippet": "from keras.layers import LSTM\nlstm_model = Sequential()\nlstm_model.add(Embedding(max_words, 64, input_length=max_len))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(1, activation='sigmoid'))\nlstm_model.summary()\n", "intent": "i) [1 point] Now built a LSTM model by replacing the simple RNN layter in the above model with a LSTM layer. Print a summary of the LSTM model.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nresult_k = knn.fit(x, y) \n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "gs_knn_p = GridSearchCV(knn, knn_params, cv=5, scoring = 'average_precision')\nresult_k_p = gs_knn_p.fit(x_train,y_train)\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=10,random_state=33)\nclf = clf.fit(X_train,y_train)\nloo_cv(X_train,y_train,clf)\n", "intent": "Try to improve performance using Random Forests\n"}
{"snippet": "_X = np.array([[1,2,3], [4,5,6]])\nX = tf.convert_to_tensor(_X)\nout = tf.ones_like(X)\nprint(out.eval())\nassert np.allclose(out.eval(), np.ones_like(_X))\n", "intent": "Q4. Let X be a tensor of [[1,2,3], [4,5,6]]. <br />Create a tensor of the same shape and dtype as X with all elements set to one.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "<img src=\"figs/fig3.png\",width=500>\n"}
{"snippet": "_x = np.array([1, 2, 3])\n_y = np.array([-1, -2, -3])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q1. Add x and y element-wise.\n"}
{"snippet": "_x = np.array([3, 4, 5])\n_y = np.array(3)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q2. Subtract y from x element-wise.\n"}
{"snippet": "_x = np.array([3, 4, 5])\n_y = np.array([1, 0, -1])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q3. Multiply x by y element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q4. Multiply x by 5 element-wise.\n"}
{"snippet": "_x = np.array([10, 20, 30], np.int32)\n_y = np.array([2, 3, 5], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.div(x, y)\nout2 = tf.truediv(x, y)\nprint(np.array_equal(out1.eval(), out2.eval()))\nprint(out1.eval(), out1.eval().dtype) \nprint(out2.eval(), out2.eval().dtype)\n", "intent": "Q5. Predict the result of this.\n"}
{"snippet": "_x = np.array([10, 20, 30], np.int32)\n_y = np.array([2, 3, 7], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q6. Get the remainder of x / y element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3], np.int32)\n_y = np.array([4, 5, 6], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q7. Compute the pairwise cross product of x and y.\n"}
{"snippet": "clf_dt=tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,min_samples_leaf=5)\nclf_dt.fit(X_train,y_train)\nmeasure_performance(X_test,y_test,clf_dt)\n", "intent": "To evaluate performance on future data, evaluate on the training set and test on the evaluation set\n"}
{"snippet": "_X = np.array([[1, -1], [3, -3]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q9. Compute the absolute value of X element-wise.\n"}
{"snippet": "_x = np.array([1, -1])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q10. Compute numerical negative value of x, elemet-wise.\n"}
{"snippet": "_x = np.array([1, 3, 0, -1, -3])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q11. Compute an element-wise indication of the sign of x, element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 2/10])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q12. Compute the reciprocal of x, element-wise.\n"}
{"snippet": "_x = np.array([1, 2, -1])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q13. Compute the square of x, element-wise.\n"}
{"snippet": "_x = np.array([2.1, 1.5, 2.5, 2.9, -2.1, -2.5, -2.9])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q14. Predict the results of this, paying attention to the difference among the family functions.\n"}
{"snippet": "_x = np.array([1, 4, 9], dtype=np.float32)\nx = tf.convert_to_tensor(_x)\n", "intent": "Q15. Compute square root of x element-wise.\n"}
{"snippet": "_x = np.array([1., 4., 9.])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q16. Compute the reciprocal of square root of x element-wise.\n"}
{"snippet": "_x = np.array([[1, 2], [3, 4]])\n_y = np.array([[1, 2], [1, 2]])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q17. Compute $x^y$, element-wise.\n"}
{"snippet": "from datetime import datetime\na = datetime.now()\nclf = ExtraTreesClassifier(n_estimators=100, random_state=10).fit(x_train, y_train)\nprint \"High Number of Trees Timing:\", datetime.now() - a\na = datetime.now()\nclf = AdaBoostClassifier(n_estimators=100, random_state=10).fit(x_train, y_train)\nprint \"AdaBoost Timing:            \", datetime.now() - a\n", "intent": "Try timing the above on your machine (you can use datetimes to get a simple approach here)\n"}
{"snippet": "_x = np.array([1, np.e, np.e**2])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q18. Compute natural logarithm of x element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 2])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q19. Compute the max of x and y element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 2])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q20. Compute the min of x and y element-wise.\n"}
{"snippet": "_x = np.array([-np.pi, np.pi, np.pi/2])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q21. Compuete the sine, cosine, and tangent of x, element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 1])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q22. Compute (x - y)(x - y) element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3])\n_y = np.array([-1, -2, -3])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.add(x, y)\nout2 = x + y \nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval()) \n_out = np.add(_x, _y)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q1. Add x and y element-wise.\n"}
{"snippet": "_x = np.array([3, 4, 5])\n_y = np.array(3)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.subtract(x, y)\nout2 = x - y \nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval()) \n_out = np.subtract(_x, _y)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q2. Subtract y from x element-wise.\n"}
{"snippet": "_x = np.array([3, 4, 5])\n_y = np.array([1, 0, -1])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.multiply(x, y)\nout2 = x * y \nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval()) \n_out = np.multiply(_x, _y)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q3. Multiply x by y element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3])\nx = tf.convert_to_tensor(_x)\nout1 = tf.scalar_mul(5, x)\nout2 = x * 5\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n_out = _x * 5\nassert np.array_equal(out1.eval(), _out)\n", "intent": "Q4. Multiply x by 5 element-wise.\n"}
{"snippet": "clf = MultinomialNB().fit(xtrain, ytrain)\nprint \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\nclf = BernoulliNB().fit(xtrain, ytrain)\nprint \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\n", "intent": "Finally we use scikit learn's Naive Bayes models to fit the training, then we score them on accuracy.\n"}
{"snippet": "_x = np.array([1, 2, 3], np.int32)\n_y = np.array([4, 5, 6], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.cross(x, y)\nprint(out1.eval())\n_out = np.cross(_x, _y)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q7. Compute the pairwise cross product of x and y.\n"}
{"snippet": "_x = np.array([1, 2, 3], np.int32)\n_y = np.array([4, 5, 6], np.int32)\n_z = np.array([7, 8, 9], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nz = tf.convert_to_tensor(_y)\nout1 = tf.add_n([x, y, z])\nout2 = x + y + z\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n", "intent": "Q8. Add x, y, and z element-wise.\n"}
{"snippet": "_X = np.array([[1, -1], [3, -3]])\nX = tf.convert_to_tensor(_X)\nout = tf.abs(X)\nprint(out.eval())\n_out = np.abs(_X)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q9. Compute the absolute value of X element-wise.\n"}
{"snippet": "_x = np.array([1, -1])\nx = tf.convert_to_tensor(_x)\nout1 = tf.negative(x)\nout2 = -x\nprint(out.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n_out = np.negative(_x)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q10. Compute numerical negative value of x, elemet-wise.\n"}
{"snippet": "_x = np.array([1, 3, 0, -1, -3])\nx = tf.convert_to_tensor(_x)\nout = tf.sign(x)\nprint(out.eval())\n_out = np.sign(_x)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q11. Compute an element-wise indication of the sign of x, element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 2/10])\nx = tf.convert_to_tensor(_x)\nout1 = tf.reciprocal(x)\nout2 = 1/x\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n_out = np.reciprocal(_x)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q12. Compute the reciprocal of x, element-wise.\n"}
{"snippet": "_x = np.array([1, 2, -1])\nx = tf.convert_to_tensor(_x)\nout1 = tf.square(x)\nout2 = x * x\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n_out = np.square(_x)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q13. Compute the square of x, element-wise.\n"}
{"snippet": "_x = np.array([1, 4, 9], dtype=np.float32)\nx = tf.convert_to_tensor(_x)\nout = tf.sqrt(x)\nprint(out.eval())\n_out = np.sqrt(_x)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q15. Compute square root of x element-wise.\n"}
{"snippet": "_x = np.array([1., 4., 9.])\nx = tf.convert_to_tensor(_x)\nout1 = tf.rsqrt(x)\nout2 = tf.reciprocal(tf.sqrt(x))\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n", "intent": "Q16. Compute the reciprocal of square root of x element-wise.\n"}
{"snippet": "from sklearn import cluster\nclf = cluster.KMeans(init='k-means++', n_clusters=10, random_state=42)\nclf.fit(X_train)\nprint clf.labels_.shape\nprint clf.labels_[1:10]\nprint_digits(images_train, clf.labels_, max_n=10)\n", "intent": "Train a KMeans classifier, show the clusters. \n"}
{"snippet": "_x = np.array([1., 2., 3.], np.float32)\nx = tf.convert_to_tensor(_x)\nout1 = tf.exp(x)\nout2 = tf.pow(np.e, x) \nprint(out1.eval())\nassert np.allclose(out1.eval(), out2.eval())\n_out = np.exp(_x)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q17. Compute $e^x$, element-wise.\n"}
{"snippet": "_x = np.array([1, np.e, np.e**2])\nx = tf.convert_to_tensor(_x)\nout = tf.log(x)\nprint(out.eval())\n_out = np.log(_x)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q18. Compute natural logarithm of x element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 2])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.maximum(x, y)\nout2 = tf.where(x > y, x, y)\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n_out = np.maximum(_x, _y)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q19. Compute the max of x and y element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 2])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.minimum(x, y)\nout2 = tf.where(x < y, x, y)\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n_out = np.minimum(_x, _y)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q20. Compute the min of x and y element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 1])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.squared_difference(x, y)\nout2 = tf.square(tf.subtract(x, y))\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n", "intent": "Q22. Compute (x - y)(x - y) element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3, 4])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q1. Create a diagonal tensor with the diagonal values of x.\n"}
{"snippet": "_X = np.array(\n[[1, 0, 0, 0],\n [0, 2, 0, 0],\n [0, 0, 3, 0],\n [0, 0, 0, 4]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q2. Extract the diagonal of X.\n"}
{"snippet": "_X = np.random.rand(2,3,4)\nX = tf.convert_to_tensor(_X)\n", "intent": "Q3. Permutate the dimensions of x such that the new tensor has shape (3, 4, 2).\n"}
{"snippet": "_X = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\nX = tf.convert_to_tensor(_X)\ndiagonal_tensor = tf.matrix_diag(X)\ndiagonal_part = tf.matrix_diag_part(diagonal_tensor)\nprint(\"diagonal_tensor =\\n\", diagonal_tensor.eval())\nprint(\"diagonal_part =\\n\", diagonal_part.eval())\n", "intent": "Q5. Predict the result of this.\n"}
{"snippet": "from sklearn.cluster import KMeans\nkmeans = KMeans(3, random_state=8)\nY_hat = kmeans.fit(X).labels_\n", "intent": "Normally, you do not know the information in `Y`, however.\nYou could try to recover it from the data alone.\nThis is what the kMeans algorithm does. \n"}
{"snippet": "_X = np.array([[1, 2, 3], [4, 5, 6]])\n_Y = np.array([[1, 1], [2, 2], [3, 3]])\nX = tf.convert_to_tensor(_X)\nY = tf.convert_to_tensor(_Y)\n", "intent": "Q7. Multiply X by Y.\n"}
{"snippet": "_X = np.arange(1, 13, dtype=np.int32).reshape((2, 2, 3))\n_Y = np.arange(13, 25, dtype=np.int32).reshape((2, 3, 2))\nX = tf.convert_to_tensor(_X)\nY = tf.convert_to_tensor(_Y)\n", "intent": "Q8. Multiply X and Y. The first axis represents batches.\n"}
{"snippet": "_X = np.arange(1, 5, dtype=np.float32).reshape((2, 2))\nX = tf.convert_to_tensor(_X)\n", "intent": "Q9. Compute the determinant of X.\n"}
{"snippet": "_X = np.arange(1, 5, dtype=np.float64).reshape((2, 2))\nX = tf.convert_to_tensor(_X)\n", "intent": "Q10. Compute the inverse of X.\n"}
{"snippet": "_X = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]], np.float32)\nX = tf.convert_to_tensor(_X)\n", "intent": "Q11. Get the lower-trianglular in the Cholesky decomposition of X.\n"}
{"snippet": "_X = np.diag((1, 2, 3))\nX = tf.convert_to_tensor(_X, tf.float32)\n", "intent": "Q12. Compute the eigenvalues and eigenvectors of X.\n"}
{"snippet": "_X = np.array(\n[[1, 0, 0, 0, 2], \n [0, 0, 3, 0, 0], \n [0, 0, 0, 0, 0], \n [0, 2, 0, 0, 0]], dtype=np.float32)\nX = tf.convert_to_tensor(_X)\n", "intent": "Q13. Compute the singular values of X.\n"}
{"snippet": "_X = np.array([[0, 1, 0],\n              [1, 1, 0]])\nX = tf.convert_to_tensor(_X)\nouts = [tf.count_nonzero(X),\n        tf.count_nonzero(X, axis=0),\n        tf.count_nonzero(X, axis=1, keep_dims=True),\n        ]\nfor out in outs:\n    print(\"->\", out.eval())\n", "intent": "Q16. Predict the results of these.\n"}
{"snippet": "_x = np.array([1, 2, 3, 4])\nx = tf.convert_to_tensor(_x)\nout = tf.diag(x)\nprint(out.eval())\n_out = np.diag(_x)\nassert np.array_equal(out.eval(), _out)\n", "intent": "Q1. Create a diagonal tensor with the diagonal values of x.\n"}
{"snippet": "from datetime import datetime\na = datetime.now()\nclf1 = ExtraTreesClassifier(n_estimators=100, random_state=10).fit(x_train, y_train)\nprint \"High Number of Trees Timing:\", datetime.now() - a\na = datetime.now()\nclf2 = AdaBoostClassifier(n_estimators=100, random_state=10).fit(x_train, y_train)\nprint \"AdaBoost Timing:            \", datetime.now() - a\n", "intent": "Try timing the above on your machine (you can use datetimes to get a simple approach here)\n"}
{"snippet": "_X = np.random.rand(2,3,4)\nX = tf.convert_to_tensor(_X)\nout = tf.transpose(X, [1, 2, 0])\nprint(out.get_shape())\n_out = np.transpose(_X, [1, 2, 0])\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q3. Permutate the dimensions of x such that the new tensor has shape (3, 4, 2).\n"}
{"snippet": "_X= np.random.rand(1, 2, 3, 4)\nX = tf.convert_to_tensor(_X)\nout1 = tf.matrix_transpose(X)\nout2 = tf.transpose(X, [0, 1, 3, 2])\nprint(out1.eval().shape)\nassert np.array_equal(out1.eval(), out2.eval())\n", "intent": "Q6. Transpose the last two dimensions of x.\n"}
{"snippet": "_X = np.array([[1, 2, 3], [4, 5, 6]])\n_Y = np.array([[1, 1], [2, 2], [3, 3]])\nX = tf.convert_to_tensor(_X)\nY = tf.convert_to_tensor(_Y)\nout = tf.matmul(X, Y)\nprint(out.eval())\n_out = np.dot(_X, _Y)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q7. Multiply X by Y.\n"}
{"snippet": "_X = np.arange(1, 13, dtype=np.int32).reshape((2, 2, 3))\n_Y = np.arange(13, 25, dtype=np.int32).reshape((2, 3, 2))\nX = tf.convert_to_tensor(_X)\nY = tf.convert_to_tensor(_Y)\nout = tf.matmul(X, Y)\nprint(out.eval())\n", "intent": "Q8. Multiply X and Y. The first axis represents batches.\n"}
{"snippet": "_X = np.arange(1, 5, dtype=np.float32).reshape((2, 2))\nX = tf.convert_to_tensor(_X)\nout = tf.matrix_determinant(X)\nprint(out.eval())\n", "intent": "Q9. Compute the determinant of X.\n"}
{"snippet": "_X = np.arange(1, 5, dtype=np.float64).reshape((2, 2))\nX = tf.convert_to_tensor(_X)\nout = tf.matrix_inverse(X)\nprint(out.eval())\n_out = np.linalg.inv(_X)\nassert np.allclose(out.eval(), _out)\n", "intent": "Q10. Compute the inverse of X.\n"}
{"snippet": "_X = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]], np.float32)\nX = tf.convert_to_tensor(_X)\nout = tf.cholesky(X)\nprint(out.eval())\n_out = np.linalg.cholesky(_X)\nassert np.array_equal(out.eval(), _out)\n", "intent": "Q11. Get the lower-trianglular in the Cholesky decomposition of X.\n"}
{"snippet": "_X = np.diag((1, 2, 3))\nX = tf.convert_to_tensor(_X, tf.float32)\neigenvals, eigenvecs = tf.self_adjoint_eig(X)\nprint(\"eigentvalues =\\n\", eigenvals.eval())\nprint(\"eigenvectors =\\n\", eigenvecs.eval())\n_eigenvals, _eigenvecs = np.linalg.eig(_X)\nassert np.allclose(eigenvals.eval(), _eigenvals)\nassert np.allclose(eigenvecs.eval(), _eigenvecs)\n", "intent": "Q12. Compute the eigenvalues and eigenvectors of X.\n"}
{"snippet": "_X = np.array(\n[[1, 0, 0, 0, 2], \n [0, 0, 3, 0, 0], \n [0, 0, 0, 0, 0], \n [0, 2, 0, 0, 0]], dtype=np.float32)\nX = tf.convert_to_tensor(_X)\nout = tf.svd(X, compute_uv=False)\nprint(out.eval())\n_out = np.linalg.svd(_X, compute_uv=False)\nassert np.allclose(out.eval(), _out)\n", "intent": "Q13. Compute the singular values of X.\n"}
{"snippet": "clf_dt = tree.DecisionTreeClassifier(max_depth=10)\nclf_dt.fit(X_train, y_train)\nclf_dt.score(X_test, y_test)\n", "intent": "Here 4 classifiers are used to train the data, namely, *Decission Tree*, *AdaBoost*, *Gradient Boosting* and  *Random Forest*.\n"}
{"snippet": "_X = np.array([[1,2,3], [4,5,6]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q2. Compute the cumulative product of X along the second axis.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [-1,-2,-3,-4], \n     [-10,-20,-30,-40],\n     [10,20,30,40]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q3. Compute the sum along the first two elements and \nthe last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [1,1/2,1/3,1/4], \n     [1,2,3,4],\n     [-1,-1,-1,-1]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q4. Compute the product along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,4,5,7], \n     [2,3,6,8], \n     [1,2,3,4],\n     [-1,-2,-3,-4]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q5. Compute the minimum along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,4,5,7], \n     [2,3,6,8], \n     [1,2,3,4],\n     [-1,-2,-3,-4]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q6. Compute the maximum along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [5,6,7,8], \n     [-1,-2,-3,-4],\n     [-5,-6,-7,-8]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q7. Compute the mean along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [-1,-2,-3,-4], \n     [-10,-20,-30,-40],\n     [10,20,30,40]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q8. Compute the sum along the second and fourth and \nthe first and third elements of X separately in the order.\n"}
{"snippet": "_X = np.random.permutation(10).reshape((2, 5))\nprint(\"_X =\", _X)\nX = tf.convert_to_tensor(_X)\n", "intent": "Q9. Get the indices of maximum and minimum values of X along the second axis.\n"}
{"snippet": "_x = np.array([0, 1, 2, 5, 0])\n_y = np.array([0, 1, 4])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q10. Find the unique elements of x that are not present in y.\n"}
{"snippet": "def run_prob_cv(X, y, clf_class, roc=False, **kwargs):\n    kf = KFold(len(y), n_folds=5, shuffle=True)\n    y_prob = np.zeros((len(y),2))\n    for train_index, test_index in kf:\n        X_train, X_test = X[train_index], X[test_index]\n        y_train = y[train_index]\n        clf = clf_class(**kwargs)\n        clf.fit(X_train,y_train)\n        y_prob[test_index] = clf.predict_prob(X_test)\n    return y_prob\n", "intent": "RandomForestClassifier, KNeighborsClassifier and LogisticRegression have predict_prob() function \n"}
{"snippet": "_x = np.array([1, 2, 6, 4, 2, 3, 2])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q12. Get unique elements and their indices from x.\n"}
{"snippet": "hypothesis = tf.SparseTensor(\n    [[0, 0],[0, 1],[0, 2],[0, 4]],\n    [\"a\", \"b\", \"c\", \"a\"],\n    (1, 5)) \ntruth = tf.SparseTensor(\n    [[0, 0],[0, 2],[0, 4]],\n    [\"a\", \"c\", \"b\"],\n    (1, 6))\n", "intent": "Q13. Compute the edit distance between hypothesis and truth.\n"}
{"snippet": "_X = np.array([[1,2,3], [4,5,6]])\nX = tf.convert_to_tensor(_X)\nout = tf.cumsum(X, axis=1)\nprint(out.eval())\n_out = np.cumsum(_X, axis=1)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q1. Compute the cumulative sum of X along the second axis.\n"}
{"snippet": "_X = np.array([[1,2,3], [4,5,6]])\nX = tf.convert_to_tensor(_X)\nout = tf.cumprod(X, axis=1)\nprint(out.eval())\n_out = np.cumprod(_X, axis=1)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q2. Compute the cumulative product of X along the second axis.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [-1,-2,-3,-4], \n     [-10,-20,-30,-40],\n     [10,20,30,40]])\nX = tf.convert_to_tensor(_X)\nout = tf.segment_sum(X, [0, 0, 1, 1])\nprint(out.eval())\n", "intent": "Q3. Compute the sum along the first two elements and \nthe last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [1,1/2,1/3,1/4], \n     [1,2,3,4],\n     [-1,-1,-1,-1]])\nX = tf.convert_to_tensor(_X)\nout = tf.segment_prod(X, [0, 0, 1, 1])\nprint(out.eval())\n", "intent": "Q4. Compute the product along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,4,5,7], \n     [2,3,6,8], \n     [1,2,3,4],\n     [-1,-2,-3,-4]])\nX = tf.convert_to_tensor(_X)\nout = tf.segment_min(X, [0, 0, 1, 1])\nprint(out.eval())\n", "intent": "Q5. Compute the minimum along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,4,5,7], \n     [2,3,6,8], \n     [1,2,3,4],\n     [-1,-2,-3,-4]])\nX = tf.convert_to_tensor(_X)\nout = tf.segment_max(X, [0, 0, 1, 1])\nprint(out.eval())\n", "intent": "Q6. Compute the maximum along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [5,6,7,8], \n     [-1,-2,-3,-4],\n     [-5,-6,-7,-8]])\nX = tf.convert_to_tensor(_X)\nout = tf.segment_mean(X, [0, 0, 1, 1])\nprint(out.eval())\n", "intent": "Q7. Compute the mean along the first two elements and the last two elements of X separately.\n"}
{"snippet": "session = tf.Session()\n", "intent": "Once the TensorFlow graph has been created, we have to create a TensorFlow session which is used to execute the graph.\n"}
{"snippet": "_x = np.array([0, 1, 2, 5, 0])\n_y = np.array([0, 1, 4])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout = tf.setdiff1d(x, y)[0]\nprint(out.eval())\n_out = np.setdiff1d(_x, _y)\nassert np.array_equal(out.eval(), _out)\n", "intent": "Q10. Find the unique elements of x that are not present in y.\n"}
{"snippet": "_X = np.arange(1, 10).reshape(3, 3)\nX = tf.convert_to_tensor(_X)\nout = tf.where(X < 4, X, X*10)\nprint(out.eval())\n_out = np.where(_X < 4, _X, _X*10)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q11. Return the elements of X, if X < 4, otherwise X*10.\n"}
{"snippet": "_x = np.array([1, 2, 6, 4, 2, 3, 2])\nx = tf.convert_to_tensor(_x)\nout, indices = tf.unique(x)\nprint(out.eval())\nprint(indices.eval())\n_out, _indices = np.unique(_x, return_inverse=True)\nprint(\"sorted unique elements =\", _out)\nprint(\"indices =\", _indices)\n", "intent": "Q12. Get unique elements and their indices from x.\n"}
{"snippet": "_x = np.array([[1, 2, 4, 8], [2, 4, 6, 8]], dtype=np.float32)\nx = tf.convert_to_tensor(_x)\nout = ...\nwith tf.Session() as sess:\n    _out = sess.run(out)\n    print(_out)    \n    assert np.allclose(np.sum(_out, axis=-1), 1)\n", "intent": "Q3. Apply `softmax` to x.\n"}
{"snippet": "_x = np.array([[1, 2, 4, 8], [2, 4, 6, 8]], dtype=np.float32)\nprint(\"_x =\\n\" , _x)\nx = tf.convert_to_tensor(_x)\nout = ...\nwith tf.Session() as sess:\n    _out = sess.run(out)\n    print(\"_out =\\n\", _out) \n", "intent": "Q4. Apply `dropout` with keep_prob=.5 to x.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Q6. Apply 2 kernels of width-height (2, 2), stride 1, and same padding to x.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Q7. Apply 3 kernels of width-height (2, 2), stride 1, dilation_rate 2 and valid padding to x.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Q8. Apply 4 kernels of width-height (3, 3), stride 2, and same padding to x.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Q9. Apply 4 times of kernels of width-height (3, 3), stride 2, and same padding to x, depth-wise.\n"}
{"snippet": "net = MLP(3, .25, outtype=\"linear\")\nnet.fit(train, train_target, 100)\n", "intent": "* We don't know how many hidden neurons, we'll need, so let's try 3.\n* We will run this for 100 iterations\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Q11. Apply conv2d transpose with 5 kernels of width-height (3, 3), stride 2, and same padding to x.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Q12. Apply conv2d transpose with 5 kernels of width-height (3, 3), stride 2, and valid padding to x.\n"}
{"snippet": "_x = np.array([[1, 2, 4, 8], [2, 4, 6, 8]], dtype=np.float32)\nx = tf.convert_to_tensor(_x)\nout = tf.nn.softmax(x, dim=-1)\nwith tf.Session() as sess:\n    _out = sess.run(out)\n    print(_out)    \n    assert np.allclose(np.sum(_out, axis=-1), 1)\n", "intent": "Q3. Apply `softmax` to x.\n"}
{"snippet": "_x = np.array([[1, 2, 4, 8], [2, 4, 6, 8]], dtype=np.float32)\nprint(\"_x =\\n\" , _x)\nx = tf.convert_to_tensor(_x)\nout = tf.nn.dropout(x, keep_prob=0.5)\nwith tf.Session() as sess:\n    _out = sess.run(out)\n    print(\"_out =\\n\", _out) \n", "intent": "Q4. Apply `dropout` with keep_prob=.5 to x.\n"}
{"snippet": "x = tf.random_normal([8, 10])\nout = tf.contrib.layers.fully_connected(inputs=x, num_outputs=2, \n                                        activation_fn=tf.nn.sigmoid,\n                                        weights_initializer=tf.contrib.layers.xavier_initializer())\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(out))\n", "intent": "Q5. Apply a fully connected layer to x with 2 outputs and then an sigmoid function.\n"}
{"snippet": "_x = np.arange(1, 11)\nepsilon = 1e-12\nx = tf.convert_to_tensor(_x, tf.float32)\n", "intent": "Q1. Apply `l2_normalize` to `x`.\n"}
{"snippet": "_x = np.arange(1, 11)\nx = tf.convert_to_tensor(_x, tf.float32)\n", "intent": "Q2. Calculate the mean and variance of `x` based on the sufficient statistics.\n"}
{"snippet": "tf.reset_default_graph()\n_x = np.arange(1, 11)\nx = tf.convert_to_tensor(_x, tf.float32)\n", "intent": "Q3. Calculate the mean and variance of `x`.\n"}
{"snippet": "tf.reset_default_graph()\nx = tf.constant([1, 1, 2, 2, 2, 3], tf.float32)\nmean, variance = tf.nn.moments(x, [0])\nwith tf.Session() as sess:\n    print(sess.run([mean, variance]))\nunique_x, _, counts = tf.unique_with_counts(x)\nmean, variance = ...\nwith tf.Session() as sess:\n    print(sess.run([mean, variance]))\n", "intent": "Q4. Calculate the mean and variance of `x` using `unique_x` and `counts`.\n"}
{"snippet": "model = iris_clf.fit(data)\n", "intent": "* The Pipeline object behaves like a classifier\n"}
{"snippet": "tf.reset_default_graph()\nlogits = tf.random_normal(shape=[2, 5, 10])\nlabels = tf.convert_to_tensor(np.random.randint(0, 10, size=[2, 5]), tf.int32)\noutput = tf.nn....\nwith tf.Session() as sess:\n    print(sess.run(output))\n", "intent": "Q7. Compute softmax cross entropy between logits and labels. Note that the rank of them is not the same.\n"}
{"snippet": "logits = tf.random_normal(shape=[2, 5, 10])\nlabels = tf.convert_to_tensor(np.random.randint(0, 10, size=[2, 5]), tf.int32)\nlabels = tf.one_hot(labels, depth=10)\noutput = tf.nn....\nwith tf.Session() as sess:\n    print(sess.run(output))\n", "intent": "Q8. Compute softmax cross entropy between logits and labels.\n"}
{"snippet": "tf.reset_default_graph()\nx = tf.constant([0, 2, 1, 3, 4], tf.int32)\nembedding = tf.constant([0, 0.1, 0.2, 0.3, 0.4], tf.float32)\noutput = tf.nn....\nwith tf.Session() as sess:\n    print(sess.run(output))\n", "intent": "Q9. Map tensor `x` to the embedding.\n"}
{"snippet": "_x = np.arange(1, 11)\nepsilon = 1e-12\nx = tf.convert_to_tensor(_x, tf.float32)\noutput = tf.nn.l2_normalize(x, dim=0, epsilon=epsilon)\nwith tf.Session() as sess:\n    _output = sess.run(output)\n    assert np.allclose(_output, _x / np.sqrt(np.maximum(np.sum(_x**2), epsilon)))\nprint(_output)\n", "intent": "Q1. Apply `l2_normalize` to `x`.\n"}
{"snippet": "_x = np.arange(1, 11)\nx = tf.convert_to_tensor(_x, tf.float32)\ncounts_, sum_, sum_of_squares_, _ = tf.nn.sufficient_statistics(x, [0])\nmean, variance = tf.nn.normalize_moments(counts_, sum_, sum_of_squares_, shift=None)\nwith tf.Session() as sess:\n    _mean, _variance = sess.run([mean, variance])\nprint(_mean, _variance)\n", "intent": "Q2. Calculate the mean and variance of `x` based on the sufficient statistics.\n"}
{"snippet": "tf.reset_default_graph()\n_x = np.arange(1, 11)\nx = tf.convert_to_tensor(_x, tf.float32)\noutput = tf.nn.moments(x, [0])\nwith tf.Session() as sess:\n    _mean, _variance = sess.run(output)\nprint(_mean, _variance)\n", "intent": "Q3. Calculate the mean and variance of `x`.\n"}
{"snippet": "tf.reset_default_graph()\nx = tf.constant([1, 1, 2, 2, 2, 3], tf.float32)\nmean, variance = tf.nn.moments(x, [0])\nwith tf.Session() as sess:\n    print(sess.run([mean, variance]))\nunique_x, _, counts = tf.unique_with_counts(x)\nmean, variance = tf.nn.weighted_moments(unique_x, [0], counts)\nwith tf.Session() as sess:\n    print(sess.run([mean, variance]))\n", "intent": "Q4. Calculate the mean and variance of `x` using `unique_x` and `counts`.\n"}
{"snippet": "tf.reset_default_graph()\nlogits = tf.random_normal(shape=[2, 5, 10])\nlabels = tf.convert_to_tensor(np.random.randint(0, 10, size=[2, 5]), tf.int32)\noutput = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\nwith tf.Session() as sess:\n    print(sess.run(output))\n", "intent": "Q7. Compute softmax cross entropy between logits and labels. Note that the rank of them is not the same.\n"}
{"snippet": "logits = tf.random_normal(shape=[2, 5, 10])\nlabels = tf.convert_to_tensor(np.random.randint(0, 10, size=[2, 5]), tf.int32)\nlabels = tf.one_hot(labels, depth=10)\noutput = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\nwith tf.Session() as sess:\n    print(sess.run(output))\n", "intent": "Q8. Compute softmax cross entropy between logits and labels.\n"}
{"snippet": "y = np.zeros(len(epochs.events), dtype=int)\ny[epochs.events[:, 2] == 3] = 1\ncv = StratifiedKFold(y=y)  \ngat = GeneralizationAcrossTime(predict_mode='cross-validation', n_jobs=1,\n                               cv=cv, scorer=roc_auc_score)\ngat.fit(epochs, y=y)\ngat.score(epochs)\ngat.plot()\ngat.plot_diagonal()\n", "intent": "Generalization Across Time\n--------------------------\nHere we'll use a stratified cross-validation scheme.\n"}
{"snippet": "x = tf.constant([[1, 0, 0, 0],\n                 [0, 0, 2, 0],\n                 [0, 0, 0, 0]], dtype=tf.int32)\nsp = tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\nprint(sp.eval())\n", "intent": "Q1. Convert tensor x into a `SparseTensor`.\n"}
{"snippet": "def dense_to_sparse(tensor):\n    indices = tf.where(tf.not_equal(tensor, 0))\n    return tf.SparseTensor(indices=indices,\n                           values=tf.gather_nd(tensor, indices) - 1,  \n                           dense_shape=tf.to_int64(tf.shape(tensor)))\nprint(dense_to_sparse(x).eval())\n", "intent": "Q3. Let's write a custom function that converts a SparseTensor to Tensor. Complete it.\n"}
{"snippet": "output = tf.sparse_to_dense(sparse_indices=[[0, 0], [1, 2]], sparse_values=[1, 2], output_shape=[3, 4])\nprint(output.eval())\nprint(\"Check if this is identical with x:\\n\", x.eval())\n", "intent": "Q4. Convert the SparseTensor `sp` to a Tensor using `tf.sparse_to_dense`.\n"}
{"snippet": "output = tf.sparse_tensor_to_dense(s)\nprint(output.eval())\nprint(\"Check if this is identical with x:\\n\", x.eval())\n", "intent": "Q5. Convert the SparseTensor `sp` to a Tensor using `tf.sparse_tensor_to_dense`.\n"}
{"snippet": "def dense_to_sparse(tensor):\n    indices = tf.where(tf.not_equal(tensor, 0))\n    return tf.SparseTensor(indices=indices,\n                           values=...,  \n                           dense_shape=tf.to_int64(tf.shape(tensor)))\nprint(dense_to_sparse(x).eval())\n", "intent": "Q3. Let's write a custom function that converts a SparseTensor to Tensor. Complete it.\n"}
{"snippet": "_X = np.arange(1, 1*2*3*4 + 1).reshape((1, 2, 3, 4))\nX = tf.convert_to_tensor(_X)\n", "intent": "Q23. Given X below, reverse the last dimension.\n"}
{"snippet": "_X = np.ones((1, 2, 3))\nX = tf.convert_to_tensor(_X)\n", "intent": "Q24. Given X below, permute its dimensions such that the new tensor has shape (3, 1, 2).\n"}
{"snippet": "_X = np.arange(1, 10).reshape((3, 3))\nX = tf.convert_to_tensor(_X)\n", "intent": "Q25. Given X, below, get the first, and third rows.\n"}
{"snippet": "_X = np.arange(1, 10).reshape((3, 3))\nX = tf.convert_to_tensor(_X)\n", "intent": "Q26. Given X below, get the elements 5 and 7.\n"}
{"snippet": "time_decod.fit(X, y)\npatterns = get_coef(time_decod, 'patterns_', inverse_transform=True)\nstc = stcs[0]  \nvertices = [stc.lh_vertno, np.array([], int)]  \nstc_feat = mne.SourceEstimate(np.abs(patterns), vertices=vertices,\n                              tmin=stc.tmin, tstep=stc.tstep, subject='sample')\nbrain = stc_feat.plot(views=['lat'], transparent=True,\n                      initial_time=0.1, time_unit='s')\n", "intent": "To investigate weights, we need to retrieve the patterns of a fitted model\n"}
{"snippet": "_X = np.arange(1, 7).reshape((2, 3))\nX = tf.convert_to_tensor(_X)\nout = tf.tile(X, [1, 3])\nprint(out.eval())\nassert np.allclose(out.eval(), np.tile(_X, [1, 3]))\n", "intent": "Q17. Lex X be a tensor<br/>\n[[ 1 2 3]<br/>\n [ 4 5 6].<br/>\nCreate a tensor looking like <br/>\n[[ 1 2 3 1 2 3 1 2 3 ]<br/>\n [ 4 5 6 4 5 6 4 5 6 ]].\n"}
{"snippet": "_X = np.arange(1, 7).reshape((2, 3))\nX = tf.convert_to_tensor(_X)\nout = tf.pad(X, [[2, 0], [0, 3]])\nprint(out.eval())\nassert np.allclose(out.eval(), np.pad(_X, [[2, 0], [0, 3]], 'constant', constant_values=[0, 0]))\n", "intent": "Q18. Lex X be a tensor <br/>\n[[ 1 2 3]<br/>\n [ 4 5 6].<br/>\nPad 2 * 0's before the first dimension, 3 * 0's after the second dimension.\n"}
{"snippet": "_X = np.arange(1, 1*2*3*4 + 1).reshape((1, 2, 3, 4))\nX = tf.convert_to_tensor(_X)\nout = tf.reverse(X, [-1]) \nprint(out.eval())\nassert np.allclose(out.eval(), _X[:, :, :, ::-1])\n", "intent": "Q23. Given X below, reverse the last dimension.\n"}
{"snippet": "_X = np.ones((1, 2, 3))\nX = tf.convert_to_tensor(_X)\nout = tf.transpose(X, [2, 0, 1])\nprint(out.eval().shape)\nassert np.allclose(out.eval(), np.transpose(_X))\n", "intent": "Q24. Given X below, permute its dimensions such that the new tensor has shape (3, 1, 2).\n"}
{"snippet": "_X = np.arange(1, 10).reshape((3, 3))\nX = tf.convert_to_tensor(_X)\nout1 = tf.gather(X, [0, 2])\nout2 = tf.gather_nd(X, [[0], [2]])\nassert np.allclose(out1.eval(), out2.eval())\nprint(out1.eval())\nassert np.allclose(out1.eval(), _X[[0, 2]])\n", "intent": "Q25. Given X, below, get the first, and third rows.\n"}
{"snippet": "_X = np.arange(1, 10).reshape((3, 3))\nX = tf.convert_to_tensor(_X)\nout = tf.gather_nd(X, [[1, 1], [2, 0]])\nprint(out.eval())\nassert np.allclose(out.eval(), _X[[1, 2], [1, 0]])\n", "intent": "Q26. Given X below, get the elements 5 and 7.\n"}
{"snippet": "_x = np.array([0, 1, 2, 3])\n_y = np.array([True, False, False, True])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout = tf.boolean_mask(x, y)\nprint(out.eval())\nassert np.allclose(out.eval(), _x[_y])\n", "intent": "Q30. Let x be a tensor [0, 1, 2, 3] and y be a tensor [True, False, False, True].<br/>\nApply mask y to x.\n"}
{"snippet": "w = tf.Variable(1.0, name=\"Weight\")\nassign_op = ...\nwith tf.Session() as sess:\n    sess.run(w.initializer)\n    for _ in range(10):\n        print(sess.run(w), \"=>\", end=\"\")\n        sess.run(assign_op)\n", "intent": "Q1. Complete this code.\n"}
{"snippet": "w1 = tf.Variable(1.0)\nw2 = tf.Variable(2.0)\nw3 = tf.Variable(3.0)\nout = w1 + w2 + w3\ninit_op = ...\nwith tf.Session() as sess:\n    sess.run(init_op) \n    print(sess.run(out))\n", "intent": "Q2. Complete this code.\n"}
{"snippet": "spoc.fit(X, y)\nlayout = read_layout('CTF151.lay')\nspoc.plot_patterns(meg_epochs.info, layout=layout)\n", "intent": "Plot the contributions to the detected components (i.e., the forward model)\n"}
{"snippet": "g = tf.Graph()\nwith g.as_default():\n    W = tf.Variable([[0,1],[2,3]], name=\"Weight\", dtype=tf.float32)\n    print(\"Q5.\", ...)\n    print(\"Q6.\", ...)\n    print(\"Q7.\", ...)\n    print(\"Q8.\", ...)\n    print(\"Q9.\", ...)\n    print(\"Q10.\", ...)\n", "intent": "Q5-8. Complete this code.\n"}
{"snippet": "tf.reset_default_graph()\nw1 = tf.Variable(1.0, name=\"weight1\")\nw2 = tf.Variable(2.0, name=\"weight2\", trainable=False)\nw3 = tf.Variable(3.0, name=\"weight3\")\nwith tf.Session() as sess:\n    sess.run(...)\n    for v in tf.global_variables():\n        print(\"global variable =>\", ...)\n    for v in tf.trainable_variables():\n        print(\"trainable_variable =>\", ...)\n", "intent": "Q11-15. Complete this code.\n"}
{"snippet": "g = tf.Graph()\nwith g.as_default():\n    with tf.variable_scope(\"foo\"):\n        v = tf.get_variable(\"vv\", [1,])  \n    ...\nassert v1 == v    \n", "intent": "Q16. Complete this code.\n"}
{"snippet": "with tf.variable_scope(\"foo\"):\n    with tf.variable_scope(\"bar\"):\n        v = tf.get_variable(\"vv\", [1])\n        print(\"v.name =\", v.name)\n", "intent": "Q17. Predict the result of this code.\n"}
{"snippet": "value = [0, 1, 2, 3, 4, 5, 6, 7]\ninit = ...\ntf.reset_default_graph()\nx = tf.get_variable('x', shape=[2, 4], initializer=init)\nwith tf.Session() as sess:\n    sess.run(x.initializer)\n    print(\"x =\\n\", sess.run(x))\n", "intent": "Q18. Complete this code.\n"}
{"snippet": "init = ...\ntf.reset_default_graph()\nx = tf.get_variable('x', shape=[10, 1000], initializer=init)\nwith tf.Session():\n    x.initializer.run()\n    _x = x.eval()\n    print(\"Make sure the mean\", np.mean(_x), \"is close to 0\" )\n    print(\"Make sure the standard deviation\", np.std(_x), \"is close to 2\" )\n", "intent": "Q19. Complete this code.\n"}
{"snippet": "tf.reset_default_graph()\nprint(\"Of course, there're no variables since we reset the graph. See\", tf.global_variables())\nwith tf.Session() as sess:\n    new_saver = ...\n    new_saver.restore(sess, 'model/my-model-10000')\n    for v in tf.global_variables():\n        print(\"Now we have a variable\", v.name)\n", "intent": "Q22. Complete the code. Make sure you've done questions 14-15.\n"}
{"snippet": "w = tf.Variable(1.0, name=\"weight\")\nwith tf.Session() as sess:\n    sess.run(w.initializer)\n    print(sess.run(w))\n", "intent": "Q0. Create a variable `w` with an initial value of 1.0 and name `weight`.\nThen, print out the value of `w`.\n"}
{"snippet": "w = tf.Variable(1.0, name=\"Weight\")\nassign_op = w.assign(w + 1.0)\nwith tf.Session() as sess:\n    sess.run(w.initializer)\n    for _ in range(10):\n        print(sess.run(w), \"=>\", end=\"\")\n        sess.run(assign_op)\n", "intent": "Q1. Complete this code.\n"}
{"snippet": "time_decod.fit(X, y)\npatterns = get_coef(time_decod, 'patterns_', inverse_transform=True)\nstc = stcs[0]  \nvertices = [stc.lh_vertno, np.array([], int)]  \nstc_feat = mne.SourceEstimate(np.abs(patterns), vertices=vertices,\n                              tmin=stc.tmin, tstep=stc.tstep, subject='sample')\nbrain = stc_feat.plot(views=['lat'], transparent=True,\n                      initial_time=0.1, time_unit='s',\n                      subjects_dir=subjects_dir)\n", "intent": "To investigate weights, we need to retrieve the patterns of a fitted model\n"}
{"snippet": "V = tf.Variable(tf.truncated_normal([1, 10]))\nW = tf.Variable(V.initialized_value() * 2.0)\ninit_op = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init_op) \n    _V, _W = sess.run([V, W])\n    print(_V)\n    print(_W)\n    assert np.array_equiv(_V * 2.0, _W)\n", "intent": "Q3-4. Complete this code.\n"}
{"snippet": "g = tf.Graph()\nwith g.as_default():\n    W = tf.Variable([[0,1],[2,3]], name=\"Weight\", dtype=tf.float32)\n    print(\"Q5.\", W.name)\n    print(\"Q6.\", W.op.name)\n    print(\"Q7.\", W.dtype)\n    print(\"Q8.\", W.get_shape().as_list())\n    print(\"Q9.\", W.get_shape().ndims)\n    print(\"Q10.\", W.graph == g)\n", "intent": "Q5-8. Complete this code.\n"}
{"snippet": "tf.reset_default_graph()\nw1 = tf.Variable(1.0, name=\"weight1\")\nw2 = tf.Variable(2.0, name=\"weight2\", trainable=False)\nw3 = tf.Variable(3.0, name=\"weight3\")\nwith tf.Session() as sess:\n    sess.run(tf.variables_initializer([w1, w2]))\n    for v in tf.global_variables():\n        print(\"global variable =>\", v.name)\n    for v in tf.trainable_variables():\n        print(\"trainable_variable =>\", v.name)\n", "intent": "Q11-15. Complete this code.\n"}
{"snippet": "g = tf.Graph()\nwith g.as_default():\n    with tf.variable_scope(\"foo\"):\n        v = tf.get_variable(\"vv\", [1,])  \n    with tf.variable_scope(\"foo\", reuse=True):\n        v1 = tf.get_variable(\"vv\")  \nassert v1 == v    \n", "intent": "Q16. Complete this code.\n"}
{"snippet": "value = [0, 1, 2, 3, 4, 5, 6, 7]\ninit = tf.constant_initializer(value)\ntf.reset_default_graph()\nx = tf.get_variable('x', shape=[2, 4], initializer=init)\nwith tf.Session() as sess:\n    sess.run(x.initializer)\n    print(\"x =\\n\", sess.run(x))\n", "intent": "Q18. Complete this code.\n"}
{"snippet": "init = tf.random_normal_initializer(mean=0, stddev=2)\ntf.reset_default_graph()\nx = tf.get_variable('x', shape=[10, 1000], initializer=init)\nwith tf.Session():\n    x.initializer.run()\n    _x = x.eval()\n    print(\"Make sure the mean\", np.mean(_x), \"is close to 0\" )\n    print(\"Make sure the standard deviation\", np.std(_x), \"is close to 2\" )\n", "intent": "Q19. Complete this code.\n"}
{"snippet": "tf.reset_default_graph()\nprint(\"Of course, there're no variables since we reset the graph. See\", tf.global_variables())\nwith tf.Session() as sess:\n    new_saver = tf.train.import_meta_graph('model/my-model-10000.meta')\n    new_saver.restore(sess, 'model/my-model-10000')\n    for v in tf.global_variables():\n        print(\"Now we have a variable\", v.name)\n", "intent": "Q22. Complete the code. Make sure you've done questions 14-15.\n"}
{"snippet": "weightVar = tf.ones(shape=[2,3], dtype=tf.int32)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    result = sess.run(weightVar)\n    print(\"result=\\n{}\".format(result))\n", "intent": "Q0. Create a variable w with an initial value of 1.0 and name weight. Then, print out the value of w.\n"}
{"snippet": "with tf.Session() as sess:\n    zeros = tf.zeros(shape=[2,3])\n    print(sess.run(zeros))\n    assert np.allclose(sess.run(zeros), np.zeros([2, 3]))\n", "intent": "Q1. Create a tensor with shape [2, 3] with all elements set to zero. It should pass the assertion\n"}
{"snippet": "import statsmodels.formula.api as smf\nlm = smf.ols(formula='Sales ~ TV', data=data).fit()\nlm.params\n", "intent": "Let's use **Statsmodels** to estimate the model coefficients for the advertising data:\n"}
{"snippet": "with tf.Session() as sess:\n    _X = np.array([[1,3,5], [4,6,8]])\n    out = tf.constant(_X, dtype=tf.float32) \n    print(sess.run(out))\n    assert np.allclose(sess.run(out), np.array([[1, 3, 5], [4, 6, 8]], dtype=np.float32))\n", "intent": "Q4. Create a constant tensor of [[1, 3, 5], [4, 6, 8]], with dtype=float32\n"}
{"snippet": "with tf.Session() as sess:\n    _X = np.linspace(5., 10., 50)\n    out = tf.convert_to_tensor(_X)\n    print(sess.run(out))\n    assert np.allclose(sess.run(out), np.linspace(5., 10., 50))\n", "intent": "Q5. Create a 1-D tensor of 50 evenly spaced numberts between 5 and 10 (inclusive). *Hint: Look up the equivalent of np linspace for tf*\n"}
{"snippet": "with tf.Session() as sess:\n    X = tf.random_normal([3, 2], mean=0,  stddev=0.01)\n    print(sess.run(X))\n", "intent": "Q6. Create a random tensor of the shape [3, 2], with elements from a normal distribution of mean=0, standard deviation=2.\n"}
{"snippet": "with tf.Session() as sess:\n    _X = np.array([[1, 2], [3, 4], [5, 6]])\n    X = tf.convert_to_tensor(_X)\n    out = tf.random_shuffle(X)\n    print(sess.run(out))\n", "intent": "Q7. Randomly shuffle the data in matrix X using tf.random_shuffle()\n"}
{"snippet": "with tf.Session() as sess:\n    _x = tf.random_uniform([], -1, 1)\n    _y = tf.random_uniform([], -1, 1)\n    out = \n    print(sess.run(_x),sess.run(_y), sess.run(_out))\n", "intent": "Q8. Let x and y be random 0-D tensors. Return x + y if x < y and x - y otherwise. Use tf.cond()\n"}
{"snippet": "_x = np.linspace(-10., 10., 1000)\nx = tf.convert_to_tensor(_x)\n", "intent": "Q9. Apply relu, elu, and softplus to x. Then observe the plot\nThe package tf.nn contains all of these methods. \n"}
{"snippet": "_x = np.linspace(-10., 10., 1000)\nx = tf.convert_to_tensor(_x)\n", "intent": "Q2. Apply sigmoid and tanh to x using tf.nn.sigmoid() and tf.nn.tanh()\n"}
{"snippet": "_x = np.array([[1, 2, 4, 8], [2, 4, 6, 8]], dtype=np.float32)\nx = tf.convert_to_tensor(_x)\nout = tf.nn.softmax(x) \nwith tf.Session() as sess:\n    _out = sess.run(out)\n    print(_out)    \n    assert np.allclose(np.sum(_out, axis=-1), 1)\n", "intent": "Q3. Apply softmax to x. Assertion should pass if you did this correctly\n"}
{"snippet": "x = tf.random_normal([8, 10])\nout = tf.contrib.layers.fully_connected(inputs=x, num_outputs=5) \nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(out))\n", "intent": "Q4. Apply a fully connected layer to x with 2 outputs and then an sigmoid function. We provide the prepared contrib fully_connected layer\n"}
{"snippet": "feature_cols = ['TV', 'Radio', 'Newspaper']\nX = data[feature_cols]\ny = data.Sales\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X, y)\nprint lm.intercept_\nprint lm.coef_\n", "intent": "Let's redo some of the Statsmodels code above in scikit-learn:\n"}
{"snippet": "import tensorflow as tf\nx = tf.Variable(3, name='x') \ny = tf.Variable(4, name='y')\nf = x*x*y + y + 2\n", "intent": "The following code creates the graph represented above. \n"}
{"snippet": "x1 = tf.Variable(1)\nx1.graph is tf.get_default_graph()\n", "intent": "Each graph seems to be related to an execution plan.\n"}
{"snippet": "kernel = 1* RBF(length_scale=[0.1], length_scale_bounds=(0.00001, 10000)) \\\n        + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e10))\ngp = GaussianProcessRegressor(kernel=kernel,\n                              n_restarts_optimizer=10,random_state=999)\n", "intent": "The above plot shows a skewed data towards the right indicating a noisy dataset.\nAdd a noise kernel to adjust for the noise in the data.\n"}
{"snippet": "callback=[keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0009, patience=2, verbose=0, mode='auto'),\n          keras.callbacks.ModelCheckpoint(filepath='imdb.model.best.hdf5',verbose=1, save_best_only=True)\n         ]\nmodel_history=model.fit(x_train, y_train, epochs=100, batch_size=500, verbose=2, validation_split=0.2, callbacks=callback)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def sigmoid(x):\n", "intent": "$$\n\\sigma = \\frac{1}{1 + e^{-x}}\n$$\n"}
{"snippet": "skipgram = Sequential()\nskipgram.add(Embedding(input_dim=vocab_size, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\nskipgram.add(Reshape((dim,)))\nskipgram.add(Dense(input_dim=dim, units=vocab_size, activation='softmax'))\nSVG(model_to_dot(skipgram, show_shapes=True).create(prog='dot', format='svg'))\n", "intent": "- Lastly, we create the (shallow) network!\n"}
{"snippet": "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\nrun_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n", "intent": "Why do we have 121 parameters?  Does that make sense?\nLet's fit our model for 200 epochs.\n"}
{"snippet": "batch_size = 32\nopt = keras.optimizers.rmsprop(lr=0.0005, decay=1e-6)\nmodel_1.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\nmodel_1.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=15,\n              validation_data=(x_test, y_test),\n              shuffle=True)\n", "intent": "We still have 181K parameters, even though this is a \"small\" model.\n"}
{"snippet": "batch_size = 32\nopt = keras.optimizers.rmsprop(lr=0.0005, decay=1e-6)\nmodel_1.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n", "intent": "We still have 181K parameters, even though this is a \"small\" model.\n"}
{"snippet": "feature_cols = ['TV', 'Radio', 'Newspaper', 'IsLarge']\nX = data[feature_cols]\ny = data.Sales\nlm = LinearRegression()\nlm.fit(X, y)\nzip(feature_cols, lm.coef_)\n", "intent": "Let's redo the multiple linear regression and include the **IsLarge** predictor:\n"}
{"snippet": "model = fit_SimpleRNN(train_X, train_y, cell_units=10, epochs=10)\n", "intent": "Great, now let's use this function to fit a very simple baseline model.\n"}
{"snippet": "KNN = KNeighborsClassifier(n_neighbors=5)\n", "intent": "Create a N-Nearest Neighbors classifier of 5 neighbors. (hint: KNeighborsClassifier?)\n"}
{"snippet": "knn = KNN.fit(X_train, y_train)\n", "intent": "Fit the model to the training set. (hint: knn.fit?)\n"}
{"snippet": "from sklearn.ensemble import VotingClassifier\nestimators = [('LR_L2', LR_L2), ('GBC', GV_GBC)]\nVC = VotingClassifier(estimators, voting='soft')\nVC = VC.fit(X_train, y_train)\n", "intent": "And now the stacked model.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression().fit(X_new, Y_new)\n", "intent": " Repeat Model building with new training data after removing higly correlated columns\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    return tf.contrib.layers.embed_sequence(input_data, vocab_size=vocab_size, embed_dim= embed_dim)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "new_docs = ['He watches basketball and baseball', 'Julie likes to play basketball', 'Jane loves to play baseball']\nnew_term_freq_matrix = tfidf_vectorizer.transform(new_docs)\nprint tfidf_vectorizer.vocabulary_\nprint new_term_freq_matrix.todense()\n", "intent": "And we can fit new observations into this vocabulary space like so:\n"}
{"snippet": "lr = LinearRegression().fit(response_design, motor_data)\n", "intent": "**(4)** Fit a linear model with the response design matrix as the independent variables and the voxel data as dependent variables.\n"}
{"snippet": "fake_linear_x5,fake_linear_y5 = create_fake_linear(100, 1, 0, .3)\nfake_linear_x6,fake_linear_y6 = create_fake_linear(100, 2, 0, .3)\nfake_linear_x7,fake_linear_y7 = create_fake_linear(100, .3, 0, .3)\n", "intent": "And now we'll create some fake linear data that has noise.\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators=100, random_state=17)\nrf.fit(X_train, y_train)\n", "intent": "Train a random forest **(RandomForestClassifier)**. Set the number of trees to 100 and use **random_state = 17**.\n"}
{"snippet": "fake_linear_x8, fake_linear_y8 = create_fake_linear(100, 1.5, 5, .5)\nfake_linear_x9, fake_linear_y9 = create_fake_linear(100, 5, -.4, .5)\nfake_linear_x10, fake_linear_y10 = create_fake_linear(100, -2, 2, .5)\n", "intent": "Again we did perfect because there was no noise in this data. Let's add some noise and see how we do.\n"}
{"snippet": "reg_model1 = LinearRegression()\nreg_model1.fit(fake_linear_x10_2D, fake_linear_y10_2D)\n", "intent": "Now we'll create a linear regression object, and use it to fit the fake x and y 2-D data.\n"}
{"snippet": "reg_model2 = LinearRegression()\nreg_model2.fit(fake_linear_x9.reshape(-1,1), fake_linear_y9.reshape(-1,1))\n", "intent": "1\\. Fit a linear regression model to the `fake_linear_x9` and `fake_linear_y9`. Print out the intercept and coefficient.\n"}
{"snippet": "reg_model_faces = LinearRegression()\nreg_model_faces.fit(faces_response_vec, data_faces)\n", "intent": "Now let's apply linear models to some real fMRI data. We'll fit the faces response vector to our FFA voxel and see how we do.\n"}
{"snippet": "reg_model_full = LinearRegression()\nreg_model_full.fit(designmat_full, data_faces_2D)\n", "intent": "Now we'll fit a multiple regression model in the same way.\n"}
{"snippet": "reg_model_simple1 = LinearRegression()\nreg_model_simple1.fit(fake_multi_x1.reshape(-1,1), fake_multiple_y1.reshape(-1,1))\nreg_model_simple2 = LinearRegression()\nreg_model_simple2.fit(fake_multi_x2.reshape(-1,1), fake_multiple_y1.reshape(-1,1))\nprint('Real Coefficient 1: %.03f, Simple Estimated Coefficient 1: %.03f' % (coef_real1, reg_model_simple1.coef_[0][0]))\nprint('Real Coefficient 2: %.03f, Simple Estimated Coefficient 2: %.03f' % (coef_real2, reg_model_simple2.coef_[0][0]))\n", "intent": "Now let's use 2 simple linear models to estimate the two coefficeints separately.\n"}
{"snippet": "designmat_fake_multiple = np.stack((fake_multi_x1, fake_multi_x2), axis=1)\nreg_model_multi1 = LinearRegression()\nreg_model_multi1.fit(designmat_fake_multiple, fake_multiple_y1.reshape(-1,1))\nprint('Real Coefficient 1: %.03f, Simple Estimated Coefficient 1: %.03f' % (coef_real1, reg_model_multi1.coef_[0][0]))\nprint('Real Coefficient 2: %.03f, Simple Estimated Coefficient 2: %.03f' % (coef_real2, reg_model_multi1.coef_[0][1]))\n", "intent": "That's not so great. Now let's see what happens when we put both independent variables in the model.\n"}
{"snippet": "reg_model_full = LinearRegression()\nreg_model_full.fit(designmat_full, five_voxels)\n", "intent": "Now we'll fit a multiple regression model in the same way.\n"}
{"snippet": "reg_model_simple1 = LinearRegression()\nreg_model_simple1.fit(fake_multi_x1.reshape(-1,1), fake_multiple_y1.reshape(-1,1))\nreg_model_simple2 = LinearRegression()\nreg_model_simple2.fit(fake_multi_x2.reshape(-1,1), fake_multiple_y1.reshape(-1,1))\nprint('Real Coefficient 1: %.03f, Simple Estimated Coefficient 1: %.03f' % (coef_real1, reg_model_simple1.coef_[0][0]))\nprint('Real Coefficient 2: %.03f, Simple Estimated Coefficient 2: %.03f' % (coef_real2, reg_model_simple2.coef_[0][0]))\n", "intent": "Now let's use 2 simple linear models to estimate the two coefficients separately.\n"}
{"snippet": "lr = LogisticRegression(random_state=5, class_weight='balanced')\n", "intent": "Now, we will create a `LogisticRegression` model and use `class_weight='balanced'` to make up for our unbalanced classes.\n"}
{"snippet": "fake_noise_sd = 5\nfake_x2, fake_y2 = create_fake_linear(fake_n, fake_slope, fake_intercept, fake_noise_sd)\n", "intent": "1\\. Run the cell below to make some new fake data that has more noise. \n"}
{"snippet": "fake_x, fake_y = create_fake_linear(fake_n, fake_slope, fake_intercept, fake_noise_sd)\n", "intent": "Generate some fake data using the values above\n"}
{"snippet": "model_fake = LinearRegression()\nmodel_fake.fit(fake_x_train, fake_y_train)\nprint('Original Data')\nprint('The estimated weight is: %.04f' % (model_fake.coef_[0][0]))\n", "intent": "Fit the model on the fake training data\n"}
{"snippet": "model_fake_outlier = LinearRegression()\nmodel_fake_outlier.fit(fake_x_train, fake_y_train_outlier)\nprint('\\nOutlier Training Data')\nprint('The estimated weight is: %.04f' % (model_fake_outlier.coef_[0][0]))\n", "intent": "Fit the model on the fake training data that has an outlier\n"}
{"snippet": "model_gradient = LinearRegression()\nmodel_gradient.fit(smoothed[:1260].reshape(1260, -1), data_train)\n", "intent": "It doesn't really look like much! But you can see roughly where the seal was.\nNow we'll use this image as an encoding model!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded_sequence = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev = 0.05))\n    embedded_sequence = tf.nn.embedding_lookup(embedded_sequence, input_data)\n    return embedded_sequence\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "svc_linear = LinearSVC()\nsvc_linear.fit(X_train, y_train)\nprint('Linear SVC classification accuracy on training set: {:.3f}'.format(svc_linear.score(X_train, y_train)))\nprint('Linear SVC classification accuracy on test set: {:.3f}'.format(svc_linear.score(X_test, y_test)))\n", "intent": "Train a [`LinearSVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n"}
{"snippet": "svc_rbf = SVC(kernel='rbf')\nsvc_rbf.fit(X_train, y_train)\nsvc_poly = SVC(kernel='poly', degree=2)\nsvc_poly.fit(X_train, y_train)\nprint('RBF SVC classification accuracy on training set: {:.3f}'.format(svc_rbf.score(X_train, y_train)))\nprint('RBF SVC classification accuracy on test set: {:.3f}'.format(svc_rbf.score(X_test, y_test)))\nprint('Poly SVC classification accuracy on training set: {:.3f}'.format(svc_poly.score(X_train, y_train)))\nprint('Poly SVC classification accuracy on test set: {:.3f}'.format(svc_poly.score(X_test, y_test)))\n", "intent": "By using the [`SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"}
{"snippet": "train_sizes = np.linspace(0.05, 1, 20)\nN_train, val_train, val_test = learning_curve(SVC(kernel='linear', C=1E-6),\n                                              X_proj, y, train_sizes)\n", "intent": "These scores are statistically equivalent: it seems that our projection is not losing significant information relevant to the classification!\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42, \n                            class_weight='balanced')\n", "intent": "Initialize Random Forest with 100 trees and balance target classes:\n"}
{"snippet": "def forward_propagation(self, x):\n    T = len(x)\n    s = np.zeros((T + 1, self.hidden_dim))\n    s[-1] = np.zeros(self.hidden_dim)\n    o = np.zeros((T, self.word_dim))\n    for t in np.arange(T):\n        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n        o[t] = softmax(self.V.dot(s[t]))\n    return [o, s]\nRNNNumpy.forward_propagation = forward_propagation\n", "intent": "Next, let's implement the forward propagation (predicting word probabilities) defined by our equations above:\n"}
{"snippet": "model.fit(x_train, y_train, epochs=1, batch_size=100, validation_split=0.1, shuffle=True)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "with train_graph.as_default():\n    saver = tf.train.Saver()\nwith tf.Session(graph=train_graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints_large'))\n    embed_mat = sess.run(embedding)\n", "intent": "Restore the trained network if you need to:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1), name='embedding')\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = tflearn.DNN(net, tensorboard_verbose=1)\nmodel.fit({'input': X}, {'target': Y}, n_epoch=20,\n           validation_set=({'input': testX}, {'target': testY}),\n           snapshot_step=100, show_metric=True, run_id='convnet_mnist')\n", "intent": "Train the network and use the test data as the validation set. \n"}
{"snippet": "base_model = applications.ResNet50(weights = \"imagenet\", include_top=False, input_shape = (224, 224, 3))\nx = base_model.output\nx = Flatten()(x)\nx = Dense(15000, activation=\"softmax\")(x)\nmodel_ResNet50 = Model(input = base_model.input, output = x)\nfor layer in base_model.layers:\n    layer.trainable = False\nmodel_ResNet50.summary()\n", "intent": "ResNet-50, Inception, and Xception.\n"}
{"snippet": "train_dataset = dsets.MNIST(root='../data', \n                            train=True, \n                            transform=transforms.ToTensor(),  \n                            download=True)\ntest_dataset = dsets.MNIST(root='../data', \n                           train=False, \n                           transform=transforms.ToTensor())\n", "intent": "Now, we'll load the MNIST data. First time we may have to download the data, which can take a while.\n"}
{"snippet": "train_dataset = dsets.MNIST(root='./data',\n                           train=True,\n                           transform=transforms.ToTensor(),\n                           download=True)\ntest_dataset = dsets.MNIST(root='./data',\n                           train=False,\n                           transform=transforms.ToTensor())\n", "intent": "Now, we'll load the MNIST data\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "intent": "Loading a pretrained model and reseting final fully connected layer.\n"}
{"snippet": "linreg = LinearRegression()\nlinreg.fit(X_train_scaled, y_train);\n", "intent": "**Train a simple linear regression model (Ordinary Least Squares).**\n"}
{"snippet": "from sklearn import manifold\niso = manifold.Isomap(n_neighbors=6, n_components=2)\niso.fit(X)\nmanifold_data = iso.transform(X)\nmanifold_data.shape\n", "intent": "**Excerise 3:** Apply ISOMAP on the data\n"}
{"snippet": "iso = manifold.Isomap(n_neighbors=6, n_components=2)\niso.fit(X)\nmanifold_data = iso.transform(X)\n", "intent": "**Excerise 2:** Apply ISOMAP on the data\n"}
{"snippet": "lle_data, _ = manifold.locally_linear_embedding(X, n_neighbors=12, n_components=2)\n", "intent": "**Excerise 3:** Apply LLE on the data\n"}
{"snippet": "lm = smf.ols(formula='sale_price ~ gross_sq_feet -1', data = REsample1).fit()\nprint(lm.summary())\n", "intent": "This is already pretty consistend with what python will give us if we take the entire training sample at once\n"}
{"snippet": "model.fit(X_train, y_train)\n", "intent": "We can use the `%%timeit` magic function to time how long a cell takes.\n"}
{"snippet": "model = KNeighborsClassifier(2)\nmodel.fit(X_r_train, y_r_train)\n", "intent": "Pick your favorite algorithm and time how long it takes to train\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nmodel_lr = LogisticRegression()\n", "intent": "$$ \\frac{1}{2m} \\sum_i (h(x_i) - y_i)^2 \\text{ mit } h(x) = m*x + t$$\n$$  $$\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.truncated_normal((vocab_size, embed_dim),\n                                               stddev=0.1))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "m1 = ols('PRICE ~ PTRATIO', bos).fit()\nprint(m1.summary())\n", "intent": "The scatterplot between price and ptratio shows no clear relationship\n"}
{"snippet": "lasso1 = Lasso(alpha=0.01, random_state=17)\nlasso1.fit(X_train_scaled, y_train)\n", "intent": "**Train a LASSO model with $\\alpha = 0.01$ (weak regularization) and scaled data. Again, set random_state=17.**\n"}
{"snippet": "cv_lin.fit(X_train_scaled, y_train.values.ravel())\n", "intent": "Gaussian SVC yields 29% accuracy, which is poor, I will try linear SVC to see if that improves prediction accuracy\n"}
{"snippet": "knn_params = {'n_neighbors': (range(1,100)), 'weights': ['uniform','distance'], 'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'] \n}\ngrid = GridSearchCV(neighbors.KNeighborsClassifier(), knn_params)\ngrid.fit(X_norm,y)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "ada = AdaBoostRegressor() \nparam_grid2 = { \n    'n_estimators': [5,50,100,200,300],\n    'learning_rate': [0.5, 1.0, 1.5]\n}\nCV_ada = GridSearchCV(estimator=ada, param_grid=param_grid2, cv= 5)\nCV_ada.fit(X_train2, y_train2)\nprint CV_ada.best_params_\n", "intent": "(base_estimator=None, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n"}
{"snippet": "knn = NearestNeighbors(n_neighbors=3)\nknn.fit(trainNorm[['zIncome', 'zLot_Size']])\ndistances, indices = knn.kneighbors(newHouseholdNorm)\nprint(trainNorm.iloc[indices[0], :])  \n", "intent": "Use k-nearest neighbour\n"}
{"snippet": "regressor.fit(X_train, y_train)\n", "intent": "`fit` learns the model parameters.\n"}
{"snippet": "from keras.layers import Dropout, Flatten, Dense\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(n_classes))\nmodel.add(Activation('softmax'))\n", "intent": "Then we can add dropout and our dense and output (softmax) layers.\n"}
{"snippet": "model = Sequential()\nmodel.add(LSTM(512, return_sequences=True, input_shape=(max_len, len(chars))))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(512, return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(len(chars)))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n", "intent": "Now we'll define our RNN. Keras makes this trivial:\n"}
{"snippet": "epochs = 10\nmodel.fit(X, y, batch_size=128, nb_epoch=epochs)\n", "intent": "Now that we have our training data, we can start training. Keras also makes this easy:\n"}
{"snippet": "model = build_model(one_hot=False, bidirectional=True)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit_generator(generator=generate_batches(batch_size, one_hot=False), samples_per_epoch=n_examples, nb_epoch=n_epochs, verbose=1)\n", "intent": "And we can try the bidirectional variations, e.g.\n"}
{"snippet": "alphas = np.logspace(-6, 2, 200)\nlasso_cv = LassoCV(random_state=17, cv=5, alphas=alphas)\nlasso_cv.fit(X_train_scaled, y_train)\n", "intent": "**Train LassoCV with random_state=17 to choose the best value of $\\alpha$ in 5-fold cross-validation.**\n"}
{"snippet": "hidden_linout = linear(X, hidden_layer_weights, hidden_layer_biases)\nhidden_output = activation(hidden_linout)\nprint('hidden output')\nprint(hidden_output)\n", "intent": "Now we can do a forward pass with our inputs $X$ to see what the predicted outputs are.\nFirst, we'll pass the input through the hidden layer:\n"}
{"snippet": "from scipy.spatial.distance import cdist\nignore_n_most_common = 50\ndef get_closest(word):\n    embedding = get_embedding(word)\n    distances = cdist(embedding, embeddings)[0]\n    distances = list(enumerate(distances))\n    distances = sorted(distances, key=lambda d: d[1])\n    for idx, dist in distances[1:]:\n        if idx > ignore_n_most_common:\n            return reverse_word_index[idx]\n", "intent": "Then we can define a function to get a most similar word for an input word:\n"}
{"snippet": "X = theano.shared(value=np.asarray([[0, 1], [1, 0], [0, 0], [1, 1]]), name='X')\ny = theano.shared(value=np.asarray([[0], [0], [1], [1]]), name='y')\nprint('X: {}\\ny: {}'.format(X.get_value(), y.get_value()))\n", "intent": "Initialize our input data `X` and output data `y`\n"}
{"snippet": "def layer(*shape):\n    assert len(shape) == 2\n    mag = 4. * np.sqrt(6. / sum(shape))\n    W_value = np.asarray(rng.uniform(low=-mag, high=mag, size=shape), dtype=theano.config.floatX)\n    b_value = np.asarray(np.zeros(shape[1], dtype=theano.config.floatX), dtype=theano.config.floatX)\n    W = theano.shared(value=W_value, name='W_{}'.format(shape), borrow=True, strict=False)\n    b = theano.shared(value=b_value, name='b_{}'.format(shape), borrow=True, strict=False)\n    return W, b\n", "intent": "A helper method for generating the matrices (as Theano shared variables) for a single layer can be written as follows.\n"}
{"snippet": "input_layer = Input(shape=(2,))\nhidden_layer = Dense(5, activation='relu')(input_layer)\noutput_layer = Dense(1, activation='sigmoid')(hidden_layer)\n", "intent": "Next, let's define our model, the same way we defined it in the Theano example.\n"}
{"snippet": "def get_weights(n_in, n_out):\n    mag = 4. * np.sqrt(6. / (n_in + n_out))\n    W_value = np.asarray(rng.uniform(low=-mag, high=mag, size=(n_in, n_out)), dtype=dtype)\n    W = theano.shared(value=W_value, name='W_%d_%d' % (n_in, n_out), borrow=True, strict=False)\n    return W\ndef get_bias(n_out):\n    b_value = np.asarray(np.zeros((n_out,), dtype=dtype), dtype=theano.config.floatX)\n    b = theano.shared(value=b_value, name='b_%d' % n_out, borrow=True, strict=False)\n    return b\n", "intent": "Next, let's make some helper methods for generating weights as shared variables, like we did with the XOR example.\n"}
{"snippet": "print(X_train[0].shape)\nmodel = Sequential()\nmodel.add(Conv2D(filters=18, kernel_size=(3,3), strides=1, padding='same', activation='relu', input_shape=X_train[0].shape))\nmodel.add(MaxPool2D(pool_size=3, padding='valid'))\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(train_labels.shape[1], activation='softmax'))\nprint(model.summary())  \n", "intent": "Design your network here using Keras\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(64, input_shape=X_train[0].shape, activation='relu'))   \nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(train_labels[0].shape[0], activation='softmax'))   \nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\nmodel.summary()\n", "intent": "Design your network here using Keras\n"}
{"snippet": "for layer in seq_model.layers:\n    layer.trainable = False\nseq_model.summary()\nseq_model.add( Flatten(name=\"my_flatten\") )\nseq_model.add( Dense(8, activation=\"relu\", name=\"predictions\") )\nseq_model.summary()\n", "intent": "Freeze the layers that remain in our model.\n"}
{"snippet": "forest = RandomForestRegressor(random_state=17)\nforest.fit(X_train_scaled, y_train)\n", "intent": "**Train a Random Forest with out-of-the-box parameters, setting only random_state to be 17.**\n"}
{"snippet": "feature_columns = learn.infer_real_valued_columns_from_input(data)\nclassifier = learn.LinearClassifier(feature_columns=feature_columns, n_classes=10)\nclassifier.fit(data, labels, batch_size=100, steps=1000)\n", "intent": "Our goal here is to get about 90% accuracy with this simple classifier.\n"}
{"snippet": "regr = LogisticRegression()\nX_test = test_dataset.reshape(test_dataset.shape[0], 28 * 28)\ny_test = test_labels\n", "intent": "I have already used scikit-learn in a previous MOOC. It is a great tool, very easy to use!\n"}
{"snippet": "regr2 = LogisticRegression(solver='sag')\nsample_size = len(train_dataset)\nX_train = train_dataset[:sample_size].reshape(sample_size, 784)\ny_train = train_labels[:sample_size]\nregr2.score(X_test, y_test)\n", "intent": "To train the model on all the data, we have to use another solver. SAG is the faster one.\n"}
{"snippet": "clf = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=1000)\n", "intent": "Now, we can initialize the RandomForestClassifier and fit a model to our training data. Use `n_estimators=100` to use 100 different trees.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train,y_train)\n", "intent": "Now its time to train our model!\n**Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.**\n"}
{"snippet": "X = college_data.drop(['Unnamed: 0', 'Private'],axis=1)\nkm.fit(X)\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "k = 1\nknn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "logreg = linear_model.LogisticRegression()\nlogreg.fit(X_train, Y_train)\n", "intent": "We will train the classifier the exact same way we did previously with the example.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel\n", "intent": "First, we create the model using the Sklearn LinearRegression model.\n"}
{"snippet": "forest_params = {'max_depth': list(range(10, 25)), \n                  'max_features': list(range(6,12))}\nlocally_best_forest = GridSearchCV(RandomForestRegressor(n_jobs=-1, random_state=17), \n                                 forest_params, \n                                 scoring='neg_mean_squared_error',  \n                                 n_jobs=-1, cv=5,\n                                  verbose=True)\nlocally_best_forest.fit(X_train_scaled, y_train)\n", "intent": "**Tune the `max_features` and `max_depth` hyperparameters with GridSearchCV and again check mean cross-validation MSE and MSE on holdout set.**\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X, y)\nscore = model.score(X, y)\nprint(f\"R2 Score: {score}\")\n", "intent": "Luckily, we can just supply our n-dimensional features and sklearn will fit the model using all of our features.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier.verbose=2\nclassifier\n", "intent": "Create a Logistic Regression Model\n"}
{"snippet": "voicemodel = LogisticRegression()\nvoicemodel\n", "intent": "Create a Logistic Regression Model\n"}
{"snippet": "voicemodel.fit(X_train,y_train)\n", "intent": "Fit (train) or model using the training data\n"}
{"snippet": "ctb.fit(X_train_part, y_train_part,\n        cat_features=categ_feat_idx)\n", "intent": "**Train Catboost without setting hyperparameters, passing only the indexes of categorical features.**\n"}
{"snippet": "input_X = T.tensor4()\ntarget_y = T.matrix(dtype='int64')\nweights = T.vector(\"weights\", dtype='float32') \nlearning_rate = T.scalar(name='learning_rate') \n", "intent": "https://github.com/Lasagne/Recipes/blob/master/modelzoo/Unet.py\n"}
{"snippet": "z_sample = T.matrix()\ninput_sample = InputLayer(input_shape, input_var=z_sample)\nl_dec_sample = DenseLayer(input_sample, HU_decoder, W=l_dec.W, b=l_dec.b)\nl_out_sample = DenseLayer(l_dec_sample, image_h * image_w * 3, W=l_out.W, b=l_out.b, nonlinearity=sigmoid)\ngenerated_x = lasagne.layers.get_output(l_out_sample)\ngen_fn = theano.function([z_sample], generated_x)\n", "intent": "This task requires deeper Lasagne knowledge. You need to perform inference from $z$, reconstruct an image given some random $z$ representations.\n"}
{"snippet": "mu_x = lasagne.layers.get_output(l_dec_mu)\nsigma_x = lasagne.layers.get_output(l_dec_logsigma)\nmu_z, sigma_z = lasagne.layers.get_output(l_enc_mu), lasagne.layers.get_output(l_enc_logsigma)\nloss_kl = - KL(mu_z, sigma_z)\nloss_ll = - log_likelihood(input_X, mu_x, sigma_x)\nloss = loss_kl + loss_ll\nparams = lasagne.layers.get_all_params(l_dec_mu, trainable=True) + [l_dec_logsigma.W, l_dec_logsigma.b]\nupdates = lasagne.updates.adam(loss, params)\ntrain_fn = theano.function([input_X], [loss_kl, loss_ll], updates=updates, allow_input_downcast=True)\ntest_fn = theano.function([input_X], lasagne.layers.get_output(l_dec_mu, deterministic=True), allow_input_downcast=True)\n", "intent": "Now build the loss and training function:\n"}
{"snippet": "import theano\nimport theano.tensor as T\nstates = T.matrix(\"states[batch,units]\")\nactions = T.ivector(\"action_ids[batch]\")\ncumulative_rewards = T.vector(\"R[batch] = r + gamma*r' + gamma^2*r'' + ...\")\n", "intent": "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n"}
{"snippet": "get_policy = theano.function([observations],probs,allow_input_downcast=True)\ndef act(obs, sample=True):\n    policy = get_policy([obs])[0]\n    if sample:\n        action = int(np.random.choice(n_actions,p=policy))\n    else:\n        action = int(np.argmax(policy))\n    return action, policy\n", "intent": "In this section, we'll define functions that take actions $ a \\sim \\pi_\\theta(a|s) $ and rollouts $ <s_0,a_0,s_1,a_1,s_2,a_2,...s_n,a_n> $.\n"}
{"snippet": "xgb_param_grid = {}\nxgb_model = train_model(\n    xgb.XGBClassifier(),\n    xgb_param_grid,\n    train_predictors, \n    train_outcome\n)\n", "intent": "Gradient boosting algorithms surprisingly did very poor job when there are not many features\n"}
{"snippet": "dt_param_grid = {}\ndt_model = train_model(\n    DecisionTreeClassifier(max_depth=9, min_samples_split=2, min_samples_leaf=3, criterion=\"entropy\"),\n    dt_param_grid,\n    train_predictors, \n    train_outcome\n)\n", "intent": "So far, this model gives us the best score: **1426**\n"}
{"snippet": "print('Checking the Training on a Single Batch...')\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(epochs):\n        batch_i = 1\n        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n", "intent": "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch.\n"}
{"snippet": "if 'my_vgg' in globals():\n    print('\"vgg\" object already exists.  Will not create again.')\nelse:\n    with tf.Session() as sess:\n        input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\n        vgg = vgg16.Vgg16()\n        vgg.build(input_)\n", "intent": "Below, feel free to choose images and see how the trained classifier predicts the flowers in them.\n"}
{"snippet": "ctb.fit(X_train, y_train,\n        cat_features=categ_feat_idx)\n", "intent": "**Train on the whole train set, make prediction on the test set. We got 0.73008 in the competition.**\n"}
{"snippet": "history = model2.fit([train_matrix,train_matrix_feature_index], \n          [train_labels],\n          nb_epoch=50,\n          verbose=2,\n          batch_size = 128,\n         shuffle = True,\n         validation_split=0.2)\n", "intent": "**Observation **\nSolution is converaging better. More epochs would result in better solution\n"}
{"snippet": "g = Graph()\n", "intent": "z = Ax + b\nA = 10\nb = 1\nz = 10x + 1\n"}
{"snippet": "kmean_clustering = KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "kmean_clustering.fit(df.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "KNN_model = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "KNN_model.fit(X_train,y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "sess = tf.Session()\nval_m2 = np.array([[2],[3]])\nres = sess.run(fetches=[product], feed_dict={m2: val_m2}) \nprint(res)\nsess.close()\n", "intent": "Provide the correct feeds (inputs) and fetches (outputs of the computational graph)\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(init_op)\n    res_val = sess.run(out, feed_dict={x:X[0:2]})\nres_val\n", "intent": "Since we fixed the random seed, you should you should get a result like:\n"}
{"snippet": "tf.reset_default_graph()\nX_ = tf.placeholder(name='X', shape=(None, 2), dtype='float32')\nW = tf.get_variable('W', shape=(2,1))\nX_rec_ = tf.matmul(tf.matmul(X_, W), tf.transpose(W))\nloss_ = tf.reduce_sum((X_rec_ - X_)**2)\n", "intent": "Calculate the PCA in tensorflow, by reducing the reconstruction error as shown above using TensorFlow.\n"}
{"snippet": "param_grid={'n_estimators': [500],\n            'learning_rate': np.logspace(-4, 0, 100),\n            'max_depth':  [10],\n            'num_leaves': [72],\n            'reg_lambda': [0.0010722672220103231],\n            'random_state': [17]}\ngs=RandomizedSearchCV(model_gb, param_grid, n_iter = 20, scoring='neg_mean_squared_error', fit_params=None, \n                n_jobs=-1, cv=kf, verbose=1, random_state=17)\ngs.fit(X_trees,y)\n", "intent": "Let's fix n_estimators=500, it is big enough but is not to computationally intensive yet, and find the best value of the learning_rate\n"}
{"snippet": "scaler.fit(bnote.drop('Class',axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "feature_columns = [contrib.layers.real_valued_column(\"\", dimension=1)]\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "lm = LinearRegression()\nlm\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "happyModel.fit(X_train, Y_train, epochs=10, batch_size=16)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model.fit([Xoh, s0, c0], outputs, epochs=5, batch_size=100)\n", "intent": "Let's now fit the model and run it for one epoch.\n"}
{"snippet": "print (\"Training started\")\nmodel.fit(x_train, y_train, epochs=30, batch_size=512, verbose=2)\nprint (\"Training completed\")\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "body_reg = linear_model.LinearRegression()\nbody_reg.fit(xVals, yVals)\n", "intent": "First we build a regression model using `sklearn`, and then we fit the model to our dependent variables, the brain weight.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, activation='relu', input_shape=(x_train.shape[1],)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss = 'mean_squared_error', optimizer='rmsprop', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "warnings.filterwarnings('ignore')\nmlog.fit(X_train,  y_train)\n", "intent": "Let's try to predict\n"}
{"snippet": "history = model.fit(Xsmall, ysmall, batch_size=500, epochs=40,verbose = 1)\nmodel.save_weights('model_weights/best_RNN_small_textdata_weights.hdf5')\n", "intent": "Now lets fit our model!\n"}
{"snippet": "model2 = models.Sequential()\nmodel2.add(layers.Dense(10, activation=\"relu\", input_dim=X.shape[1]))\nmodel2.add(layers.Dense(3, activation=\"softmax\"))\nmodel2.compile(optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"])\nmodel2.fit(X, y0, epochs=500, validation_split=0.2)\n", "intent": "Now let's fit the same model but this time encoding the data as integers\n"}
{"snippet": "model_features = models.Sequential()\nmodel_features.add(layers.Dense(1, input_dim=2, activation=\"sigmoid\"))\nmodel_features.compile(optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"])\nmodel_features.fit(X_features, y, epochs=500, validation_split=0.2)\n", "intent": "Now let's fit a logistic regression.\n"}
{"snippet": "nn_tanh = models.Sequential()\nnn_tanh.add(layers.Dense(5, input_dim=2, activation=\"tanh\"))\nnn_tanh.add(layers.Dense(5, activation=\"tanh\"))\nnn_tanh.add(layers.Dense(1, activation=\"sigmoid\"))\n", "intent": "Alternatively we could use a tanh activation function.\n"}
{"snippet": "from keras import layers\nfrom keras import models\nmodel = models.Sequential()\nmodel.add(layers.Dense(128, activation=\"relu\", input_dim=28 * 28))\nmodel.add(layers.Dense(32, activation=\"relu\"))\nmodel.add(layers.Dense(10, activation=\"softmax\"))\n", "intent": "We set up the same model as Chapter 6.5.\n"}
{"snippet": "model.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nhist_MNIST = model.fit(train_images, train_labels, epochs=20, batch_size=64,\n                       callbacks = callbacks_list,\n                       validation_data=(val_images, val_labels))\n", "intent": "Now we run the model using early stopping.\n"}
{"snippet": "ninputs = x_train.shape[1]\nlayer_sizes = [5, 3, 1] \nbiases = [tfe.Variable(np.random.normal(scale=0.05, size=n), dtype=tf.float32) for n in layer_sizes]\nweight_dims = zip([ninputs]+layer_sizes[0:-1], layer_sizes) \nweights = [tfe.Variable(np.random.normal(scale=0.05, size=s), dtype=tf.float32) for s in weight_dims]\nvariables = biases+weights\n", "intent": "Next we create the variables we are going to use. Their shapes depend on the architecture of our network so we also specify this.\n"}
{"snippet": "sess = tf.Session()\nsess.run(init)\n", "intent": "Next we start a Tensorflow session and initialise the variables.\n"}
{"snippet": "regr = LinearRegression()\nregr.fit(X_train, y_train)\n", "intent": "d) Train model on training data, and make predictions on testing data\n"}
{"snippet": "mlog2.fit(X_train,  y_train)\n", "intent": "Let's check this out\n"}
{"snippet": "from sklearn import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nclf =\n", "intent": "<h3>Train a Decision Tree Classifier</h3>\n"}
{"snippet": "from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(features, labels)\n", "intent": "<h3>Train a Decision Tree Classifier</h3>\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nclf.fit(X_train, Y_train)\n", "intent": "<h3>Train the Decision Tree Model</h3>\n"}
{"snippet": "A_days = students['A']['days']\nA_days = A_days[:, np.newaxis]\nA_morale = students['A']['morale']\nA_model = LinearRegression()\nA_model.fit(A_days, A_morale)\nprint A_model.intercept_, A_model.coef_\n", "intent": "---\nI decide to measure the morale with a linear regression, predicting it from the number of days. \nMy model is:\n"}
{"snippet": "mean_knn_n2 = KNeighborsClassifier(\n    n_neighbors=1,       \n    weights='uniform'    \n)\naccuracy_crossvalidator(X, y, mean_knn_n2, cv_indices)\n", "intent": "---\nAs you can see the mean cross-validated accuracy is very high with 5 neighbors. \nLet's see what it's like when we use only 1 neighbor:\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nX = affair[['age','religious']]\ny = affair.nbaffairs.values\nknn_uniform_n3 = KNeighborsClassifier(n_neighbors=50, weights='uniform')\nknn_uniform_n3.fit(X, y)\n", "intent": "---\nYou should choose **2 predictor variables** to predict had affair vs. not\n"}
{"snippet": "pipe.fit(X_test, y_test)\n", "intent": "Fit the pipeline with the training data, then score it on the testing data:\n"}
{"snippet": "def cluster_plotter(eps=1.0, min_samples=5):\n    dbkk = DBS(eps=eps, min_samples=min_samples)\n    dbkk.fit(X)\n    plot_clusters(X, dbkk.cluster_labels)\n", "intent": "---\nDon't pass `X` in to the function. We will just use the \"global\" X defined in the jupyter notebook earlier.\n"}
{"snippet": "graph = PredictedValuesGraph( table51 )\n", "intent": "N.B. Fractals gone from the top 50\n"}
{"snippet": "rf = RandomForestRegressor(n_estimators=300, random_state=42).fit(X_ohe_train_rf, to_log(y_train_2))\n", "intent": "Now, let's try RandomForest with default parameters on the same features (but without polynomial features):\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embedding,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_normal([vocab_size, embed_dim], stddev=0.1), name='embeddings')\n    embed = tf.nn.embedding_lookup(embeddings, input_data, name='embed')\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier().fit(train_data[['Sex', 'Age', 'Fare']], train_data.Survived)\ntree.score(test_data[['Sex', 'Age', 'Fare']], test_data.Survived)\n", "intent": "We'll use a simple Decision Tree model to predict if a passenger would survive the Titanic by making use of its gender, age and fare.\n"}
{"snippet": "lasso_reg = Lasso(alpha=1.0)\nlasso_reg.fit(X, y)\n", "intent": "$$\\frac{1}{2n} ||y - Xw||^2_2 + \\alpha ||w||_1$$\n"}
{"snippet": "LR = linear_model.LogisticRegression()\n", "intent": "Things tried but not used\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "> Note: this is only compatible with `contrib` feature_columns\n"}
{"snippet": "with tf.Session() as sess:\n    print(tf.nn.embedding_lookup(word_vector, first_sentence).eval().shape)\n", "intent": "The 10 x 50 output should contain the 50 dimensional word vectors for each of the 10 words in the sequence. \n"}
{"snippet": "for data in rand_loader:\n    input = data.to(device)\n    output = model(input)\n    print(\"Outside: input size\", input.size(),\n          \"output_size\", output.size())\n", "intent": "Run the Model\n-------------\nNow we can see the sizes of input and output tensors.\n"}
{"snippet": "nnr = MLPRegressor(max_iter = 1000)\nparameters = {'hidden_layer_sizes':[5, 100, [5, 10], [100, 50]], \n              'alpha':[1e-1, 1e-3, 1e-5], \n            'batch_size':[50, 500, 2000]}\ngcnn = GridSearchCV(nnr, parameters, cv=4)\ngcnn.fit(...)\nresults = gcnn.cv_results_\n...\n...\n", "intent": "Do a grid search on different architecture of layers (number and sizes), batch sizes, and regularizing strength and find the top 10 models.\n"}
{"snippet": "rf_best = RandomForestRegressor(n_estimators=300, max_depth=best_max_depth, random_state=42)\n", "intent": "Finnaly, let's try to fit RandomForestRegressor with best **max_depth** value, found due to cross-validation. We'll use the same datasets.\n"}
{"snippet": "mlpc = MLPClassifier(max_iter = 1000)\nparameters = {'hidden_layer_sizes':[5, 100, [5, 10], [100, 50]], \n              'alpha':[1e-1, 1e-3, 1e-5], \n            'batch_size':[50, 500, 2000]}\ngcnnc = GridSearchCV(mlpc, parameters, cv=4)\ngcnnc.fit(...)\nresults = gcnnc.cv_results_\n...\n...\n", "intent": "Do a grid search on hidden layers, batch and regularization to get the best model\n"}
{"snippet": "model.add(Dense(num_classes, activation='softmax'))\n", "intent": "Lastly, we need to add a soft-max layer since we have a multi-class output.\n"}
{"snippet": "nnr = MLPRegressor(max_iter = 1000)\nparameters = {'hidden_layer_sizes':[5, 100, [5, 10], [100, 50]], \n              'alpha':[1e-1, 1e-3, 1e-5], \n            'batch_size':[50, 500, 2000]}\ngcnn = GridSearchCV(nnr, parameters, cv=4)\ngcnn.fit(*trainn)\nresults = gcnn.cv_results_\n", "intent": "Do a grid search on different architecture of layers (number and sizes), batch sizes, and regularizing strength and find the top 10 models.\n"}
{"snippet": "mlpc = MLPClassifier([30, 10], max_iter = 1000)\nparameters = {'activation':['relu', 'logistic', 'identity']}\ngcnnc = GridSearchCV(mlpc, parameters, cv=4)\ngcnnc.fit(*traincn)\nresults = gcnnc.cv_results_\nprint(results['mean_test_score'])\n", "intent": "*First, Lets confirm which activation function works the best for classification. Is the difference as significant as for the regression problem?*\n"}
{"snippet": "mlpc = MLPClassifier(max_iter = 1000)\nparameters = {'hidden_layer_sizes':[5, 100, [5, 10], [100, 50]], \n              'alpha':[1e-1, 1e-3, 1e-5], \n            'batch_size':[50, 500, 2000]}\ngcnnc = GridSearchCV(mlpc, parameters, cv=4)\ngcnnc.fit(*traincn)\nresults = gcnnc.cv_results_\n", "intent": "Do a grid search on hidden layers, batch and regularization to get the best model\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding=tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)    \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "train_model(\n    learning_rate=0.0002,\n    steps=100,\n    batch_size=32, input_feature = 'population'\n)\n", "intent": "See if you can do any better by replacing the `total_rooms` feature with the `population` feature.\nDon't take more than 5 minutes on this portion.\n"}
{"snippet": "train_model(\n    learning_rate=0.00002,\n    steps=1000,\n    batch_size=5,\n    input_feature=\"population\"\n)\n", "intent": "Click below for one possible solution.\n"}
{"snippet": "california_housing_dataframe[\"rooms_per_person\"] = (\n    california_housing_dataframe[\"total_rooms\"] / california_housing_dataframe[\"population\"])\ncalibration_data = train_model(\n    learning_rate=0.05,\n    steps=500,\n    batch_size=5,\n    input_feature=\"rooms_per_person\")\n", "intent": "Click below for a solution.\n"}
{"snippet": "grid.fit(X_train.iloc[:500000,:], y_train.iloc[:500000])\n", "intent": "There we take only 500 000 teams out of 1 500 000. As we will see further (on learning curve), it's enough to find best params.\n"}
{"snippet": "from sklearn.cluster import KMeans\nimport numpy as np\nx_cols = df_table.values\nx_list = list(range(2,11))\nSS_list = [KMeans(n_clusters=k).fit(x_cols).inertia_ for k in x_list]\n", "intent": "Lower values of $SS$ ought to represent better clusterings as we don't want points that are excessively far from their respective cluster centroids.\n"}
{"snippet": "lm = LinearRegression(fit_intercept = False)\nlm.fit(X, bos.PRICE)\n", "intent": "**Your turn:** How would you change the model to not fit an intercept term? Would you recommend not having an intercept?\n"}
{"snippet": "h = activation(torch.mm(features, W1) + B1)\noutput = activation(torch.mm(h, W2) + B2)\nprint(output)\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "import helper\nimages, labels = next(iter(trainloader))\nimg = images[0].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, input_dim=1000))\nmodel.add(Activation('softmax'))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('sigmoid'))\nmodel.summary()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, nb_epoch=1000, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(490, activation='relu', input_dim=1000))\nmodel.add(Dense(num_classes, activation='sigmoid'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(2, input_dim=x_train.shape[1]))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "start = time() \nmodel.fit(x_train, y_train, epochs=5, batch_size=10, verbose=2)\nprint(time() - start)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "cat_boost = CatBoostClassifier(random_seed=17, iterations=10)\ncat_boost.fit(X_train, y_train, verbose=False, plot=True);\n", "intent": "Try a CatBoostClassifier.\n"}
{"snippet": "model_breast = KNeighborsClassifier(n_neighbors=6)\nmodel_breast.fit(X_train, y_train)\n", "intent": "Create a KNN classifier with six neighbors and train it with the appropriate data.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n", "intent": "Then we can create a linear model and train it on the right dataset.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\n", "intent": "Now let's use a decision tree on a real dataset.\n"}
{"snippet": "svc_minmax = SVC(C=100, gamma=\"auto\")\nsvc_minmax.fit(X_train_scaled, y_train)\nprint(\"Accuracy =\", svc_minmax.score(X_test_scaled, y_test))\n", "intent": "Train again a SVC model, but this time train it on the scaled data. Do you see any improvements ?\n"}
{"snippet": "def feed_forward(theta, X):\n    t1, t2 = deserialize(theta)  \n    m = X.shape[0]\n    a1 = X  \n    z2 = a1 @ t1.T  \n    a2 = np.insert(sigmoid(z2), 0, np.ones(m), axis=1)  \n    z3 = a2 @ t2.T  \n    h = sigmoid(z3)  \n    return a1, z2, a2, z3, h  \n", "intent": "> (400 + 1) -> (25 + 1) -> (10)\n<img style=\"float: left;\" src=\"../img/nn_model.png\">\n"}
{"snippet": "def forward_propagate(X, theta1, theta2):\n    m = X.shape[0]\n    a1 = np.insert(X, 0, values=np.ones(m), axis=1)\n    z2 = a1 * theta1.T\n    a2 = np.insert(sigmoid(z2), 0, values=np.ones(m), axis=1)\n    z3 = a2 * theta2.T\n    h = sigmoid(z3)\n    return a1, z2, a2, z3, h\n", "intent": "> (400 + 1) -> (25 + 1) -> (10)\n<img style=\"float: left;\" src=\"../img/nn_model.png\">\n"}
{"snippet": "svc1 = sklearn.svm.LinearSVC(C=1, loss='hinge')\nsvc1.fit(data[['X1', 'X2']], data['y'])\nsvc1.score(data[['X1', 'X2']], data['y'])\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n"}
{"snippet": "svc100 = sklearn.svm.LinearSVC(C=100, loss='hinge')\nsvc100.fit(data[['X1', 'X2']], data['y'])\nsvc100.score(data[['X1', 'X2']], data['y'])\n", "intent": "with large C, you try to overfit the data, so the left hand side edge case now is categorized right\n"}
{"snippet": "parameters = {'C': candidate, 'gamma': candidate}\nsvc = svm.SVC()\nclf = GridSearchCV(svc, parameters, n_jobs=-1)\nclf.fit(training[['X1', 'X2']], training['y'])\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html\n"}
{"snippet": "n_estimators = np.arange(250, 400, 50)\nmax_depth = np.arange(15, 26, 3)\nmax_features = np.arange(5, 21, 5)\nparams = {'n_estimators': n_estimators,\n          'max_depth': max_depth,\n          'max_features': max_features}\ncv_rf = GridSearchCV(rf, param_grid=params, cv=skf, scoring='roc_auc', n_jobs=-1)\ncv_rf.fit(X_train_RF, y_train)\n", "intent": "Now we can find more precision parameters.\n"}
{"snippet": "with tf.Session(graph=detection_graph) as sess:\n    image_tensor = sess.graph.get_tensor_by_name('image_tensor:0')\n    detection_boxes = sess.graph.get_tensor_by_name('detection_boxes:0')\n    detection_scores = sess.graph.get_tensor_by_name('detection_scores:0')\n    detection_classes = sess.graph.get_tensor_by_name('detection_classes:0')\n    new_clip = clip.fl_image(pipeline)\n    new_clip.write_videofile('result.mp4')\n", "intent": "**[Sample solution](./exercise-solutions/e5.py)**\n"}
{"snippet": "MEDV_to_RM = smf.ols('MEDV ~ RM', data=boston_housing_df)\nres = MEDV_to_RM.fit()\nres.summary()\n", "intent": "Perform a regression on your first feature.\n"}
{"snippet": "MEDV_to_LSTAT = smf.ols('MEDV ~ LSTAT', data=boston_housing_df)\nres = MEDV_to_LSTAT.fit()\nres.summary()\n", "intent": "Plot the predicted values versus the true values.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=17, weights='distance')\n", "intent": "---\nIt's up to you what predictors you want to use and how you want to parameterize the model.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(penalty='l2', C=1)\n", "intent": "- What is the mean accuracy?\n- What is the standard deviation of accuracy?\n"}
{"snippet": "w_opt = minimize(ridge(1e-5), w)\nlen(w_opt.x[w_opt.x==0])\n", "intent": "1e-05 0.100575155982\n"}
{"snippet": "clf2 = svm.SVC(kernel=\"poly\", degree=3, probability=True)\nclf2.fit(X, Y)\n", "intent": "The shape of the dividing lines between classes depend on the kernel used.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.01))\n    embedding = tf.nn.embedding_lookup(embedding, input_data)\n    return embedding\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "results = smf.logit('Yes ~ balance + student', data=default).fit()\n", "intent": "more than 1 predictor\n"}
{"snippet": "lr=LogisticRegression(C=100, random_state=2018, penalty='l1')\nlr.fit(X_train_B, y_train)\n", "intent": "Let's make a prediction on the hold-out set for Logistic Regression model.\n"}
{"snippet": "mod1 = smf.ols('wage ~ age', data=wage).fit()\nmod2 = smf.ols('wage ~ age + np.power(age, 2)', data=wage).fit()\nmod3 = smf.ols('wage ~ age + np.power(age, 2) + np.power(age, 3)', data=wage).fit()\nmod4 = smf.ols('wage ~ age + np.power(age, 2) + np.power(age, 3) + np.power(age, 4)', data=wage).fit()\nmod5 = smf.ols('wage ~ age + np.power(age, 2) + np.power(age, 3) + np.power(age, 4) + np.power(age, 5)', data=wage).fit()\n", "intent": "Models must be nested here, meaning that mod2 must be a superset of mod1\n"}
{"snippet": "from distutils.version import LooseVersion as Version\nfrom sklearn import __version__ as sklearn_version\nif Version(sklearn_version) < '0.18':\n    clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\nelse:\n    clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\ndoc_stream = stream_docs(path='movie_data.csv')\n", "intent": "**Note**\n- You can replace `Perceptron(n_iter, ...)` by `Perceptron(max_iter, ...)` in scikit-learn >= 0.19.\n"}
{"snippet": "classifier = LogisticRegression()\n", "intent": "Next, we instantiate the estimator object.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n", "intent": "Next, we use the learning algorithm implemented in `LinearRegression` to **fit a regression model to the training data**:\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf\n", "intent": "We can now train a classifier, for instance a logistic regression classifier, which is a fast baseline for text classification tasks:\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinear_regression = LinearRegression().fit(X_train, y_train)\nprint(\"R^2 on training set: %f\" % linear_regression.score(X_train, y_train))\nprint(\"R^2 on test set: %f\" % linear_regression.score(X_test, y_test))\n", "intent": "$$ \\text{min}_{w, b} \\sum_i || w^\\mathsf{T}x_i + b  - y_i||^2 $$\n"}
{"snippet": "from sklearn.ensemble import IsolationForest\niforest = IsolationForest(contamination=0.05)\niforest = iforest.fit(X_5)\n", "intent": "1. Let's use IsolationForest to find the top 5% most abnormal images.\n2. Let's plot them !\n"}
{"snippet": "logit.fit(X_train, y_train)\n", "intent": "Train the same logistic regression.\n"}
{"snippet": "lr = LogisticRegression(random_state=5, class_weight='balanced')\n", "intent": "Now, we will create a LogisticRegression model and use class_weight='balanced' to make up for our unbalanced classes.\n"}
{"snippet": "rf = RandomForestClassifier(random_state=2018, max_depth=8, max_features=8, n_estimators=200)\nrf.fit(X_train_B, y_train)\n", "intent": "The result on cross-calidation is capmarable and is about *0.848*.  \nLet's make a prediction on the hold-out set for Fandom Forest model.\n"}
{"snippet": "rf_grid_search = GridSearchCV(rf, parameters, n_jobs=-1, scoring='roc_auc', cv=skf, verbose=True)\nrf_grid_search = rf_grid_search.fit(X, y)\nprint(rf_grid_search.best_score_ - grid_search.best_score_)\n", "intent": "**Answer:** 1.\n**Solution:**\n"}
{"snippet": "bg = BaggingClassifier(LogisticRegression(class_weight='balanced'),\n                       n_estimators=100, n_jobs=-1, random_state=42)\nr_grid_search = RandomizedSearchCV(bg, parameters, n_jobs=-1, \n                                   scoring='roc_auc', cv=skf, n_iter=20, random_state=1,\n                                   verbose=True)\nr_grid_search = r_grid_search.fit(X, y)\n", "intent": "**Answer:** 1.\n**Solution:**\n"}
{"snippet": "sgd_reg = SGDRegressor()\nsgd_reg.fit(X_train_scaled, y_train)\n", "intent": "Train created `SGDRegressor` with `(X_train_scaled, y_train)` data. Leave default parameter values for now.\n"}
{"snippet": "rf = RandomForestClassifier()\nrf.fit(X_train,y_train)\n", "intent": "- http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size+1, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    print(\"vocab_size:\", vocab_size)\n    print(\"embed.shape:\", embed.shape)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "X_full = np.concatenate((X_train_scaled_dimred, X_test_scaled_dimred), axis=0)\nY_full = np.concatenate((Y_train, Y_test), axis=0)\nsvc_full = svm.SVC(C=100)\nsvc_full.fit(X_full, Y_full)\n", "intent": "The model seems pretty good. So, lets train on the entire training set to improve accuracy.\n"}
{"snippet": "clf = RandomForestClassifier(criterion='gini', n_estimators=50, max_depth=10)\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n", "intent": "<p> a. Train a random forest classifier on your training set using the split function assigned to your group.</p>\n"}
{"snippet": "timesteps = 28 \nnum_hidden = 128 \ndef RNN(x, weights, biases):\n    x = tf.unstack(x, timesteps, 1)\n    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n    return tf.matmul(outputs[-1], weights) + biases\nfor time_step in range(timesteps):\n    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n", "intent": "A) LSTM \nB) GRU\nC) Multi Layer\nD) Multi Layer with Dropout\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "<b>Step2:</b> Instantiate the estimator\n- Estimator is scikit-learn's term for model\n- Instantiate means, make an instance of\n"}
{"snippet": "weight={}\nfor i in [.1,.3,.5,.7,.9,1.1,1.3,1.5,1.7,1.9,2.1]:\n  class_weight={0:1,1:1.3+i}\n  params = {'n_estimators':range(140,190,10),'max_depth':[1,2],'min_samples_split':[0.4]} \n  rfc=RandomForestClassifier(random_state=42,class_weight=class_weight)\n  rfc_grid = GridSearchCV(rfc,param_grid=params,cv=2)\n  rfc_grid.fit(X_train,y_train) \n  rfc=rfc_grid.best_estimator_ \n  weight[1.3+i]=rfc_grid.best_score_ \n", "intent": "Since we have an imbalanced dataset with respect to target variable, we need to assign weights to the class with lower number of records.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X, y)\n", "intent": "<b>Using a different value for K</b>\n"}
{"snippet": "logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n", "intent": "<b>1. Logistic Regression</b>\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinear = LinearRegression()\nlinear.fit(X_train, y_train)\n", "intent": "<b>1. Linear Regression</b>\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=13, weights='uniform')\nknn.fit(X, y)\n", "intent": "<b>Using the best parameters to make predictions</b>\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X, y)\n", "intent": "Implementing Linear Regression on the Data\n"}
{"snippet": "lm_for_impute = LinearRegression() \n", "intent": "Then, we create model for regression imputation.\n"}
{"snippet": "rfr = RandomForestRegressor(n_estimators=100)\nrfr.fit(X_binarized, Y)\nsorted([i for i in zip(X_binarized.columns, rfr.feature_importances_)], key=lambda x:x[1], reverse=True)\n", "intent": "We can query the model to ask what amino acids are most important. To do so, call the `model.feature_importances_` attribute.\n"}
{"snippet": "def fit_model(model, X, Y):\n    return \nrf_mdl, rf_preds = fit_model(RandomForestRegressor, X_binarized, Y)\n", "intent": "Here, I will show you how to output the predictions of the machine learning model to disk as a CSV file.\n**Live Coding Together**\n"}
{"snippet": "print(loss_func(model(xb), yb), accuracy(model(xb), yb))\n", "intent": "Note that we no longer call ``log_softmax`` in the ``model`` function. Let's\nconfirm that our loss and accuracy are the same as before:\n"}
{"snippet": "class_weight={0:1,1:2.0} \nparams = {'n_estimators':range(140,160,3),'max_depth':[1,2,3],'min_samples_split':[0.4,.7]}\nrfc=RandomForestClassifier(random_state=42,class_weight=class_weight)\nrfc_grid = GridSearchCV(rfc,param_grid=params,cv=5)\nrfc_grid.fit(X_train,y_train)\nrfc=rfc_grid.best_estimator_ \n", "intent": "We are getting maximum accuracy at class weight of 2 for class 1.\n"}
{"snippet": "model = Mnist_Logistic()\nprint(loss_func(model(xb), yb))\n", "intent": "We instantiate our model and calculate the loss in the same way as before:\n"}
{"snippet": "fit()\nprint(loss_func(model(xb), yb))\n", "intent": "We are still able to use our same ``fit`` method as before.\n"}
{"snippet": "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\nmodel, opt = get_model()\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)\n", "intent": "Now, our whole process of obtaining the data loaders and fitting the\nmodel can be run in 3 lines of code:\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\nmodel_ft = model_ft.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "intent": "Finetuning the convnet\n----------------------\nLoad a pretrained model and reset final fully connected layer.\n"}
{"snippet": "scaler.fit(data.drop('Class', axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "kmeans.fit(df.drop('Private', axis=1))   \n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "from sklearn.svm import SVC\nsvc_model = SVC()\nsvc_model.fit(X_train, y_train)\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "param_grid = {'C':[0.1,1,10,100,1000], 'gamma':[1,0.1,0.01,0.001,0.0001]}\ngrid = GridSearchCV(SVC(), param_grid, verbose=3)\ngrid.fit(X_train, y_train)\n", "intent": "**Create a dictionary called param_grid and fill out some parameters for C and gamma.**\n"}
{"snippet": "sess = tf.Session()\nfeed_dict = { a: 5, b: 3 }\noutput = sess.run(out, feed_dict=feed_dict)\nprint(output)\n", "intent": "* Start a `tf.Session` to launch the graph\n* Setup any necessary input values\n* Use `Session.run()` to compute values from the graph\n"}
{"snippet": "weight={} \nfor i in [.1,.3,.5,.7,.9,1.1,1.3,1.5]:\n  class_weight={0:1,1:.3+i}\n  params = {'C':[0.01,.1,5,1,10,15]} \n  log=LogisticRegression(class_weight=class_weight)\n  log_grid = GridSearchCV(log,param_grid=params,cv=5)\n  log_grid.fit(X_train,y_train)\n  log=log_grid.best_estimator_\n  weight[.3+i]=log_grid.best_score_ \n", "intent": "We apply the same method as we did before for logistic regression too.\n"}
{"snippet": "model = LinearRegression(fit_intercept=True)\nmodel\n", "intent": "```\nmodel = LinearRegression(fit_intercept=True)\nmodel```\n"}
{"snippet": "model.fit(X, y)\n", "intent": "```\nmodel.fit(X, y)```\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X, y);\n", "intent": "```\nfrom sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X, y);```\n"}
{"snippet": "model_vectorizer.fit(train.data)\nvectors = model_vectorizer.transform(train.data)\n", "intent": "```\nmodel_vectorizer.fit(train.data)\nvectors = model_vectorizer.transform(train.data)```\n"}
{"snippet": "model_classifier.fit(vectors, train.target)\n", "intent": "```\nmodel_classifier.fit(vectors, train.target)```\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier().fit(X, y)\n", "intent": "scikit-learn includes DecisionTreeClassifier that is a class capable of performing multi-class classification on a dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='train',\n                input_to_softmax=final_model(), \n                model_path='model_end.h5')\n", "intent": "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='validation',\n                input_to_softmax=final_model(), \n                model_path='model_end.h5')\n", "intent": "Use the next code cell to visualize the model's prediction for the first example in the validation dataset.\n"}
{"snippet": "from sklearn import svm, datasets\nfrom sklearn.preprocessing import scale\nC = 1.0\nsvc = svm.SVC(kernel='linear', C=C).fit(scale(X), scale(y))\n", "intent": "Now we'll use linear SVC to partition our graph into clusters:\n"}
{"snippet": "pr_lr=LogisticRegression(class_weight='balanced')\n", "intent": "Let's at first train a basic LogReg without tuning hyperparametes, creating new features to establish sort of baseline:\n"}
{"snippet": "knn = KNeighborsClassifier()\nparam_dict = {'n_neighbors':range(1, 31), 'weights':['uniform', 'distance']}\ngsknn = GridSearchCV(knn, param_dict, scoring='accuracy')\ngsknn.fit(X, y)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "ridge(sample, 0)\n", "intent": "Let us run this with slpha = 0, which is OLS\n"}
{"snippet": "kidiq_X = kidiq.as_matrix([\"mom_iq\"])\nkidiq_Y = kidiq.as_matrix([\"kid_score\"])\nlr = linear_model.LinearRegression()\nlr.fit(kidiq_X,kidiq_Y)\n", "intent": "Some Pandas/Python practice: we extract the numpy arrays from the dataframe, and use linear_model from scikit-learn to perform the same fit.\n"}
{"snippet": "kidiq_X = kidiq.as_matrix([\"mom_iq\",\"mom_hs\"])\nkidiq_Y = kidiq.as_matrix([\"kid_score\"])\nlr = linear_model.LinearRegression()\nlr.fit(kidiq_X,kidiq_Y)\nprint lr.coef_, lr.intercept_\n", "intent": "Again the R^2 value is not a final test. It is merely another interpretation of the goodness of fit, that can help provide some guidance.\n"}
{"snippet": "R = 0.005 \nC = 1/R  \nsvc = svm.SVC(kernel='linear', C=C).fit(X_tr, y_tr)\npoly_svc3 = svm.SVC(kernel='poly', degree=3, C=C).fit(X_tr, y_tr)\npoly_svc5 = svm.SVC(kernel='poly', degree=5, C=C).fit(X_tr, y_tr)\npoly_svc13 = svm.SVC(kernel='poly', degree=13, C=C).fit(X_tr, y_tr)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X_tr, y_tr)\nsig_svc = svm.SVC(kernel='sigmoid', gamma=0.7, C=C).fit(X_tr, y_tr)\n", "intent": "Next, we train SVMs with several different kernels and regularization parameters.\n"}
{"snippet": "trainAA=trainA.copy(deep=True)\ntrainAA.drop(\"class\", axis=1, inplace=True)\ngnb = GaussianNB()\ngnb.fit(trainAA, trainA[\"class\"])\ngnb.score(trainAA, trainA[\"class\"])\n", "intent": "Fit a Gaussian Naive Bayes model to the original dataset A (including the outliers). Display the classification accuracy on the training dataset.\n"}
{"snippet": "kmeans = KMeans(n_clusters=5, random_state=0)\nkmeans.fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "reg = Lasso(alpha=1.e-2, fit_intercept=False)\nreg.fit(X_train20, y_train20)\nreg.coef_\n", "intent": "[sklearn.linear_model.Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html\n"}
{"snippet": "scaler.fit(df.drop('Class', axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "xg=XGBClassifier()\nxg.fit(polfeats_train,y_train_logreg)\n", "intent": "I'll fit XGboost Classifier on those 820 features to see which features were the most important from its prespective.\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=6)\ngrid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(2000, input_dim=x_train.shape[1]))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(500))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(2))\nmodel.add(Activation('sigmoid'))\nmodel.summary()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"RMSprop\", metrics = [\"accuracy\"])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.svm import SVC\nfrom sklearn.grid_search import GridSearchCV\nsvc = SVC(kernel='rbf', class_weight='balanced')\nparam_grid = {'C': [1, 5, 10, 50],\n              'gamma': [0.0001, 0.0005, 0.001, 0.005]}\ngrid = GridSearchCV(svc, param_grid)\nprint('Best parameters:', grid.best_params_)\n", "intent": "Select appropriate hyperparameters C and gamma, and run SVM on the original training dataset.\n"}
{"snippet": "svc = SVC(kernel='rbf', class_weight='balanced')\nparam_grid = {'C': [1, 5, 10, 50],\n              'gamma': [0.0001, 0.0005, 0.001, 0.005]}\ngrid = GridSearchCV(svc, param_grid)\nprint('Best parameters:', grid.best_params_)\n", "intent": "Select appropriate hyperparameters C and gamma, and run SVM on the dataset after PCA.\n"}
{"snippet": "lr = LogisticRegression(penalty= 'l2', C = 1/best_lambda)\nerror_test, error_train = Kfold_cross_validation(5, x_train.T, y_train, lr)\nprint(\"training error (lambda = \" + str(int(best_lambda)) + \"): \" + str(round(error_train,4)))\nprint(\"CV error: (lambda = \" + str(int(best_lambda)) + \"): \" + str(round(error_test,4)))\n", "intent": "Print the training error and CV error for the fitted $Logistic \\  Regression$ with $\\lambda = 100$\n"}
{"snippet": "lr = LinearRegression()\nrf = RandomForestRegressor()\nab = AdaBoostRegressor()\ndt = DecisionTreeRegressor()\n", "intent": "> **Step 3:** Now that you have imported the 4 models that can be used for regression problems, instantate each below.\n"}
{"snippet": "lr.fit(X_train, y_train)\nrf.fit(X_train, y_train)\nab.fit(X_train, y_train)\ndt.fit(X_train, y_train)\n", "intent": "> **Step 4:** Fit each of your instantiated models on the training data.\n"}
{"snippet": "fit_info = model.fit(X_train, Y_train,\n                    epochs=5,\n                    batch_size=32,\n                   validation_data=(X_Valid,Y_Valid))\n", "intent": "2\\. Fit the model with the training set with 5 epochs and batch size 32.\n"}
{"snippet": "xgbclf.fit(X_train_tb_fin,y_train_tb_fin)\n", "intent": "**XGBoost Clasiffier**\n"}
{"snippet": "knn.fit(X_train, y_train)\n", "intent": "** Ajuste este modelo KNN aos dados de treinamento. **\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\ngrid.fit(X_train, y_train)\ngrid.best_params_\n", "intent": "** Crie um objeto GridSearchCV e ajuste-o aos dados de treinamento. **\n"}
{"snippet": "bag_clf = BaggingClassifier(base_estimator=clf, n_estimators=500,\n                           bootstrap=True, oob_score=True, n_jobs=-1,\n                           random_state=42)\n", "intent": "Use out of bag samples to estimate the generalization accuracy\n"}
{"snippet": "from sklearn.pipeline import make_pipeline\npoly_model = make_pipeline(PolynomialFeatures(7),\n                           LinearRegression())\n", "intent": "This new, higher-dimensional data representation can then be plugged into a linear regression.\nLet's make a 7th-degree polynomial model in this way:\n"}
{"snippet": "x_init = x.copy()\nx = x.reshape(-1, 1)\nclf = GaussianMixture(n_components=5, max_iter=500, random_state=3).fit(x)\n", "intent": "Gaussian mixture models will allow us to approximate this density:\n"}
{"snippet": "lr = LogisticRegression(C=1)\nlr.fit(features_train, target_train)\n", "intent": "Let's start with a simple model from sklearn, namely `LogisticRegression`:\n"}
{"snippet": "km = KMeans(n_clusters=3, random_state=1)\n", "intent": "Do KMeans with 3 clusters\n"}
{"snippet": "clf = tree.DecisionTreeClassifier(random_state=0)\n", "intent": "Create a new Decision Tree classifier using default parameters:\n"}
{"snippet": "clf.fit(df[iris.feature_names], df.species)\n", "intent": "Fit the classifier with the iris data:\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nc_values = np.linspace(10, 15, 5)\nlogit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_values},\n                                           scoring='f1', n_jobs=-1, cv=skf, verbose=1)\nlogit_grid_searcher.fit(X_train_full, train_df['target'])\nprint(logit_grid_searcher.best_score_, logit_grid_searcher.best_params_)\n", "intent": "Now, let's find optimal parametr C for logregression using GridSearch\n"}
{"snippet": "import tensorflow as tf\na = tf.constant([1.0,2.0], name = \"a\")\nb = tf.constant([1.0,2.0], name = \"b\")\nc = a + b\nwith tf.Session() as sess:\n    sess.run(c)\nprint(c)\n", "intent": "Please refere to my notes in the directory of JiaxuanLi-book2017\nA test code for your environment:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\n", "intent": "In order to improve the performances of our prediction is possible to add hidden layers between the input layer and the output layer.\n"}
{"snippet": "model2 = Model(inputs=inputs, outputs=outputs)\n", "intent": "Create a new instance of the Keras Functional Model. We give it the inputs and outputs of the Convolutional Neural Network that we constructed above.\n"}
{"snippet": "model3 = load_model(path_model)\n", "intent": "Loading the model is then just a single function-call, as it should be.\n"}
{"snippet": "new_model = Sequential()\nnew_model.add(conv_model)\nnew_model.add(Flatten())\nnew_model.add(Dense(1024, activation='relu'))\nnew_model.add(Dropout(0.5))\nnew_model.add(Dense(num_classes, activation='softmax'))\n", "intent": "We can then use Keras to build a new model on top of this.\n"}
{"snippet": "model = load_model(path_best_model)\n", "intent": "We can now use the best model on the test-set. It is very easy to reload the model using Keras.\n"}
{"snippet": "model = Sequential()\n", "intent": "We are now ready to create the Recurrent Neural Network (RNN). We will use the Keras API for this because of its simplicity. See Tutorial \n"}
{"snippet": "model.add(GRU(units=8, return_sequences=True))\n", "intent": "This adds the second GRU with 8 output units. This will be followed by another GRU so it must also return sequences.\n"}
{"snippet": "model.add(Dense(1, activation='sigmoid'))\n", "intent": "Add a fully-connected / dense layer which computes a value between 0.0 and 1.0 that will be used as the classification output.\n"}
{"snippet": "x_test = tfidf.transform(test_df['clean_text'])\nX_test_full = hstack([x_test, test_df[feats].values])\nlogit.fit(X_train_full, train_df['target'])\n", "intent": "It's time to make predict for test set!\n"}
{"snippet": "def connect_encoder():\n    net = encoder_input\n    net = encoder_embedding(net)\n    net = encoder_gru1(net)\n    net = encoder_gru2(net)\n    net = encoder_gru3(net)\n    encoder_output = net\n    return encoder_output\n", "intent": "This helper-function connects all the layers of the encoder.\n"}
{"snippet": "decoder_output = connect_decoder(initial_state=decoder_initial_state)\nmodel_decoder = Model(inputs=[decoder_input, decoder_initial_state],\n                      outputs=[decoder_output])\n", "intent": "Then we create a model for just the decoder alone. This allows us to directly input the initial state for the decoder's GRU units.\n"}
{"snippet": "def flatten(captions_listlist):\n    captions_list = [caption\n                     for captions_list in captions_listlist\n                     for caption in captions_list]\n    return captions_list\n", "intent": "This helper-function converts a list-of-list to a flattened list of captions.\n"}
{"snippet": "captions_train_flat = flatten(captions_train_marked)\n", "intent": "Now use the function to convert all the marked captions from the training set.\n"}
{"snippet": "decoder_embedding = Embedding(input_dim=num_words,\n                              output_dim=embedding_size,\n                              name='decoder_embedding')\n", "intent": "This is the embedding-layer which converts sequences of integer-tokens to sequences of vectors.\n"}
{"snippet": "A = tf.placeholder(tf.float32, shape=(None, 3))\nB = A + 5\nwith tf.Session() as sess:\n    B_val_1 = B.eval(\n        feed_dict={A: [[1, 2, 3]]})\n    B_val_2 = B.eval(\n        feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\nprint(B_val_1, \"\\n\", B_val_2)\n", "intent": "* Goal: modify previous code for Minibatch gradient descent\n* Best practice: *placeholder* nodes (no computation, just data output)\n"}
{"snippet": "import tensorflow as tf\ntf.reset_default_graph()\nn_inputs = 28*28  \nn_hidden1 = 300\nn_hidden2 = 100\nn_outputs = 10\nlearning_rate = 0.01\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\ny = tf.placeholder(tf.int64,   shape=(None),           name=\"y\")\n", "intent": "* Use **mini-batch gradient descent** on MNIST dataset\n* Specify \n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, save_path) \n    X_new_scaled = mnist.test.images[:20]\n    Z = logits.eval(feed_dict={X: X_new_scaled})\n    print(np.argmax(Z, axis=1))\n    print(mnist.test.labels[:20])\n", "intent": "* Now trained - you can use the NN to predict.\n"}
{"snippet": "import tensorflow as tf\nwith tf.device(\"/cpu:0\"):\n    a,b = tf.Variable(3.0), tf.Variable(4.0)\nc = a*b\nc\n", "intent": "* Mostly up to you. To pin devices to specific device, use a device() function. Below: a,b pinned to cpu\n"}
{"snippet": "parameters = {'n_estimators':[40, 50, 60, 80, 100, 150, 200, 300], 'max_depth':[3, 4, 5, 6, 7, 8], 'min_child_weight': [1,3,5,7,9]}\nxgb = XGBClassifier(random_state=33, n_jobs=4)\nclf = GridSearchCV(xgb, parameters, scoring='roc_auc', cv=kf)\nclf.fit(X_train, y_train)\nprint('Best parameters: ', clf.best_params_)\n", "intent": "Whoah! New features gave a noticeable increase in quality! Let's tune hyperparameters via GridSearchCV.\n"}
{"snippet": "with tf.device(\"/gpu:0\"):\n    i = tf.Variable(3)\nconfig = tf.ConfigProto()\nconfig.allow_soft_placement = True\nsess = tf.Session(config=config)\ntest = sess.run(i.initializer) \nprint(test)\n", "intent": "* To allow TF to \"fall back\" to a CPU instead, use *allow_soft_placement=True*.\n"}
{"snippet": "optimizer = tf.train.GradientDescentOptimizer(0.1)\ntrain = optimizer.minimize(error)\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\n", "intent": "Like in the previous exercise, we will declare a gradient descent optimizer and run the initialization step.\n"}
{"snippet": "def bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n", "intent": "Do the same but for the bias term:\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Let's get the easy part out of the way, we create a session just like in any other TensorFlow-based code\n"}
{"snippet": "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n", "intent": "Now we can define the first convolution layer. This is a convolution using 32 5x5 filters.\n"}
{"snippet": "W_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n", "intent": "We do pretty much the same thing (convolution + maxpool), adding another 2 layers, but with 64 5x5 filters for the convolution piece:\n"}
{"snippet": "fitted_models = {}\nfor name, pipeline in pipelines.items():\n    model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1)\n    model.fit(X_train, y_train)\n    fitted_models[name] = model\n    print(name, 'has been fitted.')\n", "intent": "* Create fitted models dictionary that have been tuned using CV\n"}
{"snippet": "STEPS = 1000\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer()) \n    for step in range(STEPS):\n        sess.run(fetches = optimizer, feed_dict = {LEARNING_RATE: 0.02})\n        if step % 100 == 0:\n            print(\"STEP: {} MSE: {}\".format(step, sess.run(fetches = loss_mse)))\n    print(\"STEP: {} MSE: {}\".format(STEPS, sess.run(loss_mse)))\n    print(\"w0:{}\".format(round(float(sess.run(w0)), 4)))\n    print(\"w1:{}\".format(round(float(sess.run(w1)), 4)))\n", "intent": "Note our results are identical to what we found in Eager mode.\n"}
{"snippet": "def linear_model(img):\n    X = tf.reshape(tensor = img, shape = [-1, HEIGHT * WIDTH]) \n    W = tf.get_variable(name = \"W\", shape = [HEIGHT * WIDTH, NCLASSES], initializer = tf.truncated_normal_initializer(stddev = 0.1, seed = 1))\n    b = tf.get_variable(name = \"b\", shape = [NCLASSES], initializer = tf.zeros_initializer)\n    ylogits = tf.matmul(a = X, b = W) + b\n    return ylogits, NCLASSES\n", "intent": "Let's start with a very simple linear classifier. All our models will have this basic interface -- they will take an image and return logits.\n"}
{"snippet": "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\ntree_params = {\n    'n_estimators' : [250, 300, 350],\n    'max_depth' : [None, 10, 20],\n    'max_features' : ['sqrt', 'log2', 50]\n}\ngcv = GridSearchCV(random_forest, tree_params, scoring='neg_log_loss', cv=skf, verbose=1)\ngcv.fit(X_crossvalid, y_crossvalid)\n", "intent": " Using 3 splits as one of the most frequently used amount, shuffle samples in random order. \n"}
{"snippet": "def linear_model():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.InputLayer(input_shape = [HEIGHT, WIDTH], name = \"image\"))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(units = NCLASSES, activation = tf.nn.softmax, name = \"probabilities\"))\n    return model\n", "intent": "Let's start with a very simple linear classifier. All our models will have this basic interface -- they will take an image and return probabilities.\n"}
{"snippet": "def linear_model():\n    model = tf.keras.models.Sequential()\n    return model\n", "intent": "Let's start with a very simple linear classifier. All our models will have this basic interface -- they will take an image and return probabilities.\n"}
{"snippet": "def try_out():\n    with tf.Session() as sess:\n        fn = read_dataset(\n            mode = tf.estimator.ModeKeys.EVAL, \n            args = {\"input_path\": \"data\", \"batch_size\": 4, \"nitems\": 5668, \"nusers\": 82802})\n        feats, _ = fn()\n        print(feats[\"input_rows\"].eval())\n        print(feats[\"input_rows\"].eval())\ntry_out()\n", "intent": "This code is helpful in developing the input function. You don't need it in production.\n"}
{"snippet": "vect.fit(X_train)\nX_train_dtm = vect.transform(X_train)\nX_train_dtm\n", "intent": "Use CountVectorizer to create **document-term matrices** from X_train and X_test.\n"}
{"snippet": "X1 = tf.convert_to_tensor(_X1)\nX1\n", "intent": "Or you can convert them to a tensor like this.\n"}
{"snippet": "kde = KernelDensity(6.7858, kernel='gaussian').fit(X)\n", "intent": "So let us see the result\n"}
{"snippet": "g = gp.fit(x[:, np.newaxis], y)\n", "intent": "The `alpha` option provides a way to give the uncertainty on $y$ to the procedure.\n"}
{"snippet": "reg = AdaBoostRegressor(n_estimators=100)\n", "intent": "I will first run in default mode where the regressor is a Decision tree regressor. \n"}
{"snippet": "knn = KNeighborsRegressor(n_neighbors=3, weights='distance')\nknn_boost = AdaBoostRegressor(knn, n_estimators=100)\n", "intent": "We can also run this with a k-nearest neighbours regressor.\n"}
{"snippet": "random_forest_balanced = RandomForestClassifier(\n    random_state=17, n_estimators=350, max_features=150, min_samples_split=64)\n", "intent": "Fitting estimators on the whole train sets, evaluating on hold-out set.\n"}
{"snippet": "N = range(1, 11)\nmodels = [None for i in N]\nfor i, n in enumerate(N):\n    models[i] = GaussianMixture(n).fit(X)\n", "intent": "We then fit this dataset using a different number of components. I store each model fit in a list since I will need them later\n"}
{"snippet": "pca.fit(f)\n", "intent": "Do the PCA - you can experiment with subtracting off the mean, here I do not.\n"}
{"snippet": "def lrelu(x, leak=0.2):\n        f1 = 0.5 * (1 + leak)\n        f2 = 0.5 * (1 - leak)\n        return f1 * x + f2 * abs(x)\n", "intent": "This is the relu function suggested by my reviewer to help make the model more memory efficient.\n"}
{"snippet": "def process_single_frame(img):\n    rectangles = mult_sliding_windows(img) \n    heatmap_img = np.zeros_like(img[:,:,0])\n    heatmap_img = add_heat(heatmap_img, rectangles)\n    heatmap_img = apply_threshold(heatmap_img, 1)\n    labels = label(heatmap_img)\n    draw_img, rects = draw_labeled_bboxes(np.copy(img), labels)\n    return draw_img\n", "intent": "&nbsp;\n---\nRun your pipeline on a video stream with recurring detections frame by frame that reject outliers and follow detected vehicles.\n"}
{"snippet": "classifier.add(Dense(units = 6, kernel_initializer='uniform', activation ='relu'))\n", "intent": "- for a more complex models it might be useful to create a second hidden layer\n- the 2nd hidden layer, does not need the input_dim argument\n"}
{"snippet": "classifier.fit(X_train, y_train, batch_size=10, nb_epoch = 130)\n", "intent": "- the batch size & the Epoch\n"}
{"snippet": "model = LogisticRegression()\npredictor = ['Gender']\noutcome = 'Loan_Status'\nclassification_model(model, df, predictor, outcome)\n", "intent": "Let's start with fitting simpler model and gradually increasing model complexity just to understand overfitting and underfitting\n"}
{"snippet": "y = df[\"Hired\"]\nprint y\nX = df[features]\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X,y)\n", "intent": "Now actually construct the decision tree:\n"}
{"snippet": "duplicate_questions_model.fit(x=[padded_question_1s, padded_question_2s], y=labels, \n                              batch_size=batch_size, epochs=4, validation_split=0.1)\n", "intent": "Now, we can finally pass in our input arrays and output labels and watch the model train!\n"}
{"snippet": "lasso = LassoCV(cv =tscv, max_iter=10000)\nlasso.fit(X_train_scaled, y_train)\n", "intent": "Lets create our next model - Lasso Regression\n"}
{"snippet": "kmeans.fit(col.drop('Private', axis =1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "    lm = LinearRegression()\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "grid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\npd.Series(list(zip(feature_cols, logreg.coef_[0])))\n", "intent": "Confirm that the coefficients make intuitive sense.\n"}
{"snippet": "rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nprint(\"Accuracy: %0.3f\" % rf.score(X_train, y_train))\nprint(\"Accuracy: %0.3f\" % rf.score(X_test, y_test))\n", "intent": "(4) Repeat step 3 but with a Random Forest\n"}
{"snippet": "lm = Lasso(alpha=0.00001, normalize=True)\nlm.fit(X,y)\n", "intent": "**Fit the LASSO regression**\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X, y)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n"}
{"snippet": "feature_cols = ['season_2', 'season_3', 'season_4', 'humidity']\nX = bikes[feature_cols]\ny = bikes.total\nlinreg = LinearRegression()\nlinreg.fit(X, y)\nprint(pd.Series(list(zip(feature_cols, linreg.coef_))))\n", "intent": "- Let's see what happens if we exclude temperature\n"}
{"snippet": "regr = linear_model.LinearRegression()\nregr.fit(X_tr,y_tr)\n", "intent": "To fit the linear model, we first create a regression object and the fit the data with regression object.\n"}
{"snippet": "lr.fit(pca_features_train, y_train)\n", "intent": "Lets run the Linear Regression model once again to see if there are any improvements since last time\n"}
{"snippet": "regressor = LinearRegression()\nregressor.fit(X_train, y_train)\nprint regressor.intercept_\nprint regressor.coef_\n", "intent": "Add the code to instantiate and train (fit) the linear regression model.\n"}
{"snippet": "quadratic_regressor = LinearRegression()\nquadratic_regressor.fit(X_train_quadratic, y_train)\n", "intent": "Now fit another regressor on the expanded features.\n"}
{"snippet": "degree_7_regressor = LinearRegression()\ndegree_7_regressor.fit(X_train_degree_7, y_train)\n", "intent": "Now instantiate and train the model.\n"}
{"snippet": "starttime = datetime.datetime.now()\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=3, scoring='f1')\ngrid_search.fit(X_train_gs, y_train_gs)\nprint 'Best score:', grid_search.best_score_\nprint 'Best parameters set:'\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print '\\t', param_name, best_parameters[param_name]\nendtime = datetime.datetime.now()\nprint '\\nRun time (secs): ', timestamp(endtime) - timestamp(starttime)\n", "intent": "Start grid search and display results\n"}
{"snippet": "pipeline2 = Pipeline([\n    ('clf', RandomForestClassifier(bootstrap = False,criterion='entropy',n_jobs=-1))\n])\nn_features = int(math.ceil(math.sqrt(X_train_gs.shape[1])))\nparameters2 = {\n    'clf__n_estimators': (50, n_features),\n    'clf__max_depth': (50,100),\n    'clf__min_samples_split': (1, 2),\n    'clf__min_samples_leaf': (1, 10)\n}\n", "intent": "Set up a pipeline and the hyper-parameters\n"}
{"snippet": "starttime = datetime.datetime.now()\ngrid_search2 = GridSearchCV(pipeline2, parameters2, n_jobs=1, verbose=3, scoring='f1')\ngrid_search2.fit(X_train_gs_v, y_train_gs)\nprint 'Best score:', grid_search2.best_score_\nprint 'Best parameters set:'\nbest_parameters2 = grid_search2.best_estimator_.get_params()\nfor param_name in sorted(parameters2.keys()):\n    print '\\t', param_name, best_parameters2[param_name]\nendtime = datetime.datetime.now()\nprint '\\nRun time (secs): ', timestamp(endtime) - timestamp(starttime)\n", "intent": "Start grid search and display results\n"}
{"snippet": "predicted_classes = []\nwith tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('.'))\n    TopKV2 = sess.run(tf.nn.top_k(tf.nn.softmax(logits), 5), feed_dict={x: X_test_n})\n", "intent": "**Answer:**\nFrom the possibility distribution shown below, we can conclude that our classifier is very confident with the output it gives.\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, batch_size=16, epochs=20)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model.fit(X_train, Y_train, epochs = 20, batch_size = 32)\n", "intent": "Run the following cell to train your model on 2 epochs with a batch size of 32. On a CPU it should take you around 5min per epoch. \n"}
{"snippet": "from sklearn.linear_model import RidgeCV\nridge = RidgeCV(cv=tscv)\nridge.fit(X_train_scaled, y_train)\nplotModelResults(ridge, X_train_scaled, X_test_scaled, y_train, y_test, plot_intervals=True, plot_anomalies=True)\nplotCoefficients(ridge)\n", "intent": "Not Bad at all!\nLasso and Ridge turned out to quite close and already are superstars\nLets see the plots for Ridge\n"}
{"snippet": "sc.fit(imagedf.drop(axis=1,columns='Class'))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('lrm', lrm_best),\n('svc', SVMC_best)], voting='soft', n_jobs=4)\nvotingC = votingC.fit(X, y)\n", "intent": "Ensembling more than one model\n"}
{"snippet": "votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('lrm', lrm_best),\n('GBC', GBC_best)], voting='soft', n_jobs=4)\nvotingC = votingC.fit(X, y)\n", "intent": "Ensembling more than one model\n"}
{"snippet": "km.fit(cdf.drop(labels='Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlrm = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "knn.fit(X_Train,y_Train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "lm.fit(X_Train,y_Train)\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(X)\nlabels = ms.labels_\ncluster_centers = ms.cluster_centers_\nlabels_unique = np.unique(labels)\nn_clusters_ = len(labels_unique)\nprint(\"number of estimated clusters : %d\" % n_clusters_)\n", "intent": "Use sklearn.cluster.MeanShift method to perform clustering\n"}
{"snippet": "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nscore = cv_evaluation(KRR)\nprint('Kernel Ridge Score: {:.4f}'.format(score.mean()))\n", "intent": "* **Kernel Ridge Regression**:\n"}
{"snippet": "from sklearn.linear_model import Lasso\nLasso = Lasso(max_iter=10000)\nLasso.fit(pca_features_train, y_train)\nfrom sklearn.linear_model import Ridge\nridge = Ridge(max_iter=10000, random_state=17)\nridge.fit(pca_features_train, y_train)\n", "intent": "Now Lets see how Lasso and Ridge are performing on PCA transformed data\n"}
{"snippet": "grid_sgdlogreg.fit(X_train, y_train)\n", "intent": "Conduct the grid search and train the final model on the whole dataset:\n"}
{"snippet": "idx = 0\ntarget_y = 6\nX_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\nX_fooling = make_fooling_image(X_tensor[idx:idx+1], target_y, model)\nscores = model(Variable(X_fooling))\nassert target_y == scores.data.max(1)[1][0], 'The model is not fooled!'\n", "intent": "Run the following cell to generate a fooling image:\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\ntorch.cuda.set_device(7)\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "intent": "Finetuning the convnet\n----------------------\nLoad a pretrained model and reset final fully connected layer.\n"}
{"snippet": "ols_5 = ols('PRICE ~ RM + CRIM +PTRATIO + DIS + LSTAT',bos).fit()\nprint(ols_5.summary())\n", "intent": "using CRIM, RM, PTRATIO, DIS, and LSTAT\n"}
{"snippet": "model_NB.fit(X_text_train_tfidf, np.asarray(yelp_data_final_update['Review_category_useful_notuseful']))\n", "intent": "Use the features trained by tfidf vectorizer for the Naive Bayes Model: Useful vs not useful\n"}
{"snippet": "model_NB.fit(X_text_train_tfidf, np.asarray(yelp_data_final_update['Review_category_cool_notcool']))\n", "intent": "Use the features trained by tfidf vectorizer for the Naive Bayes Model: cool vs not cool\n"}
{"snippet": "model_NB.fit(X_text_train_tfidf, np.asarray(yelp_data_final_update['Review_category_funny_notfunny']))\n", "intent": "Use the features trained by tfidf vectorizer for the Naive Bayes Model: funny vs not funny\n"}
{"snippet": "model_random_forest_10trees.fit(X_text_train_tfidf, yelp_data_final_update['Review_category_useful_notuseful'])\n", "intent": "Random forest useful vs not useful:\n"}
{"snippet": "model_random_forest_100trees.fit(X_text_train_tfidf, yelp_data_final_update['Review_category_funny_notfunny'])\n", "intent": "Random Forest for funny versus not funny:\n"}
{"snippet": "m = Prophet()\nm.fit(train_df);\n", "intent": "Fitting the model and Creating Future Dataframes including the history\n"}
{"snippet": "model_NB.fit(X_text_train_tfidf, yelp_data_final_update['Review_category_coolest_funniest_mostuseful'])\n", "intent": "Use the features trained by teh tfidf vectorizer to train the Naive Bayes Model:\n"}
{"snippet": "model_NB.fit(X_text_train_tfidf, np.asarray(yelp_data_final_update['Review_category_cool_notcool_binary']))\n", "intent": "Use the features trained by tfidf vectorizer for the Naive Bayes Model: cool vs not cool\n"}
{"snippet": "model_random_forest_10trees.fit(X_text_train_tfidf, yelp_data_final_update['Review_category_useful_notuseful_binary'])\n", "intent": "Random forest useful vs not useful:\n"}
{"snippet": "model_random_forest_10trees.fit(X_text_train_tfidf, yelp_data_final_update['Review_category_funny_notfunny_binary'])\n", "intent": "Random Forest for funny versus not funny:\n"}
{"snippet": "model_random_forest_10trees.fit(X_text_train_tfidf, yelp_data_final_update['Review_category_cool_notcool_binary'])\n", "intent": "Random Forest for cool not cool:\n"}
{"snippet": "model_logistic_regression=LogisticRegression()\n", "intent": "Use logistic Regression to classify the reivews into useful vs not useful:\n"}
{"snippet": "model_logistic_regression=LogisticRegression()\n", "intent": "Use logistic Regression:\n"}
{"snippet": "diabetes_X_train = diabetes.data[:-20]\ndiabetes_X_test = diabetes.data[-20:]\ndiabetes_y_train = diabetes.target[:-20]\ndiabetes_y_test = diabetes.target[-20:]\nregr = LinearRegression()\nregr.fit(diabetes_X_train, diabetes_y_train)\n", "intent": "We ran linear regression using only one feature. Now we will run linear regression with all the features in the dataset.\n"}
{"snippet": "from sklearn import neighbors\nknn = neighbors.KNeighborsClassifier(n_neighbors=10)\nknn.fit(X,y) \n", "intent": "Let us choose a model and fit the training data:\n"}
{"snippet": "model_ridge = Pipeline(\n    steps=[\n        ('preprocessing', preprocessing),\n        ('ridge', Ridge(random_state=RANDOM_SEED))\n    ]\n)\ncv = GridSearchCV(model_ridge, param_grid={}, scoring='neg_mean_absolute_error', cv=TimeSeriesSplit(n_splits=5),\n                 return_train_score=True, verbose=3)\ncv.fit(X_train, y_train)\n", "intent": "Let's try Ridge from sklearn.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nclf = LinearRegression()\nclf.fit(data.data, data.target)\n", "intent": "Let's make predictions\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X,Y)\n", "intent": "Fitting of a Logistic Regression\n"}
{"snippet": "lr_model3 = LinearRegression()\nlr_model3.fit(X_train,y_train)\n", "intent": "Next, let us try using all of the variables (in the reduced selection)\n"}
{"snippet": "km = KMeans(n_clusters=20)\n", "intent": "Try creating a KMeans clusterer with 20 classes (obviously 10 would be ideal, but let's try 20 first).  Fit the model to the digits data.\n"}
{"snippet": "lsi = models.LsiModel(tfidf_corpus, id2word=id2word, num_topics=300)\n", "intent": "- Now let's build an LSI space!\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=3, metric=smp.cosine_distances)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)\n", "intent": "- And now classification:\n"}
{"snippet": "left_branch = Sequential()\nleft_branch.add(Dense(32, input_dim=784))\nright_branch = Sequential()\nright_branch.add(Dense(32, input_dim=784))\nmerged = Merge([left_branch, right_branch], mode='concat')\nfinal_model = Sequential()\nfinal_model.add(merged)\nfinal_model.add(Dense(10, activation='softmax'))\nSVG(model_to_dot(final_model, show_shapes=True).create(prog='dot', format='svg'))\n", "intent": "- Merge multiple `Sequential` models into a single layer\n- A number of options for merging outputs: `concat`, `sum`, `ave`, etc\n- Like so:\n"}
{"snippet": "skipgram = Sequential()\nskipgram.add(Embedding(input_dim=V, input_length=1, embeddings_initializer=\"glorot_uniform\", output_dim=100))\nskipgram.add(Reshape((dim, )))\nskipgram.add(Dense(input_dim=dim, units=V, activation='softmax'))\nSVG(model_to_dot(skipgram, show_shapes=True).create(prog='dot', format='svg'))\n", "intent": "- Lastly, we create the (shallow) network!\n"}
{"snippet": "estimator = LogisticRegression()\nplot_learning_curve(estimator, \"Logistic Learning Curve\",cancer.iloc[:,cancer.columns != 'survival_status'], cancer['survival_status']);\n", "intent": "Nodes are the highest indicator followed by age and year operated\n"}
{"snippet": "clf = SVC(class_weight='balanced', gamma='scale', random_state=17)\nparams = {\n    'C': [0.01, 0.1, 1, 10],\n    'kernel': ('linear', 'poly', 'rbf', 'sigmoid')\n}\ngscv = GridSearchCV(clf, params, scoring='recall', cv=cv)\ngscv.fit(X_train, y_train)\ngscv.best_params_\n", "intent": "We choose SVM classifier with balanced weights. Our scoring metrics is recall. We are tuning the regularization parameter `C` and the `kernel` type.\n"}
{"snippet": "rfmodel4=RandomForestRegressor(n_estimators = 2000, min_samples_leaf=5,  n_jobs=-1, max_features=5) \nrfmodel4.fit(X5_train,y5_train)\n", "intent": "**Random Forests again!!**\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: test_x,\n            labels_: test_y,\n            keep_prob:1.0}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "sess = tf.Session()\nval_m2 = np.array([[2],[3]])\nres = sess.run(fetches=..., feed_dict=...) \nprint(res)\nsess.close()\n", "intent": "Provide the correct feeds (inputs) and fetches (outputs of the computational graph)\n"}
{"snippet": "name = 'fc_autoencoder'\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=(784),activation='relu',name=\"bottleneck\"))\nmodel.add(Dense(784,activation='sigmoid',name=\"reconstruction\"))\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adadelta')\n", "intent": "We use a fc NN with a bottleneck architecture and the objective to reconstruct the input image.\n"}
{"snippet": "from keras.models import Model\nmodel_bottleneck = Model(inputs=model.input, outputs=model.get_layer('bottleneck').output)\n", "intent": "Extracting the bottleneck features and predicting the reconstruction\n"}
{"snippet": "from keras.models import Model\nmodel_bottleneck = Model(inputs=model.input, outputs=model.get_layer('conv2d_3').output)\n", "intent": "Extracting the bottleneck features and predicting the reconstruction\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "lm2 = LinearRegression(fit_intercept=False)\nlm2.fit(X, bos.PRICE)\n", "intent": "I would recommend not having an intercept as we are saying then that expected y given x = 0 is zero but we almost never can know that for sure.\n"}
{"snippet": "clf = SVC(kernel='linear', class_weight='balanced', gamma='scale', random_state=17)\nparams = {\n    'C': [0.005, 0.01, 0.02]\n}\ngscv = GridSearchCV(clf, params, scoring='recall', cv=cv)\ngscv.fit(X_train, y_train)\ngscv.best_params_\n", "intent": "Our best parameters are: linear kernel with C = 0.01. Now let's make one more cross-validation with closer range of the parameter `C`.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(wine_mag_train, wine_abv_train)\n", "intent": "Then, we fit the model to our data.\n"}
{"snippet": "clf = Perceptron(n_iter=10)\nclf.fit(X, y)\n", "intent": "*How do we fit the perceptron model?* [perceptron definition](\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ngs = GridSearchCV(estimator=pipe_lasso, \n                  param_grid={'lasso__alpha':lambdas}, \n                  scoring='neg_mean_squared_error', \n                  cv=10)\ngs = gs.fit(X_train, y_train)\ngs.best_params_\n", "intent": "*How does this look in Python?*\n"}
{"snippet": "x = tf.nn.softmax([2.0, 1.0, 0.2])\n", "intent": "TensorFlow Softmax\nWe're using TensorFlow to build neural networks and, appropriately, there's a function for calculating softmax.\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, save_file)\n    test_accuracy = sess.run(\n        accuracy,\n        feed_dict={features: mnist.test.images, labels: mnist.test.labels})\nprint('Test Accuracy: {}'.format(test_accuracy))\n", "intent": "Load a Trained Model\nLet's load the weights and bias from memory, then check the test accuracy.\n"}
{"snippet": "with tf.Session() as sess:\n  result = sess.run([product])\n  print(result)\n", "intent": "If you do not specify a graph, the default graph is used.\nTo launch your graph you need a `Session` object.\n"}
{"snippet": "with tf.Session() as sess:\n  with tf.device(\"/cpu:0\"):\n    matrix1 = tf.constant([[3., 3.]])\n    matrix2 = tf.constant([[2.],[2.]])\n    product = tf.matmul(matrix1, matrix2)\n    result = sess.run(product)\n    print(result)\n", "intent": "You can tell the session which device to use for doing a computation.\n"}
{"snippet": "input1 = tf.constant([3.0])\ninput2 = tf.constant([2.0])\ninput3 = tf.constant([5.0])\nintermed = tf.add(input2, input3)\nmul = tf.mul(input1, intermed)\nwith tf.Session() as sess:\n  result = sess.run([mul, intermed])\n  print(result)\n", "intent": "`Session.run()` returns the results of the defined operations.\n"}
{"snippet": "x = tf.placeholder(tf.int32)\nwith tf.Session() as sess:\n    output = sess.run(x, feed_dict={x : 23})\n    output = sess.run(x, feed_dict={x : \"5\"}) \n    output = sess.run(x, feed_dict={x : 12.34}) \nprint(output)\n", "intent": "https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops/placeholders\nBasic structure: tf.placeholder(dtype, shape=None, name=None)\n"}
{"snippet": "clf = SVC(C=0.01, kernel='linear', class_weight='balanced', gamma='scale', random_state=17)\nclf.fit(X_train, y_train)\n", "intent": "Still C = 0.01 is the best, so we use it to train the whole train dataset.\n"}
{"snippet": "hidden_inputs = tf.add(tf.matmul(features,weights_hidden),biases_hidden)\nhidden_outputs = tf.add(tf.matmul(hidden_inputs,weights_out),biases_out)\nprediction_hidden = tf.nn.relu(hidden_outputs)\n", "intent": "$$ x_h = x.W_{x -> h} + b_{h}$$\n$$ x_h = relu (x_h) $$\n$$ x_o = x.W_{h -> o} + b_{o}$$\n"}
{"snippet": "logits = tf.nn.softmax(hidden_outputs)\n", "intent": "$$ z_i = \\frac{e^{x_{o,i}}}{\\sum_i{e^{x_{o,i}} }}$$\nwhere i is the output from the i-th neuron in the softmax layer\n"}
{"snippet": "writer.add_graph(sess.graph)\n", "intent": "tensorboard --logdir=[PATH TO LOGDIR]\n"}
{"snippet": "model.fit(x_train, y_train, epochs=100, batch_size=1000, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "clf.fit(x_train,y_train)\n", "intent": "Now its time to train our model on our training data!\n** Import LinearRegression from sklearn.linear_model **\n"}
{"snippet": "clf = LogisticRegression()\nclf.fit(X_train,y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "model.fit(np.expand_dims(X_train, -1), y_train, epochs=5)\n", "intent": "Train the network and use the test data as the validation set. \n"}
{"snippet": "with tf.name_scope('loss'):\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_logits))\nwith tf.name_scope('train'):\n    train_step = tf.train.AdamOptimizer(LR, name='adam_optimizer').minimize(loss)\nwith tf.name_scope('accuracy'):\n    corrects = tf.equal(tf.argmax(tf.nn.softmax(y_logits),1), tf.argmax(y_test, 1))\n    acc = tf.reduce_mean(tf.cast(corrects, tf.float32))\n", "intent": "Set up our loss and training algorithms. We'll use `Adam` here, which is a tweak on the way that stochastic gradient descent works.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, './saved_model/cnn_mnist.ckpt')\n    preds = sess.run(tf.nn.softmax(y_logits), feed_dict={X: X_test})\n    pred_val = tf.argmax(preds, 1).eval()\n", "intent": "Collect the predictions for the test set.\n"}
{"snippet": "logit = LogisticRegression(solver='lbfgs', n_jobs=-1, random_state=7)\nlogit.fit(X_train, y_train)\n", "intent": "**The next step is to train Logistic Regression.**\n"}
{"snippet": "from sklearn import tree\nX = df.drop('Play', axis=1)\ny = df['Play']\nclf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\nclf.fit(X, y)\n", "intent": "Now we use the decision tree classifier in sklearn to build our model.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)\nclf.fit(X_train, y_train)\n", "intent": "2. import knn classifier, k = 3\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_weights = tf.Variable(tf.truncated_normal([vocab_size, embed_dim], stddev=0.1), name=\"embedding\")\n    embedding = tf.nn.embedding_lookup(embedding_weights, input_data)\n    return embedding\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1024, activation = 'relu', input_shape=(1000,)))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=50, batch_size=1000, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.metrics import roc_auc_score\nparams = {'n_neighbors': arange(1, 100, 1)}\ngrid_searcher = GridSearchCV(KNN(),\\\n                             params, cv=8, scoring='roc_auc', n_jobs=3)\nprint grid_searcher.fit(X_train, y_train)\nprint grid_searcher.best_score_\nprint grid_searcher.best_estimator_\n", "intent": "Use 5-fold crossvalidation for finding optimal K in KNN\n"}
{"snippet": "params = {'C': [0.1, 1, 10]}\ngrid_searcher = GridSearchCV(SVC(probability=True), \\\n                            params, \\\n                            cv = 8, n_jobs = -1)\nprint grid_searcher.fit(Xtrain, ytrain)\nprint grid_searcher.decision_function\nprint grid_searcher.best_score_\nprint grid_searcher.best_estimator_\n", "intent": "GridSearchCV for SVC and then mix it with RandomForestClassifier and ExtraTreesClassifier.\n"}
{"snippet": "regr = Lasso(alpha=alpha_best)\nregr.fit(X_train, y_train)\nregr.coef_\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim = x_train.shape[1]))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "lasso_cv = LassoCV(alphas=alphas, \n                   cv=3, random_state=17)\nlasso_cv.fit(X, y)\n", "intent": "**Now let's find the best value of $\\alpha$ during cross-validation.**\n"}
{"snippet": "a1 = activation(torch.mm(features, W1) + B1)\na2 = activation(torch.mm(a1, W2) + B2)\na2\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1000, input_dim = x_train.shape[1], activation = 'relu'))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.summary()\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "def classify(model, img):\n    img = img.cuda()\n    img = Variable(img, volatile=True)\n    output = model(img)\n    return output.data.max(1)[1]\n", "intent": "This function conducts inference on a single image\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "callback = [ EarlyStopping( monitor = 'val_loss', patience = 2, verbose = 1 ) ]\nmodel.fit( X_train, y_train, shuffle = True,\n           nb_epoch = 10, batch_size = 256,\n           validation_data = (X_val, y_val),\n           callbacks = callback )\n", "intent": "```python\nfrom keras.utils.visualize_util import plot\nplot( model, to_file = 'model.png', show_shapes = True, show_layer_names = True )\n```\n"}
{"snippet": "X = T.matrix('X') \ny = T.lvector('y') \n", "intent": "The first thing we need to is define our computations using Theano. We start by defining our input data matrix `X` and our training labels `y`:\n"}
{"snippet": "forward_prop = theano.function([X], y_hat)\ncalculate_loss = theano.function([X, y], loss)\npredict = theano.function([X], prediction)\nforward_prop([[1,2]])\n", "intent": "We saw how we can evaluate a Theano expression by creating a [Theano function](http://deeplearning.net/software/theano/library/compile/function.html\n"}
{"snippet": "import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nnp.random.seed(7)\n", "intent": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nte5 = TextExplainer(clf=DecisionTreeClassifier(max_depth=2), random_state=0)\nte5.fit(doc, pipe.predict_proba)\nprint(te5.metrics_)\nte5.show_weights()\n", "intent": "it uses words as features and doesn't take word position in account\n"}
{"snippet": "from xgboost import XGBRegressor \nxgb = XGBRegressor()\nxgb.fit(X_train_scaled, y_train);\n", "intent": "Why shouldn't we try XGBoost now?\n<img src=\"https://habrastorage.org/files/754/a9a/26e/754a9a26e59648de9fe2487241a27c43.jpg\"/>\n"}
{"snippet": "weight, params = [], []\nfor c in np.arange( -4, 6 ) :\n    lr = LogisticRegression( penalty = \"l1\", C = 10 ** c, random_state = 0 )\n    lr.fit( x_train_std, y_train )\n    weight.append(lr.coef_[1])\n    params.append(10 ** c)\n", "intent": "Plotting the regularization Path, weight coefficient of the different features for different regularization.\n"}
{"snippet": "knn.fit( x_train_std, y_train )\nprint( \"Training Accuracy\", knn.score( x_train_std, y_train ) )\nprint( \"Testing Accuracy\", knn.score( x_test_std, y_test ) )\n", "intent": "Compare performance. Using less feature to obtain a higher accuracy and reduce overfitting.\n"}
{"snippet": "clf = gs.best_estimator_\nclf.fit( x_train, y_train )\nprint( 'Test accuracy: %.3f' % clf.score( x_test, y_test ) )\n", "intent": "Then use the `best_estimator` to estimate the performance of the best selected model.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nparams = { 'dt__max_depth': [1, 2],\n           'lr__clf__C': [0.001, 0.1, 100.0] }\ngrid = GridSearchCV( estimator = clf_vc, \n                     param_grid = params, \n                     cv = 10, scoring = 'roc_auc' )\ngrid.fit( X_train, y_train )\nfor params, mean_score, scores in grid.grid_scores_:\n    print( \"%0.3f+/-%0.2f %r\"\n           % ( mean_score, scores.std() / 2, params ) ) \n", "intent": "We can also use it with grid search to obtain the best set of hyperparameters.\n"}
{"snippet": "import theano\nimport theano.tensor as T\nimport numpy\nx = T.fvector('x')\nW = theano.shared(numpy.asarray([0.2, 0.7]), 'W')\ny = (x * W).sum()\nf = theano.function([x], y)\noutput = f([1.0, 1.0])\nprint(output)\n", "intent": "http://www.marekrei.com/blog/theano-tutorial/\nhttps://github.com/bbc/theano-bpr\n"}
{"snippet": "model.fit(x_train, y_train, \n          batch_size=32,\n          epochs=10,\n          validation_data=(x_test, y_test),\n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "lstm_mpc = Sequential()\nlstm_mpc.add(embedding_layer)\nlstm_mpc.add(LSTM(128, dropout=.2, recurrent_dropout=.2, return_sequences=True))\nlstm_mpc.add(LSTM(128, dropout=.2, recurrent_dropout=.2))\nlstm_mpc.add(Dense(num_classes, activation='sigmoid'))\nlstm_mpc.compile(loss='categorical_crossentropy', \n              optimizer='rmsprop', \n              metrics=['accuracy'])\nprint(lstm_mpc.summary())\n", "intent": "You have free reign to provide additional analyses.\nAnother Idea: Try to create a RNN for generating novel text. \n"}
{"snippet": "kmeans.fit(college.drop('Private', axis = 1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "scaler.fit(KNN_data.drop('TARGET CLASS', axis = 1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "isof = IsolationForest(random_state=77,n_jobs=4,contamination=0.05)\n", "intent": "We will test this implementation of IF on our 2-d dataset.\n"}
{"snippet": "from sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X_train,y_train)\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n", "intent": "**Create a dictionary called param_grid and fill out some parameters for C and gamma.**\n"}
{"snippet": "kmeans_iter1 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n                     algorithm=\"full\", max_iter=1, random_state=1)\nkmeans_iter2 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n                     algorithm=\"full\", max_iter=2, random_state=1)\nkmeans_iter3 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n                     algorithm=\"full\", max_iter=3, random_state=1)\nkmeans_iter1.fit(X)\nkmeans_iter2.fit(X)\nkmeans_iter3.fit(X)\n", "intent": "Let's run the K-Means algorithm for 1, 2 and 3 iterations, to see how the centroids move around:\n"}
{"snippet": "KMeans()\n", "intent": "To set the initialization to K-Means++, simply set `init=\"k-means++\"` (this is actually the default):\n"}
{"snippet": "gms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\n", "intent": "Let's train Gaussian Mixture models with various values of $k$ and measure their BIC:\n"}
{"snippet": "min_bic = np.infty\nfor k in range(1, 11):\n    for covariance_type in (\"full\", \"tied\", \"spherical\", \"diag\"):\n        bic = GaussianMixture(n_components=k, n_init=10,\n                              covariance_type=covariance_type,\n                              random_state=42).fit(X).bic(X)\n        if bic < min_bic:\n            min_bic = bic\n            best_k = k\n            best_covariance_type = covariance_type\n", "intent": "Let's search for best combination of values for both the number of clusters and the `covariance_type` hyperparameter:\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\n", "intent": "The integer passed in the verbose argument will allow the output to be more or less detailed.\n"}
{"snippet": "grid.fit(X_train,y_train)\n", "intent": "Calling `grid.fit()` may take lots of time. This underscores the importance of `verbose = n` to display output.\n"}
{"snippet": "dropout_layer = tf.nn.dropout(full_layer, keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "from sklearn.externals import joblib\nfrom dask.distributed import Client\nclient = Client()\nparameters = {'clf__C': (0.1, 1, 10, 100)}\ngrid_search = GridSearchCV(classifier, parameters, scoring ='roc_auc', cv=skf)\nt_start = time.time()\nwith joblib.parallel_backend('dask'):\n    grid_search.fit(X_text, y_text)\nt_end = time.time()\nprint('Elapsed time for grid_search with joblib replace (s):', round((t_end - t_start)))    \n", "intent": "In this approach all we need to do is replace joblib to dask distributed. We need to initialize distributed client, and change backend\n"}
{"snippet": "slr = SimpleLinearRegression(fit_intercept=True)\nslr.fit_intercept\n", "intent": "**Now, if we instantiate the class, it will assign `fit_intercept` to the class attribute `fit_intercept`. Try it out:**\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nslr = LinearRegression()\nslr.fit(X_train, y_train)\n", "intent": "<a id='fit-on-train'></a>\nUsing the training `X` and training `y`, we can fit a linear regression with sklearn's `LinearRegression`.\n"}
{"snippet": "knn5 = KNeighborsClassifier(n_neighbors=5, weights='uniform')\nscores = accuracy_crossvalidator(xs, y, knn5, cv_indices)\n", "intent": "<a id='cv-knn5'></a>\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(Xs, y)\n", "intent": "**10.E Fit a `KNeighborsClassifier` with the best number of neighbors.**\n"}
{"snippet": "logreg_cv = LogisticRegressionCV(Cs=100, cv=5, penalty='l1', scoring='accuracy', solver='liblinear')\nlogreg_cv.fit(x_train, y_train)\n", "intent": "**Gridsearch hyperparameters for the training data.**\n"}
{"snippet": "param_test6 = {\n    'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n}\ngsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n                                        min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n                       param_grid = param_test6, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch6.fit(train[predictors],train[target])\n", "intent": "Try regularization:\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nparam_grid = {'n_neighbors': np.arange(1, int(k+(k/2)), 1),\n              'weights': [\"uniform\", \"distance\"]}\nknn_r = GridSearchCV(neighbors.KNeighborsRegressor(), param_grid)\nknn_r.fit(X_train, y_train)\n", "intent": "A more thorough analysis allows for checking multiple values for any parameter, let's look for the best model by looking at a range of values for `k`\n"}
{"snippet": "tfidf = models.TfidfModel(corpus)\ncorpus_tfidf = tfidf[corpus]\n", "intent": "For LSI we first create a tfidf similar to above with clustering:\n"}
{"snippet": "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=6)\ncorpus_lsi = lsi[corpus_tfidf]\nlsi.print_topics(6)\n", "intent": "We then build the model:\n"}
{"snippet": "', '.join(PolynomialFeatures().fit(df_train).get_feature_names())\n", "intent": "Note that with its default settings PolynomialFeatures will add 15 new features:\n"}
{"snippet": "kmeans = cluster.KMeans(n_clusters=4)\nkmeans.fit(X)\n", "intent": "- Select a number of clusters of your choice based on your visual analysis above.\n"}
{"snippet": "def cluster_plotter(eps=1.0, min_samples=5):\n    db = DBSCAN(eps=eps, min_samples=min_samples)\n    db.fit(X)\n    plot_clusters(X, db.point_cluster_labels)\n", "intent": "---\n<a id='plot-fit'></a>\nDon't pass `X` in to the function. We will just use the \"global\" X defined in the jupyter notebook earlier.\n"}
{"snippet": "dbscn = DBSCAN(eps =3, min_samples = 3).fit(X)\n", "intent": "Remember to pass an `eps` and `min_samples` of your choice.\n"}
{"snippet": "dbscn = DBSCAN(eps = .5, min_samples = 3).fit(X)\n", "intent": "**9.3 Evaluate DBSCAN visually, with silhouette, and with the metrics against the true `y`.**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1000, activation='relu'))\nmodel.add(Dropout(.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "random_forest_default_model = RandomForestClassifier()\nrandom_forest_default_model.fit(X_train, y_train)\n", "intent": "First, model with default parameters will be trained to check accuracy and AUROC.\n"}
{"snippet": "random_forrest_optimized_gsearch2 = GridSearchCV(RandomForestClassifier(bootstrap=True),\n                                                 {'n_estimators' : [110, 120, 130, 150],\n                                                  'max_features' : ['auto', 'log2', 'sqrt']},\n                                                 scoring='roc_auc',\n                                                 verbose=1,\n                                                 n_jobs=2)\nrandom_forrest_optimized_gsearch2.fit(train, train_labels)\ndisplay(random_forrest_optimized_gsearch2.best_params_)\nprint(\"Best score {}\".format(random_forrest_optimized_gsearch2.best_score_))\n", "intent": "Bootstrap value will be taken from previous search.\n"}
{"snippet": "solution_current = solution_best = np.random.binomial(1, 0.5, ncols).astype(bool)\nsolution_vars = predictors[predictors.columns[solution_current]]\ng = LinearRegression().fit(X=solution_vars, y=logsalary)\naic_best = aic(g, solution_vars, logsalary)\naic_values.append(aic_best)\n", "intent": "Initialize the annealing run and temperature schedule:\n"}
{"snippet": "def get_phrase_embedding(phrase):\n    vector = np.zeros([model.vector_size], dtype='float32')\n    word_count = 0\n    for word in phrase.split():\n        if word in model.vocab:\n            vector += model.get_vector(word)\n            word_count += 1\n    if word_count:\n        vector /= word_count\n    return vector\n", "intent": "Our categories mingled, but we can notice that years, days, languages are stays apart from authors cloud.\n"}
{"snippet": "step = theano.shared(10., name='step')\ntrain = theano.function(\n          inputs=[x, y],\n          outputs=[prediction, xent],\n          updates=((w, w - step * gw), (b, b - step * gb), (step, step * 0.99)))\npredict = theano.function(inputs=[x], outputs=prediction)\n", "intent": "Compile Theano functions:\n"}
{"snippet": "with disaster_model:\n    early_mean = pm.Exponential('early_mean', lam=1)\n    late_mean = pm.Exponential('late_mean', lam=1)\n", "intent": "Similarly, the rate parameters can automatically be given exponential priors:\n"}
{"snippet": "from pymc3.glm import glm\nwith pm.Model() as model:\n    glm('y ~ x', data)\n    fit = pm.advi(n=50000)\n", "intent": "The model can then be very concisely specified in one line of code.\n"}
{"snippet": "with pm.Model() as model:\n    alpha = pm.Gamma('alpha', 1., 1.)\n    beta = pm.Beta('beta', 1., alpha, shape=K)\n    w = pm.Deterministic('w', beta * T.concatenate([[1], T.extra_ops.cumprod(1 - beta)[:-1]])) \n    component = pm.Categorical('component', w, shape=N)\n    tau = pm.Gamma('tau', 1., 1., shape=K)\n    mu = pm.Normal('mu', 0, tau = tau, shape=K)\n    obs = pm.Normal('obs', mu[component], tau = tau[component],\n                    observed=old_faithful_df.std_waiting.values)\n", "intent": "We can implement this in `pymc3` in the following way:\n"}
{"snippet": "N = sunspot_df.shape[0]\nK = 30\nwith pm.Model() as model:\n    alpha = pm.Gamma('alpha', 1., 1.)\n    beta = pm.Beta('beta', 1, alpha, shape=K)\n    w = pm.Deterministic('w', beta * T.concatenate([[1], T.extra_ops.cumprod(1 - beta[:-1])]))\n    component = pm.Categorical('component', w, shape=N)\n    mu = pm.Uniform('mu', 0., 300., shape=K)\n    obs = pm.Poisson('obs', mu[component], observed=sunspot_df['sunspot.year'])\n", "intent": "The model is specified as:\n"}
{"snippet": "import numpy as np\nn = 1000\nboot_samples = np.empty((n, len(lrmod.coef_[0])))\nfor i in np.arange(n):\n    boot_ind = np.random.randint(0, len(X0), len(X0))\n    y_i, X_i = y.values[boot_ind], X0.values[boot_ind]\n    lrmod_i = LogisticRegression(C=1000)\n    lrmod_i.fit(X_i, y_i)\n    boot_samples[i] = lrmod_i.coef_[0]\n", "intent": "We can bootstrap some confidence intervals:\n"}
{"snippet": "km_wine = KMeans(n_clusters=3, random_state=rng)\nkm_wine.fit(X_pca)\n", "intent": "We can now create a `KMeans` object with `k=3`, and fit the data with it.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nfit = LinearRegression().fit(y=bodyfat['fat'], X=bodyfat[subsets[3]])\n", "intent": "We can use scikit-learn to estimate an ordinary least squares (OLS) model for a given set of predictors.\n"}
{"snippet": "from sklearn.linear_model import RidgeCV\nplot_learning_curve(LinearRegression())\nplot_learning_curve(Ridge())\nplot_learning_curve(RidgeCV())\n", "intent": "scikit-learn's `RidgeCV` class automatically tunes the L2 penalty using Generalized Cross-Validation.\n"}
{"snippet": "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n                          meta_classifier=LogisticRegression())\ncompare(sclf)\n", "intent": "To get more information look https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/\n"}
{"snippet": "rf = RandomForestClassifier(n_jobs=4, criterion='entropy')\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\nindices = np.argsort(importances)[::-1]\nprint(\"Feature ranking:\")\nfor f in range(X.shape[1]):\n    print(\"%d. %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n", "intent": "`RandomForestClassifier` uses the Gini impurity index by default; one may instead use the entropy information gain as a criterion.\n"}
{"snippet": "est = GradientBoostingRegressor(n_estimators=3000, max_depth=6, learning_rate=0.04,\n                                loss='huber', random_state=0)\nest.fit(X_train, y_train)\n", "intent": "Lets fit a gradient boosteed regression tree (GBRT) model to this dataset and inspect the model:\n"}
{"snippet": "z1 = X.dot(W1) + b1\na1 = T.tanh(z1)\nz2 = a1.dot(W2) + b2\ny_hat = T.nnet.softmax(z2) \nloss_reg = 1./num_examples * alpha/2 * (T.sum(T.sqr(W1)) + T.sum(T.sqr(W2))) \nloss = T.nnet.categorical_crossentropy(y_hat, y).mean() + loss_reg\nprediction = T.argmax(y_hat, axis=1)\n", "intent": "Our definition of forward propagation in Theano is identical to our pure Python implementation, except that we now define Theano expressions. \n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix,roc_auc_score\nsplit_ind = 0.7 * len(Y)\nX_train = X[:split_ind]\nY_train = Y[:split_ind]\nX_test = X[split_ind:]\nY_test = Y[split_ind:]\nn_est_lim = 1000\ngbc = GradientBoostingClassifier(n_estimators = n_est_lim, max_depth = 2)\ngbc.fit(X_train, Y_train)\n", "intent": "<p>\nNow let's compare training and test error as a function of the \n<p>\n"}
{"snippet": "def best_threshold():\n    maximum_ig = 0\n    maximum_threshold = 0\n    for threshold in X['number_pets']:\n        ig = information_gain(X['number_pets'], threshold, np.array(Y))\n        if ig > maximum_ig:\n            maximum_ig = ig\n            maximum_threshold = threshold\n    return \"The maximum IG = %.3f and it occured by splitting on %.4f.\" % (maximum_ig, maximum_threshold)\nprint best_threshold()\n", "intent": "To be more precise, we can iterate through all values and find the best split.\n"}
{"snippet": "decision_tree = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\")\ndecision_tree.fit(X, Y)\nDecision_Tree_Image(decision_tree, X.columns)\n", "intent": "Let's see how we can do this with just sklearn!\n"}
{"snippet": "decision_tree = DecisionTreeClassifier(max_depth=2, criterion=\"entropy\")\ndecision_tree.fit(X, Y)\nDecision_Tree_Image(decision_tree, X.columns)\n", "intent": "Let's add one more level to our decision tree.\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix,roc_auc_score\nsplit_ind = int(0.7 * len(Y))\nX_train = X[:split_ind]\nY_train = Y[:split_ind]\nX_test = X[split_ind:]\nY_test = Y[split_ind:]\nn_est_lim = 1000\ngbc = GradientBoostingClassifier(n_estimators = n_est_lim, max_depth = 2)\ngbc.fit(X_train, Y_train)\n", "intent": "<p>\nNow let's compare training and test error as a function of the \n<p>\n"}
{"snippet": "batch_size = 128\nz_dim = 100\nlearning_rate = 0.0002\nbeta1 = 0.3\nepochs = 2\nmnist_dataset = helper.Dataset('mnist', glob(os.path.join(data_dir, 'mnist/*.jpg')))\nwith tf.Graph().as_default():\n    train(epochs, batch_size, z_dim, learning_rate, beta1, mnist_dataset.get_batches,\n          mnist_dataset.shape, mnist_dataset.image_mode)\n", "intent": "It seems that 0.3/0.2 is a good value for beta1. What about learning rate? we will try 0.0002, 0.0004 and 0.0008 \n"}
{"snippet": "scvclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3], \n                            meta_classifier=LogisticRegression())\ncompare(scvclf)\n", "intent": "To get more information look https://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/\n"}
{"snippet": "happyModel.fit(x = X_train, y = Y_train, epochs = 40, batch_size = 16)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid=param_grid, refit=True, verbose=2)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "history = sa_model.fit(x_train, y_train, epochs=30, verbose=0, validation_split=0.33)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], minval=-1., maxval=1.))\n    embedded_sequence = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded_sequence\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "print(x_train[0].shape)\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=1000))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(2))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=40, batch_size=1000, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=32,\n          epochs=20,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "library(cluster)\nset.seed(1)\nisGoodCol <- function(col){\n   sum(is.na(col)) == 0 && is.numeric(col) \n}\ngoodCols <- sapply(nba, isGoodCol)\nclusters <- kmeans(nba[,goodCols], centers=5)\nlabels <- clusters$cluster\n", "intent": "Let's do a cluster plot. To do this, we need to remove columns which do not contain numeric values.\n"}
{"snippet": "from sklearn import svm\nclf = svm.SVC(gamma=0.001, C=100.)\n", "intent": "5b. Use the [scikit-learn SVM classifier](http://scikit-learn.org/stable/modules/svm.html\n"}
{"snippet": "sl = SuperLearner(folds=5, random_state=seed, verbose=2)\nsl.add([clf1, clf2, clf3])\nsl.add_meta(LogisticRegression())\ncompare(sl)\n", "intent": "To get more information follow the link http://ml-ensemble.com/info/start/ensembles.html\n"}
{"snippet": "def max_pool_2by2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1], padding='SAME')\n", "intent": "We are going to wrap in the same way as for the convolution\n"}
{"snippet": "conv_2_flat = tf.reshape(conv_2_pooling, [-1, 7^7^64])\nfull_layer_one = tf.nn.relu(normal_full_layer(conv_2_flat, 1024))\n", "intent": "We flatten back the matrix to a 1d vector, because now we want to use it as the input of a DNN\n"}
{"snippet": "hold_prob = tf.placeholder(tf.float32) \nfull_one_dropout = tf.nn.dropout(full_layer_one, keep_prob=hold_prob)\n", "intent": "We can create Dropout to avoid Overfitting\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlm_1 = LinearRegression()\n", "intent": "<big> Now we build the model\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1)\nlogreg.fit(features_train, target_train)\n", "intent": "---\nLet's start with a simple model from sklearn, namely `LogisticRegression`:\n"}
{"snippet": "plot_learning_curve(LinearSVC(C=10.0), \"LinearSVC(C=10.0) Features: 11&14\",\n                    X[:, [11, 14]], y, ylim=(0.8, 1.0),\n                    train_sizes=np.linspace(.05, 0.2, 5))\n", "intent": " * **decrease the number of features** (we know from our visualizations that features 11 and 14 are most informative)\n"}
{"snippet": "plot_learning_curve(LinearSVC(C=0.1), \"LinearSVC(C=0.1)\", \n                    X, y, ylim=(0.8, 1.0),\n                    train_sizes=np.linspace(.05, 0.2, 5))\n", "intent": " * **increase regularization of classifier** (decrease parameter C of Linear SVC)\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nest = GridSearchCV(LinearSVC(), \n                   param_grid={\"C\": [0.001, 0.01, 0.1, 1.0, 10.0]})\nplot_learning_curve(est, \"LinearSVC(C=AUTO)\", \n                    X, y, ylim=(0.8, 1.0),\n                    train_sizes=np.linspace(.05, 0.2, 5))\nprint \"Chosen parameter on 100 datapoints: %s\" % est.fit(X[:500], y[:500]).best_params_\n", "intent": "This already helped a bit. We can also select the regularization of the classifier automatically using a grid-search based on cross-validation:\n"}
{"snippet": "est = LinearSVC(C=0.1, penalty='l1', dual=False)\nest.fit(X[:150], y[:150])  \nprint \"Coefficients learned: %s\" % est.coef_\nprint \"Non-zero coefficients: %s\" % np.nonzero(est.coef_)[1]\n", "intent": "This also looks quite well. Let's investigate the coefficients learned:\n"}
{"snippet": "sub = Subsemble(partitions=3, random_state=seed, verbose=2, shuffle=True)\nsub.add([clf1, clf2, clf3])\nsub.add_meta(SVC())\ncompare(sub)\n", "intent": "To get more information follow the link http://ml-ensemble.com/info/start/ensembles.html\n"}
{"snippet": "from sklearn.svm import SVC\nplot_learning_curve(SVC(C=2.5, kernel=\"rbf\", gamma=1.0),\n                    \"SVC(C=2.5, kernel='rbf', gamma=1.0)\",\n                    X, y, ylim=(0.5, 1.1), \n                    train_sizes=np.linspace(.1, 1.0, 5))\n", "intent": " * **use more a complex model** (reduced regularization and/or non-linear kernel)\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm_microbiome = KMeans(n_clusters=2, random_state=123)\nkm_microbiome.fit(X_pca)\n", "intent": "We can now create a `KMeans` object with `k=2`, and fit the data with it.\n"}
{"snippet": "km_microbiome = KMeans(n_clusters=2, random_state=rng)\nkm_microbiome.fit(X_pca)\n", "intent": "We can now create a `KMeans` object with `k=2`, and fit the data with it.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded_input = tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def selu(z,\n         scale=1.0507009873554804934193349852946,\n         alpha=1.6732632423543772848170429916717):\n    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))\n", "intent": "The `tf.nn.selu()` function was added in TensorFlow 1.4. For earlier versions, you can use the following implementation:\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./my_model_final.ckpt\")\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, \"Validation accuracy:\", accuracy_val)\n    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    \n", "intent": "Actually, let's test this for real!\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./my_model_final.ckpt\")\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, \"Validation accuracy:\", accuracy_val)\n    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    \n", "intent": "And continue training:\n"}
{"snippet": "with tf.Session() as sess:\n    init.run()\n    saver.restore(sess, \"./my_model_final.ckpt\")\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, \"Validation accuracy:\", accuracy_val)\n    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")\n", "intent": "And we can train this new model:\n"}
{"snippet": "reset_graph()\nn_inputs = 28 * 28  \nn_hidden1 = 300\nn_outputs = 10\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\ny = tf.placeholder(tf.int32, shape=(None), name=\"y\")\nwith tf.name_scope(\"dnn\"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")\n", "intent": "Let's implement $\\ell_1$ regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity):\n"}
{"snippet": "be = BlendEnsemble(test_size=0.7, random_state=seed, verbose=2, shuffle=True)\nbe.add([clf1, clf2, clf3])\nbe.add_meta(LogisticRegression())\ncompare(be)\n", "intent": "To get more information follow the link http://ml-ensemble.com/info/start/ensembles.html\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "The Data API, introduced in TensorFlow 1.4, makes reading data efficiently much easier.\n"}
{"snippet": "with tf.Session() as sess:\n    try:\n        while True:\n            print(next_element.eval())\n    except tf.errors.OutOfRangeError:\n        print(\"Done\")\n", "intent": "Let's repeatedly evaluate `next_element` to go through the dataset. When there are not more elements, we get an `OutOfRangeError`:\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "The `interleave()` method is powerful but a bit tricky to grasp at first. The easiest way to understand it is to look at an example:\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "- Reset Tensorflow graph\n"}
{"snippet": "linear_regression_model = LinearRegression(iterations, normalize=True, verbose=True)\nlinear_regression_model.fit(X, y)\n", "intent": "Fitting to generated data\n"}
{"snippet": "linear_regression = linear_model.LinearRegression(normalize=True)\n", "intent": "Creating Linear Regression model with feature scaling already embedded in model\n"}
{"snippet": "linear_regression.fit(X, y)\n", "intent": "Fitting to generated data\n"}
{"snippet": "tf.reset_default_graph()\nwith tf.name_scope(\"io-scope\"):\n    inputs_ = tf.placeholder(tf.float32, [None, DATA_FEATURES], name=\"inputs\")\n    targets_ = tf.placeholder(tf.float32, [None, DATA_LABELS], name=\"targets\")\nwith tf.name_scope(\"lr-scope\"):\n    learning_rate_ = tf.placeholder(tf.float32, None, name=\"learning_rate\")\nwith tf.name_scope(\"weights-scope\"):\n    weights = tf.Variable(tf.random_normal([DATA_FEATURES, 1]), name=\"weights\")\nwith tf.name_scope(\"bias-scope\"):\n    bias = tf.Variable(tf.random_normal([DATA_LABELS]), name=\"bias\")\n", "intent": "Model implementation\n"}
{"snippet": "polynomial_regression_model = PolynomialRegression(DATA_DEGREE, iterations, normalize=True, verbose=True)\npolynomial_regression_model.fit(X, y)\n", "intent": "Fitting to generated data\n"}
{"snippet": "se = SequentialEnsemble(random_state=seed, shuffle=True)\nse.add('blend',\n             [clf1, clf2])\nse.add('stack', [clf1, clf3])\nse.add_meta(SVC())\ncompare(se)\n", "intent": "To get more information follow the link http://ml-ensemble.com/info/start/ensembles.html\n"}
{"snippet": "regression.fit(X_pow, y)\n", "intent": "Fitting to generated data\n"}
{"snippet": "linear_regression_model = RidgeRegression(iterations, normalize=True, verbose=True)\nlinear_regression_model.fit(X, y)\n", "intent": "L2 Regularization set to 0 - reducing RidgeRegression to LinearRegression\n"}
{"snippet": "ridge_regression_model = RidgeRegression(iterations, regularization_factor=10,normalize=True, verbose=True)\nridge_regression_model.fit(X, y)\n", "intent": "L2 Regularization set to 10 - using RidgeRegression\n"}
{"snippet": "model2, history2, score2 = build_and_train_model(\n    X_train_input, y_train_input, X_val_input, y_val_input, initializer=RandomUniform(minval=0.0, maxval=1.0, seed=RANDOM_SEED))\nmodel3, history3, score3 = build_and_train_model(\n    X_train_input, y_train_input, X_val_input, y_val_input, initializer=RandomUniform(minval=-1.0, maxval=1.0, seed=RANDOM_SEED))\n", "intent": "Przetestujmy zakresy: niesymetryczny [0.0, 1.0) oraz symetryczny [-1.0, 1.0).\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators=100, random_state=17)\nrf.fit(X_train, y_train) \n", "intent": "Train a random forest **(RandomForestClassifier)**. Set the number of trees to 100 and use **random_state = 17**.\n"}
{"snippet": "forest_params = {'max_depth': range(10, 21),\n                'max_features': range(5, 105, 20)}\nlocally_best_forest = GridSearchCV(rf, forest_params, n_jobs=-1) \nlocally_best_forest.fit(X_train, y_train) \n", "intent": "Train a random forest **(RandomForestClassifier)**. Tune the maximum depth and maximum number of features for each tree using **GridSearchCV**. \n"}
{"snippet": "tree_params = {'max_depth': range(2,11)}\nlocally_best_tree = GridSearchCV(DecisionTreeClassifier(random_state=17),\n                                 tree_params, cv=5)                  \nlocally_best_tree.fit(X_train, y_train)\n", "intent": "Train a decision tree **(DecisionTreeClassifier, random_state = 17).** Find optimal maximum depth using 5-fold cross-validation **(GridSearchCV)**.\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators=100, random_state = 17)\nrf.fit(X_train, y_train)\n", "intent": "Train a random forest **(RandomForestClassifier)**, set number of trees to 100 and **random_state = 17**.\n"}
{"snippet": "forest_params = {'max_depth': range(10, 21),\n                'max_features': range(5, 105, 20)}\nlocally_best_forest = GridSearchCV(RandomForestClassifier(random_state=17,\n                                                         n_jobs=-1),\n                                 forest_params, cv=3,\n                                  verbose=1)\nlocally_best_forest.fit(X_train, y_train)\n", "intent": "Train a random forest **(RandomForestClassifier)**. Tune maximum depth and maximum number of features for each tree using **GridSearchCV**. \n"}
{"snippet": "ridge.fit(X_train, y_train);\n", "intent": "Finally, train the model on the full accessible training set, make predictions for the test set and form a submission file. \n"}
{"snippet": "ridge = Ridge(alpha=1.0)\nridge.fit(X_train_sparse, y_train);\n", "intent": "**Train the same Ridge with all available data, make predictions for the test set and form a submission file.**\n"}
{"snippet": "x = torch.tensor([5.5, 3])\nprint(x)\n", "intent": "Construct a tensor directly from data:\n"}
{"snippet": "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=25)\n", "intent": "It should take around 15-25 min on CPU. On GPU though, it takes less than a\nminute.\n"}
{"snippet": "conv2 = tf.layers.conv2d(inputs=pool1 , filters=64, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\npool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n", "intent": "** Create the next convolutional and pooling layers.  The last two dimensions of the convo_2 layer should be 32,64 **\n"}
{"snippet": "logits = tf.layers.dense(inputs=dropout, units=10)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    parameter = tf.Variable(tf.random_uniform(shape=[vocab_size,embed_dim],minval=-1,maxval=1),dtype=tf.float32)\n    return tf.nn.embedding_lookup(params=parameter,ids=input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_dim=1000))\nmodel.add(Dropout(.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "hist = model.fit(x_train, y_train, epochs=10, batch_size=50, validation_data=(x_test, y_test), verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100,criter)\nrfc.fit(X_train, y_train)\n", "intent": "Now let's compare the decision tree model to a random forest.\n"}
{"snippet": "g = Graph()\n", "intent": "$$ z = Ax + b $$\nWith A=10 and b=1\n$$ z = 10x + 1 $$\nJust need a placeholder for x and then once x is filled in we can solve it!\n"}
{"snippet": "dt = DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "km.fit(df.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "scaler.fit(df.drop(['TARGET CLASS'], axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "nb = MultinomialNB().fit(X_train, y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "grid_search = GridSearchCV(estimator=SVC(), param_grid=param_grid, verbose=3)\ngrid_search.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n    result = sess.run(negMatrix)\n", "intent": "3. See where each operation is mapped to:\n"}
{"snippet": "W = tf.Variable(tf.zeros([4, 3]), name=\"weights\")\nb = tf.Variable(tf.zeros([3], name=\"bias\"))\n", "intent": "The variable initialization code:\n"}
{"snippet": "def inference(X):\n    return tf.nn.softmax(combine_inputs(X))\n", "intent": "Tensorflow contains an embedded implementation of the softmax function:\n"}
{"snippet": "def sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n", "intent": "Defining the sigmoid function:\n"}
{"snippet": "g = Graph()\ng.set_as_default()\nA = Variable([[10,20],[30,40]])\nb = Variable([1,1])\nx = Placeholder()\ny = matmul(A,x)\nz = add(y,b)\n", "intent": "** Looks like we did it! **\n"}
{"snippet": "X = df_iris[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\ny = iris.target\nclf = tree.DecisionTreeClassifier()\nclf.fit(X, y)\n", "intent": "- Check out the code above for an example\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\ny=df['y'].values\nX=df['x'].values\nlr = LinearRegression()\nlr.fit(df[['x']], df[['y']])\n", "intent": "+ Simpson's paradox! \n"}
{"snippet": "from sklearn.linear_model import LinearRegression, Ridge\nrid = Ridge(normalize=True)\nrid.fit(X_train, y_train)\n", "intent": "+ What model you select is up to you\n+ check out sklearn documentation!\n"}
{"snippet": "def fully_connected(prev_layer, num_units):\n    layer = tf.layer.dense(prev_layer, num_units, activation=tf.nn.relu)\n    return layer\n", "intent": "def fully_connected(prev_layer, num_units):\n    layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n    return layer\n"}
{"snippet": "param_grid = [\n        {'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n         'feature_selection__k': list(range(1, len(feature_importances) + 1))}\n]\ngrid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n                                scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\ngrid_search_prep.fit(housing, housing_labels)\n", "intent": "Question: Automatically explore some preparation options using `GridSearchCV`.\n"}
{"snippet": "config = tf.ConfigProto(intra_op_parallelism_threads=1,\n                        inter_op_parallelism_threads=1)\nwith tf.Session(config=config) as sess:\n    pass\n", "intent": "You should make sure TensorFlow runs your ops on a single thread:\n"}
{"snippet": "tf.reset_default_graph()\ntf.set_random_seed(42)\nrnd = tf.random_uniform(shape=[])\nwith tf.Session() as sess:\n    print(rnd.eval())\n    print(rnd.eval())\nprint()\nwith tf.Session() as sess:\n    print(rnd.eval())\n    print(rnd.eval())\n", "intent": "Every time you reset the graph, you need to set the seed again:\n"}
{"snippet": "GB = nx.Graph()\nGB.add_nodes_from(df_all.asin.values)\n", "intent": "Build the graph as we did above.\n"}
{"snippet": "GB = nx.Graph()\nGB.add_nodes_from(df_all_new.asin.values)\nb_len_list = [] \nr_len_list = [] \nfor i in range(len(df_all_new)):\n    for j in range(i+1, len(df_all_new)):\n        b_len_list.append(len(set(df_all_new.iloc[i]['also_bought'])&set(df_all_new.iloc[j]['also_bought'])))\n        r_len_list.append(len(set(df_review_chosen.iloc[i]['reviewerID'])&set(df_review_chosen.iloc[j]['reviewerID'])))\n", "intent": "Build the graph as we did above.\n"}
{"snippet": "print(tf.get_default_graph())\n", "intent": "___\nWhen you start TF, a default Graph is created, you can create additional graphs easily:\n"}
{"snippet": "compute_kmeans(pp_G, 'conference', 2, nx.to_numpy_matrix(pp_G),np.linspace(100, 1e4, 100, dtype=int))\n", "intent": "Number of clusters : 2 (EASTERN and WESTERN Conferences)\n"}
{"snippet": "_ , dbscan_labels = apply_dbscan(nx.to_numpy_matrix(pp_G), 10)\nset(dbscan_labels) \n", "intent": "K-means technique gives results slightly better then random labelling.\n"}
{"snippet": "compute_kmeans(pp_G, 'division', 6, nx.to_numpy_matrix(pp_G), np.linspace(10, 1e4, 100, dtype=int))\n", "intent": "Number of clusters : 6\n"}
{"snippet": "compute_kmeans(pp_G, 'team', 30, nx.to_numpy_matrix(pp_G), np.linspace(10, 1e4, 100, dtype=int))\n", "intent": "Number of clusters : 30\n"}
{"snippet": "def create_team_graph(team_name, full_graph, node_color, show = True):\n", "intent": "Create individual team graphs in order to build the perfect graph:\n"}
{"snippet": "pygsp_text = utils.load_text(\"books/pygsp.txt\")\noccs_pygsp, words_map_pygsp = graph.text_to_graph(pygsp_text, undirected=True, ignore_punct=True, ignore_stopwords=True, self_links=False, nlinks=nlinks, return_words_map=True)\nwords_map_inv_pygsp = utils.inverse_dict(words_map_pygsp)\nG_pygsp = graph.np_to_nx(occs_pygsp, words_map_pygsp)\npos_pygsp, partition_pygsp, betweenness_scaled_pygsp = graph.communities(G_pygsp, draw=True, cmap=cmap)\n", "intent": "We have decided to analyze the description of your (our ?) favorite library. What do you think of the result ?\n"}
{"snippet": "Ggenre = graphs.Graph(GenreW)\nGgenre.compute_laplacian('normalized')\n", "intent": "Computation of the normalized Laplacian of this weighted graph.\n"}
{"snippet": "GText = graphs.Graph(NormTextW)\nGText.compute_laplacian('normalized')\n", "intent": "Computation of the normalized Laplacian of the graph.\n"}
{"snippet": "G = graphs.Graph(WTot)\nG.compute_laplacian('normalized')\n", "intent": "Let us first compute the normalized Laplacian of the graph and its eigenvalues:\n"}
{"snippet": "graph_one = tf.get_default_graph()\ngraph_two = tf.Graph()\n", "intent": "Setting a graph as the default:\n"}
{"snippet": "logreg = linear_model.LogisticRegression(solver ='liblinear', class_weight ='balanced')\nlogreg.fit(features_mat_perso, label)\npos_important_fts  = []\nneg_important_fts = []\nfor i in range(0, len(kept_countries)):\n    pos_important_fts.append(user_sampled_data.columns[logreg.coef_[i].argsort()[-2:][::-1]])\n    neg_important_fts.append(user_sampled_data.columns[logreg.coef_[i].argsort()[:2]])\n", "intent": "Recall that a 'personnal info' feature is 'high' if the user strongly agrees with the statement. It is low if the user disagrees with the statement. \n"}
{"snippet": "laplacian = np.diag(degrees) - adj.todense()\nlaplacian = sparse.csr_matrix(laplacian)\n", "intent": "We define the graph laplacian using the formula $L = D- A$ where $D$ is the diagonal matrix containing the degrees and $A$ is the adjacency matrix.\n"}
{"snippet": "class NeuralNetwork(torch.nn.Sequential):\n    def __init__(self, hidden_layer_size):\n        self.name = 'NN'\n        self.num_classes = 2\n        super().__init__(\n            torch.nn.Linear(2, hidden_layer_size),\n            torch.nn.ReLU(),\n            torch.nn.Linear(hidden_layer_size, self.num_classes),\n        )\n    def init_params(self, train_data):\n", "intent": "We will create a simple 2 layer neural network using the default functions provided by PyTorch\n"}
{"snippet": "model_sk.fit(X_train, y_train)\n", "intent": "Fitting model on prepared data\n"}
{"snippet": "knn_sk = KNeighborsClassifier(n_jobs=-1)\n", "intent": "Let's use sklearn KNN for parameter selection\n"}
{"snippet": "model_knn_weighted_sklearn = KNeighborsClassifier(n_jobs=-1, weights=\"distance\", p=1, n_neighbors=3)\n", "intent": "In this section you should not code anything again. Just see.\n"}
{"snippet": "model_lr = LogisticRegression(C = 1e8, \n                              max_iter=200, \n                              solver=\"sag\", \n                              multi_class=\"multinomial\")\n", "intent": "Let's take Sklearn implementation of logistic ragression and build a baseline model\n"}
{"snippet": "plot_model(dummy_predictor(), X_train, y_train, (X_valid, y_valid))\n", "intent": "Plot the decision surface\n"}
{"snippet": "model_mlp = MLPHomegrownSoftmax(layer_config=[2, 100, 3], random_state=21)\n", "intent": "Now let's check our implementation.\nFirst of all, create a model:\n"}
{"snippet": "def init_weights(shape):\n    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(init_random_dist)\n", "intent": "Function to help intialize random weights for fully connected or convolutional layers, we leave the shape attribute as a parameter for this.\n"}
{"snippet": "model_sk = LinearRegression()\n", "intent": "Here we use Scikit-learn Linear Regression model as a baseline.\n"}
{"snippet": "model_2l_ext = MLPHomegrownMSEMulti(layer_config=[13, 13, 10 , 1], random_state=21, verbose=False)\nmodel_2l_ext.fit(X_train, y_train[:, None], max_iter=40000, alpha=0.006)\n", "intent": "As one can notice this 2-layer configuration worked worse than 1-layer. Let's try to investigate the architectures further.\n"}
{"snippet": "model_2l_regl2 = MLPHomegrownMSEMultiReg(layer_config=[13, 100, 30 , 1], random_state=21, verbose=False, \n                                         l2_reg=0.05)\nmodel_2l_regl2.fit(X_train, y_train[:, None], max_iter=40000, alpha=0.005)\n", "intent": "Let's try the \"deep\" 100-30 architecture with L2 regularization which was prone to overfitting the most.\n"}
{"snippet": "model_mlp_multi_regl2 = MLPHomegrownSoftmaxMultiReg(layer_config=[2, 40, 40, 40, 3], \n                                                    random_state=21, verbose=True, \n                                                    l2_reg=0.00005)\nmodel_mlp_multi_regl2.fit(*get_data(random_state=21), max_iter=10000, alpha=5e-1)\n", "intent": "Create model with the same configuration as before (for Spirals) and train it using L2 regularization\n"}
{"snippet": "plot_model(model_mlp_multi_regl2, *get_data(random_state=21), get_data(n_samples=50, random_state=12))\n", "intent": "One can notice that all the weight matrices have smaller L2 norm and thus hopefully less prone to the overfitting.\n"}
{"snippet": "model_2l_regl1 = MLPHomegrownMSEMultiReg(layer_config=[13, 100, 30 , 1], random_state=21, verbose=False, \n                                         l1_reg=0.01)\nmodel_2l_regl1.fit(X_train, y_train[:, None], max_iter=40000, alpha=0.005)\n", "intent": "Let's try the previous Boston architecture with L1 regularization\n"}
{"snippet": "model_mlp_regl1 = MLPHomegrownSoftmaxMultiReg(layer_config=[2, 100, 3], \n                                                    random_state=21, verbose=True, \n                                                    l1_reg=0.0001)\nmodel_mlp_regl1.fit(*get_data(random_state=21), max_iter=20000, alpha=5e-1)\n", "intent": "Create model with the 1-layer configuration as before (for Spirals) and train it using L1 regularization\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\nprint(net)\n", "intent": " Define a **logistic regression** for debugging.\n"}
{"snippet": "net = ModelTorch()\nnet.add(Linear(X.shape[1], 64))\nnet.add(ELU())\nnet.add(Linear(64, y.shape[1]))\nnet.add(SoftMax())\nnet.compile(ClassNLLCriterion(), nesterov, {\"learning_rate\": 0.05, \"momentum\": 0.6})\nnp.random.seed(21)\nhist_no_BN = net.fit(X, y, iteration_hist=True, epochs=5)\n", "intent": "No Batch Normalization\n"}
{"snippet": "def init_bias(shape):\n    init_bias_vals = tf.constant(0.1, shape=shape)\n    return tf.Variable(init_bias_vals)\n", "intent": "Same as init_weights, but for the biases\n"}
{"snippet": "classification_layers = [\n    Dense(units=5, activation=\"softmax\")\n]\n", "intent": "And only one layer as a classifier\n"}
{"snippet": "model = load_model(filepath=\"chkpt\")\n", "intent": "Load the best model\n"}
{"snippet": "model = load_model(filepath=\"chkpt\", compile=False)\n", "intent": "Load previously trained model\n"}
{"snippet": "sess = K.get_session()\nfor i in range(extractor_len):\n    model.layers[i].trainable = False\nfor i in range(extractor_len, len(model.layers)):\n    l = model.layers[i]\n    new_weights = l.kernel_initializer(l.get_weights()[0].shape).eval(session = sess)\n    new_bias = l.bias_initializer(l.get_weights()[1].shape).eval(session = sess)\n    l.set_weights([new_weights, new_bias])\n", "intent": "Freeze all the layers of the feature extractor part and reinitialize layers of classification part\n"}
{"snippet": "hist = model.fit(X_train_g5_reduced, y_train_g5_reduced, \n                 batch_size=16, \n                 epochs=150, \n                 verbose=0, \n                 validation_data=(X_valid_g5, y_valid_g5))\n", "intent": "And finally fine-tune model only on small amount of data\n"}
{"snippet": "model_cached = load_model(filepath=\"chkpt\", compile=True)\n", "intent": "Load previously trained model\n"}
{"snippet": "feature_extractor_model = Model(inputs=model_cached.input,\n                                outputs=model_cached.layers[extractor_len - 1].output)\n", "intent": "Create a model that takes an output from the intermediate layer\n"}
{"snippet": "model_classification = Sequential([Dense(units=5, \n                                         activation=\"softmax\", \n                                         input_shape=(X_train_g5_reduced_features.shape[1], ))])\nmodel_classification.compile(loss=\"categorical_crossentropy\",\n                             optimizer=\"adam\",\n                             metrics=[\"accuracy\"])\n", "intent": "Build and compile a new model with only classification layers\n"}
{"snippet": "classification_layer = Dense(units=10, \n                             activation=\"softmax\")(dnn_a)\n", "intent": "Now let's build simple softmax classification layer on top of the DNN A branch.\n"}
{"snippet": "def convolutional_layer(input_x, shape):\n    W = init_weights(shape)\n    b = init_bias([shape[3]])\n    return tf.nn.relu(conv2d(input_x, W) + b)\n", "intent": "Using the conv2d function, we'll return an actual convolutional layer here that uses an ReLu activation.\n"}
{"snippet": "model_sk = LinearRegression()\n", "intent": "Here we use very simple Linear Regression model.\n"}
{"snippet": "num_plot = 50 \nplot_embedding(np.squeeze(X_train[:num_plot]), emb[:num_plot], y_train[:num_plot])\n", "intent": "To make the plot, we input the image data itself, the embedding data we fit with tSNE and class labels for the the data points\n"}
{"snippet": "num_plot = 500 \nplot_embedding(np.squeeze(X_train[:num_plot]), emb[:num_plot], y_train[:num_plot])\n", "intent": "To make the plot, we input the image data itself, the embedding data we fit with tSNE, and class labels for the the data points\n"}
{"snippet": "output = Dense(n_classes, activation=\"softmax\")(averaged)\n", "intent": "Multinomial Logistic Regression classifier on top of averaged vectors\n"}
{"snippet": "model_lin_no_mask = Model(inputs=[sequence_input], outputs=[output])\n", "intent": "Build and compile model\n"}
{"snippet": "model_lin_masked = Model(inputs=[sequence_input], outputs=[output])\n", "intent": "Build and compile model\n"}
{"snippet": "start_lin_masked = time.time()\ntrace = trace_callback()\nhistory_lin_masked = model_lin_masked.fit(X_train, y_train,\n                                          epochs=100, \n                                          batch_size=32,\n                                          validation_data=(X_valid, y_valid), \n                                          verbose=0, \n                                          callbacks=[trace])\nend_lin_masked = time.time()\n", "intent": "Train model. Here we use simple callback (defined before) to trace how many epochs have been done (default output is too long for 100 epochs)\n"}
{"snippet": "fc = Dense(64, activation=\"relu\")(averaged)\noutput = Dense(46, activation=\"softmax\")(fc)\n", "intent": "Additional FC layers + Softmax Classifier\n"}
{"snippet": "model_fc = Model(inputs=[sequence_input], outputs=[output])\n", "intent": "Build and compile model\n"}
{"snippet": "full_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "embedded_sequences = Embedding(input_dim=number_of_words + 1,\n                               output_dim=EMBEDDING_DIM,\n                               input_length=MAX_SEQUENCE_LENGTH,\n                               trainable=True,\n                               mask_zero=True, \n                               name=\"Embedding\")(sequence_input)\n", "intent": "Embed each word of input sentence. Here we do not want to use pre-trained embedding. Now we do want to use masking ($\\text{mask_zero=True}$)\n"}
{"snippet": "model_no_emb = Model(inputs=[sequence_input], outputs=[output])\n", "intent": "Build and compile model\n"}
{"snippet": "start_no_emb = time.time()\ntrace = trace_callback()\nhistory_no_emb = model_no_emb.fit(X_train, y_train,\n                                 epochs=100, \n                                 batch_size=32,\n                                 validation_data=(X_valid, y_valid), \n                                 verbose=0, \n                                 callbacks=[trace])\nend_no_emb = time.time()\n", "intent": "Train model. Here we use simple callback (defined before) to trace how many epochs have been done (default output is too long for 100 epochs)\n"}
{"snippet": "model_dense = Sequential()\n", "intent": "First of all, let's build MLP model and see how it performs\n"}
{"snippet": "model_cnn = Sequential()\n", "intent": "Now it's time to build the model step-by-step\n"}
{"snippet": "model_cnn.add(Convolution2D(filters=filters, \n                            kernel_size=kernel_size,\n                            padding=\"valid\"))\nmodel_cnn.add(Activation('relu'))\n", "intent": "Let's stack one more Convolution layer on top of that:\n"}
{"snippet": "model_cnn.add(Dropout(0.5))\nmodel_cnn.add(Dense(128, activation=\"relu\"))\nmodel_cnn.add(Dropout(0.5))\nmodel_cnn.add(Dense(nb_classes, activation=\"softmax\"))\n", "intent": "Now let's add FC part with the [dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) to avoid overfitting.\n"}
{"snippet": "model_dense = Sequential()\n", "intent": "First of all, let\"s build MLP model and see how it performs\n"}
{"snippet": "hid = Dense(units=32, activation=\"relu\")(hid)\nhid = Dropout(rate=0.5)(hid)\nout = Dense(nb_classes, activation=\"softmax\")(hid)\n", "intent": "Let\"s stack fully-conected classifier on top of that:\n"}
{"snippet": "cell = tf.contrib.rnn.OutputProjectionWrapper(\n    tf.contrib.rnn.BasicRNNCell(num_units=num_neurons, activation=tf.nn.relu),\n    output_size=num_outputs)\n", "intent": "____\n____\nPlay around with the various cells in this section, compare how they perform against each other.\n"}
{"snippet": "cnn = Convolution2D(filters=32, \n                    kernel_size=(3, 3),\n                    padding=\"valid\", \n                    activation=\"relu\")(cnn)\ncnn = Convolution2D(filters=32, \n                    kernel_size=(3, 3),\n                    padding=\"valid\", \n                    activation=\"relu\")(cnn)\ncnn = MaxPooling2D(pool_size=(2, 2))(cnn)\n", "intent": "Let\"s apply the same block one more time to further reduce dimensionality\n"}
{"snippet": "rnn = Bidirectional(LSTM(units=8, return_sequences=True), merge_mode=\"concat\")(cnn)\nrnn = LSTM(units=16, return_sequences=False)(rnn)\n", "intent": "Now let\"s come back to our pixelwise LSTM idea\n"}
{"snippet": "hid = Dense(units=32, activation=\"relu\")(rnn)\nhid = Dropout(rate=0.5)(hid)\nout = Dense(nb_classes, activation=\"softmax\")(hid)\n", "intent": "Let\"s stack fully-conected classifier on top of that:\n"}
{"snippet": "out = TimeDistributed(Dense(units=len(tags_vocabulary) + 1, \n                            activation='softmax'), \n                      name=\"Softmax\")(dense)\n", "intent": "And the final layer is usual softmax $\\text{Dense}$ layer (with the same $\\text{TimeDistributed}$ wrapper)\n"}
{"snippet": "model = Model(inputs=[sequence_input], \n              outputs=[out])\n", "intent": "Let's finally build all the pieces together and create a Keras model\n"}
{"snippet": "hist = model.fit(X_train, y_train, \n                 validation_data=(X_test, y_test), \n                 batch_size=25, epochs=20, verbose=1, \n                 class_weight={0: np.sqrt(ironic_num) * 1. / (np.sqrt(ironic_num) + np.sqrt(regular_num)), \n                               1: np.sqrt(regular_num) * 1. / (np.sqrt(ironic_num) + np.sqrt(regular_num))})\n", "intent": "Due to unbalanced classes in our problem we use the same reweighting technique as we did in CNN.\n"}
{"snippet": "cnn = Convolution2D(filters=setting.filters, \n                    kernel_size=setting.kernel_size, \n                    padding=\"same\",\n                    activation=\"relu\", \n                    name=\"conv2\")(cnn)\ncnn = MaxPooling2D(pool_size=setting.pool_size_2, \n                   name=\"pool2\")(cnn)\n", "intent": "Let's apply the same block again to make our model deeper and more representative\n"}
{"snippet": "Model(inputs=[input_data], outputs=[out]).summary()\n", "intent": "That's the main part of our model. Let's compile it and see the summary\n"}
{"snippet": "model = Model(inputs=[input_data, input_labels, input_length, label_length], outputs=[loss_out])\n", "intent": "Now we can define our final model\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./rnn_time_series_model\")\n    zero_seq_seed = [0. for i in range(num_time_steps)]\n    for iteration in range(len(ts_data.x_data) - num_time_steps):\n        X_batch = np.array(zero_seq_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n        y_pred = sess.run(outputs, feed_dict={X: X_batch})\n        zero_seq_seed.append(y_pred[0, -1, 0])\n", "intent": "** Note: Can give wacky results sometimes, like exponential growth**\n"}
{"snippet": "sess = tf.Session()\nsess.run(init)\n", "intent": "Create TF session in which we're going to do all the stuff and run initialization process\n"}
{"snippet": "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n", "intent": "Now it's time to create operator that will write summary logs.\n"}
{"snippet": "model = Model(inputs=[q_input, a_input], outputs=[loss_layer])\n", "intent": "Now we are ready to build a model\n"}
{"snippet": "nn.fit(a_train_encoded)\n", "intent": "Fit nearest neighbors model on encoded train answers\n"}
{"snippet": "h = activation(torch.mm(features, W1) + B1)\ny = activation(torch.mm(h, W2) + B2)\ny\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "test_rnn = RNN(input_size=1, output_size=1, hidden_dim=10, n_layers=2)\ntime_steps = np.linspace(0, np.pi, seq_length)\ndata = np.sin(time_steps)\ndata.resize((seq_length, 1))\ntest_input = torch.Tensor(data).unsqueeze(0) \nprint('Input size: ', test_input.size())\ntest_out, test_h = test_rnn(test_input, None)\nprint('Output size: ', test_out.size())\nprint('Hidden state size: ', test_h.size())\n", "intent": "As a check that your model is working as expected, test out how it responds to input data.\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, epochs=40, batch_size=64)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model = Sequential()\n", "intent": "- define model\n- compile model\n- fit model\n"}
{"snippet": "def create_model(learn_rate = learning_rate):\n    model = Sequential()\n    model.add(Dense(output_dim , input_dim = input_dim, kernel_initializer='normal')) \n    optimizer = Adam(lr=learn_rate)\n    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n    return model\n", "intent": "(We will just pick Adam for this run.)\n"}
{"snippet": "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n", "intent": "** Now pass in the cells variable into tf.nn.dynamic_rnn, along with your first placeholder (X)**\n"}
{"snippet": "x = Input(batch_shape=(batch_size, original_dim)) \nh = Dense(intermediate_dim, activation='relu')(x) \nz_mean = Dense(latent_dim)(h)                     \nz_log_var = Dense(latent_dim)(h)                  \n", "intent": "First, an encoder network turns the input samples x into two parameters in a latent space, which we will note z_mean and z_log_sigma.\n"}
{"snippet": "decoder_h = Dense(intermediate_dim, activation='relu')  \nh_decoded = decoder_h(z)\ndecoder_mean = Dense(original_dim, activation='sigmoid') \nx_decoded_mean = decoder_mean(h_decoded)\nh_decoded.shape, x_decoded_mean.shape\n", "intent": "Finally, a decoder network maps these latent space points back to the original input data.\n"}
{"snippet": "g = Graph()\ng.set_as_default()\n", "intent": "$$ z = Ax + b $$\n- A = 10\n- b = 1\n- Evaluate x = 10\n"}
{"snippet": "g = Graph()\ng.set_as_default()\n", "intent": "$$ z = Ax + b $$\n- A = \n\\begin{bmatrix}\n    10 & 20 \\\\\n    30 & 40\n\\end{bmatrix}\n- b = \n\\begin{bmatrix}\n    1 & 1\n\\end{bmatrix}\n- Evaluate x = 10\n"}
{"snippet": "g = Graph()\ng.set_as_default()\n", "intent": "$$z=w^Tx+b$$\n$$a=sigmoid(z)$$\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "```python\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\n```\n"}
{"snippet": "num_neurons = 3\nX_t0_ph = tf.placeholder(tf.float32,[None,2])\nX_t1_ph = tf.placeholder(tf.float32,[None,2])\nWx = tf.Variable(tf.random_normal(shape=[2,num_neurons]))\nWy = tf.Variable(tf.random_normal(shape=[num_neurons,num_neurons]))\nb = tf.Variable(tf.zeros([1,num_neurons]))\ny_t0_pred = tf.tanh(tf.matmul(X_t0_ph,Wx) + b)\ny_t1_pred = tf.tanh(tf.matmul(y_t0_pred,Wy) + tf.matmul(X_t1_ph,Wx) + b)\n", "intent": "$$y_t = tanh(W_x^Tx_t + b)$$\n$$y_{t+1}=tanh(W_y^Ty_t + W_x^Tx_{t+1} + b)$$\n"}
{"snippet": "x_t = tf.placeholder(tf.int32, (1,))\nh_t = tf.Variable(np.zeros([1, rnn_num_units], np.float32))  \nnext_probs, next_h = rnn_one_step(x_t, h_t)\n", "intent": "Once we've trained our network a bit, let's get to actually generating stuff. All we need is the `rnn_one_step` function you have written above.\n"}
{"snippet": "model = LinearRegression(normalize=True)\nprint model.normalize\n", "intent": "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated, and have suitable default values:\n"}
{"snippet": "regressor.fit(X['train'], y['train'], monitors=[validation_monitor], logdir=LOG_DIR)\n", "intent": "- fit: fitting using training data\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\ngrid.fit(X_train_scaled, y_train)\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\nprint(\"Best set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\nprint(\"Best parameters: \", grid.best_params_)\n", "intent": "* Find the best parameters for *SVC* using *GridSearch* $\\to$ Just add the grid parameters.\n"}
{"snippet": "param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n               'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\nprint(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(grid.best_params_))\n", "intent": "Pipelines and grid search are combined in the same way that any estimator.\n"}
{"snippet": "fc1 = tf.layers.dense(images, HIDDEN1_SIZE, activation=tf.nn.relu, name=\"fc1\")\nfc2 = tf.layers.dense(fc1, HIDDEN2_SIZE, activation=tf.nn.relu, name=\"fc2\")\ndropped = tf.nn.dropout(fc2, keep_prob=0.9, name=\"dropout1\")\ny = tf.layers.dense(dropped, NUM_CLASSES, name=\"output\")\n", "intent": "Up until this point, everything is effectively identical to authoring the deep neural networks with the low-level APIs. But from here on, we diverge.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogistic = LogisticRegression(C = 10000, fit_intercept=False)\nlogistic.fit(mX,mY)\n", "intent": "*Exercise: Use Sklearn to fit a logistic regression model on the gaussian mixture data.*\n"}
{"snippet": "X2 = np.vstack((np.ones((150,2)),-1*np.ones((150,2)))) + 0.3*np.random.normal(size=(300,2))\nY2 = np.hstack((np.ones(150),-1*np.ones(150)))\nfrom sklearn import svm\nsvm_instance = svm.SVC(kernel='linear')\nsvm_instance.fit(X2,Y2)\nutil.plot_svm(X2, Y2, svm_instance)\n", "intent": "*Exercise: Use Scikit.learn linear svm object to find the maximum margin hyperplane separating the dataset generated below.* \n"}
{"snippet": "X, truth = tut.load_2d_hard()\nlabels, centres = kmeans(X, 3)\ntut.plot_2d_clusters(X, labels, centres)\n", "intent": "**5)** Run K-means on the following dataset with 3 clusters and visualise the results\n"}
{"snippet": "plotKmeans(X,y)\n", "intent": "Plotting with the original labels\n"}
{"snippet": "batch_size = 128\nepochs = 2\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\n", "intent": "Now, we can fit the model.  This should take about 10-15 seconds per epoch on a commodity GPU, or about 2-3 minutes for 12 epochs.\n"}
{"snippet": "model = LinearRegression().fit(X, y)\n", "intent": "Now we can re-train the model on the entire training data\n"}
{"snippet": "lm.fit(X, bos.PRICE)\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)\n", "intent": "Train and evaluate\n^^^^^^^^^^^^^^^^^^\nIt should take around 15-25 min on CPU. On GPU though, it takes less than a\nminute.\n"}
{"snippet": "X_train_pre = np.array([grayscale(x)[:, :, np.newaxis] for x in X_train])\nX_validation_pre = np.array([grayscale(x)[:, :, np.newaxis] for x in X_validation])\nsign_pipeline(X_train_pre, y_train, X_validation_pre, y_validation, input_shape=(None, 32, 32, 1), n_classes=43, \\\n                  conv1_shape=(5, 5, 1, 6), conv2_shape=(5, 5, 6, 16), fc1_shape=(400, 120), fc2_shape=(120, 43), \\\n                  epochs=20, batch_size=128, learning_rate=0.001, save_path='checkpoint/network02.ckpt')\n", "intent": "**Apply grayscale and run the network again**\n"}
{"snippet": "X_train_pre = np.array([normalize(x, method=2) for x in X_train])\nX_validation_pre = np.array([normalize(x, method=2) for x in X_validation])\nsign_pipeline(X_train_pre, y_train, X_validation_pre, y_validation, input_shape=(None, 32, 32, 3), n_classes=43, \\\n                  conv1_shape=(5, 5, 3, 6), conv2_shape=(5, 5, 6, 16), fc1_shape=(400, 120), fc2_shape=(120, 43), \\\n                  epochs=20, batch_size=128, learning_rate=0.001, save_path='checkpoint/network03.ckpt')\n", "intent": "**Apply normalization and run the network again with more epochs**\n"}
{"snippet": "X_train_pre = np.array([augment_image(x) for x in X_train])\nsign_pipeline(X_train_pre, y_train, X_validation, y_validation, input_shape=(None, 32, 32, 3), n_classes=43, \\\n                  conv1_shape=(5, 5, 3, 6), conv2_shape=(5, 5, 6, 16), fc1_shape=(400, 120), fc2_shape=(120, 43), \\\n                  epochs=20, batch_size=128, learning_rate=0.001, save_path='checkpoint/network04.ckpt')\n", "intent": "**Apply augmentation and run the network**\n"}
{"snippet": "X_train_pre = np.array([normalize(grayscale(x), method=2)[:,:,np.newaxis] for x in X_train])\nX_validation_pre = np.array([normalize(grayscale(x), method=2)[:,:,np.newaxis] for x in X_validation])\nsign_pipeline(X_train_pre, y_train, X_validation_pre, y_validation, input_shape=(None, 32, 32, 1), n_classes=43, \\\n                  conv1_shape=(5, 5, 1, 6), conv2_shape=(5, 5, 6, 16), fc1_shape=(400, 120), fc2_shape=(120, 43), \\\n                  epochs=20, batch_size=128, learning_rate=0.001, save_path='checkpoint/network05.ckpt')\n", "intent": "**Apply both grayscale and normalization and run the network again with epochs=20**\n"}
{"snippet": "X_train_pre = np.array([normalize(grayscale(x), method=2)[:,:,np.newaxis] for x in X_train])\nX_validation_pre = np.array([normalize(grayscale(x), method=2)[:,:,np.newaxis] for x in X_validation])\nsign_pipeline(X_train_pre, y_train, X_validation_pre, y_validation, input_shape=(None, 32, 32, 1), n_classes=43, \\\n                  conv1_shape=(5, 5, 1, 12), conv2_shape=(5, 5, 12, 32), fc1_shape=(800, 240), fc2_shape=(240, 43), \\\n                  epochs=20, batch_size=128, learning_rate=0.001, save_path='checkpoint/network07.ckpt')\n", "intent": "**Apply normalization and grayscale pre-processing and double network settings**\n"}
{"snippet": "sign_pipeline(X_train_pre, y_train, X_validation_pre, y_validation, input_shape=(None, 32, 32, 1), n_classes=43, \\\n                  conv1_shape=(5, 5, 1, 12), conv2_shape=(5, 5, 12, 32), fc1_shape=(800, 240), fc2_shape=(240, 43), \\\n                  epochs=50, batch_size=128, learning_rate=0.0005, save_path='checkpoint/network08.ckpt')\n", "intent": "**Increase Number of Epochs, Descrease Learning rate with the double network settings**\n"}
{"snippet": "theta_avp,theta_hist = structure_perceptron.estimate_perceptron(training_set,\n                                                       features.word_feats,\n                                                       tagger_base.classifier_tagger,\n                                                       20,\n                                                       all_tags)\n", "intent": "The cell below takes 30 seconds to run on my laptop.\n"}
{"snippet": "theta_avp_ja,theta_hist_ja =\\\nstructure_perceptron.estimate_perceptron(training_set_ja,\n                                         features.word_feats,\n                                         tagger_base.classifier_tagger,\n                                         20,\n                                         all_tags_ja)\n", "intent": "The cell below takes approximately 40 seconds to run on my laptop.\n"}
{"snippet": "lm_no_intercept = LinearRegression(fit_intercept=False)\nlm_no_intercept.fit(X, bos.PRICE)\nprint \"Using Intercept: \",lm.score(X, bos.PRICE)\nprint \"Not using Intercept: \",lm_no_intercept.score(X, bos.PRICE)\n", "intent": "The no intercept model can be created by passing the fit_intercept option as False as shown below\n"}
{"snippet": "tagger_base.apply_tagging_model(constants.JA_TEST_FILE_HIDDEN,\n                               tagger_base.classifier_tagger,\n                               features.word_suff_feats,\n                               theta_suff_avp_ja,\n                               all_tags,\n                               'avp-words-suff-te.ja.preds')\n", "intent": "10% better on Japanese! Why might that be?\n"}
{"snippet": "theta_neighbor_avp_ja,_ =\\\nstructure_perceptron.estimate_perceptron(training_set_ja,\n                                         features.word_neighbor_feats,\n                                         tagger_base.classifier_tagger,\n                                         20,\n                                         all_tags_ja)\n", "intent": "Even better for English than the suffix features! Let's try Japanese.\nThe code below takes 60 seconds to run on my laptop.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier() \nrf.fit(features, labels) \n", "intent": "You can then create a random forest and fit it to the examples by using the following:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[10,20,10], n_classes=2)\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "kmeans.fit(cd.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "lm.fit(X, bos.PRICE)\nlm.coef_\nlm.intercept_\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "W = tf.Variable(tf.random_normal([3,3,1,32],stddev=0.01))\nW\n", "intent": "---\nTEST for understanding Convolution Network\n"}
{"snippet": "L = tf.nn.conv2d(X, W, strides=[1,1,1,1], padding='SAME')\nL\n", "intent": "* [3,3,1,32] / [3,3] filter(kernel) size / [1] input_channels / [32] output channels\n"}
{"snippet": "m_new = ols('PRICE ~ np.log(CRIM) + RM + PTRATIO + AGE + DIS', bos).fit()\nprint(m_new.summary())\n", "intent": "Adding two more variables - to the model and running the regression again,\n"}
{"snippet": "y = df[\"Hired\"]\nX = df[features]\nclf = tree.DecisionTreeClassifier()  \nclf = clf.fit(X,y)\n", "intent": "Now actually construct the decision tree:\n"}
{"snippet": "actualDemo = BernoulliNB(alpha=0.001) \nactualDemo.fit(bow_counts_train_x,np.array(train_label_y))\n", "intent": "<b>as the hyper parameter value increasing accuracy is decreasing.</b>\n"}
{"snippet": "actualDemotfidf = BernoulliNB(alpha=0.001) \nactualDemotfidf.fit(tfidf_train_x,np.array(train_label_y))\n", "intent": "<b>in starting with alpha value accuracy is very high but suddenly with the alpha increasing accuracy decreased and then become nearly stable.</b>\n"}
{"snippet": "Multinomialtfidf = MultinomialNB(alpha=0.001) \nMultinomialtfidf.fit(tfidf_train_x,np.array(train_label_y))\n", "intent": "<b>As the alpha increases accuracy decreases and after one level it became stable.</b>\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ndef Classification_model_gridsearchCV(model,param_grid,train_X,train_y, test_X, test_y):\n    clf = GridSearchCV(model,param_grid,cv=10,scoring=\"accuracy\")\n    clf.fit(train_X,train_y)\n    print(\"The best parameter found on development set is :\")\n    print(clf.best_params_)\n    print (clf.score(test_X,test_y))\n", "intent": "Use GridSearchCV to find the best parameters:\n"}
{"snippet": "lm = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=data).fit()\nprint(lm.pvalues)\n", "intent": "We could try a model with all features, and only keep features in the model if they have **small p-values**:\n"}
{"snippet": "from sklearn import linear_model\nreg = linear_model.LinearRegression()\nreg.fit (X[:,1:], y)\nprint(\"Regression coefficiants:\",reg.coef_)\nprint(\"Regression intercept:\",reg.intercept_)\n", "intent": "Just for completion, we can compare our hard coded result, to `scikit learn`'s ordinary least squares implementation of linear regression:\n"}
{"snippet": "from sklearn import linear_model\nlogreg = linear_model.LogisticRegression()\nlogreg.fit(X[:,1:], y)\nprint(\"Regression coefficiants:\",logreg.coef_)\nprint(\"Regression intercept:\",logreg.intercept_)\n", "intent": "Lets compare to `scikit`'s implementation:\n"}
{"snippet": "from sklearn import linear_model\nreg = linear_model.LinearRegression()\nreg.fit(X[:,1:], y)\nprint(\"Regression coefficiants:\",reg.coef_)\nprint(\"Regression intercept:\",reg.intercept_)\n", "intent": "Just for completion, we can compare our hard coded result, to `scikit learn`'s ordinary least squares implementation of linear regression:\n"}
{"snippet": "from nltk.classify.scikitlearn import SklearnClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nbernoulli = SklearnClassifier(BernoulliNB())\nbernoulli.train(trainData)\nprint(nltk.classify.accuracy(bernoulli, testData))\n", "intent": "Nltk also has functions that allow us to call other machine learning libraries, including scikit-learn, using wrapper classes.\n"}
{"snippet": "shared_weights = theano.shared(np.zeros(64))\ninput_X = T.matrix()\ninput_y = T.ivector()\n", "intent": "As we see, on the last iteration the train and test auc reach 1.0. Now let's try to initialize the weights to zeros.\n"}
{"snippet": "import lasagne\nimport theano\nimport theano.tensor as T\ninput_X = T.tensor4(\"X\")\ninput_shape = [None,3,32,32]\ntarget_y = T.vector(\"target Y integer\",dtype='int32')\n", "intent": "* lasagne is a library for neural network building and training\n* it's a low-level library with almost seamless integration with theano\n"}
{"snippet": "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\ndense_1 = lasagne.layers.DenseLayer(input_layer,num_units=100,\n                                   nonlinearity = lasagne.nonlinearities.sigmoid,\n                                   name = \"hidden_dense_layer\")\ndense_output = lasagne.layers.DenseLayer(dense_1,num_units = 10,\n                                        nonlinearity = lasagne.nonlinearities.softmax,\n                                        name='output')\n", "intent": "Defining network architecture\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nkfold = StratifiedKFold(n_splits=6, random_state=42, shuffle=True)\ngrid_searcher = GridSearchCV(RandomForestClassifier(\n                                random_state=42, n_estimators=20), \n                             {'criterion': ['gini', 'entropy'],\n                              'max_depth': [1, 3, 5, 7, 10, None]},\n                              cv=kfold, n_jobs=3)\ngrid_searcher.fit(X, y)\nprint 'best score:', grid_searcher.best_score_, \\\ngrid_searcher.best_params_\n", "intent": "<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], minval=-1, maxval=1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def random_forest_classifier_trainer(X,y):\n    n_estimators = [100]\n    min_samples_split = [2]\n    min_samples_leaf = [1]\n    parameters = {'n_estimators': n_estimators, 'min_samples_leaf': min_samples_leaf,\n                  'min_samples_split': min_samples_split}\n    classifier = GridSearchCV(RandomForestClassifier(verbose=1,n_jobs=-1), cv=4, param_grid=parameters)\n    classifier.fit(X, y)\n    return classifier\n", "intent": "**RandomForestClassifier:**\n"}
{"snippet": "def svm_classifier_trainer(X,y):\n    parameters = {'C': [10,15,20,25],'random_state':[2016]}\n    svc_classifier = GridSearchCV(SVC(kernel = 'rbf'), cv=4, param_grid=parameters)\n    svc_classifier.fit(X, y)\n    return svc_classifier\n", "intent": "**SVM Classifier:**\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\ndef naive_bayes_trainer(X,y):\n    naive_bayes_classifier = MultinomialNB()\n    naive_bayes_classifier.fit(X, y)\n    return naive_bayes_classifier\n", "intent": "**Naive Bayes Classifier:**\n"}
{"snippet": "classifier = RandomForestRegressor(n_estimators= 35, \n                                   max_depth= 19,\n                                   min_samples_leaf= 9,\n                                   criterion='mse', \n                                   random_state=seed)\n", "intent": "Best score=0.5309 (EI)\nExpected Improvement (EI) best parameters:\n- n_estimators= 35  \n- max_depth= 19\n- min_samples_leaf= 9 \n"}
{"snippet": "from sklearn.svm import LinearSVC\nsvm = SklearnClassifier(LinearSVC())\nsvm.train(trainData)\nprint(nltk.classify.accuracy(svm, testData))\n", "intent": "We can use any of the learning algorithms implemented by scikit-learn ([decision trees](http://scikit-learn.org/stable/modules/tree.html\n"}
{"snippet": "model.fit(X, y, epochs=200, verbose=0)\n", "intent": "We can now fit the model on the training dataset.\n"}
{"snippet": "model = Sequential()\nmodel.add(LSTM(100, activation='relu', \n               input_shape=(n_steps_in, n_features)))\nmodel.add(RepeatVector(n_steps_out))\nmodel.add(LSTM(100, activation='relu', \n               return_sequences=True))\nmodel.add(TimeDistributed(Dense(1)))\nmodel.compile(optimizer='adam', loss='mse')\n", "intent": "We define an Encoder-Decoder LSTM model for multi-step time series forecasting.\n"}
{"snippet": "model = Sequential()\nmodel.add(LSTM(100, activation='relu', \n               return_sequences=True, \n               input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(100, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer='adam', loss='mse')\n", "intent": "We definea Stacked LSTM with vector output (or an encoder-decoder model could be used).\n"}
{"snippet": "model = Sequential()\nmodel.add(LSTM(200, activation='relu', input_shape=(n_steps_in, n_features)))\nmodel.add(RepeatVector(n_steps_out))\nmodel.add(LSTM(200, activation='relu', return_sequences=True))\nmodel.add(TimeDistributed(Dense(n_features)))\nmodel.compile(optimizer='adam', loss='mse')\n", "intent": "We define a Encoder-Decoder model, but we could have used a Vector Output or Encoder-Decoder LSTM to model.\n"}
{"snippet": "visible = Input(shape=(10,))\nhidden1 = Dense(10, activation='relu')(visible) \nhidden2 = Dense(20, activation='relu')(hidden1) \nhidden3 = Dense(10, activation='relu')(hidden2) \noutput = Dense(1, activation='sigmoid')(hidden3) \nmodel = Model(inputs=visible, outputs=output)\n", "intent": "We define a Multilayer Perceptron model for binary classification.\n"}
{"snippet": "def collate(samples):\n    graphs, labels = map(list, zip(*samples))\n    batched_graph = dgl.batch(graphs)\n    return batched_graph, torch.tensor(labels)\n", "intent": "Define a function that form a mini-batch from a given list of graph and label pairs.\n"}
{"snippet": "def init_weights(shape):\n    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(init_random_dist)\n", "intent": "1. Conv2D\n2. Regular functions that come along with it.\n"}
{"snippet": "from keras.layers import Dropout, Flatten, Dense\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(n_classes))\nmodel.add(Activation('softmax'))\n", "intent": "Then we can add dropout and our fully-connected (\"Dense\") and output (softmax) layers.\n"}
{"snippet": "epochs = 10\nmodel.fit(X, y, batch_size=128, epochs=epochs)\n", "intent": "Now that we have our training data, we can start training. Keras also makes this easy:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(\n    tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "loss_func(model(xb), yb)\n", "intent": "Let's double-check that our loss has gone down:\n"}
{"snippet": "model = Mnist_Logistic()\nloss_func(model(xb), yb)\n", "intent": "We instantiate our model and calculate the loss in the same way as before:\n"}
{"snippet": "def get_model():\n    model = Mnist_Logistic()\n    return model, optim.SGD(model.parameters(), lr=lr)\n", "intent": "We'll define a little function to create our model and optimizer so we can reuse it in the future.\n"}
{"snippet": "train_dl,valid_dl = get_data(train_ds, valid_ds, bs)\nmodel,opt = get_model()\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)\n", "intent": "Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:\n"}
{"snippet": "model = nn.Sequential(\n    Lambda(preprocess),\n    nn.Conv2d(1,  16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n    nn.AvgPool2d(4),\n    Lambda(lambda x: x.view(x.size(0),-1))\n)\n", "intent": "The model created with `Sequential` is simply:\n"}
{"snippet": "model = nn.Sequential(\n    ResizeBatch(1,28,28),\n    conv2d_relu(1,  16), \n    conv2d_relu(16, 16),\n    conv2d_relu(16, 10),\n    PoolFlatten()\n)\n", "intent": "Using our newly defined layers and functions, we can instead now define the same networks as:\n"}
{"snippet": "loss_fn(model(x_valid[0:bs]), y_valid[0:bs])\n", "intent": "**Test our loss function**\nWe try out our loss function on one batch of X features and y targets to make sure it's working correctly.\n"}
{"snippet": "loss_func(model(xb), yb)\n", "intent": "Note that we no longer call `log_softmax` in the `model` function. Let's confirm that our loss is the same as before:\n"}
{"snippet": "def apply_threshold(heatmap,threshold):\n    heatmap[heatmap<= threshold] = 0\n    return heatmap\nprint('Done with applying threshold for the heatmap!')\n", "intent": "The threshold function is aiming to eliminate the false positive in case.\n"}
{"snippet": "clf=AdaBoostClassifier()\nclf.fit(x_train,y_train)\nimp_features=clf.feature_importances_\nfeature_plot(imp_features,x_test)\nclassification_metric(y_test,pred)\n", "intent": "Models with the best results are:\n1. AdaBoostClassifier\n2. GradientBoostingClassifier\n3. SVC\n"}
{"snippet": "classifier = Sequential()\n", "intent": "Definimos nuestro red neuronal\n"}
{"snippet": "classifier.add(Dense(units=6,kernel_initializer='uniform',activation='relu',input_dim=11))\nclassifier.add(Dense(units=6,kernel_initializer='uniform',activation='relu'))\n", "intent": "Creamos dos hidden layers\n"}
{"snippet": "classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n", "intent": "Creamos el output layer\n"}
{"snippet": "def build_classifier():\n    classifier = Sequential()\n    classifier.add(Dense(units=6,kernel_initializer='uniform',activation='relu',input_dim=11))\n    classifier.add(Dropout(p=0.1))\n    classifier.add(Dense(units=6,kernel_initializer='uniform',activation='relu'))\n    classifier.add(Dropout(p=0.1))\n    classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\n", "intent": "Funcion que crea la red\n"}
{"snippet": "kmeans.fit(data.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "from keras import optimizers\nopt = optimizers.RMSprop(lr=0.001)\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=1000))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, validation_split=0.2, batch_size=32, epochs=4, verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(640, input_dim=1000))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), name=\"embedding\")\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf=RandomForestClassifier()\nclf.fit(x_train,y_train)\nimp_features=clf.feature_importances_\nfeature_plot(imp_features,x_test)\nclassification_metric(y_test,pred)\n", "intent": "Models with the best results are:\n1. RandomForestClassifier\n2. DecisionTreeClassifier\n3. GradientBoostingClassifier\n"}
{"snippet": "lm1_ex1 = smf.ols(formula = 'y~x1+x2',data=ex1).fit()\nprint(lm1_ex1.params)\n", "intent": "* (c) Verify your answer by implementing the OLS regression function with python *statsmodels* module.\n"}
{"snippet": "lm2_ex2 = smf.ols(formula = 'TotalIncome~TotalPop+PrivateOnly+BothPublicAndPrivate',data=ex2).fit()\n", "intent": "**Since the p-value of PublicOnly is much higher than others,I would try to remove it.**\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: test_x,\n            labels_: test_y}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "get_predictions(index=0, \n                partition='train',\n                input_to_softmax=final_model(), \n                model_path='results/model_end.h5')\n", "intent": "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='validation',\n                input_to_softmax=final_model(), \n                model_path='results/model_end.h5')\n", "intent": "Use the next code cell to visualize the model's prediction for the first example in the validation dataset.\n"}
{"snippet": "kmeans.fit(df.drop(['Private','Unnamed: 0'],axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_prime(x):\n    return sigmoid(x) * (1-sigmoid(x))\ndef error_formula(y, output):\n    return - y*np.log(output) - (1 - y) * np.log(1-output)\ndef error_term_formula(y, output):\n    return (y - output) * output * (1 - output)\n", "intent": "The following function trains the 2-layer neural network. First, we'll write some helper functions.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(train_features_encoded, train_target)\nprint('Model fitted')\n", "intent": "As you can see, it created a new variable for each `weather` value. We can now fit our model to the encoded data using Scikit-learn estimators.\n"}
{"snippet": "scaler.fit(bank.drop('Class',axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nX = [[0, 0], [1, 1]]\nY = [0, 1]\nclf = RandomForestClassifier(n_estimators=10)\nclf = clf.fit(X, Y)\n", "intent": "Let's train a simple random forest model and deploy it in the Predictive Service\n"}
{"snippet": "import graphlab as gl\nmodel = gl.load_model('pattern_mining_model.gl')\nmodel\n", "intent": "Let's train a simple pattern mining model\n<img src=\"images/left.png\"></img>\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1000, activation=\"relu\"))\nmodel.add(Dense(2, activation='softmax'))\nmodel.summary()\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "ada=AdaBoostRegressor()\nparams_b={'base_estimator':[knn],'learning_rate':[1.0,0.9,0.8],'n_estimators':[5,10,30,50],'random_state':[50]}\nGS_b=GridSearchCV(ada, params_b);\nGS_b.fit(X_train_pca,y_train);\nprint GS_b.best_estimator_\n", "intent": "Tune the AdaBoost parameters\n"}
{"snippet": "t_knn=GS_a.best_estimator_\nstart=time()\nt_knn.fit(X_pca_pd,y_train)\nend=time()\n", "intent": "Training the tuned model\n"}
{"snippet": "ada_neigh=GS_b.best_estimator_\nstart=time()\nada_neigh.fit(X_pca_pd,y_train)\nend=time()\n", "intent": "Train the AdaBoosting tuned K-Nearest Neighbor model\n"}
{"snippet": "m = ols('PRICE ~ PTRATIO',bos).fit()\nprint(m.summary())\n", "intent": "- $F-statistic$ is 175,and not significant\n"}
{"snippet": "modules = list(model_ft.children())[:-1]\nresnet18_feature = nn.Sequential(*modules)\ninputs, classes = next(iter(dataloaders['val']))\nprint(classes)\n", "intent": "Now extract 512-features as the network output after removing the last fc layer.\n"}
{"snippet": "from keras import backend as K\ncfg = K.tf.ConfigProto()\ncfg.gpu_options.allow_growth = True\nK.set_session(K.tf.Session(config=cfg))\n", "intent": "Build a VGG19 and extract the last layer.\n"}
{"snippet": "k_means = KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = torch.nn.Linear(num_ftrs, 2)\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "intent": "Let's borrow pretrained ResNet18 from PyTorch, reset final fully connected layer. \n"}
{"snippet": "final_model = Sequential([\n        Merge([image_model, caption_model], mode='concat', concat_axis=1),\n        Bidirectional(LSTM(256, return_sequences=False)),\n        Dense(vocab_size),\n        Activation('softmax')\n    ])\n", "intent": "Merging the models and creating a softmax classifier\n"}
{"snippet": "config = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\nsess.run(tf.global_variables_initializer())\n_, _, varLoss, losses, varAcc, acc = run_model(sess,y_out,mean_loss,train_data,train_labels,1,64,100,train_step,False, xVal=eval_data, yVal=eval_labels)\n", "intent": "Train the model without batch normalization on mnist dataset, as the dataset is simple, one epoch is enough to overfit the data\n"}
{"snippet": "tf.reset_default_graph()\nX = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.int64, [None])\nis_training = tf.placeholder(tf.bool)\ny_out= cnn_model(X,y,is_training)\n", "intent": "Here we conduct experiment on a normally used way of regularization: L2 Loss on parameters. This is also known as weight decay.\n"}
{"snippet": "model_inject2.fit([X1train, X2train], ytrain, epochs=6, verbose=2, callbacks=[checkpoint], \n          validation_data=([X1test, X2test], ytest))\n", "intent": "We further retrain the model on Flickr8k dataset (on a different machine) with default ADAM optimizer, and use standard BLEU evaluation. \n"}
{"snippet": "modelCVa = LogisticRegressionCV(cv=5)\nresults5 = modelCVa.fit(X ,y)\nresults5.coef_\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n"}
{"snippet": "model = LogisticRegression()\nresults1 = model.fit(X, y)\nresults1.coef_\n", "intent": "Logistic Regression WITHOUT Cross Validation\n"}
{"snippet": "class Sigmoid(Node):\n    def __init__(self, node):\n        Node.__init__(self, [node])\n    def _sigmoid(self, x):\n        return 1. / (1. + np.exp(-x)) \n    def forward(self):\n        input_value = self.inbound_nodes[0].value\n        self.value = self._sigmoid(input_value)\n", "intent": "Here's how I implemented the sigmoid function.\n"}
{"snippet": "X, y = Input(), Input()\nW1, b1 = Input(), Input()\nW2, b2 = Input(), Input()\nl1 = Linear(X, W1, b1)\ns = Sigmoid(l1)\nl2 = Linear(s, W2, b2)\ncost = MSE(l2, y)\n", "intent": "Writing this out in MiniFlow, it would look like:\n"}
{"snippet": "k_means.fit(college.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "x = tf.nn.softmax([2.0, 1.0, 0.2])\n", "intent": "We're using TensorFlow to build neural networks and, appropriately, there's a function for calculating softmax.\n"}
{"snippet": "import tensorflow as tf\ndef run():\n    output = None\n    logit_data = [2.0, 1.0, 0.1]\n    logits = tf.placeholder(tf.float32)\n    softmax = tf.nn.softmax(logits)\n    with tf.Session() as sess:\n        output = sess.run(softmax, feed_dict={logits: logit_data})\n    return output\n", "intent": "Use the softmax function in the quiz below to return the softmax of the logits.\n"}
{"snippet": "rfr = RandomForestRegressor().fit(sensorReadings, lable)\n", "intent": "Create two classifiers used to evaluate a subset of attributes.\n"}
{"snippet": "reload(bilstm);\ntorch.manual_seed(765);\nembedding_dim=30\nhidden_dim=30\nmodel = bilstm.BiLSTM(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim)\n", "intent": "- Once, you have defined the parameters of the model: check if you have done it right using the unit test.\n"}
{"snippet": "lstm_feats = model(sentence)\nprint (lstm_feats[0][0:5])\n", "intent": "- this calls the `forward()` function on the model, which returns the tag_scores for each tag for each particular token in the sentence\n"}
{"snippet": "reload(bilstm);\ntorch.manual_seed(765);\nloss = torch.nn.CrossEntropyLoss()\nmodel, losses, accuracies = bilstm.train_model(loss, model, X_tr,Y_tr, word_to_ix, tag_to_ix, \n                                        X_dv, Y_dv, num_its=10, status_frequency=2, \n                                        optim_args = {'lr':0.1,'momentum':0}, param_file = 'best.params')\n", "intent": "- Train the model for now\n"}
{"snippet": "reload(bilstm);\ntorch.manual_seed(765);\nembedding_dim=64\nhidden_dim=30\nmodel = None\nloss = None\ndel model\ndel loss\nmodel = bilstm.BiLSTM(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim, word_embeddings)\n", "intent": "- Now, all we need to do is send in the word_embeddings when initializing your model\n"}
{"snippet": "theta_perc,theta_perc_history = perceptron.estimate_perceptron(x_tr_pruned,y_tr,20)\n", "intent": "For reference, here is the running time on a relatively modern consumer-grade machine:\n"}
{"snippet": "reload(logreg)\ntorch.manual_seed(765)\nmodel = logreg.build_bakeoff(X_tr_var,Y_tr)\nmodel.add_module('log_softmax',torch.nn.LogSoftmax(dim=1))\nloss = torch.nn.MultiMarginLoss() \nmodel,losses,accuracies = logreg.train_model(loss,model,X_tr_var,Y_tr_var,\n                                             Y_dv_var=Y_dv_var,X_dv_var = X_dv_var,\n                                             num_its=1000,status_frequency=35, optim_args = {'lr':0.1,'weight_decay':0.05},)\n", "intent": "See if these features help!\n"}
{"snippet": "fitted = scaler.fit(data.drop('TARGET CLASS',axis=1))\nfitted\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "def try_out():\n  with tf.Session() as sess:\n    fn = read_dataset(tf.estimator.ModeKeys.EVAL, \n                    {'input_path': 'data', 'batch_size': 8, 'nitems': 5668, 'nusers': 82802})\n    feats, _ = fn()\n    print(feats['input_rows'].eval())\ntry_out()\n", "intent": "This code is helpful in developing the input function. You don't need it in production.\n"}
{"snippet": "simple_model = LogisticRegression().fit(train_matrix_word_subset, train_data['sentiment'])\nsimple_model\n", "intent": "We will now build a classifier with **word_count_subset** as the feature and **sentiment** as the target. \n"}
{"snippet": "neigh = KNeighborsClassifier(n_neighbors=3)\n", "intent": "We will use kNN Classifier with a k value of 3\n"}
{"snippet": "features = ['CRS_DEP_TIME', 'DAY_OF_WEEK', \"DAY_OF_MONTH\" ]\nX = df[features]\ny = df['DEP_DEL15']\nmodel.fit(X, y)\n", "intent": "First, let's run a model with slightly different inputs\n"}
{"snippet": "from sklearn.cluster import KMeans\ncluster_model = KMeans(n_clusters = 5)\ncluster_model.fit(X)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "k = 5\nkmeans = cluster.KMeans(n_clusters=k, n_jobs=-1)\nkmeans.fit(dn1)\n", "intent": "Cluster the Data - We are going to use 5 clusters based off of the above scatterplot\n"}
{"snippet": "model = AgglomerativeClustering(linkage='ward',\n                                            n_clusters=3)\n", "intent": "Note that Agglomerative Clustering is This method is most appropriate for quantitative variables, and not binary variables.\n"}
{"snippet": "model_ft=model_ft\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model_ft = nn.DataParallel(model_ft)\nmodel_ft.to(device)\nmodel_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)\n", "intent": "Train and evaluate\n^^^^^^^^^^^^^^^^^^\nIt should take around 15-25 min on CPU. On GPU though, it takes less than a\nminute.\n"}
{"snippet": "max_depth = 7\nmodel = DecisionTreeRegressor(max_depth=max_depth)\nmodel.fit(x.reshape(-1, 1), y)\n", "intent": "We'll create a decision tree to predict the points. We won't make it that deep because we want to visualize the prediction.\n"}
{"snippet": "scaler.fit(df.drop(\"Class\", axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "X_insects = insects[['continent', 'latitude', 'sex']]\ny_insects = insects['wingsize']\ninsects_regression.fit(X_insects, y_insects)\n", "intent": "2\\. Use the `fit` method on the regression object to train the model using your data.\n"}
{"snippet": "X_wells_names = np.array(['arsenic', 'dist', 'assoc', 'educ'])\nX_wells = wells[X_wells_names]\ny_wells = wells['switch']\nwells_regression.fit(X_wells, y_wells)\n", "intent": "We can use the `fit` method of the `LogisticRegression` object to train the model using our data.\n"}
{"snippet": "standardizer.fit(X_wells)\n", "intent": "2\\. Use the `fit` method.  Behind the scenes this computes and memorizes the mean and standard deviation of all the columns in the dataset.\n"}
{"snippet": "X = np.array([[1], [2], [3], [4]])\nP = PolynomialExpansion(3)\nP.fit(X)\nP.transform(X)\n", "intent": "Let's test this out on a simple example.\n"}
{"snippet": "wells_pipeline.fit(X_wells, y_wells)\n", "intent": "This is now a pipline of considerable complexity.  Even so, using it is exactly the same as any of the simpler pipelines that we constructed earlier.\n"}
{"snippet": "lm.fit(X, bos.PRICE)\nlm.coef_\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, input_dim=1000))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(32))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(2))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, nb_epoch=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Activation\nlogreg = Sequential()\nlogreg.add(Flatten(input_shape=(28, 28, 1)))\nlogreg.add(Dense(10))\nlogreg.add(Activation('softmax'))\nlogreg.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n", "intent": "<img src=\"../data/images/logreg.png\" width=\"800\">\n<img src=\"../data/images/sigmoid.png\" width=\"400\">\n"}
{"snippet": "kmeans.fit(data.drop(\"Private\", axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "import helper_udacity\nimages, labels = next(iter(trainloader))\nimg = images[0].view(1, 784)\nwith torch.no_grad():\n    logps = model(img)\nps = torch.exp(logps)\nhelper_udacity.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "with tf.Session() as sess:\n    outs = sess.run(f)\n    sess.close()\nprint(\"outs ={}\".format(outs))\n", "intent": "Once, we have finished our graph the next step is to run the computations that it represents, to do this we must create and run a session. \n"}
{"snippet": "a =  tf.constant(5)\nprint(a.graph is g)\nprint(a.graph is tf.get_default_graph())\n", "intent": "Look that we have two graphs with different identification, also the graph _g_ is not associated to the default graph. Let's see the associations.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "from sklearn.ensemble import VotingClassifier\nvector_mach = SVC()\ndecision = DecisionTreeClassifier(max_depth=4)\nvc = VotingClassifier([('svc',vector_mach),('dt',decision)])\nover_cross(vc,canc)\n", "intent": "Next, we need to upgrade sklearn to v0.17\n"}
{"snippet": "mod = SVC(kernel='rbf')\nscores = over_cross(mod,canc)\nnp.mean(scores)\n", "intent": "**8) Train an SVM using the RBF kernel. Is this model better or worse?**\n"}
{"snippet": "bag_o_words.todense().shape\n", "intent": "Note that this is a sparse matrix! You can convert to dense matrices with `.todense()` (named for clarity, clearly)\n"}
{"snippet": "dtc = DecisionTreeClassifier(max_depth=30).fit(X_train, y_train)\ndtc.score(X_test,y_test)\n", "intent": "Let's try with a decisiontree\n"}
{"snippet": "fit_grid = gs_clf.fit(X_train,y_train)\n", "intent": "What's going on here? Note: if you have a windows machine, keep n_jobs=1\n"}
{"snippet": "scaler.fit(df.drop([\"TARGET CLASS\"], axis=1))\nscaler\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "def discrim(X, w, w2, w3, wy):    \n    h = relu(abstract_conv.conv2d(X,w,subsample=(2, 2), border_mode=(2, 2)),alpha=0.2)\n    h2 = relu(batchnorm(abstract_conv.conv2d(h,w2,subsample=(2, 2), border_mode=(2, 2))),alpha=0.2)\n    h2 = T.flatten(h2, 2)    \n    h3 = relu(batchnorm(T.dot(h2,w3)),alpha=0.2)\n    y = sigmoid(T.dot(h3,wy))\n    return y\n", "intent": "**Problem 5 (1pt)** Implement discriminator by plugging in the hidden variables and weights. Hint: if the sizes mismatched, theano will complain.\n"}
{"snippet": "def gen(Z, w,  w2, w3, wx):\n    h = relu(batchnorm(...))\n    h2 = relu(batchnorm(...))\n    h2 = h2.reshape((h2.shape[0], ngf*2, 7, 7))\n    h3 = relu(batchnorm(trconv(..., ..., output_shape=(None,None,7,7),filter_size=(5,5),subsample=(2, 2), border_mode=(2, 2))))\n    x = sigmoid(trconv(..., ..., output_shape=(None, None, 14, 14),\n                                   filter_size=(5, 5), subsample=(2, 2), border_mode=(2, 2)))\n    return x\n", "intent": "**Problem 4 (1pt)** Implement generator by plugging in the hidden variables and weights. Hint: if the sizes mismatched, theano will complain.\n"}
{"snippet": "def discrim(X, w, w2, w3, wy):    \n    h = relu(conv2d(... subsample=(2, 2), border_mode=(2, 2)),alpha=0.2)\n    h2 = relu(batchnorm(conv2d(... subsample=(2, 2), border_mode=(2, 2))),alpha=0.2)\n    h2 = T.flatten(h2, 2)    \n    h3 = relu(batchnorm(T.dot(...)),alpha=0.2)\n    y = sigmoid(T.dot(...))\n    return y\n", "intent": "**Problem 5 (1pt)** Implement discriminator by plugging in the hidden variables and weights. Hint: if the sizes mismatched, theano will complain.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def train_model(model, epochs=10):\n    model.fit(x_train, y_train, epochs=epochs, batch_size=100, verbose=0, validation_split=0.2)\n    return model\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn import grid_search\nalphas = np.logspace(-10, 10, 21)\ngs = grid_search.GridSearchCV(\n    estimator=linear_model.Ridge(),\n    param_grid={'alpha': alphas},\n    scoring='mean_squared_error')\ngs.fit(modeldata, y)\n", "intent": "**4.** Do 2 grid searches to find out the optimal alpha for Lasso and Ridge. Again calculate the MSE and R2\n"}
{"snippet": "lm = linear_model.LogisticRegression() \nY = data['admit']\nX = data[train_cols]\nmodel = lm.fit(X,Y)\nscore = model.score(X,Y)\nprint \"The score for this model is: \" + str(score) + \" out of 1\"\n", "intent": "First let's run the model using all the predictors and see what scores we get.\n"}
{"snippet": "lm2 = linear_model.LogisticRegression() \nY2 = data['admit']\nX2 = data[train_cols]\nmodel2 = lm2.fit(X2,Y2)\nscore2 = model2.score(X2,Y2)\nprint \"The score for this model is: \" + str(score2) + \" out of 1\"\n", "intent": "Let's take another look at cross-validation and other scoring in sklearn.\n"}
{"snippet": "X.todense()\n", "intent": "Convert the matrix from the vectorizer to a dense matrix, then sum by column to get the counts per term.\n"}
{"snippet": "knn.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "svc_params = {'kernel': ['rbf', 'linear'], 'gamma': [0.01, 0.001, 0.0001], 'C': [1, 10, 100]}\nclf = GridSearchCV(estimator=SVC(), param_grid=svc_params, cv=3, n_jobs=-1, \n                   scoring='accuracy', verbose=10)\nclf.fit(cnn_codes_train, y_train)\n", "intent": "Unfortunately I had to limit train data to 20% because of my computer's weak parameters.\n"}
{"snippet": "mod = Sequential()\nmod.add(Dense(32, activation='sigmoid', input_dim=x_train.shape[1]))\nmod.add(Dropout(0.2))\nmod.add(Dense(64, activation='relu'))\nmod.add(Dropout(0.2))\nmod.add(Dense(64, activation='sigmoid'))\nmod.add(Dropout(0.2))\nmod.add(Dense(num_classes, activation='sigmoid'))\nmod.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "mod.fit(x_train, y_train,\n          batch_size=32,\n          epochs=30,\n          validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "import tensorflow as tf\nhello_constant = tf.constant(\"Hello, World!\")\nint_constant   = tf.constant(1234)\narray_constant = tf.constant([[10,20], [30,40]])\nwith tf.Session() as sess:\n    output = sess.run(hello_constant)\n    print(output)\n", "intent": "Vincent Vanhoucke, Principal Scientist at Google Brain, introduces you to deep learning and Tensorflow, Google's deep learning framework.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed_input = tf.nn.embedding_lookup(embedding, input_data)\n    return embed_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "W_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n", "intent": "In order to build a deep network, we **stack several layers of this type**. The second layer will have 64 features for each 5x5 patch.\n"}
{"snippet": "W_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n", "intent": "Finally, we add a layer, just like for the one layer softmax regression above.\n"}
{"snippet": "tr = DecisionTreeRegressor()\n", "intent": "Train and examine the performance of a `DecisionTreeRegressor` and `LinearRegression` model here.  Which does better?  Visualize your results.\n"}
{"snippet": "seed = 0 \nrng = np.random.RandomState(seed)\nrng.seed(seed)\nkmeans = KMeans(n_clusters=5, random_state=rng)  \nkmeans.fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "lm.fit(X_train, y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "model.fit(x_train, y_train, epochs=25, batch_size=50, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "batch_size = 128\nn_epochs = 1\nmodel.fit(X_train, y_train,\n          batch_size=batch_size,\n          nb_epoch=n_epochs,\n          validation_data=(X_test, y_test))\n", "intent": "(try more epochs, 30secs to 1min per epoch aprox)\n"}
{"snippet": "model.set_weights(init_weights)\nh = model.fit(X_train, y_train, nb_epoch=15, batch_size=1, validation_split=.15, verbose=0)\n", "intent": "entrenamos el modelo con los datos de entrenamiento. Observe how after a few iterations we get little improvement\n"}
{"snippet": "fg_W1 = theano.function(inputs=[], outputs=g_W1, givens={tX: X,ty: y})\n", "intent": "get compiled evaluable function from symbolic gradient\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n", "intent": "Let's check random forests.\n"}
{"snippet": "lm = smf.ols(formula='sale_price ~ gross_sq_feet -1', data = REsample1).fit()\nprint(lm.summary())\n", "intent": "This is already pretty consistent with what python will give us if we take the entire training sample at once\n"}
{"snippet": "import tensorflow as tf\ntf.reset_default_graph()\n", "intent": "$$y = softmax(Wx+b)$$\n"}
{"snippet": "sess = tf.Session()\nsess.run(init)\n", "intent": "We can now launch the model in a Session, and run the operation that initializes the variables:\n"}
{"snippet": "def KMeanDict(data,nClusters):\n    kmLabels = KMeans(n_clusters=nClusters, random_state=None).fit(data).labels_\n    classDict = {label: data[label==kmLabels] for label in np.unique(kmLabels)}    \n    for i in classDict:\n        classDict[i] = np.matrix(classDict[i])\n    return classDict\n", "intent": "We will now define a helper function which will allow us to perform kMeans and return the results in a form easy to work with. \n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "y = tf.nn.softmax(tf.matmul(x,W) + b)\n", "intent": "<center><h2>Predicted Class and Cost Function</h2></center>\n"}
{"snippet": "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n", "intent": "Convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool.\n"}
{"snippet": "W_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n", "intent": "Add a softmax layer, just like for the one layer softmax regression above.\n"}
{"snippet": "X = news_A.drop('class',axis=1)\ny = news_A['class']\nclf2 = GaussianNB()\nclf2.fit(X,y)\nca2 = clf2.score(X,y)\nprint(\"The classification accuracy without cleaning is {}\".format(ca2))\n", "intent": "Fit a Gaussian Naive Bayes model to the original dataset A (including the outliers). Display the classification accuracy on the training dataset.\n"}
{"snippet": "svc = LinearSVC()\nsvc.fit(X_train,y_train)\nprint('The accuracy of training data is {:.3f}'.format(svc.score(X_train,y_train)))\nprint('The accuracy of test data is {:.3f}'.format(svc.score(X_test,y_test)))\n", "intent": "Train a [`LinearSVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n"}
{"snippet": "svc1 = SVC(kernel='rbf')\nsvc2 = SVC(kernel='poly')\nsvc1.fit(X_train,y_train)\nsvc2.fit(X_train,y_train)\nprint('The accuracy of training data with rbf is {:.3f}'.format(svc1.score(X_train,y_train)))\nprint('The accuracy of test data with rbf is {:.3f}'.format(svc1.score(X_test,y_test)))\nprint('The accuracy of training data with poly is {:.3f}'.format(svc2.score(X_train,y_train)))\nprint('The accuracy of test data with poly is {:.3f}'.format(svc2.score(X_test,y_test)))\n", "intent": "By using the [`SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"}
{"snippet": "from sklearn.dummy import DummyClassifier\ndc1 = DummyClassifier(strategy='most_frequent')\ndc1.fit(X_tr,y_tr)\nprint('The dummy classifier has accuracy: {:.3f} on the validation set.'.format(dc1.score(X_val,y_val)))\n", "intent": "I chose to return the most frequent class as output for my dummy classifier as simply guessing non-person will get an accuracy of 50%.\n"}
{"snippet": "kmeans = KMeans(n_clusters=5,random_state=1337)\nkmeans.fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "model.fit(X_train,y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "nb.fit(X_train, y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nmodel = SVC()\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "def sigmoid(x):\n    return 1/(1+exp(-x))\n", "intent": "The sigmoid converts values into range [0,1]\n"}
{"snippet": "def flatten(data):\n    data = None\n    return data\n", "intent": "---\nThe data has to be flattened after the last convolutional layer and before the first fully connected layer. \n"}
{"snippet": "def flatten(data):\n    data = tf.contrib.layers.flatten(inputs=data)\n    return data\n", "intent": "---\nThe data has to be flattened after the last convolutional layer and before the first fully connected layer. \n"}
{"snippet": "def fullyConnectedLayer(data, hiddenLayerSize, useActivationFunc=False):\n    ny = data.get_shape().as_list()[1]\n    nx = hiddenLayerSize\n    W = tf.get_variable(name='W', shape=(ny, nx), initializer=tf.contrib.layers.xavier_initializer())\n    z = tf.matmul(data, W, name='matmul')\n    if useActivationFunc==True:\n        a = tf.tanh(z, name='activation_function')\n    else:\n        a = z\n    return a\n", "intent": "---\nImplement a fully connected layer using tf.matmul()\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(x_train, y_train_class1)\nclf.score(x_train, y_train)   \n", "intent": "First, try simple random forest.\n"}
{"snippet": "from sklearn import linear_model\nmodel = linear_model.Perceptron() \nmodel.fit(X, y)\nplot_decision_boundary(model, X, plot_boundary=True)\nscatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)\n", "intent": "The `Perceptron` is a simple algorithm suitable for large scale learning.\n"}
{"snippet": "classifier = svm.SVC(kernel='rbf', gamma=gammas[np.argmax(scores)])\nclassifier.fit(X, y) \nX_test_norm = scaler.transform(X_test) \nprint classifier.score(X_test_norm, y_test)\n", "intent": "Check the score of the best model in the test set.\n"}
{"snippet": "import numpy as np\nrnd_data = np.random.multivariate_normal([0, 0], [[3, 4], [5, 6]], 1000)\nwhitened_data, whitening_matrix = whiten(rnd_data)\nassert(np.allclose(np.identity(2), np.cov(whitened_data)))\nprint(\"Passed\")\n", "intent": "See if the function returns whitened data\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, verbose=2)\ngrid.fit(X_test, y_test)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "sfa_dims = 2\nrnd_data = np.random.multivariate_normal([0, 0], [[3, 4], [5, 6]], 1000)\nsfa_homemade = slow_feature_analysis(rnd_data, sfa_dims)\nassert(np.allclose(np.identity(2), np.cov(sfa_homemade.T)))\nprint(\"Passed\")\n", "intent": "Identity covariance test:\n"}
{"snippet": "rnd_data = np.random.multivariate_normal([0, 0], [[3, 4], [5, 6]], 1000)\nprint(rnd_data.shape)\nwhitened_data, whitening_matrix = whiten(rnd_data)\nassert(np.allclose(np.identity(2), np.cov(whitened_data)))\nprint(\"Passed\")\n", "intent": "Identity covariance test:\n"}
{"snippet": "import numpy as np\nrnd_data = np.random.multivariate_normal([0, 0], [[3, 4], [5, 6]], 1000)\nprint(rnd_data.shape)\nwhitened_data, whitening_matrix = whiten(rnd_data)\nassert(np.allclose(np.identity(2), np.cov(whitened_data)))\nprint(\"Passed\")\n", "intent": "See if the function returns whitened data\n"}
{"snippet": "data = df_cp[['Population', 'Violent_crime', 'Property_crime']]\nmodel=lg.fit(data[['Population', 'Violent_crime']],data['Property_crime'])\nprint (\"Regression formula: y = {0}x + {1}\".format(model.coef_, model.intercept_))\n", "intent": "<h1> Model2:  Linear for 2 variables: Population + violent </h1>\n"}
{"snippet": "X = df_cp.drop('Property_crime',axis=1)\ny = df_cp.Property_crime\nmodel=lg.fit(X,y)\nprint (\"Regression formula: y = {0}x + {1}\".format(model.coef_, model.intercept_))\n", "intent": "<h1>Model3: Linear regression for all variables in table</h1>\n"}
{"snippet": "X = data[['amount','fico_avr']]\ny = data['interest']\nlg=linear_model.LinearRegression()\nmodel1=lg.fit(X,y)\nprint (\"formula: y = {0}x + {1}\".format(model1.coef_, model1.intercept_))\n", "intent": "We will use linear regression function to model the interest rate of borrower\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(min_samples_split=20, random_state=0,n_estimators=10)\nrf.fit(X, y)\n", "intent": "<h1> Random Forest </h1>\n"}
{"snippet": "rfc= RandomForestClassifier()\nmodel = rfc.fit(data[data.columns[0:-2]],data.Activity)\nmodel\n", "intent": "<h1> Classification Model </h1>\n"}
{"snippet": "rf_op = RandomForestRegressor(bootstrap=True, compute_importances=None,\n           criterion='mse', max_depth=12, max_features='auto',\n           max_leaf_nodes=None, min_density=None, min_samples_leaf=2,\n           min_samples_split=2, n_estimators=1000, n_jobs=-1,\n           oob_score=False, random_state=None, verbose=0)\nrf_op=rf_op.fit(x_data, y_data)\n", "intent": "RandomForestRegressor is trained using the best parameters.\n"}
{"snippet": "weights = {\n    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n}\nbiases = {\n    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n", "intent": "The variable n_hidden_layer determines the size of the hidden layer in the neural network. This is also known as the width of a layer.\n"}
{"snippet": "n_clusters = 5\nestimator = KMeans(n_clusters=n_clusters)\nestimator.fit(X_rolling)\n", "intent": "We have already completed the code to fit the estimator using sklearn. You will see how simple it is!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embedded = tf.nn.embedding_lookup(embedding, input_data)  \n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "opt_tree_model = DecisionTreeRegressor(max_depth=np.argmin(cv_test_errors))\nopt_tree_model.fit(X = X_train.values.reshape(-1,1), y = y_train.values.reshape(-1,1))\nprint(opt_tree_model.get_params)\ndot_data = StringIO() \nexport_graphviz(opt_tree_model, out_file=dot_data) \ngraph = pydot.graph_from_dot_data(dot_data.getvalue()) \ndisplay.Image(graph.create_png())\n", "intent": "It's hard to tell exactly which depth is optimal from the graph, so we'll use `np.argmin` to choose the lowest value from the `cv_test_errors`.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Random init is too complicated\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim),-1,1))\n    embed_layer = tf.nn.embedding_lookup(embedding, input_data)\n    return embed_layer\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "mod = Sequential()\nmod.add(Dense(10, input_dim=784,activation='softmax'))\nopti = keras.optimizers.RMSprop(lr=0.001, rho=0.95, epsilon=1e-08, decay=0.0)\nmod.compile(optimizer = opti , loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "* By varying the number of hidden layers, see if you can improve the score on the test data.\n"}
{"snippet": "model1=Sequential()\nmodel1.add(Dense(64,input_dim=3072,activation='relu'))\nmodel1.add(Dense(10,activation='softmax'))\nmodel1.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\nmodel1.summary()\n", "intent": "* Example of a standard network with one hidden layer\n"}
{"snippet": "model = Sequential()\nmodel.add(Conv2D(16, (3, 3), padding='same',input_shape=train_X.shape[1:],activation='relu'))\nmodel.add(Conv2D(16, (3, 3),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(num_classes,activation='softmax'))\n", "intent": "Modification of Ayman's and Iliana's model\n"}
{"snippet": "W = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.nn.softmax(tf.matmul(x, W) + b)\n", "intent": "We define our linear model for the score function after introducing two of parameters, **W** and **b**.\n"}
{"snippet": "from keras import regularizers\nmodel = Sequential()\nmodel.add(Dense(256, activation='relu', input_shape=(1000,), activity_regularizer=regularizers.l1(0.01)))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model = make_model(y=0.)\nmcmc = pm.MCMC(model)\nmcmc.use_step_method(ps.RandomWalk, model['theta'])\nsmc = ps.SMC(mcmc, num_particles=100, num_mcmc=1, verbose=4, ess_reduction=0.9)\nsmc.initialize(0.)\nsmc.move_to(1.)\n", "intent": "Now we initialize the model and run PySMC.\n"}
{"snippet": "scaler.fit(X_train)\n", "intent": "2) Fit using only the data.\n"}
{"snippet": "pca.fit(X_train)\n", "intent": "2) Fit to training data\n"}
{"snippet": "grid_search.fit(X_train, y_train)\n", "intent": "A GridSearchCV object behaves just like a normal classifier.\n"}
{"snippet": "clf = LogisticRegression()\nclf = clf.fit(X_train, y_train)\n", "intent": "Now will use the Logistic Regression class from the scikit-learn library to create a classifier object and train it on our data.\n"}
{"snippet": "clf = MultinomialNB()\nclf = clf.fit(X_train, y_train)\n", "intent": "Similarly fiddle around with the hyper parameters of this model. You can find the documentation on the scikit-learn website.\n"}
{"snippet": "regr = lm.LinearRegression()\nregr.fit(car_data.horsepower.values.reshape(len(car_data),1), car_data.mpg)\nseaborn.regplot(x=\"horsepower\", y=\"mpg\", data=car_data)\nprint(\"Pearson correlation: %.3f, Regresion coefficient: %.3f\" % (car_data.horsepower.corr(car_data.mpg), regr.coef_[0]))\n", "intent": "**Pozor** sklon regresnej ciary nehovori o sile korelacie. Len o smere.\n"}
{"snippet": "dtree=DecisionTreeClassifier()\ndtree.fit(X_train,y_train)\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "kmeans=KMeans(n_clusters=2)\nkmeans.fit(data.drop('Private',axis=1))\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "model.fit(x_train, y_train, epochs=50, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(X_train,y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "nb=MultinomialNB()\nnb.fit(X_train,y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "grid=GridSearchCV(SVC(),param_grid,verbose=3)\ngrid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=5)\n", "intent": "<a id=\"tuning-a-knn-model\"></a>\n---\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nfeatures = ['al']\nX = glass.loc[:, features]\ny = glass.loc[:, 'ri']\nlinreg.fit(X,y)\n", "intent": "- Instantiate and fit a linear regression model predicting `ri` from `al` (and an intercept).\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nfeature_cols = ['al']\nX = glass.loc[:, feature_cols]\ny = glass.loc[:, 'ri']\nlinreg.fit(X, y)\n", "intent": "- Instantiate and fit a linear regression model predicting `ri` from `al` (and an intercept).\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nX = mammals.loc[:, [\"body\"]]\npf = PolynomialFeatures(degree=3, include_bias=False)\npf.fit(X)\npf.transform(X)\n", "intent": "**sklearn has a \"transformer\" that generates polynomial terms**\n"}
{"snippet": "X = boston.loc[:,[\"DIS\"]]\ny = boston.loc[:,\"MEDV\"]\nlrb = LinearRegression()\nlrb.fit(X,y)\n", "intent": "- Create a linear regression model for MEDV against DIS with no higher-order polynomial terms.\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\npf = PolynomialFeatures(degree=7, include_bias=False)\npf.fit(X)\nX7 = pf.transform(X)\nlr_boston7 = LinearRegression()\nlr_boston7.fit(X7, y)\n", "intent": "- Create a linear regression model for y against X polynomial terms up to and including degree seven.\n"}
{"snippet": "model.fit(Xsmall, ysmall, batch_size=500, epochs=40,verbose = 1)\nmodel.save_weights('model_weights/best_RNN_small_textdata_weights.hdf5')\n", "intent": "Now lets fit our model!\n"}
{"snippet": "X = boston.loc[:, ['DIS']]\ny = boston.loc[:, 'MEDV']\nlr_boston1 = LinearRegression()\nlr_boston1.fit(X, y)\n", "intent": "- Create a linear regression model for MEDV against DIS with no higher-order polynomial terms.\n"}
{"snippet": "pf = PolynomialFeatures(degree=7, include_bias=False)\npf.fit(X)\nX7 = pf.transform(X)\nlr_boston7 = LinearRegression()\nlr_boston7.fit(X7, y)\n", "intent": "- Create a linear regression model for y against X polynomial terms up to and including degree seven.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\nmodel.fit(X_train, Y_train, validation_data = (X_val, Y_val), nb_epoch=20, \n          batch_size=128, verbose=True, validation_split=0.15, \n          callbacks=[best_model, early_stop]) \n", "intent": "So, how hard can it be to build a Multi-Layer percepton with keras?\nIt is baiscly the same, just add more layers!\n"}
{"snippet": "from keras.optimizers import SGD\nfrom keras.preprocessing.text import one_hot, text_to_word_sequence, base_filter\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.preprocessing import sequence\nimport tensorflow as tf\ntf.python.control_flow_ops = tf\n", "intent": "_source: http://colah.github.io/posts/2015-08-Understanding-LSTMs_\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(256, activation='relu', input_shape=x_train[0].shape))\nmodel.add(Dense(2, activation='softmax'))\nopti = optimizers.SGD(lr=0.01, momentum=0.3, decay=0.0, nesterov=False)\nmodel.compile(loss='categorical_crossentropy', optimizer=opti, metrics=['accuracy'])\nmodel.summary()\nprint(x_train.shape)\nprint(y_train.shape)\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "startTime = time.time()\ntotal_time = timeit.timeit('model.fit(x_train, y_train, epochs=200, batch_size=100, verbose=2)', number=1)\nelapsedTime = time.time() - startTime\nprint(elapsedTime)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "one_hot.fit(sport_encoded.reshape(-1,1))\none_hot_sport = one_hot.transform(sport_encoded.reshape(-1, 1))\n", "intent": "Next, we need to fit our data to the transformer. Let's do that here:\n"}
{"snippet": "km = KMeans(n_clusters=3, random_state=42)\n", "intent": "You guessed it! We're using sklearn.\n"}
{"snippet": "km = KMeans(n_clusters=3)\nkm.fit(iris_features_df_scaled)\n", "intent": "You guessed it! We're using sklearn.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X, y)\n", "intent": "In order to **build a model**, the features must be **numeric**, and every observation must have the **same features in the same order**.\n"}
{"snippet": "knc = KNeighborsClassifier(n_neighbors=29)\nknc.fit(X_train_scaled, y_train)\n", "intent": "Finally, fit a KNeighborsClassifier with our optimal k.\n"}
{"snippet": "ks = list(range(1,31))\ngs_train_accuracies = []\ngs_test_accuracies = []\nfor k in ks:\n    gs = GridSearchCV(KNeighborsClassifier(n_neighbors=k), param_grid={}, cv=5)\n    gs.fit(X_train_scaled, y_train)\n    train_score = gs.score(X_train_scaled, y_train)\n    test_score = gs.score(X_test_scaled, y_test)\n    gs_train_accuracies.append(train_score)\n    gs_test_accuracies.append(test_score)\n", "intent": "Plot your train and test scores against your different values of K. Which K should you choose? Does CV do anything to help the train or test score?\n"}
{"snippet": "gs_knc = GridSearchCV(KNeighborsClassifier(), param_grid=params, cv=5, n_jobs=-1)\n", "intent": "Instantiate GridSearchCV with KNeighborsClassifier, your param_grid, and 5-fold cross-validation.\n"}
{"snippet": "gs_knc.fit(X_train_scaled, y_train)\n", "intent": "Fit your grid search object to your scaled X_train and y_train\n"}
{"snippet": "Ridge().get_params().keys()\n", "intent": "---\nIs it better than the Linear regression? If so, why might this be?\n"}
{"snippet": "y = admissions.admit.values\nX = admissions[['gpa']].values\nlinmod = LinearRegression()\nlinmod.fit(X, y)\nprint 'Intercept:', linmod.intercept_\nprint 'Coef(s):', linmod.coef_\n", "intent": "<a id='pred-admit'></a>\n---\nLet's try predicting the `admit` binary indicator using just `gpa` with a Linear Regression to see what goes wrong.\n"}
{"snippet": "gspipe.fit(features_dummies, target)\n", "intent": "This will take a while...\n"}
{"snippet": "pipe = r.pipeline()\n", "intent": "Now, let's use `redis pipeline` to load our lookup.\n"}
{"snippet": "def make_prediction(title):\n    vec = nlp(title).vector\n    model = pickle.loads(r.get('model'))\n    distances , indices = model.kneighbors(vec.reshape(1,-1))\n    indices = indices[0]\n    pipe = r.pipeline()\n    for index in indices:\n        pipe.get(bytes(str(index), 'utf-8'))\n    return list(zip(pipe.execute(), distances[0]))\n", "intent": "What stuff do we need for our script?\n* redis (r)\n* nlp (English())\n* pickle\n* argparser\n"}
{"snippet": "from sklearn.pipeline import make_pipeline\npipe = make_pipeline(vect, nb)\n", "intent": "[make_pipeline documentation](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)\n"}
{"snippet": "KNN = KNeighborsClassifier()\ngs = GridSearchCV(KNN, param_grid={'n_neighbors':range(3,21,2)}) \ngs.fit(latent_semantic_analysis, news_df['group_num'])\n", "intent": "We used KNN here, but we can use RandomForest, LogisticRegression, XGBoost (Gradient Descent Trees), Gausean, Naive Bayes, any classifiers. \n"}
{"snippet": "encoder.fit(titanic_df['embarked'])\ntitanic_embarked_encoded = encoder.transform(titanic_df['embarked'])\ntitanic_embarked_encoded\n", "intent": "1. Create a new `LabelEncoder`. \n1. Use it to label encode the `embarked` column.\n1. Save the label encoded vector back to the `titanic_df`.\n"}
{"snippet": "rf.fit(X_train, y_train)\ngdbr.fit(X_train, y_train)\nabr.fit(X_train, y_train)\n", "intent": "better means lowest mse, higher r squared; the magnitude of differences in this sample is way too small to use.\n"}
{"snippet": "C = [1E-1,1,10,100]\ngamma = 250\nclassifiers = []\nfor C in C:\n    clf = SVC(C=C, gamma=gamma)\n    clf.fit(X, y)\n    decision_boundary(clf,X,y)\n", "intent": "No visible changes the C value increases.\n"}
{"snippet": "clf = SVC(kernel = 'linear')\nclf.fit(X, y)\ndecision_boundary(clf,X,y)\n", "intent": "we have two different groups , but one group has misclassified one data point\n"}
{"snippet": "logreg = LogisticRegression()\nlogreg.fit(X_small,y_small)\ndecision_boundary(logreg,X_small,y_small)\n", "intent": "The class counts look more obvious in this case , rather distinct and more seperated\n"}
{"snippet": "clf = SVC()\nclf.fit(X_small, y_small)\ndecision_boundary(clf,X_small,y_small)\n", "intent": "Two points are misclassified\n"}
{"snippet": "clf = SVC(kernel = 'sigmoid')\nclf.fit(X_small, y_small)\ndecision_boundary(clf,X_small,y_small)\n", "intent": "the classification is better using SVM and the decision boundaries make them more distinct, the poly kernel serves as the best decision boundary\n"}
{"snippet": "X = df.values\nkm = KMeans(n_clusters=3).fit(X)\nkm.labels_\n", "intent": "There are some dense blobs inside, but it's rather hard to tell.\n"}
{"snippet": "from sklearn import metrics\ncuisine_similarity = []\nfor idx in range(cuisine_dtm.shape[0]):\n    similarity = metrics.pairwise.linear_kernel(cuisine_dtm[idx, :], cuisine_dtm).flatten()\n    cuisine_similarity.append(similarity)\n", "intent": "[How to calculate document similarity](http://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity/12128777\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(2000, activation='relu', input_dim=x_train.shape[1]))\nmodel.add(Dense(500, activation='relu'))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.summary()\noptimizer = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=85, batch_size=2000, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "logreg = LogisticRegression()\nlogreg\n", "intent": "Is your model predicting well? If not, how would you improve it?\n"}
{"snippet": "estimator = GridSearchCV(knn2, search_parameters, cv=5, verbose=1, n_jobs=4)\nresults = estimator.fit(trainX, trainY)\n", "intent": "+ Now use 10-fold cross-validation to choose the most efficient `k`\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(64, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "history = model.fit(x_train, y_train, epochs=5, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "ratings_true = ratings_mat[user_id, items_rated_by_this_user].todense()\nratings_pred = pred_ratings[items_rated_by_this_user]\nfor i,j in zip(np.array(ratings_true).squeeze(),ratings_pred):\n    print i, j\nerr_one_user = ratings_true-ratings_pred\nprint 'Mean Absolute Error:', abs(err_one_user).mean()\n", "intent": "We have checked total error for all users, here we want to check error for each rated item of picked user.\n"}
{"snippet": "clf_A.fit(nnn, y)\n", "intent": "This is an overfitting due to we don't have properly formatted features. (Supervised learning failed). \n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nclf= DecisionTreeClassifier(random_state=1)\nclf.fit(training_inputs, training_classes)\n", "intent": "Now create a DecisionTreeClassifier and fit it to your training data.\n"}
{"snippet": "import numpy as np\nfrom sklearn import linear_model\nX = np.array([[-1, -1], [-2, -1], [-2, -1], [-2, -1], [-2, -1], [2, 1]])\nY = np.array([1, 3, 3, 3, 3, 2])\nclf = linear_model.SGDClassifier(max_iter=15, tol=None)\nclf.fit(X, Y)\n", "intent": "                          *** Implement Incremental/Online Learning using SGDClassifier **\n"}
{"snippet": "results = smf.ols('Sales ~ Price + US', data=carseats).fit()\nresults.summary()\n", "intent": "d) Reject null for US and Price\n"}
{"snippet": "estimates = defaultdict(list)\nfor predictor in list(boston_df.drop('CRIM',axis=1)):\n    X = sm.add_constant(boston_df[predictor])\n    estimate=sm.OLS(boston_df['CRIM'],X).fit()\n    estimates[predictor] = estimate\n    print(\"{0:8s}: [beta_0, beta_1] = [{1:5.2f},{2:6.2f}]:    p-value= {3:10.6f}\".format(predictor, estimate.params[0], estimate.params[1], estimate.pvalues[1]))\n", "intent": "We will regress 'CRIM' response onto each of the predictors and save the coeffecients and the p-values from each regression. \n"}
{"snippet": "X_train = df_X_train[\"Lag2\"].values.reshape(-1,1)\ny_train = df_X_train[\"Direction2\"].values\nknn_20 = KNeighborsClassifier(n_neighbors= 20)\nknn_20.fit(X_train, y_train)\n", "intent": "Using only 1 Nearest Neighbor, the model performs at chance level.\n"}
{"snippet": "X_train = training_df[[\"Lag1\", \"Lag2\"]].values\nY_train = training_df[\"Direction2\"].values\nlda_model = LDA(solver=\"lsqr\")\nlda_model.fit(X_train, Y_train)\nprint('Class Priors =', lda_model.priors_)\nprint('Class Means =', lda_model.means_)\nprint('Class Coefficients =', lda_model.coef_)\n", "intent": "Predict the market direction using Lag1 and Lag2 as predictors using scikit learn LDA module.\n"}
{"snippet": "from sklearn import svm, datasets\nC = 1.0 \nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\n", "intent": "Now we'll use linear SVC to partition our graph into clusters:\n"}
{"snippet": "logreg = linear_model.LogisticRegression(solver = \"newton-cg\", multi_class = 'multinomial')\n", "intent": "Next, let's try multinomial logistic regression! Follow the same steps as with Naive Bayes, and plot the confusion matrix.\n"}
{"snippet": "svm = svm.LinearSVC()\nsvm_model = svm.fit(X_train, y_train)\n", "intent": "Now do the same for a Support Vector Machine.\n"}
{"snippet": "ada_reg = AdaBoostRegressor(base_estimator=None,  \n                                    n_estimators=50,  \n                                    learning_rate=1.0,  \n                                    random_state=10, \n                                    loss='linear')  \nada_model = ada_reg.fit(X_train, y_train)\nprint(ada_model.score(X_train, y_train))\nprint(ada_model.score(X_val, y_val))\n", "intent": "Using an Ada Boost Regressor is nearly identical to using the other regressors covered today.\n"}
{"snippet": "from sklearn import svm\nC = 1.0\nsvc = svm.SVC(kernel='linear', C=C)\n", "intent": "Next try using svm.SVC with a linear kernel. How does it compare to the decision tree?\n"}
{"snippet": "model.add(\n    Dense(num_classes, kernel_initializer='normal', activation='softmax')\n)\n", "intent": "Add in the output layer to our model.\n"}
{"snippet": "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n", "intent": "Let's now fit our model on the training dataset. We fit our model over 10 epochs and update it every 200 images. It might take a few minutes.\n"}
{"snippet": "import numpy\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras import backend as K\nK.set_image_dim_ordering('th')\n", "intent": "Keras also provides useful methods to create a CNN.\n"}
{"snippet": "def CNN_model():\n    model = Sequential()\n    model.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n", "intent": "Let's define a function that creates our CNN. Read through the comments to understand what each line does.\n"}
{"snippet": "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n", "intent": "Let's now fit our model on the training datasets. We fit our model over 10 epochs and update it every 200 images. It might take a few minutes.\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(init)\n    epochs = 100\n    for i in range(epochs):\n        sess.run(train)\n    final_slope, final_intercept = sess.run([m, b])\n", "intent": "- **Create Session and Run!**\n"}
{"snippet": "def get_oob(df):\n    m = RandomForestRegressor(n_estimators=30, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)\n    x, _ = split_vals(df, n_trn)\n    m.fit(x, y_train)\n    return m.oob_score_\n", "intent": "- Let's try removing some of these related features to see if the model can be simplified without impacting the accuracy.\n"}
{"snippet": "model.fit(X_train_indices, Y_train_oh, epochs = 100, batch_size = 32, shuffle=True)\n", "intent": "Fit the Keras model on `X_train_indices` and `Y_train_oh`. We will use `epochs = 50` and `batch_size = 32`.\n"}
{"snippet": "vgg_model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n", "intent": "Now lets add your first pooling layer. Pooling reduces the width and height of the input by aggregating adjacent cells together.\n"}
{"snippet": "def batch_lrelu(x, alpha=0.1, batch_normalization=True, is_train=True):\n    if batch_normalization:\n        temp = tf.layers.batch_normalization(x, training=is_train)\n        return tf.maximum(alpha * x, x)\n    else:\n        return tf.maximum(alpha * x, x) \n", "intent": "Each convolution goes throught the same process of batch_normalization and then a leaky relu\n"}
{"snippet": "predictions = Dense(2, activation='softmax')(fc1)\n", "intent": "Finally, for the output, we need a softmax layer with 2 neurons (two classes: 0/1). \n"}
{"snippet": "model.fit(XTrain_s_batch, yTrain_batch, epochs=30) \n", "intent": "Fit the model on the batched training data with 100 epochs, you should see the accuracy on the test set increase dramatically.\n"}
{"snippet": "inputs = Input(shape=(None, 17)) \nlstm = LSTM(512, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n            bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n            recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n            kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, \n            recurrent_dropout=0.0, implementation=1, return_sequences=True, return_state=False, \n            go_backwards=False, stateful=False, unroll=False)(inputs)\n", "intent": "Let's use as a first layer a 512 LSTM without regularisation\n"}
{"snippet": "predictions = TimeDistributed(Dense(17, activation='linear'))(lstm)\nmodel_lstm512 = Model(inputs=inputs, outputs=predictions)\nmodel_lstm512.compile(optimizer='rmsprop',\n              loss='mean_squared_error',\n              metrics=['accuracy'])\n", "intent": "The only thing which we need to change is the output dimensions of the network from 1 to 17. \n"}
{"snippet": "model_lstm512.fit(X_batch, y_batch, epochs=500)\n", "intent": "Let's turn the magic on, train for 500 epochs, more may be better but may crash your computer.\n"}
{"snippet": "rnn_model.fit(XTrain_s_batch, yTrain_batch, epochs=15)\n", "intent": "Now we're good to fit this for a few epochs and check the performances. \n"}
{"snippet": "hist = model.fit(x_train, y_train, batch_size = 128, epochs = 8, validation_split = 0.2, verbose = 0, callbacks=[plot_losses], shuffle = True)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-softmax'))\n    feed = {x: test_x,\n            y: test_y}\n    out = sess.run(tf.nn.softmax(logits), feed_dict=feed)\n    out_y =sess.run(tf.argmax(out, axis=1))\n    label_y = sess.run(tf.argmax(test_y, axis=1))\n    test_los,test_acc = sess.run([cost,accuracy], feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "load the trained model and run it\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-softmax'))\n    feed = {x: test_x,\n            y: test_y}\n    out = sess.run(tf.nn.softmax(logits), feed_dict=feed)\n    out_test_y =sess.run(tf.argmax(out, axis=1))\n    label_test_y = sess.run(tf.argmax(test_y, axis=1))\n    test_los,test_acc = sess.run([cost,accuracy], feed_dict=feed)\n    print(\"Test accuracy: {:.4f}%\".format(test_acc*100))\n", "intent": "load the trained model and run it\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_normal([vocab_size, embed_dim], -1 ,1))\n    return tf.nn.embedding_lookup(embeddings, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-softmax'))\n    feed = {x: test_x,\n            y: test_y}\n    out = sess.run(tf.nn.softmax(logits), feed_dict=feed)\n    out_test_y =sess.run(tf.argmax(out, axis=1))\n    label_test_y = sess.run(tf.argmax(test_y, axis=1))\n    test_los,test_acc = sess.run([cost,accuracy], feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc*100))\n", "intent": "load the trained model and run it\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-softmax'))\n    feed = {x: train_x,\n            y: train_y}\n    out = sess.run(tf.nn.softmax(logits), feed_dict=feed)\n    out_train_y =sess.run(tf.argmax(out, axis=1))\n    label_train_y = sess.run(tf.argmax(train_y, axis=1))\n    test_los,test_acc = sess.run([cost,accuracy], feed_dict=feed)\n    print(\"Train accuracy: {:.4f}\".format(test_acc))\n", "intent": "show the lowest confidence for correct output\nshow the highest confidence for wrong output\nBUT keep the number of correct prediction at 95%\n"}
{"snippet": "for i in tf.get_default_graph().get_operations():\n    print(i.name)\n", "intent": "https://stackoverflow.com/questions/35336648/list-of-tensor-names-in-graph-in-tensorflow\n"}
{"snippet": "import functools\ndef get_embed(input_data, vocab_size, embed_dim):\n    input_data_list = input_data.get_shape().as_list()\n    input_dim = functools.reduce(lambda x,y: x*y, input_data_list)\n    embedding = tf.Variable(tf.truncated_normal([vocab_size, embed_dim], stddev = np.sqrt(1.0/input_dim)))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "kmeans(<FILL IN>)\n", "intent": "Test your algorithm on the dataset\n"}
{"snippet": "kmeans(rdd, N_CENTERS, 20)\n", "intent": "Test your algorithm on the dataset\n"}
{"snippet": "print(x4.todense())\n", "intent": "Let's print it and see what it looks like \n"}
{"snippet": "simple_model = Sequential()\nsimple_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape,padding='same'))\nsimple_model.add(MaxPooling2D((2, 2),padding='same'))\nsimple_model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\nsimple_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nsimple_model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))\nsimple_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nsimple_model.add(Flatten())\nsimple_model.add(Dense(128, activation='relu'))\nsimple_model.add(Dense(num_classes, activation='softmax'))\n", "intent": "Finally, we initialize a model and start adding layers \n"}
{"snippet": "simple_train = simple_model.fit(X_train, y_train_onehot, batch_size=batch_size, epochs=num_epochs, verbose=1,\n                                validation_data=(X_valid, y_valid_onehot))\n", "intent": "**Part B**: Now we're ready to actually train the model! You might want to grab a magazine or something because this'll take a minute!   \n"}
{"snippet": "def FCBlock(model):\n    model.add(Dense(4096, activation='relu'))\n    model.add(Dropout(0.5))\n", "intent": "...and here's the fully-connected definition.\n"}
{"snippet": "import scipy\nscipy.stats.norm.fit(house.SalePrice)\n", "intent": "That doesn't look promising.\n"}
{"snippet": "import fitter\nf = fitter.Fitter(house.SalePrice)\nf.fit()\nf.summary()\n", "intent": "Pull out the big guns -- what distribution might it be?\n"}
{"snippet": "f = fitter.Fitter(house.LogPrice)\nf.fit()\nf.summary()\n", "intent": "Yeah! That looks normal. Let's double check..\n"}
{"snippet": "import sklearn.linear_model\ngrid = sklearn.model_selection.GridSearchCV(\n    sklearn.linear_model.Lasso(normalize=True),\n    param_grid = {'alpha': np.arange(0.0001, 0.0010, 0.00001)\n                 }\n)\ngrid.fit(numeric_house, house.LogPrice)\ngrid.best_score_\n", "intent": "So, how does this prediction model work?\n"}
{"snippet": "log_sci_model = LogisticRegression()\n", "intent": "Instantiate a logistic regression model as:\n"}
{"snippet": "from sklearn import metrics, cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfeatures = ['Pclass', 'Imputed_Age', 'SibSp', 'Parch', 'Fare', 'C', 'Q', 'female']\nlog_sci_model = LogisticRegression()\nlog_sci_model = log_sci_model.fit(train_data[features], train_data['Survived'])\nlog_sci_model.score(train_data[features], train_data['Survived'])\n", "intent": "Train the model with 'Fare':\n"}
{"snippet": "from sklearn import metrics, cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfeatures = ['Fare']\nlog_sci_model = LogisticRegression()\nlog_sci_model = log_sci_model.fit(train_data[features], train_data['Survived'])\nlog_sci_model.score(train_data[features], train_data['Survived'])\n", "intent": "Train the model with 'Fare':\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded_input = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.01))\n    embedded_input = tf.nn.embedding_lookup(embedded_input, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "cells = tf.contrib.rnn.MultiRNNCell([create_single_cell(lstm_size, keep_prob) for _ in range(number_of_layers)])\n", "intent": "Stack the LSTM cell in to Mutliple layers\n"}
{"snippet": "lm = Sequential([ Dense(2, activation='softmax', input_shape=(1000,)) ])\nlm.compile(optimizer=RMSprop(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "Now we can define our linear model, just like we did earlier:\n"}
{"snippet": "with tf.variable_scope('softmax_variables'):\n    softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n    softmax_b = tf.Variable(tf.zeros(num_classes))\n", "intent": "The shape of the weights will be Number of hidden units in a LSTM cell and the size of class/unique characters.\n"}
{"snippet": "logits = tf.nn.softmax(prediction, name='predictions')\n", "intent": "Apply the softmax function for output logits from each LSTM\n"}
{"snippet": "from time import time\nfrom sklearn.linear_model import LogisticRegression\ntic = time()\nlogisticRegr = LogisticRegression(solver = 'lbfgs')\nlogisticRegr.fit(train_img, train_label)\nscore = logisticRegr.score(test_img, test_label)\ntoc = time()\nprint('The total time is %s seconds ' % (toc-tic))\nprint('The classification accuracy is %s ' % score)\n", "intent": "Hint: Use LogisticRegression from sklearn.linear_model. To increase speed, change the default solver to 'lbfgs'\n"}
{"snippet": "iris_model = KNeighborsClassifier(5).fit(iris_features_train, iris_target_train)\n", "intent": "Train the KNN classifier\n"}
{"snippet": "def cross_validate(features, target, classifier, k_fold=10, r_state=None) :\n    k_fold_indices = KFold(k_fold,shuffle=True, random_state=r_state)\n    k_score_total = 0\n    for train_indices, test_indices in k_fold_indices.split(features, target):\n        model = classifier.fit(features[train_indices],target[train_indices])\n        k_score = model.score(features[test_indices],target[test_indices])\n        k_score_total += k_score\n    return k_score_total/k_fold\n", "intent": "Function for calculating cross-validation\n"}
{"snippet": "cross_validate(features, target, KNeighborsClassifier(5), 10, 0)\n", "intent": "Test the cross_validate function with different numbers of neighbors\n"}
{"snippet": "titanic_features_normalized = titanic_data[['Age_norm', 'Sex', 'Pclass_1', 'Pclass_2', 'Pclass_3']].values\nknn_accuracy_with_neighbors_normalized = [(neighbor, cross_validate(titanic_features_normalized, titanic_target, KNeighborsClassifier(neighbor, weights='distance'), 10, 0)) for neighbor in range(1, 70)]\nzipped_data_normalized = list(zip(*knn_accuracy_with_neighbors_normalized))\nprint(zipped_data_normalized[1])\npercentages = list(zipped_data_normalized[1])\nprint(sum(percentages)/len(percentages))\n", "intent": "Create new features using normalized data\n"}
{"snippet": "random_forest_model = RandomForestClassifier(random_state=0)\n", "intent": "Create an instance of a random forest classifier.  Random state is used to set random number generator for reproducible results\n"}
{"snippet": "model.add(layers.Dense(units=84, activation='relu'))\n", "intent": "The sixth layer is a fully connected layer (F6) with 84 units.\n"}
{"snippet": "lm = Sequential([ Dense(2, activation='softmax', input_shape=(1000,)) ])\n", "intent": "Do you remember how we defined our linear model? Here it is again for reference:\n"}
{"snippet": "for k in range(1, 21):\n    knn = KNeighborsClassifier(n_neighbors = k)\n    x,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\n    knn.fit(x_train,y_train)\n    print('KNN (K=' + str(k) + ') accuracy is: ',knn.score(x_test,y_test)) \n", "intent": "Let's look at how changing the number of neigbors affects the accuracy of our prediction. \n"}
{"snippet": "for k in range(1, 20):\n    knn = KNeighborsClassifier(n_neighbors = k)\n    x,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\n    knn.fit(x_train,y_train)\n    print('KNN (K=' + str(k) + ') accuracy is: ',knn.score(x_train,y_train)) \n", "intent": "Now, let's do what we said we weren't going to do and test with our training data instead of our testing data. \n"}
{"snippet": "estimator.fit(trainingImages, trainingValues)\n", "intent": "Now we fit the `estimator` just like we would any other model, but now it's doing a fit for each pair of values in the grid search\n"}
{"snippet": "estimator.fit(X_train, y_train)\n", "intent": "Now we fit the `estimator` just like we would any other model, but now it's doing a fit for each pair of values in the grid search\n"}
{"snippet": "from sklearn import linear_model\ncls = linear_model.LinearRegression()\ncls.fit(X, y)\n", "intent": "**Sklearn Method for Part 2**\n"}
{"snippet": "from sklearn import linear_model\nmodel = linear_model.LogisticRegression(penalty='l2', C=1.0)\nmodel.fit(X2, y2)\nmodel.coef_\n", "intent": "**Sklearn Regularized Logistic Regression Solution**\n"}
{"snippet": "def sigmoidGradient(z):\n    return np.multiply(sigmoid(z), (1 - sigmoid(z)))\nsigmoidGradient(0)\n", "intent": "**2. Backpropagation**\n"}
{"snippet": "clf = sklearn.svm.SVC(C=1000, kernel='linear')\nclf.fit(df[['X1','X2']], df['y'])\ngraph_svm_decision_boundary(clf)\n", "intent": "Large C (No Regularization)\n"}
{"snippet": "clf = sklearn.svm.SVC(C=1, kernel='linear')\nclf.fit(df[['X1','X2']], df['y'])\ngraph_svm_decision_boundary(clf)\n", "intent": "Small C (Lots of Regularization)\n"}
{"snippet": "model.add(Dense(4096, activation='relu'))\n", "intent": "And do you remember the definition of a fully connected layer in the original VGG?:\n"}
{"snippet": "modelW0 = sm.OLS(y, X)\nresultsW0 = modelW0.fit()\nprint (resultsW0.summary())\n", "intent": "Now let's fit a linear regression model with an intercept. \n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim],-0.01,0.01),dtype=tf.float32)\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn import linear_model\nX = df_Features\ny = df_Target\nlg = linear_model.LogisticRegression(penalty=\"l2\", dual=False)\nlg.fit(X,y)\nlg.score(X,y)\n", "intent": "Build a logit model and fit\n"}
{"snippet": "k = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(dn)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "k = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(newDF)\n", "intent": "Cluster the Data to our our target groups. **BONUS**: Perform multiple clustering tests on various indicator variables\n"}
{"snippet": "k = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(X)\n", "intent": "Run the clustering analysis using scikit-learn.\n*Hint*: Estimate the number of clusters, k, based on your visual examination of the distributions.\n"}
{"snippet": "dbscn = DBSCAN(eps = 3.2, min_samples = 3, random_state=5).fit(X)  \n", "intent": "Let's set up the DBSCAN from scikit using an epsilon of .5 and a minimum sample number of 5\n"}
{"snippet": "db2 = DBSCAN(eps=.2, min_samples=3, random_state=5).fit(X)\ndb2\n", "intent": "Let's set up the DBSCAN from scikit using an epsilon of .5 and a minimum sample number of 5\n"}
{"snippet": "W = 0.01 * np.random.randn(784,10)\nb = np.zeros((1,10))\nX_in = Input()\nW_in = Input()\nb_in = Input()\ny_in = Input()\nf = Linear(X_in, W_in, b_in)\nf = CrossEntropyWithSoftmax(f, y_in)\n", "intent": "You should hit above 90% accuracy on even a 1-layer neural network.\n"}
{"snippet": "def fit_model(model, batches, val_batches, nb_epoch=1):\n    model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=nb_epoch, \n                        validation_data=val_batches, nb_val_samples=val_batches.N)\n", "intent": "We'll define a simple function for fitting models, just to save a little typing...\n"}
{"snippet": "model.add(Convolution2D(filters=32, kernel_size=3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.summary()\n", "intent": "- Add the second convolutional block to the model\n- Call `keras.models.Model.summary()` to verify the dimensions are correct\n"}
{"snippet": "model.add(Convolution2D(filters=64, kernel_size=3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2))\n", "intent": "- Add the second convolutional block to the model\n- Call `keras.models.Model.summary()` to verify the dimensions are correct\n"}
{"snippet": "model.add(Flatten())\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n", "intent": "- Add the first bottleneck block to the model\n- Call `keras.models.Model.summary()` to verify the dimensions are correct\n"}
{"snippet": "model.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\nmodel.summary()\n", "intent": "- Add the second bottleneck block to the model\n- Call `keras.models.Model.summary()` to verify the dimensions are correct\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=42)\ndtc.fit(X_train, y_train);\n", "intent": "[sklearn.tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n"}
{"snippet": "def leaky_relu(x,alpha=0.1,name='leaky_relu'):\n    return tf.maximum(alpha*x,x,name=name)\n", "intent": "Implement a leaky relu function\n"}
{"snippet": "wf_array.todense()\n", "intent": "The sparse arrays can be converted to dense with the `todense` method. The dense version can be read by pandas and converted into a dataframe.\n"}
{"snippet": "knn_classifier = KNeighborsClassifier(n_neighbors = 3, weights = 'distance')\nknn_classifier.fit(wine_topics, wine_df['rating'])\n", "intent": "We can now use our topics as features\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\ndtc = DecisionTreeClassifier(max_depth = 3, \n                             min_samples_leaf = 10) \n", "intent": "What about a different method?\n"}
{"snippet": "fc_model.fit(trn_features, trn_labels, nb_epoch=8, \n             batch_size=batch_size, validation_data=(val_features, val_labels))\n", "intent": "And fit the model in the usual way:\n"}
{"snippet": "model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Flatten(input_shape=(64, 64,)))\nmodel.add(MyLinearLayer(len(labels)))\nmodel.compile(\n    optimizer=tf.train.AdamOptimizer(),\n    loss='categorical_crossentropy',\n    metrics=['accuracy'])\nmodel.summary()\n", "intent": "**Model definition**\nWe can now define our TF model using `tf.keras.Sequential`.\n"}
{"snippet": "model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=1)\n", "intent": "**Training the model**\n"}
{"snippet": "model.save('./tmp.h5')\nloaded_model = tf.keras.models.load_model('./tmp.h5', dict(MyLinearLayer=MyLinearLayer))\n", "intent": "**Saving and loading a (trained) model**\n"}
{"snippet": "treereg = DecisionTreeRegressor(max_depth=6, random_state=1)\ntreereg.fit(X,y)\n", "intent": "Ok, looks like 9 is the right max depth for this model\n"}
{"snippet": "treereg = DecisionTreeRegressor(max_depth=9, random_state=1)\ntreereg.fit(X,y)\n", "intent": "Ok, looks like 9 is the right max depth for this model\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr_all = LinearRegression()\n", "intent": "- Make a new instance of the LinearRegression class. Call it lr_all to distinguish it from our last model.\n"}
{"snippet": "X = bikes.iloc[:, 0:8]\ny = bikes.loc[:,'num_total_users']\nlr_all.fit(X,y)\n", "intent": "- Train the model instance using our new feature matrix $X$ and the same target variable $y$.\n"}
{"snippet": "lr_temp_atemp = LinearRegression()\nX = bikes.loc[:,['temp_celsius','atemp']]\nlr_temp_atemp.fit(X,y)\n", "intent": "**Exercise.**\n- Create another `LinearRegression` instance that is fit using `temp_celsius` and `atemp`, and print the coefficients.\n"}
{"snippet": "lr_temp_atemp = LinearRegression()\nX = bikes.loc[:, ['temp_celsius', 'atemp']]\nlr_temp_atemp.fit(X, y)\n", "intent": "**Exercise.**\n- Create another `LinearRegression` instance that is fit using `temp_celsius` and `atemp`, and print the coefficients.\n"}
{"snippet": "def embedding_input(name, n_in, n_out, reg):\n    inp = Input(shape=(1,), dtype='int64', name=name)\n    return inp, Embedding(n_in, n_out, input_length=1, W_regularizer=l2(reg))(inp)\n", "intent": "Previously we didnt add bias terms which is very crucial. Let us try with bias\n"}
{"snippet": "knn =  KNeighborsClassifier(n_neighbors=5)\n", "intent": "<a id=\"tuning-a-knn-model\"></a>\n---\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg\nX = glass.loc[:, ['al']]\ny = glass.loc[:, 'ri']\nlinreg.fit(X,y)\n", "intent": "- Instantiate and fit a linear regression model predicting `ri` from `al` (and an intercept).\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nX = mammals.loc[:, ['body']]\npf = PolynomialFeatures(degree=3, include_bias = False)\npf.fit(X)\npf.transform(X)\n", "intent": "**sklearn has a \"transformer\" that generates polynomial terms**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "logistic_regression = LogisticRegression()\nclassification_model(logistic_regression)\n", "intent": "<img src=\"Images/logistic_regression.jpeg\" height=\"742\" width=\"742\" align=\"middle\" > \n* Slightly good result \n"}
{"snippet": "decision_trees = DecisionTreeClassifier()\nclassification_model(decision_trees)\n", "intent": "<img src=\"Images/decision_trees.png\" height=\"742\" width=\"742\" align=\"middle\" > \nOverfitting\n"}
{"snippet": "random_forests = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\nclassification_model(random_forests)\n", "intent": "<img src=\"Images/random_forests.jpg\" height=\"742\" width=\"742\" align=\"middle\" > \nGood classifier\n"}
{"snippet": "model.fit(\n    X_train,\n    y_train,\n    batch_size=512,\n    nb_epoch=30,\n    validation_split=0.1,\n    verbose=1)\n", "intent": "Adjust hyperparameters and train the model.\n"}
{"snippet": "resultados_lrgs = [lrgs.fit(x[train_indices], y[train_indices]).score(x[test_indices],y[test_indices]) for train_indices, test_indices in kf_total]\n", "intent": "Com isso podemos procurar nosso modelo \"tunado\":\n"}
{"snippet": "user_in = Input(shape=(1,), dtype='int64', name='user_in')\nu = Embedding(n_users, n_factors, input_length=1, W_regularizer=l2(1e-4))(user_in)\nmovie_in = Input(shape=(1,), dtype='int64', name='movie_in')\nm = Embedding(n_movies, n_factors, input_length=1, W_regularizer=l2(1e-4))(movie_in)\n", "intent": "The most basic model is a dot product of a movie embedding and a user embedding. Let's see how well that works:\n"}
{"snippet": "gs = RandomizedSearchCV(pipe, param_grid, n_jobs = 7, n_iter = 10)\ngs.fit(X, y)\nprint \"Best CV score\", gs.best_score_\nprint gs.best_params_\n", "intent": "We can instead randomly sample from the parameter space with __RandomizedSearchCV__:\n"}
{"snippet": "nf = 22 \nlsi = models.LsiModel(corpus, id2word=dictionary, num_topics=nf, extra_samples=150,)\nindex = similarities.MatrixSimilarity(lsi[corpus], num_features=nf) \n", "intent": "define Latent Semantic Index of Documents\n"}
{"snippet": "post_profile.todense()\n", "intent": "And converting the sparce vector to a dense one, we can see we are really saving memory here, as most of the values are zeroes.\n"}
{"snippet": "def get_post_profile_sorted_features(post_id):\n    profile_values = get_post_profile_values(post_id)\n    tfidf_values = profile_values.todense().flatten().tolist()[0]\n    tfidf_feature_values = zip(tfidf_feature_names, tfidf_values)\n    sorted_tfidf_feature_values = sorted(tfidf_feature_values, key=lambda c: c[1], reverse=True)\n    return filter(lambda p: p[1] > 0, sorted_tfidf_feature_values)\npost_features = get_post_profile_sorted_features('z121itaiezmtzbp5j04cg5owalnmwjqab20')\npost_features\n", "intent": "It's possible to retrieve the most \"important\" words for a post\n"}
{"snippet": "def get_global_relevant_words():\n    tfidf_global_feature_values = tfidf_sparce_matrix.mean(axis=0).flatten().tolist()[0]\n    feature_values = zip(tfidf_feature_names, tfidf_global_feature_values)\n    sorted_feature_values = sorted(feature_values, key=lambda c: c[1], reverse=True)\n    return sorted_feature_values\nglobal_relevant_words = get_global_relevant_words()[:200]\nglobal_relevant_words[:20]\n", "intent": "Finally, we average TF-IDF vectors of all posts, and get the most \"important\" terms for the company professionals.\n"}
{"snippet": "regr3 = linear_model.Ridge(alpha = 0.6)\nregr3.fit(X_train, y_train)\nprint('Coefficients: ', regr3.coef_[0][0])\nprint('Variance score: %.2f' % regr3.score(X_test, y_test))\n", "intent": "Fit the ridge regressor on all the attributes and calculate the fits for a given $\\alpha$\n"}
{"snippet": "param_grid = [{'n_estimators': [25, 50, 100, 200],\n               'max_depth': [16, 32],\n               'min_samples_split' : [2, 4],\n               'min_samples_leaf' : [1],\n               'bootstrap' : [True]}]\nt0 = time()\nrfr = grid_search.GridSearchCV(ensemble.RandomForestRegressor(),\n                               param_grid, cv=5, verbose=0)\nrfr.fit(boston_train, bostony_train)\nprint('Done in %0.3f[s]' %(time() - t0))\n", "intent": "In this example we use a Random Forest Regressor and we choose the best estimator based on Cross Validation score.\n"}
{"snippet": "gbr = ensemble.GradientBoostingRegressor()\nparams = {'n_estimators':[100, 200, 500],\n          'max_depth':[4, 6],\n          'learning_rate':[0.1, 0.01],\n          'subsample':[0.5]}\nt0 = time()\ngrid = grid_search.GridSearchCV(gbr, params, n_jobs=-1)\ngrid.fit(boston_train, bostony_train)\nprint('Done in %0.3f[s]' %(time() - t0))\n", "intent": "In this example we use a Gradient Boosting Regressor Tree and we choose the best estimator based on Cross Validation score.\n"}
{"snippet": "forest = ensemble.RandomForestRegressor(n_estimators=500, max_features=0.3)\nforest.fit(Cal_train, Caly_train)\n", "intent": "In the following example, we use scikit-learn's RandomForestRegressors to asses variable importance. \n"}
{"snippet": "model = Sequential([\n        BatchNormalization(axis=1, input_shape=(3,224,224)),\n        Flatten(),\n        Dense(10, activation='softmax')\n    ])\nmodel.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n                 nb_val_samples=val_batches.nb_sample)\n", "intent": "Our hypothesis was correct. It's nearly always predicting class 1 or 6, with very high confidence. So let's try a lower learning rate:\n"}
{"snippet": "regr3 = linear_model.Ridge(alpha = 0.6)\nregr3.fit(X_train, y_train)\nprint 'Coefficients: ', regr3.coef_\nprint ('Variance score: %.2f' % regr3.score(X_test, y_test))\n", "intent": "Fit the ridge regressor on all the attributes and calculate the fits for a given $\\alpha$\n"}
{"snippet": "param_grid = [{'n_estimators': [25, 50, 100, 200],\n               'max_depth': [16, 32],\n               'min_samples_split' : [2, 4],\n               'min_samples_leaf' : [1],\n               'bootstrap' : [True]}]\nt0 = time()\nrfr = grid_search.GridSearchCV(ensemble.RandomForestRegressor(),\n                               param_grid, cv=5, verbose=0)\nrfr.fit(boston_train, bostony_train)\nprint 'Done in %0.3f[s]' %(time() - t0)\n", "intent": "In this example we use a Random Forest Regressor and we choose the best estimator based on Cross Validation score.\n"}
{"snippet": "gbr = ensemble.GradientBoostingRegressor()\nparams = {'n_estimators':[100, 200, 500],\n          'max_depth':[4, 6],\n          'learning_rate':[0.1, 0.01],\n          'subsample':[0.5]}\nt0 = time()\ngrid = grid_search.GridSearchCV(gbr, params, n_jobs=-1)\ngrid.fit(boston_train, bostony_train)\nprint 'Done in %0.3f[s]' %(time() - t0)\n", "intent": "In this example we use a Gradient Boosting Regressor Tree and we choose the best estimator based on Cross Validation score.\n"}
{"snippet": "batch_size =  \nepochs = \nlr = \nmomentum = \ntrain_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\ndev_loader = torch.utils.data.DataLoader(dev, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)\nmodel = Model()\noptimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum) \n", "intent": "Set your batch size, learning rate, number of epochs etc. Experiment with various hyper parameters.\n"}
{"snippet": "batch_size =  32\nepochs = 25\nlr = 1e-2\nmomentum=.9\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\ndev_loader = torch.utils.data.DataLoader(dev, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)\nmodel = Model()\noptimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n", "intent": "Set your batch size, learning rate, number of epochs etc. Experiment with various hyper parameters.\n"}
{"snippet": "b = torch.Tensor([[1,2,3],[4,5,6]])\nprint(b)\nprint(b.size())\n", "intent": "We can also take an array, and convert it to a tensor.\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS',axis = 1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "mod = KNeighborsClassifier(n_neighbors = 1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "mod.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=1, \n             validation_data=(conv_val_feat, val_labels))\n", "intent": "Now we can train the model as usual, with pre-computed augmented data.\n"}
{"snippet": "msno.matrix(players.sample(500),\n            figsize=(16, 7),\n            width_ratios=(15, 1))\n", "intent": "https://github.com/ResidentMario/missingno\n"}
{"snippet": "msno.matrix(players.sample(500),\n            figsize=(16, 7),\n            width_ratios=(15, 1))\n", "intent": "We've removed 468 players from the table who had no skintone rating.\nLet's look again at the missing data in this table. \n"}
{"snippet": "players.head()\n", "intent": "This looks correlated enough to me -- let's combine the rater's skintone ratings into a new column that is the average rating. \n"}
{"snippet": "sns.distplot(players.skintone, kde=False);\n", "intent": "What is the skintone distribution?\n"}
{"snippet": "sns.distplot(players.height.dropna());\n", "intent": "It's possible that size might correspond with redcards. \n"}
{"snippet": "clean_players.head()\n", "intent": "There are a couple of ways to do this -- set operations and joins are two ways demonstrated below: \n"}
{"snippet": "model = LeNet.build(width=32, height=32, depth=1, classes=2)\nopt = SGD(lr=0.01)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\nmodel.fit(X_train, Y_train, batch_size=10, nb_epoch=300,verbose=1)\n", "intent": "We build the neural network and fit it on the training set\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.5))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def sigmoid_gradient(z):\n    return np.multiply(sigmoid(z), (1 - sigmoid(z)))\n", "intent": "The first thing we need is a function that computes the gradient of the sigmoid function we created earlier.\n"}
{"snippet": "conv1 = Sequential([\n    Embedding(vocab_size, 32, input_length=seq_len, dropout=0.2),\n    Dropout(0.2),\n    Convolution1D(64, 5, border_mode='same', activation='relu'),\n    Dropout(0.2),\n    MaxPooling1D(),\n    Flatten(),\n    Dense(100, activation='relu'),\n    Dropout(0.7),\n    Dense(1, activation='sigmoid')])\n", "intent": "A CNN is likely to work better, since it's designed to take advantage of ordered data. We'll need to use a 1D CNN, since a sequence of words is 1D.\n"}
{"snippet": "svc = svm.SVC(C=100, gamma=10, probability=True)\nsvc\n", "intent": "For this data set we'll build a SVM classifier using the built-in RBF kernel and examine its accuracy on the training data.  \n"}
{"snippet": "x = torch.Tensor([1, 2, 3])\ny = torch.Tensor([4, 5, 6])\nprint(x)\nprint(y)\nw = torch.matmul(x, y)\nprint(w)\n", "intent": "Simple mathematical operations: <b>Addition, Multiplication</b>\n"}
{"snippet": "var_x = autograd.Variable(x, requires_grad=True)\nvar_y = autograd.Variable(y, requires_grad=True)\nvar_z = var_x + var_y\nprint(var_z.grad_fn)\n", "intent": "Now we wrap the torch tensors in <code>autograd.Variable</code>. The <code>var_z</code> contains the information for backpropagation:\n"}
{"snippet": "var_z_data = var_z.data\nnew_var_z = autograd.Variable(var_z_data)\nprint(new_var_z.grad_fn)\n", "intent": "But what happens if we extract the wrapped tensor object out of <code>var_z</code> and re-wrap the tensor in a new <code>autograd.Variable</code>?\n"}
{"snippet": "if torch.cuda.is_available():\n    a = torch.LongTensor(10).fill_(3).cuda()\n    print(type(a))\n    b = a.cpu()\n", "intent": "CUDA\n----\nCheck wether GPU accelaration with **CUDA** is available\n"}
{"snippet": "from keras.layers import Flatten\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras import backend as K\nK.set_image_dim_ordering('th')\n", "intent": "So, let's use Keras to change our ANN from an MLP to an CNN network, and see if we perform better ...\n"}
{"snippet": "w_layer1 = np.random.rand(4)\ndef neuron(x):\n    return sigmoid(x.dot(w_layer1))\n", "intent": "Next, let's create a function, which computed the logistic function for a single datapoint:\n"}
{"snippet": "w_layer2 = np.random.rand(4,4)\ndef layer2(x):\n    return sigmoid(x.dot(w_layer2))\n", "intent": "To create a 2nd layer, we only have to change the weight matrix:\n"}
{"snippet": "def sigmoid(x):\n    return 1/(1+np.exp(-x))\nw_layer1 = np.random.rand(4,4)\ndef layer1(x):\n    return sigmoid(x.dot(w_layer1))\n", "intent": "As a next step, let's vectorize our implementation:\n"}
{"snippet": "graph_in = Input ((vocab_size, 50))\nconvs = [ ] \nfor fsz in range (3, 6): \n    x = Convolution1D(64, fsz, border_mode='same', activation=\"relu\")(graph_in)\n    x = MaxPooling1D()(x) \n    x = Flatten()(x) \n    convs.append(x)\nout = Merge(mode=\"concat\")(convs) \ngraph = Model(graph_in, out) \n", "intent": "We use the functional API to create multiple conv layers of different sizes, and then concatenate them.\n"}
{"snippet": "hidden_out = tf.add(tf.matmul(x, W1), b1)\nhidden_out = tf.nn.relu(hidden_out)\n", "intent": "Next, we have to setup node inputs and activation functions of the hidden layer nodes:\n"}
{"snippet": "params_grid = {'n_estimators': [50, 100, 300, 500], 'max_depth': [2,3,4]}\ngsearch = GridSearchCV(estimator = gbr, param_grid = params_grid,\n                       scoring='neg_mean_absolute_error', n_jobs=4,iid=False, cv=5)\ngsearch.fit(X_train, y_train)\ngsearch.grid_scores_, gsearch.best_params_, gsearch.best_score_\n", "intent": "Lista de scoring functions:\nhttp://scikit-learn.org/stable/modules/model_evaluation.html\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_table = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding_table, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_table = tf.Variable(tf.random_uniform((vocab_size + 1, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding_table, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "my_tree = tree.DecisionTreeClassifier(criterion=\"entropy\")\nmy_tree.fit(X_train,y_train)\n", "intent": "Train a decision tree\n"}
{"snippet": "my_tree = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=2)\nmy_tree = my_tree.fit(X_train,y_train)\n", "intent": "Train a decision tree, limiting its depth to 2\n"}
{"snippet": "param_grid = [\n {'n_estimators': list(range(100, 501, 50)), 'max_features': list(range(1, 10, 2)), 'min_samples_split': [20] }\n]\nmy_tuned_model = GridSearchCV(ensemble.RandomForestClassifier(), param_grid, cv=5)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "param_grid = [\n {}\n]\nmy_tuned_model = GridSearchCV(linear_model.LogisticRegression(), param_grid, cv=5)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "param_grid = [\n {}\n]\nmy_tuned_model = GridSearchCV(neighbors.KNeighborsClassifier(), param_grid, cv=5)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "model = Sequential ([\n    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, weights=[emb]),\n    Dropout (0.2),\n    graph,\n    Dropout (0.5),\n    Dense (100, activation=\"relu\"),\n    Dropout (0.7),\n    Dense (1, activation='sigmoid')\n    ])\n", "intent": "We then replace the conv/max-pool layer in our original CNN with the concatenated conv layers.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(input_dim=784, units=512))\nmodel.add(Activation(\"sigmoid\"))\nmodel.add(Dense(units=207))\nmodel.add(Activation(\"sigmoid\"))\nmodel.add(Dense(units=102))\nmodel.add(Activation(\"sigmoid\"))\nmodel.add(Dense(units=10))\nmodel.add(Activation(\"softmax\"))\n", "intent": "Specfiy the structure of the neural network model\n"}
{"snippet": "def create_model(optimiser = \"rmsprop\", hidden_units = 512):\n    model = Sequential()\n    model.add(Dense(input_dim=784, units=hidden_units))\n    model.add(Activation(\"sigmoid\"))\n    model.add(Dense(units=10))\n    model.add(Activation(\"softmax\"))\n    model.compile(optimizer=optimiser,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n    return model\n", "intent": "Specfiy the structure of the neural network model and training parameters in a function\n"}
{"snippet": "tf_size_factor = tf.Variable(np.random.randn(), name=\"size_factor\")\ntf_price_offset = tf.Variable(np.random.randn(), name=\"price_offset\")\n", "intent": "Define the variables holding size_factor and price_offset <br>\ny = size_factor * x + price_offset\n"}
{"snippet": "W = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n", "intent": "variable weight and biases\n"}
{"snippet": "def weight_variable(shape):\n    initial = tf.truncated_normal(shape=shape, stddev=0.1)\n    return tf.Variable(initial)\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n", "intent": "We are using ReLU activation function so we will set the Weight and biases to some random number otherwise ReLU will give activation function 0.\n"}
{"snippet": "def conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\"SAME\")\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n", "intent": "we use convolution and then pooling to avoid overfitting\n"}
{"snippet": "W_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1)+ b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n", "intent": "Define layers in the CNN\n"}
{"snippet": "keep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n", "intent": "Remember the keep_prob is placeholder, it will be hyperparameter and will get as a training input\n"}
{"snippet": "model = Model(inputs, output)\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n", "intent": "Instantiate the model and compile the model\n"}
{"snippet": "model = Sequential([\n    Embedding(vocab_size, 32, input_length=seq_len, mask_zero=True,\n              W_regularizer=l2(1e-6), dropout=0.2),\n    LSTM(100, consume_less='gpu'),\n    Dense(1, activation='sigmoid')])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "We haven't covered this bit yet!\n"}
{"snippet": "model = LinearRegression()\nx = df[[\"TV\"]]\nmodel.fit(x,df[\"Sales\"])\n", "intent": "Will will try building a simple linear model to predict Sales based on the TV budget spend:\n"}
{"snippet": "x = df[[\"Newspaper\"]]\nmodel = LinearRegression()\nmodel.fit(x,df[\"Sales\"])\n", "intent": "We can repeat the same process using a different features, such as newspaper budget spend:\n"}
{"snippet": "model = LinearRegression()\nmodel.fit(x,df[\"Sales\"])\n", "intent": "Now use all 3 input variables to fit linear regression model:\n"}
{"snippet": "model = KNeighborsClassifier(n_neighbors=3)\nmodel.fit(dataset_train, target_train)\nprint(model)\n", "intent": "Next, we will fit a k-nearest neighbor model to the data using $k=3$ nearest neighbours:\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=3)\nmodel.fit(data_train, target_train)\nprint(model)\n", "intent": "Now we will build a KNN classifier ($k=3$) as we have seen previously. Note that we only use the training data to build the model:\n"}
{"snippet": "model = GridSearchCV(LogisticRegressionCV(solver='liblinear'), {'cv':np.arange(2, 17, 1), 'Cs':np.arange(1, 12, 1), 'penalty': ['l1']},\n                    n_jobs=-1, verbose=1)\nmodel.fit(X, y)\nmodel.best_estimator_\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "k = 2\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(df2)\n", "intent": "Cluster the Data to our our target groups. **BONUS**: Perform multiple clustering tests on various indicator variables\n"}
{"snippet": "model = naive_bayes.MultinomialNB()\nmodel.fit(X_train, y_train)\n", "intent": "Instantiate the Naive Bayes predictor from scikit-learn with the training data. \n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n", "intent": "Define a logistic regression and train it with the feature and target set\n"}
{"snippet": "model = simple_gallup_model(gallup_2012)\nmodel\n", "intent": "Now, we run the simulation with this model, and plot it.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "logreg.fit(X_training, y_training.ravel())\n", "intent": "<hr />\n<b>Then we need to fit the training data to the model</b>\n"}
{"snippet": "from sklearn import tree\nmodel = tree.DecisionTreeClassifier()\n", "intent": "Lets initialize our model so we can take a look at it's attributes.\n"}
{"snippet": "from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = ExtraTreesClassifier(n_estimators=400)\nclf = clf.fit(train, targets)\n", "intent": "<hr/>\n<img style=\"width:400px;\" src=\"http://labs.centerforgov.org/Analytics-Training/images/data_prep.png\" />\n<hr/>\n"}
{"snippet": "grid_search = GridSearchCV(forest, param_grid=parameter_grid, cv=cross_validation)\ngrid_search.fit(train_new, targets)\nprint('Best score : {}'.format(grid_search.best_score_))\nprint('Best parameters : {}'.format(grid_search.best_params_))\n", "intent": "<hr/>\n<img style=\"width:450px;\" src=\"https://thinkr.fr/wp-content/uploads/random-forest.jpg\" />\n<hr/>\n"}
{"snippet": "from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = ExtraTreesClassifier(n_estimators=400)\nclf = clf.fit(train, targets)\n", "intent": "<img style=\"width:400px;\" src=\"http://labs.centerforgov.org/Analytics-Training/images/data_prep.png\" />\n<br/><br/><hr/><br/><br/>\n"}
{"snippet": "grid_search = GridSearchCV(forest, param_grid=parameter_grid, cv=cross_validation)\ngrid_search.fit(train_new, targets)\nprint('Best score : {}'.format(grid_search.best_score_))\nprint('Best parameters : {}'.format(grid_search.best_params_))\n", "intent": "<br/><br/><hr/><br/><br/>\n<img style=\"width:400px;\" src=\"https://thinkr.fr/wp-content/uploads/random-forest.jpg\" />\n"}
{"snippet": "X = T.vector()\n", "intent": "Numpy style indexing\n===========\n"}
{"snippet": "cnn_layers = googlenet.build_model()\ncnn_input_var = cnn_layers['input'].input_var\ncnn_feature_layer = cnn_layers['loss3/classifier']\ncnn_output_layer = cnn_layers['prob']\nget_cnn_features = theano.function([cnn_input_var], lasagne.layers.get_output(cnn_feature_layer))\n", "intent": "Build the model and select layers we need - the features are taken from the final network layer, before the softmax nonlinearity.\n"}
{"snippet": "model = uncertain_gallup_model(gallup_2012)\nmodel = model.join(electoral_votes)\n", "intent": "We construct the model by estimating the probabilities:\n"}
{"snippet": "knn_final = KNeighborsRegressor(n_neighbors=10, weights='distance', metric='euclidean', n_jobs=-1)\nknn_final.fit(X, y)\n", "intent": "If you are happy with your model we can re-train it using all observations, and then use it to make predictions.\n"}
{"snippet": "model = applications.VGG16(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\nfor layer in model.layers[:5]:\n    layer.trainable = False\nx = model.output\nx = Flatten()(x)\nx = Dense(1024, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\nx = Dense(1024, activation=\"relu\")(x)\npredictions = Dense(2, activation=\"softmax\")(x)\nmodel_final = Model(input = model.input, output = predictions)\n", "intent": "Now we will invoke the VGG16 pretrained model, not including the top flattening layers.\n"}
{"snippet": "model.fit(X['train'], y['train'], batch_size=512, epochs=10, validation_split=0.08)\n", "intent": "Now it's time to run the model and adjust the weights. The model fitter will use 8% of the dataset values as the validation set.\n"}
{"snippet": "vector = np.arange(-5,5,0.1)\ndef relu(x):\n    return max(0.,x)\nrelu = np.vectorize(relu)\n", "intent": "- 28x28=728 elements in input layer\n- 10 elements in output layer\n- 3 hidden layers: 350, 200, 100\n"}
{"snippet": "tf_keras.activations.relu(inputs)\ntf_keras.activations.sigmoid(inputs)\ntf_keras.activations.softmax(inputs)\ntf_keras.activations.tanh(inputs)\ntf_keras.activations.elu(inputs)\ntf_keras.activations.hard_sigmoid(inputs)\ntf_keras.activations.softplus(inputs)\ntf_keras.activations.softsign(inputs)\ntf_keras.activations.linear(inputs)\n", "intent": "This is a layer of neurons that applies the non-saturating activation function. It increases the nonlinear properties of the decision function\n"}
{"snippet": "model.fit(x=X_train, y=Y_train, batch_size=64,\n          verbose=1, epochs=6, validation_data=(X_test,Y_test))\n", "intent": "To train our CNN, we simply need to call the fit function.\nLet's pass in both our train and test data and set the batch size to 64.\n"}
{"snippet": "def compile_model(model):\n    loss = tf_keras.losses.categorical_crossentropy\n    optimizer = tf_keras.optimizers.Adam(lr=hyper_params[\"learning_rate\"])\n    metrics = [tf_keras.metrics.categorical_accuracy,\n               tf_keras.metrics.top_k_categorical_accuracy]\n    model.compile(loss = loss,\n                  optimizer = optimizer,\n                  metrics = metrics)\n    print(model.summary())\n    return model\n", "intent": "Add the loss function, the optimizer, and the metrics to the model.\n"}
{"snippet": "def updateTargetGraph(tfVars,tau):\n    total_vars = len(tfVars)\n    op_holder = []\n    for idx,var in enumerate(tfVars[0:total_vars//2]):\n        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n    return op_holder\ndef updateTarget(op_holder,sess):\n    for op in op_holder:\n        sess.run(op)\n", "intent": "These functions allow us to update the parameters of our target network with those of the primary network.\n"}
{"snippet": "def update_target_graph(from_scope, to_scope):\n    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n    op_holder = []\n    for from_var,to_var in zip(from_vars,to_vars):\n        op_holder.append(to_var.assign(from_var))\n    return op_holder\n", "intent": "These functions allow us to update the parameters of our target network with those of the primary network.\n"}
{"snippet": "MLP_G = Sequential([\n    Dense(200, input_shape=(100,), activation='relu'),\n    Dense(400, activation='relu'),\n    Dense(784, activation='sigmoid'),\n])\n", "intent": "We'll keep thinks simple by making D & G plain ole' MLPs.\n"}
{"snippet": "X = iris[[\"petal_length\", \"setosa\", \"virginica\"]]\nX = sm.add_constant(X) \ny = iris[\"petal_width\"]\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n", "intent": "Now we perform a multilinear regression with the dummy variables added.\n"}
{"snippet": "accuracy = DecisionTreeClassifier(max_depth=5).fit(X_train, y_train).score(X_test, y_test)\nprint('Model\\'s classification accuracy on the test set: {0}%'.format(accuracy*100))\n", "intent": "**1.3** Report the model's classification accuracy on the test set.\n"}
{"snippet": "i = 5    \ns = 2    \nk = 3    \np = 0    \nx = torch.randn(1,1,i,i)\ncl = nn.Conv2d(1,1,k,s,padding=p)\nprint('Output size:                    ', cl(x).size())\nprint('Expected (spatial) output size: ', np.floor((i-k)/s).astype(np.int)+1)\n", "intent": "We expect (**Relationship 5**)\n$$ o = \\lfloor (i-k)/s \\rfloor +1$$\n"}
{"snippet": "i = 4    \ns = 1    \nk = 3    \np = 0    \ntcl = nn.ConvTranspose2d(1,1,k,s,p)\nprint(tcl(out).size())\n", "intent": "Lets implement the same using PyTorch's `nn.ConvTranspose2d` layer.\n"}
{"snippet": "i = 9   \ns = 2    \nk = 3    \np = 1    \nx = torch.randn(1,1,i,i)\ncl = nn.Conv2d(1,1,k,s,p)\nout = cl(x)\nprint(out.size())\ntcl = nn.ConvTranspose2d(1,1,k,s,p)\nprint(tcl(out).size())\n", "intent": "Here, we assume, $i-2p+k$ is a multiple of $s$ (otherwise, we have the same problem as mentioned above).\n"}
{"snippet": "simp_model = resnet18()                           \nsimp_model.load_state_dict(simp_model_state_dict) \nfreeze_model(simp_model)                          \nsimp_model.fc = nn.Linear(simp_model.fc.in_features, 2) \nprint(simp_model.fc.weight.requires_grad)\n", "intent": "Next, we replace the last layer of our model!\n"}
{"snippet": "a = ll.weight.data.numpy().flatten()\nb = x.data.numpy()\nprint('Dot product:', (a*b).sum())\nassert(np.abs(ll(x).item() -(a*b).sum())<1e-9)\n", "intent": "Lets quickly check the result, by computing the dot product $\\langle w,x\\rangle$:\n"}
{"snippet": "hl = nn.Linear(2,2, bias=False)\nx = torch.randn(2)\nprint(\"Output:\", hl(x))\nprint(hl.weight)\na = hl.weight.data.numpy()\nprint(\"Output 1: {:.4f}\".format((a[0,:]*x.data.numpy()).sum()))\nprint(\"Output 2: {:.4f}\".format((a[1,:]*x.data.numpy()).sum()))\n", "intent": "In fact, we have now one neuron that takes 2 inputs and produces 1 output. Now, lets try to have two 2 neurons, each producing one output:\n"}
{"snippet": "from exercise_code.model_savers import save_test_model\nsave_test_model(model)\n", "intent": "Now you need to save the model. We provide you with all the functionality, so you will only need to execute the next cell.\n"}
{"snippet": "CNN_D = Sequential([\n    Convolution2D(256, 5, 5, subsample=(2,2), border_mode='same', \n                  input_shape=(28, 28, 1), activation=LeakyReLU()),\n    Convolution2D(512, 5, 5, subsample=(2,2), border_mode='same', activation=LeakyReLU()),\n    Flatten(),\n    Dense(256, activation=LeakyReLU()),\n    Dense(1, activation = 'sigmoid')\n])\nCNN_D.compile(Adam(1e-3), \"binary_crossentropy\")\n", "intent": "The discriminator uses a few downsampling steps through strided convolutions.\n"}
{"snippet": "def get_gradient(X, y, w, mini_batch_indices, lmbda):\n    x0 = np.zeros((mini_batch_indices.shape[0], X.shape[1]))\n    y0 = np.zeros(mini_batch_indices.shape[0])\n    for i in range(len(mini_batch_indices)):\n        x0[i] = X[mini_batch_indices[i],:]\n        y0[i] = y[mini_batch_indices[i]]\n    dw = np.zeros(w.shape[0])\n    dw = x0.T.dot(sigmoid(x0.dot(w))-y0)/len(y0)+lmbda*w    \n    return dw\n", "intent": "Make sure that you compute the gradient of the loss function $\\mathcal{L}(\\mathbf{w})$ (not simply the NLL!)\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(256, input_dim=x_train.shape[1]))\nmodel.add(Activation('softmax'))\nmodel.add(Dropout(0.8))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=10, validation_data=(x_test,y_test), verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = KNeighborsClassifier(10)\nmodel.fit(X_train, Y_train)\ndata_tools.Decision_Surface(X, Y, model, cell_size=.05, surface=True, points=False)\n", "intent": "Now, let's try setting the number of neighbors to use, $k$, to a few different values and look at the results.\n"}
{"snippet": "W_conv2 = weight_variable([2, 2, 32, 64])  \nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n", "intent": "In order to build a deep network, we stack several layers of this type. The second layer will have 64 features for each 5x5 patch.\n"}
{"snippet": "W_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n", "intent": "Finally, we add a softmax layer, just like for the one layer softmax regression above.\n"}
{"snippet": "km = KMeans(n_clusters=4, random_state=1)\nkm.fit(X_scaled)\nbeer['cluster'] = km.labels_\nbeer.sort('cluster')\n", "intent": "Ignore past 6. Optimum number is 4. \n"}
{"snippet": "feature_cols = ['Pclass','Parch']\nX = titanic[feature_cols]\ny = titanic.Survived\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n", "intent": "Define **Pclass** and **Parch** as the features, and **Survived** as the response.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntreeclf = DecisionTreeClassifier(max_depth = 7, random_state=1)\ntreeclf.fit(X, y)\n", "intent": "Use 10-fold cross-validation to evaluate a decision tree model with those same features (fit to any \"max_depth\" you choose).\n"}
{"snippet": "PATH = 'data/cifar10/'\ndata = datasets.CIFAR10(root=PATH, download=True,\n   transform=transforms.Compose([\n       transforms.Scale(sz),\n       transforms.ToTensor(),\n       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n   ])\n)\n", "intent": "Pytorch has the handy [torch-vision](https://github.com/pytorch/vision) library which makes handling images fast and easy.\n"}
{"snippet": "km = KMeans(n_clusters=2, n_init=20)\nkm.fit(X_train)\n", "intent": "**Because the diabetes results are binary (either you have it or you dont) then we must use k=2. i.e. we are only looking for 2 populations**\n"}
{"snippet": "logreg = LogisticRegression(penalty='l1', C=10)\n", "intent": "penalty: 'l1' for lasso, 'l2' for ridge\n"}
{"snippet": "neigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(data, target) \n", "intent": "Set kNN's k value to 3 and fit the data\n"}
{"snippet": "classifier = LogisticRegression()\nclassifier.fit(train_arrays, train_labels)\n", "intent": "Now we train a logistic regression classifier using the training data.\n"}
{"snippet": "from keras.layers import Convolution2D, MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "from sklearn.svm import SVC \nmodel = SVC(kernel = 'linear', C = 1E10)\nmodel.fit(X, y)\n", "intent": "In SVMs the line that maximizis the margin the more is the one we will choose\n"}
{"snippet": "gmm = GMM(110, covariance_type='full', random_state=0)\ngmm.fit(data)\nprint(gmm.converged_)\n", "intent": "It appears that around 110 components minimizes the AIC\n"}
{"snippet": "sess = tf.Session()\nsess.run(init)\n", "intent": "This is the session where calculations encoded in the graph will\ntake place.\n"}
{"snippet": "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n", "intent": "We then convolve x_image with the weight tensor, add the bias, apply the\nReLU function, and finally max pool.\n"}
{"snippet": "from torch import FloatTensor as FT\ndef Var(*params): return Variable(FT(*params).cuda())\n", "intent": "Just some shortcuts to create tensors and variables.\n"}
{"snippet": "from sklearn.cluster import KMeans\nkmeans = KMeans(nb_clusters, random_state=8)\nY_hat = kmeans.fit(X).labels_\n", "intent": "Normally, you do not know the information in Y, however.\nYou could try to recover it from the data alone.\nThis is what the kMeans algorithm does.\n"}
{"snippet": "model.fit(x_train,y_train,epochs=10,batch_size=100)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform(dtype=tf.float32,shape=[vocab_size,embed_dim],minval=-.1,maxval=.1))\n    embed = tf.nn.embedding_lookup(params=embedding,ids=input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "get_predictions(index=0, \n                partition='train',\n                input_to_softmax=final_model(input_dim=161,units=200, recur_layers=2), \n                model_path='results/model_end.h5')\n", "intent": "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='validation',\n                input_to_softmax=final_model(input_dim=161,units=200, recur_layers=2), \n                model_path='results/model_end.h5')\n", "intent": "Use the next code cell to visualize the model's prediction for the first example in the validation dataset.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1000))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "history = model.fit(x_train, y_train, epochs=10, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = Sequential()\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "model = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n", "intent": "In the next two steps, we build the decision tree model from the training set and export it to a file for viewing in GraphViz.\n"}
{"snippet": "K.set_value(m_final.optimizer.lr, 1e-4)\nm_final.fit([arr_lr, arr_hr], targ, 16, 3, **parms)\n", "intent": "We use learning rate annealing to get a better fit.\n"}
{"snippet": "import statsmodels.formula.api as smf\nlm = smf.ols(formula='Sales ~ TV', data=data).fit()\nlm.summary()\n", "intent": "Let's use `Statsmodels` to estimate the associations between advertising efforts and sales. \n"}
{"snippet": "kmeans=KMeans(n_clusters=2,verbose=5)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "kmeans.fit(collage.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "lm=LinearRegression()\n", "intent": "Now its time to train our model on our training data!\n** Import LinearRegression from sklearn.linear_model **\n"}
{"snippet": "logmodel.fit(X_train,y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n", "intent": "Score and plot your predictions. What do these results tell us?\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\nmodel.add(Dropout(.75))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=200)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "top_model = Model(inp, outp)\n", "intent": "We are only interested in the trained part of the model, which does the actual upsampling.\n"}
{"snippet": "lm = smf.ols(formula='sales ~ TV + radio + newspaper', data=data).fit()\nprint lm.pvalues\n", "intent": "We could try a model with all features, and only keep features in the model if they have **small p-values**:\n"}
{"snippet": "lm = smf.ols(formula='sales ~ TV + radio', data=data).fit()\nlm.rsquared\n", "intent": "We could try models with different sets of features, and **compare their R-squared values**:\n"}
{"snippet": "import tensorflow as tf\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n", "intent": "** Import tensorflow.contrib.learn.python.learn as learn**\n"}
{"snippet": "data_dict_expanded = []\nfor transaction in data_dict:\n    for det in transaction['dets']:\n        data_dict_expanded.append(transaction.copy())\n        data_dict_expanded[-1].pop('dets', None)\n        data_dict_expanded[-1]['det'] = flatten(det, '.')\n        data_dict_expanded[-1]['num_items'] = len(transaction['dets'])\nprint_pretty_json(data_dict_expanded[0])\n", "intent": "__And expand the data dictionary over each det__\n"}
{"snippet": "df['ds'] = df.index\nmy_model.fit(df)\n", "intent": "and fit it with our dataframe df\n"}
{"snippet": "happyModel.fit(x=X_train,y=Y_train,epochs=5,batch_size=10)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "test_input = [[0.25, 0.15]]\ngraph=tf.Graph()\nwith tf.Session() as session:\n    tf.global_variables_initializer().run(session=session)\n    output = session.run([out],feed_dict=feed_dict) \n    print(output[0]) \n", "intent": "As with the previous example, we can use `session.run()` to execute this computation graph, and use a `feed_dict` to feed in our input:\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 2))\nnet.add(SoftMax())\nnet.add(Linear(2, 4))\nnet.add(ReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\ncriterion = MSECriterion()\nprint(net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "clz = DBSCAN(min_samples=4, eps=21.2)\nclz.fit(X)\n", "intent": "{'min_samples': 4, 'eps': 21.2}\n"}
{"snippet": "inp = Input((288,288,3))\nref_model = Model(inp, ReflectionPadding2D((40,2))(inp))\nref_model.compile('adam', 'mse')\n", "intent": "Testing the reflection padding layer:\n"}
{"snippet": "tf.reset_default_graph()\nbatch_size = 32\nwith tf.device('/cpu:0'):\n    real_images, one_hot_labels, _ = data_provider.provide_data(\n        'train', batch_size, MNIST_DATA_DIR)\ncheck_real_digits = tfgan.eval.image_reshaper(real_images[:20,...], num_cols=10)\nvisualize_digits(check_real_digits)\n", "intent": "<a id='conditional_input'></a>\n"}
{"snippet": "tf.reset_default_graph()\nbatch_size = 32\nwith tf.device('/cpu:0'):\n    real_images, _, _ = data_provider.provide_data(\n        'train', batch_size, MNIST_DATA_DIR)\ncheck_real_digits = tfgan.eval.image_reshaper(real_images[:20,...], num_cols=10)\nvisualize_digits(check_real_digits)\n", "intent": "<a id='infogan_input'></a>\nThis is the same as the unconditional case (we don't need labels).\n"}
{"snippet": "new_model = keras.models.load_model('my_model.h5')\nnew_model.summary()\n", "intent": "Now recreate the model from that file:\n"}
{"snippet": "model = models.Model(inputs=[inputs], outputs=[outputs])\n", "intent": "Using functional API, you must define your model by specifying the inputs and outputs associated with the model. \n"}
{"snippet": "history = model.fit(train_ds, \n                   steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))),\n                   epochs=epochs,\n                   validation_data=val_ds,\n                   validation_steps=int(np.ceil(num_val_examples / float(batch_size))),\n                   callbacks=[cp])\n", "intent": "Don't forget to specify our model callback in the `fit` function call. \n"}
{"snippet": "import tensorflow.contrib.eager as tfe\nw = tfe.Variable(3.0)\nwith tf.GradientTape() as tape:\n  loss = w ** 2\ndw, = tape.gradient(loss, [w])\nprint(\"\\nYou can use `tf.GradientTape` to compute the gradient of a \"\n      \"computation with respect to a list of `tf.contrib.eager.Variable`s;\\n\"\n      \"for example, `tape.gradient(loss, [w])`, where `loss` = w ** 2 and \"\n      \"`w` == 3.0, yields`\", dw,\"`.\")\n", "intent": "Create variables with `tf.contrib.eager.Variable`, and use `tf.GradientTape`\nto compute gradients with respect to them.\n"}
{"snippet": "from sklearn.dummy import DummyClassifier\nclf = DummyClassifier('most_frequent')\nclf.fit(train_data_finite, train_labels)\nprint(\"predcition accuracy: %f\" % clf.score(test_data_finite, test_labels))\n", "intent": "- most common dummy, if we predict all most_frequent we'd get this many predictions correct\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\n", "intent": "<img src=\"figures/supervised_workflow.svg\" width=\"100%\">\n"}
{"snippet": "model = svm.SVC(kernel='linear')\nmodel.fit(ingredients, type_label)\n", "intent": "__Step 4:__ Fit the Model\n"}
{"snippet": "class Evaluator(object):\n    def __init__(self, f, shp): self.f, self.shp = f, shp\n    def loss(self, x):\n        loss_, self.grad_values = self.f([x.reshape(self.shp)])\n        return loss_.astype(np.float64)\n    def grads(self, x): return self.grad_values.flatten().astype(np.float64)\n", "intent": "Since we use scipy bfgs for gradient descent, we need to send ti loss function and gradients seperately.\n"}
{"snippet": "from sklearn.neural_network import MLPRegressor\nmodelo_MLP=MLPRegressor(hidden_layer_sizes=(13),activation='relu',solver='adam',alpha=0.0001)\nmodelo_MLP.fit(X_train,Y_train)\n", "intent": "* Train a regression model using MLPRegressor in order to predict the output variable MEDV.\n"}
{"snippet": "centr_model = LinearRegression()\ncentr_model.fit(X_train_s,Y_train)\n", "intent": "1. Train a linear regression model using the scaled data.\n"}
{"snippet": "from sklearn.neural_network import MLPRegressor\nmodelo_MLP_cent=MLPRegressor(hidden_layer_sizes=(128,512),activation='relu',solver='adam',alpha=0.0001)\nmodelo_MLP_cent.fit(X_train_s,Y_train)\n", "intent": "2. Train a regression model using a 2-layer MultiLayer Perceptron (128 neurons in the first and 512 in the second) and with the scaled data.\n"}
{"snippet": "import tensorflow as tf\na = tf.constant(5, name='a')\nb = tf.constant(3, name='b')\nc = tf.add(a, b)\nwith tf.Session() as sess:\n    print(sess.run(c))\n    writer = tf.summary.FileWriter('tmp/', sess.graph)\nwriter.close()\n", "intent": "- Basic operations\n- Tensor types\n- Project speed dating\n- Placeholders and feeding inputs\n- Lazy loading\n"}
{"snippet": "random_normal = tf.random_normal([1]) \ntruncated_normal = tf.truncated_normal([1]) \nwith tf.Session() as sess:\n    print('random_normal = \\n', str(sess.run(random_normal)))\n    print('truncated_normal = \\n', str(sess.run(truncated_normal)))\n", "intent": "Randomly Generated Constants\n"}
{"snippet": "from tensorflow.python.framework import ops\nops.reset_default_graph()\ng=tf.get_default_graph()\n[op.name for op in g.get_operations()]\n", "intent": "- first clear default graph\n"}
{"snippet": "tf.reset_default_graph()\ndimensions = [512, 256, 128, 64]             \nn_features = mnist.train.images.shape[1]     \nX = tf.placeholder(tf.float32, [None, n_features])  \n", "intent": "<img src='pic/autoencoder_0.png',height=400, width = 900>\n"}
{"snippet": "from tensorflow.python.framework import ops\nops.reset_default_graph()\ng=tf.get_default_graph()\n[op.name for op in g.get_operations()]\n", "intent": "- using 1 layer first\n"}
{"snippet": "def init_weight(shape):\n    init_random_weight = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(init_random_weight)\ndef init_bais(shape):\n    init_random_bais = tf.constant(0.1, shape=shape)\n    return tf.Variable(init_random_bais)\n", "intent": "1. Helper\n2. INIT Weight\n3. INIT Bias\n4. CONV 2D\n5. Pooling\n"}
{"snippet": "K.set_value(m_sr.optimizer.lr, 1e-4)\nm_sr.fit([arr_lr, arr_hr], targ, 16, 1, **parms)\n", "intent": "We use learning rate annealing to get a better fit.\n"}
{"snippet": "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n", "intent": "<h4>5.3.1.2. Testing the model with best hyper paramters</h4>\n"}
{"snippet": "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n", "intent": "<h4>5.3.2.2. Testing model with best hyper parameters</h4>\n"}
{"snippet": "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42,class_weight='balanced')\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)\n", "intent": "<h3>5.4.2. Testing model with best hyper parameters</h3>\n"}
{"snippet": "clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)\n", "intent": "<h3>5.5.2. Testing model with best hyper parameters (One Hot Encoding)</h3>\n"}
{"snippet": "clf = RandomForestClassifier(max_depth=max_depth[int(best_alpha%4)], n_estimators=alpha[int(best_alpha/4)], criterion='gini', max_features='auto',random_state=42)\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y,cv_x_responseCoding,cv_y, clf)\n", "intent": "<h3>5.5.4. Testing model with best hyper parameters (Response Coding)</h3>\n"}
{"snippet": "rgrs = LinearRegression()\nrgrs.fit(x_tr, y_tr)\n", "intent": "d) Train model on training data, and make predictions on testing data\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable( tf.random_uniform([vocab_size, embed_dim], -1, 1) )\n    embeded = tf.nn.embedding_lookup(embedding, input_data)\n    return embeded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from distutils.version import LooseVersion\nimport warnings\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nassert LooseVersion(tf.__version__) == LooseVersion('1.1.0'), 'Please use TensorFlow version 1.1'\nprint('TensorFlow Version: {}'.format(tf.__version__))\nif not tf.test.gpu_device_name():\n    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\nelse:\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n", "intent": "This will check to make sure you have the correct version of TensorFlow and access to a GPU\n"}
{"snippet": "inp = Input((288,288,3))\nref_model = Model(inp, ReflectionPadding2D((40,10))(inp))\nref_model.compile('adam', 'mse')\n", "intent": "Testing the reflection padding layer:\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "Get Linear Regression model object\n"}
{"snippet": "lm.fit(x, y)\n", "intent": "Fit linear regression model on data\n"}
{"snippet": "def sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n", "intent": "Define sigmoid function. <br>\n           1. Input: An array.\n           2. Output: Sigmoid of Input.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1024, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=200, batch_size=100, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from distutils.version import LooseVersion\nimport tensorflow as tf\ntf.reset_default_graph()\nassert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\nprint('TensorFlow Version: {}'.format(tf.__version__))\n", "intent": "This will check to make sure you have the correct version of TensorFlow\n"}
{"snippet": "one_class_clf = OneClassSVM(nu=.00101001, gamma=0.000010, kernel='rbf')\none_class_clf.fit(X_resampled)\n", "intent": "Here I tried some inputs nu and gamma. <br/>\nThe following one was the highest auc that I achieved\n"}
{"snippet": "gauss_clf = GaussianMixture(3)\ngauss_clf.fit(X_resampled)\n", "intent": "Let's see a histogram of the classifier with the best AuC score.\n"}
{"snippet": "t_clf = t_mix(n_components=1, random_state=0)\nt_clf.fit(X_resampled)\n", "intent": "Let's try with the t distribution, and compare them\n"}
{"snippet": "import theano\nimport theano.tensor as T\ncurrent_states = T.matrix(\"states[batch,units]\")\nactions = T.ivector(\"action_ids[batch]\")\nrewards = T.vector(\"rewards[batch]\")\nnext_states = T.matrix(\"next states[batch,units]\")\nis_end = T.ivector(\"vector[batch] where 1 means that session just ended\")\n", "intent": "First step is initializing input variables\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embbeding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embbeding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0))\n    return tf.nn.embedding_lookup(embeddings, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embeddings, ids=input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = ARIMA(series, order=(5,1,0))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())\n", "intent": " we can see that there is a positive correlation with the first 10-to-12 lags that is perhaps significant for the first 5 lags.\n"}
{"snippet": "session = tf.Session()\n", "intent": "We now create a $TensorFlow$ session which is used to execute the graph.\n"}
{"snippet": "history_new_cont = model_new.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch-775,\n                                 verbose=0, validation_data=(X_test, Y_test), callbacks=callbacks_list_new)\nmodel_new.load_weights(checkpoints_filepath_new)\n", "intent": "After epoch 475 nothing will be saved because precision doesn't increase anymore.\n"}
{"snippet": "def discriminator(x, n_units=128, reuse=False, alpha=0.01):\n    with tf.variable_scope('discriminator', reuse=reuse):\n        h1 = tf.layers.dense(x, n_units, activation=None)\n        h1 = tf.maximum(alpha * h1, h1)\n        h2 = tf.layers.dense(h1, n_units / 2, activation=None)\n        h2 = tf.maximum(alpha * h2, h2)\n        logits = tf.layers.dense(h2, 1, activation=None)\n        out = tf.sigmoid(logits)\n        return out, logits\n", "intent": "The discriminator network is almost exactly the same as the generator network, except that we're using a sigmoid output layer.\n"}
{"snippet": "def discriminator(x, n_units=128, reuse=False, alpha=0.01):\n    with tf.variable_scope('discriminator', reuse=reuse):\n        h1 = tf.layers.dense(x, n_units, activation=None)\n        h1 = tf.maximum(alpha * h1, h1)\n        h2 = tf.layers.dense(h1, n_units, activation=None)\n        h2 = tf.maximum(alpha * h2, h2)\n        logits = tf.layers.dense(h1, 1, activation=None)\n        out = tf.sigmoid(logits)\n        return out, logits\n", "intent": "The discriminator network is almost exactly the same as the generator network, except that we're using a sigmoid output layer.\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingRegressor\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\ngbrt.fit(X, y)\n", "intent": "Now lets apply a Gradient Boosting Regressor and see how it functions\n"}
{"snippet": "import numpy as np\nimport tensorflow as tf\nm1 = np.array([[1., 2.], [3., 4.], [5., 6.], [7., 8.]], dtype=np.float32)\nm1_input = tf.placeholder(tf.float32, shape=[4, 2])\nm2 = tf.Variable(tf.random_uniform([2, 3], -1.0, 1.0))\nm3 = tf.matmul(m1_input, m2)\nm3 = tf.Print(m3, [m3], message=\"m3 is: \")\ninit = tf.global_variables_initializer()\n", "intent": "A jupyter notebook version of the simple 'starter' example.\nFirst, define a constant, and define the TensorFlow graph.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n", "intent": "The library is same as it was for Simple Linear Regression\n"}
{"snippet": "model = nn.Sequential()\nmodel.add_module('flatten', Flatten())\nmodel.add_module('dense1', nn.Linear(3 * 32 * 32, 64))\nmodel.add_module('dense1_relu', nn.ReLU())\nmodel.add_module('dense2_logits', nn.Linear(64, 10)) \n", "intent": "Let's start with a dense network for our baseline:\n"}
{"snippet": "input_image = Variable(content_img.clone().data, requires_grad=True)\noptimizer = torch.optim.LBFGS([input_image])\n", "intent": "We can now optimize both style and content loss over input image.\n"}
{"snippet": "regr.fit(X, Y);\nprint \"Aerogerador (Single):\", r_squared(regr.intercept_, regr.coef_, X, Y), \"\\n\\n\\n\";\n", "intent": "2. Execute o notebook usando agora o dataset aerogerador na pasta do github.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(units=512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units=256, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units=2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, \n          y_train, \n          epochs = 15,\n          batch_size = 64,\n          shuffle=True,\n          verbose = 2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model.fit(x_train, \n          y_train, \n          epochs = 10,\n          batch_size = 64,\n          shuffle=True,\n          verbose = 2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def random_forest_scores(n_iterations):\n    test_scores = []\n    for n in np.arange(n_iterations):\n        rf = RandomForestClassifier()\n        rf.fit(train_data, train_target)\n        test_scores.append(rf.score(test_data, test_target))\n    return pd.Series(test_scores)\n", "intent": "Let's try random forest next to get a comparison.\n"}
{"snippet": "data = import_data('train.csv')\ndata = preprocess_data(data)\nrf = RandomForestClassifier()\nrf.fit(data.drop(['Survived'], axis=1), data.Survived)\n", "intent": "Train the competition classifier.\n"}
{"snippet": "with tf.Session() as session:\n    init.run()\n    print(\"Initialized\")\n    print(\"m2: {}\".format(m2))\n    print(\"eval m2: {}\".format(m2.eval()))\n    feed_dict = {m1_input: m1}\n    result = session.run([m3], feed_dict=feed_dict)\n    print(\"\\nresult: {}\\n\".format(result))\n", "intent": "Then, run the graph in a `session`, specifying a value for the `m1_input` placeholder.\n"}
{"snippet": "housingModel = createCompileFitModel(X_train, y_train, learning_rate=0.01, epochs=500)\n", "intent": "Increasing the learning rate and number of epochs\n"}
{"snippet": "model.fit(X_train, y_train_cat, batch_size=128,\n          epochs=2, verbose=1, validation_split=0.3)\n", "intent": "Tons of parameters. That's gonna take sometime to train\n"}
{"snippet": "model.fit(X_train, y_train_cat, batch_size=128,\n          epochs=2, verbose=1, validation_split=0.3)\n", "intent": "As you can see there are fewer parameters\n"}
{"snippet": "model = Sequential()\nmodel.add(Embedding(input_dim=max_features, output_dim=100, embeddings_initializer=\"glorot_uniform\", input_length=maxlen))\nmodel.add(LSTM(20, return_sequences=False)) \nmodel.add(Dense(46))  \nmodel.add(Activation('softmax'))\nnp.random.seed(123)\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['accuracy'])\n", "intent": "<img src='../imgs/learning_a.png'/>\n"}
{"snippet": "model = Sequential()\nmodel.add(Embedding(input_dim=max_features, output_dim=500, embeddings_initializer=\"glorot_uniform\", input_length=maxlen))\nmodel.add(LSTM(256, return_sequences=False, dropout_W=0.2, dropout_U=0.2)) \nmodel.add(Dense(46))  \nmodel.add(Activation('softmax'))\nnp.random.seed(123)\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['accuracy'])\n", "intent": "<img src='../imgs/learning_b.png'/>\n"}
{"snippet": "dbscn = DBSCAN(eps = .6, min_samples = 5).fit(X)\n", "intent": "Let's use 5 min points (num dimensions + 1)\n"}
{"snippet": "def model( inputs, h, b ):\n    lastY = inputs\n    for i, (hi, bi) in enumerate( zip( h, b ) ):\n        y =  tf.add( tf.matmul( lastY, h[i]), b[i] )    \n        if i==len(h)-1:\n            return y\n        lastY =  tf.nn.sigmoid( y )\n        lastY =  tf.nn.dropout( lastY, dropout )\n", "intent": "<h2>Define the Layer operations as a Python funtion</h2>\n"}
{"snippet": "feature_columns = boston.feature_names\nregressor = learn.DNNRegressor( feature_columns=None,\n                                hidden_units=[10, 10] )\nregressor.fit( X_train, y_train, steps=5000, batch_size=1 )\n", "intent": "<h1>Building a [10,10] Neural Net</h1>\n<br />\n"}
{"snippet": "regressor = learn.DNNRegressor( feature_columns=None,\n                               hidden_units=[13, 13])\nregressor.fit(X_train, y_train, steps=5000, batch_size=10)\n", "intent": "<h1>Building a [13,13] Neural Net</h1>\n<br />\n"}
{"snippet": "graph = tf.Graph()\nnum_steps = 5000\nwith graph.as_default():\n  features = tf.placeholder(tf.float32, shape=[4, 2])\n  labels = tf.placeholder(tf.int32, shape=[4])\n  train_op, loss, gs = make_graph(features, labels)\n  init = tf.global_variables_initializer()\n", "intent": "Build the graph -- define the placeholders, and call make_graph().\nThen add an op to init the variables.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_mat = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding_mat, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embedded = tf.nn.embedding_lookup(embedding,input_data)\n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model.add(Dense(100, activation='relu', input_dim=X_train.shape[1]))\n", "intent": "In the first hidden layer, we must also specify the dimension of our input layer. This will simply be the number of elements (pixels) in each image.\n"}
{"snippet": "model.add(Dense(100, activation='relu'))\n", "intent": "A dense layer is when every node from the previous layer is connected to each node in the current layer.\n"}
{"snippet": "model.add(Dense(num_classes, activation='softmax'))\n", "intent": "We also need to specify the number of output classes. In this case, the number of digits that we wish to classify.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform(shape = [vocab_size, embed_dim], minval = -1, maxval = 1))\n    embedded_input = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model.fit(norm_data[:,0], norm_data[:,1],\n          epochs=10,\n          validation_split=0.2)\n", "intent": "Then, we can train the model.\n"}
{"snippet": "reset_graph()\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\ny = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n", "intent": "**Defining placeholders**\n"}
{"snippet": "reset_graph()\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\ny = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n", "intent": "**Using dense instead of neuron_layer function()**\n"}
{"snippet": "x = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.matmul(x, W) + b\ny_ = tf.placeholder(tf.float32, [None, 10])\n", "intent": "Next, create the model graph. It includes input 'placeholders'.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"model_ckps/my_model_final.ckpt\") \n    X_new_scaled = mnist.test.images[:20]\n    Z = logits.eval(feed_dict={X: X_new_scaled})\n    y_pred = np.argmax(Z, axis=1)\n", "intent": "**Using the Neural Network**\n"}
{"snippet": "from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(random_state=42, max_iter=10) \nsgd_clf.fit(X_train, y_train_5)\n", "intent": "Now, for binary classification '5' and 'Not 5', we train the SGD Classifier on the training dataset.\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n", "intent": "Training a classifier\n"}
{"snippet": "from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(random_state=42) \nsgd_clf.fit(X_train_scaled, y_train)\n", "intent": "Let us try <b>SGDClassifier</b> first\n"}
{"snippet": "from sklearn.ensemble import VotingClassifier\nlog_clf_ens = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=42)\nrnd_clf_ens = RandomForestClassifier(n_estimators=100, max_depth=50, random_state=42)\nvoting_clf = VotingClassifier(\n    estimators=[('lr', log_clf_ens), ('rf', rnd_clf_ens)],\n    voting='soft')\nvoting_clf.fit(X_train_scaled, y_train)\n", "intent": "Now, let us try <b>Ensemble</b> with <b>soft voting</b>\n"}
{"snippet": "from sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X, y)\nplot_decision_boundary(model, X, y)\n", "intent": "Try to find a model in scikit-learn that actually works on this datasets\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform(shape=[vocab_size, embed_dim], \n                                              minval=-1.0, maxval=1.0), \n                            name='embedding')\n    print('Embedding Table Shape: {0}'.format(embedding.get_shape()))\n    embed = tf.nn.embedding_lookup(embedding, ids=input_data, name='embed')\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Conv2D(32, (3,3), input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Activation('elu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('elu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform(shape=(vocab_size, embed_dim), \n                            minval=-1, maxval=1, dtype=tf.float32)) \n    embed = tf.nn.embedding_lookup(embedding, ids=input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "NUM_STEPS = 10000\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n", "intent": "Next, create a session and initialize the graph variables. We'll also set the number of steps we'll train with.\n"}
{"snippet": "import tensorflow as tf\nhello = tf.constant('It works!')\nsess = tf.Session()\nprint(sess.run(hello))\n", "intent": "This snippet of Python creates a simple graph.\n"}
{"snippet": "print(x4.todense()) \n", "intent": "Let's print it and see what it looks like \n"}
{"snippet": "mnb=MultinomialNB()\nprint_model_metrics(mnb,data_fill,features,label,20)\n", "intent": "As a reminder, here are the metrics for the data where we fill in missing Ages with the median and where we drop those rows.\n"}
{"snippet": "rf=RandomForestClassifier(random_state=1212, n_jobs=-1, max_depth=4,max_features=None,bootstrap=False)\nprint_model_metrics(rf,data_impute_rf,features_rf,label,20)\n", "intent": "That's much better than the resuls we got from our Naive Bayes imputation. \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.3))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=80, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "X = matutils.corpus2dense(lsi_corpus, num_terms=200).transpose()\nX.shape\n", "intent": "We have (very good, 300-dimensional) vectors for our documents now!  So we can do any machine learning we want on our documents!\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nX = body[['Weight', 'Height']] \ny = body.Male \nclf = LogisticRegression().fit(X, y)\npd.Series(clf.coef_[0], index=['Weight', 'Height'])\n", "intent": "*Do we already know any machine learning models?*\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import  KNeighborsClassifier\nX = body[['Weight', 'Height']]\ny = body.Male\nclf = KNeighborsClassifier(n_neighbors=50).fit(X, y)\n", "intent": "Another model: nearest neighbor\n"}
{"snippet": "def serving_input_receiver_fn():\n    feature_tensor = tf.placeholder(tf.float32, [None, 784])\n    return tf.estimator.export.ServingInputReceiver({'x': feature_tensor}, {'x': feature_tensor})\nexported_model_dir = mnist_classifier.export_savedmodel(MODEL_DIR, serving_input_receiver_fn)\ndecoded_model_dir = exported_model_dir.decode(\"utf-8\")\n", "intent": "Finally, export the model for later serving.\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid, verbose=3)\ngrid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "svc_rbf = SVC(kernel='rbf', random_state = 0, probability = True)\nsvc_rbf.fit(X_train_robust, y_train)\nvalidation_score = svc_rbf.score(X_val_robust, y_val)\nprint('Classification accuracy on preprocessed validation set: {:.6f}\\n'.format(validation_score))  \n", "intent": "<h2>SVC attempt with Radial Basis and RobustScaler</h2>\n"}
{"snippet": "svc_poly = SVC( kernel='poly', random_state = 0)\nsvc_poly.fit(X_train_robust, y_train)\nvalidation_score = svc_poly.score(X_val_robust, y_val)\nprint('Polynomial Classification accuracy on preprocessed validation set: {:.6f}\\n'.format(validation_score))  \n", "intent": "<h2>SVC attempt with Polynomials and RobustScaler</h2>\n"}
{"snippet": "kmeans = KMeans(n_clusters=5, random_state = 0).fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1)) \n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import KMeans\ncluster = KMeans(n_clusters=2)\ncluster.fit(distances.values)\n", "intent": "The clustering code is very simple: The code below will create two groups of stations.\n"}
{"snippet": "model_rnn=Sequential()\n", "intent": "The first thing is to create a model.\n"}
{"snippet": "model_rnn.add(SimpleRNN(1,input_shape=(1,1)))\n", "intent": "then we add the layer that we want\n"}
{"snippet": "model_LSTM=Sequential()\nmodel_LSTM.add(SimpleRNN(1,input_shape=(1,1)))\nmodel_LSTM.compile(loss='mean_squared_error',optimizer='Adam')\n", "intent": "and we can do the same for LSTM and GRU\n"}
{"snippet": "transfer_learning.maybe_download_and_extract(INCEPTION_MODEL_DIR)\ngraph, bottleneck_tensor, jpeg_data_tensor, resized_image_tensor = (\n    create_inception_graph())\nprint(\"Finished.\")\n", "intent": "Define the Inception-based graph we'll use to generate the 'bottleneck' values. Wait for this to print \"Finished\" before continuing.\n"}
{"snippet": "sess=tf.Session()\n", "intent": "Now, nothing has run yet, we need to create a session and run it to see how thinks change\n"}
{"snippet": "X=tf.placeholder(shape=(4,2),dtype=tf.float32,name='input')\ny=tf.placeholder(shape=(4,1),dtype=tf.float32,name='output')\nW=tf.Variable([[1],[1]],dtype=tf.float32,name='weights')\nb=tf.Variable([0],dtype=tf.float32,name='bias')\nglobal_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n", "intent": "and the objects were\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    writer = tf.summary.FileWriter('./cooler_graphs', sess.graph)\n    for i in range(10000):\n        _,_,summary=sess.run([loss,optimizer,summary_op],feed_dict={X:X_data,y:y_data})\n        writer.add_summary(summary,global_step=i)\n", "intent": "we are now ready for training\n"}
{"snippet": "X = tf.placeholder(shape=(4),dtype=tf.int32,name='INPUT')\ny_ = tf.placeholder(shape=(1),dtype=tf.int32,name='OUTPUT')\nU = tf.Variable(tf.random_normal([8238,128], stddev=0.5),name=\"U\",dtype=tf.float32)\nV = tf.Variable(tf.random_normal([128,8238], stddev=0.5),name=\"V\",dtype=tf.float32)\naveg_creator = tf.constant([[0.25,0.25,0.25,0.25]],name='average_creator')\n", "intent": "We first create the objects that will appear on the graphs\n"}
{"snippet": "u= tf.nn.embedding_lookup(U,X)\nu_aveg= tf.matmul(aveg_creator,u,name='average')\nv= tf.matmul(u_aveg,V)\ny_hat = tf.nn.softmax(v)\n", "intent": "Now, we create the graph\n"}
{"snippet": "sess=tf.Session()\nsess.run(tf.global_variables_initializer())\n", "intent": "We are ready to start the session\n"}
{"snippet": "sess=tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.global_variables())\n", "intent": "Let's run this in a Session.\n"}
{"snippet": "output,state= tf.contrib.rnn.static_rnn(cell,inputs,dtype=tf.float32,initial_state=initial_state )\n", "intent": "We now 'unroll' it to length 4.\n"}
{"snippet": "sess=tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.global_variables())\n", "intent": "and we can run the session.\n"}
{"snippet": "age = tf.contrib.layers.real_valued_column(\"age\")\neducation_num = tf.contrib.layers.real_valued_column(\"education_num\")\ncapital_gain = tf.contrib.layers.real_valued_column(\"capital_gain\")\ncapital_loss = tf.contrib.layers.real_valued_column(\"capital_loss\")\nhours_per_week = tf.contrib.layers.real_valued_column(\"hours_per_week\")\nprint('continuous columns configured')\n", "intent": "Second, configure the real-valued columns using `real_valued_column()`. \n"}
{"snippet": "cell = tf.contrib.rnn.BasicRNNCell(1)\n", "intent": "and create the cell \n"}
{"snippet": "inputs=tf.unstack(X,axis=0)\ninitial_state=tf.constant([[0],[0],[0]],dtype=tf.float32)\noutput,state= tf.contrib.rnn.static_rnn(cell,inputs,dtype=tf.float32,initial_state=initial_state )\n", "intent": "next we create the network, but recall the signature takes a list not a tensor, so we need to use unpack\n"}
{"snippet": "sess=tf.Session()\nsess.run(tf.global_variables_initializer())\n", "intent": "and it is time to use the session\n"}
{"snippet": "output,_= tf.nn.dynamic_rnn(cell,X,dtype=tf.float32)\n", "intent": "and we create the rnn layer now\n"}
{"snippet": "sess=tf.Session()\nsess.run(tf.global_variables_initializer())\n", "intent": "we are ready to run a session\n"}
{"snippet": "with tf.name_scope(\"embedding\"):\n    W = tf.Variable(tf.random_uniform([vocab_size, emb_dim], -1.0, 1.0),name=\"W\")\n    embedded_chars = tf.nn.embedding_lookup(W, input_x)\n", "intent": "Next, we create the variables we are going to need.\n"}
{"snippet": "with tf.name_scope(\"hidden\"):\n    W_h= tf.Variable(tf.random_uniform([sentence_len*emb_dim, hidden_dim], -1.0, 1.0),name=\"w_hidden\")\n    b_h= tf.Variable(tf.zeros([hidden_dim],name=\"b_hidden\"))\n    hidden_output= tf.nn.relu(tf.matmul(emb_vec,W_h)+b_h)\n", "intent": "we can now go over the hidden dimension, but first we need a variable for this\n"}
{"snippet": "with tf.name_scope(\"output_layer\"):\n    W_o= tf.Variable(tf.random_uniform([hidden_dim,2], -1.0, 1.0),name=\"w_o\")\n    b_o= tf.Variable(tf.zeros([2],name=\"b_o\"))\n    score = tf.nn.relu(tf.matmul(hidden_output,W_o)+b_o)\n    predictions = tf.argmax(score, 1, name=\"predictions\")\n", "intent": "finally, the output layer\n"}
{"snippet": "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\noptimizer=tf.train.AdamOptimizer(1e-4).minimize(loss)\nloss_summary = tf.summary.scalar(\"loss\", loss)\nacc_summary = tf.summary.scalar(\"accuracy\", accuracy)\nsummary_op=tf.summary.merge([loss_summary,acc_summary])\n", "intent": "We are almost ready to start the session, we need the training operation\n"}
{"snippet": "age = layers.real_valued_column(\"age\")\neducation_num = layers.real_valued_column(\"education_num\")\ncapital_gain = layers.real_valued_column(\"capital_gain\")\ncapital_loss = layers.real_valued_column(\"capital_loss\")\nhours_per_week = layers.real_valued_column(\"hours_per_week\")\nprint('continuous columns configured')\n", "intent": "Second, configure the real-valued columns using `real_valued_column()`. \n"}
{"snippet": "with tf.name_scope(\"embedding\"):\n    W = tf.Variable(emb_matrix,name=\"W\",dtype=tf.float32)\n    embedded_chars = tf.nn.embedding_lookup(W, input_x)\n", "intent": "Next, we create the variables we are going to need.\n"}
{"snippet": "sess = tf.Session()\nsess.run(tf.global_variables_initializer())\ntrain_summary_writer = tf.summary.FileWriter('./summaries2/', sess.graph)\n", "intent": "Running the session\n"}
{"snippet": "embeddings=tf.Variable(emb_matrix)\nembeddings= tf.nn.embedding_lookup(embeddings,input_placeholder)\nembeddings= tf.reshape(embeddings,shape=(-1,embed_size*n_features))\n", "intent": "and the embedding layer\n"}
{"snippet": "W=tf.Variable(tf.random_normal(shape=(n_features*embed_size,hidden_size)))\nb1=tf.Variable(tf.zeros(hidden_size))\nU = tf.Variable(tf.random_normal(shape=(hidden_size,n_classes)))\nb2=tf.Variable(tf.zeros(n_classes))\npred = tf.nn.relu(tf.matmul(embeddings,W)+b1)\npred = tf.nn.dropout(pred,dropout)\npred = tf.matmul(pred,U)+b2\n", "intent": "and then the prediction layer\n"}
{"snippet": "sess= tf.Session()\nsess.run(tf.global_variables_initializer())\n", "intent": "We are ready to run the session.\n"}
{"snippet": "with tf.name_scope(\"Recurrent_layers\"):\n    lstms=[tf.contrib.rnn.LSTMCell(100) for i in range(3)]\n    staked_lstm = tf.contrib.rnn.MultiRNNCell(lstms)\n    initial_state=state=staked_lstm.zero_state(50,dtype=tf.float32)\n    output = tf.reshape(input_tokens, (-1,5,1),name=\"reshaped_input\")\n    output,_ = tf.nn.dynamic_rnn(staked_lstm,output,dtype=tf.float32)\n    output = tf.reshape(output,[-1,100])\n", "intent": "This will be feed to a RNN made of LSTM's, hence the first step \n<img src=\"lstmRNN.png\">\n"}
{"snippet": "with tf.name_scope(\"Softmax_layer\"):\n    W = tf.Variable(tf.random_normal(shape=(100,26)))\n    b= tf.Variable(tf.zeros(26))\n    output = tf.matmul(output,W)+b\n", "intent": "next we create the softmax layer\n"}
{"snippet": "with tf.name_scope(\"optimizer\"):\n    global_step=tf.Variable(0,dtype=tf.int32,trainable=False,name=\"global_step\")\n    optimizer=tf.train.AdamOptimizer().minimize(loss,global_step=global_step)\n", "intent": "let's use Adam optimizer for the gradient descend\n"}
{"snippet": "sess=tf.Session()\n", "intent": "we are ready for the training part\n"}
{"snippet": "kmeans_model = KMeans(n_clusters=2, random_state=42)\nkmeans_model.fit(X_scaled)\n", "intent": "From above, it appears that 2 would be a good number of clusters\n"}
{"snippet": "clustering = AgglomerativeClustering(linkage='ward', n_clusters=2)\nclustering.fit(data3_g)\ndata3_g.groupby(clustering.labels_).count()\n", "intent": "** From the result above, I will use 2 clusters**\n"}
{"snippet": "clustering = AgglomerativeClustering(linkage='complete', n_clusters=2)\nclustering.fit(data3_g)\ndata3_g.groupby(clustering.labels_).count()\n", "intent": "** From the result above, I will use 2 clusters**\n"}
{"snippet": "kernel = 1* RBF(length_scale=100.0, length_scale_bounds=(1e-3, 1e3)) \\\n    + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e10))\ngp = GaussianProcessRegressor(kernel=kernel,\n                              n_restarts_optimizer=10,random_state=1)\ngp.fit(X, Y)\n", "intent": "**1) The kernel after parameter optimization and fitting to the observed data.**\n**2) The log marginal likelihood of the training data.  **\n"}
{"snippet": "from sklearn.mixture import GaussianMixture\nGM=GaussianMixture(n_components=5,random_state=999)\nGM.fit(Data3.iloc[:,1:])\n", "intent": "**b. Cluster with Gaussian Mixture. Please repeat (2)a but use loglikelihood for each record.**\n"}
{"snippet": "from sklearn.ensemble import IsolationForest\nrng = np.random.RandomState(42)\nclf = IsolationForest(max_samples=100, random_state=rng)\nclf.fit(X)\n", "intent": "(3) Choose one more anomaly detection model you prefer and report the top 5 most anomalous counties by the model you choose. \n"}
{"snippet": "from pgmpy.models import BayesianModel\nfrom pgmpy.estimators import MaximumLikelihoodEstimator, BayesianEstimator\nmodel = BayesianModel(best_model.edges())\nmodel.fit(data, estimator=MaximumLikelihoodEstimator)\nfor cpd in model.get_cpds():\n    print(\"CPD of {variable}:\".format(variable=cpd.variable))\n    print(cpd)\nprint model.get_independencies()\n", "intent": "[('Crime', 'ChildrenPoverty'), ('Smokers', 'Obese'), ('Smokers', 'PM2.5'), ('ChildrenPoverty', 'Smokers'), ('ChildrenPoverty', 'Income')] \n"}
{"snippet": "reverse_site_dict = dict((v,k) for (k,v) in site_dict.items())\nunique, counts = np.unique(train_dataset[train_dataset['target'] == 1][site_cols].values.flatten(), return_counts=True)\ntop30 = [s[0] for s in sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)[0:31]]\ntop30.remove(0)\n[reverse_site_dict[site_id] for site_id in top30]\n", "intent": "Let's figure out top 30 popular sites for our train set:\n"}
{"snippet": "get_predictions(index=0, \n                partition='train',\n                input_to_softmax=final_model(input_dim=13, \n    filters=150,\n    kernel_size=11,\n    conv_stride=3,\n    conv_border_mode='same',\n    units=100), \n                model_path='results/model_end.h5')\n", "intent": "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='validation',\n                input_to_softmax=final_model(input_dim=13, \n    filters=150,\n    kernel_size=11,\n    conv_stride=3,\n    conv_border_mode='same',\n    units=100), \n                model_path='results/model_end.h5')\n", "intent": "Use the next code cell to visualize the model's prediction for the first example in the validation dataset.\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, save_file)\n    test_accuracy = sess.run(\n        accuracy,\n        feed_dict={features: mnist.test.images, labels: mnist.test.labels})\nprint('Test Accuracy: {}'.format(test_accuracy))\n", "intent": "Let's load the weights and bias from memory, then check the test accuracy.\n"}
{"snippet": "tf.reset_default_graph()\nweights = tf.Variable(tf.truncated_normal([2, 3]))\nbias = tf.Variable(tf.truncated_normal([3]))\nsaver = tf.train.Saver()\n", "intent": "Now that the Tensor Variables are saved, let's load them back into a new model.\n"}
{"snippet": "print(\"< operations >\")\nfor op in tf.get_default_graph().get_operations():\n    print(op.name)\nprint()\nprint(\"< variables >\")\nfor var in tf.global_variables():\n    print(var.name)\n", "intent": "It is also possible to automatically print all the ops and variables by iterating through the graph.\n"}
{"snippet": "loss_per_pixel = tf.square(tf.subtract(l_out, x_pl))\nloss = tf.reduce_mean(loss_per_pixel, name=\"mean_error\")\nreg_scale = 0.0005\nregularize = tf.contrib.layers.l2_regularizer(reg_scale)\nparams = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\nreg_term = tf.reduce_sum([regularize(param) for param in params])\nloss += reg_term\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.25)\ntrain_op = optimizer.minimize(loss)\n", "intent": "Following we define the TensorFlow functions for training and evaluation.\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 4))\nnet.add(LeakyReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\nprint(net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\n", "intent": "<a id='train_model'></a>\n"}
{"snippet": "log_mod_10 = linear_model.LogisticRegression(C = 10.0, class_weight = {0:0.45, 1:0.55}) \nlog_mod_10.fit(Comps_10, y_train)\n", "intent": "Execute the code in the cell below to define and fit a logistic regression model using the 10 components of the transformed features. \n"}
{"snippet": "nr.seed(3456)\nparam_grid = {\"C\": [0.1, 1, 10, 100, 1000]}\nlogistic_mod = linear_model.LogisticRegression(class_weight = {0:0.45, 1:0.55}) \nclf = ms.GridSearchCV(estimator = logistic_mod, param_grid = param_grid, \n                      cv = inside, \n                      scoring = 'roc_auc',\n                      return_train_score = True)\nclf.fit(Features_reduced, Labels)\nclf.best_estimator_.C\n", "intent": "The code in the cell below performs the grid search for the optimal model hyperparameter. As before, the scoring metric is AUC.   \n"}
{"snippet": "log_mod_10 = linear_model.LogisticRegression(C = 100) \nlog_mod_10.fit(Comps_10, y_train)\n", "intent": "Execute the code in the cell below to define and fit a logistic regression model using the 10 components of the transformed features. \n"}
{"snippet": "nr.seed(3456)\nparam_grid = {\"C\": [0.1, 1, 10, 100, 1000]}\nlogistic_mod = linear_model.LogisticRegression(class_weight = {0:0.1, 0:0.9}) \nclf = ms.GridSearchCV(estimator = logistic_mod, param_grid = param_grid, \n                      cv = inside, \n                      scoring = sklm.make_scorer(sklm.roc_auc_score),\n                      return_train_score = True)\nclf.fit(Features_reduced, Labels)\nclf.best_estimator_.C\n", "intent": "The code in the cell below performs the grid search for the optimal model hyperparameter. As before, the scoring metric is AUC.   \n"}
{"snippet": "alg = linear_model.LogisticRegression()\nalg.fit(xtrain, ytrain)\n", "intent": "Finally, lets try machine learning!\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n", "intent": "Scikit-Learn makes this shockingly easy.\n"}
{"snippet": "lm = LinearRegression()\nlm.fit(X_train, y_train)\n", "intent": "Create a linear regression model object and fit (train it with) the training data.\n"}
{"snippet": "logmodel = LogisticRegression()\nlogmodel.fit(X_train, y_train)\n", "intent": "Train the model with the training chunks.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier()\ntree.fit(X_train, y_train)\n", "intent": "Let's start with a single, simple decision tree, without any pruning or other bells and whistles.\n"}
{"snippet": "dtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\n", "intent": "Let's begin with a basic sort of decision tree.\n"}
{"snippet": "a = np.array([[5.0, 5.0]]) \nb = np.array([[2.0], [2.0]]) \nmat_a = tf.constant(a)\nmat_b = tf.constant(b)\nmatmul = tf.matmul(mat_a, mat_b)\nwith tf.Session() as sess:\n    result = sess.run(matmul)\n    print(result)\n", "intent": "Tensorflow does matrix operation very well, as you would expect.\n"}
{"snippet": "images, labels = next(iter(trainloader))\nimg = images[7].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "import sklearn.naive_bayes, sklearn.tree\nc2 = sklearn.naive_bayes.BernoulliNB()\ncv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True, random_state=123456)\n", "intent": "Not too bad, but can we do better?\n"}
{"snippet": "c = sklearn.ensemble.BaggingClassifier(base_estimator=sklearn.linear_model.LogisticRegression())\ncv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True, random_state=123456)\n", "intent": "Test your understanding: what happens if you Bag with Logistic Regression?\n"}
{"snippet": "lr2 = LinearRegression()\nlr2.fit(pca_features_train,y_train)\nplotModelResults(lr2,pca_features_train,y_train.reshape(-1,1),plot_intervals=True)\n", "intent": "Now train linear regression on pca features and plot its forecast.\n"}
{"snippet": "c = sklearn.ensemble.AdaBoostClassifier(base_estimator=sklearn.tree.DecisionTreeClassifier(max_depth=1))\ncv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True, random_state=123456)\n", "intent": "This approach was developed with things like decision stumps in mind:\n"}
{"snippet": "c21 = sklearn.ensemble.GradientBoostingClassifier()\ncv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True, random_state=123456)\n", "intent": "Gradient Boosting is beloved by some, although I have not had life-changing success with it, myself:\n"}
{"snippet": "import sklearn.naive_bayes\nclf = sklearn.naive_bayes.BernoulliNB()\nclf.fit(__________________, __________________)\n", "intent": "Now, let us try a range of prediction methods:\n"}
{"snippet": "oos_accuracy(sklearn.linear_model.LinearRegression())  \n", "intent": "Bonus challenge: Figure out a way to make it work for the linear regression predictor\n(Hard because we had to round the numeric predictions)\n"}
{"snippet": "import sklearn.naive_bayes\nclf = sklearn.naive_bayes.BernoulliNB()\nclf.fit(X_train, y_train)\n", "intent": "Now, let us try a range of prediction methods:\n"}
{"snippet": "oos_accuracy(sklearn.naive_bayes.BernoulliNB())  \n", "intent": "Figure out a way to test it:\n"}
{"snippet": "clf = sklearn.linear_model.LogisticRegression(penalty='l2', C=10)\nclf.fit(X, y)\nnp.where(clf.coef_ > .1)[1]\n", "intent": "Well, that did not do anything interesting...\nBut I will not give up yet.\n"}
{"snippet": "clf = sklearn.linear_model.LogisticRegression(penalty='l2', C=10)\nclf.fit(X, y)\nfor i in np.where(clf.coef_ > .5)[1]:\n    print vectorizer.get_feature_names()[i],\nprint\nprint\nfor i in np.where(clf.coef_ < -.5)[1]:\n    print vectorizer.get_feature_names()[i],\n", "intent": "Try it out with an arbitrary value of C:\n"}
{"snippet": "model = Sequential()\nmodel.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1,28,28), dim_ordering='th'))\nprint(model.output_shape)\nmodel.add(Convolution2D(32, 3, 3, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n", "intent": "Example implementations in Keras:\nhttps://github.com/fchollet/keras/tree/master/examples\n"}
{"snippet": "logit = LogisticRegression(n_jobs=-1, random_state=7)\nlogit.fit(X_train, y_train)\n", "intent": "**The next step is to train Logistic Regression.**\n"}
{"snippet": "from sklearn import svm\nmodel = svm.SVC(kernel='linear', C=10.) \nmodel.fit(X, y)\nplot_decision_boundary(model, X, plot_hyperplanes=True)\nscatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)\nSV = model.support_vectors_\nscatter(SV[:, 0], SV[:, 1], c=y[model.support_],\n        cmap=cm_bright, s=300, marker='o', facecolors='none', linewidths=2.);\n", "intent": "E' necessario che le classi siano separate.\n"}
{"snippet": "classifier = svm.SVC(kernel='rbf', gamma=gs.best_params_['gamma'])\nclassifier.fit(U, y)\nX_test_norm = scaler.transform(X_test)\nU_test = pca.transform(X_test_norm)\nprint classifier.score(U_test, y_test)\n", "intent": "Check the score of the best model that uses PCA in the test set.\n"}
{"snippet": "test_input = test_orig[0:1]\ntest_output = test_refc[0:1]\ntest = Variable(torch.from_numpy(test_image), requires_grad=False).cuda()\n", "intent": "We can now start feeding data from our testing set and seeing how it does. To start, let's just grab a single image.\n"}
{"snippet": "result = model(test)\nloss = loss_fn(result, test_output)\nprint(\"test Loss\", loss.data[0])\n", "intent": "Now we can feed it through the network and compare the result.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport pydotplus\nfrom IPython.display import Image\ntreeclf=DecisionTreeClassifier(max_depth=5)\ntreeclf.fit(X_train,y_train)\ndot_data=export_graphviz(treeclf, out_file=None)\ngraph = pydotplus.graph_from_dot_data(dot_data)  \nImage(graph.create_png())\n", "intent": "7) Train a Decision Tree classifier with maximum depth 5 and plot the decision tree. How does performance compare?\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_dim=1000))\nmodel.add(Dropout(.2))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=100, validation_data=(x_test,y_test), verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "V_data = [1., 2., 3.]\nV = torch.tensor(V_data)\nprint(V)\nM_data = [[1., 2., 3.], [4., 5., 6]]\nM = torch.tensor(M_data)\nprint(M)\nT_data = [[[1., 2.], [3., 4.]],\n          [[5., 6.], [7., 8.]]]\nT = torch.tensor(T_data)\nprint(T)\n", "intent": "Creating Tensors\n~~~~~~~~~~~~~~~~\nTensors can be created from Python lists with the torch.Tensor()\nfunction.\n"}
{"snippet": "import sys\nsys.path.append('/Users/dmitrys/xgboost/python-package/')\nfrom xgboost import XGBRegressor \nxgb = XGBRegressor()\nxgb.fit(X_train_scaled, y_train)\n", "intent": "Why shouldn't we try XGBoost now?\n<img src=\"https://habrastorage.org/files/754/a9a/26e/754a9a26e59648de9fe2487241a27c43.jpg\"/>\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train.values.reshape(-1, 1), y_train)\n", "intent": "In `scikit-learn` the function to use (for almost all models) is `.fit()`\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train.values, y_train)\n", "intent": "Remember the `.values.reshape()` is only required if you have **only one** predictor\n"}
{"snippet": "lr_l1_penalty = LogisticRegression(penalty=\"l1\", C=0.1)\nlr_l2_penalty = LogisticRegression(penalty=\"l2\", C=0.1)\nlr_l1_penalty.fit(X_train, y_train)\nlr_l2_penalty.fit(X_train, y_train);\n", "intent": "Based on what we learned for linear regression, how can logistic regression be regularised?\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=3)\ndt.fit(df[iris.feature_names], df[\"target\"])\n", "intent": "- what are \"maximum tree depth\" and \"minimum samples per leaf\" examples of?\n"}
{"snippet": "from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(min_samples_leaf=5)\ndt.fit(X_train, y_train)\n", "intent": "- Make sure you choose the appropriate kind of tree (classification or regression?)\n- Choose an appropriate metric to go with your type of tree\n"}
{"snippet": "rf2 = RandomForestRegressor()\nrf2.fit(X_train, y_train)\nfor z in zip(X_train.columns, rf2.feature_importances_):\n    print(z)\n", "intent": "Remember: if you used `cross_val_score` above, you need to fit another random forest!\n"}
{"snippet": "from sklearn.tree import export_graphviz\ndt = DecisionTreeRegressor(min_samples_leaf=5, max_depth=4)\ndt.fit(X_train, y_train)\nexport_graphviz(dt, \"tree.dot\", feature_names=X_train.columns)\n", "intent": "When you view your tree, what does it tell you? What features are more/less important than others?\n"}
{"snippet": "rf = RandomForestRegressor(n_estimators=11, min_samples_leaf=5)\nrf.fit(X_train, y_train)\nfor z in zip(X_train.columns, rf.feature_importances_):\n    print(z)\n", "intent": "Remember: if you used `cross_val_score` above, you need to fit another random forest!\n"}
{"snippet": "kmeans = KMeans(n_clusters=len(X))\nkmeans.fit(X)\n", "intent": "As with machine learning in general, increasing complexity will increase the \"fit\"\n"}
{"snippet": "lm = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=data).fit()\nprint lm.pvalues\n", "intent": "We could try a model with all features, and only keep features in the model if they have **small p-values**:\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(estimator=KNeighborsClassifier(),\n                    param_grid={'n_neighbors': [3,5,7,9,11,13,15]},\n                    cv=10,\n                    scoring='f1',\n                    return_train_score=True)\ngrid.fit(X_train,y_train);\n", "intent": "- look at the best score of your grid search\n- look at the best hyperparameter\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(estimator=KNeighborsClassifier(),\n                    param_grid={\"n_neighbors\": [3, 5, 7, 9, 11, 13, 15, 17]},\n                    cv=10,\n                    scoring=\"f1\")\ngrid.fit(X_train, y_train)\n", "intent": "- look at the best score of your grid search\n- look at the best hyperparameter\n"}
{"snippet": "def plot_weights(W, img_shape, layout, cmp=cm.gray):\n    nrows, ncols = layout\n    fig, axes = subplots(nrows = nrows, ncols = ncols, \n                         figsize=(1.5*ncols, 1.5*nrows))\n    axes = axes.flatten()\n    for (i, im) in enumerate(W):\n        axes[i].imshow(im.reshape(img_shape), cmp)\n", "intent": "***Summary of SVM ***\n- Most of time we prefer to use an implemented SVC version in a library, e.g., SGDClassifier/LogisticRegression in sklearn.\n"}
{"snippet": "x = T.dmatrix('x')\ns = 1 / (1 + T.exp(-x))\nlogistic = function([x], s)\nprint logistic([[0, 1], [-1, -2]])\ns2 = (1 + T.tanh(x/2)) / 2\nlogistic2 = function([x], s2)\nprint logistic2([[0, 1], [-1, -2]])\n", "intent": "***Example of Logistic Regression***\n"}
{"snippet": "from theano.ifelse import ifelse\nimport time\na, b = T.scalars('a', 'b')\nx, y = T.matrices('x', 'y')\nz_switch = T.switch(T.lt(a, b), T.mean(x), T.mean(y)) \nz_lazy = ifelse(T.lt(a, b), T.mean(x), T.mean(y)) \nf_switch = theano.function([a, b, x, y], z_switch, mode = theano.Mode(linker = 'vm'))\nf_lazyifelse = theano.function([a, b, x, y], z_lazy, mode = theano.Mode(linker = 'vm'))\nval1, val2 = 0., 1.\nbig_mat1, big_mat2 = np.ones((10000, 1000)), np.ones((10000, 1000))\n", "intent": "***Flow of Control***\n"}
{"snippet": "import scipy.sparse as sp\nfrom theano import sparse\nprint sparse.all_dtypes\nx = sparse.csc_matrix('x')\nprint x.type\nprint sparse.dense_from_sparse(x).type\nprint sparse.csr_from_dense(sparse.dense_from_sparse(x)).type\n", "intent": "***Sparse Matrices in Theano***\n"}
{"snippet": "reload(theanoml.autoencoder)\nac = theanoml.autoencoder.ContractiveAutoEncoder(n_epochs=5)\nac.fit(X)\nprint ac.transform(X).shape\n", "intent": "***TEST Contractive AutoEncoder***\n"}
{"snippet": "reload(theanoml.autoencoder)\nda = theanoml.autoencoder.DenoisingAutoEncoder()\nda.fit(X)\nprint da.transform(X).shape\n", "intent": "***Test Denoising AutoEncoder***\n"}
{"snippet": "kmeans = MiniBatchKMeans(n_clusters = 1000, batch_size=10000, random_state=0)\nkmeans.fit(all_feats)\n", "intent": "** IT SEEMS SURF FEATURES behave quite well in terms of generating similiar-range features. And so normalization is not necessary before clustering**\n"}
{"snippet": "lm = smf.ols(formula='Sales ~ TV + Radio', data=data).fit()\nlm.rsquared\n", "intent": "We could try models with different sets of features, and **compare their R-squared values**:\n"}
{"snippet": "model.fit(X, y)\n", "intent": "**4\\. Fit the model to your data (i.e. learning)**\n"}
{"snippet": "from sklearn.svm import SVC \nmodel = SVC(kernel='linear', C=1E10)\nmodel.fit(X, y)\n", "intent": "SVMs requires Hyperparameter. We'll discuss them later.\n"}
{"snippet": "clf = SVC(kernel='rbf', C=1E6)\nclf.fit(X, y)\n", "intent": "* The kernel trick is build into SVM class of Scikit-Learn.\n* We can simply change the `kernel` parameter to use the RBF kernel\n"}
{"snippet": "model.fit(Xtrain, ytrain)\n", "intent": "Then we fit the model:\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier().fit(X, y)\n", "intent": "In Scikit-Learn a `DecisionTreeClassifier` estimator is used to construct Decision Trees.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100, random_state=0)\nvisualize_classifier(model, X, y);\n", "intent": "We've built a Random Forest by hand. But Scikit-Learn comes with a `RandomForestClassifier` estimator which is easier to handle:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data) \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nprint(knn)\n", "intent": "**Step1:** Build the Classifier\n"}
{"snippet": "knn.fit(X, y)\n", "intent": "**step2:** Train the Classifier on your train dataset\n- Model is learning the relationship between X and y\n- Occurs in-place (inside the same object)\n"}
{"snippet": "from statsmodels.tsa.arima_model import ARMA\nstore1_sales_data = store1_open_data[['Sales']].astype(float)\nmodel = ARMA(store1_sales_data, (1, 0)).fit()\nmodel.summary()\n", "intent": "Remember, an ARMA model is a combination of autoregressive and moving average models.\nWe can train an AR model by turning off the MA component (q=0).\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(6, input_dim=train_features.shape[1], kernel_initializer='uniform', activation='sigmoid'))\nmodel.add(Dense(train_targets.shape[1], activation='linear'))\n", "intent": "We create the keras model here.\n"}
{"snippet": "history = model.fit(X, Y, validation_split=0.33, epochs=epochs, batch_size=100, verbose=0)\n", "intent": "We are going to use the 33% of the data as validation data:\n"}
{"snippet": "clf_1 = DecisionTreeClassifier(max_depth = 2)\nclf_1.fit(X_train,y_train)\nclf_2 = DecisionTreeClassifier(max_depth = 5)\nclf_2.fit(X_train,y_train)\n", "intent": "***\nLucius chose **Depths of 2 and 5 respectively **and then compared the results to see which is better\n"}
{"snippet": "gNB = GaussianNB()\n", "intent": "$$P(x|c_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^c}} e^{-\\frac{{(x-\\mu_k)}^2}{2\\sigma_k^2}}$$\n"}
{"snippet": "wcss = {}\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters= i, init= 'k-means++', max_iter= 300)\n    kmeans.fit(rfm_scaled)\n    wcss[i] = kmeans.inertia_\n", "intent": "With the Elbow method, we can get the optimal number of clusters.  \n"}
{"snippet": "lr.fit(X, y)\n", "intent": "Now we will fit the model on the dataset\n"}
{"snippet": "lr = LogisticRegression(penalty=\"l2\")\ngrid = GridSearchCV(estimator=lr, param_grid=param_grid, cv=3, n_jobs=-1)\nstart_time = time.time()\ngrid_result = grid.fit(X, y)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nprint(\"Execution time: \" + str((time.time() - start_time)) + \" ms\")\n", "intent": "You have defined the grid. Let's run the grid search over them and see the results with execution time.\n"}
{"snippet": "def k_nearest_vectors(k, mtx, candidate_vector):\n    cos_similarities = cosine_similarity(mtx, candidate_vector).flatten()\n    k_sorted = np.flip(np.argsort(cos_similarities)[-k:], axis = 0)\n    cos_sorted = np.flip(np.sort(cos_similarities), axis = 0)[:k]\n    return k_sorted, cos_sorted\n", "intent": "We define the following helper function to reduce the search space. \n"}
{"snippet": "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n", "intent": "Now we'll copy the files to S3 for Amazon SageMaker training to pickup.\n"}
{"snippet": "tf.reset_default_graph()\nweights = tf.Variable(tf.truncated_normal([2, 3]))\nbias = tf.Variable(tf.truncated_normal([3]))\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, save_file)\n    print('Weight:')\n    print(sess.run(weights))\n    print('Bias:')\n    print(sess.run(bias))\n", "intent": "Now that the Tensor Variables are saved, let's load them back into a new model.\n"}
{"snippet": "drugTree.fit(X_trainset,y_trainset)\n", "intent": "Next, we will fit the data with the training feature matrix <b> X_trainset </b> and training  response vector <b> y_trainset </b>\n"}
{"snippet": "dtree = DecisionTreeClassifier(criterion='gini',max_depth=None)\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "k_means.fit(X)\n", "intent": "Now let's fit the KMeans model with the feature matrix we created above, <b> X </b>\n"}
{"snippet": "k = 2\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(data)\n", "intent": "From the plot, we can see that the data can be divided into 2 main groups. Therefore, we will try using `k = 2` for our *k*-means model.\n"}
{"snippet": "k = 5\nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh\n", "intent": "Lets start the algorithm with k=4 for now:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_shape=(784,)))\nmodel.add(Activation('relu')) \nmodel.add(Dropout(0.2))   \nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10))\nmodel.add(Activation('softmax')) \n", "intent": "Build the neural-network. Here we'll do a simple 3 layer fully connected network.\n<img src=\"figure.png\" />\n"}
{"snippet": "model.fit(X_train, Y_train,\n          batch_size=128, nb_epoch=4\n          , verbose=1,\n          validation_data=(X_test, Y_test))\n", "intent": "This is the fun part: you can feed the training data loaded in earlier into this model and it will learn to classify digits\n"}
{"snippet": "pipe = make_pipeline(PreProcessing(),\n                    RandomForestClassifier())\n", "intent": "We'll create a `pipeline` to make sure that all the preprocessing steps that we do are just a single `scikit-learn estimator`.\n"}
{"snippet": "grid.fit(X_train, y_train)\n", "intent": "- Fitting the training data on the `pipeline estimator`:\n"}
{"snippet": "k = 5\nclf =  DecisionTreeClassifier(criterion='gini', max_depth=15)\nKfoldPlot(X, y, clf, k)\n", "intent": "Run your a classifier for your choice and say couple of words about the results.\n"}
{"snippet": "regr = Lasso(alpha=alpha_best)\nregr.fit(X_train, y_train)\nprint('Coefficients: \\n', regr.coef_)\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "X_train = train.ix[:,0:1].copy()\nX_test = test.ix[:,0:1].copy()\ny_train = train.ix[:,6].copy()\ny_test = test.ix[:,6].copy()\nreg_tree = tree.DecisionTreeRegressor()\nreg_tree.fit(X_train,y_train)\nresult = reg_tree.feature_importances_\nprint('The test score is '+ '%.4f'% reg_tree.score(X_test,y_test))\n", "intent": "Train with all data using outdoor temperature only\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding  = tf.Variable(tf.random_uniform((vocab_size, embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf = svm.SVC(kernel='rbf', C=C_opt, gamma=gamma)\nclf.fit(X_tr2, np.ravel(Y_tr2))\npe_tst = 1.0 - clf.score(Xtest, Ytest)\nprint(\"The test error for the selected model is {0}\".format(pe_tst))\n", "intent": "Evaluate the classifier performance using the test data, for the selected hyperparameter values.\n"}
{"snippet": "num_topics = 50\nldag = gensim.models.ldamodel.LdaModel(corpus=corpus_bow, id2word=D, num_topics=num_topics)\n", "intent": "** Exercise 3**: Create an LDA model with the 50 topics using `corpus_bow` and the dictionary, `D`.\n"}
{"snippet": "def most_relevant_projects(ldag, topicid, corpus_bow, nprojects=10):\n    print('Computing most relevant projects for Topic', topicid)\n    print('Topic composition is:')\n    print(ldag.show_topic(topicid))\n    document_topic = [el for el in ldag[corpus_bow]]\n    document_topic = gensim.matutils.corpus2dense(document_topic, ldag.num_topics).T\n    return np.argsort(document_topic[:,topicid])[::-1][:nprojects].tolist()\nproject_id = most_relevant_projects(ldag, 17, corpus_bow[:10000])\nfor idproject in project_id:\n    print(NSF_data[idproject]['title'])\n", "intent": "**Exercise 6**: Build a function that returns the most relevant projects for a given topic\n"}
{"snippet": "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer_1']), biases['hidden_layer_1'])\nlayer_1 = tf.nn.relu(layer_1)\nlayer_1 = tf.nn.dropout(layer_1, keep_prob)\nlayer_2 = tf.add(tf.matmul(layer_1, weights['hidden_layer_2']), biases['hidden_layer_2'])\nlayer_2 = tf.nn.relu(layer_2)\nlayer_2 = tf.nn.dropout(layer_2, keep_prob)\nlayer_3 = tf.add(tf.matmul(layer_2, weights['hidden_layer_3']), biases['hidden_layer_3'])\nlayer_3 = tf.nn.relu(layer_3)\nlayer_3 = tf.nn.dropout(layer_3, keep_prob)\n", "intent": "Now that we have an input layer, we can add the first hidden layer!\n"}
{"snippet": "model = sm.OLS(y, X)\nresults = model.fit()\nresults.summary()\n", "intent": "This means that our best fit line is:\n$$y = a + b x$$\nwhere $a = -0.363075521319$ and $b = 0.41575542$.\nNext let's use `statsmodels`.\n"}
{"snippet": "food = linear_model.LinearRegression()\n", "intent": "- Create a LinearRegression instance and use `fit()` function to fit the data.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(250, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(150, activation='relu'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=600)\n", "intent": "Now its time to train our model!\n**Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.**\n"}
{"snippet": "model.fit(colleges.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "kmodel.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlmodel = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "y_tempe_train = df_tempe_since_1960['5 year mean'].values\nlr_tempe.fit(X_tempe_train, y_tempe_train)\n", "intent": "Linear Regression model is trained with features matrix (years vs indicators) and 5-year-mean global temperature anomaly vectors.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train, y_train)\n", "intent": "Now let's compare the decision tree model to a random forest.\n"}
{"snippet": "km = KMeans(2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "X = df.drop('Private', axis=1)\nkm.fit(X, df['Private'])\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "model.fit(x_train, y_train, epochs=2, batch_size=1500, verbose=0);\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "knn.fit(X, y)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "lm.fit(X, y)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "gs = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)\ngs.fit(X, y)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "line_reg.fit(train_data[['X']], train_data['Y'])\nnp.hstack([line_reg.coef_, line_reg.intercept_]) \n", "intent": "We can then fit the model in one line (this solves the normal equations.\n"}
{"snippet": "from sklearn import neighbors\nfrom sklearn import preprocessing\nX_scaled = preprocessing.scale(X) \nclf1 = neighbors.KNeighborsRegressor(10)\ntrain_X = X_scaled[:half]\ntest_X = X_scaled[half:]\nclf1.fit(train_X,train_Y)\n", "intent": "** *k*-Nearest Neighbor (KNN) Regression **\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nclf2 = RandomForestRegressor(n_estimators=100, \n                            criterion='mse', max_depth=None, \n                            min_samples_split=2, min_samples_leaf=1, \n                            max_features='auto', max_leaf_nodes=None, \n                            bootstrap=True, oob_score=False, n_jobs=1, \n                            random_state=None, verbose=0, warm_start=False)\nclf2.fit(train_X,train_Y)\n", "intent": "Pretty good intro\nhttp://blog.yhathq.com/posts/random-forests-in-python.html\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=200,oob_score=True)\nclf.fit(X,Y)\n", "intent": "Let's look at random forest\n"}
{"snippet": "def apply_threshold(heatmap, threshold):\n    heatmap[heatmap <= threshold] = 0\n    return heatmap\n", "intent": "This method is adopted from Udacity lesson \"Multiple Detections and False Positives\"\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB().fit(X_train_sc,y_train)\n", "intent": "Let's train our classifier by calling its `fit()` method:\n"}
{"snippet": "model = RandomForestClassifier(n_estimators=100)\n", "intent": "Try a random forest model by running the cell below. \n"}
{"snippet": "linreg.fit(sd_like.values, reading)\nfor coef, var in zip(linreg.coef_, sd_like.columns):\n    print(var, coef)\n", "intent": "- What is the mean cross val score?\n"}
{"snippet": "params = {'penalty':['l1', 'l2'], 'C': [1, 10], 'max_iter': [50, 100]}\nlr = LogisticRegression(fit_intercept = True, random_state = seed)\nclf = GridSearchCV(lr, params, scoring = 'neg_log_loss')\nresult = clf.fit(X_train, Y_train)\n", "intent": "Use GridSearch to find the best model\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform( (vocab_size, embed_dim), -1, 1 ))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "class FastText(nn.Module):\n    def __init__(self, vocab_size, emb_dim):\n        super(FastText, self).__init__()\n        self.dummy_layer = nn.Linear(5,1)\n    def forward(self, data, length):\n        return nn.functional.sigmoid(self.dummy_layer(data.float()).view(-1))\nmodel = FastText(vocab_size, emb_dim)\n", "intent": "Please refers to https://arxiv.org/abs/1607.01759 for Fast Text model (Joulin et al.)\n"}
{"snippet": "for epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        if (i+1) % 100 == 0:\n            print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n                   % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n", "intent": "Define the training procedure:\n"}
{"snippet": "lm = LinearRegression()\nlm.fit(feature, outcome)\n", "intent": "- Second, fit the linear regression model.\n"}
{"snippet": "lm.fit(Employee['Salary'].iloc[complete], outcome)\n", "intent": "Passing wrong type to the `lm.fit` doesn't work. For example, if we pass a `Series` as features:\n"}
{"snippet": "logit = linear_model.LogisticRegression(C=1e4)\n", "intent": "Build a logistic regression model **`logit`** with the data.\n- What's the score?\n"}
{"snippet": "from sklearn import naive_bayes\ngnb = naive_bayes.GaussianNB()\n", "intent": "<p><a name=\"gnb-sklearn\"></a></p>\n**Exercise**\nWe will work on the iris dat. Fit a Gaussian Naive Bayes and print out the accuracy:\n"}
{"snippet": "model = SVC()\n", "intent": "Try a Support Vector Machines model by running the cell below. \n"}
{"snippet": "model1=RandomForestRegressor()\n", "intent": "Random Forest Model\n---\n"}
{"snippet": "Xtrain_sm = sm.add_constant(Xtrain) \nlogreg_sm = sm.GLM(ytrain, Xtrain_sm, family=sm.families.Binomial()).fit()\nsummary_string = logreg_sm.summary().as_csv()\nlogreg_sm.summary()\n", "intent": "**Statsmodels logistic regression**\n"}
{"snippet": "svc = SVC(probability=True)\n", "intent": "**Support vector machine model (default parameters)**\n"}
{"snippet": "tree = DecisionTreeClassifier()\n", "intent": "**Decision tree and random forest classifiers**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\ntfidf_log_models = []\nfor i, ngram_model in enumerate(tfidf_learning_inputs): \n    lr = LogisticRegression(solver = 'saga', multi_class='multinomial', max_iter = 200)\n    lr.fit(ngram_model[0], ngram_model[2])\n    print('{}-gram TF-IDF:\\t Train Accuracy {}'.format(i+1,lr.score(ngram_model[0], ngram_model[2])))\n    print('{}-gram TF-IDF:\\t Test Accuracy {}'.format(i+1,lr.score(ngram_model[1], ngram_model[3])))\n    tfidf_log_models.append(lr)\n", "intent": "Basic logistic regression with one-versus-rest (OVR) multiclass scheme. \n"}
{"snippet": "lm_nointercept = LinearRegression(fit_intercept=False)\nlm_nointercept.fit(X,bos.PRICE)\n", "intent": "To not fit the intercept, we create a linear regression model with an additional argument: \n"}
{"snippet": "knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance') \nknn.fit(iris.data[:,2:], iris.target)\nprint knn.score(iris.data[:,2:], iris.target)\n", "intent": "Do we see a change in performance when using the distance weight?\n"}
{"snippet": "scaler.fit(data.drop(columns=['Class']))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\ndnn_classifier = learn.DNNClassifier(hidden_units=[10,20,10], n_classes=2, feature_columns=feature_columns)\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "model = GradientBoostingClassifier()\n", "intent": "Try a Gradient Boosting Classifier model by running the cell below. \n"}
{"snippet": "kmeans.fit(data.drop(columns=['Unnamed: 0', 'Private']))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "nb = MultinomialNB()\n", "intent": "Time to train a model!\n** Import MultinomialNB and create an instance of the estimator and call is nb **\n"}
{"snippet": "svc_model = SVC()\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "X_sparse_toy.todense()\n", "intent": "**Number of columns in `X_sparse_toy` should be 11, because in the toy example 3 users visited 11 unique websites.**\n"}
{"snippet": "model.fit(X_train, y_train,\n          validation_data=(X_val, y_val), epochs=20);\n", "intent": "Keras models follow __Scikit-learn__'s interface of fit/predict with some notable extensions. Let's take a tour.\n"}
{"snippet": "class ReLU(Layer):\n    def __init__(self):\n    def backward(self, input, grad_output):\n", "intent": "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\n"}
{"snippet": "weights = tf.Variable(tf.random_normal(shape=(X.shape[1],1), stddev=0.01, dtype=tf.float32))\nb = tf.Variable(0.0, dtype=tf.float32)\ns.run(tf.global_variables_initializer())\n", "intent": "__Your code goes here.__ For the training and testing scaffolding to work, please stick to the names in comments.\n"}
{"snippet": "autoencoder.fit(x=X_train,y=X_train,epochs=32, verbose=2,\n                validation_data=[X_test,X_test])\n", "intent": "Training may take some 20 minutes.\n"}
{"snippet": "autoencoder.fit(x=X_train,y=X_train,epochs=32,\n                validation_data=[X_test,X_test])\n", "intent": "Training may take some 20 minutes.\n"}
{"snippet": "model = KNeighborsClassifier(n_neighbors = 3)\n", "intent": "Try a k-nearest neighbors model by running the cell below. \n"}
{"snippet": "history = model.fit(X_train, y_train, batch_size = 50, validation_split = 0.3, epochs = 100, verbose = 1)\n", "intent": "- Training the model with training data provided\n"}
{"snippet": "model.add(Conv2D(32, (3, 3), input_shape=(50,65,3),activation='relu'))\nmodel.add(Conv2D(32,(3,3),activation='relu'))\n", "intent": "Declare 2 convolutional layers for the model\n"}
{"snippet": "a = mx.sym.Variable('a', shape=(1,))\nb_pl = mx.sym.Variable(\"b_pl\", shape=[0])\nc = ???\nd = ???\ne = ???(???, name='sum')\nf = ???(???, name='mean')\ng = ???\nmx.viz.plot_network(g)\n", "intent": "<img src=\"figs/fig3_new.png\",width=200>\n"}
{"snippet": "a = mx.sym.Variable('a', shape=(1,))\nb_pl = mx.sym.Variable(\"b_pl\", shape=[0])\nc = a * b_pl\nd = a + b_pl\ne = mx.sym.sum(c, name='sum')\nf = mx.sym.mean(d, name='mean')\ng = e - f\nmx.viz.plot_network(g)\n", "intent": "<img src=\"figs/fig3_new.png\",width=200>\n"}
{"snippet": "W = tf.Variable([.3], tf.float32)\nb = tf.Variable([-.3], tf.float32)\nx = tf.placeholder(tf.float32)\nlinear_model = W * x + b\n", "intent": "Has Variables, which Can be Updated Within the Graph \n"}
{"snippet": "fit_1 = fit = sm.GLS(df.wage, X1).fit()\nfit_2 = fit = sm.GLS(df.wage, X2).fit()\nfit_3 = fit = sm.GLS(df.wage, X3).fit()\nfit_4 = fit = sm.GLS(df.wage, X4).fit()\nfit_5 = fit = sm.GLS(df.wage, X5).fit()\nsm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1)\n", "intent": "Selecting a suitable degree for the polynomial of age.\n"}
{"snippet": "explainerAnchor.fit(dataset.train, dataset.labels_train, dataset.validation, dataset.labels_validation)\n", "intent": "The fit function inside anchor facilitate a discretization process if needed for ordinal features (in this dataset we use only categorical features)\n"}
{"snippet": "from xgboost import XGBRegressor\nmy_model = XGBRegressor()\nmy_model.fit(train_X, train_y, verbose=False)\n", "intent": "We build and fit a model just as we would in scikit-learn.\n"}
{"snippet": "from sklearn.svm import SVC \nclf = SVC(kernel='linear')\nclf.fit(X, y)\n", "intent": "**Let's Try**\nFit the model:\n"}
{"snippet": "model = GaussianNB()\n", "intent": "Try a Gaussian Naive Bayes model by running the cell below. \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, kernel_initializer='uniform',activation = 'relu', input_shape=x_train[0].shape))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nfrom keras import optimizers\nsgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nadadelta = keras.optimizers.Adadelta(lr=0.1, rho=0.95, decay=0.0)\nmodel.compile(loss='categorical_crossentropy', optimizer=adadelta,metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.summary()\nmodel.fit(x_train, y_train, epochs=600, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = Sequential()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "start = time.time()\nmodel.fit(x_train,y_train, batch_size=32, epochs=20,verbose=2)\nend = time.time()\nprint(\"time to train\",end - start)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, alpha=\"auto\")\nmodel\n", "intent": "Teraz wytrenujmy sam model LDA.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nfeature_cols = ['al']\nX = glass.loc[:, feature_cols]\ny = glass.loc[:, 'ri']\nlinreg = LinearRegression()\nlinreg.fit(X, y)\n", "intent": "- Instantiate and fit a linear regression model predicting `ri` from `al` (and an intercept).\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=64,\n          epochs=20,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, inputs_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "import tensorflow as tf\nx = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n", "intent": "This is the tensorflow way:\n"}
{"snippet": "model = LogisticRegression()\n", "intent": "Try a Logistic Regression model by running the cell below. \n"}
{"snippet": "W_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n", "intent": "$5\\times 5$ windows and $32$ features each\n"}
{"snippet": "batch_size=32\nEx1_history = Linear_model.fit(train_data,train_labels, batch_size=batch_size, epochs=5,\n                               validation_data =(valid_data,valid_labels))\n", "intent": "Training 10 epochs.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\n", "intent": "First Step, find the coefficients of your regression line\n"}
{"snippet": "for i in [X2,X4]:\n    lm1 = smf.ols(formula='y ~ i', data=BostonData).fit()\n    print(lm1.summary())\n    print \"\\n \\n ------------------------------------------ \\n \\n\"\n", "intent": "Answer: If the goal is interpretability, use either model 2 or 4. If all variables are significant, use model 4\n"}
{"snippet": "lm = LogisticRegression()\nlm.fit(X1,y)\n", "intent": "Run your regression line on X1 and interpret your MOMMY AND DADMY coefficients.\n"}
{"snippet": "def PredictThreshold(Predictprob,Threshhold):\n    y_predict = 0\n    if (Predictprob >= Threshhold):\n        y_predict = 1\n    return y_predict\n", "intent": "Answer: Lower the threshold for predicting Myopia. Under 2% would mean that <= 1 false negative exists in the \n"}
{"snippet": "treereg = DecisionTreeClassifier(max_depth=1,min_samples_leaf=5)\ntreereg.fit(X,y)\n", "intent": "Depth of 1 appears to be ideal for this tree\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nRF = RandomForestRegressor(n_estimators = 10000, \n                           max_features = 4,     \n                           min_samples_leaf = 5, \n                           oob_score = True)      \nRF.fit(X,y)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n"}
{"snippet": "rfc = RandomForestClassifier(n_estimators=1000)\n", "intent": "Now its time to train our model!\n**Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.**\n"}
{"snippet": "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n", "intent": "- **Kernel Ridge Regression** :\n"}
{"snippet": "model = SVC()\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "grid = GridSearchCV(estimator=SVC(), param_grid=param_grid, verbose=3, refit=True)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nX = mammals.loc[:, ['body']]\npf = PolynomialFeatures(degree=3, include_bias=False) \npf.fit(X)\nX = pf.transform(X)\n", "intent": "**sklearn has a \"transformer\" that generates polynomial terms**\n"}
{"snippet": "clf = tree.DecisionTreeClassifier(max_depth=3)\nclf.fit(X,y)\n", "intent": "* Now you get to build your decision tree classifier! First create such a model with `max_depth=3` and then fit it your data:\n"}
{"snippet": "images, labels = next(iter(trainloader))\nimg = images[63].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nprint(logits)\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "rf2 = RandomForestClassifier(n_estimators=500, \n                            criterion='entropy', \n                            max_features=1, \n                            max_depth=10, \n                            n_jobs=2)\nrf2.fit(X_train, y_train)\nfor i, column in enumerate(credit_clean.drop('Approve', axis = 1)):\n    print(column, rf2.feature_importances_[i])\n", "intent": "Compare the feature importances as estimated with the decision tree and random forest classifiers.\n"}
{"snippet": "lsvc = LinearSVC();\nlsvc.fit(X_train, y_train)\nprint('Train score: {:.3f}'.format(lsvc.score(X_train, y_train)))\nprint('Test score: {:.3f}'.format(lsvc.score(X_test, y_test)))\n", "intent": "Train a [`LinearSVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n"}
{"snippet": "rbf_svc = SVC(kernel=\"rbf\")\nrbf_svc.fit(X_train, y_train)\nprint('Train score: {:.3f}'.format(rbf_svc.score(X_train, y_train)))\nprint('Test score: {:.3f}'.format(rbf_svc.score(X_test, y_test)))\npoly_svc = SVC(kernel=\"poly\", degree=2)\npoly_svc.fit(X_train, y_train)\nprint('Train score: {:.3f}'.format(poly_svc.score(X_train, y_train)))\nprint('Test score: {:.3f}'.format(poly_svc.score(X_test, y_test)))\n", "intent": "By using the [`SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nC_vals = [0.0001, 0.001, 0.01, 0.1, .15, .25, .275, .33, 0.5, .66, 0.75, 1.0, 2.5, 5.0, 10.0, 100.0, 1000.0]\npenalties = ['l1','l2']\ngs = GridSearchCV(model, {'penalty': penalties, 'C': C_vals}, verbose=False, cv=15)\ngs.fit(X, y)\n", "intent": "For now, focus on giving Gridsearch different parameters and different penalities, where `C_vals =` something and `penalties = ` something\n"}
{"snippet": "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n", "intent": "- **Gradient Boosting Regression** :\nWith **huber**  loss that makes it robust to outliers\n"}
{"snippet": "from sklearn.cluster import DBSCAN\ndbscn = DBSCAN(eps = 3, min_samples = 3).fit(X)\n", "intent": "Remember to pass an epsilon and min_points of your choice.\n"}
{"snippet": "def relu(input):\n", "intent": "The Rectified Linear Activation Function\n"}
{"snippet": "best_param = param_range[np.argmin(test_errors)]\nprint 'our best cross-validated choice of parameter was: ' + param_name + ' = ' + str(best_param)\nclf = KernelRidge(kernel = 'poly',degree = best_param,alpha = 0)\ndata_x = data[:,0]\ndata_y = data[:,1]\nclf.fit(data_x[:, np.newaxis], data_y)  \nutils.plot_data(data,true_func)\nutils.plot_approx(clf,data)\n", "intent": "Now that we have computed the training and testing errors, lets choose the best one and then fit the corresponding regression model to our dataset.\n"}
{"snippet": "best_param = param_range[np.argmin(test_errors)]\nprint 'our best cross-validated choice of parameter was: ' + param_name + ' = ' + str(best_param)\nclf = MLPRegressor(solver = 'lbgfs',hidden_layer_sizes = best_param)\ndata_x = data[:,0]\ndata_y = data[:,1]\nclf.fit(data_x[:, np.newaxis], data_y)  \nutils.plot_data(data,true_func)\nutils.plot_approx(clf,data)\n", "intent": "Now lets fit a neural network model to the dataset with minimial testing error.\n"}
{"snippet": "best_param = param_range[np.argmin(test_errors)]\nprint 'our best cross-validated choice of parameter was: ' + param_name + ' = ' + str(best_param)\nclf = GradientBoostingRegressor(n_estimators = best_param, max_depth = 2)\ndata_x = data[:,0]\ndata_y = data[:,1]\nclf.fit(data_x[:, np.newaxis], data_y)  \nutils.plot_data(data,true_func)\nutils.plot_approx(clf,data)\n", "intent": "Now lets fit a gradient boosting model to the dataset with minimial testing error.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embedded_input = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.cluster import KMeans\nk = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(X_scaled)\n", "intent": "Cluster the Data to our our target groups.\n"}
{"snippet": "dbscn = DBSCAN(eps = 3, min_samples = 3).fit(X)  \n", "intent": "Remember to pass an epsilon and min_points of your choice.\n"}
{"snippet": "from statsmodels.tsa.arima_model import ARMA\nmodel = ARMA(store1_diff, (1, 0)).fit()\nmodel.summary()\n", "intent": "- Create an AR(1) model on the training data and compute the mean absolute error of the predictions. How effective is this model?\n"}
{"snippet": "rf_lv2=RandomForestClassifier(n_estimators=200, n_jobs=6, min_samples_split=5, max_depth=7,\n                          criterion='gini', random_state=0)\nrf_lv2_outcomes = cross_validate_sklearn(rf_lv2, lv1_train_df, y_train ,lv1_test_df, kf, \n                                            scale=True, verbose=True)\nrf_lv2_cv=rf_lv2_outcomes[0]\nrf_lv2_train_pred=rf_lv2_outcomes[1]\nrf_lv2_test_pred=rf_lv2_outcomes[2]\n", "intent": "Now let's try a few more algorithms on level 2, and let's revisit random forest again.\n"}
{"snippet": "X2=df1[['SibSp','Parch', 'Fare', 'first', 'second',\n       'third', 'embarked_C', 'embarked_Q', 'Child', 'Older Person',\n       'Male', 'unknownAge', 'unknownCabin']]\nlm2 = LogisticRegression()\nlm2.fit(X2, y)\nlm2.score(X2, y)\n", "intent": "Logistic model with all variables and using grid search cross validation \n"}
{"snippet": "with pm.Model() as model:\n    pm.glm.glm('default ~ SEX + EDUCATION + MARRIAGE + AGE', data, family=pm.glm.families.Binomial())\n    trace_logistic_model = pm.sample(2000, pm.NUTS(), progressbar=True)\n", "intent": "Note, this will take some time to run! It's fitting 2000 different samples and finding best parameters over time as it does so!\n"}
{"snippet": "benchmark_raw = LinearRegression()\n", "intent": "Instantiate a new `LinearRegression` model and save it as `benchmark_raw`.\n"}
{"snippet": "type(Ridge()) == Ridge\n", "intent": "These are your benchmark results. You will refer to these for analysis during the next phase.\n"}
{"snippet": "cross_validated_ridge = RidgeCV(alphas=np.logspace(-2,4,20))\ncross_validated_lasso = LassoCV(alphas=np.logspace(-2,4,20))\ncross_validated_elasticnet = ElasticNetCV(alphas=np.logspace(-2,4,20), l1_ratio=np.linspace(0,1,20))\ncross_validated_ridge_scaled = RidgeCV(alphas=np.logspace(-2,4,20))\ncross_validated_lasso_scaled = LassoCV(alphas=np.logspace(-2,4,20))\ncross_validated_elasticnet_scaled = ElasticNetCV(alphas=np.logspace(-2,4,20), l1_ratio=np.linspace(0,1,20))\n", "intent": "Perform the cross-validation using an `np.logspace(-2,4,7)`.\n"}
{"snippet": "OUTPUT_DIMENSION = 3\nNUMBER_MIXTURES = 5\nmodel = keras.Sequential()\nmodel.add(keras.layers.LSTM(HIDDEN_UNITS, batch_input_shape=(None,SEQ_LEN,OUTPUT_DIMENSION), return_sequences=True))\nmodel.add(keras.layers.LSTM(HIDDEN_UNITS))\nmodel.add(mdn.MDN(OUTPUT_DIMENSION, NUMBER_MIXTURES))\nmodel.compile(loss=mdn.get_mixture_loss_func(OUTPUT_DIMENSION,NUMBER_MIXTURES), optimizer=keras.optimizers.Adam())\nmodel.summary()\n", "intent": "Now let's set up the model:\n"}
{"snippet": "def cross_entropy(A, X_train2, Y_train2):\n    Y_pred = softmax(X_train2@A) \n    return -np.sum(Y_train2*np.log(Y_pred))/X_train2.shape[0]\n", "intent": "but accuracy function is not differentiable because it has jumps, hence we can minimize error function instead\n....Or minimize Error\n"}
{"snippet": "lasso = Lasso(alpha=0.01, normalize=True)\nlasso.fit(X_train_red, y_train_red)\n", "intent": "$\\lambda$ must be lowered because there is less noise to learn from.\n"}
{"snippet": "kmeans = KMeans(n_clusters=6, init='k-means++')\n", "intent": "  * Maak K-Means aan door middel van SK-Learn. Kies de instellingen `n_clusters=6`, en `init='k-means++'`\n"}
{"snippet": "logit_lv2=LogisticRegression(random_state=0, C=0.5)\nlogit_lv2_outcomes = cross_validate_sklearn(logit_lv2, lv1_train_df, y_train ,lv1_test_df, kf, \n                                            scale=True, verbose=True)\nlogit_lv2_cv=logit_lv2_outcomes[0]\nlogit_lv2_train_pred=logit_lv2_outcomes[1]\nlogit_lv2_test_pred=logit_lv2_outcomes[2]\n", "intent": "Logistic Regression, take 2\n"}
{"snippet": "kmeans = KMeans(n_clusters=6, init='k-means++').fit(X)\n", "intent": "  * Maak K-Means aan door middel van SK-Learn. Kies de instellingen `n_clusters=6`, en `init='k-means++'`\n"}
{"snippet": "plot_KMeans(X, centroids, labels, N=None)\n", "intent": "  * Plot de data, centroids en labels.\n  * Wat gebeurt er als je het algoritme meerdere keren draait? Worden altijd dezelfde zes clusters gevonden?\n"}
{"snippet": "perceptron.fit(X_pairs, y_and);\n", "intent": "  * Train de perceptron op `X_pairs` met `y_and` als target.\n"}
{"snippet": "perceptron.fit(X_pairs, y_or);\n", "intent": "  * Train de perceptron op `X_pairs` met `y_or` als target.\n"}
{"snippet": "perceptron.fit(X_pairs, y_xor);\n", "intent": "  * Train de perceptron op `X_pairs` met `y_xor` als target.\n"}
{"snippet": "mlp = MLPClassifier(hidden_layer_sizes=(2,),\n                    max_iter=2000,\n                    learning_rate_init=0.8,\n                    solver='sgd', \n                    activation='logistic',\n                    momentum=0,\n                    tol=0.00000001)\n", "intent": "Hieronder wordt een Multi-Layer Perceptron gedefinieerd met twee hidden units.\n"}
{"snippet": "mlp.fit(X_pairs, y_xor);\n", "intent": "  * Train de MLP op `X_pairs` met `y_xor` als target.\n"}
{"snippet": "mlp = MLPClassifier(solver='sgd', \n                    activation='logistic',\n                    momentum=0.2,\n                    tol=0.00001)\n", "intent": "Hieronder is een MLP gedefinieerd. Enkele parameters zijn al ingesteld. \n"}
{"snippet": "hiddenunits_params = [(1),(3),(10),(20),(30), (50), (100)]\nlearningrate_params = [0.01, 0.05, 0.1, 0.5, 1.0]\nparameters = {'hidden_layer_sizes':hiddenunits_params, 'learning_rate_init':learningrate_params}\nclf = GridSearchCV(mlp, parameters, cv=3).fit(X_train, y_train)\n", "intent": "  * Voer een 5-fold cross validated grid search uit om de optimale waarden te vinden voor het aantal hidden units en de learning rate\n"}
{"snippet": "logit_lv3=LogisticRegression(random_state=0, C=0.5)\nlogit_lv3_outcomes = cross_validate_sklearn(logit_lv3, lv2_train, y_train ,lv2_test, kf, \n                                            scale=True, verbose=True)\nlogit_lv3_cv=logit_lv3_outcomes[0]\nlogit_lv3_train_pred=logit_lv3_outcomes[1]\nlogit_lv3_test_pred=logit_lv3_outcomes[2]\n", "intent": "and of course that something linear is going to be Logistic Regression\n"}
{"snippet": "perceptron.fit(X_pairs,y_or)\n", "intent": "  * Train de perceptron op `X_pairs` met `y_or` als target.\n"}
{"snippet": "perceptron.fit(X_pairs,y_xor)\n", "intent": "  * Train de perceptron op `X_pairs` met `y_xor` als target.\n"}
{"snippet": "mlp = MLPClassifier(hidden_layer_sizes=(2),\n                    max_iter=2000,\n                    learning_rate_init=0.8,\n                    solver='sgd', \n                    activation='logistic',\n                    momentum=0,\n                    tol=0.00000001)\n", "intent": "Hieronder wordt een Multi-Layer Perceptron gedefinieerd met twee hidden units.\n"}
{"snippet": "mlp.fit(X_pairs,y_xor)\n", "intent": "  * Train de MLP op `X_pairs` met `y_xor` als target.\n"}
{"snippet": "c_params = [0.01, 0.03, 0.1, 0.3, 1, 3]\nhidden_layer_params = [(100,),(50,),(10,),(1,)]\nparameters = {'hidden_layer_sizes':hidden_layer_params, 'learning_rate_init':c_params}\nclf = GridSearchCV(mlp, parameters, cv=5).fit(X_train, y_train)\n", "intent": "  * Voer een 5-fold cross validated grid search uit om de optimale waarden te vinden voor het aantal hidden units en de learning rate\n"}
{"snippet": "c_params = [0.01, 0.03, 0.1, 0.3, 1, 3]\nkernel_params = ['linear', 'rbf']\nparameters = {'kernel':kernel_params, 'C':c_params}\nclfSVM = GridSearchCV(SVC(), parameters, cv=5).fit(xtc, y)\n", "intent": "We proberen SVM en Naive Bayes als learning algoritme om de voorspelling te doen welke data bij welke wandelaar hoort.\n"}
{"snippet": "alpha_params = [0.1, 0.3, 0.5, 0.7, 1.0]\nparameters = {'alpha':alpha_params}\nclfNB = GridSearchCV(MultinomialNB(), parameters, cv=5).fit(xtc,y)\n", "intent": "Score SVM op trainingsdata is bij k=30 is 0.72. Bij k=20 was SVM score 0.68. Meer clusters geeft hogere score.\nEens kijken wat Naive Bayes doet. \n"}
{"snippet": "mlp = MLPClassifier(solver='lbfgs', \n                    activation='logistic',\n                    momentum=0.2,\n                    tol=0.00001)\n", "intent": "Naive Bayes doet het slechter dan SVM. We proberen een decision tree.\n"}
{"snippet": "tree = DecisionTreeClassifier()\n", "intent": "Ook de multilayer classifier werkt niet beter als SVM. Als laatste een decision tree.\n"}
{"snippet": "KRR = KernelRidge(alpha=0.1, coef0=20, degree=2, gamma=100.0, kernel='polynomial',kernel_params=None)\n", "intent": "- **Kernel Ridge Regression** :\n"}
{"snippet": "mlp = MLPClassifier(solver='lbfgs', \n                    activation='logistic',\n                    momentum=0.2,\n                    tol=0.00001)\n", "intent": "Naive Bayes doet het slechter dan SVM. We proberen een neuraal net.\n"}
{"snippet": "max_K = 30\ndistortions =  np.zeros(max_K)\nfor K in range(max_K):\n    kmeans = KMeans(n_clusters=K+1)\n    kmeans.fit(np.vstack(X_train_windows))\n    distortions[K] = kmeans.inertia_\n", "intent": "Eerst bepalen wat een goed aantal clusters is.\n"}
{"snippet": "km = KMeans(n_clusters=10, init='k-means++', max_iter=100, n_init=1)\n", "intent": "Do the actual clustering\n"}
{"snippet": "model = Sequential()\nmodel.add(Embedding(10000, 32))\nmodel.add(SimpleRNN(32, return_sequences=True))\nmodel.add(SimpleRNN(32, return_sequences=True))\nmodel.add(SimpleRNN(32, return_sequences=True))\nmodel.add(SimpleRNN(32)) \nmodel.summary()\n", "intent": "Stack recurrent layers\n"}
{"snippet": "model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation=tf.sigmoid))\nmodel.summary()\n", "intent": "let's train the network\n"}
{"snippet": "advifit_2 = pm.ADVI( model=nn_mixture)\nadvifit_2.fit(n=15000, obj_optimizer=pm.adam())\n", "intent": "**D2**: Sample from the posterior predictive as you did in B4 and produce a diagram like C4 and B5 for this model.\n"}
{"snippet": "def make_pred(X_set):\n    output = model2.forward(Variable(torch.from_numpy(X_set).float()))\n    return output.data.numpy().argmax(axis=1)\n", "intent": "We can wrap this machinery in a function, and pass this function to `points_plot` to predict on a grid and thus give us a boundary viz\n"}
{"snippet": "import sklearn.linear_model\nols = sklearn.linear_model.LinearRegression()\ncolumns = ['dwelling_type_Condo','dwelling_type_Multi-Family','dwelling_type_Residential']\nols.fit(sacramento_with_dummies[columns],sacramento_with_dummies.price)\nzip(columns,ols.coef_)\n", "intent": "even ignoring one column gives us sensible,comprehensive results.\n"}
{"snippet": "knn = sklearn.neighbors.KNeighborsRegressor()\nknn.fit(table[['sq__ft']],table['price'])\n", "intent": "K Neighbors Regressor\n"}
{"snippet": "GBR = GradientBoostingRegressor(n_estimators=100, learning_rate=0.05,\n                                max_depth=4, max_features=0.1,\n                                min_samples_leaf=17, min_samples_split=10, \n                                loss='huber', random_state =2017)\n", "intent": "- **Gradient Boosting Regression** :\nWith **huber**  loss that makes it robust to outliers\n"}
{"snippet": "lr.fit(titanic_temp[['Pclass']], titanic_temp.Survived)\n", "intent": "Errm, predicting based on age doesn't work.\n"}
{"snippet": "base_model = ResNet50(input_tensor=Input((224, 224, 3)), weights='imagenet', include_top=False)\nfor layers in base_model.layers:\n    layers.trainable = False\nx = GlobalAveragePooling2D()(base_model.output)\nx = Dropout(0.25)(x)\nx = Dense(1, activation='sigmoid')(x)\nmodel = Model(base_model.input, x)\n", "intent": "https://github.com/fchollet/keras/blob/master/keras/applications/resnet50.py\n"}
{"snippet": "X = bikes.loc[:, ['temp_celsius', 'atemp_celsius']]\ny = bikes.loc[:, 'num_total_users']\nlr_temp_atemp = LinearRegression()\nlr_temp_atemp.fit(X,y)\n", "intent": "**Exercise.**\n- Create another `LinearRegression` instance that is fit using `temp_celsius` and `atemp_celsius`, and print the coefficients.\n"}
{"snippet": "X = bikes.loc[:, ['atemp_celsius']]\ny = bikes.loc[:, 'num_total_users']\nlr_atemp = LinearRegression()\nlr_atemp.fit(X,y)\n", "intent": "- Create another `LinearRegression` instance that is fit using `atemp_celsius` only, and print the coefficients.\n"}
{"snippet": "feature_cols = ['temp_celsius', 'atemp_celsius']\nX = bikes.loc[:, feature_cols]\ny = bikes.loc[:, 'num_total_users']\nlr_temp_atemp = LinearRegression()\nlr_temp_atemp.fit(X, y)\nlist(zip(X.columns, lr_temp_atemp.coef_))\n", "intent": "**Exercise.**\n- Create another `LinearRegression` instance that is fit using `temp_celsius` and `atemp_celsius`, and print the coefficients.\n"}
{"snippet": "feature_cols = ['atemp_celsius']\nX = bikes[feature_cols]\ny = bikes.num_total_users\nlr_atemp = LinearRegression()\nlr_atemp.fit(X, y)\nlist(zip(X.columns, lr_atemp.coef_))\n", "intent": "- Create another `LinearRegression` instance that is fit using `atemp_celsius` only, and print the coefficients.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nX = glass.loc[:, ['al']] \ny = glass.loc[:, 'ri']\nlinreg.fit(X, y)\n", "intent": "- Instantiate and fit a linear regression model predicting `ri` from `al` (and an intercept).\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nX = mammals.loc[:, ['body']]\npf = PolynomialFeatures(degree=3, include_bias=False)\npf.fit(X)\npf.transform(X)  \n", "intent": "**sklearn has a \"transformer\" that generates polynomial terms**\n"}
{"snippet": "feature_cols = ['DIS']\nX = boston.loc[:, feature_cols]\ny = boston.loc[:, 'MEDV']\nlr_boston1 = LinearRegression()\nlr_boston1.fit(X, y)\n", "intent": "- Create a linear regression model for MEDV against DIS with no higher-order polynomial terms.\n"}
{"snippet": "reg = xgb.XGBRegressor(objective='reg:linear',n_estimators=200)\nreg.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=10,eval_metric=rmsle_metric)\n", "intent": "Now with the same data we use objective='reg:linear', which is MSE. Although objective doesn't match our metric, the results are better.\n"}
{"snippet": "X_train_dtm.todense()\n", "intent": "Q: Why is it stored as a sparse matrix?\n"}
{"snippet": "inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "intent": "Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit.\n"}
{"snippet": "model.fit(X_t, y, batch_size=128, epochs=2, validation_split=0.35)\n", "intent": "Now we're ready to fit out model! Use `validation_split` when not submitting.\n"}
{"snippet": "def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r\n", "intent": "Fit a model for one dependent at a time:\n"}
{"snippet": "clf = DecisionTreeClassifier(random_state=3, max_depth=3)\nmodel = clf.fit(X, y)\n", "intent": "Here we create a Decision Tree classifier and fit the model to the training data.\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 4))\nnet.add(ReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "y = df[\"Hired\"]\nX = df[features]\nclf = tree.DecisionTreeClassifier(random_state=1)\nclf = clf.fit(X,y)\n", "intent": "Now actually construct the decision tree:\n"}
{"snippet": "model = LogisticRegression(penalty = 'l1', C = .1) \nmodel.fit(X, y)\nexamine_coefficients(model, X)\n", "intent": "- Change the `C` parameter\n    - how do the coefficients change? (use `examine_coeffcients`)\n    - how does the model perfomance change (using AUC)\n"}
{"snippet": "gs2 = GridSearchCV(KNeighborsClassifier(),\n                   {'n_neighbors': np.arange(1,50,5),\n                    'weights': ['uniform', 'distance'],\n                    'algorithm': ['ball_tree', 'kd_tree', 'brute']},\n                   cv=5)\ngs2.fit(X_train,y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "print(\"-- Grid Parameter Search via 10-fold CV\")\nparam_grid = {\"criterion\": [\"mse\"],\n              \"max_depth\": [None, 2, 5], \n              'n_estimators': [500, 700], \n              \"min_samples_leaf\": [2, 5], \n              'min_samples_split': [2, 4] \n             }\nrfr = RandomForestRegressor()\nrfr_grid = run_gridsearch(x_localtrain[col], y_localtrain, rfr, param_grid, cv=5)\n", "intent": "** Find Parameters **\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nparam_dict = dict(n_neighbors=range(1, 31),\\\n                  weights=['uniform', 'distance'])\ngsknncv = GridSearchCV(knn, param_dict, scoring='accuracy')\ngscv_model = gsknncv.fit(X_train, y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "gsap = GridSearchCV(lr_tts, logreg_parameters,\\\n                  verbose=False, cv=15, scoring='average_precision')\ngsap.fit(X_train, y_train)\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "X_train = X[:60000]\ny_train = y[:60000]\nX_test = X[60000:]\ny_test = y[60000:]\nlogistic = LogisticRegression(penalty = 'l2', C = 1.0, solver = 'newton-cg', multi_class = 'multinomial')\nlogistic.fit(X_train, y_train)\n", "intent": "The l2 model performed the best, followed by the l1 model, and then by the model with no/very little regularization.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding=tf.Variable(tf.random_normal((vocab_size,embed_dim),-1,1))\n    embed=tf.nn.embedding_lookup(embedding,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "tree = DecisionTreeClassifier().fit(np.squeeze(Xtrain), ytrain)\n", "intent": "Now we are going to fit the decision tree classifier to the entire (un-PCA'd) dataset (not just the first 2 features)\n"}
{"snippet": "from sklearn.svm import LinearSVC\nfrom sklearn.grid_search import GridSearchCV\ngrid = GridSearchCV(LinearSVC(), {'C': [0.125, 0.25, 0.5, 1.0]})\ngrid.fit(X_train, y_train)\ngrid.best_score_\n", "intent": "Let's try the support vector machine, with a grid search over a few choices of the C parameter:\n"}
{"snippet": "gnb = GaussianNB(priors=[0.5,0.5])\ngnb.fit(data_train, target)\n", "intent": "What if we adjust the priors? Say, that there is a 50% prior likelihood of sand pixels\n"}
{"snippet": "import  tensorflow  as tf\nx = tf.constant(35, name='x')\ny = tf.Variable(x + 5, name='y')\nprint(y)\n", "intent": "a TensorFlow equivalent:\n"}
{"snippet": "weights = {\n    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n    'out': tf.Variable(tf.random_normal([n_hidden_2, N]))\n}\nbiases = {\n    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n    'out': tf.Variable(tf.random_normal([N]))\n}\n", "intent": "Set up network weights and biases\n"}
{"snippet": "from sklearn.cluster import MiniBatchKMeans\nnum_clusters = 30 \nkmeans_model = MiniBatchKMeans(n_clusters=num_clusters,\n                               init='k-means++',\n                               n_init=1,\n                               init_size=1000, batch_size=1000, verbose=0, max_iter=1000)\n", "intent": "K-means clustering obejctive is to minimize the average squared Euclidean distance of the document / description from their cluster centroids. \n"}
{"snippet": "if decim>1:\n   Zx = Zx[::decim]\n   Zy = Zy[::decim]\ngraph = load_graph(classifier_file)\nw1 = []\nZ,ind = sliding_window(result, (tile,tile,3), (tile, tile,3))\nif decim>1:\n   Z = Z[::decim]\n", "intent": "Step 3: Load graph and partition image into tiles\n"}
{"snippet": "n = len(alabs)\ncm = np.zeros((n,n))\nfor amat, pmat in zip(a.flatten(), c2.flatten()):\n    cm[amat][pmat] += 1\n", "intent": "Let's create a confusion matrix to look at class-by-class comparisons\n"}
{"snippet": "def pool(T):\n    return tf.nn.max_pool(T, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n", "intent": "The pooling layer performs max. pooling with a 2x2 filter and stride of 2\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), minval=-1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "lmi = LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False)\nlmi\n", "intent": "**Your turn:** How would you change the model to not fit an intercept term? Would you recommend not having an intercept?\n"}
{"snippet": "def conv_block(x, filters, size, stride=(2,2), mode='same', act=True):\n    x = Conv2D(filters, (size, size), strides=stride, padding=mode)(x)\n    x = BatchNormalization()(x)\n    return Activation('relu')(x) if act else x\n", "intent": "conv with mode = 'same'\n"}
{"snippet": "def up_block(x, filters, size):\n    x = keras.layers.UpSampling2D()(x)\n    x = Conv2D(filters, (size, size), padding='same')(x)\n    x = BatchNormalization()(x)\n    return Activation('relu')(x)\n", "intent": "deconv (fractionaly strided conv with stride 1/2)\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    domain = (-0.1, 0.1)\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), domain[0], domain[1]))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def fully_connected(prev_layer, num_units, is_training):\n    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n    layer = tf.layers.batch_normalization(layer, training=is_training)\n    layer = tf.nn.relu(layer)\n    return layer\n", "intent": "Modified `fully_connected` to add batch normalization to the fully connected layers it creates. \n"}
{"snippet": "import sagemaker as sage\nsess = sage.Session()\n", "intent": "The session remembers our connection parameters to Amazon SageMaker. We'll use it to perform all of our SageMaker operations.\n"}
{"snippet": "class Variable():\n    def __init__(self, initial_value=None):\n        self.value = initial_value\n        self.output_nodes = []\n        _default_graph.variables.append(self)\n", "intent": "Changeable parameter of a Graph, mainly, _weights_.\n"}
{"snippet": "class Graph():\n    def __init__(self):\n        self.operations = []\n        self.placeholders = []\n        self.variables = []\n    def set_as_default(self):\n        global _default_graph\n        _default_graph = self\n", "intent": "Global variable joining variables, placeholders and operations.\n"}
{"snippet": "drop_layer = tf.nn.dropout(full_layer, hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "outputs, states = tf.nn.dynamic_rnn(rnn_cell, X, dtype = tf.float32)\n", "intent": "** Now pass in the cells variable into tf.nn.dynamic_rnn, along with your first placeholder (X)**\n"}
{"snippet": "    x = np.random.normal(0,1, (45, 28*28)).astype('float32') \n    model = lasagne.layers.get_output(l_out, sym_x)\n    out = model.eval({sym_x:x}) \n    print(\"l_out\", out.shape)\n", "intent": "Now we can use the power of Theano to implement the train, validation, and test functions.\n"}
{"snippet": "model =  Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax' ))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy', \n              optimizer='rmsprop', \n              metrics=['accuracy'] )\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "checkpoint = ModelCheckpoint(filepath='mnist.model.best.hdf5',\n                            verbose=1, save_best_only=True)\nhist = model.fit(x_train, y_train,\n          batch_size=32, epochs=10,  \n          validation_split=0.2, callbacks=[checkpoint],\n          verbose=2, shuffle=True)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "hold_1 = tf.nn.dropout(full_1, keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "data = bank_note.drop('Class', axis = 1)\nscaler.fit(data)\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "estimator.fit(inputs=inputs)\n", "intent": "The call to fit will launch the training job and regularly report on the different performance metrics related to the training. \n"}
{"snippet": "model.fit(x_train, y_train, epochs=6, batch_size=100, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=400)\nrfc.fit(X_train, y_train)\n", "intent": "Now let's compare the decision tree model to a random forest.\n"}
{"snippet": "scalar.fit(df.drop('TARGET CLASS',axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "gs = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\ngs.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "model = LogisticRegression()\nmodel = model.fit(X,y)\nmodel.score(X,y)\n", "intent": "Using Logistic Regression on the entire dataset lets observe the accuracy\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    genrule = np.sqrt(2/vocab_size)\n    embedding = tf.Variable(tf.random_uniform(shape=(vocab_size, embed_dim),\n                                              minval=-genrule, maxval=genrule))\n    embed = tf.nn.embedding_lookup(ids=input_data,\n                                   params=embedding,\n                                   name=\"embed\")\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, epochs=100, batch_size=128)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model.fit(X_train, Y_train, epochs=2, batch_size=32)\n", "intent": "Run the following cell to train your model on 2 epochs with a batch size of 32. On a CPU it should take you around 5min per epoch. \n"}
{"snippet": "inference_model = music_inference_model(\n    LSTM_cell, densor, n_values=78, n_a=64, Ty=50)\n", "intent": "Run the cell below to define your inference model. This model is hard coded to generate 50 values.\n"}
{"snippet": "from sagemaker import Model\nalgorithm_name = \"pytorch-fairseq-serve\"\nimage = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, algorithm_name)\nmodel = Model(model_data=trained_model_location,\n              role=role,\n              image=image,\n              predictor_cls=JSONPredictor,\n             )\n", "intent": "We can now use the Model class to deploy the model artificats (the pre-trained model), and deploy it on a CPU instance. Let's use a `ml.m5.xlarge`. \n"}
{"snippet": "A = tf.placeholder(tf.float32, shape = (None, 3))\nB = A + 5\nwith tf.Session() as s:\n    B1 = B.eval(feed_dict = { A: [[1, 2, 3]] })\n    B2 = B.eval(feed_dict = { A: [[4, 5, 6], \n                                  [7, 8, 9]] })\nprint(B1)\nprint(B2)\n", "intent": "Modify code to run as mini-batch GD\n"}
{"snippet": "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\nkeep_prob = 0.5\nX_drop = dropout(X, keep_prob, is_training=is_training)\nhidden1 = fully_connected(X_drop, n_hidden1, scope='hidden1')\nhidden1_drop = dropout(hidden1, keep_prob, is_training=is_training)\nhidden2 = fully_connected(hidden1_drop, n_hidden2, scope='hidden2')\nhidden2_drop = dropout(hidden2, keep_prob, is_training=is_training)\nlogits = fully_connected(\n    hidden2_drop, n_outputs, activation_fn=None, scope='outputs')\n", "intent": "Note: if overfitting, increase the dropout rate; if underfitting, decrease the dropout rate\n"}
{"snippet": "reset_graph()\nX0 = tf.placeholder(tf.float32, [None, n_inputs])\nX1 = tf.placeholder(tf.float32, [None, n_inputs])\nbasic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\noutput_seqs, states = tf.contrib.rnn.static_rnn(\n    basic_cell, [X0, X1], dtype=tf.float32)\nY0, Y1 = output_seqs\n", "intent": "Doing the same as above, using `static_rnn()`\n"}
{"snippet": "reset_graph()\nn_steps = 20\nn_inputs = 1\nn_outputs = 1\nn_neurons = 100\nETA = 0.001\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\ny = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n", "intent": "(Somewhat more complicated, but also more efficient)\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(\n        tf.truncated_normal([vocab_size, embed_dim], stddev=0.1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "normal_01_weights = [\n    tf.Variable(tf.random_normal(layer_1_weight_shape, stddev=0.1)),\n    tf.Variable(tf.random_normal(layer_2_weight_shape, stddev=0.1)),\n    tf.Variable(tf.random_normal(layer_3_weight_shape, stddev=0.1))]\nhelper.compare_init_weights(\n    mnist,\n    'Uniform [-0.1, 0.1) vs Normal stddev 0.1',\n    [(uniform_neg01to01_weights, 'Uniform [-0.1, 0.1)'),\n     (normal_01_weights, 'Normal stddev 0.1')])\n", "intent": "Let's compare the normal distribution against the previous uniform distribution.\n"}
{"snippet": "trunc_normal_01_weights = [\n    tf.Variable(tf.truncated_normal(layer_1_weight_shape, stddev=0.1)),\n    tf.Variable(tf.truncated_normal(layer_2_weight_shape, stddev=0.1)),\n    tf.Variable(tf.truncated_normal(layer_3_weight_shape, stddev=0.1))]\nhelper.compare_init_weights(\n    mnist,\n    'Normal vs Truncated Normal',\n    [(normal_01_weights, 'Normal'),\n     (trunc_normal_01_weights, 'Truncated Normal')])\n", "intent": "Again, let's compare the previous results with the previous distribution.\n"}
{"snippet": "from sklearn import tree\ntrain_X = train[[\"Sepal.Width\",\"Sepal.Length\", \"Petal.Length\", \"Petal.Width\"]]\ntrain_Y = train[\"Species\"]\ntest_X = test[[\"Sepal.Width\",\"Sepal.Length\", \"Petal.Length\", \"Petal.Width\"]]\ntest_Y = test[\"Species\"]\ntree_classifier = tree.DecisionTreeClassifier()\ntree_classifier.fit(train_X, train_Y)\n", "intent": "1) Fit a decision tree using all the features.\n"}
{"snippet": "def init_weights(shape):\n    return tf.Variable(tf.random_normal(shape, stddev=1))\n", "intent": "let's try another segment (i.e. 26160,10080,10200) and check the prediction results ..\n"}
{"snippet": "estimator.fit(inputs=inputs)\n", "intent": "The call to fit will launch the training job and regularly report on the different performance metrics such as losses. \n"}
{"snippet": "clf = tree.DecisionTreeClassifier()\nclf = clf.fit(training_data.values,training_data.TARGET)\nclf\n", "intent": "Let's try another algorithm which is \"Decision Tree - DT\" \n"}
{"snippet": "rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\n", "intent": "- http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"}
{"snippet": "clf.fit(train_X, train_y)\n", "intent": "from sklearn.externals import joblib\njoblib.dump(clf, '/home/jli/pr_xgb.pkl', compress=1)\n"}
{"snippet": "ada = AdaBoostClassifier(n_estimators=300)\n", "intent": "Compare and contrast Decision Trees and AdaBoost\n"}
{"snippet": "grid_dt = GridSearchCV(estimator = DecisionTreeClassifier(), \n                        param_grid = param_grid_dt, cv = 5, scoring = \"accuracy\")\ngrid_dt.fit(X, y)\n", "intent": "The CV in GridSearchCV stands for cross validation which means we have to set a cv and scoring value.\n"}
{"snippet": "grid_dt = GridSearchCV(estimator = DecisionTreeClassifier(), \n                        param_grid = param_grid_dt, cv = 5, scoring = \"accuracy\")\ngrid_dt.fit(X, y)\n", "intent": "It's going to cross-validate every combination between the criterion parameters and depth parameters.\n"}
{"snippet": "nn = NearestNeighbors(n_neighbors=5, metric=\"euclidean\")\nnn.fit(cities_ss)\n", "intent": "Time to fit the data on a NearestNeighbors object\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators = 50)\nrf.fit(X, y)\n", "intent": "Fit RF model on data and visualize it\n"}
{"snippet": "km4 = KMeans(n_clusters=4)\nkm4.fit(data)\nlabels = km4.labels_\nlabels\n", "intent": "Let's try it again with four clusters\n"}
{"snippet": "kmeans = sklearn.cluster.KMeans(n_clusters=10).fit(train_set[0])\n", "intent": "Again for simplicity, let's stick with the k-means algorithm.\n"}
{"snippet": "km5 = KMeans(n_clusters=5, random_state = 10)\nkm5.fit(X)\nlabs5 = km5.labels_\ndf2[\"labs5\"] = labs5\npd.value_counts(labs5)\n", "intent": "Exercise: Analyze the data using five clusters\n"}
{"snippet": "model = Sequential()\n", "intent": "Time to design the model.\nSetting up a Keras model takes more work than your a Sklearn model.\n"}
{"snippet": "n_cols = X.shape[1]\nmodel.add(Dense(10, activation=\"relu\", input_shape=(n_cols,)))\n", "intent": "Adding an input layer to our model using the Dense function\n"}
{"snippet": "model.fit(X, y_binary)\n", "intent": "Fitting time! Call .fit() like you would a sklearn model.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape=(n_cols,)))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(2, activation=\"softmax\"))\nmodel.compile(optimizer=\"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n", "intent": "This is a very simple model, it only has one shallow layer. Let's add some more layers.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape=(n_cols,)))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(2, activation=\"sigmoid\"))\nmodel.compile(optimizer=\"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\nmodel.fit(X, y_binary, epochs=40, validation_split = 0.25)\n", "intent": "We're trained a really good model, but principles of cross validation also to deep learning. Here's how we'll evaluate the model on a testing data.\n"}
{"snippet": "model.fit(Xr, yr, epochs = 20, validation_split=.25)\n", "intent": "Let's try it again but with train test split\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape=(n_cols,)))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer=\"adam\", loss = \"mean_squared_error\")\n", "intent": "**Back to the drawing board!**\nWe need more layers!!\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape=(n_cols,)))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer=\"adam\", loss = \"mean_squared_error\")\n", "intent": "Let's visualize the performance over the epochs, but first we have to reset the model.\n"}
{"snippet": "import sagemaker as sage\nsess = sage.Session()\n", "intent": "The session remembers our connection parameters to SageMaker. We use it to perform all of our SageMaker operations.\n"}
{"snippet": "knn3 = KNeighborsClassifier(n_neighbors=3)\nknn3.fit(X,y)\nscore3 = knn3.score(X,y)\nprint (\"The model accurately labelled {:.2f} percent of the data\".format(score3*100))\n", "intent": "Train a KNN model using 3 neighbors\n"}
{"snippet": "knn5 = KNeighborsClassifier(n_neighbors=5)\nknn5.fit(X,y)\nscore5 = float(knn5.score(X,y))\nprint (\"The model accurately labelled {:.2f} percent of the data\".format(score5*100))\n", "intent": "Now with 5 neighbors\n"}
{"snippet": "knn7 = KNeighborsClassifier(n_neighbors=7)\nknn7.fit(X, y)\nknn7.score(X, y)\n", "intent": "What about 7 neighbors?\n"}
{"snippet": "knn29 = KNeighborsClassifier(n_neighbors=29)\nknn29.fit(X, y)\nknn29.score(X, y)\n", "intent": "Let's try something much higher\n"}
{"snippet": "knn5 = KNeighborsClassifier(n_neighbors=5)\nknn5.fit(X_train, y_train)\nknn5.score(X_test, y_test)\n", "intent": "Fit model with 5 neighbors on training data and test model on testing data\n"}
{"snippet": "lm = smf.ols(formula='sales ~ TV + radio + newspaper', data=data).fit()\nlm.pvalues\n", "intent": "We could try a model with all features, and only keep features in the model if they have **small p-values**:\n"}
{"snippet": "lr = LinearRegression()\nX = df.drop(\"Y\", axis = 1)\ny = df.Y\nlr.fit(X, y)\nlr.score(X,y)\n", "intent": "Let's model this data using linear regresion in sklearn\n"}
{"snippet": "lr = LinearRegression()\nX = df[[\"X1\", \"X3\", \"X4\"]]\ny = df.Y\nlr.fit(X, y)\nlr.score(X,y)\n", "intent": "Run this again but only X1, X3, X4, the three most correlated variables with Y\n"}
{"snippet": "X = df.drop(\"target\", axis=1)\ny = df.target\nlr = LogisticRegression()\nlr.fit(X,y)\nscore = lr.score(X,y)\nprint (\"The model produces an accuracy score of {:.2f} percent\".format(score*100))\n", "intent": "Train a logistic regression model on the data to predict whether or not I will like a certain song\n"}
{"snippet": "import sagemaker as sage\nfrom time import gmtime, strftime\nsess = sage.Session()\n", "intent": "The session remembers our connection parameters to SageMaker. We'll use it to perform all of our SageMaker operations.\n"}
{"snippet": "dt = DecisionTreeClassifier(max_depth=4)\ndt.fit(X, y)\n", "intent": "Bonus!!\n<br><br>\nLet's see the most important features and visualize the decision tree\n"}
{"snippet": "def iris_model(x):\n    if x.petal_length < 2.5:\n        return \"setosa\"\n    else:\n        if x.petal_width > 1.75:\n            return \"virginica\"\n        else:\n            return \"versicolor\"\n", "intent": "x.feature_name allow us to apply function to data frame as opposed to a series.\n"}
{"snippet": "grid_dt = RandomizedSearchCV(estimator = DecisionTreeClassifier(), n_iter = 15,\n                        param_distributions = param_grid_dt, cv = 5, scoring = \"accuracy\")\nt = time()\ngrid_dt.fit(X, y)\nprint (time() - t)\n", "intent": "Obviously grid search takes a long time and in some case can cause memory errors. This is where RandomizedSearchCV comes in.\n"}
{"snippet": "fraud_ds_X = fraud_ds.drop(\"Class\", axis = 1)\nfraud_ds_y = fraud_ds.Class\nlr = LogisticRegression()\nlr.fit(fraud_ds_X, fraud_ds_y)\n", "intent": "Train Logistic Regression on downsampled data and evaluate it on testing data\n"}
{"snippet": "randsearch_cv = RandomizedSearchCV(pipe_cv, n_iter = 5,\n                        param_distributions = param_grid_cv, cv = 5, scoring = \"accuracy\")\nt = time()\nrandsearch_cv.fit(X, y)\nprint time() - t\n", "intent": "Randomized Search option\n"}
{"snippet": "randsearch_cv = RandomizedSearchCV(pipe_cv, n_iter = 10,\n                        param_distributions = param_grid_cv, cv = 5, scoring = \"accuracy\")\nt = time()\nrandsearch_cv.fit(X, y)\nprint (time() - t)\n", "intent": "Countvectorizer randomized search\n"}
{"snippet": "randsearch_tf = RandomizedSearchCV(pipe_tf, n_iter = 10,\n                        param_distributions = param_grid_tf, cv = 5, scoring = \"accuracy\")\nt = time()\nrandsearch_tf.fit(X, y)\nprint (time() - t)\n", "intent": "Tfidfvectorizer randomized search\n"}
{"snippet": "km4 = KMeans(n_clusters=4)\nkm4.fit(dtm)\n", "intent": "It is standard practice to cluster with tfidf data instead of the count vectorized data\n"}
{"snippet": "km4 = KMeans(n_clusters=4)\nkm4.fit(dist)\n", "intent": "Let's try this exercise again but this time we'll cluster the cosine distances.\n"}
{"snippet": "model_file_name = \"DEMO-local-xgboost-model\"\nbt._Booster.save_model(model_file_name)\n", "intent": "Note that the model file name must satisfy the regular expression pattern: `^[a-zA-Z0-9](-*[a-zA-Z0-9])*;`. The model file also need to tar-zipped. \n"}
{"snippet": "def iris_model(x):\n", "intent": "x.feature_name allow us to apply function to data frame as opposed to a series.\n"}
{"snippet": "randsearch_cv = RandomizedSearchCV(pipe_cv, n_iter = 5,\n                        param_distributions = param_grid_cv, cv = 5, scoring = \"accuracy\")\nt = time()\nrandsearch_cv.fit(X, y)\nprint (time() - t)\n", "intent": "Randomized Search option\n"}
{"snippet": "sm = pystan.StanModel(model_code=simple_STAN)\n", "intent": "Let's ellaborate a bit more with the input/outputs.\n"}
{"snippet": "sm = pystan.StanModel(model_code=simple_vector_STAN)\n", "intent": "With one data point at a time, we won't go that far, so it's time to work with vectors...  \n"}
{"snippet": "sm = pystan.StanModel(model_code=GMM_STAN)\n", "intent": "Study the code carefully\n"}
{"snippet": "sm = pystan.StanModel(model_code=model_definition)\nfit = sm.sampling(data=data, iter=1000, chains=4, algorithm=\"NUTS\", seed=42, verbose=True)\nprint(fit)\n", "intent": "Please run the above model, with the corresponding data\n"}
{"snippet": "alpha = pystan_utils.vb_extract_variable(fit, \"alpha\", var_type=\"real\")\nprint(\"alpha:\", alpha)\nbeta = pystan_utils.vb_extract_variable(fit, \"beta\", var_type=\"vector\")\nprint(\"beta:\", beta)\n", "intent": "We can also use pystan_utils to extract the mean values of the posteriors:\n"}
{"snippet": "sm = pystan.StanModel(model_code=model_definition)\nfit = sm.vb(data=data, iter=10000, algorithm=\"meanfield\", grad_samples=10, seed=42, verbose=True)\n", "intent": "Compile the model and run inference using ADVI:\n"}
{"snippet": "sm = pystan.StanModel(model_code=model_definition)\n", "intent": "Compile STAN program:\n"}
{"snippet": "from sagemaker.amazon.amazon_estimator import get_image_uri\ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')\n", "intent": "This involves creating a SageMaker model from the model file previously uploaded to S3.\n"}
{"snippet": "model.fit(X_train, y_train,\n          validation_data=(X_val, y_val), callbacks =[es], epochs=10);\n", "intent": "Keras models follow __Scikit-learn__'s interface of fit/predict with some notable extensions. Let's take a tour.\n"}
{"snippet": "class ReLU(Layer):\n    def __init__(self):\n        return np.maximum(input, 0)\n    def backward(self, input, grad_output):\n", "intent": "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\n"}
{"snippet": "class ReLU(Layer):\n    def __init__(self):\n        return np.maximum(0,input)\n    def backward(self, input, grad_output):\n", "intent": "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\n"}
{"snippet": "model(X_train, y_train, X_val, y_val, num_of_neurons = [784,256,128,128,128,10], init = 'Xavier')\n", "intent": "**Running the models with different initialization methods on a deep (5 layers) network:**\n"}
{"snippet": "weights = tf.Variable(initial_value=np.random.randn(X.shape[1], 1)*0.01,\n                              name=\"weights\", dtype=\"float32\")\nb = tf.Variable(initial_value=0, name=\"bias\", dtype=\"float32\")\n", "intent": "__Your code goes here.__ For the training and testing scaffolding to work, please stick to the names in comments.\n"}
{"snippet": "autoencoder.fit(x=X_train,y=X_train,epochs=80,\n                validation_data=[X_test,X_test])\n", "intent": "Training may take some 20 minutes.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=32,\n          epochs=10,\n          validation_data=(x_test,y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nclr = RandomForestRegressor(min_samples_leaf=20, n_estimators=50, min_weight_fraction_leaf=0.01, min_samples_split=10)\nclr.fit(X_train, y_train)\n", "intent": "[RandomForestRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingRegressor\nest = GradientBoostingRegressor(min_samples_leaf=20, n_estimators=50, min_weight_fraction_leaf=0.01, min_samples_split=10)\nest.fit(X_train, y_train)\n", "intent": "[GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n"}
{"snippet": "bucket='<<bucket-name>>' \nprefix = 'object2vec-movie-genre-prediction'\ncontainer = get_image_uri(boto3.Session().region_name, 'object2vec')\ntrain_s3_path = \"s3://{}/{}/data/train/\".format(bucket, prefix)\nvalidation_s3_path = \"s3://{}/{}/data/validation/\".format(bucket, prefix)\ntest_s3_path = \"s3://{}/{}/data/test/\".format(bucket, prefix)\nauxiliary_s3_path = \"s3://{}/{}/data/auxiliary/\".format(bucket, prefix)\nprediction_s3_path = \"s3://{}/{}/predictions/\".format(bucket, prefix)\n", "intent": "Let us start with defining some configurations \n"}
{"snippet": "np.sum(np.isnan(X.flatten()))\n", "intent": "**All digit got almost the same counts**\n"}
{"snippet": "model.fit(x_train, y_train, epochs=25, batch_size=200, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "X = df.drop('Private', axis=1)\nkmeans.fit(X)\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "df_features = df.drop('TARGET CLASS', axis=1)\nscaler.fit(df_features)\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlog = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "grid_model = GridSearchCV(SVC(), param_grid, verbose=4)\ngrid_model.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, batch_size=16, epochs=2)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "W = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n", "intent": "Here x and y_ aren't specific values. Rather, they are each a placeholder -- a value that we'll input when we ask TensorFlow to run a computation.\n"}
{"snippet": "def weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n", "intent": "Assign random numbers of weights and biases.\n"}
{"snippet": "kmeans.fit(kmeans.record_set(train_data))\n", "intent": "Then we train the model on our training data.\n"}
{"snippet": "W_fc2 = weight_variable([100, 10])\nb_fc2 = bias_variable([10])\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n", "intent": "Finally, we add a layer, just like for the one layer softmax regression above.\n"}
{"snippet": "predictors = [x for x in train.columns if x not in [target] + IDcol]\nalg6 = RandomForestRegressor(n_estimators=400,\n                             max_depth=6, \n                             min_samples_leaf=100,\n                             n_jobs=4)\nmodelfit(alg6, train, test, predictors, target, IDcol, 'alg6.csv')\ncoef6 = pd.Series(alg6.feature_importances_, predictors).sort_values(ascending=False)\ncoef6.plot(kind='bar', title='Feature Importances')\n", "intent": "- Try making another random forest with max_depth of 6 and 400 trees. \n"}
{"snippet": "clf = OneVsRestClassifier(RandomForestClassifier(n_jobs=-1, n_estimators=100, max_depth=10, random_state=42))\n", "intent": "This model was included as a example for the activity, we integrated this to our solution as a benchmark for our own model.\n"}
{"snippet": "model.fit(Xsmall, ysmall, batch_size=100, epochs=20,verbose = 1)\n", "intent": "Now lets fit our model!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), name='embedding')\n    embed = tf.identity(tf.nn.embedding_lookup(embedding, input_data), name='embed')\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nX = np.array([[xi**2, xi, 1] for xi in x])\nlin = LinearRegression(fit_intercept=False)\nlin.fit(X, y)\nprint(lin.coef_)\n", "intent": "We looked at a chart of our data and decided to describe it with:\n* A quadratic term  \n* A linear term\n* An offset\n"}
{"snippet": "import pulp\nb = pulp.LpVariable('b')\nw1 = pulp.LpVariable('w1')\nw2 = pulp.LpVariable('w2')\n", "intent": "Let's get hacking. First make variables for the separating hyperplane.\nNote these are _unrestricted in sign_.\n"}
{"snippet": "from sklearn.cluster import KMeans\nclust = KMeans(n_clusters=3)\nclust.fit(X)\n", "intent": "Assume we know a prioi how many clusters there are.\n"}
{"snippet": "w = []\nfor j in range(len(centers)):\n    x10 = pulp.LpVariable('x_10%d' % j, lowBound=0, upBound=1)\n    x20 = pulp.LpVariable('x_20%d' % j, lowBound=0, upBound=1)\n    w.append([x10, x20])\n", "intent": "Primary decision variables = locations of cluster centers\n"}
{"snippet": "uncorr_data = UncorrelationMethod(sensitive_feature_gender, 0.0)\nuncorr_data.fit(training_data_matrix) \nnew_training_data_matrix = np.hstack([uncorr_data.new_representation(training_data_matrix[:, :-1]),\n                                     training_data_matrix[:, -1:-2:-1]])\nnew_test_data_matrix = np.hstack([uncorr_data.new_representation(test_data_matrix[:, :-1]),\n                                  test_data_matrix[:, -1:-2:-1]])\n", "intent": "At this point we are ready to apply this algorithm to our data.\n"}
{"snippet": "x = []\nfor r in range(len(routes)):\n    x.append(pulp.LpVariable('x%d' % r, cat=pulp.LpBinary))\n", "intent": "Binary variables indicate which routes to use.\n"}
{"snippet": "z = pulp.LpVariable('z', lowBound=0)\nprm += z == sum(route_costs[r] * x[r] for r in range(len(routes)))\nprm.setObjective(z)\nassert prm.solve() == pulp.LpStatusOptimal\nbest_total_cost = z.value()\nprint('total cost:', best_total_cost)\n", "intent": "Objective = minimize total distance.\n"}
{"snippet": "z2 = pulp.LpVariable('z2', lowBound=0)\nfor r in range(len(routes)):\n    prm += z2 >= route_costs[r] * x[r]\nprm.setObjective(z2)\nassert prm.solve() == pulp.LpStatusOptimal\nprint('total cost:', z.value())\nprint('max route length:', z2.value())\n", "intent": "Some of our pedicabs complain that they have to travel much farther than others. Let's fix that by minimizing the maximum trip length.\n"}
{"snippet": "model.fit( map( list, ('CAGCATCAGT', 'C', 'ATATAGAGATAAGCT', 'GCGCAAGT', 'GCATTGC', 'CACATCACGACTAATGATAAT') ) )\nprint model.log_probability( list('CAGCATCAGT') ) \nprint model.log_probability( list('C') )\nprint model.log_probability( list('CACATCACGACTAATGATAAT') )\n", "intent": "We can fit the model to sequences which we pass in, and as expected, see that these sequences subsequently have a higher likelihood.\n"}
{"snippet": "full_1_drop=tf.nn.dropout(x=normal_full_1, keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\nencoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n", "intent": "Setup embeddings (see tutorial 1)\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embeded_layer = tf.nn.embedding_lookup(embedding, input_data)\n    return embeded_layer\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "ml_model(LogisticRegression(), 'Logistic Regression')\nml_model(GaussianNB(), 'Naive Bayes')\n", "intent": "We can now call our function for the two models that we've created and compare the algorithms:\n"}
{"snippet": "from sklearn.svm import SVC\nml_model(SVC(), 'SVM')\n", "intent": "It's also really easy for us to import and run a new model, in this case a Support Vector Machines (SVM) classifier:\n"}
{"snippet": "sage_session = sagemaker.session.Session()\ns3_bucket = sage_session.default_bucket()  \ns3_output_path = 's3://{}/'.format(s3_bucket) \n", "intent": "Set up the linkage and authentication to the S3 bucket that we want to use for checkpoint and metadata.\n"}
{"snippet": "global session\nsession = tf.Session()\n", "intent": "Once the TensorFlow graph has been created, we have to create a TensorFlow session which is used to execute the graph.\n"}
{"snippet": "import tensorflow as tf\nx = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.nn.softmax(tf.matmul(x, W) + b) \ny\n", "intent": "Define a neural network\n"}
{"snippet": "happyModel.fit(x=X_train,y=Y_train,batch_size=32,epochs=10,validation_split=0.1)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "import os\nos.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\nplot_model(model, to_file='model.png')\nSVG(model_to_dot(model).create(prog='dot', format='svg'))\n", "intent": "Finally, run the code below to visualize your ResNet50. You can also download a .png picture of your model by going to \"File -> Open...-> model.png\".\n"}
{"snippet": "G = nx.Graph()\nG.add_nodes_from(A, bipartite=0)\nG.add_nodes_from(B, bipartite=1)\nfor i in A:\n    for j in B:\n        G.add_edge(i,j, weight= - abs(score_treated.loc[i].prop_1 - score_untreated.loc[j].prop_1))\n", "intent": "We now construct the vertices by mapping every element of A to every element of B\n"}
{"snippet": "_, p = train.shape\nX = tf.placeholder('float', shape=[None, p], name=\"X\")\ny = tf.placeholder('float', shape=[None, 1], name=\"y\")\nV = tf.Variable(tf.random_normal([k, p], stddev=0.01), name=\"V\")\nw0 = tf.Variable(tf.zeros([1]), name=\"w0\")\nw = tf.Variable(tf.zeros([p]), name=\"w\")\n", "intent": "First, define the placeholders and variables needed for the model.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(50, activation='sigmoid', input_shape=(1000,)))\nmodel.add(Dropout(.15))\nmodel.add(Dense(20, activation='relu', input_shape=(100,)))\nmodel.add(Dropout(.15))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "import tensorflow as tf\nsoftmax_data = [0.7, 0.2, 0.1]\none_hot_data = [1.0, 0.0, 0.0]\nsoftmax = tf.placeholder(tf.float32)\none_hot = tf.placeholder(tf.float32)\ncross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\nwith tf.Session() as sess:\n    print(sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data}))\n", "intent": "to calculate the cross-entropy loss function use the below\n"}
{"snippet": "images, labels = next(iter(trainloader))\nimg = images[0].view(1,784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits,dim=1)\n", "intent": "with the network trained, we can check out it's predictions.\n"}
{"snippet": "sage_session = sagemaker.session.Session()\ns3_bucket = sage_session.default_bucket()  \ns3_output_path = 's3://{}/'.format(s3_bucket)\nprint(\"S3 bucket path: {}\".format(s3_output_path))\n", "intent": "Set up the linkage and authentication to the S3 bucket that you want to use for checkpoint and the metadata. \n"}
{"snippet": "def conv2d(x,W,b,strides=1):\n    x = tf.nn.conv2d(x,W,strides = [1,strides,strides,1],padding =\"same\")\n    x = tf.nn.bias_add(x,b)\n    return tf.nn.relu(x)\n", "intent": "* **tf.nn.conv2d()**\n* **tf.nn.bias_add()**\n* **tf.nn.relu()**\n"}
{"snippet": "def maxpool2d(x,k=2):\n    return tf.nn.max_pool(x,ksize=[1,k,k,1],strides=[1,k,k,1],\n                         padding = \"same\")\n", "intent": "* **tf.nn.max_pool()**\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers.core import Dense,Dropout,Activation\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils\nmodel = Sequential()\nmodel.add(Dense(18,activation =\"tanh\",input_shape=(1000,)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2,activation=\"softmax\"))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=1000, batch_size=124, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size,embed_dim],-1,1))\n    embed = tf.nn.embedding_lookup(embedding,input_data)\n    print(embed.get_shape())\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn import svm\nclf = svm.SVC(C=10)\nclf.fit(X_train, y_train)\n", "intent": "Train a SVM classifier\n"}
{"snippet": "neigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X, y);\n", "intent": "What we have above are one dimensional points. Logically, a decision boundary should exist at x = 1.5. We will verify if it holds\n"}
{"snippet": "k_means = KMeans(n_clusters=3, init='random')\nk_means.fit(X)\nlabels = k_means.labels_\n", "intent": "What is the difference between calling KMeans and any classifier using sklearn?\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu',input_shape=(1000,)))    \nmodel.add(Dropout(.4)) \nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(.4))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(optimizer='adam',\n             loss='categorical_crossentropy',\n             metrics=['accuracy']) \n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "sage_session = sagemaker.session.Session()\ns3_bucket = sage_session.default_bucket()  \ns3_output_path = 's3://{}/'.format(s3_bucket) \nprint(\"S3 bucket path: {}\".format(s3_output_path))\n", "intent": "Set up the linkage and authentication to the S3 bucket that you want to use for checkpoint and the metadata. \n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embed_var = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embedding = tf.nn.embedding_lookup(embed_var, input_data)\n    return embedding\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "discriminator_probabilities = tf.nn.softmax(discriminator_samples_logits)\ngenerator_loss = - tf.log(discriminator_probabilities[:, 1])\ngenerator_loss = tf.reduce_mean(generator_loss)\n", "intent": "Consider what you have seen above to define the generator loss.\n"}
{"snippet": "discriminator_samples_logits = ...     \ndiscriminator_probabilities = tf.nn.softmax(discriminator_samples_logits)\ngenerator_loss = ...\n", "intent": "Consider what you have seen above to define the generator loss.\n"}
{"snippet": "mean = tf.Variable(tf.zeros(shape=(), dtype=tf.float32), name=\"mean\")\nstd = tf.Variable(tf.ones(shape=(), dtype=tf.float32), name=\"std\")\nmodel = tfd.Normal(mean, std)\n", "intent": "We start with a model that is initialized to be Gaussian with 0 mean and standard deviation of 1.\n"}
{"snippet": "tf.reset_default_graph()\na = tf.constant(5, name='a')\nb = tf.constant(-1, name='b')\nc = tf.add(a, b, name='c')\nshow_graph(tf.get_default_graph())\n", "intent": "To solve this issue, tensorflow has `tf.reset_default_graph()`, which clears everything from the default graph.\n"}
{"snippet": "with tf.Session() as session:\n  print('a:', session.run(a))  \n  print('[b, c]:', session.run([b, c]))\n  print(session.run({'a': a, 'c': c}))\n", "intent": "You can run any node from your graph, or a combination of them.\n"}
{"snippet": "shape = tf.shape(inputs)\nnum_total_elements = tf.reduce_prod(shape)\nwith tf.Session() as session:\n  print(session.run([shape, num_total_elements], feed_dict={\n      inputs: np.array(np.random.random((3, 2, 2)))\n  }))\n", "intent": "The **dynamic shape itself is a tensor** and may (only) be evaluated or computed with once the graph is run in a session.\n"}
{"snippet": "linear = snt.Linear(output_size=5)\nlinear\n", "intent": "Start by creating a Linear module (dense layer).\n"}
{"snippet": "pre_activations = linear(inputs_placeholder)\n", "intent": "As in tensorflow, we \"call\" the module on the tensor that we want it to compute on. This yields a tensor, the output of the calculation.\n"}
{"snippet": "from sagemaker.tensorflow.serving import Model\nmodel = Model(model_data=estimator.model_data,\n              role=role)\npredictor = model.deploy(initial_instance_count=1, instance_type=instance_type)\n", "intent": "Now let us deploy the RL policy so that we can get the optimal action, given an environment observation.\n"}
{"snippet": "show_graph()   \n", "intent": "Let's see the graph we built.\n"}
{"snippet": "show_graph()\n", "intent": "We can verify on the graph that everything is as expected. The `name_scope` and `name` instructions make the graph easier to interpret.\n"}
{"snippet": "show_graph()\n", "intent": "**It is worth noting here the effect of this call on the graph.**\n"}
{"snippet": "tf.reset_default_graph()\nmodel = MySimpleModule(num_hidden=5)\n", "intent": "We can make a particular instance of the module we defined like so:\n"}
{"snippet": "show_graph()\n", "intent": "The connection triggered the `_build()` function and we can see the graph corresponding to the model is built.\n"}
{"snippet": "test_inputs, test_labels = test_dataset.make_one_shot_iterator().get_next()\ntest_outputs = model(test_inputs)\n", "intent": "The beauty of sonnet is that we can **connect the same `model` instance to the test data tensor and it will automatically share variables**.\n"}
{"snippet": "model.fit(x_train,y_cat_train,epochs=10)\n", "intent": "**TASK 6: Train/Fit the model to the x_train set. Amount of epochs is up to you.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size , embed_dim) , stddev=0.1) )\n    embed = tf.nn.embedding_lookup(embedding , input_data )\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model.fit(X, y)\nprint(model)\n", "intent": "Next, we fit the model to our data using the fit method. \n"}
{"snippet": "job_name_prefix = 'rl-roboschool-distributed-' + roboschool_problem\naws_region = boto3.Session().region_name\n", "intent": "We define variables such as the job prefix for the training jobs *and the image path for the container (only when this is BYOC).*\n"}
{"snippet": "model.add(\n    Dense(100, activation='relu', input_dim=X_train.shape[1])\n)\n", "intent": "In the first hidden layer, we must also specify the dimension of our input layer. This will simply be the number of elements (pixels) in each image.\n"}
{"snippet": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nn_folds = 5\nparameters = {'max_features': [2,3]}\nrf = RandomForestClassifier(max_depth=4)\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)\n", "intent": "Let's see how the model performance varies with ```max_features```, which is the maximum numbre of features considered for splitting at a node.\n"}
{"snippet": "param_grid = {\n    'max_depth': [4,8,10],\n    'min_samples_leaf': range(100, 400, 200),\n    'min_samples_split': range(200, 500, 200),\n    'n_estimators': [100,200, 300], \n    'max_features': [2,4]\n}\nrf = RandomForestClassifier()\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1,verbose = 1)\n", "intent": "We can now find the optimal hyperparameters using GridSearchCV.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=10,\n                             min_samples_leaf=100, \n                             min_samples_split=200,\n                             max_features=4,\n                             n_estimators=100)\n", "intent": "**Fitting the final model with the best parameters obtained from grid search.**\n"}
{"snippet": "lm.fit(X, bos.PRICE)\nprint 'Score: %f' % lm.score(X, bos.PRICE)\nprint 'Coefficients: \\n' + str(lm.coef_)\nprint 'Intercept: %f' % lm.intercept_\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "import tensorflow as tf\ntf.reset_default_graph()\nlearning_rate = 0.001\ntraining_epochs = 10\nbatch_size = 128\ndisplay_step = 1\nn_input = 784     \nn_classes = 10    \n", "intent": "We define some global parameters\n"}
{"snippet": "n_hidden_layer = 256 \nweights = {'hidden_layer' : tf.Variable(tf.random_normal([n_input, n_hidden_layer], name = 'W_0')),\n           'out' : tf.Variable(tf.random_normal([n_hidden_layer, n_classes]), name = 'W_1')}\nbiases = {'hidden_layer' : tf.Variable(tf.random_normal([n_hidden_layer]), name = 'b_0'),\n          'out' : tf.Variable(tf.random_normal([n_classes]), name = 'b_1')}\nx = tf.placeholder('float', [None, 28,28,1], name = 'input_x')\ny = tf.placeholder('float', [None, n_classes], name = 'target_y')\nx_flat = tf.reshape(x,[-1, n_input], name = 'input_x_flat')\n", "intent": "We define some parameter/variables for our neural networks\n"}
{"snippet": "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\nlayer_1 = tf.nn.relu(layer_1)\nlogits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n", "intent": "We define our layer with ReLU as following\n"}
{"snippet": "s3_bucket = '< ENTER BUCKET NAME HERE >'\nprefix = 'Scikit-LinearLearner-pipeline-abalone-example'\nimport sagemaker\nfrom sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n", "intent": "Let's first create our Sagemaker session and role, and create a S3 prefix to use for the notebook example.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogistic = LogisticRegression()\nlogistic.fit(X, y)\n", "intent": "(c) Fit a logistic regression model to the data, using X1 and X2 as predictors.\n"}
{"snippet": "clust = KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "clust.fit(cdf.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "km.fit(colleges.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "y = df_raw['TARGET CLASS']\nscaler.fit(df_raw.drop('TARGET CLASS',axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "scalar.fit(df.drop('TARGET CLASS', axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "model1 = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "model1.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogm = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "prefix = 'Scikit-iris'\nimport sagemaker\nfrom sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n", "intent": "First, lets create our Sagemaker session and role, and create a S3 prefix to use for the notebook example.\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid=param_grid, verbose=3)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "history = model.fit(x_train, y_train, epochs=2500, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\n", "intent": "We'll start just by training a single decision tree.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ndt = DecisionTreeRegressor()\nparameter_grid = {'min_impurity_split': 2. ** np.arange(-5, 5)}\nparam_searcher = GridSearchCV(dt, parameter_grid, cv=10)\nparam_searcher.fit(train_df[predictors], train_df[target])\n", "intent": "Sklearn doesn't allow _post-pruning_, but we can try some pre-pruning parameters\n"}
{"snippet": "W_l = tf.Variable(tf.zeros([5,1], name=\"weights\"))\nb_l = tf.Variable(0., name=\"bias\")\ndef combine_inputs(X):\n    return tf.matmul(X,W_l) + b\ndef inference_l(X):\n    return tf.sigmoid(combine_inputs(X))\n", "intent": "Logistic Regression\n"}
{"snippet": "model = MultinomialNB()\nmodel.fit(train_sparse, train_label)\n", "intent": "We have used sklearn library which implements 'Naive Bayes classifier for multinomial models'\n"}
{"snippet": "from sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nDT = DecisionTreeClassifier(random_state=0)\n", "intent": "Simple Decision Tree Classifier\n"}
{"snippet": "from sklearn.naive_bayes import BernoulliNB\nBNV = BernoulliNB()\nBNV.fit(X_train[features], y_train)\n", "intent": "Bernoulli Naive Bayes\n"}
{"snippet": "for i in [1,2,3,4]:\n    svcs = SVC(C=i)\n    svcs.fit(X_train[features], y_train)\n    print 'Accuracy for SVC with C=', str(i)\n    print svcs.score(X_test[features],y_test)\n", "intent": "Gains in Accuracy with greater C\n"}
{"snippet": "import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.session import Session\nsagemaker_session = sagemaker.Session()\nregion = sagemaker_session.boto_session.region_name\nrole = get_execution_role()\n", "intent": "First, we'll just set up a few things needed for this example\n"}
{"snippet": "ll = linear_model.LassoLars(alpha=.1)\nll.fit(X, y)\n", "intent": "Lasso Lars Regression\n"}
{"snippet": "br = linear_model.BayesianRidge()\n", "intent": "Bayesian Ridge Regression\n"}
{"snippet": "sv_reg = SVR()\n", "intent": "Using Nested GridSearch CV with more regressors\n"}
{"snippet": "neigh_reg = KNeighborsRegressor()\n", "intent": "K Neighbors Regressors\n"}
{"snippet": "linear_regression = linear_model.LinearRegression(normalize=False, fit_intercept=True)\n", "intent": "Now Rerunning the Linear Regression after the label encoding and binarization have been made. \n"}
{"snippet": "def r2_gbd_est_two(X,y, X_new, y_new, loss_, _rate):\n    gbd = GradientBoostingRegressor(loss = loss_, learning_rate = _rate)\n    gbd.fit(X,y)\n    return gbd.score(X_new, y_new)\ndef r2_gbd_est_two_huber(X,y, X_new, y_new):\n    return r2_gbd_est_two(X,y, X_new, y_new, 'huber', .1)\n", "intent": "Evaluating for impact of each feature\n"}
{"snippet": "from yellowbrick.features import Rank2D\nvisualizer = Rank2D(features=X_num[features].columns, algorithm='pearson')\nvisualizer.fit(X_num[features], y)                \nvisualizer.transform(X_num[features])             \nvisualizer.poof()  \n", "intent": "Feature Importance with Pearson Correlation\n"}
{"snippet": "from yellowbrick.regressor import ManualAlphaSelection\nfrom sklearn.linear_model import Ridge\nmodel = ManualAlphaSelection(Ridge(), cv=12, \n                             scoring='neg_mean_squared_error')\nmodel.fit(X_train, y_train)\nmodel.poof()\n", "intent": "ALpha Selection with Ridge CV but throws weird error \n"}
{"snippet": "import yellowbrick as yb\nfrom yellowbrick.features.pcoords import ParallelCoordinates\nfrom yellowbrick.base import VisualizerGrid\nvisualizers = [\n               ResidualsPlot(ridge),\n               ResidualsPlot(ridge),\n              ]\nvg = VisualizerGrid(visualizers, ncols=2)\nvg.fit(X,y)\nvg.poof()\n", "intent": "Grid Visualizations - Not working\n"}
{"snippet": "sagemaker.Session().delete_endpoint(predictor.endpoint)\n", "intent": "To avoid incurring charges to your AWS account for the resources used in this tutorial you need to delete the SageMaker Endpoint:\n"}
{"snippet": "chosen_model_ridge = linear_model.Ridge(alpha=0.1)\n", "intent": "We still have a seemingly acceptable RMSE with an alpha of 10^-1, let's test it and see how it performs\n"}
{"snippet": "gs.fit(X_train, y_train)\nprint(\"model score: %.3f\" % gs.score(X_test, y_test))\n", "intent": "The pipeline transformations are fit to the training set and applied to the test set to obtain a score and thus preventing data leakage.\n"}
{"snippet": "clf = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n", "intent": "Same tree with Max Depth parameter - higher depth, higher complexity\n"}
{"snippet": "model = load_model('bitcoin_lstm_v0.h5')\n", "intent": "Let's start by loading our previously trained model. \n"}
{"snippet": "model_v0 = load_model('bitcoin_lstm_v0.h5')\n", "intent": "For reference, let's load data for `v0` of our model and train it alongside future modifications.\n"}
{"snippet": "models = [model_v0, model_v1, model_v2, model_v3]\nfor i, M in enumerate(models):\n    predicted_days = evaluate_model(M, kind='other')\n    plot_weekly_predictions(predicted_days, 'model_v{}'.format(i), display_plot=False)\n", "intent": "Finally, let's evaluate each one the models trained in this activity in sequence. \n"}
{"snippet": "model.fit(x, y)\n", "intent": "Let us train this on the very simple data set from above.\n"}
{"snippet": "m = svm.SVR(kernel='rbf', C=1, gamma=0.1)\nm.fit(xtrain, ytrain)\n", "intent": "Now, train again the SVR only on the training data. I have chosen an rbf kernel now, just to see how that performs.\n"}
{"snippet": "knn_new = KNeighborsClassifier(n_neighbors = 3)\n", "intent": "Now as a function of m (training instances)\n"}
{"snippet": "import boto3\nregion = boto3.Session().region_name\ntrain_data = 's3://sagemaker-sample-data-{}/tensorflow/pipe-mode/train'.format(region)\neval_data = 's3://sagemaker-sample-data-{}/tensorflow/pipe-mode/eval'.format(region)\ntensorflow.fit({'train':train_data, 'eval':eval_data})\n", "intent": "After we've created the SageMaker Python SDK TensorFlow object, we can call fit to launch TensorFlow training:\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors = 15)\nknn.fit(np.array(X_train), np.ravel(y_train))\nknn.score(np.array(X_test), y_test)\n", "intent": "Now we redo this with a higher K:\n"}
{"snippet": "regressor_reg = Ridge().fit(X_train, y_train)\n", "intent": "Frequently, polynomial features are used in conjunction with regularization to avoid overfitting\n"}
{"snippet": "clf_svm = svm.SVC()\n", "intent": "Let's try GridSearchCV with our SVM Classifier\n"}
{"snippet": "history = model.fit(X_train_centered, y_train_onehot, batch_size=100, epochs=50, verbose=1, validation_split=0.1)\n", "intent": "The categorical cross entropy is the cost function for logistic regression generalized for multiclass problems\n"}
{"snippet": "history = model.fit(X_train_centered, y_train_onehot, batch_size=50, epochs=60, verbose=1, validation_split=0.1)\n", "intent": "The model is converging to a training loss of 0.07 and a validation loss of 0.11. Let's see if we can do better with different hyperparameters\n"}
{"snippet": "p = tf.Graph()\n", "intent": "Next, let's use Tensorflow methods to calculate sum and mean of arrays\n"}
{"snippet": "ridge = Ridge(alpha = 0.01)\n", "intent": "Higher alpha implies higher degree of regularization and lower model complexity i.e. sensitivity to overfitting\n"}
{"snippet": "ridge = Ridge(alpha = 10)\nridge.fit(X_train_scaled, y_train)\n", "intent": "Now let's rerun with a higher alpha:\n"}
{"snippet": "estimator = SGDClassifier()\n", "intent": "Nested CV with Time Series Split\n"}
{"snippet": "sagemaker.Session().delete_endpoint(predictor.endpoint)\n", "intent": "To avoid incurring charges to your AWS account for the resources used in this tutorial you need to delete the **SageMaker Endpoint:**\n"}
{"snippet": "best_ada = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n          learning_rate=0.5, n_estimators=50, random_state=1)\n", "intent": "Recreating the three classifiers with their best hyperparameters\n"}
{"snippet": "clf = LogisticRegression(C=1, max_iter=100, class_weight = 'balanced')\n", "intent": "Tune According to Campaign ROI as defined by differenced between avg loan and deposit interest rates and fixed/var campaign costs\n"}
{"snippet": "clf = LogisticRegressionCV(cv=5, Cs = [10**i for i in range(-2,2)],\n                           penalty = 'l2', random_state=0, \n                           scoring = 'accuracy',\n                           multi_class='ovr').fit(X_train, y_train)\nprint(clf.get_params)\nprint('\\n')\nprint(\"Prediction %s on Test Set: %.6f\" %(clf.scoring, clf.score(X_test, y_test)))\n", "intent": "Baseline model with Cross Validation\n"}
{"snippet": "def create_new_model(optimizer='rmsprop', init='glorot_uniform'):\n    model = Sequential()\n    model.add(Dense(72, input_dim=72,kernel_initializer=init, activation='relu'))\n    model.add(Dense(1, kernel_initializer=init,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n", "intent": "Testing Model with Hyperparameter Tuning\n"}
{"snippet": "def gridSearch_clf(clf, param_grid, X_train, y_train):\n    gs = GridSearchCV(clf, param_grid).fit(X_train, y_train)\n    print(\"Best Parameters\")\n    print(gs.best_params_)\n    return gs.best_estimator_\n", "intent": "We will implement a stacking algorithm that uses all three and makes a prediction based on their predictions\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB().fit(xTrainStandard,ytrain)\n", "intent": "Let's train our classifier by calling its `fit()` method:\n"}
{"snippet": "mu.save_predictions_from_model(best_model, 'vw_{}'.format(best_model['group']))\n", "intent": "Save the model for bagging\n"}
{"snippet": "svc = SVC(C=1.0, cache_size=500, coef0=0.0, degree=3,\n gamma=0.0, kernel='poly', max_iter=-1, probability=False,\nrandom_state=None, shrinking=True, tol=0.001, verbose=True)\nsvc.fit(X_train, Y_train)\nprint(svc)\n", "intent": "Now that the vector are ready, its time to apply the machine learning algorithms\n"}
{"snippet": "model1 = load_model(save_model_name, custom_objects={'my_iou_metric': my_iou_metric})\ninput_x = model1.layers[0].input\noutput_layer = model1.layers[-1].input\nmodel = Model(input_x, output_layer)\nc = optimizers.RMSprop(lr=0.01)\nmodel.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric_2])\n", "intent": "Then the default threshod for pixel prediction is 0 instead of 0.5, as in my_iou_metric_2.\n"}
{"snippet": "estimator_local.fit({\"train\":s3_train_path, \"test\":s3_test_path})\n", "intent": "Call `fit()` to start the local training \n"}
{"snippet": "model1 = load_model(save_model_name, custom_objects={'my_iou_metric': my_iou_metric})\ninput_x = model1.layers[0].input\noutput_layer = model1.layers[-1].input\nmodel = Model(input_x, output_layer)\nc = optimizers.Adam(lr = 0.001)\nmodel.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric_2])\n", "intent": "Then the default threshod for pixel prediction is 0 instead of 0.5, as in my_iou_metric_2.\n"}
{"snippet": "def conv_block_simple(prevlayer, filters, prefix, strides=(1, 1)):\n    conv = Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", strides=strides, name=prefix + \"_conv\")(prevlayer)\n    conv = BatchNormalization(name=prefix + \"_bn\")(conv)\n    conv = Activation('relu', name=prefix + \"_activation\")(conv)\n    return conv\ndef conv_block_simple_no_bn(prevlayer, filters, prefix, strides=(1, 1)):\n    conv = Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", strides=strides, name=prefix + \"_conv\")(prevlayer)\n    conv = Activation('relu', name=prefix + \"_activation\")(conv)\n    return conv\n", "intent": "https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py\n"}
{"snippet": "from keras import models\nfrom keras import layers\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\nnetwork.add(layers.Dense(10, activation='softmax'))\n", "intent": "Let's create the network by using the `Sequential` API of Keras\nDocumentation : https://keras.io/getting-started/sequential-model-guide/\n"}
{"snippet": "smaller_model_hist = smaller_model.fit(x_train, y_train,\n                                       epochs=20,\n                                       batch_size=512,\n                                       validation_data=(x_test, y_test))\n", "intent": "Train the model with the same parameters as the previous one\nDocumentation : https://keras.io/models/sequential/\n"}
{"snippet": "l2_model_hist = l2_model.fit(x_train, y_train,\n                             epochs=20,\n                             batch_size=512,\n                             validation_data=(x_test, y_test))\n", "intent": "Train the model with the same parameters as the previous one\nDocumentation : https://keras.io/models/sequential/\n"}
{"snippet": "dpt_model_hist = dpt_model.fit(x_train, y_train,\n                               epochs=20,\n                               batch_size=512,\n                               validation_data=(x_test, y_test))\n", "intent": "Train the model with the same parameters as the previous one\nDocumentation : https://keras.io/models/sequential/\n"}
{"snippet": "from keras.layers import Dense\nmodel = Sequential()\n", "intent": "> TODO : URL SimpleRNN\n"}
{"snippet": "res = []\nfor k in xrange(1,121):\n    res.append(cross_validate(X, y, neighbors.KNeighborsClassifier(k).fit, 5))\n", "intent": "To find the optimal k we accumulate the scores of a 5 folds cross validation in a list of results. Then we find the max score in that list.\n"}
{"snippet": "model = tree.DecisionTreeClassifier(random_state=0)\n", "intent": "Create a new Decision Tree classifier using default parameters:\n"}
{"snippet": "estimator.fit({\"train\":s3_train_path, \"test\":s3_test_path})\n", "intent": "Call `fit()` to start the training\n"}
{"snippet": "model = tree.DecisionTreeClassifier(random_state=0).fit\ncross_validate(X_train, y_train,model,10)\n", "intent": "What does cross-validation look like if we use only our Train set?\n"}
{"snippet": "model = tree.DecisionTreeClassifier(max_depth=3, random_state=0).fit\ncross_validate(iris.data, iris.target,model,10)\n", "intent": "model.feature_importances_\nDoes limiting the max depth affect our average model performance with this training data?\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation=\"relu\", input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation=\"softmax\"))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=30, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from pgmpy.models import BayesianModel\nmodel = BayesianModel([('fruit', 'tasty'), ('size', 'tasty')])  \n", "intent": "We know that the variables relate as follows:\n"}
{"snippet": "from sklearn.cluster import KMeans\nn=2 \ndd=X \ntar=y \nkm=KMeans(random_state=324,n_clusters=n)\nres=km.fit(dd)\nprint(res.labels_)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier as RFC\nrf = RFC(n_estimators=30, n_jobs=-1,max_leaf_nodes=10)\nrf.fit(X_train, Y_train)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"}
{"snippet": "from sklearn import svm\nclf = svm.SVC(kernel='linear',C=10**100)  \nclf.fit(X, Y)\n", "intent": "http://scikit-learn.org/stable/modules/svm.html\nhttp://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"}
{"snippet": "import boto3\nregion = boto3.Session().region_name\ntrain_data = 's3://sagemaker-sample-data-{}/tensorflow/pipe-mode/train'.format(region)\neval_data = 's3://sagemaker-sample-data-{}/tensorflow/pipe-mode/eval'.format(region)\ntensorflow.fit({'train':train_data, 'eval':eval_data})\n", "intent": "After we've created the SageMaker Python SDK TensorFlow object, we can call ``fit()`` to launch TensorFlow training:\n"}
{"snippet": "kmeans = KMeans(n_clusters=5, random_state=1337)  \nkmeans.fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "model.fit(x_train, y_train, epochs=1000, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n", "intent": "Next we will create the simplest possible neural network. It has 1 layer, and that layer has 1 neuron, and the input shape to it is just 1 value.\n"}
{"snippet": "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n", "intent": "Let's now design the model. There's quite a few new concepts here, but don't worry, you'll get the hang of them. \n"}
{"snippet": "test_accuracy = tfe.metrics.Accuracy()\nfor (x, y) in tfe.Iterator(test_dataset):\n  prediction = tf.argmax(model(x), axis=1, output_type=tf.int32)\n  test_accuracy(prediction, y)\nprint(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))\n", "intent": "Unlike the training stage, the model only evaluates a single [epoch](https://developers.google.com/machine-learning/glossary/\n"}
{"snippet": "x = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(1, activation='sigmoid')(x)\nmodel = Model(pre_trained_model.input, x)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=RMSprop(lr=0.0001),\n              metrics=['acc'])\n", "intent": "Now let's stick a fully connected classifier on top of `last_output`:\n"}
{"snippet": "model = Sequential()\nmodel2 = Dense()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_prime(x):\n    return sigmoid(x) * (1-sigmoid(x))\ndef error_formula(y, output):\n    return - y*np.log(output) - (1 - y) * np.log(1-output)\n", "intent": "The following function trains the 2-layer neural network. First, we'll write some helper functions.\n"}
{"snippet": "def sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n", "intent": "$g(z) = \\frac{1}{1+e^{-z}}$ expressed as:\n"}
{"snippet": "import os\nimport sagemaker\nfrom sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nregion = sagemaker_session.boto_session.region_name\n", "intent": "Let's start by setting up the environment:\n"}
{"snippet": "model = LogisticRegression(tol = .00000000000000000000001)\nmodel = model.fit(X, y)\nmodel.score(X, y)\n", "intent": "Now let's try fitting and scoring the model again:\n"}
{"snippet": "with tf.Session() as sess:\n    print(\"a: %i\" % sess.run(a), \"b: %i\" % sess.run(b))\n    print(\"Addition with constants: %i\" % sess.run(a+b))\n    print(\"Multiplication with constants: %i\" % sess.run(a*b))\n", "intent": "Let us launch the default graph.\n"}
{"snippet": "with tf.Session() as sess:\n    print(\"Addition with variables: %i\" % sess.run(add, feed_dict={a: 2, b: 3}))\n    print(\"Multiplication with variables: %i\" % sess.run(mul, feed_dict={a: 2, b: 3}))\n", "intent": "Let us launch the default graph.\n"}
{"snippet": "X = tf.placeholder(\"float\")\nY = tf.placeholder(\"float\")\nW = tf.Variable(rng.randn(), name=\"weight\")\nb = tf.Variable(rng.randn(), name=\"bias\")\n", "intent": "The input graph is created and weights for the model are set.\n"}
{"snippet": "def neural_net(x_dict):\n    x = x_dict['images']\n    layer_1 = tf.layers.dense(x, n_hidden_1)\n    layer_2 = tf.layers.dense(layer_1, num_classes)\n    out_layer = tf.layers.dense(layer_2, n_hidden_2)\n    return out_layer\n", "intent": "Define the neural network.\n"}
{"snippet": "def neural_net(x_dict):\n    x = x_dict['images']\n    layer_1 = tf.layers.dense(x, n_hidden_1)\n    layer_2 = tf.layers.dense(layer_1, n_hidden_2)\n    out_layer = tf.layers.dense(layer_2, num_classes)\n    return out_layer\n", "intent": "Define the neural network.\n"}
{"snippet": "G = nx.barabasi_albert_graph(1000000, 1)\ndegrees = G.degree()\ndegree_values = sorted(set(degrees.values()))\nhistogram = [list(degrees.values()).count(i)/float(nx.number_of_nodes(G)) for i in degree_values]\n", "intent": "To illustrate this, we create a graph with 1000000 nodes starting from 1 node.\n"}
{"snippet": "lm2 = LinearRegression(fit_intercept=False)\nlm2.fit(X, bos.PRICE)\n", "intent": "**Your turn:** How would you change the model to not fit an intercept term? Would you recommend not having an intercept?\n"}
{"snippet": "kmn.fit(df.drop(columns=['Private'],axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "sagemaker.Session().delete_endpoint(predictor.endpoint)\n", "intent": "Let's delete the endpoint we just created to prevent incurring any extra costs.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**KNN model instance with n_neighbors=1**\n"}
{"snippet": "log_reg.fit(X_train,y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nrfreg = RandomForestClassifier()\nrfreg\nimport numpy as np\n", "intent": "Build Random Forest Model\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Flatten, Dense\nfrom keras.layers import Embedding\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=200))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n", "intent": "We will be using the same model architecture as before  with an additional Dense layer using `relu` activation:\n"}
{"snippet": "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\nmodel.save_weights('pre_trained_spacy_model.h5')\n", "intent": "Let's compile our model and train it:\n"}
{"snippet": "from keras.layers import Dense\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n                    epochs=10,\n                    batch_size=128,\n                    validation_data=(x_val, y_val))\n", "intent": "Let's train a simple recurrent network using an `Embedding` layer and a `SimpleRNN` layer:\n"}
{"snippet": "from datetime import timedelta\nltg30_blobs = goesio.get_ltg_blobs(irdt + timedelta(minutes=30), timespan_minutes=15)\nltg30 = goesio.create_ltg_grid(ltg30_blobs, griddef, influence_km)\nmodel = LogisticRegression()\ndf = create_prediction_df(ref, ltg30, griddef)\nx = df.drop(['lat', 'lon', 'ltg'], axis=1)\nmodel = model.fit(x, df['ltg'])\nprint(model.coef_, model.intercept_)\nprint('Model accuracy={}%'.format(100*model.score(x, df['ltg'])))\n", "intent": "How about if we try to predict lightning 30 minutes into the future?\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], minval=-1, maxval=1), name='embedding')\n    embed = tf.nn.embedding_lookup(embedding, input_data) \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def fit_and_plot_dt(x, y, depth, title, ax):\n    dt = tree.DecisionTreeClassifier(max_depth = depth)\n    dt.fit(x, y)\n    ax = plot_tree_boundary(x, y, dt, title, ax)\n    return ax\n", "intent": "And a Decision Tree classification for our two new images are:\n"}
{"snippet": "estimator.fit(training_data)\n", "intent": "The `estimator.fit` call bellow starts training and creates a data channel named `training` with the contents of the\n S3 location `training_data`.\n"}
{"snippet": "kmeans = KMeans(n_clusters=2)\n", "intent": "**Erstelle eine Instanz von einem K Means Modell mit 2 Clustern.**\n"}
{"snippet": "kmeans.fit(df.drop('Private',axis=1))\n", "intent": "**Fitte das Modell auf alle Daten (ohne die \"Private\" Spalte).**\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS',axis=1))\n", "intent": "**Wende den Scaler auf die Eigenschaften an.**\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Erstelle ein KNN Modell mit n_neighbors=1.**\n"}
{"snippet": "knn.fit(X_train,y_train)\n", "intent": "**Fitte dieses Modell zum Trainingsset.**\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "**Erstelle eine Instanz von LinearRegression() namens lm.**\n"}
{"snippet": "lm.fit(X_train,y_train)\n", "intent": "**Trainiere lm mit den Trainingsdaten.**\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\ngrid.fit(X_train,y_train)\n", "intent": "**Erstelle ein GridSearchCV Objekt und fitte es auf die Trainingsdaten.**\n"}
{"snippet": "model.fit(X_train, y_train)\n", "intent": "Train the model using the training data\n"}
{"snippet": "from sagemaker.tensorflow.serving import Model\nenv = {'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'mobilenet_v2_140_224'}\nmodel = Model(model_data=model_data, role=sagemaker_role, framework_version=1.12, env=env)\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge')\n", "intent": "Now that the model archive is in S3, we can create a Model and deploy it to an \nEndpoint with a few lines of python code:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform(shape=(vocab_size,embed_dim),minval=-1,maxval=1))\n    embed = tf.nn.embedding_lookup(embedding,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "for k in range(1,30)[::-1]:\n    knn = KneighborsClassifier(n_neighbors=k)\n    knn.fit(wine_df,y)\n    print (k, knn.score(wine_df,y))\n", "intent": "Using the best features, explore what the best value of `k` is for this dataset. \n"}
{"snippet": "for k in range(1,30)[::-1]:\n    knn = KneighborsClassifier(n_neighbors=k)\n    knn.fit(x_train,y_train)\n    print (k, knn.score(x_test,y_test)    \n", "intent": "Are our choice of hyper-parameters and features the same as they were when we were validating based on a training set alone?\n"}
{"snippet": "depth_range = np.arange(1, 25)\ntrain_scores, test_scores = validation_curve(DecisionTreeClassifier(), \n                                             X, \n                                             y, \n                                             param_name=\"max_depth\", \n                                             param_range=depth_range,\n                                             cv=3, \n                                             scoring=\"accuracy\")\n", "intent": "Code adapted from [www.chrisalbon.com](www.chrisalbon.com)\n"}
{"snippet": "ada = AdaBoostClassifier(n_estimators=300) \n", "intent": "Compare and contrast Decision Trees and AdaBoost\n"}
{"snippet": "km = KMeans(n_clusters=4)\nkm.fit(data)\nlabels = km.labels_\ncents = km.cluster_centers_\npd.value_counts(labels)\n", "intent": "What cluster value should we set?\n"}
{"snippet": "dt = DecisionTreeClassifier(max_depth=4)\ndt.fit(X,y)\n", "intent": "Bonus!!\n<br><br>\nLet's see the most important features and visualize the decision tree\n"}
{"snippet": "model = Sequential()\nmodel.compile(optimizer=\"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\nmodel.fit(X, y_binary, epochs=40, validation_split = 0.25)\n", "intent": "We're trained a really good model, but principles of cross validation also to deep learning. Here's how we'll evaluate the model on a testing data.\n"}
{"snippet": "model = Sequential()\nmodel.compile(optimizer=\"adam\", loss = \"mean_squared_error\")\n", "intent": "**Back to the drawing board!**\nWe need more layers!!\n"}
{"snippet": "ntm.fit({'train': s3_train, 'validation': s3_val, 'auxiliary': s3_aux, 'test': s3_test})\n", "intent": "We are ready to run the training job. Again, we will notice in the log that the top words are printed together with the WETC and TU scores.\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3, input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten(input_shape=(32, 32, 3)))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "X_train_new = X_train.iloc[:,40:60]\nX_test_new = X_test.iloc[:,40:60]\nlr = LinearRegression()\nlr.fit(X_train_new, y_train)\nprint 'Train R-squared: {:.3}'.format(lr.score(X_train_new, y_train))\nprint 'Test R-squared: {:.3}'.format(lr.score(X_test_new, y_test))\n", "intent": "This is the most naive approach to dimensionality reduction; just pick $k$ original features at random.\n"}
{"snippet": "X, actual, y = generate_sample_data(20, true_function)\nlr = LinearRegression(fit_intercept=True)\nlr.fit(X, y)\ncompare_fitted_function(X, y, actual, lr)\n", "intent": "Generate some data, fit a linear regression model, and plot the results.\n"}
{"snippet": "degree = 2\npoly = PolynomialFeatures(degree = degree, include_bias=False)\nlr = LinearRegression(fit_intercept=True)\npipeline = Pipeline([(\"polynomial_features\", poly),\n                     (\"linear_regression\", lr)])\npipeline.fit(X, y)\ncompare_fitted_function(X, y, true_function(X), pipeline)\n", "intent": "We can use sklearn's Pipeline framework to connect the polynomial expansion and the linear regression into one 'model'\n"}
{"snippet": "dt.fit(train_df[predictors], train_df[target])\n", "intent": "Fit to the training data\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ndt = DecisionTreeRegressor(max_depth = 6)\nparameter_grid = {'max_depth': [2,3,4,5,6]}\nsearcher = GridSearchCV(dt, param_grid = parameter_grid, cv = 10)\nsearcher.fit(train_df[predictors], train_df[target])\ndt.fit(train_df[predictors], train_df[target])\n", "intent": "Sklearn doesn't allow _post-pruning_ of decision trees, but we can try some pre-pruning parameters\n"}
{"snippet": "categorical_pipe = Pipeline(\n)\n", "intent": "**Exercise**: Combine the categorical steps into a single pipeline\n"}
{"snippet": "degree = 6\npoly = PolynomialFeatures(degree = degree, include_bias=False)\nlr = LinearRegression(fit_intercept=True)\npipeline = Pipeline([(\"polynomial_features\", poly),\n                     (\"linear_regression\", lr)])\npipeline.fit(X, y)\ncompare_fitted_function(X, y, true_function(X), pipeline)\n", "intent": "We can use sklearn's Pipeline framework to connect the polynomial expansion and the linear regression into one 'model'\n"}
{"snippet": "def random_features(nfeatures):\n    cols = np.random.choice(X_train.columns, nfeatures, replace=False)\n    lr = LinearRegression()\n    lr.fit(X_train.loc[:, cols], y_train)\n    return lr.score(X_test.loc[:, cols], y_test)\nout = [random_features(i) for i in range(1, 120)]\n", "intent": "This is the most naive approach to dimensionality reduction; just pick $k$ features at random.\n"}
{"snippet": "f = io.BytesIO()\ntorch.onnx.export(pytorch_model, inputs, f, verbose=True)\nonnx_model = onnx.ModelProto.FromString(f.getvalue())\nprint(\"Check the ONNX model.\")\nonnx.checker.check_model(onnx_model)\n", "intent": "Run the PyTorch exporter to generate an ONNX model.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ndt = DecisionTreeRegressor()\nparameter_grid = {'max_depth': [2, 3, 4, 5, 6]}\nsearcher = GridSearchCV(dt, param_grid=parameter_grid, cv=10)\nsearcher.fit(train_df[predictors], train_df[target])\n", "intent": "Sklearn doesn't allow _post-pruning_ of decision trees, but we can try some pre-pruning parameters\n"}
{"snippet": "x = tf.placeholder(tf.float32)\nW = tf.Variable([3.], tf.float32)\nb = tf.Variable([-3.], tf.float32)\nlinear_model = W * x + b\n", "intent": "Think about the linear equation\n$$\ny = 3 x - 3\n$$\n"}
{"snippet": "scaler.fit(bank_data.drop('Class', axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=1)]\nlearn = learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[20, 20, 20], n_classes=2)\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "kmeans.fit(college.drop(['Unnamed: 0', 'Private'], axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "standardScaler.fit(df.drop('TARGET CLASS', axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "naive_bayes.fit(X_train, y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid= param_grid, verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nlgreg_cv_model = LogisticRegressionCV(n_jobs=-1, random_state=42, Cs = [1], cv = 10, \\\n                                      class_weight = 'balanced', refit=False, scoring = 'accuracy',\\\n                                     penalty = 'l2') \nlgreg_cv_model.fit(X_train, Y_train.ravel()) \nlgreg_cv_model.scores_\nprint(np.mean(lgreg_cv_model.scores_['Male']))\n", "intent": "<div class=\"span5 alert alert-info\">\n> \n"}
{"snippet": "pytorch_results = pytorch_model(*inputs)\n_, caffe2_results = c2_native_run_net(init_net, predict_net, caffe2_inputs)\n", "intent": "Run PyTorch and Caffe2 models separately, and get the results.\n"}
{"snippet": "idf = np.array([np.log(3), np.log(3), np.log(3./2), np.log(3), np.log(3./2), np.log(3./2)])\nXtfidf = np.dot(X.todense(), np.diag(idf))\n", "intent": "**Q**: Compute the td-ifd matrix for the training set \n"}
{"snippet": "regrAll = LinearRegression()\nregrAll.fit(X, y)\nprint \"sales = \", regrAll.intercept_, \" + \", regrAll.coef_[0], \" x TV + \", \\\n       regrAll.coef_[1], \" x Radio + \", \\\n       regrAll.coef_[2], \" x Newspaper\"\n", "intent": "<br>\nNow we'll fit a multiple linear regression model for the three features simultaneously. \n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nnames = [\"Nearest Neighbors\"]\ncl = [KNeighborsClassifier(3)] \n", "intent": "**Activity! Get up!**\n"}
{"snippet": "from sklearn import linear_model\nobj = linear_model.LinearRegression()\nobj.fit(np.array(data.height.values.reshape(-1,1)), data.stories )\nprint( obj.coef_, obj.intercept_ )\n", "intent": "We fit a linear regression model below. We try to use height to predict the number of stories in a building.\n"}
{"snippet": "obj2 = linear_model.LinearRegression()\nX = np.array( (data.height.values, data.year.values))\nobj2.fit(X.transpose() , data.stories)\nprint(obj2.coef_, obj2.intercept_)\n", "intent": "Now we will do multiple linear regression. This means we will use more than one predictor when we fit a model and predict our response variable \n"}
{"snippet": "print x4.todense()\n", "intent": "Let's print it and see what it looks like \n"}
{"snippet": "def convolutional_layer(input_x,shape):\n    W = init_weights(shape)\n    b = init_bias([shape[3]])\n    return tf.nn.relu(conv2d(input_x,W)+b)\n", "intent": "Convolutional layer\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(20, input_dim = 2, activation = 'sigmoid'))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam', metrics= ['accuracy'])\n", "intent": "Can you draw a graph and see how it looks?\n"}
{"snippet": "kernels = tf.get_default_graph().get_operation_by_name('convolution/kernel').values()[0]\nshape = kernels.shape.as_list()  \nw = tf.reshape(kernels, [shape[3], shape[0], shape[1], shape[2]])\ntf.summary.image('kernel_weights', w, max_outputs=shape[3], collections=None)\n", "intent": "This will add the kernels to the TensorBoard output, so that we can visualize them.\n"}
{"snippet": "pytorch_time = benchmark_pytorch_model(pytorch_model, inputs)\ncaffe2_time = benchmark_caffe2_model(init_net, predict_net)\nprint(\"PyTorch model's execution time is {} milliseconds/ iteration, {} iterations per second.\".format(\n    pytorch_time, 1000 / pytorch_time))\nprint(\"Caffe2 model's execution time is {} milliseconds / iteration, {} iterations per second\".format(\n    caffe2_time, 1000 / caffe2_time))\n", "intent": "The following code measures the performance of PyTorch and Caffe2 models.\nWe report:\n- Execution time per iteration\n- Iterations per second\n"}
{"snippet": "merged = tf.summary.merge_all()\nreset_vars()\ntrain_writer = tf.summary.FileWriter(logs_path + '/train', graph=tf.get_default_graph())\ntest_writer = tf.summary.FileWriter(logs_path + '/test', graph=tf.get_default_graph())\n", "intent": "We will then initialize our variables, launch our graph, and train our model. \n"}
{"snippet": "W1 = tf.Variable(initializer([N_PIXELS, hidden_size]), name=\"weights\")\nb1 = tf.Variable(tf.zeros([hidden_size]), name=\"biases\")\nhidden = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\nW2 = tf.Variable(initializer([hidden_size, N_CLASSES]), name=\"weights2\")\nb2 = tf.Variable(tf.zeros([N_CLASSES]), name=\"biases2\")\ny = tf.matmul(hidden, W2) + b2\n", "intent": "This training placeholder will be used later to indicate whether we are in the process of training or prediction.  Right now, it is extraneous.\n"}
{"snippet": "hidden = tf.layers.dense(x, hidden_size, activation=tf.nn.sigmoid, use_bias=True,\n    kernel_initializer=tf.truncated_normal_initializer(stddev=N_PIXELS**-0.5))\n", "intent": "We have been using **dense** layers.  That is, each neuron is connected to all of the inputs to the layer.  First we create thie hidden layer:\n"}
{"snippet": "draw_graph(\"exercise\")\n", "intent": "Implement the following graph in TensorFlow.\n"}
{"snippet": "with tf.Session(graph=g2) as s2:\n    print s2.run(sum_g2)\n", "intent": "Alternatively, you can use sessions as context managers.  They close themselves on exit, so you don't have to do this explicitly.\n"}
{"snippet": "sess2 = tf.Session()\nsess2.run(tf.global_variables_initializer())\nprint \"New session:\", sess2.run(var)\nprint \"Old session:\", sess.run(var)\n", "intent": "The value of each variable exists only within a given session.  If you start with another session, variables will have a separate value in that one.\n"}
{"snippet": "p_insc = tf.Variable(4 * np.sqrt(2), name=\"Inscribed_Perimeter\", dtype=np.float64)\np_circ = tf.Variable(8.0, name=\"Circumscribed_Perimeter\", dtype=np.float64)\n", "intent": "Now we can re-run our estimation for $2\\pi$ using TensorFlow variables.  We'll start them off with the values for $n = 4$.\n"}
{"snippet": "reset_tf()\nF = tf.Variable([0,1])\nupdate = F.assign([F[1], F[0] + F[1]])\n", "intent": "Note that using int64 is important.  The default int dtype (on my system, at least) is int32, which overflows.\n"}
{"snippet": "reset_tf()\nF_low = tf.Variable(1)\nF_high = tf.Variable(1)\nF_next = F_low + F_high\nupdate1 = F_low.assign(F_high)\nupdate2 = F_high.assign(F_next)\n", "intent": "This doesn't work, because the definition of F_next depends on F_high, which is already updated.\n"}
{"snippet": "from torch.autograd import Variable\nbatch_size = 1    \nx = Variable(torch.randn(batch_size, 3, 224, 224), requires_grad=True)\ntorch_out = torch.onnx._export(torch_model,             \n                               x,                       \n                               \"squeezenet.onnx\",       \n                               export_params=True)      \n", "intent": "and export the pytorch model as onnx model:\n"}
{"snippet": "reset_tf()\nW = tf.Variable(tf.zeros((13, 1)), name=\"weight\")\nb = tf.Variable(tf.zeros(1), name=\"bias\")\nx = tf.placeholder(shape=[None, 13], dtype=tf.float32, name='x')\ny_label = tf.placeholder(shape=[None, 1], dtype=tf.float32, name='y_label')\ny = tf.matmul(x, W) + b\nloss = tf.reduce_mean(\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=y_label))\n", "intent": "The setup is nearly identical to the case of linear regression.\n"}
{"snippet": "y_pred, out_shape, lstm_new_state = make_lstm(x_enc, lstm_init_value, n_chars, lstm_size, n_layers)\n", "intent": "These values get fed into our net.\n"}
{"snippet": "final_out = tf.reshape(tf.nn.softmax(y_pred), \n                       (out_shape[0], out_shape[1], n_chars))\n", "intent": "The predictions come out flattened, so if we want an array in the same shape as the input, we need to reshape it.\n"}
{"snippet": "merged = tf.summary.merge_all()\ntrain_writer = tf.summary.FileWriter(logs_path + '/train', graph=tf.get_default_graph())\n", "intent": "As we have done in the previous model we built, we then merge our summaries and initialize and lunch the graph.\n"}
{"snippet": "y = df.imdb\nones = (df.director*0+1)\nones = ones.rename('bias')\nX = pd.concat([ones, df.likes, df.year], axis=1)\nmodel = sm.OLS(y, X) \nfit = model.fit()\nfit.summary()\n", "intent": "I'll use the movie release year as the categorical variable, because it is the only categorical variable I have in the dataset.\n"}
{"snippet": "lr=LogisticRegression()\nfit = lr.fit(X,y)\nfit.coef_\n", "intent": "The KNN and Logistic Regression accuracy scores are not any better than the \"predict PG-13 for every movie\" accuracy score.\n"}
{"snippet": "lr = LogisticRegression()\nfit = lr.fit(X,y)\nfit.coef_\n", "intent": "What are the coefficients of logistic regression? Which features affect the outcome how?\n"}
{"snippet": "fit = LogisticRegression().fit(X,y)\nfit.coef_\n", "intent": "What are the coefficients of logistic regression? Which features affect the outcome how?\n"}
{"snippet": "m, train_err, test_err = learning_curve(LogisticRegression(),X,y,cv=5)\ntrain_cv_err = np.mean(train_err,axis=1)\ntest_cv_err = np.mean(test_err,axis=1) \n", "intent": "Draw the learning curve for logistic regression in this case.\n"}
{"snippet": "from torch.autograd import Variable\nimport torch.onnx\nimport torchvision\ndummy_input = Variable(torch.randn(1, 3, 224, 224))\nmodel = torchvision.models.alexnet(pretrained=True)\ntorch.onnx.export(model, dummy_input, \"alexnet.onnx\")\n", "intent": "If you already have your model built, it's just a few lines:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1,1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "num_examples = len(X) \nnn_input_dim = 2 \nnn_output_dim = 2 \nepsilon = 0.01 \nreg_lambda = 0.01 \ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef dsigmoid(y):\n    return y * (1.0 - y)    \n", "intent": "Now we are ready for our implementation. We start by defining some useful variables and parameters for gradient descent:\n"}
{"snippet": "lm1 = smf.ols(formula='Sales ~ TV', data=data).fit()\nlm1.params\n", "intent": "Let's estimate the model coefficients for the advertising data:\n"}
{"snippet": "feature_cols = ['TV', 'Radio', 'Newspaper', 'Size_large']\nX = data[feature_cols]\ny = data.Sales\nlm2 = LinearRegression()\nlm2.fit(X, y)\nzip(feature_cols, lm2.coef_)\n", "intent": "Let's redo the multiple linear regression and include the **Size_large** feature:\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nfeature_cols = ['al']\nX = glass[feature_cols]\ny = glass.ri\nlinreg.fit(X, y)\n", "intent": "Let's create a linear regression model.  What are our 6 steps for creating a model?\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=50)\n", "intent": "Instead of treating this as a regression problem, treat it as a classification problem and see what testing accuracy you can achieve with KNN.\n"}
{"snippet": "km = KMeans(n_clusters=4, random_state=1)\nkm.fit(X_scaled)\nbeer['cluster'] = km.labels_\nbeer.sort_values('cluster')\n", "intent": "7) Which k gives the best Silhouette score? use this value instead of 'x' below.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation = 'relu', input_shape = (1000,)))\nmodel.add(Dropout(.4))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=5000, verbose=1, validation_data = (x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "import onnx\nmodel = onnx.load(\"alexnet.onnx\")\nonnx.checker.check_model(model)\nprint(onnx.helper.printable_graph(model.graph))\n", "intent": "**That's it!**\nYou can also use ONNX tooling to check the validity of the resulting model or inspect the details\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1000, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.4))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss ='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=1000, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "baseline=DummyClassifier()\nbaseline.fit(X_val,y_val)\nprint(\"the classification accuracy score for the baseline classifier is:\", baseline.score(X_val,y_val))\n", "intent": "In my opinion, I will use DummyClassifier as my baseline classifier, because this classifier makes predictions using simple rules.\n"}
{"snippet": "do_dbscan(data, 10, 0.25)\ndo_dbscan(data, 10, 0.35)\ndo_dbscan(data, 10, 0.45)\n", "intent": "Now compare three different max. dist values\n"}
{"snippet": "test_accuracy = tfe.metrics.Accuracy()\nfor (x, y) in test_dataset:\n  prediction = tf.argmax(model(x), axis=1, output_type=tf.int32)\n  test_accuracy(prediction, y)\nprint(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))\n", "intent": "Unlike the training stage, the model only evaluates a single [epoch](https://developers.google.com/machine-learning/glossary/\n"}
{"snippet": "import numpy\nimport theano\nimport theano.tensor as T\nclass MLP(object):\n     def __init__(self, rng, input, n_in, n_hidden, n_out):\n", "intent": "Our Multi-Layer-Perceptron now plugs everything together, i.e. one hidden layer and the softmax layer.\n"}
{"snippet": "def build_mlp(n_in, n_hidden, n_out, input_var=None):\n    l_in = lasagne.layers.InputLayer(shape=(None, n_in), input_var=input_var)\n    l_hid1 = lasagne.layers.DenseLayer(incoming=l_in,\n                num_units=n_hidden, nonlinearity=lasagne.nonlinearities.tanh,\n                W=lasagne.init.GlorotUniform())\n    l_out = lasagne.layers.DenseLayer(incoming=l_hid1, \n            num_units=n_out, nonlinearity=lasagne.nonlinearities.softmax)\n    return l_out\n", "intent": "Now we use the provided layers from Lasagne to build our MLP\n"}
{"snippet": "X_2 = news_A.drop('class',axis=1)\ny_2 = news_A['class']\nclf2 = GaussianNB()\nclf2.fit(X_2,y_2)\nprint('The classification accuracy on the original training dataset is:',format(clf2.score(X_2,y_2)))\n", "intent": "Fit a Gaussian Naive Bayes model to the original dataset A (including the outliers). Display the classification accuracy on the training dataset.\n"}
{"snippet": "kmeans = KMeans(n_clusters = 5, random_state = 1234)\nkmeans.fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "p_y_mean_test = sigmoid(beta_coef(x_test, beta_mean))\np_y_post_test = sigmoid(beta_coef(x_test, trace['betas'].T))\npost_pred_test = np.random.binomial(1, p_y_post_test)\np_y_pp = post_pred_test.mean(axis=1)\np_y_post_thresh = p_y_post_test[:, :]\np_y_post_thresh[p_y_post_thresh > 0.5] = 1\np_y_post_thresh[p_y_post_thresh <= 0.5] = 0\np_y_cdf = p_y_post_thresh.mean(axis=1)\n", "intent": "We perform the same analysis on the testing set.\n"}
{"snippet": "def fully_connected(x, dim_in, dim_out, name):\n    with tf.variable_scope(name) as scope:\n        w = tf.get_variable('w', shape=[dim_in, dim_out], \n                            initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n        b = tf.get_variable('b', shape=[dim_out])\n        out = tf.matmul(x, w) + b\n        return out    \n", "intent": "To implement a fully-connected layer, we can simply use [tf.matmul](https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops.html\n"}
{"snippet": "kmeans_cluster = KMeans(2).fit(zipcode_shape.iloc[:, range(5,26)])\ncenters = kmeans_cluster.cluster_centers_\nzipcode_shape['km_labels'] = kmeans_cluster.labels_\n", "intent": "- It can also be seen how the data starts to segregate in the best way possible for 2 clusters.\n"}
{"snippet": "kmeans_cluster_3 = KMeans(3).fit(zipcode_shape.iloc[:, range(5,26)])\ncenters_3 = kmeans_cluster_3.cluster_centers_\nzipcode_shape['km_labels'] = kmeans_cluster_3.labels_\n", "intent": "- It can also be seen how the data starts to segregate in the best way possible for 3 clusters.\n"}
{"snippet": "kmeans_cluster_4 = KMeans(4).fit(zipcode_shape.iloc[:, range(5,26)])\ncenters_4 = kmeans_cluster_4.cluster_centers_\nzipcode_shape['km_labels'] = kmeans_cluster_4.labels_\n", "intent": "- It can also be seen how the data starts to segregate in the best way possible for 4 clusters.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded_input = tf.contrib.layers.embed_sequence(input_data,\n                                                      vocab_size=vocab_size,\n                                                      embed_dim=embed_dim)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "import statsmodels.api as sm\nfrom statsmodels.stats.gof import chisquare\nmodels = []\nselectedIndeces = [17,16,15]\nfor i in selectedIndeces:\n    models.append(sm.OLS(mtabyride[i], \n                         sm.add_constant(np.arange(mta.shape[2]))).fit())\nfor i,m in enumerate(models):\n    print (selectedIndeces[i], \"R^2 line fit %.2f\"%m.rsquared)\n", "intent": "Figure 4: the steepest smooth increase in ridership type is for Senior passes. Above is the average over all station of the senior pass popularity\n"}
{"snippet": "from sklearn.cluster import KMeans\nnc = 9\ndata = np.load(\"MTA_Fare.npy\")\ntots = data.transpose(2,0,1).reshape(data.shape[2], \n                                     data.shape[1]*data.shape[0]).T\ntots = tots[tots.std(1)>0]\nkm = KMeans(n_clusters=nc)\nvals = ((tots.T - tots.mean(1))/tots.std(1)).T\nkm.fit(vals)\n", "intent": "Figure 8: The 4 station that shows the most clear periodicity at 1-year\n"}
{"snippet": "test_models = [LinearRegression(), Ridge(alpha=10), Lasso(alpha=10)]\nscores = [analyze_performance(my_model) for my_model in test_models] \n", "intent": "Let's try a few degrees with a regularized model.\n"}
{"snippet": "from eml.net import describe as ndescribe\nfrom eml.net.reader import keras_reader\nnet = {}\ndef convert_keras_net(knet, net):\n    for target in sim_out:\n        net[target] = keras_reader.read_keras_sequential(knet[target])\n    net['all'] = keras_reader.read_keras_sequential(knet['all'])\nconvert_keras_net(knet, net)\n", "intent": "Now, the EMLlib allows us to convert the keras networks into an internal format with one line of code:\n"}
{"snippet": "from sklearn import mixture\ngmm = mixture.GaussianMixture(n_components=2,covariance_type='full').fit(train_x)\n", "intent": "<font color = \"blue\">\n"}
{"snippet": "target = train_df['is_iceberg'].values\nIcebergModel.fit(x=X_band,y=target,epochs=20,batch_size=128)\n", "intent": "Now we fit the model with the training data having epochs = 20 and random batch_size = 128 . \n"}
{"snippet": "IcebergModel = Iceberg_model((75,75,3))\n", "intent": "For  creating the model we provide the shape of each training data (75,75,2) .\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "X = df.drop(['Unnamed: 0','Private'], axis=1)\ny = df['Private']\nkmeans.fit(X)\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid, refit=True, verbose=2)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=50)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.cluster import DBSCAN\ndbscan = DBSCAN(eps = 3, min_samples = 3)\ndbscan.fit(Xs)\n", "intent": "Remember to pass an `eps` and `min_samples` of your choice.\n"}
{"snippet": "model = dt_classifier.fit(X_train, y_train)\n", "intent": "Then we use the `fit` method on the train data to fit our model.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=128,\n         validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = rf_classifier.fit(X_train, y_train)\n", "intent": "Now we fit the model on our training data.\n"}
{"snippet": "param_grid = {'min_samples_split': range(2,10),\n              'min_samples_leaf': range(1,10)}\nmodel_r = GridSearchCV(ensemble.RandomForestClassifier(), param_grid)\nmodel_r.fit(X_train, y_train)\nbest_index = np.argmax(model_r.cv_results_[\"mean_test_score\"])\nprint(\"Best index:\", model_r.cv_results_[\"params\"][best_index])\nprint(\"Mean test score:\", max(model_r.cv_results_[\"mean_test_score\"]))\nprint(\"Held-out:\", model_r.score(X_test, y_test))\n", "intent": "Let's do another grid search to determine the best hyperparameters:\n"}
{"snippet": "from sklearn import linear_model\nlin_reg = linear_model.LinearRegression(n_jobs=1)  \nmodel = lin_reg.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\n", "intent": "We'll start with a basic OLS linear regression model:\n"}
{"snippet": "from tpot import TPOTRegressor\ntpot = TPOTRegressor(generations=5, population_size=20, verbosity=2)  \ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_boston_pipeline.py')\n", "intent": "Now TPOT will make the model.\n"}
{"snippet": "kmeans = KMeans(n_clusters=3,\n               max_iter=300 \n               ).fit(X)\n", "intent": "Now we can create the model:\n"}
{"snippet": "from sklearn.cluster import AgglomerativeClustering\nward = AgglomerativeClustering(n_clusters=2,\n                               linkage='ward', \n                               affinity='euclidean') \n", "intent": "We'll use two clusters this time, and use ward linkage.\n"}
{"snippet": "ward.fit(noisy_moons)\n", "intent": "Now we'll fit the clustering model on the dataset.\n"}
{"snippet": "from IPython.display import display\nfor i in range(25):\n    run_iteration('IR_input_data.csv')\n    if (i + 1) in [1, 2, 3, 4, 5, 15, 25]:\n        print 'Iteration', i + 1\n        display(check_ir_model())\n", "intent": "The remaining iterations work with the model that was generated.\n"}
{"snippet": "with tf.Session() as sess:\n    saver = tf.train.Saver()\n    saver.restore(sess, './model/model.checkpoint')\n    dev_predicted = sess.run(predict, feed_dict=test_feed_dict)\n    dev_accuracy = nn.calculate_accuracy(dev_orders, dev_predicted)\ndev_accuracy\n", "intent": "The following code loads your model, computes accuracy, and exports the result. **DO NOT** change this code.\n"}
{"snippet": "from sklearn.svm import SVC\nir = SVC()\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\nmnb.fit(train_dtm, y_train)\n", "intent": "Use `sklearn` to build a `MultinomialNB` classifier against your training data.\n"}
{"snippet": "km = KMeans(n_clusters=4, init='k-means++', n_init=10, max_iter=300, random_state=1)\n", "intent": "Initialize a new kmeans classifier\n"}
{"snippet": "clf = DecisionTreeClassifier(max_depth=2,random_state=0)\nvisualize_tree(clf,X,y)\n", "intent": "**Let's plot out a Decision Tree boundary with a max depth of two branches**\n"}
{"snippet": "clf = DecisionTreeClassifier(max_depth=4,random_state=0)\nvisualize_tree(clf,X,y)\n", "intent": "**How about 4 levels deep?**\n"}
{"snippet": "model.add(Dense(16, input_shape=(4,)))\nmodel.add(Activation('sigmoid'))\n", "intent": "The next two lines define the size input layer (input_shape=(4,), and the size and activation function of the hidden layer\n"}
{"snippet": "model.add(Dense(3))\nmodel.add(Activation('softmax'))\n", "intent": "... and the next line defines the size and activation function of the ouput layer.\n"}
{"snippet": "print \"Hit Rates:\"\nmodels = [(\"LR\", LogisticRegression()), (\"LDA\", LDA()), (\"QDA\", QDA())]\nfor m in models:\n    fit_model(m[0], m[1], X_train, y_train, X_test, pred)\nprint pred.head()\n", "intent": "Create and fit three basic models:\n"}
{"snippet": "def build_linear_regression_model(X, y):\n    linear_mdl = linear_model.LinearRegression()  \n    X = np.reshape(X, (X.shape[0], 1))\n    y = np.reshape(y, (y.shape[0], 1))\n    linear_mdl.fit(X, y)  \n    return linear_mdl\n", "intent": "   **Design the Benchmark Regression Model **\n"}
{"snippet": "regression_model = build_linear_regression_model(X_train,y_train)\n", "intent": "   **Compile the model and fit the data **\n"}
{"snippet": "param_grid = {'C':[0.1,1,10,100],'gamma':[1,0.1,0.01,0.001]}\nir1 = GridSearchCV(SVC(),param_grid,verbose=3)\n", "intent": "**Create a dictionary called param_grid and fill out some parameters for C and gamma.**\n"}
{"snippet": "lstm_basic_model = build_basic_model(shape,neurons)\n", "intent": "**Build and compile basic LSTM model**\n"}
{"snippet": "epochs = 1\nlstm_basic_model.fit(X_train, y_train,epochs=epochs,validation_split=0.2)\n", "intent": "**Train basic LSTM model**\n"}
{"snippet": "def build_optimized_lstm_model(shape, neurons, dropout):\n    model = Sequential()\n    model.add(LSTM(neurons[0], input_shape=(shape[0], shape[1]), return_sequences=True))\n    model.add(Dropout(dropout))\n    model.add(LSTM(neurons[1], input_shape=(shape[0], shape[1]), return_sequences=False))\n    model.add(Dropout(dropout))\n    model.add(Dense(neurons[2],activation='linear'))\n    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n    model.summary()\n    return model\n", "intent": " **Step : 1 - Build the enhanced Model**\n"}
{"snippet": "optimized_lstm_model.fit(\n                        X_train,\n                        y_train,\n                        batch_size=batch_size,\n                        epochs=epochs,\n                        validation_split=validation_split,\n                        verbose=1)\n", "intent": "**Train using Optimized LSTM model**\n"}
{"snippet": "def build_basic_gru_model(layers):\n        model = Sequential()\n        model.add(GRU(2048, input_shape=(layers[0]+1, layers[1]-1), return_sequences=True))\n        model.add(GRU(1024, return_sequences=True))\n        model.add(GRU(512, return_sequences=False))       \n        model.add(Dense(1, activation='linear'))\n        model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n        return model\n", "intent": "**Define the basic GRU model**\n"}
{"snippet": "gru_basic_model = build_basic_gru_model([seq_len, no_of_features])\n", "intent": "**Compile the basic GRU model**\n"}
{"snippet": "gru_basic_model.fit(X_train,\n                    y_train,\n                    epochs=epochs, \n                    validation_split=0.2)\n", "intent": "**Train basic GRU model**\n"}
{"snippet": "gru_df = df\nno_of_features = len(gru_df.columns)\nseq_len= 21\nsplit = 0.8\nlayers = [seq_len, 10]\nepochs = 1\nbatch_size = 100\ngru_opt_model = build_optimized_gru_model([seq_len, no_of_features],0.2)\n", "intent": "**Compile the enhanced GRU model**\n"}
{"snippet": "gru_opt_model.fit(\n                X_train,\n                y_train,\n                batch_size=batch_size,\n                epochs=epochs, \n                validation_split=0.2)\n", "intent": "**Train the enhanced GRU model**\n"}
{"snippet": "ir1.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "gru_df = df\nno_of_features = len(gru_df.columns)\nseq_len= 21\nsplit = 0.8\nlayers = [seq_len, 10]\nepochs = 5\nbatch_size = 100\ngru_opt_model = build_optimized_gru_model([seq_len, no_of_features],0.2)\n", "intent": "**Compile the enhanced GRU model**\n"}
{"snippet": "from sklearn import neighbors\nmodel_knn = neighbors.KNeighborsClassifier(10,weights='uniform')\npredictor_var = ['ApplicantIncome']\nclassification_model(model_knn, df,predictor_var,outcome_var)\n", "intent": "* K Nearest neighbor\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddingMatrix = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    return tf.nn.embedding_lookup(embeddingMatrix, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "drop_out_layer = tf.nn.dropout(flat_x,keep_prob=hold_prob )\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "lda = LdaModel(corpus, id2word=id2word, num_topics=5, passes=10)\n", "intent": "Using the standard model\n"}
{"snippet": "lda_jit = LdaJitModel(corpus, id2word=id2word, num_topics=5, passes=10)\n", "intent": "Faster training time with ``LdaJitModel``, an optimized version of ``LdaModel`` by speeding up critical components of training procedure using Numba.\n"}
{"snippet": "lda_jit = LdaJitModel(corpus, id2word=id2word, num_topics=5, passes=50)\n", "intent": "Train for real (more passes)\n"}
{"snippet": "f ='home_win ~ home_strength + away_strength + home_rest + away_rest'\nres = (sm\n         .Logit\n         .from_formula(f, df)\n         .fit()\n)\n", "intent": "using formulas from ``patsy`` to describe our regression structure\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2) \nprint(num_ftrs)\nprint(model_ft.parameters())\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "intent": "Loading a pretrained model and reseting final fully connected layer.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(\n        tf.random_uniform(shape=[vocab_size, embed_dim], minval=-1, maxval=1,\n                          name='embedding'))\n    embed = tf.nn.embedding_lookup(\n        embedding, input_data, name='embed')\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "lle_data, _ = manifold.locally_linear_embedding(X, n_neighbors=12,\n                                             n_components=2)\nplot_clustering(lle_data[indices], X[indices], y_num, title='LLE')\n", "intent": "**Excerise 4:** Apply LLE on the data\n"}
{"snippet": "from sklearn import manifold\niso = manifold.Isomap(n_neighbors=6, n_components=2)\niso.fit(X)\nmanifold_data = iso.transform(X)\nplot_clustering(manifold_data, X, y, title='ISOMAP')\n", "intent": "**Excerise 2:** Apply ISOMAP on the data\n"}
{"snippet": "lle_data, _ = manifold.locally_linear_embedding(X, n_neighbors=12,\n                                             n_components=2)\nplot_clustering(lle_data, X, y, title='LLE')\n", "intent": "**Excerise 3:** Apply LLE on the data\n"}
{"snippet": "seq_len = 50 + 1\nsequences = torch.FloatTensor([data[t:t+seq_len] for t in range(len(data)-seq_len)])\nprint('Dimensions: {} x {}'.format(*sequences.shape))\n", "intent": "We first divide the raw dataset into 4950 series with seq_len of 51 (with overlaps). \n"}
{"snippet": "lr.fit(X_train, y_train)\n", "intent": "Now, we can fit the model on the training data:\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(max_depth=2)\ntree.fit(X_train, y_train)\n", "intent": "Let's start by building a very small tree (``max_depth=2``) and visualizing it.\nThe model fitting shouldn't be anything new:\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid=param_grid,\n                    cv=10., return_train_score=True)\ngrid.fit(X_train, y_train)\n", "intent": "Tune the ``max_leaf_nodes`` parameter using ``GridSearchCV``:\n"}
{"snippet": "model.fit(X_train, y_train, epochs=10, verbose=1, validation_split=.1)\n", "intent": "In keras, the ``fit`` method takes options like the number of epochs (iterations) to run, and whether to use a validation set for early stopping.\n"}
{"snippet": "history = model.fit(x_train, y_train, epochs=50, batch_size=100, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "print \"Dimensions of teams DataFrame:\", teams.shape\nprint \"Dimensions of players DataFrame:\", players.shape\nprint \"Dimensions of salaries DataFrame:\", salaries.shape\nprint \"Dimensions of fielding DataFrame:\", fielding.shape\nprint \"Dimensions of master DataFrame:\", master.shape\n", "intent": "We can print the dimensions (i.e. number of rows and columns) for each of the DataFrames.  \n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nlogreg = LogisticRegression()\nCs = np.logspace(-5,1.0,50)\nlogreg_parameters = {\n    'penalty':['l1','l2'],\n    'C': Cs,\n    'solver':['liblinear']\n}\ngrid_log = GridSearchCV(logreg, logreg_parameters, cv =5)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nlogreg = LogisticRegression()\nCs = np.logspace(-5,1.0,50)\nlogreg_parameters = {\n    'penalty':['l1','l2'],\n    'C': Cs,\n    'solver':['liblinear']\n}\ngrid_log_precision = GridSearchCV(logreg, logreg_parameters, scoring='average_precision', cv =5)\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "et.fit(X,y)\n", "intent": "Now it's your turn: repeat the investigation for the extra trees model.\n"}
{"snippet": "k=3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(X,y)\n", "intent": "Run the clustering analysis using scikit-learn.\n*Hint*: Estimate the number of clusters, k, based on your visual examination of the distributions.\n"}
{"snippet": "k = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(Xs)\n", "intent": "Cluster the Data to our our target groups. **BONUS**: Perform multiple clustering tests on various indicator variables\n"}
{"snippet": "kmeans = KMeans(n_clusters=2)\nclusters = kmeans.fit(Xs)\n", "intent": "Set up the k-means clustering analysis. Use the graph from above to derive \"k\"\n"}
{"snippet": "knn = KNeighborsClassifier()\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "model = LinearRegression()\n", "intent": "With the data being split, we'll now create the LinearRegression module. Write the line in the cell below:\n`model = LinearRegression()`\n"}
{"snippet": "lm.fit(X, bos.PRICE)\n", "intent": "The `lm.fit()` function estimates the cofficients the linear regression using least squares. \n"}
{"snippet": "def churn_graph(data, feature):\n    crosstab=pd.crosstab(data[feature],data_all['is_churn'])\n    crosstab.plot(kind='bar', stacked=True)\n    crosstab[\"Ratio\"] =  crosstab[1] / (crosstab[0]+ crosstab[1])\n    print (crosstab)\n", "intent": "Data exploration on training data\n"}
{"snippet": "churn_graph(data_all, feature='last_trans_to_expire_day')\n", "intent": "Looks like give 100% discount is not a good idea.\n"}
{"snippet": "churn_graph(data_all, feature='membership_length')\n", "intent": "Only 3% of churn customers have negative value, so I am going to exclude negative last_trans_to_expire_day\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators =100,  max_features='log2', max_depth=5)\n", "intent": "Train the model with the best params\n"}
{"snippet": "lda = LDA(n_components=2)\nlabels = digits.target\nlda.fit(flat, labels)\nreduced_data_lda = lda.transform(flat)\n", "intent": "**Now do the same as for PCA but now project onto 2 LDA components to get the following:**\n**Marks: 0**\n"}
{"snippet": "model = KMeans(n_clusters=n_digits, init='k-means++')\nmeans = model.fit(lda_digit_train)\nconfusion_matrix_stats(means, lda_digit_test, labels_test, ['N/A']*10)\n", "intent": "Then we fit a k-means model to the training data and have a look at the models accuracy.\n"}
{"snippet": "km = KM(codes=10,itr=2)\nkm.fit(X_train_flat)\nkm_cluster_centers = km.get_means\n", "intent": "**Unsupervised k-means:**\n"}
{"snippet": "from keras.callbacks import ModelCheckpoint   \ncheckpointer = ModelCheckpoint(filepath='imdb.model.best.hdf5', \n                               verbose=1, save_best_only=True)\nhist = model.fit(x_train, y_train, batch_size=100, epochs=50,\n          validation_split=0.2, callbacks=[checkpointer],\n          verbose=1, shuffle=True)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "scaler.fit(project_data.drop('TARGET CLASS', axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "lm = LinearRegression()\nlm.fit(X[['PTRATIO']], bos.PRICE)\n", "intent": "Try fitting a linear regression model using only the 'PTRATIO' (pupil-teacher ratio by town)\nCalculate the mean squared error. \n"}
{"snippet": "RF_clf = RandomForestClassifier(random_state=seed)\nRF_clf\n", "intent": "Train and evaluate the Random Forest Classifier with Cross Validation\n"}
{"snippet": "RF_clf.fit(X_train, y_train)\n", "intent": "Fitting the Random Forest Classifier on training set\n"}
{"snippet": "import tensorflow as tf\ndef get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = build_model(40, verbose = True)\n", "intent": "By trial and error it was found that a hidden layer with 3 nodes produced a reasonable model.\n"}
{"snippet": "model = build_model(3, verbose = True)\n", "intent": "By trial and error it was found that a hidden layer with 3 nodes produced a reasonable model.\n"}
{"snippet": "Cs = np.logspace(-6, -1, 10)\nclf = model_selection.GridSearchCV(estimator=svm.LinearSVC(), param_grid=dict(C=Cs), n_jobs=-1)\nclf.fit(X_train, Y_train)\nprint(clf.best_score_)\nprint(clf.best_estimator_.C)\nprint(\"Score: %s\" % clf.score(X_test, Y_test))\n", "intent": "retrain the model by tuning the parameters\n"}
{"snippet": "rfc = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_features='sqrt', oob_score=True)\nparam_grid = { \n    'n_estimators': [200, 700],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\nclf = model_selection.GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nclf.fit(X_train, Y_train)\nprint(clf.best_params_)\nclf.fit(X_train,Y_train)\nprint(\"Score: %s\" % clf.score(X_test, Y_test))\n", "intent": "train a model using random forest classifier\n"}
{"snippet": "from sklearn.cluster import KMeans\nnum_clusters =4\nkmeans_pca = KMeans(num_clusters).fit(stats_PCA)\n", "intent": "Finally, let's apply clustering once more, this time using the sklearn library.\n"}
{"snippet": "from sklearn.cluster import KMeans\nnum_clusters =3\nkmeans_pca = KMeans(num_clusters).fit(stats_PCA)\n", "intent": "Finally, let's apply clustering once more, this time using the sklearn library.\n"}
{"snippet": "modelW0 = sm.OLS(y, X)\nresultsW0 = modelW0.fit()\nprint resultsW0.summary()\n", "intent": "Now let's fit a linear regression model with an intercept. \n"}
{"snippet": "class Sigmoid(Activation):\n    def __init__(self):\n        super().__init__()\n    def function(self,z):\n        denominator = (1 + np.exp(-z))\n        self.x = 1/denominator\n        return self.x\n    def derivative(self,z = None):\n        self.dx = self.x * (1 - self.x)\n        return np.multiply(z,self.dx)\n", "intent": "Let's define at least three activation functions. Let's start with sigmoid, which we already know by heart.\n"}
{"snippet": "class TanH(Activation):\n    def __init__(self):\n        super().__init__()\n    def function(self,z):\n        denominator =  1 + np.exp(-2 * z)\n        self.x = (2 / denominator) - 1\n        return self.x\n    def derivative(self,z):\n        self.dx = 1 - self.x ** 2\n        return np.multiply(z,self.dx)\n", "intent": "Now let's go with another familiar one, tanh.\n"}
{"snippet": "import tensorflow as tf\nhello_constant = tf.constant('Hello World!')\nwith tf.Session() as sess:\n    output = sess.run(hello_constant)\n    print(output)\n", "intent": "TensorFlow hello world program\n"}
{"snippet": "if gpus <= 1:\n    model = create_conv_model()\nelse:\n    with tf.device(\"/cpu:0\"):\n        model = create_conv_model()\n    model = multi_gpu_model(model, gpus=gpus)\n", "intent": "if more than one GPU is present on the machine we need to create a copy of the model on each GPU and sync them on the CPU\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "from pyspark.ml.classification import GBTClassifier\ngb = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\npipeline = Pipeline(stages=[gb])\ngbModel = pipeline.fit(trainingData)\n", "intent": "1. Train a GBTClassifier on the training data, call the trained model 'gbModel'\n"}
{"snippet": "from sklearn.svm import SVC\nclf = SVC(kernel='linear')\nclf.fit(X, y)\n", "intent": "**Let's Try**\nFit the model:\n"}
{"snippet": "Param_grid_Lr={\"Scaler__with_std\":[True,False],\"LR__C\":np.logspace(-5,5,11),\"LR__penalty\":['l2'],\"LR__class_weight\":[\"balanced\"]}\nbest_Lr_recall=GridSearchCV(Pipe_Lr,Param_grid_Lr,cv=4,verbose=1,n_jobs=1,scoring='recall')\nbest_Lr_recall.fit(Train_X,Train_Y)\n", "intent": "Lets Now run the same procedure with recall as the optimization parameter\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(500, input_dim=x_train.shape[1]))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "import statsmodels.formula.api as smf\npollster_anova = smf.ols(formula=\"bias ~ pollster - 1\", data=tmppolls).fit()\npollster_anova.summary()\n", "intent": "We can also explore these biases using a linear regression to explain `bias in terms of the categorical variable `pollster`.\n"}
{"snippet": "nodes = [2**n for n in range(6,12)]\nlosses = run_model(nodes)\n", "intent": "Varying the amount of nodes in each of the hidden layers\n"}
{"snippet": "hidden_scores = [i for i in range(1, 6)]\nlayer_losses = run_model(hidden_layers)\n", "intent": "Varying the amount of hidden layers\n"}
{"snippet": "dropout_score = mlp_model(1, dropout=False)\nprint('Score without dropout: {}'.format(dropout_score))\n", "intent": "Removing dropout layers\n"}
{"snippet": "activation_score = mlp_model(1, activation=False)\nprint('Score without dropout: {}'.format(activation_score))\n", "intent": "Score is slightly lower without drop out\n"}
{"snippet": "batch_size = [2**n for n in range(5, 12)]\nbatch_scores = run_model(batch_size)\nplot_scores(batch_scores['nodes'], batch_scores['scores'], 'batch_size', 'scores')\n", "intent": "WOOH now that's a huge drop\n"}
{"snippet": "best_model = mlp_model(1)\nprint('Final score: {}'.format(best_model))\n", "intent": "Running the model with all the best parameters\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, input_dim=x_train[-1].shape[0], activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metric=[\"accuracy\"])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, batch_size=512, epochs = 125, validation_split=0.2, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model.fit(Xsmall, ysmall, batch_size=256, epochs=40,verbose = 1)\nmodel.save_weights('model_weights/best_RNN_small_textdata_weights.hdf5')\n", "intent": "Now lets fit our model!\n"}
{"snippet": "k = 1 \nclassifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors=k)\nclassifier.fit(x,y)\n", "intent": "\"train\" a KNN Classifier\n========================\n*** Why is this not really training? ***\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state=0)\nmodel\n", "intent": "Let's use a Decision tree model\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(random_state=0)\nmodel.fit(X_train, y_train)\nmy_score = model.score(X_test, y_test)\nprint(\"Classification Score: %0.3f\" % my_score)\nprint(\"Benchmark Score: %0.3f\" % benchmark_accuracy)\n", "intent": "Let's try with a Random Forest Classifier\n"}
{"snippet": "model = RandomForestClassifier(random_state=0)\nmodel.fit(X_train, y_train)\nmy_score = model.score(X_test, y_test)\nprint(\"Classification Score: %0.3f\" % my_score)\nprint(\"Benchmark Score: %0.3f\" % benchmark_accuracy)\n", "intent": "Random Forest Classifier\n"}
{"snippet": "scaler.fit(bnd.drop('Class',axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "scaler.fit(knn_data.drop('TARGET CLASS',axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "mimetic_digraph = build_graph(mimetic_digraph, user_id, mimetic_following_set, mimetic_followers_set, level=0)\nstore_objects(mimetic_digraph=mimetic_digraph)\n", "intent": "And store the graph as a pickle so I don't have to keep on recreating it.\n"}
{"snippet": "def polynomial_ridge_regression(data, deg, l2_penalty):\n    feature_cols = []\n    for i in xrange(deg):\n        feature_cols.append('X' + str(i+1))\n    poly_data = polynomial_features(data,deg)\n    X = poly_data[feature_cols]\n    y = poly_data.Y\n    model = Ridge(alpha = l2_penalty)\n    model.fit(X, y) \n    return model\n", "intent": "Function to solve the ridge objective for a polynomial regression model of any degree:\n"}
{"snippet": "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price') \nmy_weights = np.array([1., 10.])\ntest_predictions = predict_output(example_features, my_weights) \nerrors = test_predictions - example_output \nprint feature_derivative_ridge(errors, example_features[:,1], my_weights[1], 1, False)\nprint np.sum(errors*example_features[:,1])*2+20.\nprint ''\nprint feature_derivative_ridge(errors, example_features[:,0], my_weights[0], 1, True)\nprint np.sum(errors)*2.\n", "intent": "To test your feature derivartive run the following:\n"}
{"snippet": "best_model = linear_model.Lasso(alpha=10)\nX = training[all_features]\ny = training['price']\nbest_model.fit(X, y)\nprint best_model.coef_\nprint best_model.intercept_\n", "intent": "Notice best l1_penalty = 10\nGet the coefficents and intercepts of the corresponding model.\n"}
{"snippet": "from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn. ensemble import VotingClassifier\nlr = LogisticRegression()\ndt = DecisionTreeClassifier()\nsvm = SVC(kernel = 'poly', degree = 2 )\nevc = VotingClassifier( estimators= [('lr',lr),('dt',dt),('svm',svm)], voting = 'hard')\nevc.fit(X_train,y_train)\nevc.score(X_test, y_test)\n", "intent": "voting for classification \n"}
{"snippet": "clf = cluster.KMeans(init='k-means++', n_clusters=3, random_state=33)\nclf.fit(X_train4)\n", "intent": "Now, calculate the clusters, using the four attributes\n"}
{"snippet": "def regression_model():\n    model = Sequential()\n    model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(50, activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model\n", "intent": "Let's define a function that defines our regression model for us so that we can conveniently call it to create our model.\n"}
{"snippet": "tensor_example = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\nprint(\"What is the value on 1st-row first two columns? \", tensor_example[0, 0:2])\nprint(\"What is the value on 1st-row first two columns? \", tensor_example[0][0:2])\n", "intent": "Let us see how  we use slicing with 2D tensors to get the values in the above picture.\n"}
{"snippet": "tensor_example = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\nprint(tensor_example)\nprint(\"What is the value on 3rd-column last two rows? \", tensor_example[1:3, 2])\n", "intent": "Let's see the code below.\n"}
{"snippet": "X = torch.tensor([[1, 0],[0, 1]]) \nY = torch.tensor([[2, 1],[1, 2]])\nX_plus_Y = X + Y\nprint(\"The result of X + Y: \", X_plus_Y)\n", "intent": "Let us see how tensor addition works with <code>X</code> and <code>Y</code>.\n"}
{"snippet": "Y = torch.tensor([[2, 1], [1, 2]]) \ntwo_Y = 2 * Y\nprint(\"The result of 2Y: \", two_Y)\n", "intent": "Let us try to calculate the product of <b>2Y</b>.\n"}
{"snippet": "X = torch.tensor([[1, 0], [0, 1]])\nY = torch.tensor([[2, 1], [1, 2]]) \nX_times_Y = X * Y\nprint(\"The result of X * Y: \", X_times_Y)\n", "intent": "The code below calculates the element-wise product of the tensor <strong>X</strong> and <strong>Y</strong>:\n"}
{"snippet": "A = torch.tensor([[0, 1, 1], [1, 0, 1]])\nB = torch.tensor([[1, 1], [1, 1], [-1, 1]])\nA_times_B = torch.mm(A,B)\nprint(\"The result of A * B: \", A_times_B)\n", "intent": "We use <code>torch.mm()</code> for calculating the multiplication between tensors with different sizes.\n"}
{"snippet": "print(\"Type of the first element: \", type(dataset[0]))\nprint(\"The length of the tuple: \", len(dataset[0]))\nprint(\"The shape of the first element in the tuple: \", dataset[0][0].shape)\nprint(\"The type of the first element in the tuple\", type(dataset[0][0]))\nprint(\"The second element in the tuple: \", dataset[0][1])\nprint(\"The type of the second element in the tuple: \", type(dataset[0][1]))\nprint(\"As the result, the structure of the first element in the dataset is (tensor([1, 28, 28]), tensor(7)).\")\n", "intent": "Each element of the dataset object contains a tuple. Let us see whether the first element in the dataset is a tuple and what is in it.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -.1, .1), name='embedding')\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "rf_model = rf_classifier.fit(X_train, y_train)\n", "intent": "Now we fit the model on our training data.\n"}
{"snippet": "param_grid = {'min_samples_split': range(2,10),\n              'min_samples_leaf': range(1,10)}\nmodel_rf = GridSearchCV(ensemble.RandomForestClassifier(n_estimators=10), param_grid, cv=3, iid=False)\nmodel_rf.fit(X_train, y_train)\nbest_index = np.argmax(model_rf.cv_results_[\"mean_test_score\"])\nprint(\"Best parameter values:\", model_rf.cv_results_[\"params\"][best_index])\nprint(\"Best Mean cross-validated test accuracy:\", model_rf.cv_results_[\"mean_test_score\"][best_index])\nprint(\"Overall Mean test accuracy:\", model_rf.score(X_test, y_test))\n", "intent": "Let's do another grid search to determine the best parameters:\n"}
{"snippet": "from sklearn import linear_model\nlin_reg = linear_model.LinearRegression(n_jobs=1)  \n", "intent": "We'll start with a basic [OLS linear regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n"}
{"snippet": "ridge_cv = linear_model.RidgeCV(alphas=alphas,\n                               normalize=False,\n                               store_cv_values=True)\nridge_cv.fit(X_train, y_train);\nprint('Selected Alpha:', ridge_cv.alpha_)\n", "intent": "By default the `RidgeCV` uses \"Leave One Out Cross Validation\" (LOOCV). Let's fit the Ridge model\n"}
{"snippet": "kmeans = KMeans(n_clusters=3,\n               max_iter=300 \n               ).fit(X)\n", "intent": "Now we can create the model. Notice how we are chaining the 'fit' function onto the model instantiation. \n"}
{"snippet": "from sklearn.cluster import AgglomerativeClustering\nward = AgglomerativeClustering(n_clusters=3,\n                               linkage='ward', \n                               affinity='euclidean') \n", "intent": "We'll use two clusters this time, and use ward linkage.\n"}
{"snippet": "dbscan = DBSCAN(eps=0.2)\ndbscan.fit(blobs)\n", "intent": "Now let's fit another DBSCAN model to the blobs data.\n"}
{"snippet": "from tpot import TPOTRegressor\ntpot = TPOTRegressor(generations=5, population_size=20, verbosity=1, scoring='r2')  \ntpot.fit(X_train, y_train.ravel())\nprint(tpot.score(X_test, y_test.ravel()))\ntpot.export('tpot_heart_pipeline.py')\n", "intent": "Now TPOT will make the model.\n"}
{"snippet": "input_dim = x_train.shape[1]\nmodel = Sequential()\nmodel.add(Dense(2, input_dim=input_dim))\nmodel.add(Activation(\"sigmoid\"))\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "test_input = [[0.25, 0.15]]\ngraph=tf.Graph()\nwith tf.Session() as session:\n    tf.global_variables_initializer().run(session=session)\n    output = session.run([out],feed_dict=feed_dict)\n    print(output[0]) \n", "intent": "As with the previous example, we can use `session.run()` to execute this computation graph, and use a `feed_dict` to feed in our input:\n"}
{"snippet": "x = torch.randn((2, 2))\ny = torch.randn((2, 2))\nz = x + y  \nvar_x = autograd.Variable(x)\nvar_y = autograd.Variable(y)\nvar_z = var_x + var_y\nprint(var_z.grad_fn)\nvar_z_data = var_z.data  \nnew_var_z = autograd.Variable(var_z_data)\nprint(new_var_z.grad_fn)\n", "intent": "Understanding what is going on in the block below is crucial for being a\nsuccessful programmer in deep learning.\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, epochs=20, batch_size=32, validation_data=(X_test, Y_test))\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "clf = RandomForestClassifier(n_estimators = 100, max_features = 'auto', n_jobs=-1, max_depth = 3, class_weight = 'balanced')\nclf.fit(features_train, target_train)\n", "intent": "Train the random forest classifier\n"}
{"snippet": "from sklearn.svm import SVC\nclf = SVC(kernel='linear', C=1000000000)\nclf.fit(X, y)\n", "intent": "By reducing C, we are penalizing the errors less\n"}
{"snippet": "logreg = LogisticRegression(C=gs.best_params_['C'],\\\n                            penalty=gs.best_params_['penalty'])\ncv_model = logreg.fit(X_train, y_train)\n", "intent": "Compared to my score for the vanilla logistic regression of .78 the gridsearchcv is 2% better (.80).\n"}
{"snippet": "z_sample = T.matrix()\ngen_fn = theano.function([z_sample], generated_x)\n", "intent": "This task requires deeper Lasagne knowledge. You need to perform inference from $z$, reconstruct an image given some random $z$ representations.\n"}
{"snippet": "n_steps = T.scalar(dtype='int32')\nfeedback_loop = Recurrence(\n    state_variables={step.h_new:step.h_prev,\n                     step.next_token:step.inp},\n    tracked_outputs=[step.next_token_probas,],\n    batch_size=theano.shared(1),\n    n_steps=n_steps,\n    unroll_scan=False,\n)\n", "intent": "here we re-wire the recurrent network so that it's output is fed back to it's input\n"}
{"snippet": "n_steps = T.scalar(dtype='int32')\nfeedback_loop = Recurrence(\n    state_variables=<...>,\n    tracked_outputs=<...>,\n    batch_size=theano.shared(1),\n    n_steps=n_steps,\n    unroll_scan=False,\n)\n", "intent": "here we re-wire the recurrent network so that it's output is fed back to it's input\n"}
{"snippet": "model.fit(X_train, X_train, nb_epoch=5, batch_size=258)\n", "intent": "Note that 784 dimension is reduced through encoding to 144 in the hidden layer and again in layer 3 constructed back to 784 using decoder.\n"}
{"snippet": "k_values = [1,2,5,10,20] \nknn_performance = [] \nfor kk in k_values:\n    knn_model = neighbors.KNeighborsClassifier(n_neighbors = kk)\n", "intent": "Plot the performance of the kNN classifier for different values of `k`. For which `k` is best performance achieved?\n"}
{"snippet": "basic_pipe = Pipeline([('bt', bt), ('ridge', Ridge())])\nbasic_pipe.fit(train, y)\nbasic_pipe.score(train, y)\n", "intent": "Our transformer can be part of pipeline.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1000))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(256))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(2))\nmodel.add(Activation(\"sigmoid\"))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "history = model.fit(x_train, y_train,\n                    epochs=20,\n                    verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "history = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from keras.callbacks import ModelCheckpoint  \ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best_adamax.ResNet50.hdf5', \n                               verbose=1, save_best_only=True)\nepochs = 30\nbatch_size = 64\nResNet_model.fit(train_ResNet50, train_targets, \n          validation_data=(valid_ResNet50, valid_targets),\n          epochs=epochs, batch_size=batch_size, callbacks=[checkpointer], verbose=1)\n", "intent": "Here we train the model in the code cell below.\n"}
{"snippet": "print(x_train.shape)\nprint(y_train.shape)\nmodel.fit(x_train, y_train, epochs=20, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "LR = LogisticRegression(C=0.01)\nLR.fit(X_train,y_train)\n", "intent": "Lets build our model:\n"}
{"snippet": "lr.fit(X_train[[\"PROF\"]],y_train)\nlr.score(X_test[[\"PROF\"]], y_test)\n", "intent": "We can fit the model and calculate the R^2:\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=10)\nmodel.fit(X, y)\nmodel.score(X, y)\n", "intent": "- Initialize kNN, fit the model on all data, and compute the accuracy of your model.\n"}
{"snippet": "lr.fit(X_train, y_train)\nlr.score(X_test, y_test)\n", "intent": " We can also use multiple linear regression, the R^2 is approximately 0.155.\n"}
{"snippet": "lr.fit(Z_train, y_train)\nlr.score(Z_test, y_test)\n", "intent": " We can fit the model and calculate the R^2:\n"}
{"snippet": "rr = Ridge(alpha=0.1,normalize=True)\nrr.fit(X, y)\n", "intent": " We create a Ridge Regression object and fit the model:\n"}
{"snippet": "Alpha=[0.001, 0.001,0.01,0.1,1]\nRsq_ridge=[]\nfor a in Alpha:\n    rr = Ridge(alpha=a,normalize=True)\n    rr.fit(X_train, y_train)\n    print(rr.score(X_test, y_test))\n    Rsq_ridge.append(rr.score(X_test, y_test))\n", "intent": "We test the R^2  for different Alpha: \n"}
{"snippet": "Rsq_ridge_poly=[]\nfor a in Alpha:\n    clf = Ridge(alpha=a,normalize=True)\n    clf.fit(Z_train, y_train)\n    Rsq_ridge_poly.append(clf.score(Z_test, y_test))\n", "intent": " We can perform the same set of operations with polynomial features:  \n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded = tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "X, actual, y = generate_sample_data(20, true_function)\nlr = LinearRegression()\nlr.fit(X,y)\ncompare_fitted_function(X , y, actual, lr)\n", "intent": "Generate some data, fit a linear regression model, and plot the results.\n"}
{"snippet": "degree = 4\npoly = PolynomialFeatures(degree = degree, include_bias=False)\nlr = LinearRegression(fit_intercept=True)\npipeline = Pipeline([(\"polynomial_features\", poly),\n                     (\"linear_regression\", lr)])\npipeline.fit(X, y)\ncompare_fitted_function(X, y, true_function(X), pipeline)\n", "intent": "We can use sklearn's Pipeline framework to connect the polynomial expansion and the linear regression into one 'model'\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ndt = DecisionTreeRegressor()\nparameter_grid = {\n    'min_samples_leaf': [1, 2, 5, 10, 20, 30, 40, 50, 60, 70, 80],\n    'max_depth': [3, 4, 5]}\nparam_searcher = GridSearchCV(dt, parameter_grid, cv=10)\n", "intent": "Sklearn doesn't allow _post-pruning_ of decision trees, but we can try some pre-pruning parameters\n"}
{"snippet": "model = LinearRegression(fit_intercept=False)\nprint model.fit(X, y).score(X, y)\nprint model.intercept_\nprint model.coef_\n", "intent": "Note that the first coefficient is zero, since we already fitted an intercept. \nAlternatively, we can do:\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3)\nkm.fit(X)\n", "intent": "Use KMeans to segment the iris data into two clusters\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=2)\nkm.fit(X)\n", "intent": "Use KMeans to segment the iris data into two clusters\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Let's see how to train a model and save its weights.\nFirst start with a model:\n"}
{"snippet": "tf.reset_default_graph() \nx = tf.placeholder(tf.float32, (None, 32, 32, 3))\ny = tf.placeholder(tf.int32, (None))\none_hot_y = tf.one_hot(y, 43)\n", "intent": "x is a placeholder for a batch of input images. y is a placeholder for a batch of output labels.\nYou do not need to modify this section.\n"}
{"snippet": "model = xgb.XGBClassifier(max_depth=19,\n                        subsample=0.8325428224507427,\n                          min_child = 6,\n                        objective='binary:logistic',\n                        n_estimators=50,\n                        learning_rate = 0.1)\neval_set = [(train_X, train_Y), (test_X, test_Y)]\nmodel.fit(train_X, train_Y.values.ravel(), early_stopping_rounds=15, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)\n", "intent": "x_max_depth': 19, 'x_min_child': 6, 'x_subsample': 0.8325428224507427\n"}
{"snippet": "model.add(Dense(num_classes, activation='softmax',kernel_initializer='normal'))\n", "intent": "Create and add in the output layer to our model. Use 'softmax' activation and a normal kernel_initializer.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train,\n          batch_size=32,\n          epochs=10,\n          validation_data=(x_test, y_test), \n          verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "kmeans = KMeans(n_clusters=5,random_state=0).fit(X)\nkmeans.labels_\nkmeans\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "model = sm.ols(formula=\"sl ~ sx + yr\", data=data).fit()\nmodel.summary()\n", "intent": "We can use categorical features as well, and `statsmodels` will use `patsy` for that just as we have been before.\n"}
{"snippet": "kmeans = KMeans(n_clusters=5,random_state=0).fit(X)\nkmeans.labels_\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n    embedded = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "outputs, states = tf.nn.dynamic_rnn(cell,x,dtype=tf.float32)\n", "intent": "** Now pass in the cells variable into tf.nn.dynamic_rnn, along with your first placeholder (X)**\n"}
{"snippet": "from sklearn import svm\nfrom sklearn.metrics import accuracy_score\nclf=svm.SVC(gamma=0.001, C=100)\n", "intent": "I followed this [scikit-learn example](http://scikit-learn.org/stable/tutorial/basic/tutorial.html\n"}
{"snippet": "from sklearn import datasets, neighbors, linear_model\ndef train_models(train_size, train_vector, train_labels):\n    train_vector_small=train_vector[:train_size,:]\n    train_labels_small=train_labels[:train_size]\n    clf=svm.SVC(gamma=0.001, C=100)\n    logistic = linear_model.LogisticRegression()\n    clf.fit(train_vector_small, train_labels_small)\n    logistic.fit(train_vector_small, train_labels_small)\n    return clf, logistic\n", "intent": "Now define a function that trains the model for a given training size\n"}
{"snippet": "def softmax(x):\n    return np.exp(x)/np.sum(np.exp(x), axis=0)\n", "intent": "Define the softmax function (generalization of logistic)\n$$\\sigma(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$$\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n                                            hidden_units=[10, 20, 10],\n                                            n_classes=2,\n                                            model_dir=\"./project_output\")\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.1))\n    embed = tf.nn.embedding_lookup(embedding,  input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nprint(x_train.shape)\nmodel.add(Dense(512,activation = 'relu', input_shape = (1000,)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation  = 'softmax'))\nmodel.compile(loss= 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "print model.fit(X, y).score(X, y)\nprint model.fit(X_scaled, y).score(X_scaled, y)\n", "intent": "Note that this does not affect the modeling performance at all\n"}
{"snippet": "kmeans.fit(data.ix[:,2:])\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "data_features = data.drop('TARGET CLASS', axis=1)\nscaler.fit(data_features)\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "knn.fit(x_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "def soft_threshold(rho,lamda):\n    if rho < - lamda:\n        return rho + lamda\n    elif rho > lamda:\n        return rho - lamda\n    else: \n        return 0\nlamda = 3    \ny_st = [soft_threshold(xii,lamda) for xii in x1]\n", "intent": "$ S(\\rho_j, \\lambda)$\n"}
{"snippet": "lambda_range = np.logspace(0,2,num = 100)/1000\ntheta_0_list_reg_l1 = []\ntheta_1_list_reg_l1 = []\nfor l in lambda_range:\n    model_sk_reg = linear_model.Lasso(alpha=l, fit_intercept=False)\n    model_sk_reg.fit(X,y_noise)\n    t0, t1 = model_sk_reg.coef_\n    theta_0_list_reg_l1.append(t0)\n    theta_1_list_reg_l1.append(t1)\n", "intent": "Note we are using Sklearn as there is no closed form solution\n"}
{"snippet": "happyModel.fit(x = X_train, y = Y_train, epochs = 20, batch_size = 16)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size,embed_dim],-1,1))\n    embedded = tf.nn.embedding_lookup(embedding,input_data)\n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "dropout_layer = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "model.fit(X, data.Insult)  \ncoef = pd.Series(model.coef_[0], index=cv.get_feature_names())  \ncoef.sort()  \n", "intent": "Let's look at the most important features.\n"}
{"snippet": "import tensorflow as tf\nhello = tf.constant('It works!')\nname = tf.constant(\"My name is .\")\nsess = tf.Session()\nprint(sess.run(hello))\nprint(sess.run(name))\n", "intent": "This snippet of Python creates a simple graph.\n"}
{"snippet": "lm1 = smf.ols(formula='sales ~ TV', data=data).fit()\nlm1.params\n", "intent": "Let's estimate the model coefficients for the advertising data:\n"}
{"snippet": "model.fit(X_train, newsgroups_train.target)\n", "intent": "Let's fit the model and analyze the coeficients.\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\nX = df.drop('class', axis =1)\ny = df['class']\ngnb.fit(X,y)\nprint(gnb.score(X,y))\n", "intent": "Fit a Gaussian Naive Bayes model to the original dataset A (including the outliers). Display the classification accuracy on the training dataset.\n"}
{"snippet": "kmeans = KMeans(n_clusters=5, random_state=1).fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "slr = LinearRegression() \nslr.fit(X,y)\nprint('Intercept:', slr.intercept_) \nprint('Slope:', slr.coef_)\n", "intent": "We will now use Scikit-Learn's `LinearRegression` module to create the slr model. \n"}
{"snippet": "X = data.iloc[:,0:2] \ny = data.iloc[:,2] \nmlr = LinearRegression() \nmlr.fit(X,y)\nprint(mlr.coef_)\nprint(mlr.intercept_)\nprint(mlr.score(X,y))\n", "intent": "We can use the `LinearRegression` class from Scikit-Learn to create multiple regression models.\n"}
{"snippet": "X = mpg.iloc[:,1:6]\nprint(X.head())\ny = mpg.iloc[:,0]\nprint(X.shape)\nmpg_lm = LinearRegression()\nmpg_lm.fit(X,y)\nprint(mpg_lm.intercept_)\nprint(mpg_lm.coef_)\nprint(mpg_lm.score(X,y))\n", "intent": "We will create a regression model using columns 1 through 5 as features. \n"}
{"snippet": "X = x.reshape(len(x),1) \nlinearMod = LinearRegression() \nlinearMod.fit(X,y)\nprint(\"Intercept: \", linearMod.intercept_)\nprint(\"Coefficeint: \", linearMod.coef_)\nprint(\"Training r^2: \", linearMod.score(X,y))\n", "intent": "We will start by using Scikit-Learn to produce a linear model.\n"}
{"snippet": "knn_mod = KNeighborsRegressor(8)\nknn_mod.fit(sX_train, y_train)\nprint('--- KNN Model (K=8) ---')\nprint('Training r2:', knn_mod.score(sX_train, y_train))\nprint('Testing r2: ', knn_mod.score(sX_test, y_test))\n", "intent": "It appears that any *K* in the range from about 8 to 20 would be good. \n"}
{"snippet": "lr_mod = LogisticRegression()\nlr_mod.fit(X_train, y_train)\nprint('Training Accuracy:', lr_mod.score(X_train, y_train))\nprint('Testing Accuracy:', lr_mod.score(X_test, y_test))\n", "intent": "When creating a Logistic Regression mode, there will be no hyper-parameters that you are asked to specify. \n"}
{"snippet": "knn_mod = KNeighborsClassifier(n_neighbors=6)\nknn_mod.fit(X_train, y_train)\nprint('Training Accuracy:', knn_mod.score(X_train, y_train))\nprint('Testing Accuracy:', knn_mod.score(X_test, y_test))\n", "intent": "When creating a KNN model, you will be provided with the number of neighbors to use in the algorithm. No other hyper-parameters will be used. \n"}
{"snippet": "model = DecisionTreeClassifier()\n", "intent": "We'll start training one decision tree.\n"}
{"snippet": "ols_mod = LinearRegression()\nols_mod.fit(X_train, y_train)\nprint('Training r-squared:', ols_mod.score(X_train, y_train))\nprint('Testing r-squared:', ols_mod.score(X_test, y_test))\n", "intent": "With ordinary least squares regression (i.e. regular linear regression), there are no hyper-parameters to specify. \n"}
{"snippet": "knn_mod = KNeighborsRegressor(n_neighbors=10)\nknn_mod.fit(X_train, y_train)\nprint('Training r-squared:', knn_mod.score(X_train, y_train))\nprint('Testing r-squared:', knn_mod.score(X_test, y_test))\n", "intent": "When creating a KNN model, you will be provided with the number of neighbors to use in the algorithm. No other hyper-parameters will be used. \n"}
{"snippet": "rtree_mod = DecisionTreeRegressor(max_depth=8, min_samples_leaf=5)\nrtree_mod.fit(X_train, y_train)\nprint('Training r-squared:', rtree_mod.score(X_train, y_train))\nprint('Testing r-squared:', rtree_mod.score(X_test, y_test))\n", "intent": "You will be asked to specifty two parameters for the decision tree model: `max_depth` and `min_samples_leaf`. These values will be provided to you.\n"}
{"snippet": "Xorig, y_orig = getFeaturesAndOutput(partA, 'class')\nprint Xorig.shape\nprint y_orig.shape\nclsOrig = MultinomialNB()\nclsOrig.fit(Xorig, y_orig)\n", "intent": "Fit a Gaussian Naive Bayes model to the original dataset A (including the outliers). Display the classification accuracy on the training dataset.\n"}
{"snippet": "dt = DecisionTreeClassifier(criterion='entropy', max_depth=2)\ndt\n", "intent": "MAKE SURE YOU NAME your tree classifier as \"dt\"\n"}
{"snippet": "linearSVC = LinearSVC()\n", "intent": "Train a [`LinearSVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n"}
{"snippet": "rbfSVC = SVC(kernel='rbf')\n", "intent": "By using the [`SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"}
{"snippet": "def scorePolySVC(degree=2):\n    polySVC = SVC(kernel='poly',degree=degree).fit(Xtrain, yTr)\n    return polySVC.score(Xtrain, yTr), polySVC.score(Xvalid, yVal)\n", "intent": "The transformation of the data using RBF has improved the classification accuracy\n"}
{"snippet": "Xorig, y_orig = getFeaturesAndOutput(partA, 'class')\nprint Xorig.shape\nprint y_orig.shape\nclsOrig = MultinomialNB()\nclsOrig.fit(Xorig, y_orig)\n", "intent": "<span style=\"color:red\">wrong, see my comment for 2.3\n"}
{"snippet": "model = RandomForestClassifier(n_estimators=20)\n", "intent": "Let's try a small forest with a number of trees.\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 4))\nnet.add(ReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\nprint(net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "a = 3\ndef testGlobalVariable():\n    print(a)\ntestGlobalVariable()\n", "intent": "* An instance variable is visible to all functions, constructors and blocks, within the file that it is declared in.\n"}
{"snippet": "history = model.fit(x_train, y_train, epochs=epocs, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "import tensorflow as tf\ninput = tf.placeholder(tf.float32, (None, 32, 32, 3))\nfilter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) \nfilter_bias = tf.Variable(tf.zeros(20))\nstrides = [1, 2, 2, 1] \npadding = 'SAME'\nconv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias\n", "intent": "Usando TensorFlow ficaria:\n"}
{"snippet": "ss.fit(bank_data.drop('Class', axis = 1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "km.fit(data.drop('Private', axis = 1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "features = ['XVPM', 'GWYH', 'TRAT', 'TLLZ', 'IGGA', 'HYKR', 'EDFS', 'GUUB', 'MGJM','JHZC']\nscaler.fit(df[features])\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "knn = KNeighborsClassifier()\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "knn.fit(train_x, train_y)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "C = 1.0  \nmodels = [\n    ['SVC with linear kernel', svm.SVC(kernel='linear', C=C).fit(X, y)],\n    ['LinearSVC (linear kernel)', svm.LinearSVC(C=C).fit(X, y)],\n    ['SVC with RBF kernel', svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)],\n    ['SVC with polynomial (degree 3) kernel', svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)]]\n", "intent": "Train models. We create an instance of SVM and fit out data. We do not scale our data since we want to plot the support vectors.\n"}
{"snippet": "nb.fit(train_x, train_y)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "clf1 = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_split=2)\nclf2 = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_split=90)\nclf1.fit(X_train, y_train)\nclf2.fit(X_train, y_train)\nacc_min_samples_split_2 = clf1.score(X_test, y_test)\nacc_min_samples_split_50 = clf2.score(X_test, y_test)\nprint(\"Accuracy with min split = 2:\", acc_min_samples_split_2)\nprint(\"Accuracy with min split = 100:\", acc_min_samples_split_50)\n", "intent": "Try out a different min samples split and see the difference\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dropout(0.2, input_shape = (1000,)))\nmodel.add(Dense(100, activation = 'sigmoid'))\nmodel.add(Dense(50, activation = 'sigmoid', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dense(2, activation = 'sigmoid'))\nsgd = SGD(lr=0.05, momentum=0.9, decay=0.0, nesterov=False)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=128,\n          epochs=20,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def conv2d(input):\n    F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3)))\n    F_b = tf.Variable(tf.zeros(3))\n    strides = [1, 2, 2, 1]\n    padding = 'VALID'\n    return tf.nn.conv2d(input, F_W, strides, padding) + F_b\nconv_out = conv2d(X)\n", "intent": "```\nnew_height = (input_height - filter_height + 2 * P)/S + 1\nnew_width = (input_width - filter_width + 2 * P)/S + 1\n```\n"}
{"snippet": "model.fit(X_train,y_train)\n", "intent": "Train a logistic regression model on the train set.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), dtype=tf.float32)\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "graph_model = DecisionTreeClassifier(criterion='entropy', random_state=42)\ngraph_X = X[:200]\ngraph_y = y[:200]\n", "intent": "`When entropy is 1, the classes are balanced and when entropy is 0 everything is the same class.` \n"}
{"snippet": "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1, verbose=False)\nkm.fit(X)\n", "intent": "Do the actual clustering.\n"}
{"snippet": "from sklearn.pipeline import Pipeline\npipeline = Pipeline([\n        ('features', vectorizer),\n        ('model', model)   \n    ])\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: tst_x,\n            labels_: tst_y,\n            keep_prob_: 1.0}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(\n        tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0),\n        name=\"embedding_matrix\")\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./rnn_time_series_model\")\n    zero_seq_seed = [0. for i in range(num_time_steps)]\n    for iteration in range(len(ts_data.x_data) - num_time_steps):\n        X_batch = np.array(zero_seq_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n        y_pred = sess.run(outputs, feed_dict={X: X_batch})\n        zero_seq_seed.append(y_pred[0, -1, 0])\n        print(X_batch)\n        print(\"\\n\")\n", "intent": "** Note: Can give wacky results sometimes, like exponential growth**\n"}
{"snippet": "word_embedding = td.Embedding(\n    *weight_matrix.shape, initializer=weight_matrix, name='word_embedding')\n", "intent": "Create the word embedding using [`td.Embedding`](https://github.com/tensorflow/fold/blob/master/tensorflow_fold/g3doc/py/td.md\n"}
{"snippet": "kk = 6\nimg_kmeans = KMeans(n_clusters= kk).fit(img_new)\n", "intent": "** 2. ** Fit kmeans on the model and predict what cluster each pixel is in.\n"}
{"snippet": "def kmeans(X,k,max_iters =100):\n    centroids = random_centroids(X,k)\n    labels = old_centroids = None\n    iteration = 1\n    while not should_stop(old_centroids,centroids,iteration,max_iters):\n        labels = closest_centroids(X,centroids)\n        old_centroids = centroids\n        centroids = calc_new_centroids(X,labels,k)\n        iteration += 1\n    return centroids, labels\n", "intent": "We've broken down the algo it small parts now lets piece it altogether for our final implementation. \n"}
{"snippet": "x_split = tf.split(0, len(char_dic), x_data) \noutputs, state = tf.nn.rnn(cell = rnn_cell, inputs = x_split, initial_state = initial_state)\nprint outputs, '\\n'\nprint state\n", "intent": "x_split output \n```python\n[[1,0,0,0]] \n[[0,1,0,0]] \n[[0,0,1,0]] \n[[0,0,0,1]] \n```\n"}
{"snippet": "X4_train = np.c_[X2_train.iloc[:,0:2], X2_train.iloc[:, 0] * X2_train.iloc[:, 1]]\nX4_test = np.c_[X2_test.iloc[:,0:2], X2_test.iloc[:, 0] * X2_test.iloc[:, 1]]\nregr4 = LinearRegression()\nregr4.fit(X4_train, y2_train)\nprint('Variance score: %.4f' % regr4.score(X4_test, y2_test))\n", "intent": "The hyperplane plot suggests that we should consider adding the interaction term \"TV*radio\". \n"}
{"snippet": "model.fit(X_train, y_train)\n", "intent": "Training RBM-Logistic Pipeline.\n"}
{"snippet": "X_over, Y_over = oversample(X_train, Y_train, minority_weight=.5)\nlr = LogisticRegression(penalty='l1')\ncv = cross_validation(lr, X_over, Y_over , n_splits=5,init_chunk_size = 100, chunk_spacings = 50, average = \"binary\")\ncv.validate_for_holdout_set(X_holdout, Y_holdout)\ncv.plot_learning_curve()\n", "intent": "Regularization: Lasso \n"}
{"snippet": "X_over, Y_over = oversample(X_train, Y_train, minority_weight=.5)\nlr = LogisticRegression(penalty='l2')\ncv = cross_validation(lr, X_over, Y_over , n_splits=5,init_chunk_size = 100, chunk_spacings = 50, average = \"binary\")\ncv.validate_for_holdout_set(X_holdout, Y_holdout)\ncv.plot_learning_curve()\n", "intent": "Regularization: Ridge \n"}
{"snippet": "sm = SMOTE(kind='regular')\nX_smote, Y_smote= sm.fit_sample(X_train, Y_train)\nlr = LogisticRegression()\ncv = cross_validation(lr, X_smote, Y_smote , n_splits=5, init_chunk_size = 100, chunk_spacings = 10, average = \"binary\")\ncv.validate_for_holdout_set(X_holdout, Y_holdout)\ncv.plot_learning_curve()\n", "intent": "Regularization: It dosen't matter (you'll see).\n"}
{"snippet": "X_under, Y_under = undersample(X_train, Y_train, majority_weight=.5)\nlr = LogisticRegression(penalty='l2')\ncv = cross_validation(lr, X_under, Y_under , n_splits=5,init_chunk_size = 100, chunk_spacings = 50, average = \"binary\")\ncv.validate_for_holdout_set(X_holdout, Y_holdout)\ncv.plot_learning_curve()\n", "intent": "Now that we've feature engineered two statistically sound features, let's see how Logistical Regression responds. \n"}
{"snippet": "X_under, Y_under = undersample(X_train, Y_train, majority_weight=.5)\nlr = LogisticRegression(penalty='l2')\ncv = cross_validation(lr, X_under, Y_under , n_splits=5,init_chunk_size = 100, chunk_spacings = 10, average = \"binary\")\ncv.validate_for_holdout_set(X_holdout, Y_holdout)\ncv.plot_learning_curve()\n", "intent": "Regularization: Rigde \n"}
{"snippet": "sm = SMOTE(kind='regular')\nX_smote, Y_smote = sm.fit_sample(X_train, Y_train)\nlr = LogisticRegression()\ncv = cross_validation(lr, X_smote, Y_smote , n_splits=5, init_chunk_size = 100, chunk_spacings = 10, average = \"binary\")\ncv.validate_for_holdout_set(X_holdout, Y_holdout)\ncv.plot_learning_curve()\n", "intent": "Regularization: Ridge\n"}
{"snippet": "lr = LogisticRegression(penalty='l2')\ncv = cross_validation(lr, \n                      X_train_scaled, \n                      Y_train_scaled , \n                      n_splits=10,\n                      init_chunk_size = 100, \n                      chunk_spacings = 25, \n                      average = \"binary\")\ncv.train_for_learning_curve()\ncv.plot_learning_curve()\n", "intent": "Next we are going to simply pass in a scaled version of the data set. \n"}
{"snippet": "lr = LogisticRegression(penalty='l2')\ncv = cross_validation(lr, \n                      X_train_scaled, \n                      Y_train_scaled , \n                      n_splits=10,\n                      init_chunk_size = 100, \n                      chunk_spacings = 25, \n                      average = \"binary\")\ncv.validate_for_holdout_set(X_test_scaled, Y_test_scaled)\ncv.plot_learning_curve()\n", "intent": "----\nAs a reminder, let's get visualize LR's learning curves as a reminder.\n"}
{"snippet": "svc = SVC(C = 100.0, \n          gamma = 1.0, \n          kernel = \"poly\",\n          probability = True)\n", "intent": "Optimized Hyper-Parameters\n     (F1 = 0.91093502317158193, \n     {'C': 100.0, \n     'gamma': 1.0, \n     'kernel': 'poly'})\n"}
{"snippet": "logistic_classifier = linear_model.LogisticRegression(C=100.0)\nlogistic_classifier.fit(X_train, y_train)\n", "intent": "Just a plain logistic regression.\n"}
{"snippet": "svc = SVC(C = 100.0, \n          gamma = 1.0, \n          kernel = \"poly\",\n         probability = True)\n", "intent": "Optimized Hyper-Parameters\n     (F1 = 0.91093502317158193, {'C': 100.0, 'gamma': 1.0, 'kernel': 'poly'})\n"}
{"snippet": "lb_view = client.load_balanced_view()\nmodel = RandomForestClassifier()\nrfc_params = {'max_depth': [None, 2, 6, 10],\n              'min_samples_split': [2, 4, 6],\n              'min_samples_leaf': [1, 3, 5]}\n", "intent": "Now we repeat the same process for a another classifier\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), name='embedding')\n    embed_vec = tf.nn.embedding_lookup(embedding, input_data, name='embed_vec')\n    return embed_vec\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "lm2 = LogisticRegression()\nlm2.fit(analyst[['Austin', 'Chicago', 'New York', 'San Francisco', 'Seattle']], analyst['parsed_salary'])\n", "intent": "I chose to take a look at the model with the \"analyst\" variable as it was the most robust variable out of the data set.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn_parameters = {\n    'n_neighbors': range(1,11),\n    'weights': ['uniform','distance'],\n    'algorithm':['auto']\n}\nknn = KNeighborsClassifier()\ngs = GridSearchCV(estimator=knn, param_grid=knn_parameters)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "lr = ols(\"price ~ age + gender + delay + delay1 + weekday\", data=feat3)\nres = lr.fit()\nres.summary()\n", "intent": "On apprend avec la variable *weekday* :\n"}
{"snippet": "lr = ols(\"price ~ age + gender + delay + delay1 + price_label\", data=feat3)\nres = lr.fit()\nres.summary()\n", "intent": "Et on compare avec la variable *price_label* :\n"}
{"snippet": "from tpot import TPOTRegressor\ntpot = TPOTRegressor(generations=2, population_size=50, verbosity=2)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_boston_pipeline.py')\n", "intent": "[TPOT](https://github.com/rhiever/tpot) est un module d'apprentissage automatique.\n"}
{"snippet": "a = np.array([4, 3, 6])\nb = np.array([3, -1, 2])\nc = np.add(a, b)\nprint(c)\na = tf.constant([4, 3, 6])\nb = tf.constant([3, -1, 2])\nc = tf.add(a, b)\nwith tf.Session() as sess:\n  result = sess.run(c)\n  print(result)\n", "intent": "*Step 2: Perform Basic Matrix Operations Using TensorFlow*\n"}
{"snippet": "from sklearn import linear_model\nclf = linear_model.LinearRegression()\n", "intent": "** Linear Regression **\nhttp://scikit-learn.org/stable/modules/linear_model.html\n"}
{"snippet": "def runLogisticRegression(a,b,c,d):\n", "intent": "*Step 5: Compare Classification Algorithms*\n"}
{"snippet": "estimator.fit(bodyfat_features, bodyfat['body.fat'])\nexpected_coef = pd.Series(regressor.coef_, bodyfat_features.columns)\nprint(regressor.intercept_)\nprint(expected_coef)\n", "intent": "To confirm that our results are valud, we'll compare them with the results of the scikit-learn estimator.\n"}
{"snippet": "tf.reset_default_graph()\nsess = tf.Session()\n", "intent": "Create a new ``tf.Session``.\n"}
{"snippet": "with tf.name_scope('model'):\n    slope = tf.Variable(0.0, name='slope')\n    intercept = tf.Variable(0.0, name='intercept')\n    y = slope * x_placeholder + intercept\n", "intent": "We do the same to define the linear model in the ``model`` scope:\n"}
{"snippet": "tf.reset_default_graph()\nsess = tf.Session()\n", "intent": "Initialize a new ``tf.Session``.\n"}
{"snippet": "LOGDIR = 'graphs'\nwriter = tf.summary.FileWriter(join(LOGDIR, '02_logistic_regression'))\nwriter.add_graph(sess.graph)\n", "intent": "Before starting the traning, we setup summary statistics to be visualized in TensorBoard.\n"}
{"snippet": "history = model.fit(train_data,labels,epochs=150,callbacks=[PlotLossesKeras(dynamic_x_axis=False)])\n", "intent": "Let's go ahead and train the model, and look at how the loss and accuracy change with time.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_matrix = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n    return tf.nn.embedding_lookup(embedding_matrix, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "hidden_layer = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\nhidden_layer = tf.nn.relu(hidden_layer)\noutput = tf.add(tf.matmul(hidden_layer, weights['out']), biases['out'])\n", "intent": "A ReLu activation function will be used on the hidden layer before it is connected to the output. Each layer will implement a linear function `wx+b`\n"}
{"snippet": "from sklearn import neighbors\nfrom sklearn import preprocessing\nX_scaled = preprocessing.scale(X) \nclf1 = neighbors.KNeighborsRegressor(5)\ntrain_X = X_scaled[:half]\ntest_X = X_scaled[half:]\nclf1.fit(train_X,train_Y)\n", "intent": "** *k*-Nearest Neighbor (KNN) Regression **\n"}
{"snippet": "from sklearn.tree import DecisionTreeRegressor\ntree_carseats = DecisionTreeRegressor(min_samples_split = 10, min_samples_leaf = 5)\ntree_carseats.fit(train_x, train_y)\n", "intent": "b) Fit a regression tree to the training set. Plot the tree, and interpret the results. What test error rate do you obtain ?\n"}
{"snippet": "with pm.Model() as logistic_model:\n    pm.glm.GLM.from_formula('income_more_50K ~ age + educ', data, family=pm.glm.families.Binomial())\n    map_estimate = pm.find_MAP()\n    print(map_estimate)\n", "intent": "Sumbit MAP estimations of corresponding coefficients:\n"}
{"snippet": "xgb_model.save_model('Pipeline.model')\n", "intent": "RMSE for XGBoost is 0.857923\nTrain R-squared for XGBoost is 0.507406\n"}
{"snippet": "model = 'Logistic Regression A'\nclf_lgra = LogisticRegression(random_state=123)\ngs_params = {'C': [.01, 0.1, 1.0, 10], 'solver': ['liblinear', 'lbfgs']}\ngs_score = 'roc_auc'\nclf_lgra, pred_lgra = bin_classify(model, clf_lgra, features_extr, params=gs_params, score=gs_score)\nprint('\\nBest Parameters:\\n',clf_lgra)\nmetrics_lgra, roc_lgra, prc_lgra = bin_class_metrics(model, y_test, pred_lgra.y_pred, pred_lgra.y_score, print_out=True, plot_out=True)\n", "intent": "Engines in the above charts represent the queue or number of engines to be maintain per period, i.e. maintenance capacity.\n"}
{"snippet": "clf = GradientBoostingClassifier(verbose=True)\nclf.fit(X_train, y_train)\nprint('Train Score:', round(clf.score(X_train,y_train),3))\nprint('Test Score:', round(clf.score(X_test,y_test),3))\n", "intent": "Train and tune a single model.\n"}
{"snippet": "param_test1 = {'n_estimators':range(20,81,10)}\ngsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, \n                                                                min_samples_split=500,\n                                                               min_samples_leaf=50,\n                                                               max_depth=8,\n                                                               max_features='sqrt',\n                                                               subsample=0.8,\n                                                               random_state=2007), \nparam_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch1.fit(X_train,y_train)\n", "intent": "now lets try some hyperparameter tunning.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1024, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=500, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_normal((vocab_size, embed_dim), -1, 1), name=\"embedding\")\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from nltk.classify.scikitlearn import SklearnClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nbernoulli = SklearnClassifier(BernoulliNB())\nbernoulli.train(trainData)\nprint nltk.classify.accuracy(bernoulli, testData)\n", "intent": "Nltk also has functions that allow us to call other machine learning libraries, including scikit-learn, using wrapper classes.\n"}
{"snippet": "param_grid = {'alpha': np.logspace(-3, 3, 13)}\ngrid = GridSearchCV(lm.Ridge(), param_grid, cv=10,return_train_score=True)\ngrid.fit(x_train, y_train)\n", "intent": "**Ridge regression with Gridsearch**\n"}
{"snippet": "param_grid = {'alpha': np.logspace(-3, 3, 13)}\ngrid = GridSearchCV(lm.Ridge(), param_grid, cv=10,return_train_score=True)\ngrid.fit(x_train, y_train)\n", "intent": "Tuning parameters for Ridge Regression:\n"}
{"snippet": "param_grid = {'alpha': np.logspace(-3, 3, 13)}\ngrid = GridSearchCV(lm.Lasso(), param_grid, cv=10,return_train_score=True)\ngrid.fit(x_train, y_train)\n", "intent": "Tuning parameters for Lasso Regression:\n"}
{"snippet": "param_grid = {'alpha': np.logspace(-3, 3, 13)}\ngrid = GridSearchCV(lm.ElasticNet(), param_grid, cv=10,return_train_score=True)\ngrid.fit(x_train, y_train)\n", "intent": "Tuning parameters for Elastic Nets:\n"}
{"snippet": "logit_1 = LogisticRegression()\nlogit_1.fit(X_train_1, y_train_1)\n", "intent": "**default model settings:**\n"}
{"snippet": "train_score = []\nvalid_score = []\noob_score = []\nfor trees in tqdm(tree_sizes, desc='Training Models', unit='Model'):\n    clf = bagging_classifier(trees, 0.2, 1.0, clf_parameters)\n    clf.fit(X_train, y_train)\n    train_score.append(clf.score(X_train, y_train.values))\n    valid_score.append(clf.score(X_valid, y_valid.values))\n    oob_score.append(clf.oob_score_)\n", "intent": "With the bagging classifier built, lets train a new model and look at the results.\n"}
{"snippet": "from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(4,4,4),max_iter=50000)\n", "intent": "- Finally, we will build an MLP!\n"}
{"snippet": "mlp2 = MLPClassifier(hidden_layer_sizes=(8,8,8),max_iter=50000)\n", "intent": "- Let's try a different configuration.\n"}
{"snippet": "from sklearn import ensemble\ntask = oml.tasks.get_task(3954)\nclf = ensemble.RandomForestClassifier()\nflow = oml.flows.sklearn_to_flow(clf)\nrun = oml.runs.run_flow_on_task(task, flow)\n", "intent": "We can run (many) scikit-learn algorithms on (many) OpenML tasks.\n"}
{"snippet": "from sklearn.svm import LinearSVC\nsvm = SklearnClassifier(LinearSVC())\nsvm.train(trainData)\nprint nltk.classify.accuracy(svm, testData)\n", "intent": "We can use any of the learning algorithms implemented by scikit-learn ([decision trees](http://scikit-learn.org/stable/modules/tree.html\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\nnames = [\"Gradient Boosting 1 tree\", \"Gradient Boosting 3 trees\", \"Gradient Boosting 100 trees\"]\nclassifiers = [\n    GradientBoostingClassifier(n_estimators=1, random_state=0, learning_rate=0.5),\n    GradientBoostingClassifier(n_estimators=3, random_state=0, learning_rate=0.5),\n    GradientBoostingClassifier(n_estimators=100, random_state=0, learning_rate=0.5)\n    ]\nmorelearn.plots.plot_classifiers(names, classifiers, figuresize=(20,8))\n", "intent": "Each tree provides good predictions on part of the data, use voting for final prediction\n* Soft voting for classification, mean values for regression\n"}
{"snippet": "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\ngbrt.fit(X_train, y_train)\nplot_feature_importances_cancer(gbrt)\n", "intent": "Gradient boosting machines use much simpler trees\n- Hence, tends to completely ignore some of the features\n"}
{"snippet": "mglearn.plots.plot_svm_linear()\n", "intent": "SVM result. The circled samples are support vectors, together with their coefficients.\n"}
{"snippet": "svc = SVC()\nsvc.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n        svc.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\n", "intent": "Much better results, but they can still be tuned further\n"}
{"snippet": "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=1)\ngrid.fit(X_train, y_train)\n", "intent": "* We don't know the optimal polynomial degree or alpha value, so we use a grid search (or random search) to find the optimal values\n"}
{"snippet": "score = Ridge().fit(X_train_log, y_train).score(X_test_log, y_test)\nprint(\"Test score: {:.3f}\".format(score))\n", "intent": "And our Ridge regressor now performs a lot better.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nX_test_selected = select.transform(X_test)\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nprint(\"Score with all features: {:.3f}\".format(lr.score(X_test, y_test)))\nlr.fit(X_train_selected, y_train)\nprint(\"Score with only selected features: {:.3f}\".format(\n        lr.score(X_test_selected, y_test)))\n", "intent": "As usual, we need to check how the transformation affects the performance of our learning algorithms.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Please note that the graph visualizations only work in Chrome :(\n"}
{"snippet": "def reset_model():\n    net = Net()\n    net = net.cuda()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n    return net,criterion,optimizer\n", "intent": "<b>The below function is called to reinitialize the weights of the network and define the required loss criterion and the optimizer.</b> \n"}
{"snippet": "X = iris[[\"petal_length\"]]\ny = iris[\"petal_width\"]\nmodel = linear_model.LinearRegression()\nresults = model.fit(X, y)\nprint results.intercept_, results.coef_\n", "intent": "Now let's use scikit-learn to find the best fit line.\n"}
{"snippet": "def reset_model(is_teacher = True):\n    if is_teacher == True:\n        net = Teacher()\n    else:\n        net = Student()\n    net = net.cuda()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n    return net,criterion,optimizer\n", "intent": "<b>The below function is called to reinitialize the weights of the network and define the required loss criterion and the optimizer.</b> \n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 1)\n", "intent": "1. Import class you plan to use \n2. make an instance of the model in scikit learn \n   tune the model parameters \n3. Fit the model with the data \n"}
{"snippet": "odd_ratio_lm = lmodel.fit().params.apply(np.exp)\nprint (odd_ratio_lm)\n", "intent": "hint 1: np.exp(X)\nhint 2: conf['OR'] = params\n           conf.columns = ['2.5%', '97.5%', 'OR']\n"}
{"snippet": "model = ExtraTreesClassifier()\n", "intent": "Imported library, now let's create our model making a call in the library.\n"}
{"snippet": "model = DecisionTreeClassifier()\n", "intent": "After that, let's instance out model calling the method Classifier.\n"}
{"snippet": "middle = 30\nw_1 = tf.Variable(tf.truncated_normal([784, middle]))\nb_1 = tf.Variable(tf.truncated_normal([1, middle]))\nw_2 = tf.Variable(tf.truncated_normal([middle, 10]))\nb_2 = tf.Variable(tf.truncated_normal([1, 10]))\n", "intent": "Now we fill the weights and biases. \n"}
{"snippet": "rfc = RandomForestClassifier(n_estimators=100, min_samples_leaf=10, random_state=1, n_jobs=2)\ngbc = GradientBoostingClassifier()\netc = ExtraTreesClassifier()\nabc = AdaBoostClassifier()\nsvc = SVC()\nknc = KNeighborsClassifier()\ndtc = DecisionTreeClassifier()\nptc = Perceptron()\nlrc = LogisticRegression()\n", "intent": "Now, we'll instance our objects with the classifiers. \n"}
{"snippet": "rfc.fit(X_train, Y_train)\ngbc.fit(X_train, Y_train)\netc.fit(X_train, Y_train)\nabc.fit(X_train, Y_train)\nsvc.fit(X_train, Y_train)\nknc.fit(X_train, Y_train)\ndtc.fit(X_train, Y_train)\nptc.fit(X_train, Y_train)\nlrc.fit(X_train, Y_train)\n", "intent": "With the training sets, we'll fit all models for each classifier.\n"}
{"snippet": "lasso_baseline = get_baseline_results(X_train, X_test, Y_train, Y_test, Lasso(), 'Lasso')\nridge_baseline = get_baseline_results(X_train, X_test, Y_train, Y_test, Ridge(), 'Ridge')\nelastic_net_baseline = get_baseline_results(X_train, X_test, Y_train, Y_test, ElasticNet(), 'ElasticNet')\nxgboost_baseline = get_baseline_results(X_train, X_test, Y_train, Y_test, XGBRegressor(), 'XGBoost')\n", "intent": "First at all lets get the results of our baseline models. \n"}
{"snippet": "model = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n", "intent": "This means that our best fit line is:\n$$y = a + b x$$\nwhere $a = -0.363075521319$ and $b = 0.41575542$.\nNext let's use `statsmodels`.\n"}
{"snippet": "import eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())\n", "intent": "Now use the following cell to create the materials for the doctors.\n"}
{"snippet": "import eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())\n", "intent": "Here is how to calculate and show importances with the [eli5](https://eli5.readthedocs.io/en/latest/) library:\n"}
{"snippet": "from tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\nfashion_model  = Sequential()\nq_1.check()\n", "intent": "Create a `Sequential` model called `fashion_model`. Don't add layers yet.\n"}
{"snippet": "import statsmodels.formula.api as smf\nlm = smf.ols(formula='Sales ~ TV', data=data).fit()\nlm2 = smf.ols(formula='Sales ~ Radio', data=data).fit()\nlm.params\nlm2.params\nlm2.summary()\n", "intent": "Let's use **Statsmodels** to estimate the model coefficients for the advertising data:\n"}
{"snippet": "aa = linear_model.Ridge( copy_X=True, fit_intercept=False, max_iter=None,\n      normalize=False, random_state=0, solver='auto',alpha=6.0,tol=1.0\n                                   ).fit(df_train_data, df_train_target)\n", "intent": "df2_train = df2_train.values\nresult_last_var = svc_last.predict(df2_train)\nresult_last = pd.DataFrame({'Id':df2['Id'],'SalePrice':result_last_var})\n"}
{"snippet": "def create_baseline_model():\n    model = Sequential()\n    model.add(Dense(60, input_dim=60, init='normal', activation='relu'))\n    model.add(Dense(1, init='normal', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\nestimator = KerasClassifier(build_fn=create_baseline_model, nb_epoch=100, batch_size=5, verbose=0)\n", "intent": "Create baseline model\n"}
{"snippet": "def create_nn():\n    model = Sequential()\n    model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n    model.add(Dense(8, init='uniform', activation='relu'))\n    model.add(Dense(1, init='uniform', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n", "intent": "Define a neural network model\n"}
{"snippet": "model = create_nn()\nmodel.fit(X, Y, nb_epoch=150, batch_size=10, verbose=0)\n", "intent": "Fit and evaluate this model\n"}
{"snippet": "model = create_nn()\nfile_path = './models/c14/nn-best-model.h5'  \ncheckpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nmodel.fit(X_train, Y_train, validation_data=(X_val, Y_val), nb_epoch=150, batch_size=10, callbacks=[checkpoint], verbose=0)\n", "intent": "Create a model and a checkpoint instance. When the val_acc is improved, the weights will be saved.\n"}
{"snippet": "X = iris[[\"petal_length\", \"setosa\", \"versicolor\", \"virginica\"]]\nX = sm.add_constant(X) \ny = iris[\"petal_width\"]\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n", "intent": "Now we perform a multilinear regression with the dummy variables added.\n"}
{"snippet": "def get_dense_layers():\n    return [\n        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n        Flatten(),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(1000, activation='relu')\n        ]\n", "intent": "This is our usual Vgg network just covering the dense layers:\n"}
{"snippet": "def fit_model(model, batches, val_batches, nb_epoch=1):\n    model.fit_generator(batches, steps_per_epoch=steps_per_epoch, epochs=nb_epoch, \n                        validation_data=val_batches, validation_steps=validation_steps)\n", "intent": "We'll define a simple function for fitting models, just to save a little typing...\n"}
{"snippet": "fc_model.fit(trn_features, trn_labels, epochs=8, \n             batch_size=batch_size, validation_data=(val_features, val_labels))\n", "intent": "And fit the model in the usual way:\n"}
{"snippet": "user_in = Input(shape=(1,), dtype='int64', name='user_in')\nu = Embedding(input_dim=n_users, output_dim=n_factors, input_length=1, embeddings_regularizer=l2(1e-4))(user_in)\nmovie_in = Input(shape=(1,), dtype='int64', name='movie_in')\nm = Embedding(input_dim=n_movies, output_dim=n_factors, input_length=1, embeddings_regularizer=l2(1e-4))(movie_in)\n", "intent": "The most basic model is a dot product of a movie embedding and a user embedding. Let's see how well that works:\n"}
{"snippet": "graph_in = Input ((vocab_size, 50))\nconvs = [ ] \nfor fsz in range (3, 6): \n    x = Conv1D(64, fsz, padding='same', activation=\"relu\")(graph_in)\n    x = MaxPooling1D()(x) \n    x = Flatten()(x) \n    convs.append(x)\nout = Concatenate(axis=-1)(convs) \ngraph = Model(graph_in, out) \n", "intent": "We use the functional API to create multiple conv layers of different sizes, and then concatenate them.\n"}
{"snippet": "model = Sequential ([\n    Embedding(vocab_size, 50, input_length=seq_len, weights=[emb]),\n    SpatialDropout1D(0.2),\n    Dropout (0.2),\n    graph,\n    Dropout (0.5),\n    Dense (100, activation=\"relu\"),\n    Dropout (0.7),\n    Dense (1, activation='sigmoid')\n    ])\n", "intent": "We then replace the conv/max-pool layer in our original CNN with the concatenated conv layers.\n"}
{"snippet": "model = Sequential([\n    Embedding(vocab_size, 32, input_length=seq_len, mask_zero=True,\n              embeddings_regularizer=l2(1e-6)),\n    SpatialDropout1D(0.2),\n    LSTM(100, implementation=2),\n    Dense(1, activation='sigmoid')])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "We haven't covered this bit yet!\n"}
{"snippet": "model=Sequential([\n        Embedding(vocab_size, n_fac, input_length=cs),\n        SimpleRNN(n_hidden, activation='relu', recurrent_initializer='identity'),\n        Dense(vocab_size, activation='softmax')\n    ])\n", "intent": "This is nearly exactly equivalent to the RNN we built ourselves in the previous section.\n"}
{"snippet": "model=Sequential([\n        SimpleRNN(n_hidden, return_sequences=True, input_shape=(cs, vocab_size),\n                  activation='relu', recurrent_initializer='identity'),\n        TimeDistributed(Dense(vocab_size, activation='softmax')),\n    ])\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam())\n", "intent": "This is the keras version of the theano model that we're about to create.\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\ntype(knn)\n", "intent": "**Step 2:** \"Instantiate\" the \"estimator\"\n- \"Estimator\" is scikit-learn's term for \"model\"\n- \"Instantiate\" means \"make an instance of\"\n"}
{"snippet": "model.fit([conv_feat, trn_sizes], trn_labels, batch_size=batch_size, epochs=3, \n             validation_data=([conv_val_feat, val_sizes], val_labels))\n", "intent": "And when we train the model, we have to provide all the input layers' data in an array.\n"}
{"snippet": "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, epochs=1, \n             validation_data=(conv_val_feat, val_labels))\n", "intent": "Now we can train the model as usual, with pre-computed augmented data.\n"}
{"snippet": "K.set_value(answer.optimizer.lr, 1e-2)\nhist=answer.fit(inps, answers_train, **parms, epochs=4, batch_size=32,\n           validation_data=(val_inps, answers_test))\n", "intent": "And it works extremely well\n"}
{"snippet": "K.set_value(answer.optimizer.lr, 5e-3)\nhist=answer.fit(inps, answers_train, **parms, epochs=8, batch_size=32,\n           validation_data=(val_inps, answers_test))\n", "intent": "Fitting this model can be tricky.\n"}
{"snippet": "def relu(x): return Activation('relu')(x)\ndef dropout(x, p): return Dropout(p)(x) if p else x\ndef bn(x): return BatchNormalization()(x)  \ndef relu_bn(x): return relu(bn(x))\n", "intent": "These components should all be familiar to you:\n* Relu activation\n* Dropout regularization\n* Batch-normalization\n"}
{"snippet": "def conv(x, nf, sz, wd, p):\n    x = Conv2D(nf, (sz, sz), kernel_initializer='he_uniform', padding='same',  \n                          kernel_regularizer=l2(wd))(x)\n    return dropout(x,p)\n", "intent": "Convolutional layer:\n* L2 Regularization\n* 'same' border mode returns same width/height\n* Pass output through Dropout\n"}
{"snippet": "rn_top = Model(model.input, mid_out.output)\nrn_top_avg = Sequential([rn_top, AveragePooling2D((7,7))])\n", "intent": "We put an average pooling layer on top to make it a more managable size.\n"}
{"snippet": "rn_bot_inp = Input(shp[1:])\nx = rn_bot_inp\nx = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\nx = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\nx = Flatten()(x)\nrn_bot = Model(rn_bot_inp, x)\nrn_bot.output_shape\n", "intent": "Our final layers match the original resnet, although we add on extra resnet block at the top as well.\n"}
{"snippet": "lm_inp = Input(shape=(2048,))\nlm = Model(lm_inp, Dense(ndim)(lm_inp))\n", "intent": "We add a linear model on top to predict our word vectors.\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train_dtm, y_train)\n", "intent": "Use Naive Bayes to predict the star rating for reviews in the testing set, and calculate the accuracy.\n"}
{"snippet": "img_nn = NearestNeighbors(3, metric='cosine', algorithm='brute').fit(pred_wv)\n", "intent": "Something very nice about this kind of model is we can go in the other direction as well - find images similar to a word or phrase!\n"}
{"snippet": "ft_model = Sequential([rn_top_avg, rn_bot_seq])\n", "intent": "Since that worked so well, let's try to find images with similar content to another image...\n"}
{"snippet": "def meanshift(data):\n    X = torch.FloatTensor(np.copy(data))\n    for it in range(5):\n        for i, x in enumerate(X):\n            dist = torch.sqrt((sub(x, X)**2).sum(1))\n            weight = gaussian(dist, 3).unsqueeze(1)  \n            num = mul(weight, X).sum(0).unsqueeze(0)  \n            X[i] = num / weight.sum()\n    return X\n", "intent": "And the implementation of meanshift is nearly identical too!\n"}
{"snippet": "K.set_value(m_sr.optimizer.lr, 1e-4)\nm_sr.fit([arr_lr, arr_hr], targ, 16, 100, **parms)\n", "intent": "We use learning rate annealing to get a better fit.\n"}
{"snippet": "inp = Input((288,288,3))\nref_model = Model(inp, ReflectionPadding2D((1,288,288,3), padding=(40,10))(inp))\nref_model.compile('adam', 'mse')\n", "intent": "Testing the reflection padding layer:\n"}
{"snippet": "outputs = net(Variable(images).cuda())\n_, predicted = torch.max(outputs.data, 1)\n' '.join('{}'.format([classes[predicted[j]] for j in range(4)]))\n", "intent": "Okay, now let us see what the neural network thinks these examples above are:\n"}
{"snippet": "correct,total = 0,0\nfor data in testloader:\n    images, labels = data\n    outputs = net(Variable(images).cuda())\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels.cuda()).sum()\nprint('Accuracy of the network on the 10000 test images: {} %%'.format(100 * correct / total))\n", "intent": "The results seem pretty good. Let us look at how the network performs on the whole dataset.\n"}
{"snippet": "def get_contin(feat):\n    name = feat[0][0]\n    inp = Input((1,), name=name+'_in')\n    return inp, Dense(1, name=name+'_d', kernel_initializer=my_init(1.))(inp)  \n", "intent": "Helper function for continuous inputs.\n"}
{"snippet": "def relu(x): return Activation('relu')(x)\ndef dropout(x, p): return Dropout(p)(x) if p else x\ndef bn(x): return BatchNormalization()(x)  \ndef relu_bn(x): return relu(bn(x))\ndef concat(xs): return concatenate(xs)  \n", "intent": "This should all be familiar.\n"}
{"snippet": "x = dataset['sex'].values\nn = dataset['total'].values\ny_obs = dataset['>50'].values\nwith pm.Model() as edu_A1:\n    sigma = pm.HalfCauchy('sigma',beta=2)\n    alpha = pm.Normal('alpha',mu=0,sd=10)\n    beta_m = pm.Normal('beta_m',mu=0,sd=1)\n    alpha_edu = pm.Normal('alpha_edu', mu=alpha, sd=sigma, shape=7)\n    p = pm.Deterministic('p', pm.math.invlogit(alpha_edu[np.arange(14)//2] + beta_m * x))\n    y = pm.Binomial('y_obs', p=p, n=n, observed=y_obs)\n", "intent": "**Answer:** Let's follow the HW10 first and build a similar model (edu_A1).\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=24, verbose=1)  \n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "scaler.fit(df.drop('Class', axis=1), df['Class'])\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "lm.fit(X_train, y_train) \n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlm = LogisticRegression()\nlm.fit(X_train, y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "gsc = GridSearchCV(SVC(), param_grid, refit=True,verbose=3)\ngsc.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "gbc_params = {'n_estimators':[100,500,1000],\n             'subsample':[0.3,0.7,1.0],\n             'min_samples_split':[2,4,6,8],\n             'max_depth':[3,7,10],\n             'max_features':['auto','sqrt','log2',None]\n             }\ngcv = GridSearchCV(gbc,gbc_params,cv=5)\ngcv.fit(Xtrain,ytrain)\nprint(gcv.score(Xtrain, ytrain))\nprint(gcv.best_params_)\n", "intent": "Gradient boosting seems to perform the best so I then tune the hyperparameters to increase performance even more.\n"}
{"snippet": "best_model.save_model(\"pubg_model.model\")\n", "intent": "The mean absolute error for the model tuned in xgboost is almost half versus the model made in scikit-learn.\n"}
{"snippet": "def softmax(sess, x):\n    kh = tf.cast(x, tf.float32)\n    return sess.run(tf.exp(kh) / tf.reduce_sum(tf.exp(kh)))\n", "intent": "$$\\text{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$$\n"}
{"snippet": "def softmax_matrix(sess, x):\n    kh = tf.cast(x, tf.float32)\n    return sess.run(tf.nn.softmax(kh))\n", "intent": "$$\\text{softmax}(X)_{ij} = \\frac{\\exp(X_{ij})}{\\sum_j \\exp(X_{ij})}$$\n"}
{"snippet": "x_shared = shared(x_train)\ny_shared = shared(y_train)\nlogistic_model = pm.Model()\nwith logistic_model:\n    betas = pm.Normal('betas', 0, tau=1/100 , shape=(D, ))\n    p = pm.math.invlogit(t.dot(x_shared, betas))\n    y_obs = pm.Bernoulli('y_obs', p=p, observed=y_shared)\n", "intent": "**Setting Up the Model in Pymc3**\n"}
{"snippet": "sess.close()\ntf.reset_default_graph()\nsess = tf.Session(config=config)\nwriter = tf.summary.FileWriter('log/')\n", "intent": "`tf.gradients(ys, xs)` Constructs symbolic partial derivatives of sum of ys w.r.t. x in xs.\n"}
{"snippet": "criterion = ClassNLLCriterion()\nnet = Sequential()\nnet.add(Linear(2, 4))\nnet.add(ReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\nprint(net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "input_X_reshaped = tf.reshape(input_X, shape=[-1, 1*28*28], \n                              name=\"reshape_X\")\nl1 = tf.layers.dense(input_X_reshaped, units=50, \n                     activation=tf.nn.sigmoid)\nl2 = tf.layers.dense(l1, units=10, activation=None)\nl_out = tf.nn.softmax(l2)\ny_predicted = tf.argmax(l2, axis=-1)\n", "intent": "Defining network architecture\n"}
{"snippet": "def softmax(sess, x):\n    <student.implement_softmax()>\n", "intent": "$$\\text{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$$\n"}
{"snippet": "def softmax_matrix(sess, x):\n    <student.implement_softmax()>\n", "intent": "$$\\text{softmax}(X)_{ij} = \\frac{\\exp(X_{ij})}{\\sum_j \\exp(X_{ij})}$$\n"}
{"snippet": "kmeans = KMeans(n_clusters=3, random_state=0)\nkmeans.fit(iris.data)\n", "intent": "Most algorithms have a `fit` method for training and then either a `transform` or a `predict` method that acts on new data.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n                batch_size=32,\n                epochs=10,\n                validation_data=(x_test, y_test),\n                verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "inputs = tf.placeholder(tf.float32, [None, n_states])\nweights = tf.Variable(tf.zeros([n_states,n_actions]))\noutputs = tf.matmul(inputs, weights)\ntargets = tf.placeholder(tf.float32, [None, n_actions])\nselected_actions = tf.argmax(outputs,1)\n", "intent": "Define Policy Network\n"}
{"snippet": "p_y_mean_test = sigmoid(beta_coef(x_test, beta_mean))\np_y_post_test = sigmoid(beta_coef(x_test, trace['betas'].T))\npost_pred_test = np.random.binomial(1, p_y_post_test)\np_y_pp = post_pred_test.mean(axis=1)\np_y_post_thresh = p_y_post_test[:, :]\np_y_post_thresh[p_y_post_thresh > 0.5] = 1\np_y_post_thresh[p_y_post_thresh <= 0.5] = 0\np_y_cdf = p_y_post_thresh.mean(axis=1)\n", "intent": "**II. Testing Data**\nWe perform the same analysis on the testing set.\n"}
{"snippet": "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\nhidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\nhidden_2 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\nhidden_3 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\noutput_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)\n", "intent": "Now that we've built a framework allowing us to define networks as a series of layers, we can easily build deeper networks.\n"}
{"snippet": "def sigmoid(x):\n    return 1.0/(1.0+np.exp(-x))\n", "intent": "$$ B = \\sigma(A) $$ or\n$$ b_1 = \\sigma(a_1) $$\n$$ b_2 = \\sigma(a_2) $$\n$$ b_3 = \\sigma(a_3) $$\n$$ b_4 = \\sigma(a_4) $$\n"}
{"snippet": "P = sigmoid(C)\narray_print(P)\n", "intent": "So we can simply code this up as:\n"}
{"snippet": "dPdC = sigmoid(C) * (1-sigmoid(C))\narray_print(dPdC)\n", "intent": "So, coding this up is simply:\n"}
{"snippet": "def setup_layers(hidden_neurons, outputs):\n    layers = []\n    for i in range(len(hidden_neurons)):\n        layer = FullyConnected(neurons=hidden_neurons[i], activation_function=sigmoid)\n        layers.append(layer)\n    output_layer = FullyConnected(neurons=outputs, activation_function=sigmoid)\n    layers.append(output_layer)\n    return layers\n", "intent": "First, a helper function to set up the layers of the neural net:\n"}
{"snippet": "def sigmoid(x, bprop=False):\n    if bprop:\n        return sigmoid(x) * (1-sigmoid(x))\n    else:\n        return 1.0/(1.0+np.exp(-x))\n", "intent": "We'll need to redefine our functions to have `bprop` option:\n"}
{"snippet": "def setup_layers(hidden_neurons, outputs, learning_rate=1.0):\n    layers = []\n    for i in range(len(hidden_neurons)):\n        layer = FullyConnectedLR(neurons=hidden_neurons[i], activation_function=sigmoid)\n        setattr(layer, \"learning_rate\", learning_rate)\n        layers.append(layer)\n    output_layer = FullyConnectedLR(neurons=outputs, activation_function=sigmoid)\n    setattr(output_layer, \"learning_rate\", learning_rate)\n    layers.append(output_layer)\n    return layers   \n", "intent": "We'll modify the a new `setup_layers` function to give each layer a learning rate: \n"}
{"snippet": "def _sigmoid(x):\n    return 1.0/(1.0+np.exp(-x))\n", "intent": "Refresher on the sigmoid\n"}
{"snippet": "B = _sigmoid(A)\n", "intent": "$$ B = \\sigma(A) $$ or\n$$ b_1 = \\sigma(a_1) $$\n$$ b_2 = \\sigma(a_2) $$\n$$ b_3 = \\sigma(a_3) $$\n$$ b_4 = \\sigma(a_4) $$\n"}
{"snippet": "MLR.fit()\n", "intent": "We defined our multinomial logistic model with Cross Entropy Loss above.  Let's train it now.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\nprint(model)\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "from sklearn import linear_model\nmodel_lr = linear_model.LogisticRegression()\n", "intent": "$$ \\frac{1}{2m} \\sum_i (h(x_i) - y_i)^2 \\text{ mit } h(x) = m*x + t$$\n$$  $$\n"}
{"snippet": "scaler.fit(df.drop(\"TARGET CLASS\", axis = 1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "                knn.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "logreg = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn.linear_model import Ridge\nmodel = make_pipeline(GaussianFeatures(30), \n                      Ridge(alpha=.05))\nbasis_plot(model, title='Ridge Regression')\n", "intent": "- Penalizes sum of squares (2-norms) of the model coefficients\n"}
{"snippet": "from sklearn.linear_model import Lasso\nmodel = make_pipeline(GaussianFeatures(30),\n                      Lasso(alpha=0.001))\nbasis_plot(model, title='Lasso Regression')\n", "intent": "- Penalizes sum of absolute values of regression coefficients\n- Favors sparse models where possible\n"}
{"snippet": "toy_sess = tf.Session()\n", "intent": "In order to evaluate `c` we have to provide values for the inputs ``a`` and ``b`` using feed_dict.\n"}
{"snippet": "x = tf.placeholder(\"float\")\ny = tf.placeholder(\"float\")\nop1 = tf.add(x, y)\nop2 = tf.multiply(x, y)\nuseless = tf.multiply(x, op1)\nop3 = tf.pow(op2, op1)\nwith tf.Session() as sess:\n    op3_value, not_useless_value = sess.run([op3, useless], feed_dict={x:2, y:3})\n    print(\"opt3 value: \",op3_value,  \"\\nnot_useless value\", not_useless_value)\n", "intent": "We can use symbolic inputs to define a single time the graph which we can use for as many values as we want\n"}
{"snippet": "mlrs = [MLR(lr=0.1, weight_decay=0.01, max_epoch=15).fit(trainloader, cvloader)]\n", "intent": "We construct a validation set by spliting 10% of data from training set.\n"}
{"snippet": "aux = tf.linspace(start=1., stop=10., num=5, name=None) \nwith tf.Session() as sess:\n    print(\"value in x: \", aux.eval())  \n", "intent": "- `tf.linspace`\n- `tf.range`\n"}
{"snippet": "x = tf.Variable(3, name=\"x\")\ny = tf.Variable(4, name=\"y\")\nf = x * y + y + 1\nsess = tf.Session()\nsess.run(x.initializer)\nsess.run(y.initializer)\nresult = sess.run(f)\nprint(result)\n", "intent": "If we initialize the variables the code will work as expected\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    result = sess.run(f)\n    print(result)\n", "intent": "Initializing all variables with `tf.global_variables_initializer()`\n"}
{"snippet": "W = tf.Variable(tf.truncated_normal([700, 10]))\nwith tf.Session() as sess:\n    sess.run(W.initializer)\n    print(W.eval())\n", "intent": "Recall that `var.eval()` is the same as `sess.run(W)`\n"}
{"snippet": "W = tf.Variable(3.14)\nW.assign(10)\nwith tf.Session() as sess:\n    sess.run(W.initializer)   \n    print(W.eval())\n", "intent": "This function allows to initialize and assign a value to a variable.\n"}
{"snippet": "with tf.Session() as sess:\n    s= saver.restore(sess, \"./saved_tests/my_save_test_begin.cpkt\")\n    sess.graph\n    print(sess.run(x))\n    print(sess.run(y))\n", "intent": "Restoring a model is as easy as calling saver.restore()\nAfter a session is closed, we can open a new session and use `save.restore\nis executed\n"}
{"snippet": "import tensorflow as tf\nimport numpy as np\nph = tf.placeholder(shape=[None,None], dtype=tf.int32)\nx = tf.slice(ph, [0, 0], [-1, 2])\ninput_ = np.array([[ 1, 2, 3, 4, 5, 6, 7, 8],\n                   [11,21,31,41,51,61,71,81],\n                   [11,21,31,41,51,61,71,81]])\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(x, feed_dict={ph: input_}))\n", "intent": "- `tf.slice` allow us to get an slice (or subset) of a variable.\nWe can use `tf.slice` on a placeholder\n"}
{"snippet": "tf.reset_default_graph()\na = tf.Variable(1, name='a')\npprint(tf.get_default_graph().get_operations())\nprint(len(tf.get_default_graph().get_operations()))\n", "intent": "This example adds nodes in the graph (see it ends up with 19 nodes)\n"}
{"snippet": "tf.reset_default_graph()\na = tf.Variable(1, name='a')\nb = tf.Variable(2, name='b')\ncompute_addition_a_b = tf.add(a, b)\n", "intent": "This example does not add nodes in the graph (well, only one, see it ends up with 10 nodes).\n"}
{"snippet": "lrs = [0.1, 0.01]\nbatch_sizes = [20, 50, 100, 200]\nhidden_dims = [25, 50, 100]\nmlps = [[[MLPClassifier(lr=lr, batch_size=b, hidden_dim=h).fit(trainset, verbose=False) \\\n         for h in hidden_dims] for b in batch_sizes] for lr in lrs]\n", "intent": "After a few trials, we decide to set `max_epoch` to 12.\n"}
{"snippet": "logistic = LogisticRegression(random_state=seed)\n", "intent": "instantiate the random search and fit it\n"}
{"snippet": "rf = RandomForestClassifier()\n", "intent": "instantiate the random search and fit it\n"}
{"snippet": "svm =svm.SVC(probability=True)\n", "intent": "instantiate the random search and fit it\n"}
{"snippet": "criterion = ClassNLLCriterion()\nnet = Sequential()\nnet.add(Linear(2, 4))\nnet.add(ReLU())\nnet.add(Dropout(p=0.95))\nnet.add(Linear(4, 4))\nnet.add(ReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\nprint(net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "test_scores = []\nfor feature in dataset_2.columns:\n    results = run_model(Lasso(alpha=100), 'variable ranking', 50, \n                        dataset_2[[feature]], target_2)\n    test_score = results['r2_test_score']\n    if test_score > 0.2:\n        test_scores.append({'feature': feature, 'score' : test_score})\n", "intent": "    run_model(model, model_name, n_pcnt, data, labels)\n"}
{"snippet": "results = run_model(Lasso(alpha=100), 'lasso', 100, dataset_2, target_2)\n", "intent": "Fit the full model in order to find the largest coefficients.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr_model = LogisticRegression()\n", "intent": "We import the `LogisticRegression` class from the `sklearn.linear_model` library and instantiate the `lr_model` object.\n"}
{"snippet": "cluster_model = KMeans(n_clusters=3)\n", "intent": "After importing the model, you instantiate `cluster_model`, an object of class `KMeans` with `n_clusters` set to 3.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)    \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "lams = [0.01, 0.001, 0.0001, 0]\nmlps2 = [MLPClassifier(lr=mlp_best.lr, batch_size=mlp_best.batch_size, \\\n                       hidden_dim=mlp_best.hidden_dim, weight_decay=l)\\\n         .fit(trainset, verbose=False) for l in lams]\n", "intent": "We use the best parameters chosen from the previous part and tune $\\lambda$ only.\n"}
{"snippet": "from distutils.version import LooseVersion\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nassert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\nprint('TensorFlow Version: {}'.format(tf.__version__))\n", "intent": "This will check to make sure that you have correct version of TensorFlow\n"}
{"snippet": "rf = RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=17) \nrf.fit(X_train, y_train)\n", "intent": "Train a random forest **(RandomForestClassifier)**. Set the number of trees to 100 and use **random_state = 17**.\n"}
{"snippet": "forest_params = {'max_depth': range(10, 21),\n                'max_features': range(5, 105, 20)}\nlocally_best_forest = GridSearchCV(rf,forest_params,n_jobs=-1,cv=2,verbose=True)\nlocally_best_forest.fit(X_train, y_train) \n", "intent": "Train a random forest **(RandomForestClassifier)**. Tune the maximum depth and maximum number of features for each tree using **GridSearchCV**. \n"}
{"snippet": "alice_top_sites = pd.Series(train_df[train_df['target'] == 1][sites].values.flatten()).value_counts().sort_values(ascending = False).head(5)\nprint(alice_top_sites)\nsites_dict.loc[alice_top_sites.index.tolist()]\n", "intent": "- videohostings\n- social networks\n- torrent trackers\n- news\n"}
{"snippet": "lr = linear_model.LinearRegression()\nsales = sales.dropna(how='any') \ndf = sales[[\"2015 Volume Sold (Liters)\", \"2015 Sales Q1\", \"2015 Margin mean\"]]\ny = sales['2016 Sales Q1'][df['2015 Sales Q1'] > 8000]\n", "intent": "Make some plots, look at correlations, etc.\n"}
{"snippet": "mean_knn_n1 = KNeighborsClassifier(n_neighbors=1,\n                              weights='uniform',\n                              p=2,\n                              metric='minkowski')\naccuracy_crossvalidator(X, Y, mean_knn_n1, cv_indices)\n", "intent": "As you can see the mean cross-validated accuracy is very high with 5 neighbors. \nLet's see what it's like when we use only 1 neighbor:\n"}
{"snippet": "from sklearn.manifold import TSNE\ntsne = TSNE()\ntsne.fit(X)\n", "intent": "TSNE is good to preserve local structure, but not global structure\n"}
{"snippet": "from sklearn.svm import SVC \nmodelSVMLinear = SVC(kernel=\"linear\")\nmodelSVMLinear.fit(XTrain,yTrain)\n", "intent": "<div class=\"exo\"> <b>Question:</b> Use Scikit-Learn to perform a first classification and evaluate the obtained performance.\n</div>\n"}
{"snippet": "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)\n", "intent": "Let's now evaluate the model using coherence score.\n"}
{"snippet": "mnist_MLP= Artificial_Neural_Network(MLP,reg_rate=0.001,learning_rate=0.1,batch_size=100, hidden=100)\nmnist_MLP.fit()\n", "intent": "**It looks like a learning rate of 0.1 is pretty clearly optimal.  As noted earlier, batch size has less effect, but let's use a size of 100 anyway**\n"}
{"snippet": "param_dist = {\"max_depth\": ,\n              \"max_features\":,\n             }\nclf = RandomForestClassifier(n_jobs=-1, n_estimators = 10)\nn_iter_search = 7\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   cv=4,\n                                   n_iter=n_iter_search)\nrandom_search.fit(X, y)\n", "intent": "Your turn: Define feature space\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nlogreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\nlr = LogisticRegression()\nclf = GridSearchCV(lr, logreg_parameters, cv = 5)\nclf.fit(X_train,y_train)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nkn_parameters = {\n    'n_neighbors':list(range(1,51)),\n    'weights':['uniform', 'distance'],\n    'leaf_size':list(range(20,51))\n}\nclf_k = GridSearchCV(KNeighborsClassifier(), kn_parameters, cv = 5)\nclf_k.fit(X_train,y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "logreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\nlr = LogisticRegression()\nclf_pr = GridSearchCV(lr, logreg_parameters, cv = 5, scoring='average_precision')\nclf_pr.fit(X_train, y_train)\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\n", "intent": "Best results out of the three simple classifiers I tried.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=100, verbose=2, validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train , y_train)\n", "intent": "Standard Linear Regression Model\n"}
{"snippet": "model.fit(X_train , y_train,\n          cat_features=categorical_features,\n          eval_set = (X_test , y_test),\n          plot = True)\n", "intent": "now let's fit our model to our training data and plot the results to visualise the increase of accuracy\n"}
{"snippet": "hist = model.fit(x_train, y_train, epochs=20, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "mlps = []\naccuracies = []\nlambdas = [0.001, 0.01, .1, .5]\nfor lambda_i in lambdas: \n    print(\"MNIST MLP -- Lambda {}\".format(lambda_i))\n    mnist_MLP= Artificial_Neural_Network(MLP,reg_rate=lambda_i,learning_rate=0.1,batch_size=100, hidden=100)\n    mnist_MLP.fit(show_validation=False)\n    accuracies.append(mnist_MLP.get_params(\"validation_scores\"))\n    mnist_MLP.score(dataset=\"Validation\", save_misclassified=True) \n    mlps.append(mnist_MLP)\n", "intent": "**Great.  Test set accuracy of ~96%.  Let's try some different regularization rates.**\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dropout,Dense,Flatten\nmodel = Sequential()\nmodel.add(Dense(512,activation='relu',input_shape=(1000,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train,y_train,epochs=10,batch_size=10,verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def sigmoid(v):\n    a=np.exp(-v)\n    b=1+a\n    yhat=1/b\n    return yhat\n", "intent": "This function takes input as vector and outputs sigmoid of every element of that vector. Here v is vector.\n"}
{"snippet": "def gradient_ascent(Xs,y,learning_rate,beta,m,num_iters):\n    for i in range(num_iters):\n        yhat=sigmoid(np.dot(Xs,beta))\n        temp = beta + (learning_rate/m) * -(np.dot(Xs.transpose(), yhat-y))\n        beta = temp\n        if(i%100==0):\n            cost= compute_cost(Xs,y,beta,m)\n            print(\"Cost\")\n            print(cost)\n    return beta      \n", "intent": "Write the code to perform Gradient Descent in the function below.\n"}
{"snippet": "W1 = tf.get_variable(\"W1\", [30, 30], initializer = tf.random_normal_initializer)\n", "intent": "Remember that the parameters/weights have to be Tensorflow Variables.<br>\nWe should not initialize W as zeros. We can initialize b as 0.\n"}
{"snippet": "theta = tf.Variable([a_init, b_init])\nsess.run(tf.global_variables_initializer())\n", "intent": "$$\\theta = \\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}$$\n"}
{"snippet": "def inception(x, reuse):\n    preprocessed = tf.multiply(tf.subtract(x, 0.5), 2.0)\n    arg_scope = nets.inception.inception_v3_arg_scope(weight_decay=0.0)\n    with slim.arg_scope(arg_scope):\n        logits, _ = nets.inception.inception_v3(\n            preprocessed, 1001, is_training=False, reuse=reuse)\n        logits = logits[:,1:] \n        probs = tf.nn.softmax(logits) \n    return logits, probs\nlogits, probs = inception(tf.expand_dims(x, 0), reuse=False)\n", "intent": "Next, we load the Inception v3 model.\n"}
{"snippet": "utils.plot_model(x1, y_hat)\n", "intent": "<font color=\"1874CD\"> **A line.**\n"}
{"snippet": "utils.plot_model(x2, y_hat-w[1]*x1)\n", "intent": "Using the cell below, plot `y_hat-w[1]*x1` as a function of `x2`.\n"}
{"snippet": "m1 = Model(dfs[1]).run()\n", "intent": "We can test the model on one of the restaurant we chose.\n"}
{"snippet": "utils.plot_model(x1, y_hat)\n", "intent": "The cell below will plot the input values against the output values for your model.\n"}
{"snippet": "x = torch.Tensor(5,3)\nx\n", "intent": "Construct a 5x3 matrix, uninitialized:\n"}
{"snippet": "for epoch in range(epochs):\n  hidden_activation = sigmoid(np.dot(X, Theta_hidden))\n  y_hat = np.dot(hidden_activation, Theta_output)\n  loss = criterion(y_hat, y)\n  delta_output = learning_rate * loss\n  Theta_output += hidden_activation.T.dot(delta_output)\n  delta_hidden = delta_output.dot(Theta_output.T) * sigmoid_prime(hidden_activation)\n  Theta_hidden += X.T.dot(delta_hidden)\n  if epoch % 5000 == 0:\n    print(f'epoch {epoch}: error {np.mean(loss)}')\n", "intent": "We can now train our model via the Backpropagation algorihm. It is time to use those derivates:\n"}
{"snippet": "X = Variable(torch.Tensor([[1.0], [2.0], [3.0]]))\nY = Variable(torch.Tensor([[2.0], [4.0], [6.0]]))\n", "intent": "We'll have to wrap the data in PyTorch variables & Tensors:\n"}
{"snippet": "for epoch in range(10):\n    y_pred = model(X)\n    loss = criterion(y_pred, Y)\n    print(\"epoch:\", epoch, loss.data[0])\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n", "intent": "Training using PyTorch is much simpler:\n"}
{"snippet": "hour_var = Variable(torch.Tensor([[4.0]]))\ny_pred = model(hour_var)\nprint(\"predict (after training)\",  4, model(hour_var).data[0][0])\n", "intent": "Let's see what model predicts:\n"}
{"snippet": "with pm.Model() as hierarchical_model:\n    m = pm.Categorical('m', np.asarray([.5, .5]))\n    kappa = 12\n    omega = pm.math.switch(eq(m, 0), .25, .75)\n    theta = pm.Beta('theta', omega*(kappa)+1, (1-omega)*(kappa)+1)\n    y = pm.Bernoulli('y', theta, observed=[1,1,1,1,1,1,0,0,0])    \n", "intent": "Coin is flipped nine times, resulting in six heads.\n"}
{"snippet": "with pm.Model() as hierarchical_model2:\n    m = pm.Categorical('m', np.asarray([.5, .5]))\n    omega_0 = .25\n    kappa_0 = 12\n    theta_0 = pm.Beta('theta_0', omega_0*(kappa_0)+1, (1-omega_0)*(kappa_0)+1)\n    omega_1 = .75\n    kappa_1 = 12\n    theta_1 = pm.Beta('theta_1', omega_1*(kappa_1)+1, (1-omega_1)*(kappa_1)+1)\n    theta = pm.math.switch(eq(m, 0), theta_0, theta_1)\n    y2 = pm.Bernoulli('y2', theta, observed=[1,1,1,1,1,0,0,0])    \n", "intent": "Coin is flipped nine times, resulting in six heads.\n"}
{"snippet": "lda = LDA(n_components=2)\nlda.fit(wine_train, wine_train_labels)\nwine_train_lda = lda.transform(wine_train)\nwine_test_lda = lda.transform(wine_test)\n", "intent": "Fit an LDA model to the training data, and transform the test data onto 2 LDA components\n"}
{"snippet": "model = neighbors.KNeighborsClassifier(n_neighbors = 1).\\\n    fit(X, c)\n", "intent": "- (http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed=tf.nn.embedding_lookup(embedding,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "mod=clf.fit(X,y)\n", "intent": "Regularized Cost Function, l1 norm = $-\\sum_{1}^{n}[ y_i log(p_i)+(1-y_i)log(1-p_i)]+\\frac{1}{C}|\\beta|$, here $\\frac{1}{C}=\\alpha$\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n", "intent": "Now that we have a nice vector representation of all sample documents, lets apply a classifier. \n"}
{"snippet": "input_bias = tf.Variable(tf.zeros(shape=(*input_nodes,)), name='input_bias')\n", "intent": "We use the same pattern for the biases.\n"}
{"snippet": "input_layer = tf.nn.relu(pre_activation, name='input_layer_output')\n", "intent": "Finally we can form the layer by squeezing the output through a rectified linear unit.\n"}
{"snippet": "h_w = tf.Variable(tf.random_normal((*input_nodes, *hidden_nodes)), name='hidden_weights')\nh_b = tf.Variable(tf.zeros((*hidden_nodes, ), name='hidden_bias'))\nhidden_layer = tf.nn.relu(tf.add(tf.matmul(input_layer, h_w), h_b))\n", "intent": "We create a hidden layer using the same logic.\n"}
{"snippet": "o_w = tf.Variable(tf.random_normal((*hidden_nodes, *output_shape)), name='output_weights')\no_b = tf.Variable(tf.zeros((*output_shape, ), name='output_bias'))\noutput = tf.add(tf.matmul(hidden_layer, o_w), o_b)\n", "intent": "The output layer has no activation function (aka a linear activation function).  This allows the network to output negative values.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n", "intent": "Logistic regresssion\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg_bal = LogisticRegression(class_weight='balanced')\nlogreg_bal.fit(X_train, y_train)\n", "intent": "balanced logistic regresion\n"}
{"snippet": "X = df[ ['Income', 'Cards', 'Age', 'Education', 'Gender_Male', 'Ethnicity_Asian', 'Ethnicity_Caucasian'] ]\ny = df.Balance\nmodel = linear_model.LinearRegression()\nmodel.fit(X,y)\nprint model.intercept_\nprint model.coef_\n", "intent": "First, find the coefficients of your regression line.\n"}
{"snippet": "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\nparam_grid = dict(optimizer=optimizer)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_log_loss')\ngrid_result = grid.fit(X, y)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n", "intent": "build fn create model is called via an optimizer parameter. \n"}
{"snippet": "model =create_model(1000,100,30)\n", "intent": "create  the model and train and validate on valiation data save the best weights \nby watching the validation logarthmic loss ,monitoring\n"}
{"snippet": "model = ks.models.Sequential()\nmodel.add(ks.layers.Conv2D(32, (3,3), activation = \"relu\", input_shape=(150,150,3)))\nmodel.add(ks.layers.MaxPooling2D(2,2))\nmodel.add(ks.layers.Conv2D(64, (3,3), activation = \"relu\"))\nmodel.add(ks.layers.MaxPooling2D(2,2))\nmodel.add(ks.layers.Flatten()) \nmodel.add(ks.layers.Dropout(0.5))\nmodel.add(ks.layers.Dense(512,activation=\"relu\"))\nmodel.add(ks.layers.Dense(1,activation=\"sigmoid\"))\n", "intent": "<h4>Define the network</h4>\n"}
{"snippet": "scaler.fit(X_train)\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "with tf.Session() as sess:\n    saver = tf.train.Saver()   \n    saver.restore(sess, './model/model.checkpoint')\n    test_predicted = sess.run(predict, feed_dict=test_feed_dict)\n    test_accuracy = nn.calculate_accuracy(test_orders, test_predicted)\ntest_accuracy\n", "intent": "The following code loads your model, computes accuracy, and exports the result. **DO NOT** change this code.\n"}
{"snippet": "with tf.Graph().as_default():\n    x = tf.get_variable(\"param\", [])  \n    loss = -tf.log(tf.sigmoid(x))  \n    optim = tf.train.AdamOptimizer(learning_rate=0.1)\n    min_op = optim.minimize(loss)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(x.assign(1.5))\n        for i in range(10):\n            print(sess.run([min_op, loss], {})[1])\n", "intent": "- Adadelta\n- **Adam**\n"}
{"snippet": "with tf.Graph().as_default():\n    x = tf.placeholder(tf.float32, [None], \"input\")\n    x_dropped = tf.nn.dropout(x, 0.7)  \n    with tf.Session() as sess:        \n        print(sess.run(x_dropped, {x: np.random.rand(6)}))\n", "intent": "<img src=\"http://everglory99.github.io/Intro_DL_TCC/intro_dl_images/dropout1.png\" width=1000></img>\nFor RNNs: `tf.nn.rnn_cell.DropoutWrapper`\n"}
{"snippet": "def plot_top_k(alpha, label='pos', k=10):\n    positive_words = [w for (w,y) in alpha.keys() if y == label]\n    sorted_positive_words = sorted(positive_words, key=lambda w:-alpha[w,label])[:k]\n    util.plot_bar_graph([alpha[w,label] for w in sorted_positive_words],sorted_positive_words,rotation=45)\nplot_top_k(alpha)\n", "intent": "Let us look at the $\\balpha$ parameters that correspond to the probability of generating a word given a class.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nimport numpy as np\nlr = LogisticRegression(C=0.25)\nlr.fit(train_X, train_Y)\n", "intent": "Finally, we can train the logistic regression model using a regularisation parameter $C=0.25$.\n"}
{"snippet": "logr = LogisticRegression()\nlogr.fit(X,y);\n", "intent": "Let's fit a logistic regression model on the data above and plot the predicted labels and the probabilities\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nimport numpy as np\nlr = LogisticRegression(C=0.25, class_weight='balanced')\nlr.fit(train_X, train_Y)\n", "intent": "Train the logistic regression with l2 regularisation $C=0.25$\n"}
{"snippet": "dotprod_pos, dotprod_neg, diff_dotprod, placeholders = ie.create_model_f_reader(max_lens_rel, max_lens_ents, repr_dim, vocab_size_rels,\n                          vocab_size_ents)\nloss = tf.reduce_sum(tf.nn.softplus(-dotprod_pos)+tf.nn.softplus(dotprod_neg))\n", "intent": "Now that we have read in the data, vectorised it and created the universal schema relation extraction model, let's start training\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    W = tf.Variable(\n    tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0), name='W')\n    embedding_layer = tf.nn.embedding_lookup(W, input_data)\n    return embedding_layer\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "ridge = Ridge(alpha=0.5)\nridge.fit(X_train, y_train)\n", "intent": "We're really just doing this to make sure that we don't have any issues in our data that will break our model fitting.\n"}
{"snippet": "gd = GradientDescent(lrf.cost, lrf.gradient, lrf.predict, \n                     fit_intercept=False, num_iterations=100)\ngd.fit(X, y)\n", "intent": "Let's test this on our fake data.\n"}
{"snippet": "gd_standardized = GradientDescent(lrf.cost, lrf.gradient, lrf.predict, \n                                  fit_intercept=True, standardize=True,\n                                  num_iterations=100)\ngd_standardized.fit(X, y)\n", "intent": "Let's fit to our simulated data.\n"}
{"snippet": "gd_regularized = GradientDescent(lrf.cost, lrf.gradient, lrf.predict, \n                                 fit_intercept=True, standardize=True,\n                                 num_iterations=100,\n                                 cost_func_parameters={'lam': 5})\ngd_regularized.fit(X, y)\n", "intent": "Let's test this out by adding some simple ridge shrinkage to our test model.\n"}
{"snippet": "lambdas = [0.0, 0.5, 1, 2, 5, 10, 25, 50]\nmodels = OrderedDict()\nfor lam in lambdas:\n    gd = GradientDescent(lrf.cost, lrf.gradient, lrf.predict,\n                         fit_intercept=True, standardize=True,\n                         num_iterations=100,\n                         cost_func_parameters={'lam': lam})\n    gd.fit(X, y)\n    models[lam] = gd\n", "intent": "It's fun to see what happens to our regularized decision boundary as we vary the parameter:\n"}
{"snippet": "gd.fit(X, y)\n", "intent": "We can also use some ipython magic to time the two fit methods.\n"}
{"snippet": "V = df[[\"valence\"]]\nlr_V = LogisticRegression()\nlr_V.fit(V, y);\n", "intent": "<b>Train model using one feature: \"danceability\"</b>\n"}
{"snippet": "clf = LogisticRegression(intercept_scaling=100).fit(X, y)\n", "intent": "2\\. Fit `LogisticRegression` with `gym_hours` and `email_hours` as features and `data_scientist` as the response.\n"}
{"snippet": "preds = ['2015 Avg Price Per Liter']\nx = model_copy[preds]\ny = model_copy['2015 Sales']\nlmodel = Lasso(alpha = 1)\nlmodel.fit(x ,y)\n", "intent": "Using scikit-learn or statsmodels, build the necessary models for your scenario. Evaluate model fit.\n"}
{"snippet": "formula = 'label ~ ' + ' + '.join(numeric_cols) + ' -1'\neg_lr = smf.logit(formula, data=su)\neg_results = eg_lr.fit()\nprint eg_results.summary()\n", "intent": "And print out the results as shown in the example above.\n---\n"}
{"snippet": "lr_biz = LogisticRegression()\nlr_biz_parameters = {\n    'penalty':['l1','l2'],\n    'solver':['liblinear'],\n    'C':np.logspace(-4,0,20),\n}\nlr_biz_gs = GridSearchCV(lr_biz, lr_biz_parameters, cv=5, verbose=1)\nlr_biz_gs.fit(Xinter_n, business)\n", "intent": "Include Ridge and Lasso.\n---\n"}
{"snippet": "lr_biz_pr = LogisticRegression()\nlr_biz_pr_parameters = {\n    'penalty':['l1','l2'],\n    'solver':['liblinear'],\n    'C':np.logspace(-4,0,20)\n}\nlr_biz_pr_gs = GridSearchCV(lr_biz_pr, lr_biz_pr_parameters, \n                               cv=5, verbose=2, scoring='precision')\nlr_biz_pr_gs.fit(Xinter_n, business)\n", "intent": "Look at the documentation.\n---\n"}
{"snippet": "best_title_model = LogisticRegression(penalty = 'l2', C = best_c_title, class_weight= None, solver = 'liblinear')\n", "intent": "Print out the top 20 or 25 word features as ranked by their coefficients.\n---\n"}
{"snippet": "ind = np.unravel_index(np.argmax(np.mean(neg_mses, axis=-1)), dims=neg_mses.shape[:-1])\nregression_tree = sklearn.tree.DecisionTreeRegressor(max_depth=depth_params[ind[1]],\n                                                     max_leaf_nodes=leaf_params[ind[0]]).fit(X, y_obs)\n", "intent": "We can now use the hyperparameters selected by cross validation to retrain the model and compare it tto the generative model.\n"}
{"snippet": "Cs = np.exp(np.linspace(-5, 8, 20))\nnfold = 5\nlrCV = skl.LogisticRegressionCV(Cs=Cs,\n                                cv=nfold, penalty=\"l1\",\n                                solver='saga',\n                                multi_class=\"multinomial\",\n                                scoring=\"neg_log_loss\",\n                                refit=True)\nlrCV.fit(x_train, y_train)\n", "intent": "We use cross validation to select the tuning parameter.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(\n    tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\nembed = get_embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "logr = LogisticRegression();\n", "intent": "Let's fit a logistic regression model on the data above and plot the predicted labels and the probabilities\n"}
{"snippet": "model = LeNetOneShot()\ndef eval_func(input_data, output, loss):\n    anchor, positive, negative = output\n    positive_distance = F.pairwise_distance(anchor, positive)\n    negative_distance = F.pairwise_distance(anchor, negative)\n    return (positive_distance < negative_distance).sum().item()\nlearner = Learner(model, data_loader_train, data_loader_test)\nlearner.fit(model_loss_func, 0.002, num_epochs=100, eval_func=eval_func, early_stop='loss')\n", "intent": "***BE CAREFUL***\nThe accuracy depends on threshold. Right now it's unreliable. We will find the best threshold later\n"}
{"snippet": "model = FaceNetModel(embedding_size=128, num_classes=10000)\ndef eval_func(input_data, output, loss):\n    anchor, positive, negative = output\n    positive_distance = F.pairwise_distance(anchor, positive)\n    negative_distance = F.pairwise_distance(anchor, negative)\n    return (positive_distance < negative_distance).sum().item()\nlearner = Learner(model, data_loader_train, data_loader_test)\nlearner.fit(model_loss_func, 0.002, num_epochs=100, eval_func=eval_func, early_stop='loss')\n", "intent": "***BE CAREFUL***\nThe accuracy depends on threshold. Right now it's unreliable. We will find the best threshold later\n"}
{"snippet": "lr = LinearRegression()\ndtr = DecisionTreeRegressor()\nrfr = RandomForestRegressor()\nabr = AdaBoostRegressor()\n", "intent": "> **Step 3:** Now that you have imported the 4 models that can be used for regression problems, instantate each below.\n"}
{"snippet": "lr.fit(X_train, y_train)\ndtr.fit(X_train, y_train)\nrfr.fit(X_train, y_train)\nabr.fit(X_train, y_train)\n", "intent": "> **Step 4:** Fit each of your instantiated models on the training data.\n"}
{"snippet": "model.fit(x=x_train, y=y_train, epochs=5, batch_size=128)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state=0)\n", "intent": "Initialize a decision tree model\n"}
{"snippet": "model = KNeighborsClassifier(5).fit(f_train, t_train)\n", "intent": "Train the KNN classifier\n"}
{"snippet": "print crossValidate(features, target, KNeighborsClassifier(3),10, 0)\nprint crossValidate(features, target, KNeighborsClassifier(4),10, 0)\nprint crossValidate(features, target, KNeighborsClassifier(5),10, 0)\n", "intent": "Test the cross-validation function on different numbers of neighbors\n"}
{"snippet": "model = RandomForestClassifier(random_state=0)\n", "intent": "Create an instance of a random forest classifier.  Random state is used to set random number generator for reproducible results\n"}
{"snippet": "model = tree.DecisionTreeRegressor(random_state = 0).\\\n    fit(train_X, train_y)\n", "intent": "- (http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor)\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_matrix = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    word_embeddings = tf.nn.embedding_lookup(embedding_matrix, input_data)\n    return word_embeddings\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "rf_clf = RandomForestClassifier(n_estimators=200, oob_score=True, n_jobs=-1,\n                                random_state=42, verbose=1, class_weight='balanced')\nrf_clf.fit(X, y)\nprint('Out-of-bag generalization accuracy estimate: {0:.2f} %'.format(rf_clf.oob_score_))\n", "intent": "Now the data is ready for modeling, with the adopted_user column serving as our target variable. Random forest will be used for the predictions here.\n"}
{"snippet": "happyModel.fit(x = X_train, y = Y_train , epochs = 40, batch_size = 16)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model.fit(X_train, Y_train, epochs = 100, batch_size = 32)\n", "intent": "Run the following cell to train your model on 2 epochs with a batch size of 32. On a CPU it should take you around 5min per epoch. \n"}
{"snippet": "from sklearn.kernel_ridge import KernelRidge\nparam_grid={'alpha':[0.2,0.3,0.4], 'kernel':[\"polynomial\"], 'degree':[3],'coef0':[0.8,1]}\ngrid(KernelRidge()).grid_get(train_sel_60,target_df,param_grid)\n", "intent": "    3. Kernel Ridge\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.25))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.16))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=300, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "regr_1 = DecisionTreeRegressor(max_depth=1)\nregr_1.fit(X, y)\n", "intent": "-What does the data look like before the last line?\n-Why did I put the pi in the definition of y and what is the effect? \n"}
{"snippet": "mod = smf.ols(formula='log_marketcap ~ log_revenue + log_employees  + log_assets', data=df_companies)\nres = mod.fit()\nprint(res.summary())\n", "intent": "Revenue, employees and assets are highly correlated.\nLet's imagine wwe want to explain the market capitalization in terms of the other variables. \n"}
{"snippet": "model = tree.DecisionTreeClassifier(max_depth = 2,\n    min_samples_leaf = 5,\n    random_state = 0).\\\n        fit(train_X, train_y)\n", "intent": "- (http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier)\n"}
{"snippet": "from sklearn import linear_model\nreg = linear_model.Lasso(alpha = 0.01)\nreg.fit(X_train,y_train)\nprint(\"log_revenue \tlog_employees \tlog_assets \")\nprint(reg.coef_)\n", "intent": "**Lasso**\n- Have a penalty\n- Discards the variables with low weights.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    word_embeddings = tf.get_variable(\"word_embeddings\",[vocab_size, embed_dim])\n    embedded_word_ids = tf.gather(word_embeddings, input_data)\n    print (embedded_word_ids)\n    return embedded_word_ids\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "rfcl = RandomForestClassifier(random_state=seed)\nrfcl.fit(xTrain, yTrain)\n", "intent": "We can see that maximum score can be achieved using regularization of max_dept = 3.\n"}
{"snippet": "rfcl = RandomForestClassifier(n_estimators=3, max_depth=5, random_state=seed)\nrfcl.fit(xTrain, yTrain)\nrfcl.score(xTest, yTest)\n", "intent": "Random Forest classifier has given us an accuracy of **77.05%** which is more than Decision tree classifier.\n"}
{"snippet": "tss = TimeSeriesSplit(n_splits=10)\nlogit_cv2 = LogisticRegressionCV(Cs = [3], n_jobs=-1, scoring ='roc_auc', cv=tss)\nlogit_cv2.fit(X_train, y_train)\n", "intent": "Perform cross-validation with logistic regression.\n"}
{"snippet": "tss = TimeSeriesSplit(n_splits=7)\nlogit_cv = LogisticRegressionCV(Cs = [1], n_jobs=-1, scoring ='roc_auc', cv=tss)\nlogit_cv.fit(X_train, y_train)\n", "intent": "Perform cross-validation with logistic regression.\n"}
{"snippet": "print(\"C \\t train \\t test\")\nfor C in [0.1,0.5,0.7,1,3,10,100,1000]:\n    svm=SVC(C=C)\n    svm.fit(Xtrain_scaled,ytrain)\n    print(\"{} \\t {:.3f} \\t {:.3f}\".format(C,svm.score(Xtrain_scaled,ytrain), svm.score(Xtest_scaled,ytest)))\n", "intent": "1. Scaling made a huge difference!\n1. We can alter the fit parameters 'C',gamma for a better model score.\n"}
{"snippet": "sales_reduced_columns = [u'2015 Sales mean', u'Price per Liter mean', u'2015 Volume Sold (Liters)',\n                         u'2015 Volume Sold (Liters) mean', u'2015 Margin mean']\nfor x in sales_reduced_columns:\n    model = sm.OLS(sales[\"2015 Sales\"], sales[x]).fit()\n    print x, \":\", model.mse_model\n", "intent": "Using scikit-learn or statsmodels, build the necessary models for your scenario. Evaluate model fit.\n"}
{"snippet": "k=2\nkmeans=cluster.KMeans(n_clusters=k)\nkmeans.fit(dn)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "model = ensemble.RandomForestClassifier(n_estimators = 1000,\n        max_features = 4,\n        min_samples_leaf = 5,\n        oob_score = True,\n        random_state = 0).\\\n    fit(train_X, train_y)\n", "intent": "- (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier)\n"}
{"snippet": "lm = smf.ols(formula='speed ~ wind_speed', data = df).fit()\nprint (lm.summary())\n", "intent": "The R-squared is 0.026 which is small, so it seems 'Visibility' has no correlation to 'speed'.\n"}
{"snippet": "from sklearn import ensemble\nclf =  ensemble.RandomForestClassifier(max_depth=3, criterion=\"gini\", random_state=222).fit(titanic_short.values, titanic[\"Survived\"])\nclf.score(titanic_short.values, titanic[\"Survived\"])\n", "intent": "changing model to Random Forest\n"}
{"snippet": "clf =  ensemble.GradientBoostingClassifier(max_depth=4, random_state=222).fit(titanic_short.values, titanic[\"Survived\"])\nclf.score(titanic_short.values, titanic[\"Survived\"])\n", "intent": "changing model to Gradient Boosted Trees\n"}
{"snippet": "lr = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "rfc_grid2 = RandomForestClassifier(n_jobs=-1, random_state = 42,bootstrap= False, criterion = 'entropy' )\nparam_grid2 = {\n              \"max_features\": [ 10,'auto' ],\n              \"min_samples_split\": [2, 10, 15],\n              \"min_samples_leaf\": [1, 10, 15] }\nCV_rfc2 = GridSearchCV(estimator=rfc_grid2, param_grid=param_grid2, cv= 3, scoring= \"f1\" , n_jobs=-1)\nCV_rfc2.fit(X_train_stem, y_train_stem)\nprint (CV_rfc2.best_params_)\n", "intent": "Keep from the last grid search:  bootstrap= False, criterion = 'entropy'.\n"}
{"snippet": "rfc = RandomForestClassifier(n_jobs=-1, random_state = 36) \nparam_grid = { \n           \"n_estimators\" : [24,26,28], \n           \"max_depth\" : [30,35,40], \n           \"min_samples_leaf\" : [8,10,12]}\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 3, scoring= \"f1\" , n_jobs=-1)\nCV_rfc.fit(x_train_res, y_train_res)\nprint (CV_rfc.best_params_)\n", "intent": "For Grid Search CV : scoring can be  =  f1, accuracy, recall, or precision    \n"}
{"snippet": "est = LogisticRegression(dual=False)\n", "intent": "We create an instance of a Logistic Regression Classifier ...\n"}
{"snippet": "est.fit(X, y)\n", "intent": "... and fit it to the training set (i.e., we learn the patterns required to predict digits).\n"}
{"snippet": "scaler_model.fit(data)\n", "intent": "just convert int32 to float64\n"}
{"snippet": "mlp = MLPClassifier(hidden_layer_sizes=(20,10), max_iter=1000, activation = \"logistic\", solver = \"lbfgs\", learning_rate_init=0.9, learning_rate = \"constant\", verbose = True, early_stopping = False, tol=0.001, random_state = 10000)\n", "intent": "accuracy of the model could vary depending on which rows were selected for the training and test sets.\n"}
{"snippet": "model.fit(x_train,y_train,epochs=10,batch_size=100,validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "dtree =  DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "LogReg = LogisticRegression()\nLogReg.fit(X_train, y_train)\n", "intent": "<h2> Logistic regression </h2>\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\nrf.fit(X_train, y_train)\n", "intent": "<h3> Random Forest </h3>\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(train[['Gr Liv Area']], train['SalePrice'])\nprint(lr.coef_)\nprint(lr.intercept_)\na0 = lr.intercept_\na1 = lr.coef_\n", "intent": "lr.coef_ : W <br>\nlr.intercept: b\n"}
{"snippet": "test_input = [[0.25, 0.15]]\ngraph=tf.Graph()\nwith tf.Session() as session:\n    tf.global_variables_initializer().run(session=session)\n    output = session.run([out], feed_dict = feed_dict)\n    print(output[0]) \n", "intent": "As with the previous example, we can use `session.run()` to execute this computation graph, and use a `feed_dict` to feed in our input:\n"}
{"snippet": "params = {'max_depth':(5, 10, 15),\n         'min_samples_split':(.05, .075, .1),\n         'min_samples_leaf':(.05, .075, .1)}\ndtr2 = DecisionTreeRegressor(random_state=0,\n                            criterion='mae',\n                            splitter='best',\n                            max_features='auto')\nclf2 = GridSearchCV(dtr2, params)\nclf2.fit(train[columns], train['cnt'])\n", "intent": "* Based on the first result from GridSearchCV, re-arrange the parameters and execute the second GridSearchCV.\n"}
{"snippet": "with train_graph.as_default():\n    saver = tf.train.Saver()\nwith tf.Session(graph=train_graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    embed_mat = sess.run(embedding)\n", "intent": "Restore the trained network :\n"}
{"snippet": "RF = RandomForestRegressor(n_estimators = 10000, \n                           max_features = 4,     \n                           min_samples_leaf = 10, \n                           oob_score = True,    \n                           random_state = 1)      \nRF.fit(X,y)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n"}
{"snippet": "lm.fit(df_train,train_target)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "lg=LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dense, Activation, LSTM\nfrom keras.optimizers import RMSprop\nmodel = Sequential()\nmodel.add(LSTM(128, input_shape=(MAXLEN, len(chars)), dropout=0.5))\nmodel.add(Dense(len(chars)))\nmodel.add(Activation('softmax'))\noptimizer = RMSprop(lr=0.001)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\n", "intent": "Build an LSTM language model\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=17)\nrf.fit(X_train, y_train)\n", "intent": "Train a random forest **(RandomForestClassifier)**. Set the number of trees to 100 and use **random_state = 17**.\n"}
{"snippet": "forest_params = {'max_depth': range(10, 21),\n                'max_features': range(5, 105, 20)}\nlocally_best_forest = GridSearchCV(rf, forest_params, cv=5, n_jobs=-1, verbose=True) \nlocally_best_forest.fit(X_train, y_train)\n", "intent": "Train a random forest **(RandomForestClassifier)**. Tune the maximum depth and maximum number of features for each tree using **GridSearchCV**. \n"}
{"snippet": "scaler.fit(bn.drop('Class', axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "model_l1_cv.fit(X_train, y_train)\n", "intent": "Now we can run the cross-validated optimization on our training set.\n"}
{"snippet": "model_l1.fit(X_train, y_train)\n", "intent": "We train the model on the training set with the standard 'fit' method:\n"}
{"snippet": "nclasso = ncLasso(transposed_incidence=tim, lambda1=0.001, lambda2=0.001)\nnclasso.fit(X_2W_tr, y_2W_tr)\n", "intent": "__Use the network-constrained Lasso on the data. What do you observe?__\n"}
{"snippet": "X = CreditData[['Education','Race_Asian','Race_Caucasian','Gender_Female','Age','Cards','Income']]\nX.head(2)\ny = CreditData['Balance']\nfrom sklearn.linear_model import LinearRegression  \nlinreg = LinearRegression() \nlinreg.fit(X,y)     \nprint(linreg.intercept_)\nprint(linreg.coef_)\n", "intent": "First Step, find the coefficients of your regression line\n"}
{"snippet": "input_ques = Input(shape=(None,), dtype='int32', name='ques')\nembeded_ques = layers.Embedding(32, ques_vocab_size)(input_ques)\nencoded_ques = layers.LSTM(16)(embeded_ques)\n", "intent": "**Defining input for question**\n"}
{"snippet": "input_ = layers.concatenate([encoded_text, encoded_ques], axis=-1)\n", "intent": "**Concatanating the input into one**\n"}
{"snippet": "answer = layers.Dense(ans_vocab_size, activation='softmax')(input_)\n", "intent": "**Defining Output**\n"}
{"snippet": "hist = model.fit([input_text, input_ques], answer, epochs=10, batch_size=128)\n", "intent": "**Fitting the model**\n"}
{"snippet": "clf = RandomForestRegressor(n_jobs=-1)\nclf.fit(raw.drop(['SalePrice'], axis=1), raw.SalePrice)\n", "intent": "**Using RandomForestRegressor algorithm as baseline algorithm**\n"}
{"snippet": "x = tf.Variable(5)\n", "intent": "Whn a tensor needs to be modified `tf.placeholder()` or `tf.constant()` can't serve the purpose. For this purpose `tf.Variable()` is used.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_matrix = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    return tf.nn.embedding_lookup(embedding_matrix, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "e, fs = anomaly.select_threshold(X, Xval, yval)\nprint('Best epsilon: {}\\nBest F-score on validation data: {}'.format(e, fs))\n", "intent": "<img style=\"float: left;\" src=\"../img/f1_score.png\">\n"}
{"snippet": "clf = linear_model.LogisticRegression(C=1)\nclf.fit(xtrain,ytrain)\n", "intent": "Let's try this special case of a SVM and see if drawing a line through the data to decide on the outcome is any better.\n"}
{"snippet": "lm = smf.ols(formula='y ~ X', data = CreditData).fit()\nlm.summary()\nprint(\"P-Vales: \", zip(['Education','Race_Asian','Race_Caucasian','Gender_Female','Age','Cards','Income'], \n                      lm.pvalues[1:8]))\n", "intent": "Second Step, find the p-values of your estimates. You have a few variables try to show your p-values along side the names of the variables.\n"}
{"snippet": "_x = np.array([1, 2, 3])\n_y = np.array([-1, -2, -3])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nwith tf.Session() as sess:\n    print(sess.run(x + y))\n", "intent": "Q1. Add x and y element-wise.\n"}
{"snippet": "_x = np.array([3, 4, 5])\n_y = np.array(3)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nwith tf.Session() as sess:\n    print(sess.run(x-y))\n", "intent": "Q2. Subtract y from x element-wise.\n"}
{"snippet": "_x = np.array([3, 4, 5])\n_y = np.array([1, 0, -1])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nwith tf.Session() as sess:\n    print(sess.run(x * y))\n", "intent": "Q3. Multiply x by y element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3])\nx = tf.convert_to_tensor(_x)\nwith tf.Session() as sess:\n    print(sess.run(x * 5))\n", "intent": "Q4. Multiply x by 5 element-wise.\n"}
{"snippet": "_x = np.array([10, 20, 30], np.int32)\n_y = np.array([2, 3, 7], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.mod(x,y)\nprint(out1.eval())\n", "intent": "Q6. Get the remainder of x / y element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3], np.int32)\n_y = np.array([4, 5, 6], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\ntf.cross(x,y).eval()\n", "intent": "Q7. Compute the pairwise cross product of x and y.\n"}
{"snippet": "_x = np.array([1, 2, 3], np.int32)\n_y = np.array([4, 5, 6], np.int32)\n_z = np.array([7, 8, 9], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nz = tf.convert_to_tensor(_y)\n(x + y + z).eval()\n", "intent": "Q8. Add x, y, and z element-wise.\n"}
{"snippet": "_X = np.array([[1, -1], [3, -3]])\nX = tf.convert_to_tensor(_X)\ntf.abs(X).eval()\n", "intent": "Q9. Compute the absolute value of X element-wise.\n"}
{"snippet": "_x = np.array([1, -1])\nx = tf.convert_to_tensor(_x)\n(-x).eval()\n", "intent": "Q10. Compute numerical negative value of x, elemet-wise.\n"}
{"snippet": "for i in [X2,X4]:\n    lm1 = smf.ols(formula='y ~ i', data=BostonData).fit()\n    print(lm1.pvalues)\n", "intent": "Answer: we will most likely use model 2 or 4. If both of these models are significant, then we use model 4. \n"}
{"snippet": "_x = np.array([1, 2, 2/10])\nx = tf.convert_to_tensor(_x)\n(1/x).eval()\n", "intent": "Q12. Compute the reciprocal of x, element-wise.\n"}
{"snippet": "_x = np.array([1, 2, -1])\nx = tf.convert_to_tensor(_x)\n(x**2).eval()\n", "intent": "Q13. Compute the square of x, element-wise.\n"}
{"snippet": "_x = np.array([1, 4, 9], dtype=np.float32)\nx = tf.convert_to_tensor(_x)\ntf.floor(x**1/2).eval()\n", "intent": "Q15. Compute square root of x element-wise.\n"}
{"snippet": "_x = np.array([1., 4., 9.])\nx = tf.convert_to_tensor(_x)\ntf.reciprocal((x**1/2)).eval()\n", "intent": "Q16. Compute the reciprocal of square root of x element-wise.\n"}
{"snippet": "_x = np.array([[1, 2], [3, 4]])\n_y = np.array([[1, 2], [1, 2]])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n(x**y).eval()\n", "intent": "Q17. Compute $x^y$, element-wise.\n"}
{"snippet": "_x = np.array([1., 2., 3.], np.float32)\nx = tf.convert_to_tensor(_x)\n(np.e ** x).eval()\n", "intent": "Q17. Compute $e^x$, element-wise.\n"}
{"snippet": "_x = np.array([1, np.e, np.e**2])\nx = tf.convert_to_tensor(_x)\ntf.log(x).eval()\n", "intent": "Q18. Compute natural logarithm of x element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 2])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\ntf.maximum(x,y).eval()\n", "intent": "Q19. Compute the max of x and y element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 2])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\ntf.minimum(x,y).eval()\n", "intent": "Q20. Compute the min of x and y element-wise.\n"}
{"snippet": "for i in [X12,X13,X14,X15]:\n    lm1 = smf.ols(formula='y ~ i', data=BostonData).fit()\n    print(lm1.pvalues)\n", "intent": "Answer: if our goal is prediction, we will most like use models 12, 13, 14 or 15. Let's test to see which one is the best!\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 1])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n((x-y)*(x-y)).eval()\n", "intent": "Q22. Compute (x - y)(x - y) element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3, 4])\nx = tf.convert_to_tensor(_x)\ntf.diag(x).eval()\n", "intent": "Q1. Create a diagonal tensor with the diagonal values of x.\n"}
{"snippet": "_X = np.array(\n[[1, 0, 0, 0],\n [0, 2, 0, 0],\n [0, 0, 3, 0],\n [0, 0, 0, 4]])\nX = tf.convert_to_tensor(_X)\ntf.diag_part(X).eval()\n", "intent": "Q2. Extract the diagonal of X.\n"}
{"snippet": "_X = np.random.rand(2,3,4)\nX = tf.convert_to_tensor(_X)\nout = tf.transpose(X, [1, 2, 0])\nprint(\"X\")\nprint(_X)\nprint(\"Transpose\")\nprint(out.eval())\n", "intent": "Q3. Permutate the dimensions of x such that the new tensor has shape (3, 4, 2).\n"}
{"snippet": "_X= np.random.rand(1, 2, 3, 4)\nX = tf.convert_to_tensor(_X)\nprint(\"X\")\nprint(_X)\nprint(\"transpose\")\nprint(tf.transpose(X,[0,1,3,2]).eval())\n", "intent": "Q6. Transpose the last two dimensions of X.\n"}
{"snippet": "_X = np.array([[1, 2, 3], [4, 5, 6]])\n_Y = np.array([[1, 1], [2, 2], [3, 3]])\nX = tf.convert_to_tensor(_X,dtype=np.int32)\nY = tf.convert_to_tensor(_Y,dtype=np.int32)\nout = tf.matmul(X, Y)\nprint(out.eval())\n_out = np.dot(_X, _Y)\nassert np.array_equal(out.eval(), _out)\n", "intent": "Q7. Multiply X by Y.\n"}
{"snippet": "_X = np.arange(1, 5, dtype=np.float32).reshape((2, 2))\nX = tf.convert_to_tensor(_X)\ntf.matrix_determinant(X).eval()\n", "intent": "Q9. Compute the determinant of X.\n"}
{"snippet": "_X = np.arange(1, 5, dtype=np.float64).reshape((2, 2))\nX = tf.convert_to_tensor(_X)\ntf.matrix_inverse(X).eval()\n", "intent": "Q10. Compute the inverse of X.\n"}
{"snippet": "_X = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]], np.float32)\nX = tf.convert_to_tensor(_X)\ntf.cholesky(X).eval()\n", "intent": "Q11. Get the lower-trianglular in the Cholesky decomposition of X.\n"}
{"snippet": "lm = LogisticRegression(solver = 'newton-cg', max_iter = 10000)\nlm.fit(X1, y)\nprint(zip(X1.columns.values, lm.coef_[0, :]))\nprint(lm.intercept_)\n", "intent": "Run your regression line on X1 and interpret your MOMMY AND DADMY coefficients. Assume variables are significant. \n"}
{"snippet": "_X = np.array(\n[[1, 0, 0, 0, 2], \n [0, 0, 3, 0, 0], \n [0, 0, 0, 0, 0], \n [0, 2, 0, 0, 0]], dtype=np.float32)\nX = tf.convert_to_tensor(_X)\ntf.svd(X)[0].eval()\n", "intent": "Q13. Compute the singular values of X.\n"}
{"snippet": "_X = np.arange(1, 7).reshape((2, 3))\n_Y = np.arange(1, 7).reshape((3, 2))\nX = tf.convert_to_tensor(_X, dtype=np.int32)\nY = tf.convert_to_tensor(_Y, dtype=np.int32)\n", "intent": "Q17. Complete the einsum function that would yield the same result as the given function.\n"}
{"snippet": "_X = np.array([[1,2,3], [4,5,6]])\nX = tf.convert_to_tensor(_X)\ntf.cumsum(X).eval()\n", "intent": "Q1. Compute the cumulative sum of X along the second axis.\n"}
{"snippet": "_X = np.array([[1,2,3], [4,5,6]])\nX = tf.convert_to_tensor(_X)\ntf.cumprod(X).eval()\n", "intent": "Q2. Compute the cumulative product of X along the second axis.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [-1,-2,-3,-4], \n     [-10,-20,-30,-40],\n     [10,20,30,40]])\nX = tf.convert_to_tensor(_X)\ntf.segment_sum(X,[0,0,1,1]).eval()\n", "intent": "Q3. Compute the sum along the first two elements and \nthe last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [1,1/2,1/3,1/4], \n     [1,2,3,4],\n     [-1,-1,-1,-1]])\nX = tf.convert_to_tensor(_X)\ntf.segment_prod(X,[0,0,1,1]).eval()\n", "intent": "Q4. Compute the product along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,4,5,7], \n     [2,3,6,8], \n     [1,2,3,4],\n     [-1,-2,-3,-4]])\nX = tf.convert_to_tensor(_X)\ntf.segment_min(X,[0,0,1,1]).eval()\n", "intent": "Q5. Compute the minimum along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,4,5,7], \n     [2,3,6,8], \n     [1,2,3,4],\n     [-1,-2,-3,-4]])\nX = tf.convert_to_tensor(_X)\ntf.segment_max(X,[0,0,1,1]).eval()\n", "intent": "Q6. Compute the maximum along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [5,6,7,8], \n     [-1,-2,-3,-4],\n     [-5,-6,-7,-8]])\nX = tf.convert_to_tensor(_X)\ntf.segment_mean(X,[0,0,1,1]).eval()\n", "intent": "Q7. Compute the mean along the first two elements and the last two elements of X separately.\n"}
{"snippet": "iris_pca_fit = pca.fit(iris)\nprint(iris_pca_fit.explained_variance_ratio_)\nsns.heatmap(iris_pca_fit.components_, cmap='viridis', xticklabels=iris.columns.tolist(),\n            yticklabels=['PCA1','PCA2'], linewidth=1, linecolor='k');\n", "intent": "We can also plot the mappings of each variable onto principal components.\n"}
{"snippet": "_X = np.random.permutation(10).reshape((2, 5))\nprint(\"_X =\", _X)\nX = tf.convert_to_tensor(_X)\ntf.argmax(X,axis=1).eval()\ntf.argmin(X,axis=1).eval()\n", "intent": "Q9. Get the indices of maximum and minimum values of X along the second axis.\n"}
{"snippet": "_x = np.array([0, 1, 2, 5, 0])\n_y = np.array([0, 1, 4])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout = tf.setdiff1d(x, y)[0]\nout.eval()\n", "intent": "Q10. Find the unique elements of x that are not present in y.\n"}
{"snippet": "_X = np.arange(1, 10).reshape(3, 3)\nX = tf.convert_to_tensor(_X)\nout = tf.where(X < 4, X, X*10)\nout.eval()\n", "intent": "Q11. Return the elements of X, if X < 4, otherwise X*10.\n"}
{"snippet": "_x = np.array([1, 2, 6, 4, 2, 3, 2])\nx = tf.convert_to_tensor(_x)\nout, indices = tf.unique(x)\nindices.eval()\n", "intent": "Q12. Get unique elements and their indices from x.\n"}
{"snippet": "hypothesis = tf.SparseTensor(\n    [[0, 0],[0, 1],[0, 2],[0, 4]],\n    [\"a\", \"b\", \"c\", \"a\"],\n    (1, 5)) \ntruth = tf.SparseTensor(\n    [[0, 0],[0, 2],[0, 4]],\n    [\"a\", \"c\", \"b\"],\n    (1, 6))\ntf.edit_distance(hypothesis,truth, normalize=True).eval()\ntf.edit_distance(hypothesis,truth, normalize=False).eval()\n", "intent": "Q13. Compute the edit distance between hypothesis and truth.\n"}
{"snippet": "_x = np.array([[1, 2, 4, 8], [2, 4, 6, 8]], dtype=np.float32)\nx = tf.convert_to_tensor(_x)\nout = tf.nn.softmax(x)\nwith tf.Session() as sess:\n    _out = sess.run(out)\n    print(_out)    \n    assert np.allclose(np.sum(_out, axis=-1), 1)\n", "intent": "Q3. Apply `softmax` to x.\n"}
{"snippet": "_x = np.array([[1, 2, 4, 8], [2, 4, 6, 8]], dtype=np.float32)\nprint(\"_x =\\n\" , _x)\nx = tf.convert_to_tensor(_x)\nout = tf.nn.dropout(x, keep_prob=.5)\nwith tf.Session() as sess:\n    _out = sess.run(out)\n    print(\"_out =\\n\", _out) \n", "intent": "Q4. Apply `dropout` with keep_prob=.5 to x.\n"}
{"snippet": "x = tf.random_normal([8, 10])\nout = tf.contrib.layers.fully_connected(x, num_outputs=2, activation_fn=tf.nn.sigmoid)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(out))\n", "intent": "Q5. Apply a fully connected layer to x with 2 outputs and then an sigmoid function.\n"}
{"snippet": "x = tf.constant([[1, 0, 0, 0],\n                 [0, 0, 2, 0],\n                 [0, 0, 0, 0]], dtype=tf.int32)\nsp = tf.SparseTensor([[0,0],[1,2]], values= [1,2], dense_shape=[3,4])\nprint(sp.eval())\n", "intent": "Q1. Convert tensor x into a `SparseTensor`.\n"}
{"snippet": "import statsmodels.api as sm\nX = np.array(xs).transpose()\nX = sm.add_constant(X)\nmod = sm.OLS(ys, X)\nres = mod.fit()\nprint res.summary()\n", "intent": "This generated a fit of $y = 3 x + 1$. Let's see what a linear regression yields.\n"}
{"snippet": "output = tf.sparse_tensor_to_dense(sp)\nprint(output.eval())\nprint(\"Check if this is identical with x:\\n\", x.eval())\n", "intent": "Q5. Convert the SparseTensor `sp` to a Tensor using `tf.sparse_tensor_to_dense`.\n"}
{"snippet": "imputer.fit(housing_num)\n", "intent": "Now you can fit the imputer instance to the training data using the fit() method:\n"}
{"snippet": "x1 = tf.Variable(1)\nx1.graph is tf.get_default_graph()\n", "intent": "Any node you create is automatically added to the default graph:\n"}
{"snippet": "test_files=test_files[:10000]\ntest_id=test_id[:10000]\nfinal_test_tensors = paths_to_tensor(test_files).astype('float32')/255\n", "intent": "Reduce test set size\n"}
{"snippet": "model = read_model()\nmodel.load_weights('cache/weights_single_model.hdf5')\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\nprint('read and compiled model')\n", "intent": "Section that loads the model and its weights\n"}
{"snippet": "lm = LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=True)\nlm.fit(X, bos.PRICE)\n", "intent": "**Your turn:** How would you change the model to not fit an intercept term? Would you recommend not having an intercept?\n"}
{"snippet": "knn_clf = neighbors.KNeighborsClassifier(n_neighbors=3, weights='uniform')\n", "intent": "-Which model is more accurate?\n"}
{"snippet": "from keras.layers import Dense, Dropout, Activation, PReLU\nmodel = Sequential()\nmodel.add(Dense(128, input_shape=(1000,)))\nmodel.add(PReLU())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=200, verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "rcv = linear_model.RidgeCV(alphas=\n                           (.001, .001, .01, .1, .5, 1, 5, 10),\n                           store_cv_values=True,\n                          )\n", "intent": "RidgeCV implements cross validation on a ridge regression with various alphas\n"}
{"snippet": "x1 = tf.placeholder(tf.float32, [2])\nx2 = tf.placeholder(tf.float32, [2])\nz = tf.add(x1,x2)\nfeed_dict = {x1: [1,5], x2:[1,1]}\nsess = tf.Session()\nsess.run(z, feed_dict)\n", "intent": "Now we can also use vectors as input, to show that we don't need to deal only with scalars\n"}
{"snippet": "x1 = tf.constant(1)\nx2 = tf.constant(2)\nz = tf.add(x1, x2)\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\nsess.run(z)\n", "intent": "Consider the following code\n"}
{"snippet": "c = tf.constant(5)\nx = c + 1\ny = x + 1\nz = x + 2\nsess = tf.Session()\nprint('y is',sess.run(y))\nprint('z is',sess.run(z))\nsess.close()\n", "intent": "Sometime tensorflow may evaluate some nodes multiple times. Pay attention. Consider the following code\n"}
{"snippet": "c = tf.constant(5)\nx = c + 1\ny = x + 1\nz = x + 2\nsess = tf.Session()\nprint('y,z are',sess.run([y,z]))\nsess.close()\n", "intent": "here ```x``` is evaluated twice! You can do it like this\n"}
{"snippet": "sess = tf.Session()\nsess.close()\n", "intent": "You can close a session in this way\n"}
{"snippet": "with tf.Session() as sess:\n", "intent": "You can also do it in this way\n"}
{"snippet": "c = tf.constant(5)\nx = c + 1\ny = x + 1\nz = x + 2\nsess = tf.Session()\nprint('y is',sess.run(y))\nprint('z is',sess.run(z))\nsess.close()\n", "intent": "you need some code to avoid the error. Consider the following code\n"}
{"snippet": "c = tf.constant(5)\nx = c + 1\ny = x + 1\nz = x + 2\nwith tf.Session() as sess:\n    print('y is',sess.run(y))\n    print('z is',sess.run(z))\n", "intent": "this can also be written as the following\n"}
{"snippet": "sess = tf.Session()\nprint(sess.run(result, feed_dict = {x1: 3, x2: 4}))\nprint(sess.run(result, feed_dict = {x1: 5, x2: 7}))\nsess.close()\n", "intent": "To run the same graph twice in the same session you can do the following\n"}
{"snippet": "logreg = LogisticRegression(solver='liblinear')\nC_vals = [0.0001, 0.001, 0.01, 0.1, .15, .25, .275, .33, 0.5, .66, 0.75, 1.0, 2.5, 5.0, 10.0, 100.0, 1000.0]\npenalties = ['l1','l2']\ngs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals}, verbose=False, cv=15)\ngs.fit(X, y)\n", "intent": "Then will pass those to GridSearchCV\n"}
{"snippet": "x = np.arange(-5,5,0.1)\nidentity = x\nsigmoid = 1.0 / (1.0 + np.exp(-x))\narctan = np.tanh(x)\nrelu = np.maximum(x, 0)\nleakyrelu = relu - 0.05 * np.maximum(-x, 0)\n", "intent": "First let's create the data that we need to plot the different activation functions\n"}
{"snippet": "tf.reset_default_graph()\nX = tf.placeholder(tf.float32, [n_dim, None])\nY = tf.placeholder(tf.float32, [1, None])\nlearning_rate = tf.placeholder(tf.float32, shape=())\nW = tf.Variable(tf.ones([n_dim, 1]))\nb = tf.Variable(tf.zeros(1))\ninit = tf.global_variables_initializer()\n", "intent": "First we define the variables and placeholders we need to build the network above.\n"}
{"snippet": "sess, cost_history = run_logistic_model(learning_r = 0.05, \n                                training_epochs = 750, \n                                train_obs = Xtrain, \n                                train_labels = ytrain, \n                                debug = True)\n", "intent": "**CAREFUL** it will take some time to run!\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogistic = LogisticRegression()\n", "intent": "As a side note, here is the code to see how easy it is in comparison to do the same with the sklearn library... \n"}
{"snippet": "tf.reset_default_graph()\nX = tf.placeholder(tf.float32, [1, None]) \nY = tf.placeholder(tf.float32, [1, None]) \nlearning_rate = tf.placeholder(tf.float32, shape=())\nW = tf.Variable(tf.ones([1, 1])) \nb = tf.Variable(tf.zeros(1)) \ninit = tf.global_variables_initializer()\ny_ = tf.matmul(tf.transpose(W),X)+b \nmse = tf.reduce_mean(tf.square(y_ - Y)) \n", "intent": "Let's build our neural network. That would be one single neuron, with an identity activation function\n"}
{"snippet": "sess, ch = run_linear_model(1e-3, 1000, x, y, True)\n", "intent": "Interesting... Let's try a smaller learning rate\n"}
{"snippet": "sess3, ch3 = run_linear_model(1e-2, 5000, x, y, True)\n", "intent": "Note that the learning rate is small, and so the convergence is slow... Let's try something faster...\n"}
{"snippet": "sess5, ch5 = run_linear_model(0.03, 5000, x, y, True)\n", "intent": "For the sake of trying to find the best parameters\n"}
{"snippet": "xt = x.reshape(7,-1)\nyt = y.reshape(7,-1)\nreg = LinearRegression().fit(xt,yt)\nreg.score(xt,yt)\n", "intent": "Now let's compare the results with a classical linear regression as we can do with ```sklearn```\n"}
{"snippet": "model = LogisticRegression(penalty = 'l1', C = 10.0) \nmodel.fit(X, y)\nexamine_coefficients(model, X)\n", "intent": "- Change the `C` parameter\n    - how do the coefficients change? (use `examine_coeffcients`)\n    - how does the model perfomance change (using AUC)\n"}
{"snippet": "def get_random_element_with_label (data, lbls, lbl):\n    tmp = lbls == lbl\n    subset = data[:,tmp.flatten()]\n    return subset[:,randint(1,subset.shape[1])]\n", "intent": "THe following function will return one numpy array (one column) with an example of a choosen label.\n"}
{"snippet": "sess1, cost_history1 = model(minibatch_size = 1, \n                              training_epochs = 50, \n                              features = train_red, \n                              classes = labels_red, \n                              logging_step = 10,\n                              learning_r = 1e-3)\n", "intent": "The dataset is pretty balanced so we are good to go.\n"}
{"snippet": "sessb, cost_history500_2 = model(minibatch_size = 60000, \n                              training_epochs = 1000, \n                              features = train, \n                              classes = labels_, \n                              logging_step = 50,\n                              learning_r = 0.01)\n", "intent": "Let's try now batch-gradient descent\n"}
{"snippet": "sessb, cost_history500 = model(minibatch_size = 50, \n                              training_epochs = 1000, \n                              features = train, \n                              classes = labels_, \n                              logging_step = 100,\n                              learning_r = 0.001)\n", "intent": "The `corr_pred` array contains `True` when the prediction is right and `False` when it is not.\n"}
{"snippet": "sessad, cost_ad, Woutput, boutput = run_linear_model(400, 1, X_, Y_, \n                                                   debug = True, learning_r = 0.05)\nprint(Woutput[-1])\nprint(boutput[-1])\n", "intent": "**Check the speed (in seconds) difference between mini_batchsize = 1 and 30.**\n"}
{"snippet": "def create_layer (X, n, activation):\n    ndim = int(X.shape[0])\n    stddev = 2.0 / np.sqrt(ndim)\n    initialization = tf.truncated_normal((n, ndim), stddev = stddev)\n    W = tf.Variable(initialization)\n    b = tf.Variable(tf.zeros([n,1]))\n    Z = tf.matmul(W,X)+b\n    return activation(Z), W, b\n", "intent": "Now let's look at what happens when we try to do linear regression with a network with 4 layers and each 20 neurons.\n"}
{"snippet": "sess, cost_history = run_logistic_model(learning_r = 5e-4, \n                                training_epochs = 600, \n                                train_obs = X_train[1], \n                                train_labels = y_train[1], \n                                debug = True)\n", "intent": "Now let's train our network on Fold with index ```1```\n"}
{"snippet": "lr = LinearRegression()\nlr.fit(X_train, y_train)\n", "intent": "We can know run linear LS on the training set to find the parameter values, as above. \n"}
{"snippet": "lr = LinearRegression()\n", "intent": "In the context of scikit-learn, we have encountered and used examples of *classes*. Recall the notation, e.g. for the **LinearRegression()** class:\n"}
{"snippet": "model = LogisticRegression(penalty = 'l2', C = .01) \nmodel.fit(X, y)\nexamine_coefficients(model, X) \n", "intent": "- Change the `C` parameter - how do the coefficients change? (use `examine_coeffcients`)\n"}
{"snippet": "model.fit(x_train, y_train, epochs=2, batch_size=100, verbose = 2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr = LinearRegression().fit(X, y)\nplot(lr, \"OLS\", \"green\")\n", "intent": "We can see there's some random perturbations in the begenning of the scatter plot. We'll see how OLS fits the data. \n"}
{"snippet": "ada_model = AdaBoostClassifier(\n                    DecisionTreeClassifier(random_state = 17),\n                    random_state = 17, learning_rate = 0.1\n                )\nstart = time.time()\nada_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(ada_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"ada_model\"></a>\n"}
{"snippet": "dtc_model = DecisionTreeClassifier(random_state = 17)\nstart = time.time()\ndtc_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(dtc_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"dtc_model\"></a>\n"}
{"snippet": "etc_model = ExtraTreesClassifier(random_state = 17)\nstart = time.time()\netc_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(etc_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"etc_model\"></a>\n"}
{"snippet": "gnb_model = GaussianNB()\nstart = time.time()\ngnb_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(gnb_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"gnb_model\"></a>\n"}
{"snippet": "gbc_model = GradientBoostingClassifier(random_state=17)\nstart = time.time()\ngbc_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(gbc_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"gbc_model\"></a>\n"}
{"snippet": "knn_model = KNeighborsClassifier()\nstart = time.time()\nknn_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(knn_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"knn_model\"></a>\n"}
{"snippet": "lin_model = LinearDiscriminantAnalysis()\nstart = time.time()\nlin_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(lin_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"lin_model\"></a>\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nrf = RandomForestClassifier()\nall_models['rf'] = {'model': rf,\n                    'score': evaluate_model(rf)}\net = ExtraTreesClassifier()\nall_models['et'] = {'model': et,\n                    'score': evaluate_model(et)}\n", "intent": "Let's see if Random Forest and Extra Trees perform better\n1. Initialize RF and ET and test on Train/Test set\n- Find optimal params with Grid Search\n"}
{"snippet": "mlp_model = MLPClassifier(random_state = 17)\nstart = time.time()\nmlp_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nmlp_model(lin_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"mlp_model\"></a>\n"}
{"snippet": "rfc_model = RandomForestClassifier(random_state = 17)\nstart = time.time()\nrfc_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(rfc_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"rfc_model\"></a>\n"}
{"snippet": "svc_model = SVC()\nstart = time.time()\nsvc_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(svc_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"svc_model\"></a>\n"}
{"snippet": "rf_model = RandomForestRegressor()\nstart = time.time()\nrf_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\n", "intent": "WARNING: Takes a lot of time\n"}
{"snippet": "xgb_model = XGBRegressor()\nstart = time.time()\nxgb_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\n", "intent": "(Training takes 544 seconds)\n(Scoring takes 453 seconds)\n"}
{"snippet": "xgb_model = XGBRegressor()\nstart = time.time()\nxgb_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\n", "intent": "(Training takes 544, 533 seconds)\n(Scoring takes 453, 487 seconds)\n"}
{"snippet": "rf_model = RandomForestRegressor()\nstart = time.time()\nrf_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"random_forest\"></a>\nWARNING: Takes a lot of time\n"}
{"snippet": "xgb_model = XGBRegressor()\nstart = time.time()\nxgb_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"xgb\"></a>\n(Training takes 544, 533 seconds)\n(Scoring takes 453, 487 seconds)\n"}
{"snippet": "num_classes = 5\nresnet_weights_path = '../data/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\nmy_new_model = Sequential()\nmy_new_model.add(ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path))\nmy_new_model.add(Dense(num_classes, activation='softmax'))\nmy_new_model.layers[0].trainable = False\nmy_new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "- Using adam instead of sgd\n- model is trained from resnet 50\n- data is augmented\n"}
{"snippet": "Cat_Naive_Bayes = naive_bayes.MultinomialNB();\nCat_Naive_Bayes.fit(feature_train, target_train)\n", "intent": "Instantiate the Naive Bayes predictor from scikit-learn with the training data. \n"}
{"snippet": "lgb_model = LGBMClassifier()\nstart = time.time()\nlgb_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(lgb_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"lgb_model\"></a>\n"}
{"snippet": "xgb_model = XGBClassifier()\nstart = time.time()\nxgb_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(xgb_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"xgb_model\"></a>\n"}
{"snippet": "vgg.fit(batches, val_batches, nb_epoch=1)\nvgg.save()\n", "intent": "This is where we fit the dogs and cats to the existing model.  This takes a while even on a GPU.\n"}
{"snippet": "def myCrossEntropyLoss(outputs, labels):\n    batch_size = outputs.size()[0]\n    tmp_outputs = F.softmax(outputs, dim=1)\n    print(tmp_outputs)\n    outputs = F.log_softmax(outputs, dim=1)\n    print(outputs)\n    outputs = outputs[range(batch_size), labels] \n    return -torch.sum(outputs)/len(labels)\n", "intent": "\"def softmax(x):\\n\",\n    \"    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)\\n\"\n"}
{"snippet": "activation( torch.mm( activation( torch.mm( features, W1) + B1 ), W2 ) + B2 )\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "regressor = Sequential()\n", "intent": "We call our recurrent neural network a regressor since we are predicting using regression, not classification.\n"}
{"snippet": "regressor.add(Dense(units = 1))\n", "intent": "Our output layer will return the predicted stock price for the next day.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\n", "intent": "Select and train a model\n"}
{"snippet": "model.fit(x_train, y_train)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "import sklearn.linear_model as linear_model\nlogistic_class = linear_model.LogisticRegression()\nlogit = logistic_class.fit(feature_train, target_train)\n", "intent": "Define a logistic regression and train it with the feature and target set\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nparameters = {\n    'vec__max_df': [0.8, 1.0],\n    'vec__ngram_range': [(1, 1), (1, 2)],\n    'clf__alpha': np.logspace(-5, 0, 6)\n}\ngs = GridSearchCV(pipeline, parameters, verbose=2, refit=False, n_jobs=3)\n_ = gs.fit(twenty_train_small.data, twenty_train_small.target)\n", "intent": "For the grid search, the parameters names are prefixed with the name of the pipeline step using \"__\" as a separator:\n"}
{"snippet": "X = iris_train.values\ny = class_train.values\nclf = tree.DecisionTreeClassifier(max_depth=3)\nclf.fit(X, y)\n", "intent": "* `max-depth` is the max depth of dicision tree plus the root node\n* Default `tree.DecisionTreeClassifier` use criterion of gini instead of entropy\n"}
{"snippet": "regr = LogisticRegression()\nregr.fit(df2_reduced, target_variable)\nneigh = KNeighborsClassifier(n_neighbors=5)\nneigh.fit(df2_reduced, target_variable)\ntree = DecisionTreeClassifier()\ntree.fit(df2_reduced, target_variable)\n", "intent": "<i>Educate ML algorithms for classification:\n- without explicitly stating a \"random_state\"\n- on the whole train data</i>\n"}
{"snippet": "k_values = range(1,50,1)\nparameters = {'n_neighbors':k_values}\nknn = sklearn.neighbors.KNeighborsClassifier()\nsearch = sklearn.grid_search.GridSearchCV(knn, param_grid=parameters, cv=5)\nsearch.fit(x_2d, Y_train)\nsearch.best_estimator_  \nsearch.best_params_  \n", "intent": "Verify that the grid search has indeed chosen the right parameter value for $k$.\n"}
{"snippet": "k_values = range(1,50,1)\nparameters = {'n_neighbors':k_values}\nknn = sklearn.neighbors.KNeighborsClassifier()\nF10 = sklearn.grid_search.GridSearchCV(knn, param_grid=parameters, cv=10)\nF10.fit(X_train, Y_train)\nF10.best_params_\n", "intent": "Verify that the grid search has indeed chosen the right parameter value for $k$.\n"}
{"snippet": "class_less = sklearn.svm.SVC(C=1, gamma=1, class_weight=None)\nclass_less.fit(key_features, target)\nplot_decision_surface(class_less, key_features, target)\n", "intent": "**(d)** The <a href='http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"}
{"snippet": "model = Sequential()\n", "intent": "Model achitecture: ConvDNA (which is Conv1D) --> GlobalMaxPooling1D  -->  Dense --> Sigmoid\n"}
{"snippet": "model = Sequential()\nmodel.add(cl.ConvDNA(filters=1,\n                    kernel_size=15,\n                    activation='relu',\n                    input_shape=(500, 4), \n                    name='convDNA'))\nmodel.add(kl.GlobalMaxPooling1D())\nmodel.add(kl.Dense(1, name='dense'))\nmodel.add(kl.Activation('sigmoid'))\n", "intent": "Model achitecture: ConvDNA (which is Conv1D) --> GlobalMaxPooling1D  -->  Dense --> Sigmoid\n"}
{"snippet": "distortions = []\nfor i in range(1,11):\n    km = KMeans(n_clusters=i)\n    km.fit(df.drop('Private',axis=1))\n    distortions.append(km.inertia_)\n", "intent": "**using the elbow method to find the optimal number of clusters **\n"}
{"snippet": "logreg = LogisticRegression(solver='liblinear')\nC_vals = np.logspace(-5,1,50)\npenalties = ['l1','l2']\ngs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals}, cv=5)\ngs.fit(X3, y)\n", "intent": "Now that I had a model I wanted to see if I could improve it at all. My first attempt at doing this was with a GridSearchCV\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nclf_knn = KNeighborsClassifier()\nk_range = list(range(1, 21))\nprint(k_range)\n", "intent": "<a href=\"https://www.ritchieng.com/machine-learning-efficiently-search-tuning-param/\">knn tuning</a>\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nclf_knn = KNeighborsClassifier()\nk_range = list(range(1, 15))\nprint(k_range)\n", "intent": "<a href=\"https://www.ritchieng.com/machine-learning-efficiently-search-tuning-param/\">knn tuning</a>\n"}
{"snippet": "with tf.Session() as session:\n    result = session.run(c)\n    print (\"c =: {}\".format(result))\n", "intent": "<div align=\"right\">\n<a href=\"\n</div>\n<div id=\"operations\" class=\"collapse\">\n```\nc=tf.sin(a)\n```\n</div>\n"}
{"snippet": "a = tf.Variable(1.0)\nb = tf.Variable(0.2)\ny = a * x_data + b\n", "intent": "First, we initialize the variables __a__ and __b__, with any random guess, and then we define the linear function:\n"}
{"snippet": "init = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n", "intent": "Don't forget to initialize the variables before executing a graph:\n"}
{"snippet": "act = tf.sigmoid(tf.matmul(i, w) + b)\nact.eval(session=sess)\n", "intent": "3D sigmoid plot. The x-axis is the weight, the y-axis is the bias.\n"}
{"snippet": "act = tf.tanh(tf.matmul(i, w) + b)\nact.eval(session=sess)\n", "intent": "3D tanh plot. The x-axis is the weight, the y-axis is the bias.\n"}
{"snippet": "act = tf.nn.relu(tf.matmul(i, w) + b)\nact.eval(session=sess)\n", "intent": "3D relu plot. The x-axis is the weight, the y-axis is the bias.\nTensorFlow has ReLU and some other variants of this function. Take a look:\n"}
{"snippet": "weights = {\n    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n}\nbiases = {\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n", "intent": "Lets create the weight and biases for the read out layer\n"}
{"snippet": "chosen_k = 3\nkmean = KMeans(n_clusters=chosen_k, random_state=43)\nclusters = kmean.fit(PCAdf)\nlabel = pd.Series(kmean.labels_, name = 'label')\nkmean_df = pd.concat([PCAdf,label], axis = 1)\nkmean_df.head()\n", "intent": "K Means also seems to prefer 2 clusters because of stronger inertia, but I think having 3 clusters would be more useful so I'm going with those.\n"}
{"snippet": "outputs, states = tf.nn.dynamic_rnn(lstm_cell, inputs=x, dtype=tf.float32)\n", "intent": "__dynamic_rnn__ creates a recurrent neural network specified from __lstm_cell__:\n"}
{"snippet": "lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_size, forget_bias=0.0)\nstacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell] * num_layers)\n", "intent": "In this step, we create the stacked LSTM, which is a 2 layer LSTM network:\n"}
{"snippet": "embedding = tf.get_variable(\"embedding\", [vocab_size, hidden_size])  \n", "intent": "We create the embeddings for our input data. embedding is dictionary of [10000x200] for all 10000 unique words.\n"}
{"snippet": "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n", "intent": "__BasicRNNCell__ is the most basic RNN cell.\n"}
{"snippet": "session = tf.Session()\nfeed_dict={input_data:x, targets:y}\nsession.run(input_data, feed_dict)\n", "intent": "Lets check the value of the input_data again:\n"}
{"snippet": "with tf.variable_scope(\"rnn\"):\n    model = LSTMModel()\n", "intent": "Now we create a LSTM model:\n"}
{"snippet": "with  tf.Session() as sess:\n    a= tf.constant([0.7, 0.1, 0.8, 0.2])\n    print sess.run(a)\n    b=sess.run(tf.random_uniform(tf.shape(a)))\n    print b\n    print sess.run(a-b)\n    print sess.run(tf.sign( a - b))\n    print sess.run(tf.nn.relu(tf.sign( a - b)))\n", "intent": "Before we go further, let's look at an example of sampling:\n"}
{"snippet": "cur_w = np.zeros([784, 500], np.float32)\ncur_vb = np.zeros([784], np.float32)\ncur_hb = np.zeros([500], np.float32)\nprv_w = np.zeros([784, 500], np.float32)\nprv_vb = np.zeros([784], np.float32)\nprv_hb = np.zeros([500], np.float32)\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n", "intent": "Let's start a session and initialize the variables:\n"}
{"snippet": "hh0 = tf.nn.sigmoid(tf.matmul(X, W) + hb)\nvv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\nfeed = sess.run(hh0, feed_dict={ X: sample_case, W: prv_w, hb: prv_hb})\nrec = sess.run(vv1, feed_dict={ hh0: feed, W: prv_w, vb: prv_vb})\n", "intent": "Now let's pass this image through the net:\n"}
{"snippet": "logreg = LogisticRegression(solver='liblinear')\nC_vals = np.logspace(-5,1,50)\npenalties = ['l1','l2']\ngs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals}, cv=5)\ngs.fit(X, y)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "def decoder(x):\n    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n                                   biases['decoder_b1']))\n    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n                                   biases['decoder_b2']))\n    return layer_2\n", "intent": "And the decoder:\nYou can see that the layer_1 in the encoder is the layer_2 in the decoder and vice-versa.\n"}
{"snippet": "epochs = 5\nbatch_size = 50\nmodel.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def discriminator(images, reuse=False, alpha=0.1):\n    with tf.variable_scope(discriminator_name, reuse=reuse):\n        x = discriminator_conv2d(images, 128, 5, alpha=alpha)\n        x = discriminator_conv2d(x, 256, 5, alpha=alpha)       \n        x = discriminator_conv2d(x, 512, 5, alpha=alpha)\n        flat = tf.reshape(x, (-1, 4*4*512))\n        logits = tf.layers.dense(flat, 1)\n        out = tf.sigmoid(logits)\n    return out, logits        \ntests.test_discriminator(discriminator, tf)\n", "intent": "I doubled the depth at each convolution layer compared to previous MNIST of SVHN examples in hope of better capturing the information in faces.\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid'))\nmodel.add(Activation('relu'))\nmodel.add(Flatten(input_shape=(32, 32, 3)))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(256, input_dim = 1000, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss='categorical_crossentropy',\n                            optimizer='adadelta',\n                            metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train,\n                        batch_size=50,\n                        epochs=10,\n                        verbose=0,\n                        validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "mod_lin = LinearRegression()\nmod_dec = DecisionTreeRegressor()\nmod_ada = AdaBoostRegressor()\nmod_rf = RandomForestRegressor()\n", "intent": "> **Step 3:** Now that you have imported the 4 models that can be used for regression problems, instantate each below.\n"}
{"snippet": "mod_lin.fit(X_train, y_train)\nmod_dec.fit(X_train, y_train)\nmod_ada.fit(X_train, y_train)\nmod_rf.fit(X_train, y_train)\n", "intent": "> **Step 4:** Fit each of your instantiated models on the training data.\n"}
{"snippet": "ag = AgglomerativeClustering(n_clusters=3)\nag.fit(X)\npredicted_labels = ag.labels_\n", "intent": "Next we'll ask `AgglomerativeClustering` to return back two clusters for Iris:\n"}
{"snippet": "GK_cv = KNeighborsClassifier(n_neighbors=8, weights='distance', n_jobs=5,)\ncv_model = GK_cv.fit(X_train, y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "clf = DecisionTreeClassifier(max_depth=4)\nclf.fit(data_missing_drop_train.iloc[:,:-1], data_missing_drop_train.iloc[:,-1])\nprint('Optimal Decision Tree Depth: 4')\nprint ('Train score:', clf.score(X_drop_train, y_drop_train))\nprint ('Test score:', clf.score(X_drop_test, y_drop_test))\n", "intent": "**From CV, it looks like best choice for Decision Tree depth is 4 **\n"}
{"snippet": "clf = RandomForestClassifier(max_depth=4)\nclf.fit(X_drop_train, y_drop_train)\nprint('Optimal Random Forest Tree Depth: 4')\nprint ('Train score:', clf.score(X_drop_train, y_drop_train))\nprint ('Test score:', clf.score(X_drop_test, y_drop_test))\n", "intent": "**From CV, it looks like best choice for Random Forest Tree depth is 4 **\n"}
{"snippet": "clf = DecisionTreeClassifier(max_depth=4)\nclf.fit(X_mean_train, y_mean_train)\nprint('Optimal Tree Depth: 4')\nprint ('Train score:', clf.score(X_mean_train,y_mean_train))\nprint ('Test score:', clf.score(X_mean_test, y_mean_test))\n", "intent": "**From CV on Mean Imputation it seems like the best choice for Decision Tree depth is 4**\n"}
{"snippet": "clf = RandomForestClassifier(max_depth=7)\nclf.fit(X_mean_train, y_mean_train)\nprint('Optimal Random Forest Tree Depth: 7')\nprint ('Train score:', clf.score(X_mean_train, y_mean_train))\nprint ('Test score:', clf.score(X_mean_test, y_mean_test))\n", "intent": "**From CV it looks like the optimal max_tree_depth for Random Forest in mean Imputation is 7**\n"}
{"snippet": "clf_cv = LogisticRegressionCV(cv = 5)\nclf_cv.fit(x_train_pca, y_train)\n", "intent": "testing accuracy lower than before. \n"}
{"snippet": "dt_single = DecisionTreeClassifier(max_depth=md_best)\ndt_single.fit(X_train, y_train).score(X_test, y_test)\n", "intent": "Pavlos' Note: One can use md = 4 even though md_best = 5 since md = 4 is consistent with of best md but a simpler tree. \n"}
{"snippet": "clf_cv = LogisticRegressionCV()\nclf_cv.fit(x_train_pca, y_train)\n", "intent": "testing accuracy lower than before. \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "kmc = KMeans(n_clusters = num_clus, init= 'random')\nkmc.fit(response_array)\n", "intent": "**Your turn (extra credit):** Play with the following: \n* Different initializations for `KMeans`\n* Other clustering algorithms in scikit-learn\n"}
{"snippet": "chosen_k = 3\nkmean = KMeans(n_clusters=chosen_k, random_state=43)\nclusters = kmean.fit(PCAdf3)\nlabel = pd.Series(kmean.labels_, name = 'label')\ncentroids = kmean.cluster_centers_\nkmean_df = pd.concat([PCAdf3,label], axis = 1)\nkmean_df.head()\n", "intent": "well that looks pretty sweet!\n"}
{"snippet": "predictors = [x for x in train.columns if x not in [target, IDcol]]\ngbm_tuned_1 = GradientBoostingClassifier(learning_rate=0.05, n_estimators=120,max_depth=9, min_samples_split=1200, \n                                         min_samples_leaf=60, subsample=0.85, random_state=10, max_features=7)\nmodelfit(gbm_tuned_1, train, test, predictors)\n", "intent": "With all tuned lets try reducing the learning rate and proportionally increasing the number of estimators to get more robust results:\n"}
{"snippet": "predictors = [x for x in train.columns if x not in [target, IDcol]]\ngbm_tuned_2 = GradientBoostingClassifier(learning_rate=0.01, n_estimators=600,max_depth=9, min_samples_split=1200, \n                                         min_samples_leaf=60, subsample=0.85, random_state=10, max_features=7)\nmodelfit(gbm_tuned_2, train, test, predictors)\n", "intent": "1/10th learning rate\n"}
{"snippet": "predictors = [x for x in train.columns if x not in [target, IDcol]]\ngbm_tuned_3 = GradientBoostingClassifier(learning_rate=0.005, n_estimators=1200,max_depth=9, min_samples_split=1200, \n                                         min_samples_leaf=60, subsample=0.85, random_state=10, max_features=7,\n                                         warm_start=True)\nmodelfit(gbm_tuned_3, train, test, predictors, performCV=False)\n", "intent": "1/50th learning rate\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\nprint(net)\nnet = Sequential()\nnet.add(Linear(2, 4))\nnet.add(ReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "net = Sequential()\nnet.add(Linear(X_train.shape[1], X_train.shape[1] / 4))\nnet.add(LeakyReLU())\nnet.add(Linear(X_train.shape[1] / 4, X_train.shape[1] / 8))\nnet.add(LeakyReLU())\nnet.add(Linear(X_train.shape[1] / 8, 10))\nnet.add(SoftMax())\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "with graph.as_default():\n    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n", "intent": "with graph.as_default():\n    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs_, initial_state=initial_state)\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1.0,1.0))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf.fit(digits.data[:-1], digits.target[:-1])\n", "intent": "Next, we can train the SVM using all but the last sample.\n"}
{"snippet": "ptrat_mod = ols('PRICE ~ PTRATIO', data=bos).fit()\nprint(ptrat_mod.summary())\n", "intent": "**Try fitting a linear regression model using only the 'PTRATIO' (pupil-teacher ratio by town) and interpret the intercept and the coefficients.**\n"}
{"snippet": "chosen_k = 4\nkmean = KMeans(n_clusters=chosen_k, random_state=43)\nkmean.fit(PCAdf3)\nlabel = pd.Series(kmean.labels_, name = 'label')\nkmean_df = pd.concat([PCAdf3,label], axis = 1)\nkmean_df.head()\n", "intent": "even better! let's see 4 and be done for the day'\n"}
{"snippet": "my_tree = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200)\nmy_tree = my_tree.fit(X_train,y_train)\n", "intent": "Train a decision tree, setting min samples per leaf to a sensible value\n"}
{"snippet": "my_model = ensemble.RandomForestClassifier(n_estimators=300, \\\n                                           max_features = 3,\\\n                                           min_samples_split=200)\nmy_model.fit(X_train,y_train)\n", "intent": "Train and evaluate a simple model \n"}
{"snippet": "param_grid = [\n {'n_estimators': list(range(100, 501, 50)), 'max_features': list(range(2, 10, 2)), 'min_samples_split': [200] }\n]\nmy_tuned_model = GridSearchCV(ensemble.RandomForestClassifier(), param_grid, cv=cv_folds, verbose = 2)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nmodel_tuned_params_list[\"Tuned Random Forest\"] = my_tuned_model.best_params_\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "my_model = ensemble.BaggingClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 50), \\\n                                      n_estimators=10)\nmy_model.fit(X_train,y_train)\n", "intent": "Train and evaluate a simple model \n"}
{"snippet": "param_grid = [\n {'n_estimators': list(range(50, 501, 50)),\n  'base_estimator': [tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth = 6, min_samples_leaf = 200)]}\n]\nmy_tuned_model = GridSearchCV(ensemble.BaggingClassifier(), param_grid, cv=cv_folds, verbose = 2)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nmodel_tuned_params_list[\"Tuned Bagging\"] = my_tuned_model.best_params_\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "my_model = ensemble.AdaBoostClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 200), \\\n                                       n_estimators=10)\nmy_model.fit(X_train,y_train)\n", "intent": "Train and evaluate a simple model \n"}
{"snippet": "param_grid = [\n {'n_estimators': list(range(50, 501, 50)),\n 'base_estimator': [tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth = 6, min_samples_leaf = 200)]}\n]\nmy_tuned_model = GridSearchCV(ensemble.AdaBoostClassifier(), param_grid, cv=cv_folds, verbose = 2)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nmodel_tuned_params_list[\"Tuned AdaBoost\"] = my_tuned_model.best_params_\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "my_model = linear_model.LogisticRegression()\nmy_model.fit(X_train,y_train)\n", "intent": "Train and evaluate a simple model \n"}
{"snippet": "my_model = neighbors.KNeighborsClassifier()\nmy_model = my_model.fit(X_train,y_train)\n", "intent": "Train and evaluate a simple model \n"}
{"snippet": "chosen_k = 3\nkmean = KMeans(n_clusters=chosen_k, random_state=43)\nclusters = kmean.fit(PCAdf3)\nlabel = pd.Series(kmean.labels_, name = 'label')\ncentroids = kmean.cluster_centers_\nkmean_df = pd.concat([PCAdf3,label], axis = 1)\nkmean_df.head()\n", "intent": "Let's go with 3--that fourth one is bleh (technically speaking)\nDid you catch how I used different variable names for the 3rd cluster? Foreshadowing!\n"}
{"snippet": "my_model = neural_network.MLPClassifier(hidden_layer_sizes=(300, 100))\nmy_model = my_model.fit(X_train,y_train)\n", "intent": "Train and evaluate a simple model \n"}
{"snippet": "param_grid = [\n               {'hidden_layer_sizes': [(400), (400, 200), (400, 200, 100)], \n               'alpha': list(10.0 ** -np.arange(1, 7))}\n]\nmy_tuned_model = GridSearchCV(neural_network.MLPClassifier(), param_grid, cv=cv_folds, verbose = 2)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nmodel_tuned_params_list[\"Tuned MLP\"] = my_tuned_model.best_params_\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_shape=(784,)))\nmodel.add(Activation(\"sigmoid\"))\nmodel.add(Dense(units=207))\nmodel.add(Activation(\"sigmoid\"))\nmodel.add(Dense(units=102))\nmodel.add(Activation(\"sigmoid\"))\nmodel.add(Dense(units=10))\nmodel.add(Activation(\"softmax\"))\n", "intent": "Specfiy the structure of the neural network model\n"}
{"snippet": "from sklearn.cluster import KMeans\nmodel = KMeans(4)  \nmodel.fit(X)\nclustering = model.labels_\n", "intent": "We will apply the k-means algorithm to automatically split the data into *k=4* clusters.\n"}
{"snippet": "ndatapoints = 20\nxis_true = np.random.uniform(0, 1, ndatapoints)\nx_grid = np.linspace(0, 1, 100)\ndef model_linear(xs, slope, intercept): return xs * slope + intercept\nyis_true = model_linear(xis_true, slope_true, intercept_true)\n", "intent": "Let's generate some data drawn from that model:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)      \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    with tf.name_scope(\"embeddings\"):\n        embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), name=\"embeddigs\")\n        emb = tf.nn.embedding_lookup(embeddings, input_data)\n    tf.summary.histogram(\"embeddings\", embeddings)\n    return emb\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    with tf.name_scope(\"embeddings\"):\n        embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), name=\"embeddigs\")\n        emb = tf.nn.embedding_lookup(embeddings, input_data)\n    return emb\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn import linear_model\nlr = linear_model.LinearRegression()\nlr.fit(X_train, Y_train)\n", "intent": "You can import a Linear Regression Classifier by using the following codes:\n"}
{"snippet": "chosen_k = 3\nkmean = KMeans(n_clusters=chosen_k, random_state=43)\nclusters = kmean.fit(PCAdf3)\nlabel = pd.Series(kmean.labels_, name = 'label')\ncentroids = kmean.cluster_centers_\nkmean_df = pd.concat([PCAdf3,label], axis = 1)\nkmean_df.head()\n", "intent": "Let's go with 3--that fourth one is bleh (technically speaking)\n"}
{"snippet": "from sklearn import linear_model\nlr = linear_model.LogisticRegression()\nlr.fit(X_train, y_train)\n", "intent": "You can import a Logistic Regression Classifier by using the following codes:\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nclf_grad = GradientBoostingClassifier(random_state=0)\nfitted = clf_grad.fit(X_train, y_train)\nclf_tree = DecisionTreeClassifier(random_state=0)\nfitted_tree = clf_tree.fit(X_train, y_train)\nclf_random = RandomForestClassifier(random_state=0)\nfitted_random = clf_random.fit(X_train, y_train)\n", "intent": "You can import a Decision Tree Classifier by using the following codes:\n"}
{"snippet": "ft_map = base_model.get_layer(index=-2).output\nx = Conv2D(128, (3,3), padding='same')(ft_map)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Conv2D(1, (3,3), activation='sigmoid', padding='same')(x)\nx = GlobalAveragePooling2D()(x)\nmodel = Model(base_model.input, x)\n", "intent": "Add new classification head. Can use max or average pooling.\n"}
{"snippet": "import tensorflow as tf\ntf.reset_default_graph()\nX_train, y_train           = current_data_dict['X_train'], current_data_dict['y_train']\nX_validation, y_validation = current_data_dict['X_valid'], current_data_dict['y_valid']\nX_test, y_test             = current_data_dict['X_test'], current_data_dict['y_test']\nX_my_test, y_my_test       = current_data_dict['X_my_test'], current_data_dict['y_my_test']\nassert(len(X_train) == len(y_train))\nassert(len(X_validation) == len(y_validation))\nassert(len(X_test) == len(y_test))\nassert(len(X_my_test) == len(y_my_test))\n", "intent": "Load the MNIST data, which comes pre-loaded with TensorFlow.\nYou do not need to modify this section.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform(shape = [vocab_size, embed_dim], minval = -1, maxval = 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\ny = spam.is_spam.values\nX = spam.iloc[:, 1:100]\nknn = KNeighborsClassifier()\n", "intent": "---\nIt's up to you what predictors you want to use and how you want to parameterize the model.\n"}
{"snippet": "mean_knn_n2 = KNeighborsClassifier(n_neighbors=1,\n                              weights='uniform')\naccs, mean_acc = accuracy_crossvalidator(X, y, mean_knn_n2)\naccs, mean_acc\n", "intent": "---\nAs you can see the mean cross-validated accuracy is very high with 5 neighbors. \nLet's see what it's like when we use only 1 neighbor:\n"}
{"snippet": "gbt = GradientBoostingClassifier()\ngbt_params = {'max_depth': [None, 4, 8, 10],\n              'min_samples_split': [2,6,10],\n             'max_leaf_nodes': [None, 4, 8, 12],\n              \"learning_rate\" : [0.01, 0.1]}\n", "intent": "The print out shows that the parameters in the top row lead to the hightest F1 score for Random Forest\n"}
{"snippet": "from sklearn.cluster import KMeans\ncluster_model = KMeans(n_clusters = 2)\ncluster_model.fit(X)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "import tensorflow as tf\nx = tf.Variable(3, name='x')\ny = tf.Variable(4, name='y')\nf = x*x*y + y + 2\n", "intent": "The following code creates the graph represented above. \n"}
{"snippet": "def sigmoid(z):\n    s = 1 / (1 + np.exp(-z))\n    return s\n", "intent": "**Sigmoid**: $sigmoid( w^T x + b) = \\cfrac{1}{1 + e^{-(w^T x + b)}}$\n"}
{"snippet": "lm.fit(X, bos.PRICE )\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "from  sklearn.naive_bayes  import MultinomialNB\nclf = MultinomialNB()\n", "intent": "> Using `scikit-learn`'s `MultinomialNB()` classifier with default parameters.\n"}
{"snippet": "model.fit(X_train,y_train)\n", "intent": "6.train the model on the training set and check its accuracy on training and test set,\nhow's your model doing? Is the loss growing smaller?\n"}
{"snippet": "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\nc = tf.matmul(a, b)\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nprint(sess.run(c))\n", "intent": "Check TensorFlow uses GPU\n"}
{"snippet": "sparse_matrix = tf.SparseTensor()\nsparse_matrix.values\n", "intent": "** tf.SparseTensor ** - A sparse representation Tensor\nFor sparse Tensors, a more efficient representation is `tf.SparseTensor`.\n"}
{"snippet": "sparse_matrix = tf.sparse.SparseTensor(dense_shape=[3,3], indices=[[0,0],[1,1],[2,2]], values=[1,1,1]) \nsparse_matrix.values\n", "intent": "** tf.SparseTensor ** - A sparse representation Tensor\nFor sparse Tensors, a more efficient representation is `tf.SparseTensor`.\n"}
{"snippet": "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n", "intent": "Run the next cell to train your model and learn the softmax parameters (W,b). \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(64, activation='softmax', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='softmax'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "with tf.Session() as sess:\n    x.initializer.run()\n    y.initializer.run()\n    result = f.eval()\nprint(result)\n", "intent": "One downside of the above code is that we keep repeating `sess.run()`. There is a better way as shown below:  \n"}
{"snippet": "model.fit(x_train, y_train, epochs=200, batch_size=100, verbose=0)\nmodel2.fit(x_train, y_train, epochs=200, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "hidden = activation(torch.mm(features, W1) + B1)\noutput = activation(torch.mm(hidden, W2) + B2)\nprint(output)\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "gm = GaussianMixture(n_components=5, random_state=229)\ngm.fit(X)\n", "intent": "**Gaussian Mixture**\n"}
{"snippet": "clf = IsolationForest(random_state=229)\nclf.fit(X)\nData3[\"isf_score\"] = clf.decision_function(X)\nData3.sort_values(by=\"isf_score\").County[:5]\n", "intent": "**Isolation Forest**\n"}
{"snippet": "outputs = net(Variable(images_copy))\n", "intent": "Now, let's forward these through our trained network and see what our predicted labels are:\n"}
{"snippet": "correct = 0\ntotal = 0\nfor i, data in enumerate(testloader, 0):\n    images, labels = data\n    outputs = net(Variable(images))\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\nprint('Accuracy on all 10000 test images: %d %%' % (100 * correct/total))\n", "intent": "Not bad I guess. Let's compute how we did over the entire test set:\n"}
{"snippet": "variables = [\"Diesel\", \"Petrol\"]\nfor i in range(len(variables)):\n    X = priceDaily[\"Crude Oil\"]\n    y = priceDaily[[variables[i]]]\n    X = sm.add_constant(X)\n    model = sm.OLS(y, X, missing='drop').fit()\n    print('Dependent: ', variables[i])\n    print('      R2: ', model.rsquared)\n", "intent": "We can see a major improvement, with the major peaks lining up. We will now check the R2 values from Linear Regression Summaries.\n"}
{"snippet": "logreg.fit(training_x, training_y)\n", "intent": "<a name=\"20180525_01\"></a>The following won't work, because our $x$ variable has to be a column vector, whereas currently it's a 1d array.\n"}
{"snippet": "logreg.fit(training_x1, training_y)\n", "intent": "And now the logistic regression function works. (Note that we leave $y$ as a 1d-array, and do <i>not</i> transform it into a column vector.)\n"}
{"snippet": "x1 = tf.Variable(1)\nx1.graph is tf.get_default_graph()\n", "intent": "Any node you create is automatically added to the default graph: \n"}
{"snippet": "W = tf.Variable(tf.zeros([flatSize, numberOfClasses]), name=\"Weights\")\nb = tf.Variable(tf.zeros([10]), name=\"Biases\")\n", "intent": "Definim weights si biases:\n"}
{"snippet": "Y_Pred_Prob = tf.nn.softmax(tf.matmul(X_vec, W) + b, name=\"softmax\")\n", "intent": "Activarea pentru layerul FC:\n"}
{"snippet": "with tf.name_scope(\"Layer_1\"):\n    W1 = tf.Variable(tf.truncated_normal([flatSize, sizeLayerOne], stddev=0.1), name=\"Weights\")\n    b1 = tf.Variable(tf.zeros([sizeLayerOne]), name=\"Biases\")\n    Y1 = tf.nn.relu(tf.matmul(X_vec, W1) + b1, name=\"Activation\")\n    Y1_Dropout = tf.nn.dropout(Y1, probKeep)\n", "intent": "Definim arhitectura modelului : \n"}
{"snippet": "with tf.name_scope(\"Output_Layer\"):\n    W5 = tf.Variable(tf.truncated_normal([sizeLayerFour, sizeLayerFive], stddev=0.1), name=\"Weights\")\n    b5 = tf.Variable(tf.zeros([sizeLayerFive]), name=\"Biases\")\n    Y_logits = tf.matmul(Y4_Dropout, W5) + b5\n    Y_Pred = tf.nn.softmax(Y_logits, name=\"Activation\")\n", "intent": "Clasificator / Output layer : \n"}
{"snippet": "with tf.name_scope(\"Layer_1\"):\n    W1 = tf.Variable(tf.truncated_normal([flatSize, sizeLayerOne], stddev=0.1), name=\"Weights\")\n    b1 = tf.Variable(tf.zeros([sizeLayerOne]), name=\"Biases\")\n    Y1 = tf.nn.sigmoid(tf.matmul(X_vec, W1) + b1, name=\"Activation\")\n", "intent": "Definim arhitectura, in termeni de dimensiuni si conexiuni intre W si b\n"}
{"snippet": "with tf.name_scope(\"Output_Layer\"):\n    W5 = tf.Variable(tf.truncated_normal([sizeLayerFour, sizeLayerFive], stddev=0.1), name=\"Weights\")\n    b5 = tf.Variable(tf.zeros([sizeLayerFive]), name=\"Biases\")\n    Y_logits = tf.matmul(Y4, W5) + b5\n    Y_Pred = tf.nn.softmax(Y_logits, name=\"Activation\")\n", "intent": "Clasificatorul final, de output:\n"}
{"snippet": "m = SequenceModel(sequence_extraction, 10, name='lstm38', cell_size=128, layers=1)\nm.setup()\nprint 'built model'\nfor i in xrange(300):\n    m.train(1)\n    m.show_example()\n", "intent": "Epoch 167: accuracy: 98.09375%\n"}
{"snippet": "input = layers.Input(shape=(SIZE, SIZE, 3))\nmodel = applications.InceptionV3(weights='imagenet', include_top=False, input_tensor=input)\n", "intent": "Pretrained `VGG-F` networks aren't available for Keras, so use `InceptionV3` instead.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embedded_seq = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded_seq\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "A = tf.placeholder(tf.float32, shape=(None, 3))\nB = A + 5\nwith tf.Session() as sess:\n    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n", "intent": "Here is another example: \n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier(7)\nknn_clf.fit(X,y)\n", "intent": "**Problem 3b**\nTrain a $k=7$ nearest neighbors machine learning model on the Titanic training set.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "A session object encapsulates the environment in which operation object are executed. Tensorflow objects are evaluated in those operations.\n"}
{"snippet": "predictor.fit(x_train, y_train,\n              batch_size=128,\n              epochs=10000,\n              callbacks=callbacks,\n              validation_data=(x_validation, y_validation))\npredictor.load_weights('best_model')  \n", "intent": "Finally, we fit our model:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1.0, 1.0))\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -0.1, 0.1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model.fit(data.drop(columns='Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "features = df.drop('TARGET CLASS', axis=1)\nscaler.fit(features)\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "lm.fit(X=X_train, y = y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid=param_grid, refit=True, verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "for v in [0., 1e-4, 1e-3, 1e-1, 1.]:\n    print \"Setting reg_lambda to \", v\n    reg_lambda = v\n    tf_non_linear_model = TFNonLinearModel()\n    train_tf_model(tf_non_linear_model, 10000, 1000)\n    wrapper = TFModelWrapper(tf_non_linear_model)\n    plot_decision_boundary(X, wrapper)\n", "intent": "Change the `l2_lambda` parameter and observe the effect it has on the decision boundary. \n**QUESTION**: Can you explain why this happens?\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\nclassifier = tf_learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[10,20,10], n_classes=2)\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "lm.fit(X=x_train,y=y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "history = model.fit(x_train, y_train, epochs=10, batch_size=50)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "dec_tree_classifier = DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1000))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(128))\nmodel.add(Dropout(0.4))\nmodel.add(Activation('relu'))\nmodel.add(Dense(2))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=12, batch_size=50, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "clf = RandomForestClassifier(\n    n_estimators=240,\n    max_depth=120,\n    random_state=0,\n    n_jobs=-1)\nclf.fit(x_train, y_train)\n", "intent": "The best result is achieved with `n_estimators=240` and `max_depth=120`. The accuracy obtained with these parameters is 0.838538.\n"}
{"snippet": "rf_ff = ensemble.RandomForestRegressor()\nrf_ff.fit(X_ff_train, y_ff_train)\nrf_ff.score(X_ff_test, y_ff_test)\n", "intent": "Try using another model (RandomForestRegressor or SVM)\n"}
{"snippet": "for v in [0., 1e-4, 1e-3, 1e-1, 1.]:\n    print(\"Setting reg_lambda to :\" + str(v))\n    reg_lambda = v\n    tf_non_linear_model = TFNonLinearModel()\n    train_tf_model(tf_non_linear_model, 10000, 1000)\n    wrapper = TFModelWrapper(tf_non_linear_model)\n    plot_decision_boundary(X, wrapper)\n", "intent": "Change the `l2_lambda` parameter and observe the effect it has on the decision boundary. \n**QUESTION**: Can you explain why this happens?\n"}
{"snippet": "plot_learning_curve(ensemble.RandomForestClassifier(), \n                    'Random Forest', seed_X, seed_y, fig_opts={'figsize':(14,10)})\n", "intent": "* Run a learning curve against the seed data? How much data do we need to train on?\n"}
{"snippet": "hold_prob = tf.placeholder(tf.float32)\nfull_one_dropot = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "x = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.nn.softmax(tf.matmul(x, W) + b)\n", "intent": "<img src=\"https://www.tensorflow.org/images/softmax-regression-scalargraph.png\" width=\"800\">\n"}
{"snippet": "dtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "model.fit(df.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS',axis=1))\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Import KNeighborsClassifier from scikit learn.**\n"}
{"snippet": "knn.fit(X_train,y_train)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "lm.fit(X = x_train, y = y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "def cell_fn_forward(X, h, model, train=True):\n    Wxh, Whh, Why = model['Wxh'], model['Whh'], model['Why']\n    bh, by = model['bh'], model['by']\n    hprev = h.copy()\n    h, h_cache = tanh_forward(np.dot(hprev, Whh) + np.dot(X, Wxh) + bh)\n    y, y_cache = fc_forward(h, Why, by)\n    cache = (X, Whh, h, hprev, y, h_cache, y_cache)\n    if not train:\n            y = softmax(y)\n    return y, h, cache\n", "intent": "Now we can implement the equation $h_t = \\sigma(W_{hh}h_{t-1} + W_{xh}x_t)$ as follows:\n"}
{"snippet": "clf_s = SVC(C = 100, class_weight = None, gamma = 1.0, probability=True)\nplot_decision_surface(clf_s, subset, y)\nclf_s = SVC(C = 100, class_weight = 'auto', gamma = 1.0, probability=True)\nplot_decision_surface(clf_s, subset, y)\n", "intent": "**(d)** The <a href='http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"}
{"snippet": "layer2_input = layer1.output.flatten(2)\nlayer2 = HiddenLayer(rng,\n                     input=layer2_input,\n                     n_in=num_kernels[1] * edge1 * edge1,\n                     n_out=sigmoidal_output_size,\n                     activation=T.tanh)\n", "intent": "The sigmoidal layer takes a vector as input.\nWe flatten all but the first two dimensions, to get an input of size **`(batch_size, 30 * 4 * 4)`**.\n"}
{"snippet": "layer3 = LogisticRegression(input=layer2.output,\n                            n_in=sigmoidal_output_size,\n                            n_out=10)\n", "intent": "A fully connected logistic regression layer converts the sigmoid's layer output to a class label.\n"}
{"snippet": "validate_model = theano.function(\n        [index],\n        layer3.errors(y),\n        givens={\n            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n        }\n    )\n", "intent": "To track progress on a held-out set, we count the number of misclassified examples in the validation set.\n"}
{"snippet": "test_model = theano.function(\n    [index],\n    layer3.errors(y),\n    givens={\n        x: test_set_x[index * batch_size: (index + 1) * batch_size],\n        y: test_set_y[index * batch_size: (index + 1) * batch_size]\n    }\n)\n", "intent": "After training, we check the number of misclassified examples in the test set.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embed_out = tf.nn.embedding_lookup(embedding,input_data)\n    return embed_out\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(nb_filter=32, nb_row=3, nb_col=3, border_mode='valid', input_shape=(32, 32,3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "model = LogisticRegression()\n", "intent": "---\nProblem 6\n---------\nTrain a logistic regressor on the image data using 50, 100, 1000 and 5000 training samples. \n"}
{"snippet": "regr = LinearRegression() \nregr.fit(X_train, y_train) \n", "intent": "d) Train model on training data, and make predictions on testing data\n"}
{"snippet": "def hidden_model(x):\n    result = x[:, 5] + x[:, 10]\n    result += np.random.normal(0, .005, result.shape)\n    return result\ndef make_x(nobs):\n    return np.random.uniform(0, 3, (nobs, 10 ** 6))\nx = make_x(20)\ny = hidden_model(x)\nprint(x.shape)\n", "intent": "Let's make the dataset, and compute the y's with a \"hidden\" model that we are trying to recover\n"}
{"snippet": "clf=DecisionTreeClassifier()\nclf.fit(feature,label)\n", "intent": "Fitting the data set to DECISION TREE CLASSIFIER\n"}
{"snippet": "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']),\\\n    biases['hidden_layer'])\nlayer_1 = tf.nn.relu(layer_1)\nlogits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n", "intent": "<img src=\"images/multi-layer.png\" style=\"height: 50%;width: 50%; position: relative; right: 5%\">\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform(shape=(vocab_size, embed_dim), minval= -0.1, maxval=0.1, dtype=tf.float32, \n                                              name='embeddings'))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "class PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self,epoch,logs):\n    if epoch % 100 == 0: print('')\n    print('.', end='')\nEPOCHS = 500\nhistory = model.fit(train_data, train_labels, epochs=EPOCHS,\n                    validation_split=0.2, verbose=0,\n                    callbacks=[PrintDot()])\n", "intent": "The model is trained for 500 epochs, and record the training and validation accuracy in the `history` object.\n"}
{"snippet": "smaller_model = keras.Sequential([\n    keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(10000,)),\n    keras.layers.Dense(4, activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\nsmaller_model.compile(optimizer='adam',\n                loss='binary_crossentropy',\n                metrics=['accuracy', 'binary_crossentropy'])\nsmaller_model.summary()\n", "intent": "Let's create a model with less hidden units to compare against the baseline model that we just created:\n"}
{"snippet": "smaller_history = smaller_model.fit(train_data,\n                                    train_labels,\n                                    epochs=20,\n                                    batch_size=512,\n                                    validation_data=(test_data, test_labels),\n                                    verbose=2)\n", "intent": "And train the model using the same data:\n"}
{"snippet": "bigger_history = bigger_model.fit(train_data, train_labels,\n                                  epochs=20,\n                                  batch_size=512,\n                                  validation_data=(test_data, test_labels),\n                                  verbose=2)\n", "intent": "And, again, train the model using the same data:\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=x_train.shape[1]))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.35))\nmodel.add(Dense(256))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(num_classes))\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "xt = x[:, best_features]\nclf = LinearRegression().fit(xt, y)\nprint(\"Score is \", clf.score(xt, y))\n", "intent": "A linear regression on the full data looks good. The \"score\" here is the $R^2$ score -- scores close to 1 imply a good fit.\n"}
{"snippet": "net = sklearn.linear_model.Perceptron(n_iter=1, warm_start=True)\n", "intent": "Create a [perceptron object](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html).\n"}
{"snippet": "def conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n", "intent": "Define the two handy functions to convolve and pool features:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.25))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf = GridSearchCV(SVC('rbf'), \n                   param_grid=parameters_grid, \n                   cv=cv,      \n                   n_jobs=-1)  \nclf.fit(X, y)\n", "intent": "Then give `GridSearchCV` the ML model and the grid of its hyperparameters and fit it on the data.\n"}
{"snippet": "db = DBSCAN(eps=3, min_samples=3, n_jobs=-1)\ndb.fit(Xs)\n", "intent": "Remember to pass an `eps` and `min_samples` of your choice.\n"}
{"snippet": "lr = LinearRegression()\nmodel1 = lr.fit(X_train_ss, y_train)\ntrain_score = model1.score(X_train_ss, y_train)\nprint('model score on training data: ', train_score)\n", "intent": "---\nCross-validate the $R^2$ of an ordinary linear regression model with 10 cross-validation folds.\nHow does it perform?\n"}
{"snippet": "rdge = Ridge(alpha=ridge_alpha)\nrdge.fit(X_train_ss, y_train)\nrdge_train = rdge.score(X_train_ss, y_train)\nrdge_test = rdge.score(X_test_ss, y_test)\nprint('model score for training dataset is ', rdge_train)\nprint('model score for test dataset is ', rdge_test)\n", "intent": "---\nIs it better than the Linear regression? If so, why might this be?\n"}
{"snippet": "lasso = Lasso(lasso_alpha)\nlasso.fit(X_train_ss, y_train)\nlasso_train = lasso.score(X_train_ss, y_train)\nlasso_test = lasso.score(X_test_ss, y_test)\nprint('lasso model score for training is ', lasso_train)\nprint('lasso model score for test is ', lasso_test)\n", "intent": "---\nIs it better than the Linear regression? Is it better than Ridge? What do the differences in results imply about the issues with the dataset?\n"}
{"snippet": "df['MDEV'] = y\nmodel = sm.formula.ols(formula=\"MDEV ~ LSTAT + NOX + CHAS -1\", data=df)\nresults = model.fit()\nresults.summary()\n", "intent": "We will walk through this after practice.  (ie: formula='Lottery ~ Literacy + Wealth + Region', data=df)\n"}
{"snippet": "smaller_frame=census_data[['educ_coll', 'average_income', 'per_vote']]\nfrom pandas.tools.plotting import scatter_matrix\naxeslist=scatter_matrix(smaller_frame, alpha=0.8, figsize=(12, 12), diagonal=\"kde\")\nfor ax in axeslist.flatten():\n    ax.grid(False)\n", "intent": "We use a SPLOM to visualize some columns of this dataset. In Panda's the SPLOM is a one-liner.\n"}
{"snippet": "enet.fit(Xn,y)\n", "intent": "---\nHow does it compare to the other regularized regressions?\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, cross_val_predict\nfrom sklearn import metrics\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport seaborn as sns\nfrom sklearn import datasets, linear_model\nlm = linear_model.LinearRegression()\n", "intent": "---\nCross-validate the $R^2$ of a linear regression model with 10 cross-validation folds.\nHow does it perform?\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nX = sample[['number_project']].values\ny = sample['satisfaction_level'].values\nlr.fit(X, y)\norig_int = lr.intercept_\norig_coef = lr.coef_[0]\nprint lr.intercept_, lr.coef_\n", "intent": "**Build a regression predicting satisfaction level from number of projects using the sample.**\nPrint out the intercept and coefficient.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(400, activation='sigmoid', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='sigmoid'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          epochs=10,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from keras.regularizers import l2\nmodel = Sequential()\nmodel.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.), input_dim=x_train.shape[1]))\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=x_train.shape[0],\n          epochs=100,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\npipeline = build_pipeline(DecisionTreeClassifier(criterion=\"entropy\"), x_train, y_train)\nclassifier= pipeline.get_params()['classifier']\ncount_vectorizer = pipeline.get_params()['count_vectorizer']\ndot_data = export_graphviz(classifier, out_file=None,max_depth=3,\n                           feature_names=count_vectorizer.get_feature_names(),class_names=class_names) \ngraph = graphviz.Source(dot_data) \ngraph\n", "intent": "Now we're ready to build and visualize the decision tree.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)  \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf = KNeighborsClassifier(20, warn_on_equidistant=False).fit(Xtrain, ytrain)\npoints_plot(Xtr, Xtrain, Xtest, ytrain, ytest, clf)\n", "intent": "We do kNN with 20 neighbors. kNN \n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 50)\nrf.fit(X, y)\n", "intent": "Fit RF model on data and visualize it\n"}
{"snippet": "ds = DecisionTreeClassifier(max_depth=1)\nada = AdaBoostClassifier(n_estimators=300)\n", "intent": "Compare and contrast Decision Trees and AdaBoost\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape=(n_cols,)))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(2, activation = \"softmax\"))\nmodel.compile(optimizer=\"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n", "intent": "This is a very simple model, it only has one shallow layer. Let's add some more layers.\n"}
{"snippet": "model.fit(X, y_binary, epochs=40, validation_split = 0.25)\n", "intent": "We're trained a really good model, but principles of cross validation also to deep learning. Here's how we'll evaluate the model on a testing data.\n"}
{"snippet": "knn3 = KNeighborsClassifier(n_neighbors=3)\nknn3.fit(X,y)\nscore3 = float(knn3.score(X,y))\nprint \"The model accurately labelled {:.2f} percent of the data\".format(score3*100)\n", "intent": "Train a KNN model using 3 neighbors\n"}
{"snippet": "knn5 = KNeighborsClassifier(n_neighbors=5)\nknn5.fit(X,y)\nscore5 = float(knn5.score(X,y))\nprint \"The model accurately labelled {:.2f} percent of the data\".format(score5*100)\n", "intent": "Now with 5 neighbors\n"}
{"snippet": "randsearch_cv = RandomizedSearchCV(pipe_cv, n_iter = 5,\n                        param_distributions = param_grid_cv, cv = 5, scoring = \"accuracy\")\nt = time()\nrandsearch_cv.fit(X, y)\nprint time() - t\n", "intent": "Countvectorizer randomized search\n"}
{"snippet": "randsearch_tf = RandomizedSearchCV(pipe_tf, n_iter = 10,\n                        param_distributions = param_grid_tf, cv = 5, scoring = \"accuracy\")\nt = time()\nrandsearch_tf.fit(X, y)\nprint time() - t\n", "intent": "Tfidfvectorizer randomized search\n"}
{"snippet": "lm.fit(X_train, Y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "clf = KNeighborsClassifier(1, warn_on_equidistant=False).fit(Xtrain, ytrain)\npoints_plot(Xtr, Xtrain, Xtest, ytrain, ytest, clf)\n", "intent": "What if we decide to get ultra local. We get high variance and Jagged islands.\n"}
{"snippet": "model = Sequential([\n    Dense(512, input_dim=num_pixels, activation='relu'),\n    Dropout(0.5), \n    Dense(512, activation='relu'),\n    Dropout(0.2), \n    Dense(num_classes, kernel_initializer='normal', activation='softmax')\n])\nmodel.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\nmodel.summary()\n", "intent": "W tym przypadku dwa razy.\n"}
{"snippet": "model = Sequential([\n    LSTM(32, input_shape=(1, 1)),\n    Dense(1)\n])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n", "intent": "Wracamy do naszej sieci.\n"}
{"snippet": "model.fit(x_train, y_train, \n          batch_size=32,\n          epochs=10, \n          validation_data=(x_test, y_test),\n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "kmeans_2 = KMeans(n_clusters=2, random_state=0)\nmodel_2 = kmeans_2.fit(data)\nlabels_2 = model_2.labels_\nh.plot_data(data, labels_2)\n", "intent": "`4.` Now try again, but this time fit kmeans using 2 clusters instead of 4 to your data.\n"}
{"snippet": "kmeans_7 = KMeans(n_clusters=7, random_state=0)\nmodel_7 = kmeans_7.fit(data)\nlabels_7 = model_7.labels_\nh.plot_data(data, labels_7)\n", "intent": "`5.` Now try one more time, but with the number of clusters in kmeans to 7.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X, y)\n", "intent": "Now we will fit a model to figure out which variables drive user retention. We will use the random forest model.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    with tf.name_scope(name='embedding'):\n        embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1, name='embeding_w'))\n        embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "happyModel.fit(x = X_train,y = Y_train,epochs=40,batch_size=50)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "W_fc2 = weight_variable([1024, labels_count])\nb_fc2 = bias_variable([labels_count])\ny = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n", "intent": "Softmax layer, the same one if we use just a simple softmax regression.\n"}
{"snippet": "clf = KNeighborsClassifier(35, warn_on_equidistant=False).fit(Xtrain, ytrain)\npoints_plot(Xtr, Xtrain, Xtest, ytrain, ytest, clf)\n", "intent": "You do it for 35 now..see what happens?\n"}
{"snippet": "svc = LinearSVC(C = 1)\nt=time.time()\nsvc.fit(X_train, y_train)\nt2 = time.time()\nprint(round(t2-t, 2), 'Seconds to train SVC...')\nprint('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n", "intent": "The best C parameter was found to be 4 since it resulted in the fastest training time.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    return tf.nn.embedding_lookup(embeding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "weights = lasagne.layers.get_all_params(nn,trainable=True)\n", "intent": "* The standard way:\n * prediction\n * loss\n * updates\n * training and evaluation functions\n"}
{"snippet": "model = KMeans(n_clusters=5,random_state=0).fit(X)\nmodel.cluster_centers_.shape\n", "intent": "In case I need to revisit later!\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nparam_grid = [{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n              {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n             ]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\nscoring='neg_mean_squared_error')\ngrid_search.fit(housing_prepared, housing_labels)\ngrid_search.best_params_\n", "intent": "The RandomForestRegressor looks promosing. Lets do some grid searching to fine tune it a little!\n"}
{"snippet": "reset_graph()\nn_inputs = 2\nX = tf.placeholder(tf.float32, shape=(None, n_inputs +1), name='X')\ny = tf.placeholder(tf.float32, shape=(None, 1), name='y')\ntheta = tf.Variable(tf.random_uniform([n_inputs + 1,1],-1.0,1.0,seed=42),name='theta')\nlogits = tf.matmul(X,theta,name='logits')\ny_proba = tf.sigmoid(logits)\n", "intent": "Try building the sigmoid function from scratch, and using the TF built in function:\n"}
{"snippet": "init = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    test2 = y_proba.eval(feed_dict={X:X_train,y:y_train})\nprint(test2.shape)\nprint(test2[:5])\n", "intent": "Good start, lets test this out to see if its working so far:\n"}
{"snippet": "K.clear_session()\nnp.random.seed(1)\nsess = K.get_session()\nyolo = load_model(\"model_data/yolo.h5\")\n", "intent": "Build `yolo_head` func, which takes in model output and converts it to...\n"}
{"snippet": "model.fit([Xoh, s0, c0], outputs, epochs=10, batch_size=100)\n", "intent": "Let's now fit the model and run it for one epoch.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ndc = DecisionTreeClassifier()\ndc.fit(features_train_std, labels_train)\n", "intent": "We'll use Random Forest classification algorithm\n"}
{"snippet": "from sklearn.datasets import fetch_mldata\nmnist = fetch_mldata('MNIST original')\nrnd_clf = RandomForestClassifier(random_state=42)\nrnd_clf.fit(mnist[\"data\"], mnist[\"target\"])\n", "intent": "**Let's try to calculate feature importances for the MNIST dataset!**\n"}
{"snippet": "from sklearn.svm import SVC\nclf_svc = SVC(kernel='rbf', C=3, gamma=0.5, probability=True)\nclf_svc.fit(X_imbal,y_imbal)\nplot_decision_boundary(clf_svc, X_imbal, y_imbal, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True)\n", "intent": "**Build a somewhat regularized model (low C and gamma params) so that we misclassify many of the 1s (the unbalanced class)**\n"}
{"snippet": "import tensorflow as tf\nx = tf.Variable(3, name='x')\ny = tf.Variable(4, name='y')\nf=x*x*y +y + 2\n", "intent": "The following code creates the graph represented below:\n"}
{"snippet": "x1 = tf.Variable(1)\nx1.graph is tf.get_default_graph()\n", "intent": "Any node you create is automatically added to the default graph.\n"}
{"snippet": "w = tf.constant(3)\nx = w + 2\ny = x + 5\nz = x * 3\nwith tf.Session() as sess:\n    print(y.eval()) \n    print(z.eval()) \n", "intent": "When you evaluate a node, TF determines set of nodes that it depends on and it evaluates these first.\n"}
{"snippet": "model = w2v_model(h_size=10, v_size=V, context_size=5)\nmodel.fit(sentences, num_neg_samples=10, learning_rate=1e-4, momentum=0.99, reg=0.1, epochs=10, plot_data=True)\n", "intent": "Word embeddings will be trash, but we want to make sure it is atleast running!\n"}
{"snippet": "df_1 = df.drop('Private',axis=1)\nkmeans.fit(df_1)\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "sc.fit(df.drop('TARGET CLASS',axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "from sklearn.svm import SVC\nclf = SVC(kernel='linear')\nclf.fit(X,y)\n", "intent": "**Let's Try**\nFit the model:\n"}
{"snippet": "clf = GaussianNB()\nclf.fit(features_train_std, labels_train)\n", "intent": "Fit the training data and target/label data.  Again, the training data is the part \n"}
{"snippet": "model_RAND   = compile_keras_sequential_model(model_layers_RAND, \"RAND\")\nmodel_LAST   = compile_keras_sequential_model(model_layers_LAST, \"LAST\")\nmodel_LAST2  = compile_keras_sequential_model(model_layers_LAST2, \"LAST2\")\nmodel_LINEAR = compile_keras_sequential_model(model_layers_LINEAR, \"LINEAR\")\nmodel_DNN    = compile_keras_sequential_model(model_layers_DNN, \"DNN\")\nmodel_CNN    = compile_keras_sequential_model(model_layers_CNN, \"CNN\")\nmodel_RNN    = compile_keras_sequential_model(model_layers_RNN, \"RNN\")\nmodel_RNN_N  = compile_keras_sequential_model(model_layers_RNN_N, 'RNN_N')\n", "intent": "<a name=\"benchmark\"></a>\nBenchmark all the algorithms. This takes a while (approx. 10 min).\n"}
{"snippet": "Linear_Model = LinearRegression()\n", "intent": "Age(y) = B0 + B1NO + B2Nt +B3pH + B4PetalCount\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\ngsm = GridSearchCV(LogisticRegressionCV(solver='liblinear'), {\n                                                                'Cs':np.arange(2,12,1), \n                                                                'cv':np.arange(2,17,1), \n                                                                'penalty':['l1','l2']\n                                                             }, verbose=1, n_jobs=-1)\ngsm.fit(X, y_ravel)\ngsm.best_estimator_\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\ngsm = GridSearchCV(LogisticRegressionCV(solver='liblinear'), {'Cs':np.arange(2,12,1), 'cv':np.arange(2,17,1), 'penalty':['l1','l2']}, verbose=1, n_jobs=-1)\ngsm.fit(X, y_ravel)\ngsm.best_estimator_\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nmodel2 = GridSearchCV(LogisticRegressionCV(solver='liblinear'), {'Cs':np.arange(2,12,1), 'cv':np.arange(2,17,1), 'penalty':['l1','l2']}, verbose=True)\nmodel2.fit(Xtr, ytr)\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embed = tf.contrib.layers.embed_sequence(input_data, vocab_size=vocab_size, embed_dim=embed_dim)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "tf.logging.set_verbosity(tf.logging.INFO)\nclassifier.fit(\n    input_fn = train_input_fn,\n    steps = 100,\n    monitors = [logging_hook, validation_monitor]\n)\n", "intent": "Let its trainnnn !!!\n"}
{"snippet": "model = Sequential()\nmodel.add(Conv2D(filters=10, kernel_size=3, strides=1, padding=\"same\", activation='relu', input_shape=(28,28,1)))\nmodel.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))\nmodel.add(Conv2D(filters=10, kernel_size=3, strides=1, padding=\"same\", activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))\nmodel.add(Flatten())\nmodel.add(Dense(units=10, activation=\"softmax\"))\nadam = Adam(lr=1e-3)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n", "intent": "Similar model as Tensorflow\nCONV3 - MAX POOL - CONV3 - MAX POOL - FC\n"}
{"snippet": "sess, weights = train_perceptron(X, y)\n", "intent": "Obtain weights and the TensorFLow session variable\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_jobs=1)\nrfc.fit(features_train_std, labels_train)\n", "intent": "We'll use Random Forest classification algorithm\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1) \n", "intent": " - ESTIMATOR is scikits term for model (estimate unknown quantities)\n- \"INSTANTIATE means make an instance of\"\n"}
{"snippet": "knn.fit(X,y)\n", "intent": "    -  model is learning the relationship btwn X and Y \n      - occurs inplace \n"}
{"snippet": "import sklearn.ensemble as sk\nclf = sk.RandomForestClassifier(n_estimators=100, oob_score=True,min_samples_split=5, min_samples_leaf= 2)\nclf = clf.fit(x_train, y_train.income)\n", "intent": "<strong>Random Forest</strong>\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_normal([vocab_size, embed_dim]))\n    embeddings = tf.nn.embedding_lookup(embeddings, input_data)\n    return embeddings\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform([vocab_size,embed_dim], -1.0,1.0))\n    embed = tf.nn.embedding_lookup(embeddings,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform([vocab_size,embed_dim], -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embeddings,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf1 = svm.SVC(gamma=0.1,C =1)\nclf2 = svm.SVC(gamma=0.1,C =1)\n", "intent": "defining the function\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable( tf.random_uniform((vocab_size, embed_dim), -1, 1) )\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "gmm = GaussianMixture(n_components=5,covariance_type='full',init_params='kmeans') \ngmm.fit(X_sv)\nX_grid = np.array([x.flatten(),y.flatten()]).T\nz = np.exp(gmm.score_samples(X_grid).reshape(x.shape)) \n", "intent": "---\nWhat do we get if we change the number of components to 5?\n"}
{"snippet": "clf = GaussianNB()\nclf.fit(features, labels)\n", "intent": "Fit the training data and target/label data.  Again, the training data is the part \n"}
{"snippet": "a = np.asarray([[1, 2, 3], [4, 5, 6]], dtype=np.float64)\nb = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nprint(a + b)\nprint(a - b)\nprint(a * b)\nprint(a / b)\nprint(a % b)\nprint(a**b)\n", "intent": "But this is where it get crazy...\n"}
{"snippet": "a = torch.rand(3)\nb = torch.autograd.Variable(a)\nprint(a)\nprint(b)\n", "intent": "To make `pytorch` track which operations are performed on a `torch.Tensor`, we have to make it into a **variable**:\n"}
{"snippet": "sess = tf.Session()\n", "intent": "First create a new session\n"}
{"snippet": "model2 = Sequential()\nmodel2.add(Dense(units=256,input_shape=X_dtm.shape[1:]))\nmodel2.add(Dropout(0.5))\nmodel2.add(Activation('relu'))\nmodel2.add(BatchNormalization())\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(units=24))\nmodel2.add(Activation('sigmoid'))\n", "intent": "The input shape for the neural network will be the X_dtm shape\nWe will use a relu activation. The output will be the categories\n"}
{"snippet": "clf = OneVsRestClassifier(RandomForestClassifier(n_jobs=-1, n_estimators=260, max_depth=10,random_state=42,bootstrap=False,max_features=10))\n", "intent": "After RandomForest tunning, the number of estimators that maximizes the AUC are 260.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nclassifier_DTC = DecisionTreeClassifier(criterion='gini', random_state = 0)\nclassifier_DTC.fit(X_train, y_train)\n", "intent": "<a id=\"decision_tree_classifier\"></a>\n"}
{"snippet": "def new_model(d_in=X.shape[1], d_hidden=20, d_out=(y.max() + 1)):\n    print(d_in, d_hidden, d_out)\n    model = {}\n    model[\"W1\"] = np.random.rand(d_in, d_hidden) \n    model[\"b1\"] = np.zeros(d_hidden) \n    model[\"W2\"] = np.random.rand(d_hidden, d_out) \n    model[\"b2\"] = np.zeros(d_out) \n    return model\n", "intent": "Now let's define a model. It will be a two-layer fully connected neural network with ReLU activation.\n"}
{"snippet": "def relu(activation):\n    return activation * (activation > 0)\ndef accuracy(y_pred, y_actual):\n    accurate = (np.argmax(y_pred, axis=1) == np.argmax(y_actual, axis=1)).sum()\n    return accurate / y_pred.shape[0]\ndef to_onehot(y_labels):\n    onehot = np.zeros((y_labels.shape[0], y_labels.max() + 1))\n    onehot[np.arange(y_labels.shape[0]), y_labels] = 1\n    return onehot\n", "intent": "We'll also define some utility functions:\n"}
{"snippet": "neural_regressor = MLPRegressor(verbose=True,\n                                hidden_layer_sizes=(10, 10),\n                               learning_rate_init=0.01,\n                               max_iter=100,\n                               tol=-100)\nneural_regressor.fit(X_train, y_train)\n", "intent": "And a neural network:\n"}
{"snippet": "feature_cols = ['TV', 'Radio', 'Newspaper']\nX = data[feature_cols]\ny = data.Sales\nlinreg = LinearRegression()\nlinreg.fit(X, y)\nprint(linreg.intercept_)\nprint(linreg.coef_)\n", "intent": "Let's redo some of the Statsmodels code above in scikit-learn:\n"}
{"snippet": "hidden_dim = 256\nlstm_layers_no = 3\nmodel = CharacterModel(hidden_dim, lstm_layers_no=lstm_layers_no).type(dtype)\nloss_fun = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\nmodel\n", "intent": "Having defined the model, we can initialize it. Feel free to play with the hyperparameters!\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\nlinear_reg.fit(X_train, y_train)\n", "intent": "It's a bunch of ones and zeros! Wouldn't it make sense to just train a linear regressor on the data?\n"}
{"snippet": "def relu(activation):\n    return activation * tf.cast((activation > 0), dtype=tf.float32)\n", "intent": "A computational graph is made of:\n* placeholders (inputs to the graph)\n* variables\n* operations on them and their results\n"}
{"snippet": "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\ntrain_dict = {X: X_train, y: y_train}\nnum_iterations = 500\nwith tf.Session() as sess:\n    with tf.device(\"/gpu:0\"): \n        tf.global_variables_initializer().run()\n        for i in range(num_iterations):\n            train_step.run(feed_dict=train_dict)\n            loss_val = loss.eval(feed_dict=train_dict)\n            if i % 50 == 0: print(loss_val)\n", "intent": "Let's train again, but without perfoming Gradient Descent manually\n"}
{"snippet": "def conv_block(out_channels: int):\n  return k.models.Sequential([\n      k.layers.Conv2D(out_channels, 3, padding='same', activation='relu'),\n      k.layers.BatchNormalization(),\n      k.layers.Dropout(0.3)\n  ])\n", "intent": "Firts, let's define our own layer:\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nrid = Ridge()\nparams = {'alpha':[0.1, 0.5, 1,2,5, 100]}\ncv_rid = GridSearchCV(rid,params)\ncv_rid.fit(X_train,y_train)\ncv_rid.best_params_\n", "intent": "Ridge model has parameter *alpha* we can tune, which controls the strength of the regularization; use GridSearchCV to try to optimize:\n"}
{"snippet": "from sklearn.linear_model import Lasso\nlas = Lasso(max_iter=4000)\nparams = {'alpha':[0.1, 0.5, 1,2,10, 100]}\ncv_las = GridSearchCV(las,params)\ncv_las.fit(X_train,y_train)\ncv_las.best_params_\n", "intent": "In Lasso, some of the coefficients can actually be set to zero.\n"}
{"snippet": "from keras.callbacks import ModelCheckpoint\nh=network.fit(train_images_1d, \n              train_labels, \n              epochs=5, \n              batch_size=128, \n              shuffle=True, \n              callbacks=[ModelCheckpoint('tutorial_MNIST.h5',save_best_only=True)])\n", "intent": "Training the network does not take long and can easily be done quickly on a CPU. Validation accuracy quickly rises to  about 99%.\n"}
{"snippet": "params_0 = {\n    'scale_pos_weight': [x for x in range(1,11)]\n}\ngs_0 = GridSearchCV(\n    estimator=model_v0, \n    param_grid=params_0,\n    scoring=scoring, \n    cv=cv_sets, \n    refit='Accuracy')\ngs_0.fit(X_train, y_train)\n", "intent": "Tuning scale_pos_weight parameter.\n"}
{"snippet": "lm = smf.ols(formula='Sales ~ TV + Radio + Newspaper + IsLarge', data=data).fit()\nlm.params\n", "intent": "Let's redo the multiple linear regression and include the **IsLarge** predictor:\n"}
{"snippet": "sm.OLS(y_2015, X_2015).fit().summary()\n", "intent": "The 2015 model actually applies pretty well for every year. I feel confident in applying it to my 2016 data.\n"}
{"snippet": "y, X = patsy.dmatrices('high_paid ~ rating_cat', data=jobs_w_salary)\nsm.Logit(y, X).fit().summary()\n", "intent": "In fact, the pseudo-r2 is even better when it's a categorized variable!\n"}
{"snippet": "sm.Logit(y, X).fit().summary()\n", "intent": "All but parent_child and fare are statistically significant. I will remove these to see how it compares.\n"}
{"snippet": "logreg_parameters = {\n                     'penalty' : ['l1','l2'],\n                     'C' : [.00001, .0001, .001, .01, .1, 1, 5, 10, 100, 1000, 10000]              \n                    }\nbest_log_reg = model_selection.GridSearchCV(log_reg, logreg_parameters)\nbest_log_reg.fit(X, y)\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "neigh = best_knn.best_estimator_\nneigh.fit(X, y)\n", "intent": "*af*\n*You probably want to use logistic regression when you have dummy variables*\n"}
{"snippet": "logreg_parameters = {\n    'penalty' : ['l1','l2'],\n    'C' : [.00001, .0001, .001, .01, .1, 1, 5, 10, 100, 1000, 10000]              }\nbest_log_reg = model_selection.GridSearchCV(log_reg, logreg_parameters)\nbest_log_reg.fit(X, y)\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "model_sgdrA = linear_model.SGDRegressor()\nmodel_sgdrB = linear_model.SGDRegressor()\nmodel_sgdrC = linear_model.SGDRegressor()\nmodel_sgdrD = linear_model.SGDRegressor()\nmodel_sgdrE = linear_model.SGDRegressor()\n", "intent": "- Stochastic Gradient Descent Regressor\n"}
{"snippet": "model_lrA = linear_model.LinearRegression()\nmodel_lrB = linear_model.LinearRegression()\nmodel_lrC = linear_model.LinearRegression()\nmodel_lrD = linear_model.LinearRegression()\nmodel_lrE = linear_model.LinearRegression()\n", "intent": "- Linear Regression\n"}
{"snippet": "model_rfrA = ensemble.RandomForestRegressor()\nmodel_rfrB = ensemble.RandomForestRegressor()\nmodel_rfrC = ensemble.RandomForestRegressor()\nmodel_rfrD = ensemble.RandomForestRegressor()\nmodel_rfrE = ensemble.RandomForestRegressor()\n", "intent": "- Random Forest Regressor\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Step 2:** \"Instantiate\" the \"estimator\"\n- \"Estimator\" is scikit-learn's term for \"model\"\n- \"Instantiate\" means \"make an instance of\"\n"}
{"snippet": "model_final = ensemble.RandomForestRegressor()\n", "intent": "**Random Forest Regressor**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, input_dim=1000))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "kmeans = KMeans(n_clusters=12)\nmodel = kmeans.fit(X)\nprint(\"Model\\n\", model)\ntype(kmeans)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nUse k-Means Clustering\n<br><br></p>\n"}
{"snippet": "kmeans.fit(College_Data.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "sigma_e_a = np.linspace(1.0, 30, num=100)\nbeta0_hat_a =np.zeros(np.size(sigma_e_a))\nbeta1_hat_a =np.zeros(np.size(sigma_e_a))\nN = 1000\nfor i, sigma_e in enumerate(sigma_e_a):\n    X, Y = GenerateDataLinearFun(N, sigma=sigma_e)\n    beta0_hat_a[i], beta1_hat_a[i] = FitLinearModel(X,Y)\n", "intent": "Obviously there is a $1/n$ relationship \nNow let's look at the relationship wrt to size of the error ($\\epsilon$)\n"}
{"snippet": "dt = DecisionTreeRegressor(max_depth=3)\n", "intent": "Let's start with a very shallow tree which we can visualize. Again we are using the exact same sklearn API.\n"}
{"snippet": "dt = DecisionTreeRegressor()\n", "intent": "Now let's actually build a complicated tree.\n"}
{"snippet": "dt = DecisionTreeRegressor(min_samples_leaf=200)\n", "intent": "To combat this we can set the minimum number of samples in each final leaf to a higher value.\n"}
{"snippet": "rf = RandomForestRegressor(n_estimators=10, n_jobs=-1)\n", "intent": "Now we are fitting several decision trees. This can be done in parallel by several processors. `n_jobs = -1` uses all available processors.\n"}
{"snippet": "from pyspark.ml.classification import GBTClassifier\ngb = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\ngbModel = gb.fit(trainingData)\n", "intent": "1. Train a GBTClassifier on the training data, call the trained model 'gbModel'\n"}
{"snippet": "rf = RandomForestRegressor(n_estimators=40, n_jobs=-1, min_samples_leaf=10)\n", "intent": "The number of trees. \n"}
{"snippet": "lr = LinearRegression()\nlr.fit(X_train, y_train)\n", "intent": "Now we proceed as before.\n"}
{"snippet": "rf = RandomForestRegressor(n_estimators=40, n_jobs=-1, min_samples_leaf=10)\n", "intent": "We already know that we can probably do better with a random forest. Se let's simply try our setting from the previous notebook.\n"}
{"snippet": "cnn = Sequential([\n    Conv2D(32, kernel_size=5, strides=2, activation='relu', padding='same', \n           input_shape=(28, 28, 1)),\n    Conv2D(64, kernel_size=3, strides=2, activation='relu', padding='same'),\n    Conv2D(128, kernel_size=3, strides=2, activation='relu', padding='same'),\n    Flatten(),\n    Dense(10, activation='softmax'),\n])\n", "intent": "To take advantage of the spatial information in images we can use convolutions rather than fully connected layers.\nhttp://setosa.io/ev/image-kernels/\n"}
{"snippet": "model = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same', input_shape=(CHANNELS, ROWS, COLS), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation = 'softmax'))\nmodel.summary()\n", "intent": "A simple convolutional network to control the lander.\n"}
{"snippet": "model1 = Sequential()\nmodel1.add(Conv2D(32, (3, 3), padding='same', input_shape=(CHANNELS, ROWS, COLS), activation='relu'))\nmodel1.add(MaxPooling2D(pool_size=(2, 2)))\nmodel1.add(Conv2D(48, (3, 3), padding='same', activation='relu'))\nmodel1.add(MaxPooling2D(pool_size=(2, 2)))\nmodel1.add(Flatten())\nmodel1.add(Dense(128, activation='relu'))\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(num_classes, activation = 'softmax'))\nmodel1.summary()\n", "intent": "CNN model - 2 layers\n"}
{"snippet": "clf1 = SuperLearnerClassifier()\nsuperlearner1 = clf1.fit(X_train_plus_valid, y_train_plus_valid)\n", "intent": "Case -1 label based training data, Stack layer classifier is Decision Tree\n"}
{"snippet": "my_model = SuperLearnerClassifier()\nmy_model.fit(X_train, y_train)\n", "intent": "Train a Super Learner Classifier using the prepared dataset\n"}
{"snippet": "scaler.fit(bank.drop('Class', axis = 1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "gbCv = CrossValidator(estimator=gb, estimatorParamMaps=gbParamGrid, evaluator=evaluator, numFolds=2)\ngbCvModel = gbCv.fit(trainingData)\n", "intent": "1. Perform cross validation of params on your 'gb' model.\n1. Print out the best params you found.\n"}
{"snippet": "kmeans = KMeans(n_clusters = 2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "kmeans.fit(df.drop('Private', axis = 1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), name='embedings'))\n    embed = tf.nn.embedding_lookup(embedings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "gs_cv = GridSearchCV(pipe, params, n_jobs=-1,\n                    cv=ShuffleSplit(n_splits=20, test_size=0.2, train_size=0.8))\ngs_cv.fit(x, y, groups)\n", "intent": "**Note:** We first use `ShuffleSplit`, which is supposed to used on i.i.d samples, to show how super accuracy is acquired.\n"}
{"snippet": "model_conv2 = tf.keras.models.Model(inputs=images, outputs=output)\nsgd_optimizer = tf.keras.optimizers.SGD(lr=0.01, momentum=0.99, decay=0.005, nesterov=True)\nmodel_conv2.compile(loss='sparse_categorical_crossentropy', optimizer=sgd_optimizer, metrics=['accuracy'])\n", "intent": "    - Define a batch generator\n    - Use it in the train process\n"}
{"snippet": "lrmodel = LinearRegression()\nlrmodel.fit(x, y)\n", "intent": "Using scikit-learn or statsmodels, build the necessary models for your scenario. Evaluate model fit.\n"}
{"snippet": "logreg = smf.logit(formula, data=su)\nlogreg_results = logreg.fit()\nprint logreg_results.summary()\n", "intent": "And print out the results as shown in the example.\n---\n"}
{"snippet": "x = tf.constant(5) + tf.constant(2)\ny = tf.constant(10) - tf.constant(4)\nz = tf.constant(2) * tf.constant(5)\nwith tf.Session() as sess:\n    print(sess.run([x, y, z]))\n", "intent": "You can also use the overloaded \\_\\_add\\_\\_(), \\_\\_sub\\_\\_(), etc functions to implement the operations.\n"}
{"snippet": "n_labels = 5\nbias = tf.Variable(tf.zeros(n_labels))\n", "intent": "The tf.zeros() function returns a tensor with all zeros.\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\ntype(knn)\n", "intent": "**Step 3:** \"Instantiate\" the \"estimator\"\n- \"Instantiate\" means \"make an instance of\"\n"}
{"snippet": "w = \nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n", "intent": "Q0. Create a variable w with an initial value of 1.0 and name weight. Then, print out the value of w.\n"}
{"snippet": "with tf.Session() as sess:\n    zeros = \n    print(sess.run(zeros))\n    assert np.allclose(sess.run(zeros), np.zeros([2, 3]))\n", "intent": "Q1. Create a tensor with shape [2, 3] with all elements set to zero.\n"}
{"snippet": "with tf.Session() as sess:\n    _X = np.array([[1,2,3], [4,5,6]])\n    X = \n    zeros = \n    print(sess.run(zeros))\n    assert np.allclose(sess.run(zeros), np.zeros_like(_X))\n", "intent": "Q2. Let X be a tensor of [[1,2,3], [4,5,6]]. \nCreate a tensor of the same shape and dtype as X with all elements set to one.\n"}
{"snippet": "with tf.Session() as sess:\n    out = \n    print(sess.run(out))\n    assert np.allclose(sess.run(out), np.array([[1, 3, 5], [4, 6, 8]], dtype=np.float32))\n", "intent": "Q3. Create a constant tensor of [[1, 3, 5], [4, 6, 8]].\n"}
{"snippet": "with tf.Session() as sess:\n    _X = np.array([[1, 2], [3, 4], [5, 6], [7,8]])\n    X = \n    out = \n    print(sess.run(out))\n", "intent": "Q6. Randomly shuffle the data in matrix X.\n"}
{"snippet": "with tf.Session() as sess:\n    x = tf.random_uniform([], -1, 1)\n    y = tf.random_uniform([], -1, 1)\n    out = \n    print(sess.run([x, y, out]))\n", "intent": "Q7. Let x and y be random 0-D tensors. Return x + y if x < y and x - y otherwise.\n"}
{"snippet": "sess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nwriter = tf.summary.FileWriter('logs/train', graph=tf.get_default_graph())\n", "intent": "Create a new interactive session and initialize all variables.\n"}
{"snippet": "y_pred = tf.nn.softmax(...)\n", "intent": "Apply softmax to the final fully connected layer.\n"}
{"snippet": "with tf.Session() as sess:\n    ??\n    best_theta_restored = ??\n", "intent": "Restoring the final model\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1e9)\nlogreg.fit(X_train, y_train)\nzip(feature_cols, logreg.coef_[0])\n", "intent": "Confirm that the coefficients make intuitive sense.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./tmp/my_model_final.ckpt\")\n    best_theta_restored = theta.eval() \n", "intent": "Restoring the final model\n"}
{"snippet": "reset_graph()\nsaver = tf.train.import_meta_graph(\"./tmp/my_model_final.ckpt.meta\")  \ntheta = tf.get_default_graph().get_tensor_by_name(\"theta:0\") \nwith tf.Session() as sess:\n    saver.restore(sess, \"./tmp/my_model_final.ckpt\")  \n    best_theta_restored = theta.eval() \nbest_theta_restored\n", "intent": "Restoring a model without the graph\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=64,\n          epochs=10,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nllinreg = LinearRegression()\n", "intent": "Valitset mallin sovittamiseen `sklearn.linear_model`-moduulin lineaarisen regression funktion. Luot oman funktion taso- ja log-malleille.\n"}
{"snippet": "logreg.fit(Xtrain, Ytrain)\n", "intent": "Sovitetaan ensin malli harjoitusdatalla eli estimoidaan mallin (1) $ \\beta $-kertoimien arvot. \n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\ntree_clf.fit(X, y)\n", "intent": "Train DecisionTreeClassifier\n"}
{"snippet": "y3 = ?? \ntree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\ntree_reg3.??\n", "intent": "Fit a third DecisionTreeRegressor on the residual error\n"}
{"snippet": "gbrt_best = GradientBoostingRegressor(??)\ngbrt_best.fit(X_train, y_train)\n", "intent": "Train another GBRT using n_estimator as the `best_n_estimator`\n"}
{"snippet": "feature_cols = ['al']\nX = glass[feature_cols]\ny = glass.ri\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X, y)\n", "intent": "Let's create a linear regression model.  What are our 6 steps for creating a model?\n"}
{"snippet": "lin1 = linear_model.LinearRegression()\nlin2 = linear_model.Ridge(alpha=10**9.5)\n", "intent": "Select linear and ridge models\n"}
{"snippet": "from sklearn.linear_model import SGDRegressor\nsgd_reg = SGDRegressor(max_iter =50, penalty=None, eta0=0.1, random_state=42)\nsgd_reg.fit(X, y.ravel())\n", "intent": "Use SGDRegressor for stochastic gd\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg.intercept_, lin_reg.coef_\n", "intent": "Use LinearRegression from sklearn.linear_model\n"}
{"snippet": "lin_reg = LinearRegression()\nlin_reg.fit(X_poly_scaled, y)\n", "intent": "sklearn Linear Regression\n"}
{"snippet": "from sklearn.svm import SVC\nsvm_clf = SVC(kernel=\"linear\", C=float('inf'))\nsvm_clf.??(X, y)\n", "intent": "Find $w_0$, $w_1$ and $b$ in $w_0x_0 + w_1x_1+b=0$\n"}
{"snippet": "from sklearn.svm import SVC\nsvm_clf = SVC(kernel=\"linear\", C=float('inf'))\nsvm_clf.fit(X, y)\n", "intent": "Find $w_0$, $w_1$ and $b$ in $w_0x_0 + w_1x_1+b=0$\n"}
{"snippet": "tree_reg1 = ??\ntree_reg1.fit(X, y)\n", "intent": "Fit a first DecisionTreeRegressor to label(y)\n"}
{"snippet": "y2 = ?? \ntree_reg2 = ??\ntree_reg2.fit(X, ??)\n", "intent": "Train a second DecisionTreeRegressor on the residual error made by tree_reg1\n"}
{"snippet": "y3 = ?? \ntree_reg3 = ??\ntree_reg3.fit(X, ??)\n", "intent": "Train a third DecisionTreeRegressor on the residual error made by tree_reg2\n"}
{"snippet": "rbm = BernoulliRBM(random_state=0)\nrbm.fit(digits_X, digits_y,)\nbiases_for_visible = rbm.intercept_visible_\nprint biases_for_visible.shape\nprint biases_for_visible\n", "intent": "Manual page for scikit-learn neural network models (unsupervised): http://scikit-learn.org/stable/modules/neural_networks.html\n"}
{"snippet": "model = ARMA(store1_sales_data, (1, 1)).fit()\nprint model.summary()\n", "intent": "Becuase of the errors, it doesn't look like an AR model is good enough -- the data isn't stationary. So let's expand to an `ARMA` model.\n"}
{"snippet": "rf_best = RandomForestClassifier(criterion='gini', class_weight='balanced', n_jobs=-1, \n                                 n_estimators=rf_cv.best_params_['n_estimators'],\n                                 min_samples_split=rf_cv.best_params_['min_samples_split'], \n                                 max_depth=rf_cv.best_params_['max_depth'])\nrf_best.fit(x_train, y_train)\n", "intent": "With the best parameters, we can re-fit another RF object with those values. \n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=30, verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model.fit(x_train, \n          y_train, \n          epochs=50, \n          batch_size=50, \n          validation_data=(x_test, y_test),\n          verbose=2);\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "x = affair[['child','sex','religious','education','rate']].values\ny = affair['target'].values\nknn_n3 = KNeighborsClassifier(n_neighbors=3,weights='uniform')\nknn_n3.fit(x,y)\nprint 'Accuracy: ', knn_n3.score(x,y)\n", "intent": "---\nYou should choose **2 predictor variables** to predict had affair vs. not\n"}
{"snippet": "from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier()\nbest_ada = ada.fit(Xn,y)\n", "intent": "---\n1. Train a AdaBoost classifier on your chosen classification problem.\n- Evaluate the classifier performance with a 5-fold cross-validation.\n"}
{"snippet": "g2 = tf.Graph()\n", "intent": "By default, it grabs the default graph.  But we could have created a new graph like so:\n"}
{"snippet": "convolved = tf.nn.conv2d(img_4d, z_4d, strides=[1, 1, 1, 1], padding='SAME')\nres = convolved.eval()\nprint(res.shape)\n", "intent": "<a name=\"convolvefilter-an-image-using-a-gaussian-kernel\"></a>\nWe can now use our previous Gaussian Kernel to convolve our image:\n"}
{"snippet": "Y_pred = tf.Variable(tf.random_normal([1]), name='bias')\nfor pow_i in range(0, 5):\n    W = tf.Variable(\n        tf.random_normal([1], stddev=0.1), name='weight_%d' % pow_i)\n    Y_pred = tf.add(tf.mul(tf.pow(X, pow_i), W), Y_pred)\ntrain(X, Y, Y_pred)\n", "intent": "But we really don't want to add *too many* powers.  If we add just 1 more power:\n"}
{"snippet": "C = 1.0  \nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(iris_X, iris_y)  \nsvc = svm.SVC(kernel='linear', C=C).fit(iris_X, iris_y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(iris_X, iris_y)\n", "intent": "* Radial Bias Function (RBF)\n* Linear\n* Poly of degree 3\n"}
{"snippet": "h, W = utils.linear(\n    x=X, n_output=20, name='linear', activation=tf.nn.relu)\n", "intent": "You can now write the same process as the above steps by simply calling:\n"}
{"snippet": "sess = tf.Session()\nsess.run(tf.initialize_all_variables())\n", "intent": "Now we'll create a session to manage the training in minibatches:\n"}
{"snippet": "g = tf.get_default_graph()\n[op.name for op in g.get_operations()]\n", "intent": "And just to confirm, let's see what's in our graph:\n"}
{"snippet": "b_1 = tf.get_variable(\n    name='b',\n    shape=[n_filters_out],\n    initializer=tf.constant_initializer())\n", "intent": "Bias is always `[output_channels]` in size.\n"}
{"snippet": "h_3, W = utils.linear(h_2_flat, 128, activation=tf.nn.relu, name='fc_1')\n", "intent": "Create a fully-connected layer:\n"}
{"snippet": "sess = tf.Session()\nsess.run(tf.initialize_all_variables())\n", "intent": "And create a new session to actually perform the initialization of all the variables:\n"}
{"snippet": "[op.name for op in tf.get_default_graph().get_operations()]\n", "intent": "Let's take a look at the graph:\n"}
{"snippet": "def decode(z, dimensions, Ws, activation=tf.nn.tanh):\n    current_input = z\n    for layer_i, n_output in enumerate(dimensions):\n        with tf.variable_scope(\"decoder/layer/{}\".format(layer_i)):\n            W = tf.transpose(Ws[layer_i])\n            h = tf.matmul(current_input, W)\n            current_input = activation(h)\n            n_input = n_output\n    Y = current_input\n    return Y\n", "intent": "Now we'll build the decoder.  I've shown you how to do this.  Read through the code to fully understand what it is doing:\n"}
{"snippet": "g = tf.get_default_graph()\nnames = [op.name for op in g.get_operations()]\nprint(names)\n", "intent": "<TODO: visual of graph>\nLet's have a look at the graph:\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, Y_train)\n", "intent": "Train the model using LogisticRegression\n"}
{"snippet": "with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n    res = softmax.eval(feed_dict={x: img_4d})[0]\nprint([(res[idx], net['labels'][idx])\n       for idx in res.argsort()[-5:][::-1]])\n", "intent": "<a name=\"dropout\"></a>\nIf I run this again, I get a different result:\n"}
{"snippet": "with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n    res = softmax.eval(feed_dict={\n        x: img_4d,\n        'vgg/dropout_1/random_uniform:0': [[1.0]],\n        'vgg/dropout/random_uniform:0': [[1.0]]})[0]\nprint([(res[idx], net['labels'][idx])\n       for idx in res.argsort()[-5:][::-1]])\n", "intent": "Looking at the network, it looks like there are 2 dropout layers.  Let's set these values to 1 by telling the `feed_dict` parameter.\n"}
{"snippet": "with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n    res = softmax.eval(feed_dict={\n        x: img_4d,\n        'vgg/dropout_1/random_uniform:0': [[1.0]],\n        'vgg/dropout/random_uniform:0': [[1.0]]})[0]\nprint([(res[idx], net['labels'][idx])\n       for idx in res.argsort()[-5:][::-1]])\n", "intent": "Let's try again to be sure:\n"}
{"snippet": "with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n    res = softmax.eval(\n        feed_dict={\n            x: style_img_4d,\n            'vgg/dropout_1/random_uniform:0': [[1.0]],\n            'vgg/dropout/random_uniform:0': [[1.0]]})[0]\nprint([(res[idx], net['labels'][idx])\n       for idx in res.argsort()[-5:][::-1]])\n", "intent": "And for fun let's see what VGG thinks of it:\n"}
{"snippet": "with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n    loss = 0.1 * content_loss + 5.0 * style_loss + 0.01 * tv_loss\n    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n", "intent": "<a name=\"training\"></a>\nWith both content and style losses, we can combine the two, optimizing our loss function, and creating a stylized coffee cup.\n"}
{"snippet": "def plot_gradient(img, x, feature, g, device='/cpu:0'):\n    with tf.Session(graph=g) as sess, g.device(device):\n        saliency = tf.gradients(tf.reduce_mean(feature), x)\n        this_res = sess.run(saliency[0], feed_dict={x: img})\n        grad = this_res[0] / np.max(np.abs(this_res))\n        return grad\n", "intent": "We'll now try to find the gradient activation that maximizes a layer with respect to the input layer `x`.\n"}
{"snippet": "for feature_i in range(len(features)):\n    with tf.Session(graph=g) as sess, g.device(device):\n        layer = ...\n        gradient = ...\n        dream(...)\n", "intent": "We'll do the same thing as before, now w/ our noise image:\n<h3><font color='red'>TODO! COMPLETE THIS SECTION!</font></h3>\n"}
{"snippet": "with tf.Session(graph=g) as sess, g.device(device):\n    tf.import_graph_def(net['graph_def'], name='net')\n", "intent": "Let's now import the graph definition into our newly created Graph using a context manager and specifying that we want to use the CPU.\n"}
{"snippet": "x = ...\nsoftmax = ...\nfor img in [content_img, style_img]:\n    with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n        res = softmax.eval(feed_dict={x: img,\n                    'net/dropout_1/random_uniform:0': [[1.0]],\n                    'net/dropout/random_uniform:0': [[1.0]]})[0]\n        print([(res[idx], net['labels'][idx])\n               for idx in res.argsort()[-5:][::-1]])\n", "intent": "Let's see what the network classifies these images as just for fun:\n<h3><font color='red'>TODO! COMPLETE THIS SECTION!</font></h3>\n"}
{"snippet": "clf = tree.DecisionTreeClassifier(max_depth=3)\nclf.fit(X, y)\n", "intent": "* Now you get to build your decision tree classifier! First create such a model with `max_depth=3` and then fit it your data:\n"}
{"snippet": "graph = tf.get_default_graph()\nnb_utils.show_graph(graph.as_graph_def())\n", "intent": "And we can see what the network looks like now:\n"}
{"snippet": "d_reg = tf.contrib.layers.apply_regularization(\n    tf.contrib.layers.l2_regularizer(1e-6), vars_d)\ng_reg = tf.contrib.layers.apply_regularization(\n    tf.contrib.layers.l2_regularizer(1e-6), vars_g)\n", "intent": "We can also apply regularization to our network.  This will penalize weights in the network for growing too large.\n"}
{"snippet": "sess = tf.Session()\ninit_op = tf.initialize_all_variables()\nsaver = tf.train.Saver()\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\nsess.run(init_op)\n", "intent": "Now create a session and create a coordinator to manage our queues for fetching data from the input pipeline and start our queue runners:\n"}
{"snippet": "nb_utils.show_graph(tf.get_default_graph().as_graph_def())\n", "intent": "Let's take a look at the graph:\n"}
{"snippet": "nb_utils.show_graph(g.as_graph_def())\n", "intent": "Let's now take a look at the model:\n"}
{"snippet": "k = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(dn)\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\nprint labels\nprint  \nprint centroids\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "db = DBSCAN(eps=0.5, min_samples=5,random_state=5).fit(X)\n", "intent": "Let's set up the DBSCAN from scikit using an epsilon of .5 and a minimum sample number of 5\n"}
{"snippet": "regr = LogisticRegressionCV(n_jobs=-1)\nregr.fit(rez, y)\n", "intent": "Perform cross-validation with logistic regression.\n"}
{"snippet": "a, b = 1.5, 2.5\ne_out = graph(a,b)\nprint e_out\n", "intent": "Now, we can call this function to execute the computation graph given some inputs `a,b`:\n"}
{"snippet": "def make_nn_layer(input, output_size):\n  input_size = input.get_shape().as_list()[1]\n  weights = tf.Variable(tf.truncated_normal(\n      [input_size, output_size],\n      stddev=1.0 / math.sqrt(float(input_size))))\n  biases = tf.Variable(tf.zeros([output_size]))\n  return tf.matmul(input, weights) + biases\n", "intent": "We start with a helper function to build a simple TensorFlow neural network layer.\n"}
{"snippet": "vae = tf.keras.Model(inputs, reconstructed_inputs)\n", "intent": "Finally we can construct our network end-to-end.\n"}
{"snippet": "util.display_model(encoder)\n", "intent": "Let's visualize the architecture of the encoder to get a more concrete understanding of this network,\n"}
{"snippet": "def train_step(model, optimizer, observations, actions, discounted_rewards):\n  with tf.GradientTape() as tape:\n      observations = tf.convert_to_tensor(observations, dtype=tf.float32)\n      logits = model(observations)\n", "intent": "Now let's use the loss function to define a backpropogation step of our learning algorithm.\n"}
{"snippet": "def create_pong_model():\n  model = tf.keras.models.Sequential([\n      tf.keras.layers.InputLayer(input_shape=(6400,), dtype=tf.float32),\n      tf.keras.layers.Reshape((80, 80, 1)),\n      tf.keras.layers.Conv2D(filters=16, kernel_size=(8,8), strides=(4,4), activation='relu', padding='same'),\n  ])\n  return model\npong_model = create_pong_model()\n", "intent": "We'll define our agent again, but this time, we'll add convolutional layers to the network to increase the learning capacity of our network.\n"}
{"snippet": "save_video_of_model(pong_model, \"Pong-v0\", filename='pong_agent.mp4')  \n", "intent": "We can now save the video of our model learning:\n"}
{"snippet": "def train_step(model, optimizer, observations, actions, discounted_rewards):\n  with tf.GradientTape() as tape:\n      observations = tf.convert_to_tensor(observations, dtype=tf.float32)\n      logits = model(observations)\n  grads = tape.gradient(loss, model.variables) \n  optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n", "intent": "Now let's use the loss function to define a backpropogation step of our learning algorithm.\n"}
{"snippet": "module = LogisticRegression()\nvector = torch.randn(10)\noutput = module(vector)\n", "intent": "We can now create a random vector and pass it through the module:\n"}
{"snippet": "from sklearn import svm, grid_search\ndef svc_param_selection(X, y, nfolds):\n    Cs = [...]\n    gammas = [...]\n    param_grid = {'C': Cs, 'gamma' : gammas}\n    search = grid_search.GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n    search.fit(X, y)\n    search.best_params_\n    return search.best_params_\n", "intent": "TODO: Fill in the following function to use grid search to find the optimal value of C and gamma (hyperparameter of the RBF kernel):\n"}
{"snippet": "svc = svm.SVC(C=..., gamma=..., probability=True)\nsvc\n", "intent": "TODO: Use your optimal values of C and gamma to train the SVM:\n"}
{"snippet": "history = model.fit(X_train, y_train, epochs=10,\n                    validation_data=(X_valid, y_valid))\n", "intent": "1.7) Try running `model.fit()` again, and notice that training continues where it left off.\n"}
{"snippet": "class EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        features = self.resnet(images)\n        features = features.reshape(features.size(0), -1)\n        features = self.bn(self.linear(features))\n        return features\n", "intent": "Now we can implement the encoder and decoder networks!\n"}
{"snippet": "reg = linear_model.LinearRegression()\nreg.normalize=True\nreg.fit (x_train_sc, y_train_sc)\ntr_coeffs = reg.coef_\ntr_resi = reg.residues_\ntr_intr = reg.intercept_\nprint(\"Coeffs: \", tr_coeffs)\nprint(\"Residuals: \", tr_resi)\nprint(\"Intercept: \", tr_intr)\n", "intent": "FINDING MODEL COEFFICIENTS\n"}
{"snippet": "reg = linear_model.LinearRegression()\nreg = reg.fit(x_train_sc, y_train_sc)\ntr_coeffs = reg.coef_\ntr_resi = reg.residues_\ntr_intr = reg.intercept_\nprint(\"Coeffs: \", tr_coeffs)\nprint(\"Residuals: \", tr_resi)\nprint(\"Intercept: \", tr_intr)\n", "intent": "FINDING MODEL COEFFICIENTS\n"}
{"snippet": "clf.fit(x,y)\n", "intent": "With all that established, all we have to do is train the classifer,\n"}
{"snippet": "i,j = np.where(M==0)\nx=np.vstack([i,j]).T\ny = j.reshape(-1,1)*0\ni,j = np.where(M==1)\nx=np.vstack([np.vstack([i,j]).T,x])\ny = np.vstack([j.reshape(-1,1)*0+1,y])\nclf.fit(x,y)\n", "intent": "Now we have a `1` entry in the previously\npure first column's second  row.\n"}
{"snippet": "lr = LogisticRegression()\n_=lr.fit(X[:,:-1],c)\n", "intent": "This establishes the training data.  The next block\ncreates the logistic regression object and fits the data.\n"}
{"snippet": "x = np.linspace(-1,1,30)\nX = np.c_[x,x+1,x+2] \npca.fit(X)\nprint(pca.explained_variance_ratio_)\n", "intent": "Let's create some very simple data and apply PCA.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsRegressor\nknr=KNeighborsRegressor(2) \nknr.fit(xin,y)\n", "intent": " We can use this data to construct a simple nearest neighbor\nestimator using Scikit-learn,\n"}
{"snippet": "dtr = DecisionTreeRegressor(max_depth=4,\n                            min_samples_split=5,\n                            max_leaf_nodes=10)\ndtr.fit(X,y)\n", "intent": "<h1> Sample Decision Tree Regressor\n"}
{"snippet": "model = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n    keras.layers.Dense(1)\n])\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n", "intent": "5.4) Now build, train and evaluate a neural network to tackle this problem. Then use it to make predictions on the test set.\n"}
{"snippet": "def show_kdtree(kdtree,depth):\n    if kdtree is None:\n        print 'None'\n        return None\n    if depth == 0:\n        print 'nodeid=',kdtree[0],':',kdtree[1]\n        return None\n    for j in [2,3]:\n        show_kdtree(kdtree[j],depth-1)        \n", "intent": "I implemented the function ``show_kdtree``, that allows us to have a look at the nodes of the generated k-d tree for a given depth.\n"}
{"snippet": "show_kdtree(kdtree,4)\n", "intent": "Going further down the tree...\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1000)) \nmodel.add(Activation(\"relu\")) \nmodel.add(Dropout(0.5)) \nmodel.add(Dense(num_classes))\nmodel.add(Activation(\"softmax\")) \nmodel.summary()\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) \n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "linear_svc = svm.SVC(kernel='linear')\nlinear_svc.kernel\n", "intent": "<h2> Kernel functions\n"}
{"snippet": "from glove import Corpus, Glove\ncorpus = Corpus()\ncorpus.fit(sentences_list, window=5)\nglove = Glove(no_components=100, learning_rate=0.05)\nglove.fit(corpus.matrix, epochs=100, no_threads=4, verbose=False)\nglove.add_dictionary(corpus.dictionary)\n", "intent": "pip install glove-python\n"}
{"snippet": "from keras import models\nfrom keras import layers\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n", "intent": "Here's what our network looks like:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    print(\"vocab_size: \", vocab_size)\n    print(\"embed_dim: \", embed_dim)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(penalty = 'l1')\n", "intent": "Then we used those features to build a classification model\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1)) \n    embed = tf.nn.embedding_lookup(embedding, input_data) \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n    keras.layers.Dense(1),\n])\n", "intent": "2) Compile the following model, passing it your custom loss function, then train it and evaluate it. **Tip**: don't forget to use the scaled sets.\n"}
{"snippet": "happyModel.fit(X_train, Y_train, epochs=40, batch_size=64)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=100, validation_split=0.2, batch_size=128, verbose=0,\n         callbacks=[EarlyStopping(patience=10)])\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed_layer = tf.nn.embedding_lookup(embedding, input_data)\n    return embed_layer\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "lam = 3.0; x_train = np.random.exponential(1/lam, size=10) \n", "intent": "**Part D**: Use the formula you found in **Part C** to estimate the rate parameter $\\lambda$ for the following training data.  \n"}
{"snippet": "knn = KNeighborsClassifier(25).fit(X_train, y_train)\nplot_knn_boundary(X_train, y_train, knn)\n", "intent": "**Part C**: Play with the value of $K$ above.  How does the character of the decision boundary change with $K$? \n"}
{"snippet": "new_perc = Perceptron(max_iter=5, alpha=0.0, shuffle=True)\nnew_perc.fit(Xnew, ynew)\nxplot = np.linspace(-1.5,1.5,20)\nw, b = new_perc.coef_[0], perc.intercept_[0]\nyplot = -(b + w[0]*xplot)/w[1] \nfig, ax = data_plot(scatter=[(Xnew, ynew)])\nax.plot(xplot, yplot, lw=3, color=\"black\");\n", "intent": "Shuffling the data gives a much better decision boundary, likely just because we didn't encounter the misclassified points at the end of the epoch. \n"}
{"snippet": "bagofwords_pipe.fit(text_train, labels_train)\n", "intent": "We can fit our model to the training data using the `.fit()` method. \n"}
{"snippet": "clf.fit(vectorized_count_train_data, train_target)\n", "intent": "Fit it to the vectorized training data and training labels\n"}
{"snippet": "lm = LinearRegression(normalize=True)\nlm.fit(X_train, y_train)\n", "intent": "<font color='red'>TASK MARK: 2</font>\n<br>\n<font color='green'>COMMENT:  - </font>\n"}
{"snippet": "model = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n    keras.layers.Dense(1),\n])\n", "intent": "2) Compile your model, passing it your custom loss function, then train it and evaluate it. **Tip**: don't forget to use the scaled sets.\n"}
{"snippet": "from sklearn import linear_model\nmodels = {}\nfor k in Phis_tr:\n    model = linear_model.LinearRegression()\n    model.fit(Phis_tr[k], Y_tr)\n    models[k] = model\n", "intent": "Train a model for each number of feature\n"}
{"snippet": "one_hot_features.todense()[:5,:]\n", "intent": "Examining a few rows we see there are multiple one hot encodings (one for flavor and one for toppings).\n"}
{"snippet": "Phi.todense()[:5,:]\n", "intent": "Again let's look at a few examples (in practice you would want to avoid the `todense()` call\n"}
{"snippet": "line_reg = linear_model.LinearRegression(fit_intercept=True)\nline_reg.fit(train_data[['X']], train_data['Y'])\n", "intent": "The following block of code creates an instance of the Least Squares Linear Regression model and the fits that model to the training data.  \n"}
{"snippet": "sin_reg = linear_model.LinearRegression(fit_intercept=False)\nsin_reg.fit(Phi, train_data['Y'])\n", "intent": "We can again use the scikit-learn package to fit a linear model on the transformed space.\n"}
{"snippet": "(theta0,theta1) = np.meshgrid(np.linspace(0,3,20), np.linspace(1,5,20))\ntheta_values = np.vstack((theta0.flatten(), theta1.flatten())).T\n", "intent": "Generate all combinations of $\\theta_0$ and $\\theta_1$ on a 20 by 20 grid.\n"}
{"snippet": "import sklearn.linear_model as linear_model\nleast_squares_model = linear_model.LinearRegression()\nleast_squares_model.fit(X,Y)\n", "intent": "Fit a least squares regression model.\n"}
{"snippet": "def neg_gradL(theta, X, Y):\n    return -(X.T @ (Y - sigmoid(X @ theta)))\n", "intent": "In the following we implement the gradient function for a dataset X and Y.\n"}
{"snippet": "lr = linear_model.LogisticRegressionCV(100)\nlr.fit(X,Y)\n", "intent": "We will use the built in cross-validation support for logistic regression in scikit-learn.\n"}
{"snippet": "def scaled_elu(z, scale=1.0, alpha=1.0):\n    is_positive = tf.greater_equal(z, 0.0)\n    return scale * tf.where(is_positive, z, alpha * tf.nn.elu(z))\n", "intent": "1) Examine and run the following code examples.\n"}
{"snippet": "lam_values = np.logspace(-1.3,2.5,20)\nmodels = []\nfor lam in lam_values:\n    model = linear_model.Lasso(alpha = lam, max_iter=100000)\n    model.fit(Phi, Y_tr)\n    models.append(model)\n", "intent": "In the following we use the scikit-learn Lasso package.  As before we will try a range of values for the regularization parameter. \n"}
{"snippet": "from keras import optimizers\nmodel = Sequential()\nmodel.add(Dense(512,activation='relu',input_shape=(num_words,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes,activation='sigmoid'))          \nmodel.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=100, verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "classifier = MLPClassifier(random_state=seed)\n", "intent": "Read the [Docs](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB \nmnb = MultinomialNB()\nmnb.fit(X_train,Y_train)\nmnb.score(X_validation,Y_validation)\n", "intent": "1. Multinomial Naive Bayes\n2. SVM\n3. XgBoost\n4. Keras Neural Net\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim),-1,1))\n    embed_input = tf.nn.embedding_lookup(embedding, input_data)\n    return embed_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nmodel.add(Flatten(input_shape=(28,28)))\nmodel.add(Dense(10, activation='softmax') )\nsgd=SGD(lr=0.2, momentum=0.0, decay=0.0)\nmodel.compile(optimizer='sgd',\n      loss='categorical_crossentropy',\n      metrics=['accuracy'])\n", "intent": "Construct the model:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation= 'relu',input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train_e, y_train, batch_size=32, epochs=20)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n    keras.layers.Dense(1),\n])\n", "intent": "8) Build and compile a Keras model for this regression task, and use your datasets to train it, evaluate it and make predictions for the test set.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: test_x,\n            labels_: test_y}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc ))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], minval = -1, maxval=1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1024, activation='sigmoid', input_shape=(1000,)))\nmodel.add(Dropout(.4))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal([vocab_size, embed_dim], stddev=0.01))\n    embedded = tf.nn.embedding_lookup(embedding, input_data) \n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n", "intent": "Build the `tf.keras` model by stacking layers. Select an optimizer and loss function used for training:\n"}
{"snippet": "x = tf.ones((2, 2))\nwith tf.GradientTape() as t:\n  t.watch(x)\n  y = tf.reduce_sum(x)\n  z = tf.multiply(y, y)\ndz_dy = t.gradient(z, y)\nassert dz_dy.numpy() == 8.0\n", "intent": "You can also request gradients of the output with respect to intermediate values computed during a \"recorded\" `tf.GradientTape` context.\n"}
{"snippet": "class Model(object):\n  def __init__(self):\n    self.W = tf.Variable(5.0)\n    self.b = tf.Variable(0.0)\n  def __call__(self, x):\n    return self.W * x + self.b\nmodel = Model()\nassert model(3.0).numpy() == 15.0\n", "intent": "Let's define a simple class to encapsulate the variables and the computation.\n"}
{"snippet": "predictions = model(features)\npredictions[:5]\n", "intent": "Let's have a quick look at what this model does to a batch of features:\n"}
{"snippet": "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\nglobal_step = tf.Variable(0)\n", "intent": "Let's setup the optimizer and the `global_step` counter:\n"}
{"snippet": "tf.reset_default_graph()\nX = tf.placeholder(tf.float32, [None, N_TIME_STEPS, N_FEATURES], name=\"input\")\nY = tf.placeholder(tf.float32, [None, N_CLASSES])\n", "intent": "Now, let create placeholders for our model:\n"}
{"snippet": "test_accuracy = tfe.metrics.Accuracy()\nfor (x, y) in test_dataset:\n  logits = model(x)\n  prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n  test_accuracy(prediction, y)\nprint(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))\n", "intent": "Unlike the training stage, the model only evaluates a single [epoch](https://developers.google.com/machine-learning/glossary/\n"}
{"snippet": "mobilenet_layer = layers.Lambda(hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/2\"))\nmobilenet_classifier = tf.keras.Sequential([mobilenet_layer])\n", "intent": "Use `hub.module` to load a mobilenet, and `tf.keras.layers.Lambda` to wrap it up as a keras layer.\n"}
{"snippet": "mobilenet_features_module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/2\")\nmobilenet_features = layers.Lambda(mobilenet_features_module)\nmobilenet_features.trainable = False\n", "intent": "TensorFlow Hub also distributes models without the top classification layer. These can be used to easily do transfer learning.\n"}
{"snippet": "model = tf.keras.Sequential([\n  mobilenet_features,\n  layers.Dense(image_data.num_classes, activation='softmax')\n])\n", "intent": "Now wrap the hub layer in a `tf.keras.Sequential` model, and add a new classification layer.\n"}
{"snippet": "class PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs):\n    if epoch % 100 == 0: print('')\n    print('.', end='')\nEPOCHS = 1000\nhistory = model.fit(\n  normed_train_data, train_labels,\n  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n  callbacks=[PrintDot()])\n", "intent": "Train the model for 1000 epochs, and record the training and validation accuracy in the `history` object.\n"}
{"snippet": "smaller_model = keras.Sequential([\n    keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n    keras.layers.Dense(4, activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\nsmaller_model.compile(optimizer='adam',\n                loss='binary_crossentropy',\n                metrics=['accuracy', 'binary_crossentropy'])\nsmaller_model.summary()\n", "intent": "Let's create a model with less hidden units to compare against the baseline model that we just created:\n"}
{"snippet": "model = create_model()\nmodel.fit(train_images, train_labels, epochs=5)\n", "intent": "Build a fresh model:\n"}
{"snippet": "saved_model_path = tf.contrib.saved_model.save_keras_model(model, \"./saved_models\")\n", "intent": "Create a `saved_model`: \n"}
{"snippet": "new_model = tf.contrib.saved_model.load_keras_model(saved_model_path)\nnew_model\n", "intent": "Reload a fresh keras model from the saved model.\n"}
{"snippet": "pred_Y = create_LSTM_model(X)\npred_softmax = tf.nn.softmax(pred_Y, name=\"y_\")\n", "intent": "Note that we named the input tensor, that will be useful when using the model from Android. Creating the model:\n"}
{"snippet": "if tf.test.is_gpu_available():\n  rnn = tf.keras.layers.CuDNNGRU\nelse:\n  import functools\n  rnn = functools.partial(\n    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n", "intent": "Next define a function to build the model.\nUse `CuDNNGRU` if running on GPU.  \n"}
{"snippet": "for input_example_batch, target_example_batch in dataset.take(1): \n  example_batch_predictions = model(input_example_batch)\n  print(example_batch_predictions.shape, \"\n", "intent": "Now run the model to see that it behaves as expected.\nFirst check the shape of the output:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(500, input_dim=1000, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\nmodel.summary()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test), verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "feature = one_hot_train.drop('Survived', axis=1)\ntarget = one_hot_train['Survived']\nrf = RandomForestClassifier(random_state=1, criterion='gini', max_depth=10, n_estimators=50, n_jobs=-1)\nrf.fit(feature, target)\n", "intent": "We are going to split the data into features and targer, create the model and verify the the score\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(8, activation='sigmoid', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(4, activation='sigmoid'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\noptimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.001)\nmodel.compile(loss = 'categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=100, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "lm = sm.OLS(y, X).fit()\nlm.summary()\n", "intent": "Fit a simple linear regression model without validating the assumptions just in order to get some inights on the assumptions that we need to check.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    Embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    Embedded = tf.nn.embedding_lookup(Embedding, input_data)\n    return Embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "pickle.dump(predictions, open(\"predictions.p\", \"wb\"))\npickle.dump(history, open(\"history.p\", \"wb\"))\ntf.train.write_graph(sess.graph_def, '.', './checkpoint/har.pbtxt')  \nsaver.save(sess, save_path = \"./checkpoint/har.ckpt\")\nsess.close()\n", "intent": "Whew, that was a lot of training. Do you feel thirsty? Let's store our precious model to disk:\n"}
{"snippet": "LogisticRegression().fit(X_train, y_train).score(X_test, y_test)\n", "intent": "Shorter, maybe less readible.\n"}
{"snippet": "pca.fit(X)\n", "intent": "2) Fit to training data\n"}
{"snippet": "param_grid = {'n_neighbors': np.arange(1, 10),\n             'weights': ['uniform', 'distance']}\nfrom sklearn.neighbors import KNeighborsClassifier\ngrid_search = GridSearchCV(KNeighborsClassifier(), param_grid, verbose=3, cv=5, n_jobs=23)\ngrid_search.fit(X_train, y=y_train)\n", "intent": "Use GridSearchCV to adjust n_neighbors of KNeighborsClassifier.\n"}
{"snippet": "keepList = dfFeatureImportances.index.tolist()\nforest = RandomForestClassifier(n_estimators=10)\nxTrain = dfInput[keepList].iloc[0:-500] \nyTrain = dfOutput.iloc[0:-500]\nxTest = dfInput[keepList].iloc[-500:].copy()\nyTest = dfOutput.iloc[-500:].copy()\nforest.fit(xTrain,yTrain)\n", "intent": "Retrain our model using the fetures which we determine were most relevent.\nThis time, we'll train on fewer features to prevent over-fitting.\n"}
{"snippet": "X = iris[[\"petal_length\"]] \ny = iris[\"petal_width\"] \nmodel = lm.LinearRegression() \nresults = model.fit(X, y) \nprint (results.intercept_, results.coef_)\n", "intent": "Now let's use scikit-learn to find the best fit line.\n"}
{"snippet": "model = smf.ols(formula = 'petal_width ~ petal_length + versicolor + virginica', data = iris)\nresults = model.fit()\nresults.summary()\n", "intent": "Now we perform a multilinear regression with the dummy variables added.\n"}
{"snippet": "lm = smf.ols(formula='sales ~ TV + radio + newspaper', data=data).fit()\nlm.summary()\n", "intent": "syntax can be found here: http://statsmodels.sourceforge.net/devel/example_formulas.html\n"}
{"snippet": "import statsmodels.formula.api as sm\nfrom scipy import stats\nstats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\nimport statsmodels.formula.api as smf\nresult = smf.logit('label ~ is_news', data=df)\nresult = result.fit()\nresult.summary()\n", "intent": "The `sm.logit` function from `statsmodels.formula.api` will perform a logistic regression using a formula string.\n"}
{"snippet": "vr_by_f1_features_to_test = []\nvr_by_f1_features_test_results = {}\nfor feature in tqdm(vr_by_f1_performant_features):\n    vr_by_f1_features_to_test.append(feature)\n    vr_by_f1_features_test_results[feature] = run_model(LogisticRegression(), 'logit', 100,\n                                                        adult_train_df[vr_by_f1_features_to_test],\n                                                        adult_train_target)\n", "intent": "Add one feature at a time.\n"}
{"snippet": "from xgboost import XGBClassifier\nmodel = XGBClassifier()\nmodel.fit(train, y_train)\nget_model_results(model, train, test, y_train, y_test)\n", "intent": "https://github.com/dmlc/xgboost\n"}
{"snippet": "SVC_linear = LinearSVC()\nSVC_linear.fit(X_train,y_train)\nprint(\"train acc:\",SVC_linear.score(X_train,y_train))\nprint(\"test acc:\",SVC_linear.score(X_test,y_test))\n", "intent": "Train a [`LinearSVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n"}
{"snippet": "mask = (df[\"dg\"] == \"doctorate\")\nXdg = df[mask][[u'yr', u'yd',\n       u'rk_assistant', u'rk_associate', u'rk_full', u'dg_doctorate',\n       u'dg_masters',u'sx_male',u'sx_female']]\nydg = df[mask][u'sl']\nlm = linear_model.LinearRegression()\nmodel = lm.fit(Xdg,ydg)\nprint \"Doctorates\", lm.score(Xdg,ydg)\n", "intent": "Did regularization improve the second fit?\nNow let's move on to the next category, \"dg\" (degree).\n"}
{"snippet": "neigh = KNeighborsClassifier(n_neighbors=10)\nneigh.fit(X,y)\n", "intent": "Build a logit model and fit\n"}
{"snippet": "k = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(matrix)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "k = 2\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(matrix)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "k  = 2\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(df.loc[:,[\"age\",\"income\"]])\n", "intent": "Cluster the Data to our our target groups. **BONUS**: Perform multiple clustering tests on various indicator variables\n"}
{"snippet": "k = 3\nfirstDimension = iris.feature_names[0]\nsecondDimension = iris.feature_names[1]\nmatrix = df.loc[:,[firstDimension,secondDimension]]\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(matrix)\n", "intent": "Run the clustering analysis using scikit-learn.\n*Hint*: Estimate the number of clusters, k, based on your visual examination of the distributions.\n"}
{"snippet": "dbscan = DBSCAN(eps=.5,min_samples=5)\ndb = dbscan.fit(X)\ndb\n", "intent": "Let's set up the DBSCAN from scikit using an epsilon of .5 and a minimum sample number of 5\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(784,)))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])\n", "intent": "We build simple model with two hidden layers to classify digits.\n"}
{"snippet": "transformations = transforms.Compose([transforms.Scale(32),transforms.ToTensor()])\n", "intent": "- Converts: a PIL.Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n"}
{"snippet": "m1 = ols('PRICE ~ CRIM + RM + PTRATIO',bos).fit()\nprint(m1.summary())\n", "intent": "1. 'CRIM' (per capita crime rate by town)\n2. 'RM' (average number of rooms per dwelling)\n3. 'PTRATIO' (pupil-teacher ratio by town)\n"}
{"snippet": "model = Sequential()\nmodel.add(Embedding(len(word2num), 50)) \nmodel.add(GRU(64))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nmodel.summary()\n", "intent": "This method is no different to the method utilised in the sentiment analysis lesson.\n"}
{"snippet": "batch_size = 128\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=5, validation_data=(X_test, y_test))\n", "intent": "I may heave cheated and run the following block 3 times. Good thing about Keras is that it remembers the last learning rate and goes from there.\n"}
{"snippet": "m_lin = \nX = \nm_lin.fit(...)\n", "intent": "The first thing we would like to do is to fit a linear model. As dependent variable, we take charges, as independent variables bmi, age and children.\n"}
{"snippet": "m_tree.fit(...)\n", "intent": "Now fit the same model on the train set.\n"}
{"snippet": "m_lin = linear_model.LinearRegression()\nX = sample[['bmi', 'age', 'children']]\nm_lin.fit(X=X, y=sample['charges'])\n", "intent": "The first thing we would like to do is to fit a linear model. As dependent variable, we take charges, as independent variables bmi, age and children.\n"}
{"snippet": "m_lin = linear_model.LinearRegression()\nm_lin.fit(X=X_train, y=y_train)\n", "intent": "Fit a linear model for charges on bmi, age and smoker, using the training set.\n"}
{"snippet": "m_tree = DecisionTreeRegressor(max_leaf_nodes = 3, random_state = 2017)\nm_tree.fit(X=X, y=sample['charges'])\n", "intent": "Fit a CART regression tree for charges on bmi, age and smoker on the whole dataset. To keep the tree parsimonious, limit the number of leafs to 3.\n"}
{"snippet": "m_tree.fit(X=X_train, y=y_train)\n", "intent": "Now fit the same model on the train set.\n"}
{"snippet": "def dsigmoid(y):\n    return y - y**2\n", "intent": "Note: We need this when we run the backpropagation algorithm\n"}
{"snippet": "m_svm.fit(X=X_train, y=y_train)\n", "intent": "Train on the train data.\n"}
{"snippet": "m_nn = MLPRegressor(hidden_layer_sizes = (5,2), max_iter = 10000, random_state = 2017)\nm_nn.fit(X=X, y=sample['charges'])\n", "intent": "Fit a 2-layer - with 5 nodes in first and 2 nodes is second layer - perceptron with _relu_ as the activation function for hidden layers.\n"}
{"snippet": "m_nn.fit(X=X_train, y=y_train)\n", "intent": "Train on the train data.\n"}
{"snippet": "m_rf.fit(X=X_train, y=y_train)\n", "intent": "Fit the model on the train set.\n"}
{"snippet": "m_km = KNeighborsClassifier(n_neighbors=5, weights='distance')\nm_km.fit(X=X_train, y=y_train)\n", "intent": "Predict the diagnosis from the test set based on 5 points using euclidean distance metric.\n"}
{"snippet": "m_nb = GaussianNB()\nm_nb.fit(X=X_train, y=y_train)\n", "intent": "Train a model based on Naive Bayes with the train data, and show the model parameters.\n"}
{"snippet": "m_logit = LogisticRegression()\nm_logit.fit(X=X_train, y=y_train)\n", "intent": "Train a logistic model with the train data.\n"}
{"snippet": "m_tree = DecisionTreeClassifier(max_leaf_nodes = 5, random_state = 2017)\nm_tree.fit(X=X_train, y=y_train)\n", "intent": "Train a logistic model with the train data. Limit the number of leafs to 5.\n"}
{"snippet": "m_nn = MLPClassifier(hidden_layer_sizes = (5,2), max_iter = 10000, random_state = 2017)\nm_nn.fit(X=X_train, y=y_train)\n", "intent": "Fit a 2-layer - with 5 nodes in first and 2 nodes is second layer - perceptron with relu as the activation function for hidden layers.\n"}
{"snippet": "x = tf.placeholder(tf.float32)\ny = tf.log(x)   \nvar_grad = tf.gradients(y, x)\nwith tf.Session() as session:\n    var_grad_val = session.run(var_grad, feed_dict={x:2})\n    print(var_grad_val)\n", "intent": "- Gradients are free!\n"}
{"snippet": "m_dbs = DBSCAN(eps=0.25, min_samples=5).fit(dfn)\n", "intent": "Another approach is density based clustering. Perform dbscan on the normalized data, with max distance 0.25.\n"}
{"snippet": "m_ar = SARIMAX(tslog, trend='ct', order=[1,0,0], seasonal_order=[1,0,1,12],\n               enforce_stationarity=False, enforce_invertibility=False\n              ).fit(maxiter=5000, method='nm')\nprint(m_ar.summary())\n", "intent": "Fit the best model and display its summary.\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\naccuracy = classifier.score(x_test, y_test)\nprint (\"Naive Bayes Classifier accuracy: \", (100* accuracy), \"%.\")\n", "intent": "Now we build and test the classifier.\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\naccuracy = classifier.score(x_test, y_test)\nprint (\"Naive Bayes Classifier accuracy: \", (100* accuracy), \"%.\")\n", "intent": "Now train and test as before.\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\naccuracy = classifier.score(x_test, y_test)\nprint (\"Naive Bayes Classifier accuracy: \", (100* accuracy), \"%.\")\n", "intent": "Now with the data sets created, lets train then test as before.\n"}
{"snippet": "knn = KNeighborsClassifier()\nknn.fit(train_data,train_labels)\n", "intent": "* Scikit KNN\nhttp://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n"}
{"snippet": "nb = GaussianNB()\nnb.fit(train_data,train_labels)\n", "intent": "* Scikit Naive Bayes\nhttp://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n"}
{"snippet": "mlp =  MLPClassifier(solver='sgd',hidden_layer_sizes=(5, 1))\nmlp.fit(train_data,train_labels)\n", "intent": "* Scikit MLP\nhttps://en.wikipedia.org/wiki/Multilayer_perceptron\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\naccuracy = classifier.score(x_test, y_test)\nprint (\"Naive Bayes Classifier accuracy: \", (100* accuracy), \"%.\")\n", "intent": "Now we build and test the classifier. We'll just use a basic calssfier here to keep things simple.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\nmodel.summary()\n", "intent": "So, how hard can it be to build a Multi-Layer percepton with keras?\nIt is baiscly the same, just add more layers!\n"}
{"snippet": "clf.fit(X_train, y_train)\n", "intent": "This is the function that actually trains the classifier with our training data. \n"}
{"snippet": "clf = SVC(kernel='rbf', probability=True)\nclf.fit(X_train, y_train)\n", "intent": "Scikit-learn is designed to let you easily swap algorithms out\n"}
{"snippet": "svm = LinearSVC()\n", "intent": "1) Instantiate an object and set the parameters\n"}
{"snippet": "model = SVC(kernel='linear')\nmodel.fit(X, y)\n", "intent": "Scikit-learn implements support vector machine models in the svm package.\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 4))\nnet.add(LeakyReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\nprint (net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "model = Sequential() \nmodel.add(Dense(2, input_dim=2, name='wx_b'))\nmodel.add(Activation('softmax', name='softmax'))\nbatch_size = 128\nnb_epoch = 100\n", "intent": "Define the model and train it.\n"}
{"snippet": "from distutils.version import LooseVersion\nimport warnings\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nprint('TensorFlow Version: {}'.format(tf.__version__))\nassert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\nif not tf.test.gpu_device_name():\n    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\nelse:\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n", "intent": "This will check to make sure you have the correct version of TensorFlow and access to a GPU\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nprint KNeighborsClassifier()\n", "intent": "<center>\n<img src=\"https://static1.squarespace.com/static/55ff6aece4b0ad2d251b3fee/t/5752540b8a65e246000a2cf9/1465017829684/\">\n</center>\n"}
{"snippet": "from sklearn.svm import SVC\nprint SVC()\n", "intent": "..and kernel application\n<center>\n<img src=\"http://i2.wp.com/blog.hackerearth.com/wp-content/uploads/2017/02/kernel.png\" width=\"500\">\n</center>\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\nmodel.fit(X_train, Y_train, validation_data = (X_val, Y_val), epochs=20, \n          batch_size=128, verbose=True)\n", "intent": "Take couple of minutes and Try and optimize the number of layers and the number of parameters in the layers to get the best results. \n"}
{"snippet": "linear_regression_model = LinearRegression()\n", "intent": "No we define the model we want to use and herein lies one of the main advantages of using this lib\n"}
{"snippet": "linear_regression_model.fit(housing_data.data, housing_data.target)\n", "intent": "Next, we can fit out Linear Regression model on our feature set to make predictions for out labels (the price of the houses). \n"}
{"snippet": "from keras import optimizers\nmodel = Sequential()\nmodel.add(Dense(1000, input_dim=1000))\nmodel.add(Dense(2000))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss= \"mean_squared_error\",  optimizer=\"adam\",  metrics=[\"accuracy\"])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nkfold = StratifiedKFold(n_splits=6, random_state=42, \n                        shuffle=True)\ngrid_searcher = GridSearchCV(\n    RandomForestClassifier(random_state=42, n_estimators=20), \n                 {'criterion': ['gini', 'entropy'],\n                  'max_depth': [1, 3, 5, 7, 10, None]},\n                  cv=kfold, n_jobs=3)\ngrid_searcher.fit(X, y)\nprint('best score:', grid_searcher.best_score_, grid_searcher.best_params_)\n", "intent": "<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    return tf.nn.embedding_lookup(embedding, input_data)    \ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf = tree.DecisionTreeRegressor()\nclf = clf.fit(x_train, y_train)\n", "intent": "Finally, we will use Decision Tree Regression model as a training model and fit it with our training data.\n"}
{"snippet": "from tensorflow.python.framework import graph_io\ngraph_io.write_graph(sess.graph, \n                     \"/root/models/optimize_me/\", \n                     \"unoptimized_gpu.pb\")\n", "intent": "We will use this later.\n"}
{"snippet": "from tensorflow.python.framework import graph_io\ngraph_io.write_graph(sess.graph, \"/root/models/optimize_me/\", \"unoptimized_cpu.pb\")\n", "intent": "We will use this later.\n"}
{"snippet": "from tensorflow.python.framework import graph_io\ngraph_io.write_graph(sess.graph, \"/root/models/optimize_me/\", \"unoptimized_gpu.pb\")\n", "intent": "We will use this later.\n"}
{"snippet": "def build_model():\n", "intent": "The above code has been written as a function. \nChange some of the **hyperparameters** and see what happens. \n"}
{"snippet": "kM.fit(df.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "**Instanstiate instance of model**\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "**Instaiate an Instance of the model**\n"}
{"snippet": "lm.fit(X_train,y_train)\n", "intent": "**Fit model on the training data**\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "A = tf.Variable(tf.zeros([2, 2]), dtype=tf.float32)\nB = tf.Variable(tf.zeros([2, 1]), dtype=tf.float32)\nX = tf.matmul(tf.matrix_inverse(A), B)\nwith tf.Session() as sess:\n    print(sess.run(X, feed_dict={A:[[3, 2], [4, -1]], B:[[15], [10]]}))\n", "intent": "$$\n    \\begin{array}{c} \n    a_{11}x + a_{12}y = b_{11} \\\\ \n    a_{21}x + a_{22}y = b_{21} \\\\ \n    \\end{array} \n$$\n"}
{"snippet": "x = tf.Variable(2.0, dtype=tf.float32, name='x') \ny = tf.pow(x, 2)\n", "intent": "Hint : tf.train.GradientDescentOptimizer(learning_rate).minimize(function)\n"}
{"snippet": "m3 = ols('PRICE ~ CRIM + RM + PTRATIO + LSTAT',bos).fit()\nprint(m3.summary())\n", "intent": "Let us add LSTAT to the set of independent variables:\n"}
{"snippet": "depth = 2\nxTrain = train.iloc[:, :-1].values\nyTrain = train.iloc[:,  -1].values\nxTest = test.iloc[:, :-1].values\nyTest = test.iloc[:,  -1].values\ntree2a = tree.DecisionTreeClassifier(max_depth = depth)\nmod2a  = tree2a.fit(xTrain , yTrain)\n", "intent": "<br>\n** Fit training data:**\n<br>\n"}
{"snippet": "from keras.layers import concatenate\nauxiliary_input = Input(shape=(5,), name='aux_input')\nx = concatenate([lstm_out, auxiliary_input])\nx = Dense(64, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\nmain_output = Dense(1, activation='sigmoid', name='main_output')(x)\n", "intent": "At this point, we feed into the model our auxiliary input data by concatenating it with the LSTM output:\n"}
{"snippet": "lambda_range = np.array(range(-7, 8, 1))\ntrain_r_sq = np.zeros(lambda_range.shape)\ntest_r_sq = np.zeros(lambda_range.shape)\nfor i , lmb in enumerate(lambda_range):\n    ridge_2 = Ridge_Reg(alpha = 10**lmb)\n    ridge_2.fit(x_2_train, y_2_train)\n    train_r_sq[i] = ridge_2.score(x_2_train, y_2_train)\n    test_r_sq[i]  = ridge_2.score(x_2_test, y_2_test)\n", "intent": "**2. Perform the ridge regression for different lambda values:**\n<br>\n"}
{"snippet": "import numpy as np\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\ninputs = np.array([0.7, -0.3])\nweights = np.array([0.1, 0.8])\nbias = -0.1\nx = np.dot(inputs,np.transpose(weights)) + bias\noutput = sigmoid(x)\nprint('Output:')\nprint(output)\n", "intent": "actually as i understand, this is logistic regression.\n"}
{"snippet": "h     = activation(torch.mm(features, W1) + B1)\ny_hat = activation(torch.mm(h, W2) + B2)\ny_hat\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "model1.fit(X, y)\n", "intent": "The model 1 displays higher score.\n"}
{"snippet": "model = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n", "intent": "This means that our best fit line is:\n$$y = a + b x$$\nwhere\n$$a = -0.363075521319, b = 0.41575542$$\nNext let's use `statsmodels`.\n"}
{"snippet": "test_models = [LinearRegression(), Ridge(alpha = 10), Lasso(alpha = 10)]\nscores = [analyze_performance(my_model) for my_model in test_models]\n", "intent": "Let's try a few degrees with a regularized model.\n"}
{"snippet": "import tensorflow as tf\nx = tf.placeholder(tf.float32)\ny = tf.placeholder(tf.float32)\nc = tf.placeholder(tf.float32)\nz = tf.subtract(tf.divide(x,y),c)\nwith tf.Session() as sess:\n    output = sess.run(z,feed_dict={x:10,y:2,c:1})\n    print(output)\n", "intent": "Use tf constants and tf functions to do basic math operations. Also use feed_dict to allow user input.\n"}
{"snippet": "n_features = 120\nn_labels = 5\nweights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\nbias = tf.Variable(tf.zeros(n_labels))\n", "intent": "It is good practice to initialize variables from a normal distribution to prevent any one variable from overwhelming the others.\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, save_file)\n    test_accuracy = sess.run(\n        accuracy,\n        feed_dict={features: mnist.test.images, labels: mnist.test.labels})\nprint('Test Accuracy: {}'.format(test_accuracy))\n", "intent": "We don't need to spend all that time retraining the model since we can just reload the parameters.\n"}
{"snippet": "autoencoder.fit(x_train_noisy, x_train,\n                epochs=100,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test_noisy, x_test),\n                callbacks=[TensorBoard(log_dir='/tmp/autoencoder_denoise', \n                                       histogram_freq=0, write_graph=False)])\n", "intent": "Let's train the AutoEncoder for `100` epochs\n"}
{"snippet": "model.fit(x_train, y_train, batch_size=10, nb_epoch=10, validation_data=(x_test, y_test), verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "scaler.fit(bank_notes.drop('Class',axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "classifier.fit(X_train, y_train, steps=2000, batch_size=20)\n", "intent": "** Now fit classifier to the training data. Use steps=200 with a batch_size of 20. You can play around with these values if you want!**\n"}
{"snippet": "decTree = DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "kmeans.fit(colleges.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "scalar.fit(classified_data.drop('TARGET CLASS', axis = 1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "linear_model = LinearRegression()\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "linear_model.fit(X_train, Y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "mnb = MultinomialNB()\nmnb.fit(X_train, y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "decoder_h = Dense(intermediate_dim, activation='relu')\ndecoder_mean = Dense(original_dim, activation='sigmoid')\nh_decoded = decoder_h(z)\nx_decoded_mean = decoder_mean(h_decoded)\n", "intent": "Finally, we can map these sampled latent points back to reconstructed inputs:\n"}
{"snippet": "df = KNeighborsClassifier(n_neighbors=3)\ndf.fit(features, target)\n", "intent": "Set kNN's k value to 3 and fit data\n"}
{"snippet": "iris = st.datasets.get_rdataset('iris','datasets')\ny = iris.data.Species \nX = iris.data.ix[:, 0:4, le]\nx = X.as_matrix(columns=None)\ny = y.as_matrix(columns=None)\nmdl = sm.MNLogit(y, x)\nmdl_fit = mdl.fit()\nmdl_fit.summary()\n", "intent": "Setup the Logistic Regression \n"}
{"snippet": "plot_graph(10,11,len(all_pickles)-1,len(all_pickles),'Training Loss min {0:.2f}'.format(min(train_loss[10])),\n          'Validation Loss min {0:.2f}'.format(min(valid_loss[10])))\n", "intent": "Great! I see, that the model_10 gave me the best result. The model_12 is in the second place. Let's plot them for the further consideration.\n"}
{"snippet": "get_predictions(index=0, \n                partition='train',\n                input_to_softmax=deep_bdrnn_model(input_dim=161,\n                            units=200,\n                            recur_layers=2),\n                model_path='./results/model_12.h5')\n", "intent": "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='validation',\n                input_to_softmax=deep_bdrnn_model(input_dim=161,\n                            units=200,\n                            recur_layers=2),\n                model_path='./results/model_12.h5')\n", "intent": "Use the next code cell to visualize the model's prediction for the first example in the validation dataset.\n"}
{"snippet": "def weight_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial, name='bias')\nx = tf.placeholder(tf.float32, [None, 784], name='x')\ny_ = tf.placeholder(tf.float32, [None, 10], name='labels')\n", "intent": "The same functions above, rewritten and redfined in order to include name labels.\n"}
{"snippet": "def weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial, name='weight')\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial, name='bias')\nx = tf.placeholder(tf.float32, [None, 784], name='x')\ny_ = tf.placeholder(tf.float32, [None, 10], name='labels')\n", "intent": "The same functions above, rewritten and redfined in order to include name labels.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=22,\n          epochs=20,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "f = theano.function([x], y)\n", "intent": "Or compile a function\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: test_x,\n            labels_: test_y, \n            keep_prob: 1}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    word_embedding = tf.Variable(initial_value=tf.random_uniform((vocab_size, embed_dim),-1,1),name='word_embedding')\n    embedded_input = tf.nn.embedding_lookup(word_embedding, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def conv_layer(prev_layer, layer_depth, is_training):\n    strides = 2 if layer_depth % 3 == 0 else 1\n    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', use_bias=False)\n    conv_layer = tf.nn.relu(\n        tf.layers.batch_normalization(conv_layer, training=is_training))\n    return conv_layer\n", "intent": "Modify `conv_layer` to add batch normalization to the convolutional layers it creates. Feel free to change the function's parameters if it helps.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform(shape=(vocab_size,embed_dim),minval=-1,maxval=1,dtype=tf.float32))\n    embed = tf.nn.embedding_lookup(embedding,input_data) \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=600)\nrfc.fit(X_train, y_train)\n", "intent": "Now its time to train our model!\n**Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.**\n"}
{"snippet": "kmeans.fit(college.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "nb = MultinomialNB()\nnb.fit(X_train,y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "lda_5_topics_raw = utils.create_lda_model(corpus_raw, id2word_raw, 5)\nlda_5_topics_raw.print_topics()\n", "intent": "Let's create LDA models with different number of topics and see if we observe significant differences.\n"}
{"snippet": "G = nx.Graph()\nfor node in nodes:\n    name = df_persons.loc[1]['Name']\n    G.add_node(node, id=node, name=name)\nG.add_edges_from(list_links)\n", "intent": "We can now build the first graph with all links and all nodes:\n"}
{"snippet": "X = T.vector()\nX = T.matrix()\nX = T.tensor3()\nX = T.tensor4()\n", "intent": "Other tensor types\n==========\n"}
{"snippet": "param_grid = {'C':[0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\ngrid = GridSearchCV(SVC(), param_grid, verbose=3)\ngrid.fit(X_train, y_train)\n", "intent": "The model was very good, still do grid search for pratice\n"}
{"snippet": "pred =multilayer_perceptron(x, weights, biases)\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n", "intent": "Prepare model and train\n"}
{"snippet": "def bag_of_words_kernel(fitted_vectoriser, X,Y):\n    X_feats = fitted_vectoriser.transform(X)\n    if X is Y :\n        Y_feats = X_feats\n    else:\n        Y_feats = fitted_vectoriser.transform(Y)\n    K = X_feats.dot(Y_feats.T).todense() / len(vectorizer.get_feature_names())\n    return K\n", "intent": "Next, let's define the bag of words kernel, as a product of embedded documents.\n"}
{"snippet": "inputs = digits.data\ntargets = digits.target\nhidden_weights = np.random.random([64,10])\noutput_weights = np.random.random(10)\nlearning_rate = 0.01\nepochs = 10\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n", "intent": "So now to build out a nerual net to classify this.\n"}
{"snippet": "hidden_input = np.dot(inputs[0], hidden_weights)\nhidden_output = sigmoid(hidden_input)\nhidden_output.shape\n", "intent": "So the dot product of each input vector and the weights gives us 10 outputs:\n"}
{"snippet": "hidden_outputs = sigmoid(hidden_inputs)\nprint(hidden_outputs.shape)\nhidden_outputs\n", "intent": "Using the sigmoid function to calculate the output of the hidden layer:\n"}
{"snippet": "input = tf.placeholder(tf.float32, (None, 32, 32, 3))\nfilter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) \nfilter_bias = tf.Variable(tf.zeros(20))\nstrides = [1, 2, 2, 1] \npadding = 'VALID'\nconv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias\n", "intent": "How to construct a simple convo network:\n"}
{"snippet": "x = tf.add(5, 2)  \nwith tf.Session() as s:\n    output = s.run(x)\nprint(x) \noutput \n", "intent": "to print the value of a tensorflow variable, you have to run it in a session:\n"}
{"snippet": "def run():\n    output = None\n    logit_data = [2.0, 1.0, 0.1]\n    logits = tf.placeholder(tf.float32)\n    softmax = tf.nn.softmax([2.0, 1.0, 0.1])     \n    with tf.Session() as sess:\n        output = sess.run(softmax)\n    return output\nrun()\n", "intent": "[softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) returns an array of prob values\n"}
{"snippet": "import numpy as np\nx = theano.shared(np.zeros((2, 3), dtype=theano.config.floatX))\n", "intent": "- Symbolic + Storage\n"}
{"snippet": "import tensorflow as tf\nx = tf.constant(8)\ny = tf.constant(9)\nz = tf.multiply(x,y)\nsess = tf.Session()\nsess.run(z)\n", "intent": "testing if tensorflow works, using [this example](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/tensorflow.html).\n"}
{"snippet": "simple_model_coef_table = list(zip(significant_words, simple_model.coef_.flatten()))\nprint(simple_model_coef_table)\n", "intent": "After following same steps as with previous, full model, we are reaching table of coefficients based on our smaller list of words.\n"}
{"snippet": "k = 2\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(dn)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "from sklearn.neighbors import NearestNeighbors\nnn = NearestNeighbors(n_neighbors = len(X)).fit(X)\ndistances, indices = nn.kneighbors(X)\n", "intent": "*average taxi out time* and *average airport departure delay*\n"}
{"snippet": "model1 = naive_bayes.GaussianNB()\nmodel1.fit(Xtrain,ytrain)\nprint 'Score =',model1.score(Xtest,ytest)\nmodel2 = naive_bayes.MultinomialNB()\nmodel2.fit(Xtrain,ytrain)\nprint 'Score =',model2.score(Xtest,ytest)\nmodel3 = naive_bayes.BernoulliNB()\nmodel3.fit(Xtrain,ytrain)\nprint 'Score =',model3.score(Xtest,ytest)\n", "intent": "Instantiate the Naive Bayes predictor from scikit-learn with the training data. \n"}
{"snippet": "logreg_parameters = {'penalty':['l2'], 'solver':['liblinear','lbfgs'],\n         'cv':[2,3,4,5,6,7,8,9,10]}\ngsm = GridSearchCV(LogisticRegressionCV(),logreg_parameters, scoring='mean_squared_error',verbose = True, n_jobs = -1, error_score= 0)\ngsm.fit(Xtrain,ytrain)\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "clf = RandomForestClassifier()\n", "intent": "Here, we will be using the *country* column as the target and (*city*, *region*) as the input for the random forest.\n"}
{"snippet": "arch=resnet34\ndata = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))\nlearn = ConvLearner.pretrained(arch, data, precompute=True)\nlearn.fit(0.01, 2)\n", "intent": "I am going to use a <b>pre-trained</b> [resnet model](https://github.com/KaimingHe/deep-residual-networks).\n"}
{"snippet": "def relu(x): \n    return (x+np.abs(x))/2\ndef calc_auc_nl(rv, rm, gv, gm, bv, bm):\n    filter_size = 45\n    gray_image = (rv*relu(rgb_img[:,:,0]/255.0-rm)+gv*relu(rgb_img[:,:,1]/255.0-gm)+\n                  bv*relu(rgb_img[:,:,2]/255.0-bm))/(rv+gv+bv)\n    score_value = gray_image.astype(np.float32).flatten()\n    fpr2, tpr2, _ = roc_curve(ground_truth_labels,score_value)\n    return {'fpr': fpr2, 'tpr': tpr2, 'auc':auc(fpr2,tpr2), 'gimg': gray_image, 'fimg': filtered_image}\n", "intent": "Here we use non-linear approaches to improve the quality of the results\n"}
{"snippet": "count = theano.shared(0)\nnew_count = count + 1\nupdates = {count: new_count}\nf = theano.function([], count, updates=updates)\n", "intent": "- Store results of function evalution\n- `dict` mapping shared variables to new values\n"}
{"snippet": "CONV_SIZE = (10,10,3)\ndef calc_auc_conv2d(rcoefs):\n    coefs = rcoefs.reshape(CONV_SIZE)/rcoefs.sum()\n    score_image = relu(convolve(grey_img,coefs))\n    score_value = score_image.flatten()\n    fpr2, tpr2, _ = roc_curve(ground_truth_labels,score_value)\n    return {'fpr': fpr2, 'tpr': tpr2, 'auc': auc(fpr2,tpr2), 'gimg': score_image}\n", "intent": "Using the RGB instead of the gray value for the CNN\n"}
{"snippet": "iris = load()\nX = iris.data\ny = iris.target\nf = X.shape[1]\nrnd_d3 = DecisionTreeClassifier(max_features=int(f ** 0.5)) \nd3 = DecisionTreeClassifier() \n", "intent": "<img src='pics/bagging.png'>\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS',axis=1))\nscaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))\n", "intent": "**Fit the features data only to this estimator (leaving the TARGET CLASS column) and transform**\n"}
{"snippet": "kmeans = KMeans(n_clusters=2,verbose=0,tol=1e-3,max_iter=300,n_init=20)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "import numpy as np\nn = 1000\nboot_samples = np.empty((n, len(lrmod.coef_[0])))\nfor i in np.arange(n):\n    boot_ind = np.random.randint(0, len(X0), len(X0))\n    y_i, X_i = y.values[boot_ind], X0.values[boot_ind]\n    lrmod_i = LogisticRegression(C=1000)\n    lrmod_i.fit(X_i, y_i)\n    boot_samples[i] = lrmod_i.coef_[0]\n", "intent": "`scikit-learn` does not calculate confidence intervals for the model coefficients, but we can bootstrap some in just a few lines of Python:\n"}
{"snippet": "est = GradientBoostingRegressor(n_estimators=1000, max_depth=3, learning_rate=0.04,\n                                loss='huber', random_state=0)\nest.fit(X_train, y_train)\n", "intent": "Lets fit a gradient boosteed regression tree (GBRT) model to this dataset and inspect the model:\n"}
{"snippet": "from pymc3.glm import GLM\nwith pm.Model() as model:\n    GLM.from_formula('y ~ x', data)\n    glm_samples = pm.sample(1000)\n", "intent": "The model can then be very concisely specified in one line of code.\n"}
{"snippet": "rand_array = np.random.rand(4, 4)\ny = tf.identity(x)\nprint(sess.run(y, feed_dict={x: rand_array}))\n", "intent": "When the session is run, the `feed_dict` argument is used to feed in the value of x into the computational graph, via the placeholders.\n"}
{"snippet": "tf.reset_default_graph()\nsess = tf.Session()\n", "intent": "There are various ways to create matrices and manipulate them in TensorFlow\n"}
{"snippet": "Y_pred = tf.nn.softmax(tf.matmul(X, W) + b)\n", "intent": "The output of the logic regression is  $softmax(Wx+b)$\nNote that the dimension of *Y_pred* is *(nBatch, dimY)*\n"}
{"snippet": "data = np.random.random((1000, 32))\nlabels = np.random.random((1000, 10))\nmodel.fit(data, labels, epochs=10, batch_size=32)\n", "intent": "For small datasets, use in-memory NumPy arrays to train and evaluate a model. The model is \"fit\" to the training data using the `fit` method:\n"}
{"snippet": "lreg = LinearRegression()\n", "intent": "Next, we create a LinearRegression object.\n"}
{"snippet": "grid.fit(X, y);\n", "intent": "- Fit the model at each grid point & keep track of the scores.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier().fit(X, y)\n", "intent": "- Scikit-Learn uses the ``DecisionTreeClassifier`` estimator.\n"}
{"snippet": "gmm = GaussianMixture(n_components=4, random_state=42)\nplot_gmm(gmm, X)\n", "intent": "- View the initial four-component GMM results.\n"}
{"snippet": "gmm2 = GaussianMixture(n_components=2, covariance_type='full', random_state=0)\nplot_gmm(gmm2, Xmoon)\n", "intent": "- A two-component GMM as a clustering model doesn't really help.\n"}
{"snippet": "gmm16 = GaussianMixture(n_components=16, covariance_type='full', random_state=0)\nplot_gmm(gmm16, Xmoon, label=False)\n", "intent": "- If we use many more components and ignore the cluster labels, the result is much better.\n"}
{"snippet": "gmm = GaussianMixture(120, covariance_type='full', random_state=0)\ngmm.fit(data)\nprint(gmm.converged_)\n", "intent": "- It looks like ~120 components minimizes the AIC. Fit this to the data and confirm that it has converged.\n"}
{"snippet": "import torch.nn.functional as F\ndef binary_accuracy(preds, y):\n    rounded_preds = torch.round(F.sigmoid(preds))\n    correct = (rounded_preds == y).float() \n    acc = correct.sum()/len(correct)\n    return acc\n", "intent": "We implement the function to calculate accuracy...\n"}
{"snippet": "import keras.utils.visualize_util as vutil\nfrom IPython.display import SVG\nSVG(vutil.to_graph(autoencoder, recursive=True, show_shape=True).create(prog='dot', format=\"svg\"))\n", "intent": "We display a graphical representation of the autoencoder. \nYou can see that the last layer has an output shape of 10.\n"}
{"snippet": "kmeans.fit(df.drop(['Private'],axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "regressor = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from keras.models import Model, Sequential\nfrom keras.layers.convolutional import Convolution2D\nfrom keras.layers import Dense, Activation, MaxPooling2D, Flatten, Dropout\nfrom keras.callbacks import History \nhistory = History()\n", "intent": "Now we can train a conv net that will hopefully learn how the image should look\n"}
{"snippet": "model.fit(x= inputs, y = targets, nb_epoch = 1, batch_size = 1,callbacks=[history])\n", "intent": "Train the model now\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embed_input = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev = 0.01))\n    embed_input = tf.nn.embedding_lookup(embed_input, input_data)\n    return embed_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "logmodel  = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "dct = DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "class ReLU(Layer):\n    def __init__(self):\n        relu_forward = np.maximum(0,input)\n        return relu_forward\n    def backward(self, input, grad_output):\n", "intent": "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\n"}
{"snippet": "decoder = models.Sequential()\ndecoder.add(Layer(input_shape=(10,)))\ndecoding_layers = create_decoding_layers()\nfor i, l in enumerate(decoding_layers):\n    decoder.add(l)\ndecode = K.function([decoder.get_input(train=False)], [decoder.get_output(train=False)])\ntrained_weights = [l.get_weights() for l in autoencoder.decoding_layers]\nfor i, l in enumerate(decoding_layers):\n    l.set_weights(trained_weights[i])\n", "intent": "By activating a single representation neuron at a time, we can get our decoder to generate parts of digits\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\nknn\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "dtree = DecisionTreeClassifier() \n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "gscv = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.cluster import DBSCAN, AgglomerativeClustering\nagg = AgglomerativeClustering(n_clusters = 7, compute_full_tree=True).fit(finaldt)\n", "intent": "* Agglomerative clustering\n"}
{"snippet": "ols.fit(X_train, y_train)\nprint(\"R^2 for train set: %f\" %ols.score(X_train, y_train))\nprint('-'*50)\nprint(\"R^2 for test  set: %f\" %ols.score(X_test, y_test))\n", "intent": "- Do multiple linear regression with new data set.\n- Report the coefficient of determination from the training and testing sets.\n"}
{"snippet": "sentence = [\"Literally just 8 really really cute dogs\"]\nvectorizer.fit(sentence)\nprint(vectorizer.vocabulary_) \n", "intent": "Fitting the `CountVectorizer` learns the vocabulary\n"}
{"snippet": "svc = svc.fit(bag_of_words, labels)\n", "intent": "Now we learn the boundary!\n"}
{"snippet": "pipeline = pipeline.fit(training.title, training.label)\n", "intent": "Now we're ready to train!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0))\n    embedded_input = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "x = T.vector('x', dtype='float32')\nsigmoid_x = T.nnet.sigmoid(x)\nf_x = theano.function(inputs=[x], outputs=sigmoid_x)\nyvals = f_x(xvals)\npl.plot(xvals, yvals)\n", "intent": "Using a vector for x lets us compute the sigmoid of a vector of values instead of doing it\none-by-one\n"}
{"snippet": "import tensorflow as tf\nhidden_layer = tf.add(tf.matmul(features, hidden_weights), hidden_biases)\nhidden_layer = tf.nn.relu(hidden_layer)\noutput = tf.add(tf.matmul(hidden_layer, output_weights), output_biases)\n", "intent": "TensorFlow provides the ReLU function as `tf.nn.relu()`, as shown below:\n"}
{"snippet": "keep_prob = tf.placeholder(tf.float32) \nhidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\nhidden_layer = tf.nn.relu(hidden_layer)\nhidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\nlogits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n", "intent": "TensorFlow provides the `tf.nn.dropout()` function, which you can use to implement dropout.\nLet's look at an example of how to use `tf.nn.dropout()`.\n"}
{"snippet": "import tensorflow as tf\nsoftmax_data = [0.7, 0.2, 0.1]\none_hot_data = [1.0, 0.0, 0.0]\nsoftmax = tf.placeholder(tf.float32)\none_hot = tf.placeholder(tf.float32)\ncross_entropy = -tf.reduce_sum(tf.multiply(one_hot_data, tf.log(softmax_data)))\nwith tf.Session() as session:\n    output = session.run(cross_entropy, feed_dict={one_hot: one_hot_data, softmax: softmax_data})\n    print(output)\n", "intent": "Print the cross entropy using `softmax_data` and `one_hot_encod_label`.\n"}
{"snippet": "svc_pipe.fit(X_train,Y_train)\nsvc_pipe.score(X_test,Y_test)\n", "intent": "The average cross validation accuracy looks promising. Let's calcualte the accuracy on the test set which was split from the training set.\n"}
{"snippet": "happyModel.fit(x = X_train, y = Y_train, epochs = 40, batch_size = 100)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "def init_weights(shape):\n    init_rand_dist= tf.truncated_normal(shape,stddev=0.1)\n    return tf.Variable(init_rand_dist)\n", "intent": "1. Weight Initialization\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim]))\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "body_regression.fit(x_values, y_values)\n", "intent": "[SciKit-Learn](http://scikit-learn.org/stable/)'s built-in model has this (straight) curve fitting for us, easily.\n"}
{"snippet": "from pyspark.ml.regression import LinearRegression\nlr = LinearRegression(maxIter=10,solver=\"l-bfgs\")\nlrModel = lr.fit(Z_DF)\nprint(\"Coefficients: %s\" % str(lrModel.coefficients))\nprint(\"Intercept: %s\" % str(lrModel.intercept))\ntrainingSummary = lrModel.summary\nprint(\"numIterations: %d\" % trainingSummary.totalIterations)\nprint(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\nprint(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n", "intent": "**Exercise:**\n* Fit the model with the normal and l-bgfs algorithms. \n* Check the Spark UI and compare the different stages\n"}
{"snippet": "input1 = tf.constant(3.0)\ninput2 = tf.constant(2.0)\ninput3 = tf.constant(5.0)\nintermed =tf.add(input2,input3)\nmul = tf.mul(input1,intermed)\nwith tf.Session() as sess:\n    result = sess.run([mul,intermed])\n    print(result)\n", "intent": "Calling sess.run(var) on a tf.Session() object retrieves its value. Can retrieve multiple variables simultaneously with sess.run([var1,var2])\n"}
{"snippet": "from keras.layers import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3,input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "B = 100 \nboot_coefs = np.zeros((X_train3m.shape[1],B)) \nfor i in range(B):\n    sample_index = np.random.choice(range(len(y_train3m)), size=len(y_train3m), replace=True)\n    X_train_samples = X_train3m[sample_index]\n    y_train_samples = y_train3m[sample_index]\n    logistic_mod_boot = LogisticRegression(C=10, fit_intercept=False)\n    logistic_mod_boot.fit(X_train_samples, y_train_samples)\n    boot_coefs[:,i] = logistic_mod_boot.coef_\nboot_coefs.shape\n", "intent": "- Use bootstrap to get the significant coefficients at level of 95%:\n"}
{"snippet": "ols = LinearRegression(fit_intercept=True)\nols.fit(X_train2, y_train2)\nprint('OLS Train Score', ols.score(X_train2, y_train2))\nprint('OLS Test Score', ols.score(X_test2, y_test2))\n", "intent": "- Fit a linear regression model:\n"}
{"snippet": "X = df[ ['Income', 'Cards', 'Age', 'Education', 'Gender_Male', 'Race_Asian', 'Race_Caucasian'] ]\ny = df['Balance']\nmodel = linear_model.LinearRegression()\nmodel.fit(X,y)\nprint model.intercept_\nprint model.coef_\n", "intent": "First, find the coefficients of your regression line.\n"}
{"snippet": "train_X = train_df[ ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth'] ]\ntrain_y = train_df['Species']\nmodel = neighbors.KNeighborsClassifier(n_neighbors = 7, weights = 'uniform')\nmodel.fit(train_X, train_y)\nprint 'train = ', model.score(train_X, train_y)\ntest_X = test_df[ ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth'] ]\ntest_y = test_df['Species']\nprint 'test  = ', model.score(test_X, test_y)\n", "intent": "The error in the training set is less than the error is the test set\n"}
{"snippet": "train_X = train_df[ ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth'] ]\ntrain_y = train_df['Species']\nmodel = neighbors.KNeighborsClassifier(n_neighbors = 5, weights = 'uniform')\nmodel.fit(train_X, train_y)\nprint 'train = ', model.score(train_X, train_y)\ntest_X = test_df[ ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth'] ]\ntest_y = test_df['Species']\nprint 'test  = ', model.score(test_X, test_y)\n", "intent": "The error in the training set is less than the error is the test set\n"}
{"snippet": "model = tree.DecisionTreeRegressor()\nmodel.fit(train_X, train_y)\n", "intent": "(Check http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html as needed)\n"}
{"snippet": "model = ensemble.RandomForestClassifier(n_estimators = 1000, max_features = 4, min_samples_leaf = 5, oob_score = True)\nmodel.fit(train_X, train_y)\n", "intent": "(Check http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html as needed)\n"}
{"snippet": "model=models.ldamodel.LdaModel(corpus = corpus, num_topics = 10, id2word = id2word, passes = 10)\n", "intent": "(Check https://radimrehurek.com/gensim/models/ldamodel as needed)\n"}
{"snippet": "a = np.zeros((3,3))\nta = tf.convert_to_tensor(a)\nwith tf.Session() as sess:\n    print(sess.run(ta))\n", "intent": "* All previous examples have manually defined tensors. How can we input external data into TensorFlow?\n* Simple solution: Import from Numpy:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_w = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n    embed = tf.nn.embedding_lookup(embedding_w, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "x = tf.placeholder(tf.string)\ny = tf.placeholder(tf.int32)\nz = tf.placeholder(tf.float32)\nwith tf.Session() as sess:\n    output = sess.run(x, feed_dict={x: \"Test String\", y: 123, z: 123.4})\n", "intent": "It's also possible to set **more than one tensor** using feed_dict\n"}
{"snippet": "x = tf.reduce_sum([1,2,3,4,5])  \nwith tf.Session() as sess:\n    output = sess.run(x)\n    print (output)\n", "intent": "The **tf.reduce_sum()** function takes an array of numbers and sums them together\n"}
{"snippet": "x = tf.log(100.0)  \nwith tf.Session() as sess:\n    output = sess.run(x)\n    print (output)\n", "intent": "**tf.log()** takes the natural log of a number.\n"}
{"snippet": "save_file = \"./notMNIST_model.ckpt\"\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, save_file)\n    test_accuracy = sess.run(\n        accuracy,\n        feed_dict={features: test_features, labels: test_labels})\nprint('Test Accuracy: {}'.format(test_accuracy))\n", "intent": "** Load the trained model:**\n"}
{"snippet": "save_file = \"./MNIST_model/mnist_model.ckpt\"\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, save_file)\n    test_accuracy = sess.run(\n        accuracy,\n        feed_dict={x: test_features, y: test_labels})\nprint('Test Accuracy: {}'.format(test_accuracy))\n", "intent": "** Load Up the Trained Model:**\n"}
{"snippet": "def build_lstm(lstm_size, n_lstm_layers, keep_prob, batch_size):\n    lstm_basic = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    lstm_drop = tf.contrib.rnn.DropoutWrapper(lstm_basic, output_keep_prob=keep_prob)\n    multi_layers_lstm = tf.contrib.rnn.MultiRNNCell([lstm_drop] * n_lstm_layers)\n    initial_state = multi_layers_lstm.zero_state(batch_size, tf.float32)\n    return multi_layers_lstm, initial_state\n", "intent": "** Suppose n_lstm_layers=2, lstm_size=512, Then:**  \n** - there are 2 LSTM layers**  \n** - each LSTM cell has 512 layers**\n"}
{"snippet": "def build_lstm(lstm_size, n_lstm_layers, keep_prob, batch_size):\n    lstm_basic = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    lstm_drop = tf.contrib.rnn.DropoutWrapper(lstm_basic, output_keep_prob=keep_prob)\n    multi_layers_lstm = tf.contrib.rnn.MultiRNNCell([lstm_drop] * n_lstm_layers)\n    initial_state = cell.zero_state(batch_size, tf.float32)\n    return multi_layers_lstm, initial_state\n", "intent": "** Suppose n_lstm_layers=2, lstm_size=512, Then:**  \n** - there are 2 LSTM layers**  \n** - each LSTM cell has 512 layers**\n"}
{"snippet": "model.fit(x_train, y_train, batch_size=32, epochs=100, validation_data=(x_test, y_test), verbose=0)\nmodel2.fit(x_train, y_train, batch_size=32, epochs=100, validation_data=(x_test, y_test), verbose=0)\nmodel3.fit(x_train, y_train, batch_size=32, epochs=100, validation_data=(x_test, y_test), verbose=0)\nmodel4.fit(x_train, y_train, batch_size=512, epochs=100, validation_data=(x_test, y_test), verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "with tf.variable_scope(\"fooo\"):\n    v = tf.get_variable(\"v\",[1])\nassert v.name == \"fooo/v:0\"\n", "intent": "* Behavior depends on whether variable reuse enabled   \n* Case 1: reuse set to false\n    * Create and return new variable\n"}
{"snippet": "W_fc1 = weight_variable([250 * 1 * 16, 32])\nb_fc1 = bias_variable([32])\nh_pool2_flat = tf.reshape(h_pool1, [-1, 250 * 1 * 16])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n", "intent": "print(h_conv2.get_shape())\nh_pool2.get_shape()\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n", "intent": "- Use entropy (deviance) as the impurity measure, $i(\\mathcal{N})=-\\sum_j  p_j \\log_2 p_j$\n- Limit the tree depth to three (to prevent overfitting).\n"}
{"snippet": "tree.fit(X_train, y_train)\n", "intent": "Fit decision tree using training set.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=1000.0, random_state=0)\nlr.fit(X_bin[:,0].reshape(-1,1), y_bin)\n", "intent": "Train a logistic regression model with scikit-learn\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr1 = LogisticRegression(C=1000.0, random_state=0)\nlr1.fit(glass['al'].reshape(-1,1), glass['household'].reshape(-1,1))\n", "intent": "Train a logistic regression model with scikit-learn\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(criterion = 'entropy', n_estimators = 100, random_state = 1, n_jobs = 2)\nX_train = dataset['train']['image']\ny_train = dataset['train']['label']\nforest.fit(X_train, y_train)\n", "intent": "- http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr3 = LogisticRegression(C=0.1, random_state=0)\nlr3.fit(X_std, y)\nplot_decision_regions(X_std, y, lr3)\n", "intent": "Question: What is the test (prediction) results when $C$ is very small?\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(max_depth=3, criterion = 'entropy', n_estimators = 100, random_state = 1, n_jobs = 2)\nX_train = dataset['train']['image']\ny_train = dataset['train']['label']\nforest.fit(X_train, y_train)\n", "intent": "- http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "with tf.variable_scope(\"foooo\"):\n    v = tf.get_variable(\"v\",[1])\nwith tf.variable_scope(\"foooo\",reuse=True):\n    v1 = tf.get_variable(\"v\",[1])\nassert v1 == v\n", "intent": "* Case 2: Variable reuse set to true\n    * Search for existing variable with given name. Raise ValueError if none found\n"}
{"snippet": "self.embedding(input).shape\n", "intent": "`embedded = self.embedding(input).view(1, 1, -1)`\n"}
{"snippet": "pred = torch.sigmoid(x)\nloss = F.binary_cross_entropy(pred, y)\nloss\n", "intent": "The above but in pytorch.\n"}
{"snippet": "def softmax(x): return x.exp() / (x.exp().sum(-1)).unsqueeze(-1)\ndef nl(input, target): return -input[range(target.shape[0]), target].log().mean()\npred = softmax(x)\nloss=nl(pred, target)\nloss\n", "intent": "This version is most similar to the math formula, but not numerically stable.\n"}
{"snippet": "p = 0.4\nm = torch.nn.Dropout(p)\n", "intent": "Create a dropout layer `m` with a dropout rate `p=0.4`: \n"}
{"snippet": "train_model(sz, bs, lr)\n", "intent": "We see a validation accuracy of 94%. \n"}
{"snippet": "train_model(sz, bs, lr)\n", "intent": "We see a validation accuracy of 86%. \n"}
{"snippet": "train_model(sz, bs, lr)\n", "intent": "This gives us a 10-year average prediction error.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression()\nlinear_model.fit(X_train,Y_train)\nCoeff = linear_model.coef_\nprint('Coefficients: \\n ', Coeff)\n", "intent": "Estimate the coefficients for each input feature. Construct and display a dataframe with coefficients and X.columns as columns\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\n", "intent": "Check: coef_ and intercept_ functions can help you get coefficients & intercept\n"}
{"snippet": "import tensorflow as tf\nx = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.nn.softmax(tf.matmul(x, W) + b) \ny\n", "intent": "blue: positive weights, red: negative weights\n"}
{"snippet": "W = tf.Variable(tf.zeros(shape=[13,1]), name=\"Weights\")\nb = tf.Variable(tf.zeros(shape=[1]), name=\"Bias\")\n", "intent": "Define Weights and Bias\n"}
{"snippet": "x = tf.placeholder(shape=[None,4],dtype=tf.float32, name='x-input')\nx_n = tf.layers.batch_normalization(x, training=True)\ny_ = tf.placeholder(shape=[None],dtype=tf.float32, name='y-input')\n", "intent": "1.Define input data placeholders\n"}
{"snippet": "W = tf.Variable(tf.zeros(shape=[4,1]), name=\"Weights\")\nb = tf.Variable(tf.zeros(shape=[1]), name=\"Bias\")\n", "intent": "2.Define Weights and Bias\n"}
{"snippet": "def con2d(x,W):\n    return tf.nn.conv2d(input=x, filter=W, strides=[1,1,1,1], padding=\"SAME\")\ndef ang_pool_2x2(x):\n    return tf.nn.avg_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n", "intent": "input ( 28 * 28 * 1)\noutput : single scalar numer - if it is fake or not\n"}
{"snippet": "param_grid = [\n        {'preparation__num_pipeline__imputer__strategy': ['mean', 'median', 'most_frequent'],\n         'feature_selection__k': [3, 4, 5, 6, 7]}\n]\ngrid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n                                scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\ngrid_search_prep.fit(housing, housing_labels)\n", "intent": "Question: Automatically explore some preparation options using `GridSearchCV`.\n"}
{"snippet": "reset_graph()\n", "intent": "Let's load the data:\n"}
{"snippet": "learning_rate = 0.01\nX = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\ny = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\ntheta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\ny_pred = tf.matmul(X, theta, name=\"predictions\")\nerror = y_pred - y\nmse = tf.reduce_mean(tf.square(error), name=\"mse\")\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ntraining_op = optimizer.minimize(mse)\ninit = tf.global_variables_initializer()\n", "intent": "To implement manin batch Gradient Descent.\nchange the definition of X and y in the construction phase to make them placeholder nodes\n"}
{"snippet": "reset_graph()\nbatch_norm_momentum = 0.9\n", "intent": "ELU activation and Batch norm -> each layer\n"}
{"snippet": "with tf.Session() as sess:\n    init.run()\n    saver.restore(sess, \"./my_model_final.ckpt\")\n    for epoch in range(n_epochs):\n        for iteration in range(mnist.train.num_examples//batch_size):\n            X_batch, y_batch = mnist.train.next_batch(batch_size)\n            sess.run(training_op, feed_dict = {X:X_batch, y:y_batch})\n        accuracy_val =accuracy.eval(feed_dict = {X: mnist.test.images, y:mnist.test.labels})\n        print(epoch, \"Test accuracy : \", accuracy_val)\n    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")\n", "intent": "We can train the new model\n"}
{"snippet": "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n", "intent": "We then convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool.\n"}
{"snippet": "from sklearn import tree\nclf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,min_samples_leaf=5)\nclf = clf.fit(X_train,y_train)\n", "intent": "Now, we can create a new DecisionTreeClassifier and use the fit method of the classifier to do the learning job.\n"}
{"snippet": "gnb_classifier_dirty = GaussianNB()\nx2 = news_A.drop({'class'}, axis=1)\ny2 = news_A['class']\ngnb_classifier_dirty.fit(x2, y2)\ngnb_classifier_dirty.score(x2, y2)\nnews_A.describe()\n", "intent": "Fit a Gaussian Naive Bayes model to the original dataset A (including the outliers). Display the classification accuracy on the training dataset.\n"}
{"snippet": "em_3 = evaluate_model(y_3, lr_f5_pred_3)\nprint('RMSE: {0}, MAE: {1}, CC: {2}'.format(em_3[1], em_3[2], em_3[3]))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "kmeans = KMeans(n_clusters=5, random_state=30)\n_= kmeans.fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\ngnb_classifier = GaussianNB()\nx = news_A_clean.drop({'class'}, axis=1)\ny = news_A_clean['class']\ngnb_classifier.fit(x, y)\ngnb_classifier.score(x, y)\n", "intent": "<span style=\"color:red\">OK\n"}
{"snippet": "gnb_classifier_dirty = GaussianNB()\nx2 = news_A.drop({'class'}, axis=1)\ny2 = news_A['class']\ngnb_classifier_dirty.fit(x2, y2)\ngnb_classifier_dirty.score(x2, y2)\nnews_A.describe()\n", "intent": "<span style=\"color:red\">OK\n"}
{"snippet": "for bin in [x / 1.0 for x in range(1, 10)]:\n    bern = BernoulliNB(binarize=bin)\n    bern.fit(train_data, train_labels)\n    print 'binarize:', bin, 'bernoulli accuracy: %3.2f' %bern.score(test_data, test_labels)\n", "intent": "What choices did you make with manipulating the features above? Try tuning these choices, can you improve the accuracy?\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_normal([vocab_size, embed_dim], stddev=0.1), name='embeddings')\n    embed = tf.nn.embedding_lookup(embedding, input_data, name='embed')\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='ADAM', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "W_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n", "intent": "In order to build a deep network, we stack several layers of this type. The second layer will have 64 features for each 5x5 patch.\n"}
{"snippet": "stride=2\nprint('Input size: ', x.shape)\nconv1=nn.Conv2d(in_channels=1, out_channels=128, kernel_size=(3,3), stride=stride, padding=(1,1))\nprint('After convolution size: ', conv1(x).shape)\npool1 = nn.MaxPool2d(kernel_size=(2,2),stride=stride)\nprint('After pool size: ', pool1(conv1(x)).shape)\nprint('Output size is: 10')\n", "intent": "Clearly mention the sizes for your input, kernel, pooling, and output at each step until you get final output vector with 10 probabilities\n"}
{"snippet": "print('Results for Adam Optimizer')\nif use_gpu:\n    model_Q3p6.cuda()\n    optimizer = torch.optim.Adam(model_Q3p6.parameters(), lr = 0.0001)\nmodel_Q3p6.train()    \ninit_weights(model_Q3p6)\nmodel_Q3p8b = train_model(model_Q3p6, criterion, optimizer,\n                       num_epochs=40, trainVal=['train','val'],verbose=False)  \ny_s, y_t=inference(model_Q3p8b,validation_loader)\nr_AUCAdam = get_AUC(y_s, y_t)\n", "intent": "We choose SGD with momentum as an optimizer. Investigate the effect of at least two other optimizers in model performance.\n"}
{"snippet": "model_conv.train()   \nmodel_conv_ft = train_model(model_conv, criterion, optimizer,\n                       num_epochs=100, trainVal=['train','val'])\n", "intent": "Train the model for 100 epochs and save the weights of the best model using the validation loss. Plot train and validation loss curves\n"}
{"snippet": "model_conv_fine_tune.train()   \nmodel_conv_fine_tune_ft = train_model(model_conv_fine_tune, criterion, optimizer,\n                       num_epochs=100, trainVal=['train','val']) \n", "intent": "Train the model for 100 epochs and save the weights of the best model using the validation loss. Plot train and validation loss curves\n"}
{"snippet": "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=5)\n", "intent": "Train and evaluate\n^^^^^^^^^^^^^^^^^^\nIt should take around 15-25 min on CPU. On GPU though, it takes less than a\nminute.\n"}
{"snippet": "def test_sentence(sentence, model):\n    model.eval()\n    test_tensor = torch.LongTensor(sentences_to_padded_index_sequences(vocab_indices, [sentence]).astype(int))\n    score = model(Variable(test_tensor)).data.numpy()[0][0][0]\n    return (\"positive\" if score > 0.5 else \"negative\", score)\n", "intent": "Play around with trained model:\n"}
{"snippet": "get_predictions(index=0, \n                partition='train',\n                input_to_softmax=final_model(input_dim=13, \n                        filters=200,\n                        kernel_size=11, \n                        conv_stride=2,\n                        conv_border_mode='valid',\n                        dropout= 0.2,\n                        units=200), \n                model_path='results/model_end.h5')\n", "intent": "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='validation',\n                input_to_softmax=final_model(input_dim=13, \n                        filters=200,\n                        kernel_size=11, \n                        conv_stride=2,\n                        conv_border_mode='valid',\n                        dropout= 0.2,\n                        units=200),\n                model_path='results/model_end.h5')\n", "intent": "Use the next code cell to visualize the model's prediction for the first example in the validation dataset.\n"}
{"snippet": "text_clf_lsvc.fit(X_train, y_train)\n", "intent": "Next we'll run Linear SVC\n"}
{"snippet": "W_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n", "intent": "Finally, we add a softmax layer, just like for the one layer softmax regression.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(256, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(32, activation='softmax'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(num_classes))\nsgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss = 'mean_squared_error', optimizer=sgd, metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "lm.fit(X_train, y_train)\nlmRidgeCV.fit(X_train, y_train)\nlmLassoCV.fit(X_train, y_train)\n", "intent": "* Luego, hacemos el fit de los tres estimadores, lo cual nos lleva a...\n"}
{"snippet": "train_cols = ['gre','gpa','prestige_2','prestige_3','prestige_4','intercept']\nlogit = sm.Logit(data[\"admit\"], data[train_cols])\nresult = logit.fit()\nprint(result.summary2())\n", "intent": "**Se descarta prestige_1 para evitar la [multicolinealidad](https://en.wikipedia.org/wiki/Multicollinearity)**\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X, y);\n", "intent": "Vamos a aplicar a este conjunto de datos un modelo de tipo Gaussian Naive Bayes.\n"}
{"snippet": "model_pca = pca.fit(Xtrain)\nX_train_pca = model_pca.transform(Xtrain)\n", "intent": "Vamos a reducir las dimensiones del conjunto de entrenamiento con PCA\n"}
{"snippet": "model_svc = SVC(kernel='rbf', class_weight='balanced', C=1, gamma = 0.001)\n", "intent": "```\nkernel: linear\nC: 0.015\ngamma: auto\n```\n"}
{"snippet": "grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\ngrid.fit(X_train, y_train)\n", "intent": "Ajustamos los modelos\n"}
{"snippet": "from IPython.display import Image\nfrom sklearn.tree import export_graphviz\nimport pydotplus\nbdtr = BaggingRegressor(DecisionTreeRegressor(max_depth=2),n_estimators=50)\nbdtr.fit(X,y)\n", "intent": "Para poder correr estas visualiaciones, abrir una terminal y ejecutar:\npip install pydotplus <br />\nconda install graphviz\n"}
{"snippet": "dbscn = DBSCAN(eps = 0.8, min_samples = 7).fit(X)\n", "intent": "Implementamos `DBSCAN`\n"}
{"snippet": "model = Sequential()\nmodel.add(LSTM(64, return_sequences=True, stateful=True, \n               batch_input_shape=(batch_size, 1, X_train.shape[-1])))\nmodel.add(LSTM(64, return_sequences=True, stateful=True))\nmodel.add(LSTM(64, stateful=True))\nmodel.add(Dense(num_classes, activation='softmax'))\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n", "intent": "We'll use a three-layer stacked LSTM with 64 cells and Adam optimizer with initial learning rate of 0.001\n"}
{"snippet": "hide_code\ndef model():\n    model = Sequential()\n    return model\nmodel = model()\n", "intent": "Define a model architecture and compile the model.\n"}
{"snippet": "hide_code\ndef rnn_model():\n    model = Sequential()\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])    \n    return model \nrnn_model = rnn_model()\n", "intent": "Define a model architecture and compile the model.\n"}
{"snippet": "hide_code\ndef rnn_model():\n    model = Sequential()\n    model.add(LSTM(52, return_sequences=True, input_shape=(1, 13)))\n    model.add(LSTM(52, return_sequences=True))\n    model.add(LSTM(52, return_sequences=False))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse', metrics=['mae'])       \n    return model \nrnn_model = rnn_model()\n", "intent": "Define a model architecture and compile the model.\n"}
{"snippet": "hide_code\ndef model():\n    model = Sequential()\n    return model\nmodel = model()\n", "intent": "Define a model architecture and compile the model for color images.\n"}
{"snippet": "hide_code\ny_train_c = np.array([np.argmax(y) for y in y_train])\ny_test_c = np.array([np.argmax(y) for y in y_test])\nclf = GradientBoostingClassifier().fit(x_train.reshape(-1, 32*32*3), y_train_c)\nclf.score(x_test.reshape(-1, 32*32*3), y_test_c)\n", "intent": "Let's compare the results with classifying algorithms.\n"}
{"snippet": "hide_code\ny_train2_c = np.array([np.argmax(y) for y in y_train2])\ny_test2_c = np.array([np.argmax(y) for y in y_test2])\nclf = GradientBoostingClassifier().fit(x_train2.reshape(-1, 32*32), y_train2_c)\nclf.score(x_test2.reshape(-1, 32*32), y_test2_c)\n", "intent": "Let's compare the results with classifying algorithms.\n"}
{"snippet": "hide_code\nbrands = data['brand_label'].values\nproducts = data['product_label'].values\nimages = data_to_tensor(data['file']);\n", "intent": "Let's create tensors of variables and display some examples of images.\n"}
{"snippet": "hide_code\ndef cb_model():\n    model = Sequential()\n    return model\ncb_model = cb_model()\n", "intent": "We should have an accuracy \n- greater than 14.3% for the first target (`brand`) and \n- greater than 10% for the second target (`product`).\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(3, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=32, verbose=1)\ncallbacks_list = [early_stopping]\nhist = model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.2, shuffle=True, verbose=2)\n", "intent": "Let's train the model with early stopping:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=3, batch_size=100, verbose=2, validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1500, activation='elu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "hist = model.fit(x_train, y_train, validation_split=0.2 ,epochs=15, batch_size=120)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model.fit(x_train, y_train, epochs=15, batch_size=120, verbose=2, validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "reg = sk_lm.LinearRegression();\nreg.fit(X,y);\n", "intent": "Now we instantiate the *LinearRegression* class and call the function *fit(X,y)* in order to estimate the linear regression parameters.\n"}
{"snippet": "reg = sk_lm.LinearRegression();\nreg.fit(np.array(X_all),y);\nreg_fs = sk_lm.LinearRegression();\nreg_fs.fit(np.array(X_all)[:,fs_indices],y);\nprint(\"Mean squared error on training data, full input variables: \", mse(reg,X_all,y));\nprint(\"Mean squared error on training data, selected input variables: \", mse(reg_fs,np.array(X_all)[:,fs_indices],y));\n", "intent": "Forward selection will make our regression fit worse on our training data, but better on test data. We can show this looking at the prediction errow:\n"}
{"snippet": "logreg_simple = skl.linear_model.LogisticRegression();\nX_house_log_area = [X_house_log[\"area\"]];\nX_house_log_area_zip = [list(i) for i in zip(*X_house_log_area)];\nlogreg_simple.fit(X_house_log_area_zip,y_house_log);\n", "intent": "A more intuitive way to show how logistic regression works is to show the output for a single input variable.\n"}
{"snippet": "model = smf.ols(formula = 'MEDV ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT', data = training_df)\nfit = model.fit()\nfit.summary()\n", "intent": "Seems comparable to before -- let's see how it performs using all predictors:\n"}
{"snippet": "model = pm.Model([theta, phi, z, eta, y_tau, y, w])\nmcmc = pm.MCMC(model)\nmcmc.sample(iter=1000, burn=100, thin=2)\n", "intent": "Let's infer the latent variables using the Metropolis sampler:\n"}
{"snippet": "xgbr_all = []\nfor target, setting in zip(targets, settings):\n    xgb2 = XGBRegressor(**setting)\n    xgb2.fit(df0[predictors], df0[target], eval_metric=[\"rmse\"], verbose=False)\n    xgbr_all.append(xgb2)\nfName = 'xgbr_' + strftime(\"%Y_%m_%d_%H_%M_%S\")\npath_to_file = 'fitted_models/'+fName+'.model'\njoblib.dump(xgbr_all, path_to_file)\nprint('\\nExtreme gradient boosting regression model saved to {0}'.format(path_to_file))\n", "intent": "<a id='Cell16'></a>\n"}
{"snippet": "xgbc_all = []\nfor target, setting in zip(targets, settings):\n    xgbc = XGBClassifier(**setting)\n    %time xgbc.fit(df0[predictors], df0[target], eval_metric=[\"logloss\"], verbose=False)\n    xgbc_all.append(xgbc)\nfName = 'xgbc_' + strftime(\"%Y_%m_%d_%H_%M_%S\")\npath_to_file = 'fitted_models/'+fName+'.model'\njoblib.dump(xgbc_all, path_to_file)\nprint('\\nExtreme gradient boosting classifier model saved to {0}'.format(path_to_file))\n", "intent": "<a id='Cell18'></a>\n"}
{"snippet": "def leastsq_reg(x,y):\n    reg = sm.OLS(y, sm.add_constant(x)).fit()\n    print reg.summary()\n    return reg\n", "intent": "With Statsmodels for good measure\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=64,\n          epochs=50,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "config = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.9\nwith tf.Session(config=config) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('./checkpoints/'))\n    new_predicted = sess.run( tf.argmax(logits, 1), feed_dict={x:X_new })\n    print('new_predicted values:', new_predicted)\n", "intent": "After prerpoced the new input data, then the pretrained model is loaded and tested with new mini dataset of 10 images.\n"}
{"snippet": "def get_top_five(in_image):\n    with tf.Session(config=config) as sess:\n        saver.restore(sess, tf.train.latest_checkpoint('./checkpoints/'))\n        softmax = tf.nn.softmax(logits)\n        top_prob = tf.nn.top_k(softmax, k =5 )\n        top_five = sess.run( top_prob, feed_dict={x: in_image })\n    return top_five\n", "intent": "A new function to get top 5 probabilities was defined.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(2000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=5, batch_size=100, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "logit = sm.Logit(data['admit'], data[train_cols])\nresult = logit.fit()\nresult\nresult.summary()\n", "intent": "hint 1: np.exp(X)\nhint 2: conf['OR'] = params\n           conf.columns = ['2.5%', '97.5%', 'OR']\n"}
{"snippet": "model_test = pm.Model([theta_t, z_t, zbar_t, y_t, w_t])\nmcmc_test = pm.MCMC(model_test)\nmcmc_test.sample(iter=1000, burn=100, thin=2)\n", "intent": "Let's run the Metropolis sampler to obtain posterior distribution over our latent variables.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_initializer = tf.random_normal_initializer(mean=0.0, stddev=1.0/ np.sqrt(embed_dim)) \n    embedding = tf.get_variable('X', shape=[vocab_size, embed_dim],  \n                                     initializer=embedding_initializer,\n                                     trainable=True,\n                                     dtype=tf.float32)\n    input_embedding = tf.nn.embedding_lookup(embedding, input_data) \n    return input_embedding\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def logModel(W, b, x, y):\n    h = np.maximum(np.dot(x, W[:, :50]) + b[:, :50], 0.)\n    y_hat = np.dot(h, W[:, 50:].T) + b[:, 50:]\n    return np.sum( .5 * -(y - y_hat)**2 )\ndef fprop(W, b, x):\n    h = np.maximum(np.dot(x, W[:, :50]) + b[:, :50], 0.)\n    return np.dot(h, W[:, 50:].T) + b[:, 50:]\ndLogModel_dW = grad(logModel)\ndLogModel_db = grad(lambda b, W, x, y: logModel(W, b, x, y))\n", "intent": "Next let's define the model: a one-hidden-layer neural network with 50 units...\n"}
{"snippet": "def init_regression_model(in_size, std=.1):\n    return {'w':tf.Variable(tf.random_normal([in_size, 1], stddev=std)), 'b':tf.Variable(tf.zeros([1,]))}\ndef linear_regressor(X, params):\n    return tf.matmul(X, params['w']) + params['b']\n", "intent": "Next we need to create functions to initialize and run the regression model...\n"}
{"snippet": "model_params = init_regression_model(input_d)\nlinear_model_out = linear_regressor(X, model_params)\nce_cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(linear_model_out, Y))\n", "intent": "Let's use those functions and create a symbolic cost...\n"}
{"snippet": "model = keras.models.load_model('../data/keras_emotion_model.h5')\nwith open(\"../data/keras_emotion_history.p\", \"rb\") as f:\n    history= pickle.load(f)\n", "intent": "Load model and history:\n"}
{"snippet": "img_rows = 28 \nimg_cols = 28\nchannels = 1\nnpixels = img_rows * img_cols * channels\nimg_shape = (img_rows, img_cols, channels)\noptimizer = keras.optimizers.Adam(0.0002, 0.5)\n", "intent": "We will first build the discriminitive model, aka the discriminator.\nWe start with some definitions.\n"}
{"snippet": "combined = keras.models.Model(noise, valid)\ncombined.compile(\n    loss='binary_crossentropy', \n    optimizer=optimizer\n)\n", "intent": "The combined model  (stacked generator and discriminator) takes noise as input => generates images => determines validity.\n"}
{"snippet": "generator = keras.models.load_model('../data/gan_generator.h5')\ndiscriminator = keras.models.load_model('../data/gan_discriminator.h5')\ncombined = keras.models.load_model('../data/gan_combined.h5')\n", "intent": "You can see that the generator very quickly becomes pretty good at generating nice digits, and then bounces around.\n"}
{"snippet": "model.add(\n    keras.layers.Dense(ncats, activation='softmax'))\n", "intent": "We add a Softmax regression layer, similar to the previous, simpler example, by adding a dense layer with softmax activation.\n"}
{"snippet": "x = Input(shape=(original_dim,))\nh = Dense(intermediate_dim, activation='relu')(x)\nz_mean = Dense(latent_dim)(h)\nz_log_sigma = Dense(latent_dim)(h)\nz = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\ndecoder_h = Dense(intermediate_dim, activation='relu')\ndecoder_mean = Dense(original_dim, activation='sigmoid')\nh_decoded = decoder_h(z)\nx_decoded_mean = decoder_mean(h_decoded)\n", "intent": "Let's define the variational autoencoder architecture:\n"}
{"snippet": "def softmax(x):\n    x = x.squeeze()\n    expx = np.exp(x - x.sum())\n    return expx / expx.sum()\ndef cross_entropy(predictions, targets):\n    return sum([-np.log(p[t]) for p, t in zip(predictions, targets)])\n", "intent": "Let's get some of the functions we already know out of the way:\n"}
{"snippet": "W_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv=tf.nn.softmax(h_fc1_drop @ W_fc2 + b_fc2)\n", "intent": "We add a Softmax regression layer, similar to the previous, simpler example, including the regression coefficients, bias, and the softmax function:\n"}
{"snippet": "model = keras.models.load_model('../data/keras_esc50_model.h5')\nwith open(\"../data/keras_esc50_history.p\", \"rb\") as f:\n    history= pickle.load(f)\n", "intent": "Load pre-saved model and history:\n"}
{"snippet": "plot_model(model, to_file='tmp.png')\nimage.load_img('tmp.png')\n", "intent": "Now, let's see the ResNet50 architecture.\n"}
{"snippet": "model = keras.models.load_model('../data/keras_cnn_model.h5') \n", "intent": "Let's load the model we trained on MNIST data in the [CNN session](K_CNN.ipynb).\n"}
{"snippet": "history = model.fit(\n    x_train, y_train,\n    batch_size=50,\n    epochs=5,\n    validation_data=(x_test, y_test)\n).history\n", "intent": "Finally, we re-train the model on the new data.\n"}
{"snippet": "knn = nb.KNeighborsClassifier()\nknn.fit(X_train, y_train) \n", "intent": "Import the nearest-neighbor classifier, then create and fit it:\n"}
{"snippet": "reg = linear_model.LinearRegression()\nreg.fit(X_train, y_train)\nscore = reg.score(X_test, y_test)\nprint('Test score:', score)\n", "intent": "We want to see if we can remove any one feature without reducing our accuracy, or even improving it.\nFirst, let's fit the full linear model:\n"}
{"snippet": "scores = np.empty(n_features)\nfor j in range(n_features):\n    reg = linear_model.LinearRegression()\n    reg.fit(X_train[:,columns[j]], y_train)\n    scores[j] = reg.score(X_test[:,columns[j]], y_test)\n", "intent": "and calculate the score without each of the columns:\n"}
{"snippet": "y = CustomVariationalLayer()([x, x_decoded_mean])\nvae = Model(x, y)\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nvae.compile(loss=None, optimizer=adam, metrics=['accuracy'])\nvae.summary()\n", "intent": "We can finally compile our variational autoencoder model using Adam optimizer:\n"}
{"snippet": "model.compile(\n    loss=keras.losses.categorical_crossentropy,\n    optimizer=keras.optimizers.Adam(), \n    metrics=['accuracy']\n)\n", "intent": "**Compile the model.**\n"}
{"snippet": "hidden_size = 100\nmodel = keras.models.Sequential()\nmodel.add(\n    keras.layers.Dense(hidden_size, input_shape=(size**2,), activation='relu'))\nmodel.add(\n    keras.layers.Dense(hidden_size, activation='relu'))\nmodel.add(\n    keras.layers.Dense(3, activation='softmax'))\nmodel.compile(keras.optimizers.sgd(lr=.2), \"mse\")\n", "intent": "Now everything is ready. \n**Create your model, train it, and score it.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), minval = -1, maxval = 1), dtype=tf.float32)\n    embed = tf.nn.embedding_lookup(params = embedding, ids = input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model=LogisticRegression(C=1.)\nmodel.fit(X_train_transform, y_train)\n", "intent": "Our matrice is only with one-gram words and still have a voculary of 586147 words\n"}
{"snippet": "parameters = {\"n_estimators\" : [10, 50, 100, 150], \"max_depth\": [None, 5, 10], \"bootstrap\" : [True, False]}\nmodel = RandomForestClassifier()\nclf = GridSearchCV(model, parameters, cv=5, scoring=\"accuracy\")\nclf.fit(X, y)\ncvres = clf.cv_results_\nfor score, param in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(score, param)\n", "intent": "Now we have our 2 best models <b>RandomForestClassifier</b> and <b>BaggingClassifier</b>. We can fine tune them usign a GridSearch\n"}
{"snippet": "model = load_model(\"model\")\n", "intent": "Now let's look at result but still with a critic eye as we still use the test set which was used for the early stop...\n"}
{"snippet": "clf = SVC()\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\nprint(clf.score(X_test, y_test))\n", "intent": "We can now try some models\n"}
{"snippet": "X_ph = tf.placeholder(tf.float32, shape=(None, m), name=\"X\")\nY_ph = tf.placeholder(tf.float32, shape=(None, 1), name=\"Y\")\ntheta = tf.Variable(tf.random_uniform([m, 1], -1, 1), name=\"theta\")\n", "intent": "Let's now create the tensorflow graph\n"}
{"snippet": "acc_summary = tf.summary.scalar(\"Accuracy\", accuracy)\nfile_writter = tf.summary.FileWriter(\"/saves/summary/BN_elu-{}-{}/\".format(size_hidden_layer_1, size_hidden_layer_2), tf.get_default_graph())\n", "intent": "For the result presented below, I changed the name of the save based on network topology and activation function.\n"}
{"snippet": "model = Sequential()\nmodel.add(LSTM(32, input_dim=1)) \nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(X_train, y_train, nb_epoch=100, batch_size=5, verbose=2)\n", "intent": "Next, we define our model:\n"}
{"snippet": "newModel = Sequential()\nnewModel.add(LSTM(units=7, stateful=True, batch_input_shape=(1,1,7), return_sequences=True))\nnewModel.set_weights(model.get_weights())\n", "intent": "Now let's create both model as one-to-many and evaluate them 20 times on 100 words generated.\n"}
{"snippet": "reference = out[-1].flatten()\nentropies = [entropy(x.flatten(), reference) for x in out]\ntexts = [\"{} comps\".format(i+1) for i in range(6)] + [\"Perfect filter\"]\n", "intent": "As we did previously, we can check the entropy vs the perfect filtered image. We can after that, plot it vs the time spet to render the time.\n"}
{"snippet": "def multilayer_perceptron(x, weights, biases):\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n    layer_1 = tf.nn.relu(layer_1)\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n    layer_2 = tf.nn.relu(layer_2)\n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n    return out_layer\n", "intent": "We are going to use Adam Optimizer to get the model right. See [here](https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218)\n"}
{"snippet": "scaler.fit(bank_note)\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "import tensorflow as tf\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\nclassifier = learn.DNNClassifier(feature_columns=feature_columns,\n                                hidden_units=[10,20,10],\n                                n_classes=3,\n                                model_dir=\"./ouput\")\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "dnn_keras_model = models.Sequential()\n", "intent": "See more of Keras [here](https://keras.io/)\n"}
{"snippet": "with tf.Session() as sess:\n    writer = tf.summary.FileWriter(\"./output\", sess.graph)\n    print(sess.run(c))\n    writer.close()\n", "intent": "Visualization with TensorBoard\n"}
{"snippet": "graph_one = tf.get_default_graph()\n", "intent": "See, a different graph (different place in memory)\n"}
{"snippet": "graph_two = tf.Graph()\n", "intent": "The same as the default graph that we saw at the beginning\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n", "intent": "Build a Linear Regression model\n"}
{"snippet": "g = Graph()\ng.set_as_default()\n", "intent": "Negative means that (2, -10) falls under the line - **blue**\n"}
{"snippet": "knn.fit(X_test, y_test)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "nb_fit = nb.fit(X_train, y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "loss_func(model(xb), yb), accuracy(model(xb), yb)\n", "intent": "Note that we no longer call ``log_softmax`` in the ``model`` function. Let's\nconfirm that our loss and accuracy are the same as before:\n"}
{"snippet": "fit()\nloss_func(model(xb), yb)\n", "intent": "We are still able to use our same ``fit`` method as before.\n"}
{"snippet": "target = torch.tensor([3], dtype=torch.long)\nloss_fn = nn.CrossEntropyLoss()  \nerr = loss_fn(out, target)\nerr.backward()\nprint(err)\n", "intent": "Define a dummy target label and compute error using a loss function.\n"}
{"snippet": "model = build_model()\n", "intent": "Model will use the streams I created earlier to train and test.\n"}
{"snippet": "batch_size = 256\nepochs = 2\nfilepath_3 =\"./models/textcnn_v2_embeddings/weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\ncheckpoint_3 = ModelCheckpoint(filepath_3, monitor='val_acc', save_best_only=True, mode='max', verbose=1)\nhistory_textcnn_2 = textcnn_model_v2.fit(x=padded_train_seq, y=train_labels, validation_split=0.1, \n                                batch_size=batch_size, epochs=epochs, callbacks=[checkpoint_3], verbose=1)\n", "intent": "<img src='./model_images/text_cnn_model_v3.png'/>\n"}
{"snippet": "validation_split = 0.2\nbatch_size = 512\nepochs = 10\nfile_name = \"./models/cnn_model/weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\ncheckpoint = ModelCheckpoint(file_name, monitor='val_acc', \n                             save_best_only=True, mode='max', verbose=1)\nr = model.fit(train_data, train_targets, batch_size=batch_size, \n                      callbacks=[checkpoint],\n                      epochs = epochs, validation_split=validation_split)\n", "intent": "<img src='./model_images/cnn_model.png'/>\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression().fit(X_train, y_train)\nprint(\"Training set accuracy: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(logreg.score(X_test, y_test)))\n", "intent": "Logistic regression is one of the most common classification algorithms.\n"}
{"snippet": "get_predictions(index=0, \n                partition='train',\n                input_to_softmax=final_model(input_dim=161,\n                        filters=200,\n                        kernel_size=11, \n                        conv_stride=2,\n                        conv_border_mode='valid',\n                        units=128,\n                        recur_layers=2), \n                model_path='results/'+'model_end.h5')\n", "intent": "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='validation',\n                input_to_softmax=final_model(input_dim=161, \n                        filters=200,\n                        kernel_size=11, \n                        conv_stride=2,\n                        conv_border_mode='valid',\n                        units=128,\n                        recur_layers=2), \n                model_path='results/'+'model_end.h5')\n", "intent": "Use the next code cell to visualize the model's prediction for the first example in the validation dataset.\n"}
{"snippet": "clf = AdaBoostClassifier(n_estimators=300)\n", "intent": "best score so far with 300 trees\n"}
{"snippet": "w = theano.shared(np.random.randn(len(features[0]), len(np.unique(species))),\n                name='w',borrow=True)\n", "intent": "We create shared variables that contain the matrix of weights and the bias terms and initialize it with normally distributed values:\n"}
{"snippet": "w = theano.shared(np.random.randn(len(features[0]),len(np.unique(species))), name='w', borrow=True)\n", "intent": "We create shared variables that contain the matrix of weights and the bias terms and initialize it with normally distributed values:\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf.fit(X_train, Y_train)\n", "intent": "Train a logistic regression using the 70% set\n---\nEntrenar una regresion logistica usando la particion del 70%\n"}
{"snippet": "feature_cols = ['temp', 'season', 'weather', 'humidity']\nX = bikes[feature_cols]\ny = bikes.total\nlinreg = LinearRegression()\nlinreg.fit(X, y)\nprint(linreg.intercept_)\nprint(linreg.coef_)\n", "intent": "Estimate a regression using more features ['temp', 'season', 'weather', 'humidity'].\nHow is the performance compared to using only the temperature?\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1e9)\nlogreg.fit(X_train, y_train)\nprint(feature_cols, logreg.coef_[0])\n", "intent": "Fit a logistic regression model and examine the coefficients\nConfirm that the coefficients make intuitive sense.\n"}
{"snippet": "np.identity(4)\n", "intent": "There's also an **identity** command that behaves as you'd expect:\n"}
{"snippet": "logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)\nprint(\"Training set accuracy: {:.3f}\".format(logreg001.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(logreg001.score(X_test, y_test)))\n", "intent": "The default value of C=1 provides with 78% accuracy on training and 77% accuracy on test set.\n"}
{"snippet": "np.identity(2) + np.array([[1,1],[1,2]])\n", "intent": "as well as when you add two matrices together. (However, the matrices have to be the same shape.)\n"}
{"snippet": "np.identity(2)*np.ones((2,2))\n", "intent": "Something that confuses Matlab users is that the times (*) operator give element-wise multiplication rather than matrix multiplication:\n"}
{"snippet": "np.dot(np.identity(2),np.ones((2,2)))\n", "intent": "To get matrix multiplication, you need the **dot** command:\n"}
{"snippet": "from sklearn.linear_model import SGDClassifier\nclf = SGDClassifier(loss=\"hinge\", alpha=0.01, n_iter=200, fit_intercept=True)\nclf.fit(X, Y)\n", "intent": "A classification algorithm may be used to draw a dividing boundary\nbetween the two clusters of points:\n"}
{"snippet": "clf1.fit(data[['area', 'area2']], data[' price'])\n", "intent": "When using sklearn there is no need to create the intercept\nAlso sklearn works with pandas\n"}
{"snippet": "from sklearn.neighbors import NearestNeighbors\nnn = NearestNeighbors(n_neighbors=2)\nnn.fit(x_vis)\nnns = nn.kneighbors(x_vis, return_distance=False)[:, 1]\n", "intent": "Find the nearest neighbour of every point\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntrees = []\ntrees.append(DecisionTreeClassifier(max_depth=1))\ntrees[t].fit(X_train, y_train, sample_weight=weights[t].values)\n", "intent": "Train the classifier\n"}
{"snippet": "W2=tf.Variable(tf.random_normal([EMB_DIM,vocab_size]),name='W2')\nb2=tf.Variable(tf.random_normal([vocab_size]),name='b2')\nprediction_layer = tf.nn.softmax(tf.add(tf.matmul(hidden_layer,W2),b2))\n", "intent": "The following layer is for predictions output/target layer\n"}
{"snippet": "with open('train.txt') as f:\n    X_train = [parseFeature(l) for l in f.readlines()]\nx_train, y_train = getTrainingData(X_train, 100)\nprint 'Training data size: %d' %len(y_train)\nclf = svm.SVC(kernel='linear', C=.1)\nclf.fit(x_train, y_train)\n", "intent": "- due to resource limitation, we only use data from a limited number of query for training\n- train SVM with linear kernel\n"}
{"snippet": "logreg100 = LogisticRegression(C=100).fit(X_train, y_train)\nprint(\"Training set accuracy: {:.3f}\".format(logreg100.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(logreg100.score(X_test, y_test)))\n", "intent": "Using C=0.01 results in lower accuracy on both the training and the test sets.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    return tf.contrib.layers.embed_sequence(input_data, vocab_size=vocab_size, embed_dim=embed_dim)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(initial_value=tf.random_uniform(shape=[vocab_size, embed_dim], minval=-1.0, maxval=1.0))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "lm.fit(X, bos.PRICE)\nprint(lm.coef_)\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "lm_no_intercept = LinearRegression(fit_intercept = False)\nlm_no_intercept.fit(X, bos.PRICE)\n", "intent": "**Your turn:** How would you change the model to not fit an intercept term? Would you recommend not having an intercept?\nprint(lm.coef_)\n"}
{"snippet": "history = model.fit(x_train, y_train_hot, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=1)\n", "intent": "Same as in earlier exercises ...\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "param_grid = {'penalty':['l1', 'l2'], \n              'C': [0.001, 0.01, 1, 10, 100, 1000]} \nlogisticModel = tuningPara_crossValid(LogisticRegression(), param_grid, X_train, X_test, y_train, y_test)\n", "intent": "<h4> Problem \n- Repeat **Problem \n"}
{"snippet": "MLPmodel=MLPClassifier(solver='sgd', activation='relu', random_state=10, hidden_layer_sizes=[10,5], alpha=0.5)\nMLPmodel.fit(X_train_transformed, y_train)\nmlp_accuracy = MLPmodel.score(X_test_transformed, y_test)\nprint('The accuracy of MLPClassifier is', mlp_accuracy)\n", "intent": "<h4> Problem \n- Repeat **Problem \n- Report accuracy, confusion matrix, precision, and recall\n"}
{"snippet": "MLPmodel=MLPClassifier(solver='lbfgs', activation='relu', random_state=10, hidden_layer_sizes=[10,5], alpha=0.5)\nMLPmodel.fit(X_train_transformed, y_train)\nmlp_accuracy = MLPmodel.score(X_test_transformed, y_test)\nprint('The accuracy of MLPClassifier is', mlp_accuracy)\n", "intent": "<h4> Problem \n- Repeat **Problem \n- Report accuracy, confusion matrix, precision, and recall\n- Comment on results\n"}
{"snippet": "mlp = MLPClassifier(max_iter=1000, random_state=0)\nmlp.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    mlp.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n", "intent": "The results are much better after scaling. As at matter of fact, we have obtained the highest test accuracy so far.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal([vocab_size, embed_dim], -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "knn_1 = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "knn_1.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, verbose=1)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "import theano\ntrain = theano.function([net_input, true_output], loss, updates=updates)\nget_output = theano.function([net_input], net_output)\n", "intent": "Finally, we create Theano-based functions that perform this training and also obtain the output of the network for testing\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1 ,1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nlogreg_parameters = {'penalty':['l1','l2'],\n                     'C':np.logspace(-5,1,50),\n                     'solver':['liblinear']}\ngrid_search = GridSearchCV(model1, param_grid=logreg_parameters, scoring=\"accuracy\")\ngrid_search.fit(X_train, y_train)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "from sklearn.neighbors import  KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nknn_parameters = {'n_neighbors' : list(range(1,20,3)), \n                  'weights' : ['uniform', 'distance']}\nknn = KNeighborsClassifier()\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "from sklearn import ensemble\nfrom sklearn.ensemble import GradientBoostingRegressor\nmodel = ensemble.GradientBoostingRegressor()\nmodel.fit(X_train, y_train)\n", "intent": "Much better! Let's try one more.\n"}
{"snippet": "from sklearn.cluster import KMeans\nn_clusters=2\nkmeans = KMeans(n_clusters,random_state=1)\nkmeans.fit(adult)\n", "intent": "Cluster the Data to our our target groups. **BONUS**: Perform multiple clustering tests on various indicator variables\n"}
{"snippet": "iris.feature_names\nn_clusters=3\nkmeans=cluster.KMeans(n_clusters=n_clusters)\n", "intent": "Run the clustering analysis using scikit-learn.\n*Hint*: Estimate the number of clusters, k, based on your visual examination of the distributions.\n"}
{"snippet": "kmeans.fit(X)\nkmeans.cluster_centers_\n", "intent": "Compute the labels and centroids\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=2)\nkm.fit(Z)\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "nb = BernoulliNB()\n", "intent": "The model should only be built (and cross-validated) on the training data.\nCross-validate the score and compare it to baseline.\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=2000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='sigmoid'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "lm.fit(x_train,y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "log_model=LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "humidity_classifier = DecisionTreeClassifier(max_leaf_nodes=10, random_state=0)\nhumidity_classifier.fit(X_train, y_train)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nFit on Train Set\n<br><br></p>\n"}
{"snippet": "model=KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "def sigmoid(z):\n    return (1.0/(1+np.exp(-z)))\n", "intent": "Define sigmoid function. <br>\n           1. Input: An array.\n           2. Output: Sigmoid of Input.\n"}
{"snippet": "def sigmoid(z):\n    return (1/(1 + np.exp(-z)))\n", "intent": "Define sigmoid function. <br>\n           1. Input: An array.\n           2. Output: Sigmoid of Input.\n"}
{"snippet": "def sigmoid(z):\n    return (1/(1+np.exp((-1)*z)))\n", "intent": "Define sigmoid function. <br>\n           1. Input: An array.\n           2. Output: Sigmoid of Input.\n"}
{"snippet": "dtree=DecisionTreeClassifier(criterion='entropy')\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "model=KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nnb=MultinomialNB()\n", "intent": "Time to train a model!\n** Import MultinomialNB and create an instance of the estimator and call is nb **\n"}
{"snippet": "meta_model = LinearRegression(n_jobs=16)\nstart = datetime.now()\nmeta_lr = meta_model.fit(X_train_level2, y_train_level2)\nstop = datetime.now()\nprint(\"Ellapsed time = \", stop - start)\n", "intent": "We will use linear regression to fit the meta-features, using the same parameters as in the model above.\n"}
{"snippet": "x = tf.placeholder(tf.string)\ny = tf.placeholder(tf.int32)\nz = tf.placeholder(tf.float32)\nwith tf.Session() as sess:\n    outX = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})\n    outY = sess.run(y, feed_dict={x: 'Test String', y: 123, z: 45.67})\n    outZ = sess.run(z, feed_dict={x: 'Test String', y: 123, z: 45.67})\n    print(outX, \" \", outY, \" \", outZ)\n", "intent": "x = 'Test String'  \ny = 123  \nz = 45.67  \n"}
{"snippet": "kmeans = KMeans(n_clusters=12)\nmodel = kmeans.fit(X)\nprint(\"model\\n\", model)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nUse k-Means Clustering\n<br><br></p>\n"}
{"snippet": "history = model.fit(x=X_train_array, y=y_train, batch_size=64, epochs=5,\n                    verbose=1, validation_data=(X_test_array, y_test))\n", "intent": "Let's go ahead and train this for a few epochs and see what we get.\n"}
{"snippet": "model = Char3Model(vocab_size, n_factors, n_hidden)\nhistory = model.fit(x=[x1, x2, x3], y=y_cat, batch_size=512, epochs=3, verbose=1)\n", "intent": "Train the model for a few iterations.\n"}
{"snippet": "model = CharLoopModel(vocab_size, cs, n_factors, n_hidden)\nhistory = model.fit(x=X_array, y=y_cat, batch_size=512, epochs=5, verbose=1)\n", "intent": "Train the model a bit and generate some predictions.\n"}
{"snippet": "model = CharRnn(vocab_size, cs, n_factors, n_hidden)\nmodel.summary()\n", "intent": "Let's look at a summary of the model.  Notice the array shapes have a third dimension to them until we get on the other side of the RNN.\n"}
{"snippet": "K.set_value(model.optimizer.lr, 0.0001)\nhistory = model.fit(x=X, y=y_cat, batch_size=512, epochs=3, verbose=1)\n", "intent": "We can train it a bit longer at a lower learning rate to reduce the loss further.\n"}
{"snippet": "model.fit(x=X, y=y_cat, batch_size=bs, epochs=8, verbose=1, callbacks=[reset_state], shuffle=False)\n", "intent": "Train the model for a while as before, with the addition of the callback to reset state between epochs.\n"}
{"snippet": "model = CharStatefulLSTM(vocab_size, cs, n_factors, n_hidden, bs)\nmodel.fit(x=X, y=y_cat, batch_size=bs, epochs=20, verbose=1, callbacks=[reset_state], shuffle=False)\n", "intent": "LSTMs need to train for a bit longer.  We'll do 20 epochs at each learning rate.\n"}
{"snippet": "svm = SVC(kernel = 'poly', degree = 3, C = 0.8)\nsvm.fit(X_train, y_train)\ntest_scores = svm.score(X_test, y_test)\nprint(\"The score of the nested CV is %.02f and when using test data %.02f\" %(best_scores, test_scores))\n", "intent": "At last an svm is trained with the optimal parameters. This is then trained with the original training data and tested with the test data.\n"}
{"snippet": "svm = SVC(kernel = 'poly', degree = 3, C = 0.8)\nsvm.fit(X_train, y_train)\ntest_scores = svm.score(X_test, y_test)\nbest_scores, test_scores\nprint(\"The score of the nested CV is %.02f and when using test data %.02f\" %(np.mean(best_scores), test_scores))\n", "intent": "At last an svm is trained with the optimal parameters. This is then trained with the original training data and tested with the test data.\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\nprint('Accuracy of K-NN classifier on training set: {:.2f}'.format(knn.score(X_train, y_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'.format(knn.score(X_test, y_test)))\n", "intent": "The above plot suggests that we should shoose n_neighbors=3. Here we are:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "knn5=KNeighborsClassifier(n_neighbors=5,weights='uniform')\nscores=accuracy_crossvalidator(Xs,y,knn5,cv_indices)\n", "intent": "<a id='cv-knn5'></a>\n"}
{"snippet": "import statsmodels.formula.api as sm\nmodel = sm.ols(formula='dep_delay ~ temp', data=flight_weather_df)\nresult = model.fit()\n", "intent": "In sum, temperature, wind speed, visibility, pressure and precipitation tend to affect the delays in flight departures.\n"}
{"snippet": "model = sm.ols(formula='dep_delay ~ wind_speed', data=flight_weather_df)\nresult = model.fit()\nprint(result.summary())\n", "intent": "For every 1 farenheit increase in temperature, the departure delay increases by 0.18 minutes.\n"}
{"snippet": "model = sm.ols(formula='dep_delay ~ visib', data=flight_weather_df)\nresult = model.fit()\nprint(result.summary())\n", "intent": "For every increase in wind speed by one unit, the departure delay increases by 0.06\n"}
{"snippet": "model = sm.ols(formula='dep_delay ~ pressure', data=flight_weather_df)\nresult = model.fit()\nprint(result.summary())\n", "intent": "for every 1 unit increase in visibility the departure delay decreases by 2.2 mins.\n"}
{"snippet": "model = sm.ols(formula='dep_delay ~ precip', data=flight_weather_df)\nresult = model.fit()\nprint(result.summary())\n", "intent": "for every one unit increase in pressure the departure delay decreses by 0.5\n"}
{"snippet": "clf_2 = LogisticRegression()\nclf_2.fit(X,y)\n", "intent": "Repeat the above process for logistic regression\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(init)\n    tfdata= {x:data[4:,1],\n             y:data[4:,2],\n             sigma:data[4:,3]}\n    for i in range(3000):\n        sess.run(train,tfdata)\n    print(\"m=%.2f, b=%.2f\" % (sess.run([m])[0],\n                              sess.run([b])[0]),\n          \"loss=%.2f\" % sess.run(loss,tfdata))\n", "intent": "and then we optimize:\n"}
{"snippet": "gb = GradientBoostingClassifier(random_state=0, max_depth=1)\ngb.fit(X_train, y_train)\nplot_feature_importances_cancer(gb1)\n", "intent": "Still, we can visualize the feature importances to get more insight into our model even though we are not really happy with the model.\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3, input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D())\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_shape = (vocab_size, embed_dim)\n    embedding = tf.Variable(tf.random_uniform(embedding_shape, -1, 1, dtype=tf.float32), name=\"embedding\")\n    embed = tf.nn.embedding_lookup(embedding, input_data, name=\"embedding_lookup\")\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "opt = tf.keras.optimizers.RMSprop(lr=learning_rate)\nmodel.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=['acc'])\n", "intent": "1. Take a look at [keras optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) and choose the models optimizer.\n"}
{"snippet": "model.fit(x_train, y_train, \n          batch_size=batch_size, \n          epochs=epochs,\n          validation_data=(x_val, y_val))\n", "intent": "1. Complete the **fit()** function bellow and let's train the model.\n"}
{"snippet": "def create_embedding(name, n_in, n_out):\n    inp = Input(name=name + '_in', dtype='int64', shape=[1])\n    return inp, Flatten()(Embedding(n_in, n_out, input_length=1, name=name)(inp))\n", "intent": "Embedding function that creates inputs and embeddings\n"}
{"snippet": "def fit_model(epochs=1, lr=1e-3):\n    model.optimizer.lr = lr\n    model.fit([x1, x2, x3], y, batch_size=64, epochs=epochs,\n              verbose=0, callbacks=[TQDMNotebookCallback()], shuffle=False)\n", "intent": "Helper function to fit the model\n"}
{"snippet": "def fit_model(epochs=1, lr=1e-3):\n    model.optimizer.lr = lr\n    model.fit(x, y, batch_size=64, epochs=epochs,\n              verbose=0, callbacks=[TQDMNotebookCallback()])\n", "intent": "Model fitting utility function\n"}
{"snippet": "clear_session()\nmodel = Sequential([Embedding(vocab_size, n_factors, input_length=cs),\n                    SimpleRNN(n_hidden, activation='relu',\n                              recurrent_initializer=identity()),\n                    Dense(vocab_size, activation='softmax')])\nmodel.compile('adam', loss='sparse_categorical_crossentropy')\nmodel.summary()\n", "intent": "Create Sequential Model\n"}
{"snippet": "def fit_model(epochs=1, lr=1e-3):\n    model.optimizer.lr = lr\n    return model.fit(x, y, batch_size=batch_size, epochs=epochs, verbose=0, callbacks=[TQDMNotebookCallback()], shuffle=False, validation_data=(x_val, y_val))\n", "intent": "Helper Function for training\n"}
{"snippet": "mlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)\nmlp.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    mlp.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test, y_test)))\n", "intent": "The results are much better after scaling, and already quite competitive.\n"}
{"snippet": "def fit_model(epochs=1, lr=1e-3):\n    if exists(DIR_OUTPUT + '/resnet.h5'):\n        model.load_weights(DIR_OUTPUT + '/resnet.h5')\n        print('Loaded weights from previous fit.\\nDelete/Rename for new fit.')\n        return None\n    model.optimizer.lr = lr\n    return model.fit(train_features, train_classes, batch_size=64, epochs=epochs, validation_data=(\n        val_features, val_classes), verbose=0, callbacks=[TQDMNotebookCallback()])\n", "intent": "Helper function to fit data\n"}
{"snippet": "model = get_model()\nfit_model(2, 1e-5, save=False)\n", "intent": "No improvement once more ;(\nLower more!\n"}
{"snippet": "model = get_model()\nfit_model(20, 1e-6, save=False)\n", "intent": "Frustrated, we can try one thing - lower the learning rate more and fit more epochs\n"}
{"snippet": "model = get_model()\nfit_model( 2, 1e-5)\n", "intent": "The predictions are not uniform either. \nIn this case, there's just one thing left to try -\nLower the learning rate!\n"}
{"snippet": "fit_model(4, 1e-4)\n", "intent": "Excellent. It seems that we are ready to increase the learning rate slightly now.\n"}
{"snippet": "model = get_model(reg=1)\nfit_model([2, 4], [1e-5, 1e-4])\n", "intent": "Clearly, this is not working. \nPerhaps the regularization is too small.\nLet's ramp it up\n"}
{"snippet": "model = get_model(reg=10)\nfit_model([2, 4], [1e-5, 1e-4])\n", "intent": "Still not enough.\nGo higher! \n"}
{"snippet": "fit_model(4)\n", "intent": "Shows promise. Let's continue training\n"}
{"snippet": "import statsmodels.api as sm\nlr = sm.Logit(target, features_array)\nlr.fit()\n", "intent": "Let's try our `Logit` model from `statsmodels`:\n"}
{"snippet": "from sagemaker.tensorflow.serving import Model\nenv = {'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'mobilenet_v2_140_224'}\nmodel = Model(model_data=model_data, role=sagemaker_role, framework_version=1.11, env=env)\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge')\n", "intent": "Now that the model archive is in S3, we can create a Model and deploy it to an \nEndpoint with a few lines of python code:\n"}
{"snippet": "number_of_clusters = 3\nkmeans = KMeans(n_clusters=number_of_clusters)\nkmeans.fit(coords)\n", "intent": "Here we'll review the idea of k-means clustering you discussed last week and see how it applies to our crime data. We'll start with three clusters.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfor C in np.arange(5., 5.51, 0.1):\n    print(C, cross_validation(LogisticRegression(random_state=501, n_jobs=3, C=C, dual=True), \n                              raw_data,\n                              ngram_range=(1, 6),\n                              n_features=2 ** 24))\n", "intent": "0.8603 -- full\n0.8251 -- train\n"}
{"snippet": "with pm.Model() as m2c_nopc:\n    betap = pm.Normal(\"betap\", 0, 1)\n    betac = pm.Normal(\"betac\", 0, 1)\n    alpha = pm.Normal(\"alpha\", 0, 100)\n    loglam = alpha + betap*df.logpop_c + betac*df.clevel\n    y = pm.Poisson(\"ntools\", mu=t.exp(loglam), observed=df.total_tools)\n    trace2c_nopc = pm.sample(5000, tune=1000)\n", "intent": "**(A)** Our complete model\n**(B)** A model with no interaction\n"}
{"snippet": "with pm.Model() as m2c_onlyp:\n    betap = pm.Normal(\"betap\", 0, 1)\n    alpha = pm.Normal(\"alpha\", 0, 100)\n    loglam = alpha + betap*df.logpop_c\n    y = pm.Poisson(\"ntools\", mu=t.exp(loglam), observed=df.total_tools)\n    trace2c_onlyp = pm.sample(5000, tune=1000)\n", "intent": "**(C)** A model with no contact term\n"}
{"snippet": "with pm.Model() as m2c_onlyc:\n    betac = pm.Normal(\"betac\", 0, 1)\n    alpha = pm.Normal(\"alpha\", 0, 100)\n    loglam = alpha +  betac*df.clevel\n    y = pm.Poisson(\"ntools\", mu=t.exp(loglam), observed=df.total_tools)\n    trace2c_onlyc = pm.sample(5000, tune=1000)\n", "intent": "**(D)** A model with only the contact term\n"}
{"snippet": "with pm.Model() as m2c_onlyic:\n    alpha = pm.Normal(\"alpha\", 0, 100)\n    loglam = alpha\n    y = pm.Poisson(\"ntools\", mu=t.exp(loglam), observed=df.total_tools)\n    trace2c_onlyic = pm.sample(5000, tune=1000)\n", "intent": "**(E)** A model with only the intercept.\n"}
{"snippet": "with pm.Model() as m1:\n    betap = pm.Normal(\"betap\", 0, 1)\n    betac = pm.Normal(\"betac\", 0, 1)\n    betapc = pm.Normal(\"betapc\", 0, 1)\n    alpha = pm.Normal(\"alpha\", 0, 100)\n    loglam = alpha + betap*df.logpop + betac*df.clevel + betapc*df.clevel*df.logpop\n    y = pm.Poisson(\"ntools\", mu=t.exp(loglam), observed=df.total_tools)\n    trace1 = pm.sample(10000, tune=2000)\n", "intent": "We can redo the coparison for non-centered models\n"}
{"snippet": "import theano.tensor as tt\ncov=np.array([[0,0.8],[0.8,0]], dtype=np.float64)\nwith pm.Model() as mdensity:\n    density = pm.MvNormal('density', mu=[0,0], cov=tt.fill_diagonal(cov,1), shape=2)\n", "intent": "Ok, so we just set up a simple sampler with no observed data\n"}
{"snippet": "mdvar = pm.ADVI(model=mdensity)\nmdvar.fit(n=40000)\n", "intent": "But when we sample using ADVI, the mean-field approximation means that we lose our correlation:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\nmodel.fit(X, Y)\n", "intent": "So, how hard can it be to build a Multi-Layer percepton with keras?\nIt is baiscly the same, just add more layers!\n"}
{"snippet": "with pm.Model() as model:\n    mus = [MvNormal('mu_%d' % i, mu=np.zeros(2), tau=np.eye(2), shape=(2,))\n           for i in range(2)]\n    pi = Dirichlet('pi', a=0.1 * np.ones(2), shape=(2,))\n    xs = DensityDist('x', logp_gmix(mus, pi, np.eye(2)), observed=data)\n    start = find_MAP()\n    step = Metropolis()\n    trace = sample(1000, step, start=start, chains=2, njobs=1)\n", "intent": "MCMC takes of ther order of a minute in time, which is 50 times more than on the small dataset.\n"}
{"snippet": "def f():\n    approx = pm.fit(n=1500, obj_optimizer=pm.adagrad(learning_rate=1e-1), model=model)\n    means = approx.bij.rmap(approx.mean.eval())\n    sds = approx.bij.rmap(approx.std.eval())\n    return means, sds, -approx.hist\nmeans, sds, elbos = f()\n", "intent": "Run ADVI. It's much faster than MCMC, though the problem here is simple and it's not a fair comparison. \n"}
{"snippet": "with pm.Model() as m3c:\n    betap = pm.Normal(\"betap\", 0, 1)\n    alpha = pm.Normal(\"alpha\", 0, 10)\n    sigmasoc = pm.HalfCauchy(\"sigmasoc\", 1)\n    alphasoc = pm.Normal(\"alphasoc\", 0, sigmasoc, shape=df.shape[0])\n    loglam = alpha + alphasoc + betap*df.logpop_c \n    y = pm.Poisson(\"ntools\", mu=t.exp(loglam), observed=df.total_tools)\nwith m3c:\n    trace3 = pm.sample(6000, tune=1000, nuts_kwargs=dict(target_accept=.95))\n", "intent": "Notice that $\\beta_P$ has a value around 0.24\nWe also implement the varying intercepts per society model from before\n"}
{"snippet": "x = first_variable\ny = (x ** x) * (x - 2) \nz = F.tanh(y) \nz.backward()\nprint(\"y.data: \", y.data)\nprint(\"y.grad: \", y.grad)\nprint(\"z.data: \", z.data)\nprint(\"z.grad: \", z.grad)\nprint(\"x.grad:\", x.grad)\n", "intent": "Now let's create some new variables. We can do so implicitly just by creating other variables with functional relationships to our variable.\n"}
{"snippet": "import theano.tensor as t\nwith pm.Model() as model1:\n    alpha=pm.Normal(\"alpha\", 0,100)\n    beta=pm.Normal(\"beta\", 0,1)\n    logmu = t.log(df.days)+alpha+beta*df.monastery\n    y = pm.Poisson(\"obsv\", mu=t.exp(logmu), observed=df.y)\n    lambda0 = pm.Deterministic(\"lambda0\", t.exp(alpha))\n    lambda1 = pm.Deterministic(\"lambda1\", t.exp(alpha + beta))\n", "intent": "Now we set up the model in pymc, including two deterministics to capture the rates\n"}
{"snippet": "class ObserverModel(pm.Model):\n    def wrapped_f(**observed):\n        try:\n            with ObserverModel(observed) as model:\n                f(**observed)\n        except TypeError:\n            with ObserverModel(observed) as model:\n                f()\n        return model\n    return wrapped_f\n", "intent": "This should be way enough!, So lets go again:\nFirst we create a decorator using code from https://github.com/ColCarroll/sampled\n"}
{"snippet": "with pm.Model() as hm1dumb:\n    mu = pm.Normal('mu', mu=178, sd=0.1)\n    sigma = pm.Uniform('sigma', lower=0, upper=50)\n    height = pm.Normal('height', mu=mu, sd=sigma, observed=df2.height)\n", "intent": "Above we had used a very diffuse value on the prior. But suppose we tamp it down instead, as in the model below.\n"}
{"snippet": "import theano.tensor as T\nwith pm.Model() as model1:\n    alpha = pm.Normal('intercept', 0, 100)\n    beta = pm.DensityDist('beta', lambda value: -1.5 * T.log(1 + value**2), testval=10)\n    eps = pm.DensityDist('eps', lambda value: -T.log(T.abs_(value)), testval=10)\n    like = pm.Normal('y_est', mu=alpha + beta * (xdata_shared - xdata_shared.mean()), sd=eps, observed=ydata)\n", "intent": "We will need to write custom densities for this. Theano provides us a way:\n"}
{"snippet": "import pymc3 as pm\nfrom pymc3.math import switch\nwith pm.Model() as coaldis1:\n    early_mean = pm.Exponential('early_mean', 1)\n    late_mean = pm.Exponential('late_mean', 1)\n    switchpoint = pm.DiscreteUniform('switchpoint', lower=0, upper=n_years)\n    rate = switch(switchpoint >= np.arange(n_years), early_mean, late_mean)\n    disasters = pm.Poisson('disasters', mu=rate, observed=disasters_data)\n", "intent": "The rate parameter varies before and after the switchpoint, which itseld has a discrete-uniform prior on it. Rate parameters get exponential priors.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\nmodel.fit(X, Y)\n", "intent": "Take couple of minutes and Try and optimize the number of layers and the number of parameters in the layers to get the best results. \n"}
{"snippet": "LR_model = models['LR'].fit(X_train,y_train)\nprint(\"Train score: \", LR_model.score(X_train,y_train))\nprint(\"Test score: \", LR_model.score(X_test, y_test))\n", "intent": "After chossing the best scoring CV model we fit it on the entire training set and then evaluate it on the test set.\n"}
{"snippet": "dTree = DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "paramGrid = {'C':[0.1,1,10,1000],'gamma':[1,0.1,0.01,0.001,0.0001]}\ngrid = GridSearchCV(SVC(), paramGrid,verbose=3)\n", "intent": "**Create a dictionary called param_grid and fill out some parameters for C and gamma.**\n"}
{"snippet": "grid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, input_dim=x_train.shape[1], activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.summary()\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x=x_train, y=y_train, batch_size=1000, epochs=100, validation_split=0.1, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), minval=-1, maxval=1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "keras.layers.recurrent.LSTM(output_dim, init='glorot_uniform', inner_init='orthogonal', \n                            forget_bias_init='one', activation='tanh', \n                            inner_activation='hard_sigmoid', \n                            W_regularizer=None, U_regularizer=None, b_regularizer=None, \n                            dropout_W=0.0, dropout_U=0.0)\n", "intent": "<img src =\"imgs/gru.png\" width=\"60%\">\n"}
