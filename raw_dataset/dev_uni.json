{"snippet": "import seaborn as sns\nscore_df = pd.DataFrame(results.grid_scores_)\nscore_df['parameters'] = score_df['parameters'].map(lambda score: score['alpha'])\nscore_df['mean_score'] = score_df['cv_validation_scores'].map(lambda scores: np.mean(scores))\nscore_df.plot(kind=\"scatter\", x=\"parameters\", y=\"mean_validation_score\", title=\"Alphas / Mean Scores\")\n", "intent": "Figure out how to plot the impact of your score given the range of parameters used.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_df=0.9, max_features=5000, sublinear_tf=True, use_idf=True)\n", "intent": "Vamos utilizar o Tf-Idf para criar o embedding dos motivos de compra e o K-Means para encontrar clusters.\n"}
{"snippet": "train_target = pd.read_csv(os.path.join(PATH_TO_DATA,'train_log1p_recommends.csv'), \n                           index_col='id')\ny_train = train_target['log_recommends'].values\n", "intent": "**Read train target and split data for validation.**\n"}
{"snippet": "data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\n        'year': [2000, 2001, 2002, 2001, 2002],\n        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\ndf = pd.DataFrame(data)\ndf\n", "intent": "Pandas `DataFrame` can be easily saved into a SQL database using the function `to_sql()`. Let's create a data frame first.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX, y = iris.data, iris.target\n", "intent": "Grid Searches\n=================\n"}
{"snippet": "df.to_csv('img2targets.csv', index=False)\n", "intent": "Save the data frame. \n"}
{"snippet": "sales = pd.read_csv('data/sales_train_v2.csv')\nshops = pd.read_csv('data/shops.csv')\nitems = pd.read_csv('data/items.csv')\nitem_cats = pd.read_csv('data/item_categories.csv')\ntest = pd.read_csv('data/test.csv')\nsubmission = pd.read_csv('data/test.csv')\n", "intent": "Load the data from the hard drive... Data files must be in a data/ subfolder relatively to the folder where notepad is installed\n"}
{"snippet": "performance_template_df = pd.DataFrame(columns= [\n        ['Params']*3 + [b for a in ['Precision', 'Recall', 'F1_score', 'Support'] for b in [a, a]],\n        ['MaxDepth', 'Nfeature', 'Features'] + ['no', 'yes']*4\n    ])\nperformance_template_df\n", "intent": "Now, let's run the performance evaluation across a number of configurations. We'll collect the results for each configuration into a dataframe.\n"}
{"snippet": "df = pd.read_csv('/home/data_scientist/data/delta.csv', index_col='Aircraft')\ndf['Clusters'] = clusters\ndf['Aircraft'] = df.index\ndf_grouped = df.groupby('Clusters').mean()\nprint(df_grouped.Accommodation)\n", "intent": "You don't have to write any code in this section, but here's one interpretaion of what we have done.\nLet's take a closer look at each cluster.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nrandom_state = np.random.RandomState(3939)\ntest_set_fraction = 0.3\nreviews_train, reviews_test, t_train, t_test = train_test_split(\n    reviews, labels, test_size=test_set_fraction, random_state=random_state)\nprint '\n", "intent": "We set aside 30% of the data set to test the classifier.\n"}
{"snippet": "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train, \n                                                stratify = labels_train, random_state = 123)\n", "intent": "Train/Validation Split\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(data[feature], data[target], test_size=0.25, random_state=27)\n", "intent": "Divide the data set into train sets and test sets.\n"}
{"snippet": "a = pd.DataFrame(X_train1.columns)\nb = pd.DataFrame(model.feature_importances_)\na.join(b,lsuffix='_feature',rsuffix='_score')\n", "intent": "Importanza delle variabili per il modello:\n"}
{"snippet": "kick = pd.read_csv(\"kick.csv\", low_memory=False)\nkick.head(3)\n", "intent": "The data corresponds to kickstarter projects, data about the projects and whether it was successfully funded or not. \n"}
{"snippet": "y = boston.target\ndf = pd.DataFrame(boston.data,columns=boston.feature_names)\nprint boston.DESCR\n", "intent": "Load the Boston housing data.  Fix any problems, if applicable.\n"}
{"snippet": "def factor_betas(pca, factor_beta_indices, factor_beta_columns):\n    assert len(factor_beta_indices.shape) == 1\n    assert len(factor_beta_columns.shape) == 1\n    return pd.DataFrame(pca.components_.T, factor_beta_indices, factor_beta_columns)\nproject_tests.test_factor_betas(factor_betas)\n", "intent": "Implement `factor_betas` to get the factor betas from the PCA model.\n"}
{"snippet": "summary = pd.DataFrame(list(zip(X.columns, \\\n    np.transpose(clf_tree.feature_importances_), \\\n    np.transpose(clf_rf.feature_importances_), \\\n    np.transpose(clf_ext.feature_importances_), \\\n    np.transpose(clf_gb.feature_importances_), \\\n    np.transpose(clf_ada.feature_importances_), \\\n    np.transpose(clf_xgb.feature_importances_), \\\n    )), columns=['Feature','Tree','RF','Extra','GB','Ada','Xtreme'])\nsummary['Median'] = summary.median(1)\nsummary.sort_values('Median', ascending=False)\n", "intent": "For additional insight we compare the *feature\\_importance* output of all the classifiers for which it exists:\n"}
{"snippet": "df1 = pd.DataFrame([pd.Series(np.arange(10, 15)) ,\n                pd.Series(np.arange(20, 25)),             \n                pd.Series(np.arange(20, 25))])\nprint(df1, '\\n')\nprint(df1.shape) \n", "intent": "* **shape**\n - row, column\n"}
{"snippet": "df = pd.read_csv(\n    '/home/data_scientist/data/2001.csv',\n    encoding='latin-1',\n    usecols=(13, 16)\n    )\n", "intent": "We use the `AirTime` column at the Willard airport.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3, random_state=0)\n", "intent": "Splitting the data into 70% training and 30% test subsets.\n"}
{"snippet": "conmat = np.array(confusion_matrix(y_test, y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['is_cancer', 'is_healthy'],\n                         columns=['predicted_cancer','predicted_healthy'])\nconfusion\n", "intent": "**Create the confusion matrix for your classfier's performance on the test set.**\n"}
{"snippet": "test_df = pd.DataFrame(X_test, columns=data.columns[:-1])\ntest_df['subscribed'] = y_test\nfor col in test_df.columns:\n    if isinstance(test_df[col][0], int):\n        test_df[col] = test_df[col].apply(pd.to_numeric, errors='coerce')\n    elif isinstance(test_df[col][0], float):\n        test_df[col] = test_df[col].apply(pd.to_numeric, errors='coerce')\ntest_df['never_contacted'] = test_df['prev_days'] == 999\ntest_df = test_df[list(test_df.columns[:-2]) + list([test_df.columns[-1]]) + list([test_df.columns[-2]])]\nc_df = pd.concat([train_df, test_df], axis=0)\n", "intent": "First, I build a test_df, convert data types, and concatenate test_df and train_df.\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "pd.DataFrame([[1,2], [3,4]])\n", "intent": "DataFrames are 2 dimensional arrays with row and column labels. You can construct them from any 2 dimensional structure you might expect.\n"}
{"snippet": "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\ny = df.iloc[0:100, 4].values\ny = np.where(y == 'Iris-setosa', -1, 1)\nX = df.iloc[0:100, [0,2]].values\n", "intent": "Dataset of three different Iris flower species and their respective features. [Source link](https://archive.ics.uci.edu/ml/datasets/Iris)\n"}
{"snippet": "df = pd.read_csv('poly_regression.csv')\ndata = df.values\nX = data[:,0]\nY = data[:,1]\nX = X.reshape(X.shape[0],1)\nY = Y.reshape(Y.shape[0],1)\n", "intent": "Import data from poly_regression.csv\n"}
{"snippet": "df=pd.read_csv('2013_movies.csv')\n", "intent": "<h1>Challenge 11</h1>\n"}
{"snippet": "df = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n", "intent": "**Create a dataframe**\n"}
{"snippet": "training_data = pd.read_csv(\"ames_train.csv\")\ntest_data = pd.read_csv(\"ames_test.csv\")\n", "intent": "As a good sanity check, we should at least verify that the data shape matches the description.\n"}
{"snippet": "price_type = df_all.price_type\nprice_type_vectorizer = CountVectorizer(binary=1, dtype='uint8', min_df=10)\nprice_type_matrix = price_type_vectorizer.fit_transform(price_type)\n", "intent": "First let's try using the price type only\n"}
{"snippet": "trips = bq.Query(taxiquery, YEAR=2014).to_dataframe()\nweather = bq.Query(wxquery, YEAR=2014).to_dataframe()\ndata2014 = pd.merge(weather, trips, on='daynumber')\ndata2014[:5]\n", "intent": "<h3> Adding 2014 data </h3>\nLet's add in 2014 data to the Pandas dataframe.  Note how useful it was for us to modularize our queries around the YEAR.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nX_train_tfidf.shape\n", "intent": "Map words <----> integer <----> frequency of integer in a doc, so called Term Frequencies.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nX_train, X_test, y_train, y_test = train_test_split(X[predictors], y, train_size=0.7, random_state=8)\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nlr.score(X_test, y_test)\n", "intent": "- Score and plot.  \n- How do your metrics change?\n"}
{"snippet": "df = pd.read_csv('../datasets/titanic.csv')\n", "intent": "Quelle: [https://www.kaggle.com/c/titanic/data)\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    contains_org = any([word.ent_type_ == 'ORG' for word in parsed])\n    contains_person = any([word.ent_type_ == 'PERSON' for word in parsed])\n    return contains_org and contains_person\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data.references_org_person][['title']].head()\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "from sklearn.datasets import fetch_olivetti_faces\ndata = fetch_olivetti_faces().images\n", "intent": "400 fotos of human faces. Each face is a 2d array [64x64] of pixel brightness.\n"}
{"snippet": "pca = sklearn.decomposition.KernelPCA(n_components=2, kernel='rbf', gamma=.001)\nXt = pca.fit_transform(X)\n", "intent": "Compare that the kernel PCA:\n"}
{"snippet": "import pandas as pd\ncust_df = pd.read_csv(\"Cust_Segmentation.csv\")\ncust_df.head()\n", "intent": "Before you can work with the data, you must use the URL to get the Cust_Segmentation.csv.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n", "intent": "Split our data into training and testing\n"}
{"snippet": "x = StandardScaler().fit_transform(x)\nprint x.shape\nprint x[0:5]\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "style_image = scipy.misc.imread(\"images/monet_800600.jpg\")\nimshow(style_image);\n", "intent": "For our running example, we will use the following style image: \n"}
{"snippet": "def get_word_table(table, key, sim_key='similarity', show_sim = True):\n    if show_sim == True:\n        return pd.DataFrame(table, columns=[key, sim_key])\n    else:\n        return pd.DataFrame(table, columns=[key, sim_key])[key]\n", "intent": " We can use analogies to see word associations. For instance, King is to Woman as Queen is to _ , we get:\n"}
{"snippet": "df_scaled = pd.DataFrame(scaled_features,columns=dataframe.columns[:-1])\ndf_scaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "max_features = 2000\n(X_train, y_train), (X_test, y_test) = reuters.load_data(\n    nb_words=max_features)\nmaxlen = 10\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nnb_epoch = 20\n", "intent": "- Let's try an RNN for the same Reuters classification task:\n"}
{"snippet": "N = 200\nnp.random.seed(0)\nX, y = sklearn.datasets.make_moons(N, noise = 0.20)\n", "intent": "Create data for a simple binary classification problem.\n"}
{"snippet": "from sklearn.datasets import make_classification\nX, y = make_classification(100, 2, 2, 0, weights=[.5, .5], random_state=0) \nclf = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\nclf.fit(X, y)\n", "intent": "Let's consider a two class example to keep things simple\n"}
{"snippet": "of_df = pd.read_csv(\"../assets/datasets/old-faithful.csv\")\nof_df.head()\n", "intent": "* Very simple: Only two features!\n    - Eruption Time\n    - Wait Time\n"}
{"snippet": "from sklearn.tree import export_graphviz\nexport_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"], \n                feature_names=cancer.feature_names, impurity=False, filled=True)\nimport graphviz\nwith open(\"tree.dot\") as f:\n    dot_graph = f.read()\ndisplay(graphviz.Source(dot_graph))\n", "intent": "Analyzing Decision Trees manually\n- Visualize and find the path that most data takes\n"}
{"snippet": "splicing_feature_data = pd.DataFrame(index=event_names)\nsplicing_feature_data['gene_name'] = gene_names\nsplicing_feature_data.head()\n", "intent": "Now let's create `splicing_feature_data` to map these event names to the gene names, and to the `gene_category` from before.\n"}
{"snippet": "def references_org_person(title):\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title']].head()\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "categorical_cols = [col for col in train_x.columns if train_x[col].dtype == object]\nfor col in categorical_cols:\n    train_x[col] = train_x[col].fillna(\"NaN\")\nnumerical_cols = [col for col in train_x.columns if any(train_x[col].isnull()) and train_x[col].dtype != object]\nfor col in numerical_cols:\n    train_x[col + \"_nan\"] = train_x[col].map(lambda x: 1 if np.isnan(x) else 0)\n    train_x[col]          = train_x[col].fillna(train_x[col].median())\nsum(train_x.isnull().sum())\n", "intent": "<a id=\"fillnan\"></a>\n"}
{"snippet": "import pandas as pd\npath = '/Users/jim_byers/Documents/GA/GA_Data_Science_course/SEA-DAT1/data/'\nurl = path + 'titanic.csv'\ntitanic = pd.read_csv(url, index_col='PassengerId')\ntitanic.shape\n", "intent": "scikit-learn models expect that all values are **numeric** and **hold meaning**. Thus, missing values are not allowed by scikit-learn.\n"}
{"snippet": "ss_ff = preprocessing.StandardScaler()\nss_ff.fit(Xff)\nX_ff_scaled = ss_ff.transform(Xff)\nX_ff_scaled = pd.DataFrame(X_ff_scalled, columns=Xff.columns)\ny_ff_scaled = log(ff.area)\n", "intent": "* Try scaling the input and using the log of the area and see if you get a better score.\n* Examine the coefficients\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nknn_pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\nknn_params = {'knn__n_neighbors': range(1, 10)}\nknn_grid = GridSearchCV(knn_pipe, knn_params,\ncv=5, n_jobs=-1,\nverbose=True)\nknn_grid.fit(X_train, y_train)\nknn_grid.best_params_, knn_grid.best_score_\n", "intent": "Now let's tune the number of neighbors $k$ for kNN:\n"}
{"snippet": "class CountVectorizer:\n    def __init__(self, lowercase=True):\n        self.lowercase = lowercase\n    def __repr__(self):\n        return \"CountVectorizer(lowercase={})\".format(self.lowercase)\ncv = CountVectorizer()\ncv\n", "intent": "By default, the class instance prints the name of the class and memory related information. We will update this to say something more meaningful\n"}
{"snippet": "vectorizer = CountVectorizer(max_features = 1000,\n                             ngram_range=(1, 2),\n                             stop_words='english',\n                             binary=True)\nvectorizer.fit(data['title'])\nX_new_text_features = vectorizer.transform(data['title'])\n", "intent": "Use `CountVectorizer` to generate vectorized text features.\n"}
{"snippet": "from sklearn.datasets import load_digits\nimport numpy as np\ndigits = load_digits()\ndigits.keys()\n", "intent": "Get some data to play with\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(df,y)\nlen(x_train), len(x_test), len(y_train), len(y_test)\n", "intent": "Split your data set into testing and training:\n"}
{"snippet": "study.plot_pca(feature_subset='gene_category: LPS Response', sample_subset='not (pooled)', plot_violins=False, show_point_labels=True)\n", "intent": "Equivalently, I could have written out the plotting command by hand, instead of using `study.interactive_pca`:\n"}
{"snippet": "label = pd.read_csv(\"preprocessed_meta/classes.csv\", index_col = 0)\nenc = LabelBinarizer()\ny_enc = enc.fit_transform(label.title.values.reshape(-1, 1))\n", "intent": "First let's create our target matrix from the dataset we made previously\n"}
{"snippet": "le = LabelEncoder()\nX['Sex'] = le.fit_transform(finalData.Sex) \n", "intent": "The values in the 'Sex' column are 'Male/Female'. So convert them into 1/0 using Label Encoding.\n"}
{"snippet": "from sklearn.datasets import make_moons\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n", "intent": "Adding `random_state=42` to make this notebook's output constant:\n"}
{"snippet": "data = {'x1': df.value.values, 'y': df.total_foreign_reserve_sgd.values}\ndf_dataset = pd.DataFrame(data=data)\ndf_dataset.head()\n", "intent": "Here's our resulting dataset after feature selection:\n"}
{"snippet": "pca = PCA(n_components=3)\nX = df_2.loc[:, 'A2':'A13_s']\ny = df_2.y\nX.columns\n", "intent": "2. PCA to plot (for classification)\n  - Plot a scatter plot with 2 feature dimensions (or 3 feature dimensions)  \n  - Use colours for y_enc\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n", "intent": "Keras includes some built-in datasets that are useful for learning and practice.\nhttps://keras.io/datasets/\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, clusters_kmeans, random_state=42)\n", "intent": "Train a decision tree using any set of clusters from earlier workshop.\n"}
{"snippet": "import google.datalab.bigquery as bq\ndf = bq.Query(query).execute().result().to_dataframe()\n", "intent": "The problem is to estimate demand for bicycles at different rental stations in New York City.  The necessary data is in BigQuery:\n"}
{"snippet": "authors_list = bq.Query(sql).execute().result().to_dataframe()['first_author'].tolist()\nwrite_list_to_disk(authors_list, \"authors.txt\")\nprint(\"Some sample authors {}\".format(authors_list[:10]))\nprint(\"The total number of authors is {}\".format(len(authors_list)))\n", "intent": "When creating the author list, we'll only use the first author information for each article.  \n"}
{"snippet": "iris_df = pd.DataFrame(IRIS.data, columns=IRIS.feature_names)\n", "intent": "We will load the Iris data into a dataframe for ease of manipulation.\n"}
{"snippet": "diabetes = datasets.load_diabetes()\ndiabetes_df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ndiabetes_df.head()\n", "intent": "<b>Step 2:</b> Load the dataset\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer(decode_error = 'ignore', binary=True)\nvect.fit(X_train)\nvect.get_feature_names()\n", "intent": "Use `sklearn.feature_extraction.text.CountVectorizer` on the training set to create a vectorizer called `vect`.\n"}
{"snippet": "ratings = pd.read_csv('ml-latest-small/ratings.csv')\nratings.head()\n", "intent": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n"}
{"snippet": "y_train = train_data['human-generated']\ncontextVectorizerTF = TfidfVectorizer().fit(train_data['context'])\nx_context = contextVectorizerTF.transform(train_data['context'])\nresponseVectorizerTF = TfidfVectorizer().fit(train_data['response'])\nx_response = responseVectorizerTF.transform(train_data['response'])\nx_train = sparse.hstack([x_context,x_response])\n", "intent": "Generate feature matricies for context and response. Fill free to create\nadditional feature extraction object if necessary.\n"}
{"snippet": "gp_features = pd.DataFrame(index=gps.index)\ngp_features = gp_features.ix[gps.index]\ngp_features['peaktime'] = xx[gps.ix[:, 4:204].as_matrix().argmax(1)]\n", "intent": "Like before, we find the peak, and order the pseudotime-varying genes.\n"}
{"snippet": "X = StandardScaler().fit_transform(circles_X)\n", "intent": "**9.2 Scale the data and fit DBSCAN on it.**\n"}
{"snippet": "url = '../data/bikeshare.csv'\nbikes = pd.read_csv(url, index_col='datetime', parse_dates=True)\n", "intent": "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n"}
{"snippet": "college = pd.read_csv('./College_Data')\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0,1))\ntraining_set_scaled = scaler.fit_transform(training_set)\n", "intent": "When there are sigmoid activation functions in the neural network, it is recommendable to apply normalization for feature scaling.\n"}
{"snippet": "import numpy as np\ndef pca(data):\n    centered_data = data - data.mean(axis=0)\n    covariance_matrix = np.cov(centered_data)\n    eigenvals, eigenvecs = np.linalg.eig(covariance_matrix)\n    pc = eigenvecs.T @ centered_data  \n    return pc\n", "intent": "Principal components are the eigenvalues of the autocorrelation matrix of a dataset. The eigenvalues give the ordering of the components.\n"}
{"snippet": "train = pd.read_csv('./data/train.csv')\ntest = pd.read_csv('./data/test.csv')\n", "intent": "* Feature Importance ( Random Forest Classifier and Gradient Boosting Classifier )\n* Feature Engineering for Tree Based Models\n"}
{"snippet": "movies = pd.read_csv(path+'movies.csv')\nmovies.head()\n", "intent": "Just for display purposes, let's read in the movie names too.\n"}
{"snippet": "print(pd.DataFrame({'effect': params.round(0),\n                    'error': err.round(0)}))\n", "intent": "With these errors estimated, let's again look at the results:\n"}
{"snippet": "from skimage.io import imread\nimage = imread('laptop.jpeg')\ntype(image)\n", "intent": "Let's use [scikit-image](http://scikit-image.rg) to load the content of a JPEG file into a numpy array:\n"}
{"snippet": "from keras.preprocessing.sequence import pad_sequences\nMAX_SEQUENCE_LENGTH = 1000\nx_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', x_train.shape)\nprint('Shape of data test tensor:', x_test.shape)\n", "intent": "Let's truncate and pad all the sequences to 1000 symbols to build the training set:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)\n", "intent": "Standardizing the data.\n"}
{"snippet": "reader = tf.TextLineReader()\nkey_op, value_op = reader.read(filename_queue)\n", "intent": "Define an op to dequeue a line from file:\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc_model = RandomForestClassifier();\nrefclasscol = strat_train_set_X.columns\nrfc_model.fit(strat_train_set_X, strat_train_set_y);\nscore = np.round(rfc_model.feature_importances_, 3)\nimportances = pd.DataFrame({'feature':refclasscol, 'importance':score})\nimportances = importances.sort_values('importance', ascending=False).set_index('feature')\nimportances.plot.bar();\n", "intent": "Let's build a basic random forest classifier and extract the features based on their importances\n"}
{"snippet": "vect = CountVectorizer(ngram_range=(1, 2))\ntokenize_test(vect)\n", "intent": "**Approach 4:** Include 1-grams and 2-grams\n"}
{"snippet": "import pandas as pd\nimport seaborn as sns\nsns.set()\ncols = ['Suggested retail price (USD)', 'Price to dealer (USD)', \n          'Engine size (liters)', 'Number of engine cylinders', \n          'Engine horsepower', 'City gas mileage' , \n          'Highway gas mileage', 'Weight (pounds)', \n          'Wheelbase (inches)', 'Length (inches)', 'Width (inches)']\ndf = pd.DataFrame(cars, columns=cols)\nsns.pairplot(df)\n", "intent": "Try to get some initial idea of the dataset:\n"}
{"snippet": "data = pd.read_csv('~/Desktop/DSI-SF-2-akodate/datasets/401_k_abadie/401ksubs.csv') \ndata.head(2)\n", "intent": "1. Read the 401ksubs.csv data into Pandas.\n2. Explore the data by sorting, plotting, group_by, and any other ideas/techniques you have been using.\n"}
{"snippet": "cvt = CountVectorizer()\nX_all = cvt.fit_transform(insults_df[\"Comment\"])\nfreqs = [(word, X_all.getcol(idx).sum()) for word, idx in cvt.vocabulary_.items()]\nprint sorted (freqs, key = lambda x: -x[1])\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "with open('data/anna.txt', 'r') as f:\n    text = f.read()\n", "intent": "Then, we'll load the Anna Karenina text file and convert it into integers for our network to use. \n"}
{"snippet": "wbc = pd.read_csv('../../data/wdbc.csv.bz2')\nwbc = wbc.drop(['id'], axis=1)\nwbc = wbc.replace({'M': 1, 'B': 0})\nprint (wbc.keys())\nprint (wbc.shape)\nprint(wbc.dtypes)\nwbc.head(10)\n", "intent": "__1.\tLoad the data. You may drop id or just ignore it in the rest of your analysis.__\n"}
{"snippet": "X1, Xt1, y1, yt1 = train_test_split(df_1.drop('label', axis=1)['text'], df_1.label, test_size=0.25, random_state=4222)\n", "intent": "* create test dataset\n"}
{"snippet": "data2 = pd.DataFrame({'Age':  [17,64,18,20,38,49,55,25,29,31,33], \n                      'Salary': [25,80,22,36,37,59,74,70,33,102,88], \n             'Loan Default': [1,0,1,0,1,0,0,1,1,0,1]})\ndata2\n", "intent": "Let's consider a more complex example by adding the \"Salary\" variable (in the thousands of dollars per year).\n"}
{"snippet": "X_new, y_new = make_data(N=50)\nX_new_tr, X_new_ts, y_new_tr, y_new_ts = train_test_split(X_new, y_new)\npoly2new = PolynomialRegression(2)\npoly2new.fit(X_new_tr, y_new_tr)\nplot_regr(X_new_tr, y_new_tr, poly2new)\n", "intent": "Let's start from smaller data size, $N=50$:\n"}
{"snippet": "import sklearn.feature_selection as fs\nx_new = fs.VarianceThreshold(threshold = 1).fit_transform(iris.data)\nx_new.shape\n", "intent": "Let's try to remove features with variances smaller than 1:\n"}
{"snippet": "import pandas as pd\ncard = pd.read_csv('./data/card.csv')\nhier.fit(card)\nlabel = hier.labels_\n", "intent": "<p><a name=\"hcase2\"></a></p>\nFit the hierarchical algorithm.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, z_train, z_test = train_test_split(X, z, random_state=1, stratify=z)\n", "intent": "Split our data into training and testing\n"}
{"snippet": "print (\"Length of test rows:\", len(test))\ninspect_test = pd.DataFrame({'Dtype': test.dtypes, 'Unique values': test.nunique() ,\n             'Number of Missing values': test.isnull().sum() ,\n              'Percentage Missing': (test.isnull().sum() / len(test)) * 100\n             }).sort_values(by='Number of Missing values',ascending = False)\ninspect_test\n", "intent": "BMI and smoking status have missing values\n"}
{"snippet": "ts = time.time()\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\nmatrix = fill_na(matrix)\ntime.time() - ts\n", "intent": "Producing lags brings a lot of nulls.\n"}
{"snippet": "import pandas as pd\nnames = ['Bob','Jessica','Mary','John','Mel']\nbirths = [968, 155, 77, 578, 973]\nBabyDataSet = list(zip(names,births))\npd.DataFrame(data = BabyDataSet, columns=['Names', 'Births'])\n", "intent": "Check if Pandas is working. After running this cell, you should see a table appear below.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(documents) \n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(features,label)\n", "intent": "**As we have often done before, we split the data into training and test:**\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ny = df.iloc[:,0]\nX = df.iloc[:,1:]\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .3, random_state = 4444)\n", "intent": "* Split data into train/test\n"}
{"snippet": "movies = pd.read_csv('challenges_data/2013_movies.csv')\n", "intent": "Fit and evaluate a decision tree classifier for your movie dataset. Examine the rules your tree uses.\n"}
{"snippet": "scaler_model = MinMaxScaler()\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "df = pd.DataFrame(data.data, columns=data.feature_names)\n", "intent": "Define the data/predictors as the pre-set feature names: \n"}
{"snippet": "def read_data():\n    data = pd.read_csv('multiclassdata.csv', delimiter=',', skiprows=0)\n    return data.values\n    raise NotImplementedError()\n", "intent": "Read in the data from the file `multiclassdata.csv` using pandas\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n                                                random_state=2)\nprint(Xtrain.shape, Xtest.shape)\n", "intent": "Let's try a classification task on the digits. The first thing we'll want to do is split the digits into a training and testing sample:\n"}
{"snippet": "centers = [[1,1], [6,6]]\nX, _ = make_blobs(n_samples=400, n_features=2, centers=centers, cluster_std=1.)\n", "intent": "---\nWe'll start with a simple example to build some intuition.\nFirst our toy dataset:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA()\npca.fit(pre90s_z)\n", "intent": "In order to do so, we will first perform PCA implementation that comes with Python's sklearn machine learning library.\n"}
{"snippet": "coef_vol = masker.inverse_transform(svc.coef_)\nnl.plotting.plot_stat_map(coef_vol, bg_img=anat, dim=-.5)\n", "intent": "Since we have a value for each voxel, we can simply map this back to the volume using our `masker`, and visualize the weights.\n"}
{"snippet": "from itertools import combinations\nauto_engineered_disances = pd.DataFrame()\nfor (feature_one, feature_two) in combinations(features_sorted[-5:], r=2):\n    for name, operation in zip((\"plus\", \"minus\"), (np.add, np.subtract, np.true_divide)):\n        new_name = \"%s_%s_%s\" % (feature_names[feature_one], name, feature_names[feature_two])\n        auto_engineered_disances[new_name] = operation(covtype.data[:, feature_one],\n                                                       covtype.data[:, feature_two])\n", "intent": "Let's add and subtract all the distances.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits(n_class=6)\nX = digits.data\ny = digits.target\nn_samples, n_features = X.shape\n", "intent": " * A thousand something 8x8 handwritten digits\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)\n", "intent": "On divise en base d'apprentissage et de test :\n"}
{"snippet": "submission = pd.concat([Test_id, pd.DataFrame(pred)], axis=1)\nsubmission.columns = ['id','trip_duration']\nsubmission['trip_duration'] = submission.apply(lambda x : 1 if (x['trip_duration'] <= 0) else x['trip_duration'], axis = 1)\nsubmission.to_csv(\"submission.csv\", index=False)\n", "intent": "Time for Submission\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntrain, test = train_test_split(dataset, train_size = 0.7)\n", "intent": "** Train / Test set split: Dividing sub-set of test and training **\n"}
{"snippet": "df_test = pd.DataFrame({\n    'AirTime': X_test['AirTime'].values,\n    'Distance': y_test['Distance'].values\n    })\ndf_pred = pd.DataFrame({\n    'AirTime': X_pred['AirTime'].values,\n    'Distance': y_pred\n    })\nax2 = plot_statsmodels_reg(df_test, df_pred)\n", "intent": "Here's an example plot. The blue points are points in the test data, and the red line is the regression model predicted by statsmodels.\n"}
{"snippet": "Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train[train['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train[train['Pclass']==3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class', '2nd class', '3rd class']\ndf.plot(kind='bar', stacked=True, figsize=(10,5))\n", "intent": "4.5 Embarked\n--------------\n**4.5.1 filling missing values**\n"}
{"snippet": "data = pd.read_csv('../../DSI-CHI-1/lessons/week-04/3.4-model-fit-and-sklearn-logistic/assets/datasets/train.tsv', sep='\\t', na_values='?')\ndata['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\ndata['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n", "intent": "- These are websites that always relevant like recipies or reviews (as opposed to current events)\n- Look at some examples\n"}
{"snippet": "classes = pd.read_csv('../../DSI-CHI-1/lessons/week-07/2.3-pca-lab-1/assets/datasets/votes.csv', index_col=0)\ny = classes['Class']\ny.head()\n", "intent": "Next, let's define the x and y variables: \n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nbow_transformer = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "le = LabelEncoder()\nan1 = (an[['Cold or Warm Blooded','Covering','Aquatic','Aerial','Lays Eggs']]).apply(lambda x: d[x.name].fit_transform(x))\nan1\n", "intent": "This time we will use the label encoder to convert our non-numerical data, and concatenate them all together:\n"}
{"snippet": "X=cv.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "dists = pd.DataFrame(dists, columns=df_wide.index)\ndists.index = dists.columns\ndists.ix[0:10, 0:10]\n", "intent": "Convert dists to a Pandas DataFrame, use the index as column index as well (distances are a square matrix).\n"}
{"snippet": "from sklearn.feature_selection import SelectFromModel\nclf=RandomForestRegressor()\nsfm = SelectFromModel(clf,threshold=0.002)\nX_train_new=sfm.fit_transform(X_train, y_train)\nfeature_test_new=sfm.transform(feature_test)\n", "intent": "2,using SelectFromModel to select features\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n                 header=None, \n                 names=['Sepal length', 'Sepal width', 'Petal length', 'Petal width', 'target'])\ndf.head()\n", "intent": "Again, let us use [Iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset.\n"}
{"snippet": "filename = '/home/data_scientist/data/2001.csv'\nusecols = (5, 15, 16)\ncolumns = ['CRSDepTime', 'DepDelay', 'Origin']\nall_data = pd.read_csv(filename, header=0, na_values=['NA'], usecols=usecols, names=columns)\nlocal = all_data.loc[all_data['Origin'] == 'ORD'].dropna()\n", "intent": "For simplicity, we limit our analysis to flights that departed from O'Hare. We will try to predict `DepDelay` from `CRSDepTime`.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\n_, itest = train_test_split(range(critics.shape[0]), test_size=0.3)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itest] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "x_standard = StandardScaler().fit_transform(x)\n", "intent": "First, let's standardize the data\n"}
{"snippet": "scaler = StandardScaler()\nscalled_set = scaler.fit_transform(airports)\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "train = pd.DataFrame(columns=['filename','label'])\ntest = pd.DataFrame(columns=['filename','label'])\nfor label in list(files_df['label'].unique()):\n    threshold = files_df.loc[files_df['label'] == label].shape[0] * 0.7\n    threshold = int(np.floor(threshold))\n    train = train.append(files_df.loc[files_df['label'] == label].iloc[:threshold,:],ignore_index=True)\n    test = test.append(files_df.loc[files_df['label'] == label].iloc[threshold:,:],ignore_index=True)\nprint(train.shape[0],test.shape[0])\n", "intent": "For each class, splitting the documents to training and test based on a 70-30 rule.\n"}
{"snippet": "Top_100_tokens = pd.DataFrame(speech_tokens_top100.toarray(), columns=count_vectorizer_100.get_feature_names())\nTop_100_tokens.head()\n", "intent": "Now let's push all of that into a dataframe with nicely named columns.\n"}
{"snippet": "f = z.open('ml-1m/users.dat')\ntmp = f.read()\ntmp = tmp.decode()\nusers = np.array([np.array(t.split('::'))[: -1] for t in tmp.splitlines()]) \nusers[:, 1] = (users[:, 1] == 'M').astype(np.int)\nusers = users.astype(np.int)\nf.close()\n", "intent": "Users' information\n* UserID, Gender (1 is M), Age, Occupation\n"}
{"snippet": "cancer = load_breast_cancer()\ndigits = load_digits()\ndata = cancer\n", "intent": "We will use the breast cancer dataset in which the target variable has 1 if the person has cancer and 0 otherwise. Let's load the data.\n"}
{"snippet": "from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_scaled = min_max_scaler.fit_transform(X)\nx_train2, x_test2, y_train2, y_test2 = train_test_split(X_scaled,Y, test_size = 0.15, random_state=100)\n", "intent": "refer:http://scikit-learn.org/stable/modules/preprocessing.html\nmore at: https://en.wikipedia.org/wiki/Feature_scaling\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(lowercase=True,stop_words='english')\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "df = pd.read_csv(\n    '/home/data_scientist/data/2001.csv',\n    encoding='latin-1',\n    usecols=(1, 2, 21)\n    )\n", "intent": "We use the following columns:\n- Column 1: Month, 1-12\n- Column 2: DayofMonth, 1-31\n- Column 21: Cancelled, was the flight cancelled?\n"}
{"snippet": "df1 = pd.read_csv('Data/Auto.csv', na_values='?').dropna()\ndf1.info()\n", "intent": "Dataset available on http://www-bcf.usc.edu/~gareth/ISL/data.html\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ntest = pd.read_csv('test.csv', encoding = \"ISO-8859-1\", header=None)\ntrain = pd.read_csv('train.csv', encoding = \"ISO-8859-1\", header=None)\ntrain.columns = ['id', 'sentiment', 'review']\ntest.columns = ['id', 'sentiment', 'review']\ntrain.head()\n", "intent": "Load the train and test files into a dataframe. Name the columns to help you in the rest of the problem.\n"}
{"snippet": "cement_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/slump_test.data\",index_col=0)\ncement_data.columns = cement_data.columns.str.lower().str.replace(\" \",\"_\").str.replace(\".\",\"\") \n", "intent": "Lets get the data directly off the web this time:\n"}
{"snippet": "df = pd.read_csv(\"data/smsspamcollection/SMSSpamCollection\", sep=\"\\t\", header=None, names=[\"target\", \"text\"])\n", "intent": "* UCI Dataset - https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n"}
{"snippet": "train_df = pd.read_csv(TRAIN_CSV)\ntest_df = pd.read_csv(TEST_CSV)\nstops = set(stopwords.words('english'))\ndef text_to_word_list(text):\n", "intent": "Create embedding matrix\n"}
{"snippet": "import seaborn as sb\nsb.pairplot(pd.DataFrame(iris.data, columns = iris.feature_names))\n", "intent": "**Using `pairplot` from `seaborn` is a quick way to see which features separate out our data**\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nimport pylab as pl\npl.gray();\n", "intent": "Import the dataset and use `pylab` to explore.\n"}
{"snippet": "path = 'https://ibm.box.com/shared/static/q6iiqb1pd7wo8r3q28jvgsrprzezjqk3.csv'\ndf = pd.read_csv(path)\ndf.head()\n", "intent": "load data and store in dataframe df:\n"}
{"snippet": "subm = np.column_stack((np.asarray(test_data.index.values), np.asarray(test_data_pred, dtype=np.float32)))\nnp.savetxt('kaggle_submissions/submission_xgb_new2.csv',subm, delimiter=',', comments='',  newline='\\n', fmt='%s', \n           header = 'id,trip_duration')\nsubmission = pd.read_csv('kaggle_submissions/submission_xgb_new2.csv', index_col= 'id')\nprint(submission.shape)\nsubmission.head()\n", "intent": "Now we create the submission file.\n"}
{"snippet": "df = pd.read_csv(\n    '/home/data_scientist/data/2001.csv',\n    encoding='latin-1',\n    usecols=(1, 2, 3, 5, 7, 13, 15, 16, 18)\n    )\n", "intent": "You can find the descriptions of the columns [here](http://stat-computing.org/dataexpo/2009/the-data.html).\n"}
{"snippet": "authorship['random'] = [random.random() for i in range(841)]\nx, y = authorship[features], authorship.Author_num.values\nx_train, x_test, y_train, y_test = train_test_split(authorship[features],\n                                                    authorship.Author_num.values,\n                                                    test_size=0.4,\n                                                    random_state=123)\n", "intent": "- Create a random variable column \n- Create test/train sets using test_train_split\n"}
{"snippet": "df = pd.read_csv('KNN_Project_Data')\n", "intent": "** Leia o arquivo csv 'KNN_Project_Data' em um DataFrame **\n"}
{"snippet": "pca_pivot = df_pca.pivot_table(index = ['cluster_id_3'], values = list(range(1,33)), aggfunc = np.sum).transpose()\n", "intent": "Based on the PCA plots, k = 3 seems to fit the data best visually, since there is much less overlap.\n"}
{"snippet": "xout=pd.DataFrame()\nxout['tag']=y\nxout['PCA1'] = X_2D[:, 0]\nxout['PCA2'] = X_2D[:, 1]\nsns.lmplot(\"PCA1\", \"PCA2\", hue='tag', data=xout, fit_reg=False, markers='.');\n", "intent": "**now reduced 64 to 2 dimensions  \n&rarr; visualize it**\n"}
{"snippet": "X = cleaned[['Pclass', 'Age', 'female']]\nX = X.values\ny = cleaned[['Survived']]\ny = y.values.ravel()\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\nm = LogisticRegression()\nm.fit(Xtrain, ytrain)\n", "intent": "Re-run the prediction above using the additional feature. How does the accuracy change?\n"}
{"snippet": "rich_features_final = rich_features_no_male.fillna(rich_features_no_male.dropna().median())\nrich_features_final.head(5)\n", "intent": "Let us not forget to imput the median age for passengers without age information:\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\ntfidf_vectorizer = TfidfVectorizer()\ncount_vectorizer = CountVectorizer()\ncorpus = data['mda']\ncorpus_tfidf = tfidf_vectorizer.fit_transform(corpus)\nprint(corpus_tfidf[0,0:100])\n", "intent": "Implementing TF-IDF in code is fairly simple.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(wine_df, y, test_size= 0.25)\n", "intent": "Let's create a test that simulates fresh data that model might be predicting on when it is put into production.\n"}
{"snippet": "count_vectorizer = CountVectorizer()\ncounts = count_vectorizer.fit_transform(df['text'].values)\ntfidf_vectorizer = TfidfVectorizer()\ntfidfs = tfidf_vectorizer.fit_transform(df['text'].values)\n", "intent": "[Count Vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n"}
{"snippet": "def train_test_split(pos_emails, neg_emails, pos_labels, neg_labels, split_value):\n    return X_train, X_test, y_train, y_test\n", "intent": "- Create four new arrays from our email data and labels.\n"}
{"snippet": "rets = prices.pct_change()[1:].fillna(0)\n", "intent": "Calculate the returns of these price data.\n"}
{"snippet": "votes = pd.read_csv('../assets/datasets/')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('data_lcurve.csv')\nimport numpy as np\nimport util\nX = np.array(data[['x1', 'x2']])\ny = np.array(data['y'])\nnp.random.seed(55)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\n", "intent": "**Model Selection - Learning Curves**\n"}
{"snippet": "from dopy.doplot.selection import add_log_to_dataframe, add_max_to_dataframe, add_min_to_dataframe\nadd_min_to_dataframe(real_dataframe_2015, 'B0_FitDaughtersConst_KS0_min_PT', ['B0_FitDaughtersConst_KS0_P1_PT', 'B0_FitDaughtersConst_KS0_P0_PT'])\nadd_min_to_dataframe(real_dataframe_2016, 'B0_FitDaughtersConst_KS0_min_PT', ['B0_FitDaughtersConst_KS0_P1_PT', 'B0_FitDaughtersConst_KS0_P0_PT'])\nadd_min_to_dataframe(real_dataframe_2015, 'test_IP', ['B0_FitDaughtersConst_KS0_P0_IPCHI2', 'B0_FitDaughtersConst_KS0_P1_IPCHI2'])\nadd_min_to_dataframe(real_dataframe_2016, 'test_IP', ['B0_FitDaughtersConst_KS0_P0_IPCHI2', 'B0_FitDaughtersConst_KS0_P1_IPCHI2'])\nadd_min_to_dataframe(real_dataframe_2015, 'B0_FitDaughtersConst_J_psi_1S_min_PT', ['B0_FitDaughtersConst_J_psi_1S_P0_PT', 'B0_FitDaughtersConst_J_psi_1S_P1_PT'])\nadd_min_to_dataframe(real_dataframe_2016, 'B0_FitDaughtersConst_J_psi_1S_min_PT', ['B0_FitDaughtersConst_J_psi_1S_P0_PT', 'B0_FitDaughtersConst_J_psi_1S_P1_PT'])\nreal_dataframe_2015['B0_FitPVConst_KS0_tau_dimless'] = real_dataframe_2015['B0_FitPVConst_KS0_tau']/real_dataframe_2015['B0_FitPVConst_KS0_tauErr']\nreal_dataframe_2016['B0_FitPVConst_KS0_tau_dimless'] = real_dataframe_2016['B0_FitPVConst_KS0_tau']/real_dataframe_2016['B0_FitPVConst_KS0_tauErr']\n", "intent": "Check fractions of the DD/LL type events\n-----------------------------------------------------------\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "**Step 5:** Create an instance of `StandardScaler` called `scaler`.\n"}
{"snippet": "ngram_vectorizer = CountVectorizer(ngram_range=(1,3))\nX_ngram = ngram_vectorizer.fit_transform(critics.quote)\n", "intent": "The random forest classifier underperforms naive bayes using both td-idf and non td-idf input data.\n"}
{"snippet": "imputer = Imputer(missing_values = 'NaN',strategy = 'mean',axis = 0)\nimputer = imputer.fit(X[:,2:])\nX[:,2:] = imputer.transform(X[:,2:])\n", "intent": "Now replace NaN or missing value with mean value of column\n"}
{"snippet": "college_data = pd.read_csv('College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_train = StandardScaler().fit_transform(X_train)\nX_train_expanded = StandardScaler().fit_transform(X_train_expanded)\n", "intent": "The only thing to do is to normalize the variables.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data, targets, test_size=0.2, random_state=check_random_state(0)\n    )\n", "intent": "We train on 80% of the data, and test the performance on the remaining 20%.\n"}
{"snippet": "model = MinMaxScaler()\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer(max_features=1000, ngram_range=(1,3))\n", "intent": "Build Tf-Idf features based on sites. You can use `ngram_range`=(1, 3) and `max_features`=100000 or more\n"}
{"snippet": "test_ids = pd.read_csv(\"processed_dataset/test.csv\", index_col=0)[\"match_id_hash\"]\n", "intent": "AUC: 0.8216585820565936\nACC: 0.740679012345679\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "Great now that we've imported the data we can load it. \n"}
{"snippet": "label_df.pivot_table(index='Id', aggfunc=len).sort_values('Image', ascending=False).head(10)\n", "intent": "There are 4251 unique classes represented in the training set.\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nmm_scaler = MinMaxScaler()\n", "intent": "We use the same `MinMaxScaler` we used earlier to scale the data.\n"}
{"snippet": "scores = spca.fit_transform(boroughs)\n", "intent": "Scores (projection of 'boroughs' on the PCs):\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(varts['counts'], varts['workingday'], test_size=0.33, random_state=42)\n", "intent": "In the next cell, we re-create our X and y matrices. This time, they contain time series of varying lengths.\n"}
{"snippet": "act_vec = CountVectorizer(ngram_range=(2,3), max_features = 22, strip_accents = 'ascii')\nact_vec.fit(movies['Actors'])\nactor_bigrams = pd.DataFrame(act_vec.transform(movies['Actors']).todense(),\n                      columns = act_vec.get_feature_names())\n", "intent": "These are the top 20 2-words plot themes to appear in the NYT top 1000 movies. Wars, youth, love, marriage, and cities seem to be common themes.\n"}
{"snippet": "d = pd.DataFrame({\n        'prediction': [0.1, 0.5, 0.95, 0.99, 0.8, 0.4, 0.03, 0.44, 0.2],\n        'y': [1, 0, 1, 1, 1, 1, 0, 0, 0]})\nd\n", "intent": "Compute AUC score by hand with the formula explained in class for the following dataset.\n"}
{"snippet": "train = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\n", "intent": "We will read the train and test datasets using pandas\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\npipeline = make_pipeline(PCA(), LogisticRegression())\nparams = {'logisticregression__C': [.1, 1, 10, 100],\n          \"pca__n_components\": [5, 10, 20, 40]}\ngrid = GridSearchCV(pipeline, param_grid=params, cv=5)\ngrid.fit(X_train, y_train)\nprint(grid.best_params_)\ngrid.score(X_test, y_test)\n", "intent": "Another benefit of using pipelines is that we can now also search over parameters of the feature extraction with ``GridSearchCV``:\n"}
{"snippet": "data['Cabin'].fillna('U0', inplace=True)\ndata['CabinSection'] = LabelEncoder().fit_transform(data['Cabin'].map(lambda x: x[0]))\ndata['CabinDistance'] = data['Cabin'].map(lambda x: x[1:])\ndata['CabinDistance'] = data['CabinDistance'].map(lambda x: x.split(' ')[0])\ndata['CabinDistance'].where(data['CabinDistance'] != '', '0', inplace=True)\ndata['CabinDistance'] = data['CabinDistance'].map(lambda x: int(x))\ndata.head()\n", "intent": "In this attemp we include all the cleaning inside one function and included comment to explain what we do in each step\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(os.path.join(basepath, csv_filename), encoding='utf-8')\n", "intent": "Read back the data-frame from file, local or remote.\n"}
{"snippet": "test_df = pd.read_csv(\"data/example1_test.csv\")\ny_test, test_design_mat = build_model1_design_mat(test_df)\nfitted_sk_ols.score(test_design_mat, y_test)\n", "intent": "**Answers**:\n1. See code below.\n"}
{"snippet": "all_data = pd.merge(all_data,\n                    id_df.loc[:, ['item_id', 'shop_id', 'ID']].drop_duplicates(),\n                    how='left',\n                    on=['item_id', 'shop_id'])\nall_data.loc[:, 'ID'].fillna(-1, inplace=True)\nall_data.loc[:, 'ID'] = all_data.loc[:, 'ID'].astype('int32')\n", "intent": "Item ID is a combination of `item_id` and `shop_id`. As before, if an ID doesn't exist for the combination, it will be assigned `-1`\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nquadratic = PolynomialFeatures(degree=2)\ncubic = PolynomialFeatures(degree=3)\nquartic = PolynomialFeatures(degree=4)\nXtrain2 = quadratic.fit_transform(Xtrain)\nXtest2 = quadratic.fit_transform(Xtest)\nXtrain3 = cubic.fit_transform(Xtrain)\nXtest3 = cubic.fit_transform(Xtest)\nXtrain4 = quartic.fit_transform(Xtrain)\nXtest4 = quartic.fit_transform(Xtest)\n", "intent": "6 - Fit a polynomial regression model to the data and print the error.  Comment on your results.\n"}
{"snippet": "pca = PCA(n_components=20)\nXtrain_pca = pca.fit_transform(Xtrain)\nXtest_pca = pca.transform(Xtest)\n", "intent": "OK, in the solution he tries to find a good number of components, and it's 20:\n"}
{"snippet": "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)\n", "intent": "Agora vamos fazer nosso split para avaliar nossos modelos:\n"}
{"snippet": "from ggplot import *\ndf = pd.DataFrame({'click_mean' : train[\"click\"].groupby(train[\"hour\"]).mean()}).reset_index()\nggplot(df, aes(x = 'hour', y = 'click_mean'))+ geom_line() + geom_point()\n", "intent": "plot and value_counts\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(yelp_class['text'], \n                                                    yelp_class['stars'], \n                                                    test_size=0.3, \n                                                    random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "env = pd.read_csv('ces3results_environment.csv')\ndemog = pd.read_csv('ces3results_demographics.csv')\nprint('Enviro cols are ', env.columns)\nprint('Demographics cols are ',demog.columns)\n", "intent": "Let's import the environmental and demographic datasets from CES:\n"}
{"snippet": "posts_frame = pd.DataFrame(converted_posts)\nposts_frame.head()\n", "intent": "Creating dataframe from parsed data\n"}
{"snippet": "loans = pd.read_csv(\"./loan_data.csv\")\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "pd.DataFrame(probs_dnn).to_hdf('data/probs_dnn.hf5', 'probs', mode='w')\n", "intent": "Save the DNN classification probabilities to compare with the visual scanning exercise results:\n"}
{"snippet": "reviews = pd.read_csv('./imdb_data/reviews.txt', header=None)\nlabels = pd.read_csv('./imdb_data/labels_ohe.csv', header=None)\n", "intent": "Read IMDB movie reviews dataset\n"}
{"snippet": "df = pd.DataFrame(data=np.c_[data, target], columns=iris['feature_names'] + ['target'])\ndf.head()\n", "intent": "Plot the data using scatterplots - take a look at all the combinations of variables to get a feel for how the data is distributed. \n"}
{"snippet": "wine = pd.read_csv('../CSV/wine_v.csv')\n", "intent": "And read in our data:\n"}
{"snippet": "from sys import path\npath.append('../2 - Text data preprocessing')\nimport load_spam\nspam_data = load_spam.spam_data_loader()\nspam_data.load_data()\nXtrain, ytrain, Xtest, ytest = spam_data.split(2000)\n", "intent": "If you have done the previous notebooks, you are used to these two examples now.\n"}
{"snippet": "f_scaled = pd.DataFrame(preprocessing.StandardScaler().fit_transform(f), columns=f.columns)\n", "intent": "We have explored feature scaling in [Problem 5.1](../Week5/Assignments/w5p1.ipynb), so I'll do this for you.\n"}
{"snippet": "tfidfvec = TfidfVectorizer(stop_words = 'english', min_df = 1, binary=True)\ncountvec = CountVectorizer(stop_words = 'english', min_df = 1, binary=True)\ntraining_dtm_tf = countvec.fit_transform(df_train.text)\ntest_dtm_tf = countvec.transform(df_test.text)\ntraining_labels = df_train.stars\ntest_labels = df_test.stars\ntest_labels.value_counts()\n", "intent": "Next we need to create a dtm for each review, and an array containing the classification label for each review (for us, this is called 'label')\n"}
{"snippet": "media_satisfaccion = df['satisfaction_level'].mean()\ndf['satisfaction_level'] = df['satisfaction_level'].fillna(media_satisfaccion)\n", "intent": "Vamos a imputar el valor nulo de satisfaction_level con la media de la columna.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nrand_state = np.random.randint(0, 100)\nX_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.1, random_state=rand_state)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)\n", "intent": "Split the data in training and test data. This is useful to check if the model is over/underfitting:\n"}
{"snippet": "sentance = [\"abc def pqr\", \"abc pqr cdf jkl cdf\"]\nexample = CountVectorizer()\nexample.fit(sentance)\nprint(example.vocabulary_) \nprint('='*50)\nprint(example.get_feature_names())\nprint(example.transform(sentance).toarray())\n", "intent": "<h2> <font color='red'>WHAT ARE THESE FUNCTIONS: FIT, TRANSFORM, FIT_TRANSFORM</font></h2>\n"}
{"snippet": "cols_to_use = ['LotArea', 'BsmtFinSF1','GrLivArea', 'OpenPorchSF', 'YearBuilt', 'SalePrice']\ndata = pd.read_csv('houseprice.csv', usecols=cols_to_use)\ndata.head()\n", "intent": "As expected, Random Forests did not see a benefit from transforming the variables to a more Gaussian like distribution.\n"}
{"snippet": "for df in [X_train, X_test, submission]:\n    df['Embarked'].fillna(X_train['Embarked'].mode()[0], inplace=True)\n    df['Cabin_categorical'].fillna('Missing', inplace=True)\n", "intent": "- Embarked NA imputed by most frequent category, because NA is low\n- Cabin_categorical imputed by 'Missing', because NA is high\n"}
{"snippet": "data_text =pd.read_csv(\"training/training_text\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\nprint('Number of data points : ', data_text.shape[0])\nprint('Number of features : ', data_text.shape[1])\nprint('Features : ', data_text.columns.values)\ndata_text.head()\n", "intent": "<h3>3.1.2. Reading Text Data</h3>\n"}
{"snippet": "train = pd.read_csv('train.csv')\n", "intent": "Load `train.csv` from Kaggle into a pandas DataFrame.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33)\n", "intent": "We notice that the scores are more or less the aame \n"}
{"snippet": "def fit_pca(df, n_components):\n    return pca, reduced\n", "intent": "Write a function named fit_pca() that takes a pandas.DataFrame and uses sklearn.decomposition.PCA to fit a PCA model on all values of df.\n"}
{"snippet": "import seaborn as sns\ncsv_file = \"https://vincentarelbundock.github.io/Rdatasets/csv/cluster/chorSub.csv\"\ndf = pd.read_csv(csv_file).iloc[:, 1:]\n", "intent": "<a id='chem'></a>\n---\nLet's load in a dataset on chemical composition and plot the silhouette scores for different numbers of clusters.\n"}
{"snippet": "donald_df = pd.DataFrame(donald_tweets)\nbernie_df = pd.DataFrame(bernie_tweets)\n", "intent": "> *Hint: this is as easy as passing it to the DataFrame constructor!*\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\nknn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(X_train, y_train)\n", "intent": "**Split the data into 66% training and 33% testing. Fit a KNN classifier with `n_neighbors=25` on the training data.**\n"}
{"snippet": "cv = CountVectorizer(preprocessor=cleaner)\ncv.fit(space_messages)\ncustom_preprocess = pd.DataFrame(cv.transform(space_messages).todense(),\n                              columns=cv.get_feature_names())\nprint(custom_preprocess.sum().sort_values(ascending=False).head(10))\n", "intent": "We can pass this function into `CountVectorizer` as a way to preprocess the text as a part of the fitting.\n"}
{"snippet": "strategy = '' \nage_imputer = Imputer(strategy=strategy)\nage_imputer.fit(df['age'].values.reshape(-1, 1))\nages = age_imputer.transform(\n    df['age'].values.reshape(-1, 1))\nprint(ages[0:5], ages.mean())\n", "intent": "Our results are as follows:\n- Mean: 16.65\n- Median: 17\n- Mode: 16.0\nWhat is most appropriate? Check in on Slack\n"}
{"snippet": "crime_df = pd.read_csv('https://raw.githubusercontent.com/albertw1/data/master/Crime.csv').drop(['Date', 'Year'], axis=1)\ncrime_df.head()\n", "intent": "We will first load in our dataset below and look at the first few rows. Then, we use the describe function to get a sense of the data.\n"}
{"snippet": "spam_train, spam_test = train_test_split(spam, test_size=0.5, random_state=1000)\n", "intent": "Let us split the dataset into a 50-50 split by using the following:\n"}
{"snippet": "df_3 = DataFrame(data_1, columns=['year', 'state', 'pop', 'unempl'])\ndf_3\n", "intent": "Like Series, columns that are not present in the data are NaN:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "t_raw, t_genres = select_genres(27)\nt_X_train, t_X_test, t_y_train, t_y_test = train_test_split(t_raw, \n                                                            t_genres, \n                                                            random_state=check_random_state(0), \n                                                            test_size=0.3)\n", "intent": "Run the cell below to split selected data (We'll use `n`=27) into training and testing sets with a test size of 0.3.\n"}
{"snippet": "random_states=[5,10,20]\nfor random_state in random_states:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n    clf = LogisticRegression(penalty='l1', C=alphas[opt_index])\n    clf.fit(X_train, y_train)\n    generate_ROCplot(fpr,tpr,'LR',roc_auc)\n    print \"The ROC curve for random state is:\", random_state\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(lfw_people.data, lfw_people.target, random_state=0)\nprint(X_train.shape, X_test.shape)\n", "intent": "We'll do a typical train-test split on the images before performing unsupervised learning:\n"}
{"snippet": "x_train, x_test,y_train,y_test=train_test_split(\nx,y,test_size=0.2,random_state=45)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "X_train,X_test,y_train,y_test=train_test_split(X, Y_binary, test_size=0.3, random_state=123)\n", "intent": "Above result confirms the four classes in the dataset. \n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nfor col in data.columns:\n    data[col] = mms.fit_transform(data[[col]]).squeeze()\n", "intent": "Scale the data again. Let's use `MinMaxScaler` this time just to mix things up.\n"}
{"snippet": "from sklearn.datasets import fetch_lfw_people\nfaces = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\nX, y = faces.data, faces.target\n", "intent": "We'll use this code to load the data:\n"}
{"snippet": "from sklearn.decomposition import PCA\ndef extract_eigenvectors(k, X_train):\n    pca = PCA(n_components=k)\n    pca.fit(X_train)\n    eigen_vectors = pca.components_\n    return eigen_vectors, pca\ndef make_pca_features(eigen_vectors, X):\n    return np.transpose(np.dot(eigen_vectors, np.transpose(X)))\n", "intent": "Use PCA to reduce features high dimensionality features into low dimansionality features\n"}
{"snippet": "data1 = pd.read_csv('hw4data1.csv')\ndata1.head()\n", "intent": "Let's explore clustering models on an artificial data set with five features.\n"}
{"snippet": "mush = pd.read_csv('../data/mushrooms.data',header=None,names=['edible?']+ordered_attributes)\nmush.head()\n", "intent": "So we have a dictionary of more verbose categories. We'll now want to apply them to the dataset.\n"}
{"snippet": "df_test = pd.DataFrame({\n    'AirTime': X_test['AirTime'].values,\n    'Distance': y_test['Distance'].values\n    })\ndf_pred = pd.DataFrame({\n    'AirTime': X_pred['AirTime'].values,\n    'Distance': y_pred\n    })\nax2 = plot_statsmodels_reg(df_test, df_pred)\n", "intent": "Here's an example. The blue points are points in the test data, and the red line is the regression model predicted by statsmodels.\n"}
{"snippet": "cereals_df = pd.read_csv('Cereals.csv')\npcs = PCA(n_components=2)\npcs.fit(cereals_df[['calories', 'rating']])\n", "intent": "Compute principal components on two dimensions\n"}
{"snippet": "elect_train = pd.DataFrame(data=normalized_electricity, index=np.arange('2012-01', '2014-01', dtype='datetime64[D]')).dropna()\nelect_test = pd.DataFrame(data=normalized_electricity, index=np.arange('2014-01', '2014-11', dtype='datetime64[D]')).dropna()\nXX_elect_train = elect_train.drop('electricity-kWh', axis = 1).reset_index().drop('index', axis = 1)\nXX_elect_test = elect_test.drop('electricity-kWh', axis = 1).reset_index().drop('index', axis = 1)\nYY_elect_train = elect_train['electricity-kWh']\nYY_elect_test = elect_test['electricity-kWh']\nprint XX_elect_train.shape, XX_elect_test.shape\n", "intent": "Analysis of electricity data.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX= df[range(1,17)]\nY= df[0]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=4444)\n", "intent": "Split the data into a test and training set. But this time, use this function: from sklearn.cross_validation import train_test_split\n"}
{"snippet": "df_comp = pd.DataFrame(pca.components_,columns=cancer['feature_names'])\n", "intent": "Compenents represent groups of features (e.g. amplitude features vs freq features)\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntrain_words,test_words = train_test_split(all_words,test_size=0.1,random_state=42)\n", "intent": "We hold out 20% of all words to be used for validation.\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features, columns= df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('../datasets/housesaleprediction/kc_house_data.csv')\n", "intent": "Vi leser inn filer med `pd.read_csv(fil)`. Denne funksjonen returnerer en ny Dataframe, som er et objekt vi kan behandle som en tabell.\n"}
{"snippet": "df = pd.read_csv(\"../../../datasets/movie_weekend/movie_weekend.csv\")\ndf.info()\n", "intent": "This is a brief intro / review to timeseries\n"}
{"snippet": "def docm(y_true, y_pred, labels=None):\n    cm = confusion_matrix(y_true, y_pred)\n    if labels is not None:\n        cols = ['p_'+c for c in labels]\n        df = pd.DataFrame(cm, index=labels, columns=cols)\n    else:\n        cols = ['p_'+str(i) for i in xrange(len(cm))]\n        df = pd.DataFrame(cm, columns=cols)\n    return df\n", "intent": "Print a confusion matrix and investigate what gets mixed\n"}
{"snippet": "def read_data():\n    data=pd.read_csv('multiclassdata.csv')\n    return np.array(data)  \n", "intent": "Read in the data from the file `multiclassdata.csv` using pandas\n"}
{"snippet": "st = pd.pivot_table(df_15, index = ['Store Number', 'County'],\\\n                       values = ['Bottles Sold',\\\n                                 'Sale (Dollars)',\\\n                                 'State Bottle Retail' \\\n                                ],\\\n                       aggfunc = [np.sum, np.mean]\\\n                      ).sort_values([('sum', 'Sale (Dollars)')], ascending = False)\nst.reset_index(inplace=True)\nst.head()                 \n", "intent": "In 2015, Polk had, by far, the highest sales in all counties in IOWA, over 2 times the next highest county, Linn.\n"}
{"snippet": "train = train.fillna({\"Embarked\": \"S\"})\n", "intent": "It's clear that the majority of people embarked in Southampton (S). Let's go ahead and fill in the missing values with S.\n"}
{"snippet": "results = [standard_classification_knn(madelon_feature_df,\n                                       madelon_target_df,\n                                       n_neighbors,\n                                       random_state=42) \n           for n_neighbors in range(2,20)]\nresults_df = pd.DataFrame(results)\nresults_df.head(2)\n", "intent": "In order to get some context into what this means, we should look at multiple values of train and testing score.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntext_train, text_test, y_train, y_test = train_test_split(text, y, \n                                                          random_state=42,\n                                                          test_size=0.25,\n                                                          stratify=y)\n", "intent": "Next, we split our dataset into 2 parts, the test and training dataset:\n"}
{"snippet": "df = pd.read_csv(\"data/tips.csv\")\ndf.head()\n", "intent": "For our example we'll work with a simple dataset on tips:\n"}
{"snippet": "train_df = pd.read_csv('../input/flight_delays_train.csv')\ntest_df = pd.read_csv('../input/flight_delays_test.csv')\n", "intent": "Download data from the [competition page](https://www.kaggle.com/c/flight-delays-fall-2018/data) and change paths if needed.\n"}
{"snippet": "from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nX_scaled = pd.DataFrame(data = X)\nX_scaled = scaler.fit_transform(X)\n", "intent": "<b>Eliminating Scaling issues using RobustScaler</b>\n"}
{"snippet": "import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\ndf = pd.read_csv('//home//anshul//anaconda3//anshul//scikit-learn//scikit-learn-application//Mastering-Machine-Learning-with-scikit-learn-Second-Edition-master//chapter08//ad.data', header=None, low_memory=False)\n", "intent": "- Decision Trees\nTree-like graphs that model a decision.\n- Algorithm to create a Decision Tree:\nIterative Dichotomiser3 (ID3)\n"}
{"snippet": "train = pd.read_csv(\"datasets/titanic/train.csv\")\ntest = pd.read_csv(\"datasets/titanic/test.csv\")\n", "intent": "```\ntrain = pd.read_csv(\"../datasets/titanic/train.csv\")\ntest = pd.read_csv(\"../datasets/titanic/test.csv\")```\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n", "intent": "Use train_test_split to create training and testing data\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nCV = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "from keras import preprocessing\npad_seq = preprocessing.sequence.pad_sequences(token_seq, maxlen=120, value=0)\n", "intent": "3\\. Use `pad_sequences` from `keras.preprocessing.sequence` to pad each sequence with zeros to **make the sequence length 120**. [2 pts]\n"}
{"snippet": "data = pd.read_csv('data/houses.csv')\n", "intent": "Load a subset of the housing data\n"}
{"snippet": "df = pd.read_csv(\"data/beer_reviews.tar.gz\", compression='gzip')\n", "intent": "Import data in pandas dataframe\n"}
{"snippet": "iris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  \ny = (iris[\"target\"] == 2).astype(np.float64)  \n", "intent": "* conventions: b = bias term; w = feature weights vector.\n"}
{"snippet": "HEADER = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n               'marital_status', 'occupation', 'relationship', 'race', 'gender',\n               'capital_gain', 'capital_loss', 'hours_per_week',\n               'native_country', 'income_bracket']\npd.read_csv(TRAIN_DATA_FILE, names=HEADER).head()\n", "intent": "The **training** data includes **32,561** records, while the **evaluation** data includes **16,278** records. \n"}
{"snippet": "X = np.vstack([logRe_kpc, mu_e, logsigma])\ndf = pd.DataFrame(X.T, columns=('logRe', 'mu_e', 'logsigma'))\n", "intent": "For the data to fit we use physical size for the radius.\n"}
{"snippet": "def load_mnist(dataset_folder, filename):\n    filepath = os.path.join(dataset_folder, filename)\n    with gzip.open(filepath, 'rb') as f:\n        file_content = f.read()\n    print(\"Loaded %s to memory\" % filepath)\n    return file_content\n", "intent": "----------\n[Back to Implementation Overview](\n"}
{"snippet": "df = pd.read_csv(\"KNN_Project_Data\")\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "from scipy.misc import imread\npath = datafolder + \"indoor/1.jpg\"\nimage1 = imread(path)\n", "intent": "Two subfolders exist: one containing a range of indoor images and another one with outdoor images. We can load an image into python by making use of:\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ncategories = ['rec.sport.hockey', 'rec.sport.baseball', 'rec.autos']\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\nnewsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n", "intent": "In this lab we will use the perceptron to \n"}
{"snippet": "test = pd.read_table(\"test.tsv\")\nX_actual_test = test['Phrase']\n", "intent": "Now load the actual test data from Kaggle and test the model\n"}
{"snippet": "tdf = pd.read_csv('titanic_train.csv')\n", "intent": "Import Titanic data\n"}
{"snippet": "X = cv.fit_transform(X)\n", "intent": "** Use the fit_transform() on the CountVectorizer object and pass in X (the 'text' column), and save this result by overwriting X.**\n"}
{"snippet": "linear_dep = pd.DataFrame()\n", "intent": "On the training set, we compute the Pearson correlation, $F$-statistic, and $p$ value of each predictor with the response variable `charged_off`.\n"}
{"snippet": "yelp_review_votes=pd.read_csv(\"/Users/yuka/Desktop/DataScience/Yelp Project/yelp_review_votes_master_data.csv\", encoding='utf-8-sig')\nyelp_data_final=pd.read_csv(\"/Users/yuka/Desktop/DataScience/Yelp Project/yelp_data_final_master_data.csv\", encoding='utf-8-sig')\n", "intent": "Exploratory analysis on the final analytics dataset: yelp_data_final_update\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n", "intent": "We've successfully loaded `MNIST` into our environment, so let's take a look at its structure.\n"}
{"snippet": "target_var = 'W_Pct'\nrs = 0\ndef get_train_test():\n    return train_test_split(X, y[target_var],\n                            test_size=0.3, random_state=rs)\n", "intent": "Predicting win percentage (W_Pct)\n"}
{"snippet": "UserMovieMatrixz = UserMovieMatrix.fillna(2.5)\n", "intent": "Initialize an `SVD` instance called `svd`\n"}
{"snippet": "x_test, y_test = sine_test_data()\nprint('The R2 score of the model on the test set is:', model.score(pol_exp.fit_transform(x_test), y_test))\n", "intent": "Now let's see what this results to in the test set.\n"}
{"snippet": "df3 = pd.read_csv('name_freq.csv')\ndf3 = df3.drop('Unnamed: 0', 1)\n", "intent": "Add first name frequency\n"}
{"snippet": "wine_mag_train, wine_mag_test, wine_abv_train, wine_abv_test = train_test_split(wine_data_mag, wine_data_abv)\n", "intent": "And, as always, let's split up the data. Our target values are going to be the continuous abv values.\n"}
{"snippet": "from sklearn.linear_model import Lasso, LinearRegression\npipe_lr = make_pipeline(PolynomialFeatures(include_bias=False), \n                        StandardScaler(),\n                        LinearRegression())\npipe_lr.fit(X_dev, y_dev)\n", "intent": "Let's train a linear regression model\n"}
{"snippet": "import numpy as np\nimport pandas as pd\ndf = pd.read_csv('https://raw.githubusercontent.com/aprilypchen/depy2016/master/adult.csv', na_values=['\n", "intent": "Task: Given attributes about a person, predict whether their income is <=50K or >50K\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\nprint(data.head(10))\ndata.isnull().sum()\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "from sklearn import cross_validation\ndef shuffle_split_data(X, y):\n    X, y =housing_prices, housing_features\n    X_train, X_test, y_train, y_test = cross_validation.train_test_split(housing_features, housing_prices, test_size=0.3, random_state=0)\n    return X_train, y_train, X_test, y_test\ntry:\n    X_train, y_train, X_test, y_test = shuffle_split_data(housing_features, housing_prices)\n    print \"Successfully shuffled and split the data!\"\nexcept RuntimeError:\n    print \"Something went wrong with shuffling and splitting the data.\", RuntimeError\n", "intent": "*Why do we split the data into training and testing subsets for our model?*\n"}
{"snippet": "iris = datasets.load_iris()\nx = iris.data[ :, [ 2, 3 ] ]\ny = iris.target\nnp.unique(y)\n", "intent": "load the iris datset, the target is already stored as integers\n"}
{"snippet": "file_path = os.path.join('data', 'model_likes_anon.psv')\ndf = pd.read_csv(file_path, sep = '|', quotechar = '\\\\')\nprint( 'Drop duplicated rows: ', df.duplicated().sum() )\ndf = df.drop_duplicates()\ndf = df[['mid', 'uid']]\nprint('dimension: ', df.shape)\ndf.head()\n", "intent": "http://blog.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/\n"}
{"snippet": "KNN_data = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "x_test, y_test = sine_test_data()\nprint 'The R2 score of the model on the test set is:', model.score(pol_exp.fit_transform(x_test), y_test)\n", "intent": "Now let's see what this results to in the test set.\n"}
{"snippet": "train = pd.read_csv(\"../input/train.csv\")\ntrain.head()\n", "intent": "Let us load in the training data provided using Pandas:\n"}
{"snippet": "from sklearn.decomposition import PCA\nwine_pca = PCA().fit(wine_cont_n)\n", "intent": "---\nCreate a new dataframe with the principal components and the `red_wine` column added back in from the original data.\n"}
{"snippet": "test_data_encoded['AMT_ANNUITY'].fillna(test_data_encoded['AMT_ANNUITY'].mean(), inplace=True)\n", "intent": "Those two values will be filled with mean value for whole feature\n"}
{"snippet": "OR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,1)})\nOR\n", "intent": "Thus, it has learned the function perfectly. Now for OR:\n"}
{"snippet": "pca_featurized = PCA(n_components = 2).fit(featurized)\n", "intent": "Run PCA with 2 components on the featurizations\n"}
{"snippet": "df_tram = pd.DataFrame(tram, columns=df.columns[:-1])\ndf_tram.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.datasets import load_boston\nX = load_boston().data\nprint X.shape\n", "intent": "http://stats.stackexchange.com/questions/69157/why-do-we-need-to-normalize-data-before-analysis\n"}
{"snippet": "vect = TfidfVectorizer(stop_words='english')\ndtm = vect.fit_transform(yelp.text)\nfeatures = vect.get_feature_names()\nprint dtm.shape\n", "intent": "---\nReddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF!\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n", "intent": "MNIST data is being built into ``from keras.datasets`` module.\n"}
{"snippet": "band_similarities = pd.DataFrame(similarity_matrix, index=data.columns[1:],columns=data.columns[1:])\nband_similarities\n", "intent": "To make a nice print of the data we will use the pandas library as follows.\n"}
{"snippet": "X, y = yelp_class['text'], yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df['TailNum'].unique().tolist()[0:10])\nle.transform(df['TailNum'].unique().tolist()[0:10]) \n", "intent": "+ THESE ARE NOT DUMMY VARIABLES\n+ Note the fit vs fit_transform function\n"}
{"snippet": "tracks = pd.read_csv('../data/fma_tracks.csv', index_col=0)\nprint('You got {} track IDs.'.format(len(tracks)))\n", "intent": "The `fma_tracks.csv` file contains a list of 2'000 track IDs that we will use through the assignment.\n"}
{"snippet": "DiffSparsBudgW = pd.read_csv('Saved_Datasets/DiffNormSparsBudgW.csv')\nDiffSparsBudgW = DiffSparsBudgW.as_matrix()\n", "intent": "It can be seen that on the plot above the movies cannot be separated. Therefore it was tried to sparsify the matrix.\n"}
{"snippet": "feat_train, feat_test, crime_train, crime_test = train_test_split(features, crime_rate, test_size = 0.2, random_state=11)\n", "intent": "Split 80-20 train and test data, to use for model.\n"}
{"snippet": "results = pd.DataFrame(columns=[\"Accuracy\"])\n", "intent": "It is important to track results of different experiments for the sake of further comparison.\n"}
{"snippet": "sentences_indexed = pad_sequences(sentences_indexed, maxlen=MAX_SEQUENCE_LENGTH)\ntags_indexed = pad_sequences(tags_indexed, maxlen=MAX_SEQUENCE_LENGTH)\n", "intent": "And then pad these sequences to the same length (equal to the maximum length of the sentence)\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(df.drop('Outcome', axis=1))\nX.shape\n", "intent": "- X feature engineering by Standard Scaler (every feature get mean = 0)\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nitrain, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itrain]= True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "n_best = 10\ntop_n_similar_bands = pd.DataFrame(index=band_similarities.columns,columns=range(1, n_best + 1))\nfor i in range(0, len(band_similarities.columns)):\n    top_n_similar_bands.iloc[i,:] = band_similarities.iloc[:,i].sort_values(ascending=False)[:n_best].index\ntop_n_similar_bands\n", "intent": "Now let's do this for all bands.\n"}
{"snippet": "stats = pd.read_csv('~/Documents/Data Science/Capstone 1/data/CLEAN/ncaa_d1_stats.csv', index_col=0)\nstats = reset_order(stats)\nstats.info()\nstats.head()\n", "intent": "- describe\n- deal with missing values\n- fix outliers\n- drop unneeded columns\n- save for later\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2018)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "authorship = pd.read_csv(\"http://people.stern.nyu.edu/jsimonof/AnalCatData/Data/Comma_separated/authorship.csv\")\nprint(authorship.columns)\n", "intent": "- \"http://people.stern.nyu.edu/jsimonof/AnalCatData/Data/Comma_separated/authorship.csv\"\n- Print the columns, print df.head()\n"}
{"snippet": "dataset = pd.read_csv('Churn_Modelling.csv')\n", "intent": "Importamos el dataset\n"}
{"snippet": "with open('reviews.txt', 'r') as f:\n    reviews = f.read()\nwith open('labels.txt', 'r') as f:\n    labels = f.read()\n", "intent": "Load data and labels.\n"}
{"snippet": "tfidf_vectorizer_title = TfidfVectorizer(ngram_range=(1, 3),\n                                         max_features=100000)\nwith open('../data/train_title.txt', encoding='utf-8') as input_train_file:\n    X_train_title_sparse = tfidf_vectorizer_title.fit_transform(input_train_file)\nwith open('../data/test_title.txt', encoding='utf-8') as input_test_file:\n    X_test_title_sparse = tfidf_vectorizer_title.transform(input_test_file)\n", "intent": "**Tf-Idf with titles.**\n"}
{"snippet": "def normalize(array):\n    scaler = StandardScaler()\n    array = scaler.fit_transform(array)\n    return scaler, array\n", "intent": "I defined a normalizer to be able to normalize the data using a simple function rather than a complex one that I would constantly have to google.\n"}
{"snippet": "dfMCats.fillna(0, inplace = True)\n", "intent": "- Random Forest Don't Like\n"}
{"snippet": "reload(preproc);\nX_tr, Y_tr = preproc.load_data(TRAIN_FILE)\nprint (bilstm.prepare_sequence(X_tr[5],word_to_ix).data.numpy())\nprint (bilstm.prepare_sequence(Y_tr[5],tag_to_ix).data.numpy())\n", "intent": "- Loading Train data for english:\n"}
{"snippet": "print('For user with id', data.iloc[user_index, 0], 'we advice:')\npd.DataFrame(scores, index=band_similarities.columns).sort_values(0, ascending=False).iloc[:5]\n", "intent": "Now let's make a nice print of the top 5 bands to advice to this user:\n"}
{"snippet": "X = pd.DataFrame(house.data, columns=house.feature_names)\ny = pd.Series(house.target)\n", "intent": "For both datasets, define the feature and target variables\n"}
{"snippet": "votes = pd.read_csv(votes_file)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "data = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=462)\n", "intent": "You're each going to be a decision tree on some data based on a bootstrap sample, and then we'll all ensemble the results.\n"}
{"snippet": "data = Series ([1., NA, 3.5, NA, 7])\ndata.fillna(data.mean())\n", "intent": "But, sometimes is important to evaluate first other methods to fill our data, for example through the application of the basic statistics\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nbc_feature_cols = over_sample.drop(['Class','Sample_code_number'], axis=1)\nbc_features = scaler.fit_transform(bc_feature_cols)\n", "intent": "**2) Are the features normalized? If not, use the scikit-learn standard scaler to normalize them.**\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == ['ORG'] or ['PERSON'] for word in parsed])\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title']].head()\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(centers=2, random_state=0)\n", "intent": "First, we will look at a **classification problem in two dimensions**. We use the synthetic data generated by the **`make_blobs`** function.\n"}
{"snippet": "importance_dt = pd.DataFrame(dt.feature_importances_, index=credit_clean.columns[:-1], \n                          columns=[\"D Tree importance\"])\nimportance_rfdt = pd.DataFrame(rfdt.feature_importances_, index=credit_clean.columns[:-1], \n                               columns=[\"Random f Importance\"])\npd.concat([importance_dt,importance_rfdt],axis=1)\n", "intent": "Compare the feature importances as estimated with the decision tree and random forest classifiers.\n"}
{"snippet": "data_path1 = os.path.join(os.getcwd(),'datasets','splice_train.csv')\ndata_path2 = os.path.join(os.getcwd(),'datasets','splice_test.csv')\nsplice_train = pd.read_csv(data_path1)\nsplice_test = pd.read_csv(data_path2)\nprint('Shape of splice_train: {}'.format(splice_train.shape))\nprint('Shape of splice_test: {}'.format(splice_test.shape))\nsplice_train.head(10)\n", "intent": "Load the `splice_train.csv` and `splice_test.csv` into two separate dataframes. Display the shape and first 10 instances for each dataframe.\n"}
{"snippet": "bank_attributes_train, bank_attributes_test, bank_labels_train, bank_labels_test = train_test_split(\n    bank_attributes, bank_labels, train_size = 0.7, stratify = bank_labels)\n", "intent": "Use the standard 70% / 30% split. Since this is a classification problem, be sure to stratify the split according to the `bank_labels`.\n"}
{"snippet": "wine = pd.read_table(\"../2. Python Data Handling - Pandas and Networkx/wine.dat\", sep=\"\\s+\")\nattributes = ['Grape','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols',\n            'Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue',\n            'OD280/OD315 of diluted wines','Proline']\nwine.columns = attributes\n", "intent": "For scatterplots, the x and y axis must be specified as columns:\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n", "intent": "* Load the data set, reshape and assign to variables\n"}
{"snippet": "from sklearn.datasets import load_digits\nfrom sklearn.cross_validation import train_test_split\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data,\n                                                    digits.target)\n", "intent": "Preprocessing and Pipelines\n=============================\n"}
{"snippet": "scaler=StandardScaler()\n", "intent": "**Create a StandardScaler() object called scaler.**\n"}
{"snippet": "url = './data/bikeshare.csv'\nbikes_cols=['season_num','is_holiday','is_workingday','weather','temp_celsius','atemp','humidity_percent',\n                'windspeed_knots','num_casual_users','num_registered_users','num_total_users']\nbikes = pd.read_csv(url, names=bikes_cols, skiprows=1)\nbikes.columns = bikes_cols\nbikes.head()\n", "intent": "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n"}
{"snippet": "path = Path('..', 'data', 'titanic.csv')\ntitanic = pd.read_csv(path)\n", "intent": "We'll build a classification tree using the Titanic survival data set:\n"}
{"snippet": "conn = pg2.connect(host=\"michaelgfrantz.com\", user=\"postgres\", password=\"dsism4\")\ncurs = conn.cursor(cursor_factory=pgex.RealDictCursor)\ncurs.execute(\"select channel from customer where region=3\")\nresults = curs.fetchall()\nconn.close()\ntarget_reg_3_df = pd.DataFrame(results)\ntarget_reg_3 = target_reg_3_df['channel']\n", "intent": "1. Use a `RealDictCursor`\n1. assign the results to `target_reg_3_df`\n1. assign the `channel` column to a series `target_reg_3`\n"}
{"snippet": "pca = PCA()\nboston_train_pca = pca.fit_transform(X_train_scaled)\nboston_test_pca = pca.transform(X_train_scaled)\n", "intent": "Just so we can visualize our data, let's do a PCA and plot our components 1 and 2, and make the color the target for our train set. \n"}
{"snippet": "pd.DataFrame(clf.cv_results_).T[2:6]\n", "intent": "Define the target and feature set for the test data\n"}
{"snippet": "data = pd.read_csv('../data/zillow_realtor_homes_201804.csv')\ndata.head()\n", "intent": "Keywords: linear regression, nonparametric regression\nThe number of bedrooms is a predictor for the list price of the house\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"../../DS-SF-32/dataset/2008.csv\").fillna(\"unk\")\n", "intent": "http://stat-computing.org/dataexpo/2009/the-data.html\n"}
{"snippet": "df.fillna(0, inplace=True) \n", "intent": "mostly categorical noise and features that we do not want our model exposed to, also fill Nulls (Nans) with zeroes, but most of all save save save!\n"}
{"snippet": "binary_vectorizer = CountVectorizer(binary=True)\nbinary_vectorizer.fit(X_text)\nX = binary_vectorizer.transform(X_text)\n", "intent": "Next, we will turn `X_text` into just `X` -- a numeric representation!\n"}
{"snippet": "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1)\nprint(f'Shape of the training data {X_train.shape}')\nprint(f'Shape of the validation dat {X_val.shape}')\n", "intent": "Split training data into training and validation data\n"}
{"snippet": "data_path = 'c:/Users/E411208/Documents/Python Scripts/data/Bluetooth/'\ndata = pd.DataFrame()\nfor id,file in enumerate(os.listdir(data_path)):\n    if file.endswith(\".csv\"):\n        print('Loading file ... ' + file)\n        measurement = pd.read_csv(data_path + file)\n        measurement['measurement']=id \n        data = data.append(measurement,ignore_index = True)\n", "intent": "Here we load all CSV files\n"}
{"snippet": "pd.pivot_table(prediction_of_actve_df,index=[\"CHURN_BAND\"],values=[\"EXTRELNO\"],aggfunc='count')\n", "intent": "    *******************************  Summarizing the probability of churn ****************************************\n"}
{"snippet": "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()\n", "intent": "What is the relationship between _Fare_ and survival\n"}
{"snippet": "data = loadmat('data/ex7data2.mat')\nX = pd.DataFrame(data['X'], columns=['X1','X2'])\nX.head()\n", "intent": "Let's load our data\n"}
{"snippet": "X=pd.DataFrame(iris.data,columns=iris.feature_names)\ny=pd.DataFrame(iris.target)\nX.head()\n", "intent": "Define the \"x\" and \"y\" variables. *Remember*, y is our classifer, and x is our attributes.\n"}
{"snippet": "MPI_data = pd.read_csv(\"MPI_data_poverty.csv\", encoding=\"ISO-8859-1\")\nloan_data = pd.read_csv(\"loans_midterm.csv\")\ncountry_mapper = pd.read_csv(\"country_mapper.csv\")\n", "intent": "Import the MPI, country mapper, and loans data.\n"}
{"snippet": "tsne_df = pd.DataFrame(coordinates, \n                       columns=['tsne_y', 'tsne_x'])\n", "intent": "These coordinates can be stored in a dataframe.  \n"}
{"snippet": "path = Path('.','data','titanic.csv')\ntitanic = pd.read_csv(path)\ntitanic.head()\n", "intent": "We'll build a classification tree using the Titanic survival data set:\n"}
{"snippet": "df = df.fillna(999)\n", "intent": "replacing the missing values with 999.\n"}
{"snippet": "documents = []\nfor ind, talk in talks.fillna('').iterrows():\n    documents.append( ' '.join([talk['title'],\n                                talk['desc'],\n                                talk['abstract'] \n                                ]) )\nprint (\"Read %d corpus of documents\" % len(documents))\n", "intent": "Build a bag of words for each document\n"}
{"snippet": "pd.DataFrame(lm.coef_, X.columns, columns = [\"coeff\"])\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "train_features = pd.read_csv('data/training_features.csv')\ntest_features = pd.read_csv('data/testing_features.csv')\ntrain_labels = pd.read_csv('data/training_labels.csv')\ntest_labels = pd.read_csv('data/testing_labels.csv')\nprint('Training Feature Size: ', train_features.shape)\nprint('Testing Feature Size:  ', test_features.shape)\nprint('Training Labels Size:  ', train_labels.shape)\nprint('Testing Labels Size:   ', test_labels.shape)\n", "intent": "First let's read in the formatted data from the previous notebook. \n"}
{"snippet": "vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(raw_documents)\nprint(\"Number of terms in model is %d\" % len(vectorizer.vocabulary_) )\n", "intent": "We can remove low frequency terms that appear in fewer than a specified number of documents:\n"}
{"snippet": "import pandas as pd\ntracks = pd.read_csv(\"fma-rock-vs-hiphop.csv\")\nechonest_metrics = pd.read_json(\"echonest-metrics.json\")\n", "intent": "First import `pandas` and use `read_csv` and `read_json` to read in the files `fma-rock-vs-hiphop.csv` and `echonest-metrics.json`.\n"}
{"snippet": "X = subframe\nscaler = Normalizer().fit(X)\nnormalized = scaler.transform(X)\nnp.set_printoptions(precision=3)\nprint(normalized[0:5,:])\n", "intent": "- Use sklearn to Normalize the subframe based on a length of 1 (experiment)\n- Display the changes in a summary\n"}
{"snippet": "pca = PCA(n_components=361)\npca.fit(faces)\nv = np.concatenate([pca.mean_[None], pca.components_])\n", "intent": "Run the provided function `PCA` on the images (rows) in `faces`:\n"}
{"snippet": "prediction = probabilities > 0.05\nconfusion_matrix_small = pd.DataFrame(metrics.confusion_matrix(Y_test, prediction, labels=[1, 0]).T,\n                                columns=['p', 'n'], index=['Y', 'N'])\nprint confusion_matrix_small\n", "intent": "What if we lower the threshold to 5%?\n"}
{"snippet": "from sklearn.cross_validation import KFold, cross_val_score\ny, X = pima_predict_df['Class variable'], pima_predict_df.drop(['Class variable'], axis=1)\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2, random_state=7)\n", "intent": "**5. What's the best performance you can get with kNN? Is kNN a good choice for this dataset?**\n==\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.fit_transform(X_test)\n", "intent": "Standardizing the data.\n"}
{"snippet": "lasso_model = Lasso(alpha=100.0)\nlasso_model.fit(X_train, y_train)\nbest_feature_selector = SelectFromModel(lasso_model, prefit=True)\nX_best = best_feature_selector.transform(X_train)\nX_best.shape\n", "intent": "Best features determined using the lasso (a linear model with L1 regularisation).\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(X, y)\nprint len(x_train), len(x_test)\nprint len(y_train), len(y_test)\n", "intent": "- Create test/train splits of the original data set\nx_train, x_test, y_train, y_test = train_test_split(...)\n"}
{"snippet": "def csv_to_df(filepath):\n    df = pd.read_csv(filepath, header = 0)\n    return df\ntrain = csv_to_df('/home/nikita/Documents/DataScienceTaskExzeo/exzeo_training_data.csv')\ntest = csv_to_df('/home/nikita/Documents/DataScienceTaskExzeo/exzeo_test_data.csv')\n", "intent": "Now first of all, we need to load the data using pandas read_csv function which will load the csv file and convert it into dataframe.\n"}
{"snippet": " ad_data = pd.read_csv(\"advertising.csv\")\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "df = pd.read_csv('https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv', index_col=0)\ndf.head()\n", "intent": "1) Load in the dataset https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv into a pandas dataframe\n"}
{"snippet": "df =pd.read_csv('https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv', index_col = 0)\n", "intent": "1) Load in the dataset `https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv` into a pandas dataframe\n"}
{"snippet": "df_data = pd.read_csv(open('vehicle_stops_2016_datasd.csv')); \ndf_data2 = pd.read_csv(open('vehicle_stops_2017_datasd.csv'));\n", "intent": "Here we import the data sets. The data is held in local files which we downloaded from https://data.sandiego.gov\n"}
{"snippet": "df = pd.read_csv(\"./datasets/iris.csv\")\n", "intent": "Load in the iris data set.\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\nlist(iris.keys())\n", "intent": "* Detect the species type based on petal width\n* Import logistic regression from sklearn linear model\n"}
{"snippet": "from sklearn.decomposition import IncrementalPCA\nn_batches = 100\ninc_pca = IncrementalPCA(n_components=154)\nfor X_batch in np.array_split(X_mnist, n_batches):\n    print(\".\", end=\"\") \n    inc_pca.partial_fit(X_batch)\nX_mnist_reduced_inc = inc_pca.transform(X_mnist)\nprint(np.sum(inc_pca.explained_variance_ratio_))\n", "intent": "* PCA normally requires entire dataset in memory for SVD algorithm.\n* **Incremental PCA (IPCA)** splits dataset into batches.\n"}
{"snippet": "for dataset in data_cleaner:\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n    dataset.drop(['Cabin', 'Ticket', 'Fare', 'Name'], axis=1, inplace = True)\n", "intent": "In the case of the port of Embarkation, we see that only 2 values are null. We will use the mode of this column to fill in these values.\n"}
{"snippet": "rmse_lambda_df = pd.DataFrame(rmse_CV_lambda,columns=['lambda', 'RMSE']).set_index('lambda')\nrmse_lambda_df.plot()\n", "intent": "We search the $\\lambda$ grid : 0.0001-0.01 to find the optimal $\\lambda$ parameter\n"}
{"snippet": "numeric_cols = pd.read_csv(DataPath+\"train_numeric.csv\", nrows = 10000).columns.values\nimp_idxs = [np.argwhere(feature_name == numeric_cols)[0][0] for feature_name in feature_names]\ntrain = pd.read_csv(DataPath+\"train_numeric.csv\", \n                index_col = 0, header = 0, usecols = [0, len(numeric_cols) - 1] + imp_idxs)\ntrain = train[feature_names + ['Response']]\n", "intent": "We determine the indices of the most important features. After that the training data is loaded\n"}
{"snippet": "pca_mod = skde.PCA()\npca_comps = pca_mod.fit(X_train)\npca_comps\n", "intent": "The code in the cell below computes the principle components for the training feature subset. Execute this code:\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\n", "intent": "The dataset we'll use for this lesson is included as part of Scikit-Learn, namely the breast cancer dataset.\n"}
{"snippet": "df = pd.read_csv('RWA_DHS6_2010_2011_HH_ASSETS.CSV', index_col=0)\ndf\n", "intent": "Load, clean, and prepare DHS asset ownership data:\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n", "intent": "The handwritten digits recognition problem we will face is already included as a testbed in keras. Loading it only requires invoking\n"}
{"snippet": "from sklearn import decomposition\nnmf = decomposition.LatentDirichletAllocation(n_topics=6)\nnmf.fit(tfidf)\nW = nmf.transform(tfidf)\nH = nmf.components_\n", "intent": "Compare the results of Nonnegative Matrix Factorization (NMF) with Latent Dirichlet Allocation (LDA).\n"}
{"snippet": "page_addoc_dist = pd.read_csv(\"../generated/final/page_addoc_ents_dist.csv\")\npage_addoc_dist.head()\n", "intent": "Can be run after feature_base_4_ents\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom IPython.display import display\nX = wells.drop(\"status_group\", axis=1)\ny = wells[\"status_group\"]\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.25,\n                                                    random_state=666)\ndisplay(y_train.value_counts() / len(y_train))\ndisplay(y_test.value_counts() / len(y_test))\n", "intent": "Why is this a problem when doing a train-test split?\n"}
{"snippet": "try_new_vectoriser(CountVectorizer(binary=False,\n                                   stop_words='english',\n                                   min_df=2\n                                  ),\n                   X_train,\n                   y_train)\n", "intent": "Instead of binary let's use actual counts\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nX = df[[\"fixed acidity\", \"chlorides\", \"total sulfur dioxide\"]]\ny = df[\"colour\"]\ny_binary = y.map({\"red\": 1, \"white\": 0})\nX_train, X_test, y_train, y_test = train_test_split(X, y_binary, stratify=y, test_size=0.3, random_state=42)\nprint(len(X_train), len(X_test))\n", "intent": "Using the make-up of classes investigated above as an indication - should you or shouldn't you stratify your samples in the train-test split?\n"}
{"snippet": "X = np.array(data.loc[:, features])\nle = LabelEncoder()\ny = le.fit_transform(np.array(data.types))\nprint X.shape, y.shape\ntrees = ExtraTreesClassifier(n_estimators=200, max_features=5, bootstrap=True, random_state=0)\ntrees.fit(X, y)\nfeature_importances = pd.Series(trees.feature_importances_, index = features)\nfeature_importances.plot(kind = 'bar')\n", "intent": "- kernel_groove_len + kernel_length\n- kernel_length\n- perimeter\n- area - very related to perimeter\n"}
{"snippet": "pca = PCA(0.50).fit(noisy)\npca.n_components_\n", "intent": "* Let's train a PCA on the noisy data, requesting that the projection preserve 50% of the variance:\n"}
{"snippet": "seed = 7\nnp.random.seed(seed)\nX_train, X_test, y_train, y_test = train_test_split(train[['O3', 'PM10', 'PM25', 'NO2', 'T2M']],\n                                                    train['mortality_rate'], \n                                                    test_size = 0.3,\n                                                    random_state=22)\n", "intent": "Our test set is like the training set above, except it does not have `mortality_rate` (that's what we're after). Ignore region and date for now.\n"}
{"snippet": "scaler = preprocessing.StandardScaler()\nX_tr_scaled = scaler.fit_transform(X_tr)\nX_te_scaled = scaler.transform(X_te)\nX_tr_scaled.mean(axis=0), X_tr_scaled.std(axis=0)\n", "intent": "Tranformation of all features to zero mean and unit variance.\n"}
{"snippet": "from scipy.misc import imread\npath = datafolder + \"indoor/1.jpg\"\nimage = imread(path)\n", "intent": "Two subfolders exist: one containing a range of indoor images and another one with outdoor images. We can load an image into python by making use of:\n"}
{"snippet": "from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\nx_scaled = min_max_scaler.fit_transform(geo_df_EU['pop_density'].values.reshape(-1,1))\ngeo_df_EU['inf_rate'] = pd.Series(x_scaled.flatten())\n", "intent": "Normalization of pop density into a contact rate between 0 and 1 \n"}
{"snippet": "from keras.datasets import mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train[:, ::2, ::2].copy()\nx_test = x_test[:, ::2, ::2].copy()\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_shape = x_train.shape[1:]\nprint('train set shape:', x_train.shape)\nprint('test set shape:', x_test.shape)\n", "intent": "*Please execute the cell bellow in order to prepare the MNIST dataset*\n"}
{"snippet": "df_ccpi = pd.DataFrame()\nfor country in ccpi_country['Country Name']:\n    df1 = wb_inter[wb_inter['Country Name'] == country]\n    df_ccpi = pd.concat( [df_ccpi,df1], ignore_index=True, axis=0)\n", "intent": "New dataframe which contains only the data from countries being mentioned in CCPI document is created.\n"}
{"snippet": "df_ccpi_feature_processed = pd.DataFrame()\nfor indi in set_indi_average:\n    df_indi = df_ccpi_feature_label_removed[(df_ccpi_feature_label_removed['Feature Interaction'] == 'average') & (df_ccpi_feature_label_removed['Indicator Name'] == indi)]\n    df_indi_ave = (df_indi.sum(axis = 0)) \n    df_ccpi_feature_processed[indi] = df_indi_ave[4:-1]/len(ccpi_country)\nprint('Dataframe shape:',df_ccpi_feature_processed.shape)    \n", "intent": "New engineered dataframe is generated. First the dataframe with the averaged features is appended. \n"}
{"snippet": "df_ccpi_clean = pd.DataFrame()\narray_flag = np.ones((len(df_ccpi)), dtype=bool)\nseries_flag = pd.Series(data = array_flag, index=range(len(df_ccpi)))\nfor indi in sorted(set(list_indi)):\n    series_flag = (series_flag & (df_ccpi['Indicator Name'] != indi))\ndf_ccpi_clean = df_ccpi[series_flag]\ndf_ccpi_clean_reset = df_ccpi_clean.reset_index(drop=True)\ndf_ccpi_clean_reset = df_ccpi_clean_reset.fillna(0)\ndf_ccpi_clean_reset.head(8)\n", "intent": "Filter the data frame so it only consists of the indicators which has complete data for the entire year (1960-2016).\n"}
{"snippet": "scaled_values = scaler.transform(X)\nX_scaled = pd.DataFrame(data=scaled_values, columns=X.columns)\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "train_df.fillna(-99, inplace=True)\ntest_df.fillna(-99, inplace=True)\n", "intent": "Now let us impute the missing values with some value which is outside the range of values of the column, say -99.\n"}
{"snippet": "df = pd.read_csv('../../assets/datasets/cars.csv')\ndf.isnull().sum()\n", "intent": "Visualize the last tree. Can you make sense of it? What does this teach you about decision tree interpretability?\n"}
{"snippet": "pop_gen = pd.DataFrame(gen_df['genre'].value_counts()).reset_index()\npop_gen.columns = ['genre', 'movies']\npop_gen.head(10)\n", "intent": "TMDB defines 32 different genres for our set of 45,000 movies. Let us now have a look at the most commonly occuring genres in movies.\n"}
{"snippet": "dataset1,dataset2 = boston_housing.load_data()\nx_train, y_train = dataset1\nx_test, y_test = dataset2\ntrain_frame = pd.DataFrame(y_train)\ntrain_frame\n", "intent": "- Define a class `MultiLayerPerceptron` which passes the Multi-Layer Perceptron tests\n"}
{"snippet": "Xs_train, Xs_test, ys_train, ys_test = train_test_split(X_start, y, train_size=0.8)\n", "intent": "Split data into training and testing sets.\n"}
{"snippet": "df_feat_scaled = pd.DataFrame(\n    scaler.transform(df.drop('TARGET CLASS', axis=1)), columns=df.columns[:-1])\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "from io import StringIO\nfrom sklearn.feature_extraction.text import CountVectorizer\nsent1 = \"The quick brown fox jumps over the lazy brown dog.\"\nsent2 = \"Mr brown jumps over the lazy fox.\"\nwith StringIO('\\n'.join([sent1, sent2])) as fin:\n    count_vect = CountVectorizer(stop_words=stoplist_combined,\n                                 tokenizer=word_tokenize)\n    count_vect.fit_transform(fin)\ncount_vect.vocabulary_\n", "intent": "We can **override the tokenizer and stop_words**:\n"}
{"snippet": "df = pd.read_csv('Iowa_Liquor_Sales.csv')\ndf.shape\n", "intent": "Perform some exploratory statistical analysis and make some plots, such as histograms of transaction totals, bottles sold, etc.\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values='NaN', strategy='median', axis=0)\ntrain = imp.fit_transform(train)\nxtrain = train[fold1,1:]\nytrain = train[fold1,0]\nxtest = train[K[0],1:]\nytest = train[K[0],0]\n", "intent": "The following code replace missing data with median in the column.\n"}
{"snippet": "cv = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "pd.DataFrame(rfe.ranking_.T, index = X.columns.values).sort_values(by = 0).head(20)\n", "intent": "Review the top ranked features from our RFE selection:\n"}
{"snippet": "nhl = pd.read_csv('https://raw.githubusercontent.com/josephnelson93/GA-DSI/master/NHL_Data_GA.csv')\nnhl.head()\n", "intent": "Feel free to also do basic EDA. At least check the head()!\n"}
{"snippet": "data.index = pd.to_datetime(data.index)\nstore1 = data[data.Store==1]\nstore1 = pd.DataFrame(store1['Weekly_Sales'].resample('1D').sum(),columns=['Weekly_Sales'])\nstore1.dropna(inplace=True)\n", "intent": "- Filter the dataframe to Store 1 sales and aggregate over departments to compute the total sales per store.\n"}
{"snippet": "n_best = 10\ntop_n_similar_bands = pd.DataFrame(index=band_similarities.columns,columns=range(1, n_best + 1))\nfor i in range(0, len(band_similarities.columns)):\nraise NotImplementedError()\ntop_n_similar_bands\n", "intent": "Now let's do this for all bands.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n", "intent": "But in reality our model has to form predictions on *unseen* data. Let's model this situation.\n"}
{"snippet": "y_scaler.inverse_transform([[0.024229725354342153]])\n", "intent": "transfer to degrees\n"}
{"snippet": "path = Path('.', 'data', 'vehicles_train.csv')\ntrain = pd.read_csv(path)\n", "intent": "<a id=\"manual-bagged\"></a>\n"}
{"snippet": "train_df = pd.read_csv(\"./input/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"./input/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]\n", "intent": "Reading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"/Users/michaelkunkel/WORK/GIT_HUB/DeepLearning/Introduction2/DataScience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "pca  =  PCA(n_components =1)\npca.fit(images)\nimages_pca = pca.transform(images)\nprint(\"original shape:   \", images.shape)\nprint(\"transformed shape:\", images_pca.shape)\n", "intent": "Here is an example of using PCA as a dimensionality reduction transform:\n"}
{"snippet": "import s3fs\nfs = s3fs.S3FileSystem(anon=True)\ntestimage = fs.ls('esipfed/cdi-workshop/semseg_data/ontario/test/')[0]\nwith fs.open(testimage, 'rb') as f:\n    img = imread(f)\n", "intent": "The ontario dataset sits on S3. Let's load an image in\n"}
{"snippet": "bos_plot = pd.DataFrame(columns=['PRICE','fittedvalues'])\nbos_plot = bos_plot.fillna(0)\nbos_plot[\"fittedvalues\"] = m.fittedvalues\nbos_plot[\"PRICE\"] = bos.PRICE\nbos_plot.head()\n", "intent": "**Your turn:** Create a scatterpot between the predicted prices, available in `m.fittedvalues` and the original prices. How does the plot look?\n"}
{"snippet": "df = pd.DataFrame(scaled_data, columns=data.columns)\ndf.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data = open('./utils/kafka.txt', 'r', encoding=\"utf-8\").read()\nchars = list(set(data)) \ndata_size, vocab_size = len(data), len(chars)\nprint('data has %d chars, %d unique' % (data_size, vocab_size))\n", "intent": "The text corpus to train your RNN on is going to be the book The metamorphosis by Franz Kafka. Run the cell below to load the text.\n"}
{"snippet": "spotify = pd.read_csv(\"../../data/spotify_data.csv\", index_col=[0])\nspotify.head()\n", "intent": "Link to my article about the project: https://opendatascience.com/blog/a-machine-learning-deep-dive-into-my-spotify-data/\n"}
{"snippet": "ss = StandardScaler()\nmm = MinMaxScaler()\nX_ss = ss.fit_transform(X)\nX_mm = mm.fit_transform(X)\n", "intent": "We can fit and transform at the same time\n"}
{"snippet": "path = \"../../data/fraud.csv\"\nfraud = pd.read_csv(path, index_col=[0])\nfraud.drop(\"Time\", axis = 1)\nfraud.head()\n", "intent": "Let's move onto the real thing by modeling credit card fraud data\nhttps://www.kaggle.com/dalpozz/creditcardfraud\n"}
{"snippet": "path = \"../../data/NLP_data/ds_articles.csv\"\narticles = pd.read_csv(path, usecols=[\"text\", \"title\"], encoding=\"utf-8\")\narticles.dropna(inplace=True)\narticles.reset_index(inplace=True, drop=True)\narticles.head()\n", "intent": "We're going to build a very simple summarizer that uses tfidf scores on a corpura of data science and artificial intelligence articles\n"}
{"snippet": "path = \"../../data/fraud.csv\"\nfraud = pd.read_csv(path, index_col=[0])\nfraud.drop(\"Time\", axis = 1, inplace = True)\nfraud.head()\n", "intent": "Let's move onto the real thing by modeling credit card fraud data\nhttps://www.kaggle.com/dalpozz/creditcardfraud\n"}
{"snippet": "from pyensae.datasource import download_data\nfile = download_data(\"Divvy_Trips_2016_Q3Q4.zip\", url=\"https://s3.amazonaws.com/divvy-data/tripdata/\")\n", "intent": "[Divvy Data](https://www.divvybikes.com/system-data) publishes a sample of the data. \n"}
{"snippet": "import pandas as pd\ndf = pd.DataFrame({'predictions':y_knn_pred, 'actual label':y_test})\nsgd_acc = round((df[df.predictions == df['actual label']] .shape[0]/df.shape[0])*100, 2)\nsgd_acc\n", "intent": "<h1><a name=\"acc knn\">Knn + GridSearchCV accuracy</a></h1>\n"}
{"snippet": "talk_webpage = request.urlopen(urls[0]).read()\ntalk_webpage\n", "intent": "Let's scrape each individual link for the abstract.\n"}
{"snippet": "pca = PCA(n_components=5)\npca.fit(X_std)\nX_5d = pca.transform(X_std)\n", "intent": "<h1>PCA using inbuilt functions</h1>\n"}
{"snippet": "test = imresize(data[idx],0.5)\n", "intent": "We can use the 'imresize' function from scipy.misc to downsample the image. Let's run a quick test.\n"}
{"snippet": "fnames = os.path.join(DATA_DIR, 'amazon', '*.csv')\nfnames = glob.glob(fnames)\nreviews = []\ncolumn_names = ['id', 'product_id', 'user_id', 'profile_name', 'helpfulness_num', 'helpfulness_denom',\n               'score', 'time', 'summary', 'text']\nfor fname in fnames[:2]:\n    df = pd.read_csv(fname, names=column_names)\n    text = list(df['text'])\n    reviews.extend(text)\n", "intent": "Read in all the `.csv` files in the folder `amazon`. Extract out only the text column from each file and store them all in a list called `reviews`.\n"}
{"snippet": "Xprime = cv.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "estimator2 = LinearRegression()\nselector2 = RFECV(estimator2, step=3, cv=5)\nselector2 = selector2.fit(X[include], y)\n", "intent": "Estimation with Linear Regression\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_ratios, y_normed, test_size=0.3, random_state=42)\n", "intent": "The Random Forest Regressor Does very well on both Training and Validation - a promising model!\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X[features_one], np.log(target), test_size=.30, random_state=1)\nX_train_two, X_val, y_train_two, y_val = train_test_split(X_train, y_train, test_size=.30, random_state=1)\n", "intent": " In this iteration, we try predicting the log of the target\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)\n", "intent": "Creating Training and Test datasets\n"}
{"snippet": "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, \n    stratify=train_df.coverage_class, \n    random_state=1234)\n", "intent": "stratified by salt coverage\n"}
{"snippet": "dist_mat = pd.DataFrame(np.zeros(shape=(len(final),len(final))))\nfor i in range(len(final)):\n    for j in range(len(final)):\n        dist_mat.iloc[i][j] = euclid_dist(final.iloc[i],final.iloc[j])\n", "intent": "This is another clustering algorithm I used. \nI think K-means performs better thanks DBScan Cluster.\n"}
{"snippet": "data311=pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/Cluster_lecture/cluster.csv\")\n", "intent": "Please carefully read 2.3.9:\nhttp://scikit-learn.org/stable/modules/clustering.html\n"}
{"snippet": "imagedata_list = []\nfor image in data:\n    ar = imresize(image,0.5)\n    imagedata_list.append(ar)\n", "intent": "The image looks reasonable, despite having 1/4 the number of pixels. Let's go ahead and downsample the entire set.\n"}
{"snippet": "df = pd.read_csv('datasets/dataset_2.txt', delimiter=',')\ndf.head()\n", "intent": "**Solution:**\nLet's start with some data exploration for ourselves, behind the scenes:\n"}
{"snippet": "df = pd.read_csv(\"Classified Data\",index_col=0)\n", "intent": "Setzen wir die erste Spalte (index_col) = 0, um sie als Index zu verwenden.\n"}
{"snippet": "from keras.datasets import mnist\n(X_train0, y_train0), (X_test0, y_test0) = mnist.load_data()\n", "intent": "- https://datascienceschool.net/view-notebook/51e147088d474fe1bf32e394394eaea7/\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\niris.keys()\n", "intent": "Utilize the starter code to practice hierarchical clustering on the iris dataset\n"}
{"snippet": "class CategoricalImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, cols):\n        self.cols = cols\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        transformed_df = X\n        for col in self.cols:\n            transformed_df.loc[:, col] = transformed_df.loc[:, col].astype(str)\n        return transformed_df\n", "intent": "For example, we could write the feature processing step above 'the sklearn way':\n"}
{"snippet": "pca = PCA()\npca.fit(X_train)\n", "intent": "Fit PCA to the training data\n"}
{"snippet": "comb_data['Electrical'] = comb_data['Electrical'].fillna(comb_data['Electrical'].mode()[0])\n", "intent": "This columns has maximum number as 'SBrk' and 'KitchenQual' resp, So we can replace value which only 1 using Mode.\n"}
{"snippet": "standardScaler = StandardScaler()\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=4444)\n", "intent": "Draw the learning curve for logistic regression in this case.\n"}
{"snippet": "def train_test_split(pos_emails, neg_emails, pos_labels, neg_labels, split_value):\n    X_train = np.concatenate((pos_emails[:split_value], \n                              neg_emails[:split_value]), axis = 0)\n    X_test = np.concatenate((pos_emails[split_value:],\n                             neg_emails[split_value:]), axis = 0)\n    y_train = np.concatenate((pos_labels[:split_value], \n                              neg_labels[:split_value]), axis = 0)\n    y_test = np.concatenate((pos_labels[split_value:],\n                             neg_labels[split_value:]), axis = 0)\n    return X_train, X_test, y_train, y_test\n", "intent": "- Create four new arrays from our email data and labels.\n"}
{"snippet": "test = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT5/master/data/vehicles_test.csv')\ntest['type'] = test.type.map({'car':0, 'truck':1})\ntest\n", "intent": "How good is scikit-learn's regression tree at predicting the price for test observations?\n"}
{"snippet": "Range = pd.date_range('05/28/2010', periods=194, freq='W')\nRange = pd.DataFrame(Range)\nprint ('The Date of the Significant Drop: ', Range.loc[126])\n", "intent": "- Guessing the time series start date - 05/28/2012 instead of 05/21/2012.\n- The range can also be created in such a way so as to reveal the data.\n"}
{"snippet": "power_series = pd.DataFrame()\nfor year in range(2013,2018):\n    power_data = pd.read_excel('Downloads/WindForecast_{0}-01-01_{0}-12-31.xls'.format(str(year)), header=3)\n    power_data = power_data.drop(columns=['Week-ahead Forecast [MW]',\n                          'Day-ahead forecast (11h00) [MW]',\n                          'Most recent forecast [MW]',\n                          'Active Decremental Bids [yes/no]'])\n    power_data['DateTime'] = pd.to_datetime(power_data['DateTime'].apply(lambda x: x[3:6]+x[0:3]+x[6:]))\n    power_data = power_data.set_index(['DateTime'])\n    power_series = power_series.append(power_data)\n", "intent": "Next step is to try an time-series forecasting approach. First algo is ARIMA\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('./some datasets/SMSSpamCollection',delimiter='\\t',header=None)\ndf.head()\n", "intent": "http://archive.ics.uci.edu/ml/machine-learning-databases/00228/\n"}
{"snippet": "def split_train_test_linear_regression_basic(df):\n    adj_close = df['Close'].tolist()\n    date      = df['Item'].tolist()\n    date = np.reshape(date, (len(date), 1))\n    adj_close = np.reshape(adj_close, (len(adj_close), 1))\n    X_train, X_test, y_train, y_test = train_test_split(date, adj_close, test_size=0.20, random_state=42)\n    return X_train, X_test, y_train, y_test\n", "intent": "   **Split the data**\n"}
{"snippet": "df = pd.read_csv(\"monthly-milk-production.csv\",index_col='Month')\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "X = pd.DataFrame(iris.data)\nX.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\ny = pd.DataFrame(iris.target)\ny.columns = ['Targets']\n", "intent": "Define the \"x\" and \"y\" variables. *Remember*, y is our classifer, and x is our attributes.\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\nprint(digits.keys())\n", "intent": "The datasets are loaded into a dictionary.\n"}
{"snippet": "dataset_wine = pd.read_csv(\"wine_dataset.csv\")\ndataset_red = pd.read_csv(\"red_wine_dataset.csv\")\ndataset_white = pd.read_csv(\"white_wine_dataset.csv\")\n", "intent": "wine_dataset is divided into red_wine_dataset and white_wine_dataset for analysis\n"}
{"snippet": "bank_attributes_train, bank_attributes_test, bank_labels_train, bank_labels_test = train_test_split(bank_attributes, bank_labels, stratify=bank_labels, train_size=0.7, test_size=0.3)\n", "intent": "Use the standard 70% / 30% split. Since this is a classification problem, be sure to stratify the split according to the `bank_labels`.\n"}
{"snippet": "esquerda = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\n                     'key2': ['K0', 'K1', 'K0', 'K1'],\n                        'A': ['A0', 'A1', 'A2', 'A3'],\n                        'B': ['B0', 'B1', 'B2', 'B3']})\ndireita = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n                               'key2': ['K0', 'K0', 'K0', 'K0'],\n                                  'C': ['C0', 'C1', 'C2', 'C3'],\n                                  'D': ['D0', 'D1', 'D2', 'D3']})\n", "intent": "**Diferentes tipos de merge (semelhantes aos do SQL JOIN)**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nparams={'C':[0.001,0.1,10.0,1.0,0.01]}\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.datasets import load_digits\nfrom sklearn.cross_validation import train_test_split\ndigits=load_digits()\nX_train,X_test,Y_train,Y_test=train_test_split(digits.data,digits.target)\ngrid=GridSearchCV(LogisticRegression(),param_grid=params)\ngrid.fit(X_train,Y_train)\ngrid.score(X_test,Y_test)\n", "intent": "Use LogisticRegression to classify digits, and grid-search the C parameter.\n"}
{"snippet": "import pandas as pd\ntrain_neg_df = pd.DataFrame({'data_train': data['train']['neg'], 'labels_train': labels['train']['neg']})\ntrain_pos_df = pd.DataFrame({'data_train': data['train']['pos'], 'labels_train': labels['train']['pos']})\ntrain_df = pd.concat([train_neg_df, train_pos_df], axis=0)\ntest_neg_df = pd.DataFrame({'data_test': data['test']['neg'], 'labels_test': labels['test']['neg']})\ntest_pos_df = pd.DataFrame({'data_test': data['test']['pos'], 'labels_test': labels['test']['pos']})\ntest_df = pd.concat([test_neg_df, test_pos_df], axis=0)\ntrain_df.head()\n", "intent": "Save loaded data into a Pandas DataFrame for easy read/write\n"}
{"snippet": "yelp_class = yelp[ (yelp['stars'] == 5) | (yelp['stars'] == 1) ]\nX = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\n_, itest = train_test_split(range(critics.shape[0]), train_size=0.7) \nmask = np.zeros(critics.shape[0], dtype=np.bool) \nmask[itest] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris.data,\n                                                    iris.target,\n                                                    train_size=0.75, test_size=0.25)\n", "intent": "Here we'll use 75% of the data for training, and test on the remaining 25%.\n"}
{"snippet": "votedf=pd.read_csv(votes_file)\nairportdf=pd.read_csv(airport_file)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "df = pd.read_csv('TS.csv')\nts = pd.Series(list(df['Sales']), index=pd.to_datetime(df['Month'],format='%Y-%m'))\n", "intent": "Let's predict sales data using ARIMA\n"}
{"snippet": "X_partno_onehot = enc_onehot.fit_transform(X_partno_labelencoded.reshape(-1, 1))\n", "intent": "scikit-learn api requires that our data is a 2-D array, so need to also perform a .reshape(-1, 1)\n"}
{"snippet": "df = pd.read_csv(\n    '~/2001.csv',\n    encoding='latin-1',\n    usecols=(13, 16)\n    )\n", "intent": "We use the `AirTime` column at the Willard airport.\n"}
{"snippet": "import pandas as pd\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'\ncol_names = ['id','ri','na','mg','al','si','k','ca','ba','fe','glass_type']\nglass = pd.read_csv(url, names=col_names, index_col='id')\nglass.sort('al', inplace=True)\nglass.head()\n", "intent": "- Using linear regression model\n"}
{"snippet": "from sklearn.utils import shuffle\nX_train, y_train = shuffle(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.02, random_state=0)\nprint(\"X_train shape\", X_train.shape)\nprint(\"X_test shape\", X_test.shape)\nprint(\"Y_valid shape\", X_validation.shape)\n", "intent": "Describe how you set up the training, validation and testing data for your model.\nOnce I have my data preprocess, next splitting the data into 80,20\n"}
{"snippet": "factor_exposures = pd.DataFrame(index=[\"factor 1\", \"factor 2\"], \n                                columns=portfolio_returns.columns,\n                                data = pca.components_).T\n", "intent": "The factor returns here are an analogue to the principal component matrix $\\mathbf{V}$ in the image processing example. \n"}
{"snippet": "X, y = (yelp_class['text'], yelp_class['stars'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "data = pd.read_csv('AirQualityUCI.csv', sep=';')\ndata.head()\n", "intent": "Try other data sets for regression\nYou can use pd.Categorical(df.variable name).codes if needed\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\nprint(y_train.shape)\nprint(y_test.shape)\n", "intent": "You will be asked to create a train/test split. You will be told the size to use for the test set. \n"}
{"snippet": "data = pd.read_csv('data/adult.csv', header=0, sep=', ', engine='python')\ndata.head()\n", "intent": "Adult dataset: https://archive.ics.uci.edu/ml/datasets/Adult\n"}
{"snippet": "print(bdata.keys())\nprint(bdata.feature_names)\nprint(bdata.DESCR)\nprint(bdata.feature_names)\nboston  = pd.DataFrame(data=bdata.data,columns=bdata.feature_names)\n", "intent": "The following commands will provide some basic information about the shape of the data:\n"}
{"snippet": "df3 = pd.DataFrame({'AAA' : [4,5,6,7], 'BBB' : [10,20,30,40],'CCC' : [100,50,-30,-50]})\ndf3\n", "intent": "Another silly example:\n"}
{"snippet": "def fit_pca(df, n_components):\n    pca = PCA(n_components=n_components)\n    reduced = pca.fit_transform(df)\n    return pca, reduced\n", "intent": "Write a function named fit_pca() that takes a pandas.DataFrame and uses sklearn.decomposition.PCA to fit a PCA model on all values of df.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nX_train, X_test, y_train, y_test = train_test_split(X0, y0, test_size=0.25, random_state=30)\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\n", "intent": "<h1>Build the model</h1>\n<p>Create train test splits for your data.  Use the training data to .</p>\n"}
{"snippet": "PATH_TO_DATA = ('../data')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntrain_df['time1'] = pd.to_datetime(train_df['time1'])\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\ntest_df['time1'] = pd.to_datetime(test_df['time1'])\n", "intent": "Reading original data\n"}
{"snippet": "df_train = pd.read_csv('../input/dog-breed-identification/labels.csv')\ndf_test = pd.read_csv('../input/dog-breed-identification/sample_submission.csv')\ndf_train.head(10)\n", "intent": "*Step 2: Describe Data*\n"}
{"snippet": "df_raw = pd.read_csv(f'{PATH}TrainAndValid.csv', low_memory=False, \n                     parse_dates=[\"saledate\"])\n", "intent": "*Question*\nWhat stands out to you from the above description?  What needs to be true of our training and validation sets?\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndataHitters = pd.read_csv('../../data/Hitters.csv', index_col = 0)\ndataHitters = dataHitters.dropna()\ndataHitters['Salary'] = np.log(dataHitters['Salary'])\ndataHitters.dtypes\n", "intent": "a) Remove the observations for whom the salary information is unknown, and then log-transform the salaries.\n"}
{"snippet": "def read_file(filename):\n    data = []\n    with open(filename, 'r') as f:\n        text = f.read().split('\\n')\n        for parag in text:\n            if len(parag) >2:\n                data = data + [a for a in parag.split('. ') if len(a)>0]\n    return data\n", "intent": "num: 547\nNote: There're 5013 texts within articles folder, so this dataframe only contains articles that are labeled as one of the 58 topics\n"}
{"snippet": "df = pd.read_csv('glass.data', names=['id','ri','Na','Mg','Al','Si','K','Ca','Ba','Fe','Type'])\n", "intent": "Let's start by reading in the adult.csv file into a pandas dataframe.\n"}
{"snippet": "from sklearn.manifold import TSNE\ntsne = TSNE(random_state=17)\nx_tsne = tsne.fit_transform(data_x_scaled)\n", "intent": "Visualize the data using t-SNE. See if tuning the perplexity parameter helps obtaining a\nbetter visualization.\n"}
{"snippet": "from sklearn import preprocessing\ndataset = oml.datasets.get_dataset(10)\nX, y, categorical = dataset.get_data(\n    target=dataset.default_target_attribute,\n    return_categorical_indicator=True)\nprint(\"Categorical features: %s\" % categorical)\nenc = preprocessing.OneHotEncoder(categorical_features=categorical)\nX = enc.fit_transform(X)\nclf.fit(X, y)\n", "intent": "You can also ask for meta-data to automatically preprocess the data\n- e.g. categorical features -> do feature encoding\n"}
{"snippet": "df=pd.read_csv(\"COGS108_IntroQuestionnaireData.csv\")\n", "intent": "Import datafile 'COGS108_IntroQuestionnaireData.csv' into a DataFrame called 'df'.\n"}
{"snippet": "datasets = datasets.load_iris()\n", "intent": "Now, let's create an object called *datasets* and load Iris dataset.\n"}
{"snippet": "titanic = pd.read_csv(\"../input/train.csv\")\ntitanic.head()\n", "intent": "Load train & test data\n======================\n"}
{"snippet": "X_new = pd.DataFrame({'TV': [data.TV.min(), data.TV.max()]})\nX_new.head()\nX_new2 = pd.DataFrame({'Radio': [data.Radio.min(), data.Radio.max()]})\nX_new2.head()\n", "intent": "Let's make predictions for the **smallest and largest observed values of x**, and then use the predicted values to plot the least squares line:\n"}
{"snippet": "X, y = sklearn.datasets.make_classification(\n    n_samples=10000, n_features=4, n_redundant=0, n_informative=2, \n    n_clusters_per_class=2, hypercube=False, random_state=0\n)\nX, Xt, y, yt = sklearn.cross_validation.train_test_split(X, y)\nind = np.random.permutation(X.shape[0])[:1000]\ndf = pd.DataFrame(X[ind])\n_ = pd.scatter_matrix(df, figsize=(9, 9), diagonal='kde', marker='o', s=40, alpha=.4, c=y[ind])\n", "intent": "Synthesize a dataset of 10,000 4-vectors for binary classification with 2 informative features and 2 noise features.\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.decomposition import PCA\niris = datasets.load_iris()\npca = PCA(n_components=2)\npca.fit(iris.data)\nX = pca.transform(iris.data)\n", "intent": "Can you run (and plot) a PCA on the Iris dataset?\n"}
{"snippet": "ultimate[\"avg_rating_by_driver\"]=ultimate[\"avg_rating_by_driver\"].fillna(-1) \nultimate[\"avg_rating_of_driver\"]=ultimate[\"avg_rating_of_driver\"].fillna(-1) \nbins = [-2, 0, 1.1, 2.1,  3.1, 4.1 ,5.1]\ngroup_names = ['None', '0-1', '1-2', '2-3', '3-4', '4-5']\nultimate['avg_rating_by_driver_grouped'] = pd.cut(ultimate[\"avg_rating_by_driver\"], bins, labels=group_names)\nultimate['avg_rating_of_driver_grouped'] = pd.cut(ultimate[\"avg_rating_of_driver\"], bins, labels=group_names)\n", "intent": "For both the ratings we will bin them into categories None ,0-1, 1-2, 2-3, 3-4, 4-5, where 0 would be the category with missing ratings.\n"}
{"snippet": "adult_train_standardize_all = (StandardScaler()\n                               .fit_transform(adult_train_standardize_all_df))\nadult_train_standardize_all_df = pd.DataFrame(adult_train_standardize_all,\n                                              columns=adult_train_standardize_all_df.columns)\n", "intent": "    StandardScaler().fit_transform()\n"}
{"snippet": "def write_to_submission_file(predicted_labels, out_file,\n                             target='target', index_label=\"session_id\"):\n    predicted_df = pd.DataFrame(predicted_labels,\n                                index = np.arange(1, predicted_labels.shape[0] + 1),\n                                columns=[target])\n    predicted_df.to_csv(out_file, index_label=index_label)\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "pipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('lr', LogisticRegression())\n]) \npipeline.fit(X_train, Y_train)\npipeline.score(x_test, y_test)\n", "intent": "Remove Tfidf also. Good / bad?\n"}
{"snippet": "def read_data():\n    return pd.read_csv('multiclassdata.csv').values\n", "intent": "Read in the data from the file `multiclassdata.csv` using pandas\n"}
{"snippet": "ames = pd.read_csv(\"https://www.openintro.org/stat/data/ames.csv\")\names.columns = [c.replace(\".\",\"\") for c in ames.columns]\n", "intent": "You can find a link to variable explanations [here](https://www.openintro.org/stat/data/?data=ames).\n"}
{"snippet": "countries = countries.fillna(countries.mean())\n", "intent": "Fill `NaN`s with the mean of that column.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train , X_test , y_train , y_test = train_test_split(X, y, test_size = 0.15, random_state = 0)\n", "intent": "Split our data into training and test sets so we could visualise the accuracy of the model by testing it on our test data\n"}
{"snippet": "train_no_dummy_workingday_piv = all_no_dummy_interpolate[all_no_dummy_interpolate['train'] == 1].pivot_table(values='count',index='hour',columns=['workingday'])\n", "intent": "** Let's cluster hour and workingday impact on rentals. **\n"}
{"snippet": "c = pd.DataFrame({'VarA':['aa','bb'], 'VarB':[20,30]}, index = ['Case1','Case2'])\nc = c.drop(['Case1'])\nc\n", "intent": "We can delete entire rows and columns:\n"}
{"snippet": "df['Apartment and Flat avg'] = df['Apartment and Flat avg'].fillna(df['Apartment and Flat avg'].mean())\ndf['Individual House avg'] = df['Individual House avg'].fillna(df['Individual House avg'].mean())\ndf['Plot avg'] = df['Plot avg'].fillna(df['Plot avg'].mean())\ndf['Avg rate per sq ft'] = df['Avg rate per sq ft'].fillna(df['Avg rate per sq ft'].mean())\n", "intent": "**Data cleaning on featured variables :**\n"}
{"snippet": "tfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nX_train_tfidf.shape\n", "intent": "Faster, do both at once! \n"}
{"snippet": "diabetes_train, diabetes_test = train_test_split(diabetes_full, test_size=1/3)\ndiabetes_train = diabetes_train.copy()\ndiabetes_test = diabetes_test.copy()\n", "intent": "Before we begin any EDA, we better hold out our final test set.  Otherwise we will be implicitly learning about our test set through EDA.\n"}
{"snippet": "lr_biz_titles_pr = LogisticRegression(penalty='l1', C=lr_biz_titles_pr_gs.best_params_['C'],\n                                      solver='liblinear')\nlr_biz_titles_pr.fit(title_X, business)\ncoefs = pd.DataFrame({'coefs':lr_biz_titles_pr.coef_[0],\n                      'variable':title_cols})\n", "intent": "Print out the top 20 or 25 word features as ranked by their coefficients.\n---\n"}
{"snippet": "test_df = pd.read_csv(\"./proj2_test_data.csv\")\ntest_df['tpep_pickup_datetime'] = pd.to_datetime(test_df['tpep_pickup_datetime'])\ntest_df.head()\n", "intent": "Here we load our testing data on which we will evaluate your model.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                            test_size = 0.2, random_state=0)\n", "intent": " Split the features and the target into a Train and a Test subsets.\n Ratio should be 80/20\n"}
{"snippet": "triplet_dataset = pd.read_csv(filepath_or_buffer=data_home+'train_triplets.txt', \n                              sep='\\t', header=None, \n                              names=['user','song','play_count'])\n", "intent": "reading user, song, play_count data\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom patsy import dmatrix\nfeatures_df = dfrttf[['CA','MDW','NE','PNW','SE','SW','TX','Junior','Senior']]\ntarget_df = dfrttf['high_salary']\nX_train, X_test, Y_train, Y_test = train_test_split(features_df, target_df, \n                                                    test_size=0.33, random_state=5)\nscaler = StandardScaler() \n", "intent": "- You can either create the dummy features manually or use the `dmatrix` function from `patsy`\n- Remember to scale the feature variables as well!\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "K-nearest neighbors (KNN) classification\n"}
{"snippet": "masked_epi = masker.inverse_transform(samples)\n", "intent": "To recover the original data shape (giving us a masked and z-scored BOLD series), we simply use the masker's inverse transform:\n"}
{"snippet": "feature_importances = pd.DataFrame(rf.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\n", "intent": "The model correctly identifies 62% of all expired loans as \"not funded\"\n"}
{"snippet": "import pandas as pd\nreviews = pd.read_csv('../data/en_reviews.csv', sep='\\t', header=None, names =['rating', 'text'])\nreviews = reviews['text'].tolist()\nprint(reviews[:2])\n", "intent": "Implement a text generator, which imitates reviewers of a travel agency using the LSTM language model.\n"}
{"snippet": "(train_data, train_label),(test_data, test_label) = reuters.load_data(num_words=10000)   \n", "intent": "**Load the data into variable**\n"}
{"snippet": "corpus_set = set(training_curpos.word)\ncount_vect = CountVectorizer(vocabulary = corpus_set, tokenizer=nltk.word_tokenize)\nbow_counts = count_vect.fit_transform(df.pureTextTweet)\nbow_counts.shape\n", "intent": "Bag-of-words where with occurences count:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=0.70, random_state=0)\n", "intent": "Do a train/test split, with 70% of the data used for training and a `random_state=0`:\n"}
{"snippet": "pd.DataFrame({\n    \"SK_ID_CURR\": test_id,\n    \"TARGET\": predictions\n}).to_csv(\"../submissions/same_filled_nan.csv\", index=False)\n", "intent": "---\n<a id=\"save\"></a>\n"}
{"snippet": "DATA_PATH = \"../data/home_default/\"\nbureau   = pd.read_csv(DATA_PATH + \"bureau.csv\")\nprev_app = pd.read_csv(DATA_PATH + \"previous_application.csv\")\nprint(\"Shape of bureau:\",    bureau.shape)\nprint(\"Shape of prev_app:\",  prev_app.shape)\n", "intent": "---\n<a id=\"load\"></a>\n"}
{"snippet": "DATA_PATH = \"../../data/home_default/\"\ntrain = pd.read_csv(DATA_PATH + \"train.csv\")\ntest  = pd.read_csv(DATA_PATH + \"test.csv\")\ntest['is_train'] = 0\ntrain['is_train'] = 1\nprint(\"Shape of train:\", train.shape)\nprint(\"Shape of test:\",  test.shape)\n", "intent": "---\n<a id=\"load\"></a>\n"}
{"snippet": "for dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n", "intent": "- This rubs me the wrong way though, I think another column should definitely be added\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.30, random_state=30)\nmodel2 = LogisticRegression()\nmodel2.fit(X_train, y_train)\n", "intent": "- You can either create the dummy features manually or use the `dmatrix` function from `patsy`\n- Remember to scale the feature variables as well!\n"}
{"snippet": "sacramento = pandas.read_csv('../../data/Sacramento-real-estate-transactions.csv')\n", "intent": "We will predict prices based on:\n* Bedrooms\n* Bathrooms\n* Other stuff\n"}
{"snippet": "df = pd.read_csv('./data/auto-mpg.csv')\n", "intent": "Categorical features\n"}
{"snippet": "data = datasets.load_boston()\nprint(data.DESCR) \n", "intent": "**Load the boston housing data with the `datasets.load_boston()` function.**\n"}
{"snippet": "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\ndata.head()\ndata.corr()\n", "intent": "Let's take a look at some data, ask some questions about that data, and then use linear regression to answer those questions!\n"}
{"snippet": "df = pd.read_csv('./dow_jones_index.data')\ndf.head()\n", "intent": "The problems will use data from the down jones index.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[np.nan, 2, np.nan, 0], [3, 4, np.nan, 1], [np.nan, np.nan, np.nan, 5]], columns=list('ABCD'))\ndf.head()\n", "intent": "from https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html\n"}
{"snippet": "import pandas as pd\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nbaseline_submission = pd.read_csv(\"baseline_submission.tsv\")\nmy_submission = pd.read_csv(\"./my_submission1.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "iris = pd.read_csv(\"./assets/datasets/iris.csv\")\niris.head(n=5)\n", "intent": "We're going to be using **Scikit-Learn** for our analysis; let's load in the [Iris](./assets/datasets/iris.csv) dataset using Pandas\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=123) \n", "intent": "> Splitting the data set into a training and a test set\n"}
{"snippet": "import pandas as pd\ndata311=pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/Cluster_lecture/cluster.csv\")\n", "intent": "Please carefully read 2.3.9:\nhttp://scikit-learn.org/stable/modules/clustering.html\n"}
{"snippet": "longitudes = [point[0] for point in centermost_points]\nlatitudes = [point[1] for point in centermost_points]\nrep_points = pd.DataFrame({'lon':longitudes, 'lat':latitudes})\n", "intent": "Assemble all center-most points into a new series\n"}
{"snippet": "from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nrfe = RFE(model, 3)\nrfe = rfe.fit(tf_train, target)\nprint(rfe.support_)wejtiaet\nprint(rfe.ranking_)\n", "intent": "To get a better idea of what our features are doing, we use scikit's feature importance library\n"}
{"snippet": "messages = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t', names=['label', 'message'])\n", "intent": "Notice the `\\t` in the string shown above. This means we have tab-separated values.\n"}
{"snippet": "X = pd.DataFrame(scaled_features, columns=X.columns)\nX.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "def fit_pca(df, n_components):\n    pca = PCA(n_components = n_components)\n    pca.fit(df)\n    reduced = pca.fit_transform(df)\n    return pca, reduced\n", "intent": "Write a function named fit_pca() that takes a pandas.DataFrame and uses sklearn.decomposition.PCA to fit a PCA model on all values of df.\n"}
{"snippet": "def analyze_wav(filename):\n    s, fs, enc = ad.wavread(filename)  \n    s = np.array(s)\n    s = s / abs(s).max() \n    left = s[:,0]\n    right = s[:,1]\n    return left, right, fs\n", "intent": "**1. Looking at WAV File Composition**\nFirst, we explore a wav file to see what the data looks like\n"}
{"snippet": "df = pd.read_csv('/Users/rulanxiao/Documents/GitHub/APMAE4990-/data/hw2data.csv')\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "ccblc = pd.read_csv('./data/input/credit_card_balance.csv')\nccblc = ccblc.drop(['SK_ID_PREV', 'MONTHS_BALANCE'], axis=1)\nccblc = summary_extra_data(ccblc, 'ccblc_')\nfull_df = full_df.join(ccblc, on='SK_ID_CURR')\nadd_features(ccblc.columns.tolist())\n", "intent": "**credit_card_balance.csv**\n"}
{"snippet": "scaler = StandardScaler()\ncustomer_sc = scaler.fit_transform(customer_features)\ncustomer_sc_df = pd.DataFrame(customer_sc, columns=customer_features.columns)\nsc_stats = customer_sc_df.describe().T\nsc_stats['skew'] = st.skew(customer_sc_df)\nsc_stats['kurt'] = st.kurtosis(customer_sc_df)\ndisplay(stats)\ndisplay(sc_stats)\n", "intent": "$$Z = \\frac{X-\\mu}{\\sigma}$$\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = pd.read_csv('../datasets/train.csv')\ndata.set_index('Date', inplace=True)\ndata.head()\n", "intent": "- Assemble observations and graphs as well as timeseries models in a notebook.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_data, test_data = train_test_split(df, test_size=0.25, random_state= 42)\n", "intent": "Now that the data is loaded, we proceed to splitting it into a training set and a testing set.\n"}
{"snippet": "path = \"../data/NLP_data/yelp.csv\"\nyelp = pd.read_csv(path, encoding='unicode-escape')\nyelp.head()\n", "intent": "Let's analyze the sentiment of yelp reviews\n"}
{"snippet": "reviews = pd.read_csv(music_fname, sep='\\t')\nreviews.head()\n", "intent": "Our first attempt at reading in the csv file failed. Why?\n"}
{"snippet": "video = io.open('run4.mp4', 'r+b').read()\nencoded = base64.b64encode(video)\n", "intent": "The model without maxpooling is seen to perform as in the video (run-4) below.\n"}
{"snippet": "import pandas as pd\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nreviews  = pd.read_csv('./data/reviews.csv')\ntrain, test = train_test_split(reviews, test_size=0.2, random_state=4622)\nX_train = train['reviews'].values\nX_test = test['reviews'].values\ny_train = train['sentiment']\ny_test = test['sentiment']\n", "intent": "Following is a dataset containing reviews and sentiments associated with it.\nCreate a SVM Classifier to predict positive or negative sentiments\n"}
{"snippet": "iris = load_iris()\nX_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\nclf = svm.SVC(kernel='linear', C=1).fit(X_train, Y_train)\nclf_poly = svm.SVC(kernel='poly', C=1, degree=2).fit(X_train, Y_train)\nclf.score(X_test, Y_test)\nclf_poly.score(X_test, Y_test)\n", "intent": "We can use the `train_test_split` for testing the SVM Algorithm\n"}
{"snippet": "from sklearn.datasets.samples_generator import make_blobs\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5, random_state=0)\n", "intent": "* http://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html\n"}
{"snippet": "stores_data = pd.read_csv('store.csv')  \nstores_data.head()\n", "intent": "the week aggregation is defined by the aggregation of observations starting from the week before the displayed week until the displayed week\n"}
{"snippet": "w=98\ncols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\ny_pred_test_mix=(w/100)*y_pred_test_nw2+y_pred_test_nw*((100-w)/100)\npd.DataFrame(y_pred_test_mix, index=dataTesting.index, columns=cols).to_csv('Final_pred_NW_MIX2.csv', index_label='ID')\n", "intent": "Export results Neural Network model for mixed model Images Transfer Learning VGG16+Text. Text model weight=0.84\n"}
{"snippet": "from sklearn.datasets import load_boston\ndataset = load_boston()\nprint(dataset.DESCR)\n", "intent": "Today, we'll apply linear regression to real-life data!\n"}
{"snippet": "features = X.columns\nfeature_importances = rf.feature_importances_\nfeatures_df = pd.DataFrame({'Features': features, 'Importance Score': feature_importances})\nfeatures_df.sort_values('Importance Score', inplace=True, ascending=False)\nfeatures_df.head(15)\n", "intent": "1. satisfaction_level \n2. time_spend_company\n3. number_project\n4. average_monthly_hours\n5. last_evaluation\n"}
{"snippet": "College_Data = pd.read_csv('College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "data  = pd.read_csv('./dataset/dataset_1_full.txt')\n", "intent": "*** 1-b-1: Load the contents of dataset_1_full.txt into a pandas dataframe, or numpy array. ***\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "def tokenize(text): \n    tknzr = TweetTokenizer()\n    return tknzr.tokenize(text)\nen_stopwords = set(stopwords.words(\"english\")) \ncount_vect = CountVectorizer(tokenizer=tokenize, stop_words=en_stopwords, min_df=5)\nskf = StratifiedKFold(n_splits=5)\ncv = skf.get_n_splits(X_train, y_train)\n", "intent": "**PART A**\nUse CountVectorizer to vectorize reviews as dictionary of term frequencies.\nDefine the crossvalidation split using StratifiedKFold.\n"}
{"snippet": "with open('ultimate_data_challenge.json') as f:\n    ultimate = json.load(f)\nultimate = pd.DataFrame(ultimate)\nultimate.signup_date = pd.to_datetime(ultimate.signup_date)\nultimate.last_trip_date = pd.to_datetime(ultimate.last_trip_date)\nultimate.head()\n", "intent": "We start of course by loading and then briefly exploring the data. \n"}
{"snippet": "x_pca = pd.DataFrame(x_pca, columns=['PC1', 'PC2', 'PC3', 'PC4'])\nx_pca.head()\n", "intent": "Calculate the correlation matrix between the principal components and the original features:\n"}
{"snippet": "y = pd.DataFrame(data.target)\nX = pd.DataFrame(data.data, columns = data.feature_names)\n", "intent": "Define the \"x\" and \"y\" variables. *Remember*, y is our classifer, and x is our attributes.\n"}
{"snippet": "X,y=make_blobs(n_samples=200, centers=2, n_features=2,\n               random_state=0,cluster_std=.5)\nsv.fit(X,y)\n", "intent": "We can create some synthetic data using `make_blobs` and then\nfit it to the SVM,\n"}
{"snippet": "users = pd.read_csv('takehome_users.csv')\nusers.info()\n", "intent": "We can read in the users file.\n"}
{"snippet": "colspecs = [(0, 11), (11, 21), (21, 31), (31, 38),(39,41),(41,72),(72,76),(76,80),(80,86)]\nstations = pd.read_fwf('/home/ubuntu/UCSD_BigData/data/weather/ghcnd-stations_buffered.txt', colspecs=colspecs, header=None,\n                       names=['station','latitude','longitude','elevation','state','name','GSNFLAG','HCNFLAG','WMOID'])\nstations = stations.ix[:,['station','longitude','latitude']]\nweights = pd.read_fwf('station_weights2',index_col=0, names=['weight'])\nstationJoin=stations.join(weights,on='station')\nstationJoin.dropna(axis=0)\nprint shape(stationJoin)\nstationJoin.head()\n", "intent": "This step prepare the source data for geographic partitiong in the next step. \n"}
{"snippet": "f = '/home/szong/bin/GISTIC_2_0_22/124_patients_20180323/hg19_cytoband_ucsc.txt'\ndf = pd.read_csv(f, header=None, sep='\\t')\ndf.columns = ['chr', 'start', 'end', 'cytoband', 'comments']\ndf.head()\n", "intent": "clean up copy number calls\n"}
{"snippet": "from sklearn.datasets import load_iris\nevaluate_features(*load_iris(True))\n", "intent": "Let's do a quick test of evaluate_features\n"}
{"snippet": "of = '/projects/trans_scratch/validations/workspace/szong/Cervical/hotspots/123_patients/mut_rate.txt'\nmut_rate = pd.read_csv(of, sep='\\t')\nmut_rate['mut_rate_per_base'] = mut_rate.mut_rate/1e6\nmut_rate.head(2)\n", "intent": "figure out mutation rate for the whole genome mu=expected mutation rate at each position\n"}
{"snippet": "df = pd.read_csv('./breast-cancer-wisconsin.data')\ndf.drop('id', axis=1, inplace=True)\ndf.head()\n", "intent": "Now let's switch gears and load in the familiar breast cancer data\n"}
{"snippet": "from sklearn.model_selection import train_test_split \nX_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, test_size=0.4, random_state=1734)\n", "intent": "**Part A**: Construct a K-Nearest Neighbors classifier to make predictions on the data. \n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size=0.2, random_state=0)\n", "intent": "<font color='red'>TASK MARK: 1</font>\n<br>\n<font color='green'>COMMENT:  - </font>\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nstandardizer = StandardScaler()\nstandardizer.fit(X)\nXst = standardizer.transform(X)\n", "intent": "In the following we standardize this dataset for numerical stability.  Note that due to the exponents extreme values can be an issue.\n"}
{"snippet": "mnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n", "intent": "Load and prepare the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. Convert the samples from integers to floating-point numbers:\n"}
{"snippet": "from sklearn.datasets import make_regression\nfrom sklearn.cross_validation import train_test_split\nX, y, true_coefficient = make_regression(n_samples=80, n_features=30, n_informative=10, noise=100, coef=True, random_state=5)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)\nprint(X_train.shape)\nprint(y_train.shape)\n", "intent": "```\ny_pred = x_test[0] * coef_[0] + ... + x_test[n_features-1] * coef_[n_features-1] + intercept_\n```\n"}
{"snippet": "from sklearn.kernel_approximation import RBFSampler\nsgd = SGDClassifier()\nkernel_approximation = RBFSampler(gamma=.001, n_components=400)\nfor i in range(9):\n    X_batch, y_batch = cPickle.load(open(\"data/batch_%02d.pickle\" % i))\n    if i == 0:\n        kernel_approximation.fit(X_batch)\n    X_transformed = kernel_approximation.transform(X_batch)\n    sgd.partial_fit(X_transformed, y_batch, classes=range(10))\n", "intent": "Kernel Approximations\n=======================\n"}
{"snippet": "url = '../assets/dataset/bikeshare/bikeshare.csv'\nbikes = pd.read_csv(url, parse_dates=True)\n", "intent": "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n"}
{"snippet": "X1train, X1test, y1train, y1test = train_test_split(X1,y1, test_size=0.3, random_state = 7)\nX2train, X2test, y2train, y2test = train_test_split(X2,y2, test_size=0.3, random_state = 7)\n", "intent": "Generate train test variables for both word sets\n"}
{"snippet": "def movie_parser(m):\n    return [m['num_votes'], m['rating'], m['tconst'], m['title'], m['year']]\nmovies = pd.DataFrame(np.array([movie_parser(m) for m in top]),\n                      columns=['num_votes','rating','tconst','title','year'])\n", "intent": "Keep the fields:\n    num_votes\n    rating\n    tconst\n    title\n    year\nAnd discard the rest\n"}
{"snippet": "data = pd.read_csv(\"http://www.ds100.org/sp18/assets/datasets/hw5_data.csv\", index_col=0)\ndata.head()\n", "intent": "Load the data.csv file into a pandas dataframe.  \nNote that we are reading the data directly from the URL address.\n"}
{"snippet": "print X.columns\npd.DataFrame(model.feature_importances_,index=X.columns)\n", "intent": "The precision of acc is 105/sum(105,1,3,3)\nthe recall of acc is 105/sum(105,4,6,3)\n"}
{"snippet": "project_unit = pd.read_csv('project_unit.csv', delimiter = ';')\nproject_unit.head(3)\n", "intent": "Let's import data into a dataframe with pandas.\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\nbreast_cancer = load_breast_cancer()\nX = breast_cancer['data']\ny = breast_cancer['target']\n", "intent": "The dataset can be loaded with *sklearn.datasets.load_breast_cancer*.\n"}
{"snippet": "yelp = pd.read_csv('yelp.csv')\nyelp.head(3)\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nknn_pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\nknn_params = {'knn__n_neighbors': range(1, 10)}\nknn_grid = GridSearchCV(knn_pipe, knn_params, cv=5, n_jobs=-1, verbose=True)\nknn_grid.fit(X_train, y_train)\nknn_grid.best_params_, knn_grid.best_score_\n", "intent": "Now, let's tune the number of neighbors $k$ for k-NN:\n"}
{"snippet": "df = pd.read_csv('health_data.txt', delimiter='\\t')\nprint(len(df), df.columns)\ndf.head()\n", "intent": "As usual start by importing our dataset:\n"}
{"snippet": "sac = pd.read_csv('/Users/smoot/Downloads/Sacramentorealestatetransactions.csv')\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "test_file = data_path + r\"\\test.json\"\nX_test2 = pd.read_json(test_file)\nprint(\"Test dataset loaded\")\ntype(X_test2)\n", "intent": "By using best fit parameter we improved the model by 1%\n"}
{"snippet": "df_fea = pd.DataFrame(scaler_fea,columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "features = pd.concat([categorical_data, data[['size']]], axis=1)\nlabels = data.review\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=10)\n", "intent": "Let us create the training and testing set to be used with our model.\n"}
{"snippet": "enc = OneHotEncoder()\ntransformed = enc.fit_transform(selected[key_attributes].apply(lambda x: x.cat.codes))\nN, D = transformed.shape\nN, D\n", "intent": "Same as before, we have three columns of categorical data. We now convert them into one-hot encoding.\n"}
{"snippet": "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2)\nX += np.random.random(X.shape)\nsets = [make_moons(noise=0.1), make_circles(noise=0.1, factor=0.5), (X, y)]\n", "intent": "<h1 align=\"center\">Warm Up: 3 datasets</h1> \n"}
{"snippet": "from sklearn import datasets\npoints, labels = datasets.make_moons(n_samples=100, noise=.05)\n", "intent": "Aside from issues with choosing the number of groups *a priori*, K-means has trouble identifying groups that are not spherical and convex.\n"}
{"snippet": "print(pd.DataFrame({'effect': params.round(0),\n                    'error': err.round(0)}))\n", "intent": "- With these errors estimated, let's again look at the results:\n"}
{"snippet": "scalerSVM = StandardScaler()\n", "intent": "* Scale Data for SVM:\n"}
{"snippet": "X_train, X_test, y_train, y_test = \\\n    train_test_split(yelp_class['text'],\n                     yelp_class['stars'],\n                     test_size=0.3,\n                     random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "tweets = pd.read_csv('/Users/macbook/GA-DSI/projects/projects-capstone/captured-tweets.txt', sep='\\n', header = None, error_bad_lines=False)\n", "intent": "```It also took forever to get the tweets to load without errors```\n"}
{"snippet": "imputer = preprocessing.Imputer(missing_values='NaN', strategy = 'mean', axis = 0)\nimputer.fit(raw_data.loc[:, idx_cols_missing])\nraw_data.loc[:, idx_cols_missing] = imputer.transform(raw_data.loc[:, idx_cols_missing])\n", "intent": "As there are only 8 missing values in InteractionTime, Therefore, the way of dealing with missing values won't affect prediction results too much \n"}
{"snippet": "assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\ntrain_features, valid_features, train_labels, valid_labels = train_test_split(\n    train_features,\n    train_labels,\n    test_size=0.05,\n    random_state=832289)\nprint('Training features and labels randomized and split.')\n", "intent": "** Split Training and Validation Sets:**\n"}
{"snippet": "df = pd.read_csv('/home/data_scientist/data/misc/sim.data')\ndf.head()\n", "intent": "The cell below reads in a simulated dataset where y is an unknown function of a, b, and c.\n"}
{"snippet": "data = pd.read_csv(\"Advertising.csv\")\n", "intent": "Load Advertising data set\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'train_20news')\nnews_A = pd.read_csv(data_path + '_partA.csv', delimiter = ',')\nnews_B = pd.read_csv(data_path + '_partB.csv', delimiter = ',')\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "url = './assets/dataset/bikeshare.csv'\nbikes = pd.read_csv(url, index_col='datetime', parse_dates=True)\n", "intent": "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n"}
{"snippet": "df = pd.DataFrame(dict(fpr=fpr_log, tpr=tpr_log, thr = thr_log))\n", "intent": "* Convertimos los valores en un objeto dataframe y graficamos la curva ROC\n"}
{"snippet": "import random\nimport numpy as np\nimport pandas as pd\nX = pd.DataFrame([x for x in range(0,15)])\nY = pd.Series([2*(1 + random.random()) + 5*x*(1+random.random()) for x in X])\nX\n", "intent": "queremos hacer una regresion lineal\ngenero una variable X de 0 a 15, y una variable Y de la forma Y = 2 + 5x con ruido\n"}
{"snippet": "ica = repres.FastICA(n_components=3)\nres = ica.fit_transform(df[feat_cols].values)\ndf['ICA-one'] = res[:,0]\ndf['ICA-two'] = res[:,1] \ndf['ICA-three'] = res[:,2]\nsns.pairplot(df, hue=\"label\", x_vars=[\"ICA-one\",\"ICA-two\"], y_vars=[\"ICA-two\",\"ICA-three\"], plot_kws={\"alpha\": 0.2},  size=4,)\n", "intent": "<h2>What would the representation of Independent Component Analysis look like?</h3>\n"}
{"snippet": "allbeers = pd.read_csv('beer_reviews.tar.gz', compression='gzip')\n", "intent": "Import data in a pandas dataframe called \"allbeers\". Use the compression keyword\n"}
{"snippet": "needs_pivot_risk = pd.pivot_table(needs.join(profile['AT-RISK']), index='AT-RISK')\nneeds_pivot_risk.head()\n", "intent": "Let's start by visually inspecting needs for those designated at-risk versus their well counterparts.\n"}
{"snippet": "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\nnsamples, width, height = X_train.shape\nnfeatures = width * height\nX_train = X_train/255\nX_test = X_test/255\n", "intent": "We use Keras' datasets module to load the data:\n"}
{"snippet": "train_data, test_data = train_test_split(df, test_size=0.2, random_state=0)\nX_train = train_data.drop('y', axis=1)\nX_test = test_data.drop('y', axis=1)\ny_train = train_data['y']\ny_test = test_data['y']\n", "intent": "Creating a 80/20 split for training and testing respectively. These variables will be used for Problem 4.2 & 4.3.\n"}
{"snippet": "size = 1000\nX_moons, y_moons = make_moons(size, noise=0.05, random_state=42)\n", "intent": "As input, moons data with 1000 points, a noise of 0.05 and a fixed random state to have a repetable model\n"}
{"snippet": "scaled_features = scaler.fit_transform(bank_note.drop('Class', axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    paribas_numeric.drop(labels=['target', 'ID'], axis=1),\n    paribas_numeric['target'],\n    test_size=0.3,\n    random_state=0\n)\nX_train.shape, X_test.shape\n", "intent": "Before doing anything, let's first split the dataset and examine only the training subset.\n"}
{"snippet": "results = pd.DataFrame(index=['k='+ str(i) for i in range(1, 11)], columns=['mean_accuracy'])\n", "intent": "Using the univariate selection method SelectKBest, which is the value of k that maximizes the accuracy of the model\n"}
{"snippet": "data = pd.read_csv('training.csv')\n", "intent": "- First step is to load the csv data into a data frame\n"}
{"snippet": "custdata14 = pd.read_csv('C:\\LoanStats14.csv',low_memory=False)\n", "intent": "We are going to be combining 2014 and 2015 dataset in one dataframe and call it \"custdata\"\n"}
{"snippet": "from sklearn.datasets import load_boston\ndataset = load_boston()\nprint(dataset.keys())\n", "intent": "<h3>  Problem \nIn this part, you should download and analyze **\"Boston House Prices\"** dataset. <br>\nUse a code below to download the  dataset: \n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconmat = np.array(confusion_matrix(Y_test, Y_pred))\nconfusion = pd.DataFrame(conmat, index=['is_benign', 'is_malignant'],columns=['predicted_benign', 'predicted_malignant'])\nconfusion\n", "intent": "Look at the confusion matrix\n"}
{"snippet": "cvec = CountVectorizer(binary=True, ngram_range=(1,3), stop_words='english', max_features=5000)\nX = cvec.fit_transform(rt[\"quote\"])\nX.shape\n", "intent": "It is up to you what ngram range you want to select. **Make sure that `binary=True`**\n"}
{"snippet": "sim_data = pd.read_csv('/home/data_scientist/data/misc/sim.bNA.data')\nsim_data.head()\n", "intent": "In the cell below a simulated dataset is read in, where y (labels) is a unknown function of a, b, and c (your features).\n"}
{"snippet": "data_clus=pd.concat([data_geog.reset_index().drop('index',axis=1),pd.DataFrame(final_kmeans.labels_,columns=['Cluster'])],axis=1)\n", "intent": "Now to make the new dataset with the cluster data\n"}
{"snippet": "sales.to_csv('sales.csv', index=False)\n", "intent": "Note - this submission (submission17) scored 0.95281 on the public leaderboard (3/31/2018)\n"}
{"snippet": "month_wise_delay=flights_df.groupby(['month'])['dep_delay'].mean()\nmonth_wise_delay = pd.DataFrame(month_wise_delay)\nmonth_wise_delay.sort_values(ascending=False,by='dep_delay')\n", "intent": "(c) Are there any seasonal patterns in departure delays for flights from NYC?\n"}
{"snippet": "df = pd.read_csv(\"seeds_dataset.csv\",names=[\"area\", \"perimeter\", \"compactness\",\"length of kernel\",\"width of kernel\",\"asymmetry coefficient\",\"length of kernel groove\",\"target\"] ) \ndf\n", "intent": "For context of the data, see the documentation here: https://archive.ics.uci.edu/ml/datasets/seeds\n"}
{"snippet": "def make_features(train_set, test_set, degrees):\n    traintestlist=[]\n    for d in degrees:\n        traintestdict={}\n        traintestdict['train'] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n        traintestdict['test'] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n        traintestlist.append(traintestdict)\n    return traintestlist\n", "intent": "Lets put this alltogether. Below we write a function to create multiple datasets, one for each polynomial degree:\n"}
{"snippet": "bonds['market_issue'] = bonds['market_issue'].fillna('Global')\ncounts = bonds['market_issue'].value_counts()\nuncommon = counts[counts < 60]\nbonds['market_issue'] = bonds['market_issue'].apply(lambda x: 'Rare' if x in uncommon.index else x)\n", "intent": "Clean market issue.\n"}
{"snippet": "from sklearn.datasets import fetch_california_housing\nhousing = fetch_california_housing()\nX = housing[\"data\"]\ny = housing[\"target\"]\n", "intent": "Let's load the dataset using Scikit-Learn's `fetch_california_housing()` function:\n"}
{"snippet": "print(\"Accuracy: \", imdb_model.evaluate(x_test_proc, y_test)[1])\n", "intent": "After training we can evaluate our model on the test set.\n"}
{"snippet": "predTree = drugTree.predict(X_testset)\n", "intent": "Let's make some <b>predictions</b> on the testing dataset and store it into a variable called <b>predTree</b>.\n"}
{"snippet": "print(classification_report(y_test, grid_predictions))\n", "intent": "NOW that's something\n"}
{"snippet": "import pylab \nimport scipy.stats as stats\nresids = X-model._raw_predict(ts.reshape(-1,1))[0][:,0]\nstats.probplot(resids, dist=\"norm\", plot=pylab)\npylab.show()\n", "intent": "After some tuning of parameteres I chose to fix noise variance. As result we get normal residuals.\n"}
{"snippet": "SGD_score_on_training = metrics.accuracy_score(y_train_1, SGD_prediction_on_training)\nSGD_precision_on_training = metrics.precision_score(y_train_1, SGD_prediction_on_training, pos_label=\"REAL\")\nSGD_recall_on_training = metrics.recall_score(y_train_1, SGD_prediction_on_training, pos_label=\"REAL\")\nSGD_f1_on_training = metrics.f1_score(y_train_1, SGD_prediction_on_training, pos_label=\"REAL\")\nprint(\"accuracy for SGD on training dataset1:   %0.3f\" % SGD_score_on_training)\nprint(\"precision for SGD on training dataset1:   %0.3f\" % SGD_precision_on_training)\nprint(\"recall for SGD on training dataset1:   %0.3f\" % SGD_recall_on_training)\nprint(\"f1 for SGD on training dataset1:   %0.3f\" % SGD_f1_on_training)\n", "intent": "**Prediction of Labels(Fake/Real)**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_predicted = net.forward(X_test)\ny_predicted = np.argmax(y_predicted, axis=1)\naccuracy_score(y_predicted, Y_test)\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "print(np.sum((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint(\"Test accuracy: {:.2f}\".format(test_accuracy) + \"%\")\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "clf, y_pred = fit_and_predict(X_train_scaled, y_train, X_test_scaled, 'rbf')\n", "intent": "Run the following cell to actually train on the flights data. It might take a while.\n"}
{"snippet": "print(classification_report(predicted_y,y_test))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nmean_absolute_error(ytest, rf1.predict(Xtest))\n", "intent": "Since our response is very skewed, we may want to suppress outliers by using the `mean_absolute_error` instead. \n"}
{"snippet": "model.predict(X[0:2,:])\n", "intent": "Let's predict probabilities for the first two examples.\n"}
{"snippet": "r_predictions = rdtree.predict(X_test)\n", "intent": "Vamos prever os valores do y_test e avaliar o nosso modelo.\n** Preveja a classe de not.fully.paid para os dados X_test. **\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "Create new predictions based on our model, using X_test\n"}
{"snippet": "print(\"precesion for BOW with alpha 0.001 is\",precision_score(test_label_y, predicted, average=\"macro\"))\nprint(\"Recall for BOW with alpha 0.001 is\",recall_score(test_label_y, predicted, average=\"macro\"))  \n", "intent": "<b>f1 score for bag of words is 0.923</b>\n"}
{"snippet": "model.predict(input_test)                \n", "intent": "We can then use our model to predict the outputs to the test set. Again this is the same syntax no matter what the model.\n"}
{"snippet": "theta_min = np.matrix(result2[0])\npredictions = predict(theta_min, X2)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y2)]\naccuracy = (sum(map(int, correct)) % len(correct))\nprint \"accuracy = {0}%\".format(accuracy)\n", "intent": "Finally, we can use the prediction function from part 1 to see how accurate our solution is on the training data.\n"}
{"snippet": "medae = metrics.median_absolute_error(y_test, predictions)\nd_medae = metrics.median_absolute_error(y_test, d_predictions)\nprint \"MedAE / LR = {0:.4}, Dummy = {1:0.4}\".format(medae, d_medae)\n", "intent": "$$ MedAE(y, \\hat y) = median( | y_1 - \\hat y_1 |, ..., | y_n - \\hat y_n | ) $$\n"}
{"snippet": "print(accuracy_score(y_test, y_pred))\n", "intent": "Let's see what the accuracy score is.\n"}
{"snippet": "y_pred = nb.predict(X_test)\nf1_score(y_test, y_pred, average='macro')\n", "intent": "  * Classificeer de test data met behulp van de getrainde Naive Bayes.\n  * Bereken de gemiddelde F1-score (average='macro')\n"}
{"snippet": "pred = svm.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "print(\"The Coefficient of Determination (R^2) is: \\n{:.5f}\".format(r2_score(y_mlr,result_mlr)))\nprint(\"The Root Mean Squared Error (RMSE) is: \\n{:.5f}\".format(np.sqrt(mean_squared_error(y_mlr,result_mlr))))\nprint(\"The Mean Absolute Error (MAE) is: \\n{:.5f}\".format(mean_absolute_error(y_mlr,result_mlr)))\nprint(\"The Correlation Coefficient (CC) is: \\n{}\".format(np.corrcoef(result_mlr,y_mlr)))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "guesswho = model.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "pred_rf = rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "y_pred = cross_val_predict(regr, X_pca, y, cv=5)\n", "intent": "* Worse than Cross validation with all 8 attributes\n"}
{"snippet": "user_prediction = predict(data_matrix, user_similarity, type='user')\nitem_prediction = predict(data_matrix, item_similarity, type='item')\n", "intent": "Finally, we will make predictions based on user similarity and item similarity.\n"}
{"snippet": "log_reg_pred = log_reg.predict_proba(test)[:, -1]\n", "intent": "Next step is predictions and selects the correct column:\n"}
{"snippet": "pred = clf.predict(transformed[:, :-222])\nis_anomaly = pred != selected['geoNetwork.country'].cat.codes\n", "intent": "We set suspect anomaly cases as those whose predicted *country* value does not match up with the value in data:\n"}
{"snippet": "exam_result_predicted = np.array([predict(a, b, time) for time in all_times])\nprint(\"Predicted:\", exam_result_predicted)\nprint(\"Actual:   \", exam_result)\n", "intent": "Let's now call `predict()` for every input value and compare our outputs to the original ones.\n"}
{"snippet": "print(linreg.predict(100))\n", "intent": "Let's predict how much is the average Sales of a company that spends 100,000 dollars in TV ads. \n"}
{"snippet": "metrics.silhouette_score(X_scaled, labels, metric='euclidean')\n", "intent": "And to compute the clusters' silhouette coefficient:\n"}
{"snippet": "logreg.predict_proba([1, 0, 29, 1])[:, 1]\n", "intent": "Predict probability of survival for **Susan**: same as Adam, except female.\n"}
{"snippet": "X_pred = autoencoder.predict([X_train[0:5]])\npl.imshow(X_pred[0].squeeze(), interpolation='nearest', cmap=cm.binary)\npl.colorbar()\n", "intent": "A theano function to get the representation of a given input (without reconstructing it)\n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import mean_squared_error\nlin_mse = mean_squared_error(y_pred, y_test)\nlin_rmse = np.sqrt(lin_mse)\nprint('Liner Regression RMSE: %.4f' % lin_rmse)\n", "intent": "Calculate root-mean-square error (RMSE)\n"}
{"snippet": "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3, n_jobs=-1)\nf1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\n", "intent": "**Warning**: the following cell may take a very long time (possibly hours depending on your hardware).\n"}
{"snippet": "y_pred = dnn_clf.predict(X_test1)\naccuracy_score(y_test1, y_pred)\n", "intent": "Let's check that we do indeed get 99.32% accuracy on the test set:\n"}
{"snippet": "def print_rmse(model, df):\n  metrics = model.evaluate(input_fn = make_eval_input_fn(df))\n  print('RMSE on dataset = {}'.format(np.sqrt(metrics['average_loss'])))\nprint_rmse(model, df_valid)\n", "intent": "Evaluate on the validation data (we should defer using the test data to after we have selected a final model).\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\nnp.sqrt(metrics.mean_squared_error(y_test, preds))\n", "intent": "Take a minute to discuss Root Mean Squared Error [RMSE](https://www.kaggle.com/wiki/RootMeanSquaredError)\n"}
{"snippet": "r2_lr = r2_score(ytest, ypred)\nprint(r2_lr)\n", "intent": "Compute and print the training and test $R^2$ score here: \n"}
{"snippet": "rms = sqrt(mean_squared_error(test.Weekly_Sales, y_hat_avg.moving_avg_forecast))\nprint(rms)\n", "intent": "We chose the data from the last eight weeks. We will now calculate RMSE to check to accuracy of our model.\n"}
{"snippet": "y_pred = LogReg1.predict(x_test1)\n", "intent": "**Use the Model to predict on x_test and evaluate the model using metric(s) of Choice.**\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "Evaluate the different classifiers with cross validation\n"}
{"snippet": "print('Model Parameters:\\n')\npprint(best_grid.get_params())\nprint('\\n')\nevaluate(best_grid, test_features, test_labels)\n", "intent": "The final model from hyperparameter tuning is as follows.\n"}
{"snippet": "preds = predict(net2, md.val_dl).argmax(1)\nplots(x_imgs[:8], titles=preds[:8])\n", "intent": "Let's look at our predictions on the first eight images:\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(ypred, ytest))\n", "intent": "We can take a look at the classification report for this classifier:\n"}
{"snippet": "prediction_maps = fully_conv_ResNet.predict(np.random.randn(1, 200, 300, 3))\nprediction_maps.shape\n", "intent": "You can use the following random data to check that it's possible to run a forward pass on a random RGB image:\n"}
{"snippet": "from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, y_score)\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\n", "intent": "Compute the average precision score\n...................................\n"}
{"snippet": "logodds = 0.64722323 + 4.1804038614510901\nodds = np.exp(logodds)\nprob = odds/(1 + odds)\nprint('Probability Calculated for al=3: ', prob)\nprint('Probability computed by model for al=3: ', logreg.predict_proba(3)[:, 1])\n", "intent": "<b>Interpretation:</b> A 1 unit increase in 'al' is associated with a 4.18 unit increase in the log-odds of 'household'.\n"}
{"snippet": "r2_rf = r2_score(ytest, ypred)\nprint(r2_rf)\n", "intent": "Compute and print the training and test $R^2$ score here: \n"}
{"snippet": "def pos_tag(sentence):\n    print('checking...')\n    tagged_sentence = []\n    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n    return zip(sentence, tags)\nimport platform\nprint(platform.python_version())\n", "intent": "for results the link of pos_tags:\nhttps://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n"}
{"snippet": "logit_1.predict_proba([[3], [4]])\n", "intent": "- We can access the probability to be in each class predicted by the logistic regression as well:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(colleges['Cluster'], kmeans.labels_))\nprint(classification_report(colleges['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "train_preds = lr.predict(X_train_level2) \nr2_train_stacking = r2_score(y_train_level2, train_preds) \ntest_preds = lr.predict(X_test_level2) \nr2_test_stacking = r2_score(y_test, test_preds) \nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "def predict(x_in): return sess.run(y, feed_dict={x: [x_in]})\n", "intent": "Demonstrate saving and restoring a model\n"}
{"snippet": "print(classification_report(y_pca_test, y_pred_et_smote_pca))\n", "intent": "** Classification Report **\n___\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "dtree_score = cross_val_score(dtree, features, labels, n_jobs=-1).mean()\nprint(\"{0} -> DTree: {1})\".format(columns, dtree_score))\n", "intent": "Notice gender has a fairly important level of importance.\nLet's test the accuracy again:\n"}
{"snippet": "inp = np.expand_dims(conv_val_feat[0],0)\nnp.round(lrg_model.predict(inp)[0], 2)\n", "intent": "We have to add an extra dimension to our input since the CNN expects a 'batch' (even if it's just a batch of one).\n"}
{"snippet": "r2_ridge = r2_score(ytest, ypred)\nprint(r2_ridge)\n", "intent": "Compute and print the training and test $R^2$ score here: \n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print (metric)\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=skf_seed) \n    NNF = NearestNeighborsFeats.NearestNeighborsFeats(n_jobs=4, k_list=k_list, metric=metric)\n    preds = cross_val_predict(NNF, X, Y, cv=skf) \n    np.save('data/knn_feats_%s_train.npy' % metric, preds)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint('The training accuracy is', train_accuracy)\nprint('The test accuracy is', test_accuracy)\n", "intent": "Now, let's see how our model does, let's calculate the accuracy over both the training and the testing set.\n"}
{"snippet": "class_pred = model.predict(images_test, batch_size=32)\nprint(class_pred[0])\n", "intent": "Predict class for test set images\n"}
{"snippet": "logreg.predict([[1.67]])[0]\n", "intent": "Scikit-learn also has an in-built `.predict()` function for our regression model which will output either `0` or `1` for a given argument:\n"}
{"snippet": "print('R-squared value is', metrics.r2_score(y, intercept + slope*x))\nprint('MAE is', metrics.mean_absolute_error(y, intercept + slope*x))\nprint('Mean and std are', df['Data'].mean(), df['Data'].std())\n", "intent": "The metrics for the whole set are:\n"}
{"snippet": "for_pred = dfor.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "roc_auc_score(df['admit'], lm.predict(feature_set))\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "y_test_6 = model_5.predict(X_test1)\ny_test_6_prob = model_5.predict_proba(X_test1)\nmetrics.roc_auc_score(db_result['FLG_DEF_6M'].tolist(), y_test_6_prob[:,1])\n", "intent": "Valuto la bonta' del risultato:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(bos.PRICE, lm.predict(X))\nprint(mse)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "print('Model Parameters:\\n')\npprint(best_grid.get_params())\nprint('\\n')\npredictions = evaluate(best_grid, test_features, test_labels)\n", "intent": "The final model from hyperparameter tuning as follows\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import GroupKFold\nkfold = GroupKFold(n_splits=7)\ny_predict = cross_val_predict(model, X_train, y_train,cv=kfold,groups=train['SentenceId'])\nprint(y_predict.shape)\ny_predict[0:10]\n", "intent": "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"}
{"snippet": "predictions_1 = model_bag.predict(testing_data)\npredictions_2 = model_forest.predict(testing_data)\npredictions_3 = model_boost.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "score = model.evaluate(X_train, y_train)\nprint(\"\\n Training Accuracy:\", score[1])\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"\\n Testing accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model. \n"}
{"snippet": "y_pred = linreg.predict(X)   \nmetrics.r2_score(y, y_pred)  \n", "intent": "Let's confirm the R-squared value for our simple linear model using `scikit-learn's` prebuilt R-squared scorer:\n"}
{"snippet": "print(\"Classification Report:\\n\", metrics.classification_report(y_test,y_test_pred))\n", "intent": "Or we can compute the full classification report, which will give us precision/recall per-feature:\n"}
{"snippet": "def gbm_predict(X, base_algorithms_list, coefficients_list):\n    return [sum([coeff * algo.predict([x])[0] for algo, coeff in zip(base_algorithms_list, coefficients_list)]) for x in X.values]\n", "intent": "$L'(y,z) = 2(y - z) = c*(y - z)$\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_vgg19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions) == np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predicted_labels = classifier.predict( testing_data )\n", "intent": "That's it! **The classifier is now trained on the training data!**\nNow predict the value of the digit on the second half:\n"}
{"snippet": "yhat=lm.predict(new_input)\nyhat[0:5]\n", "intent": "Produce a prediction \n"}
{"snippet": "y_pred = fit_decision(X_train, y_train, X_test, random_state=check_random_state(0))\naccuracy = accuracy_score(y_test, y_pred)\nprint('The accuracy score is {:0.2f}.'.format(accuracy))\n", "intent": "- Function `fit_decision()` trains a **Decision Trees** model.\n"}
{"snippet": "y_pred = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "y_pred_class = knn.predict(X)\nfrom sklearn import metrics\nprint metrics.accuracy_score(y, y_pred_class)\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "leo = np.array([[3, 22, 0]])\nkate = np.array([[1, 25, 1]])\nprint(m.predict(leo))\nprint(m.predict(kate))\n", "intent": "There is (at least) one error in the definition of the data for prediction. Can you find and fix it?\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_modelD.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50d]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = svc_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "metrics.silhouette_score(X_scale, labels, metric='euclidean') \n", "intent": "Compute the Silhoutte Score\n"}
{"snippet": "labels = model.predict(points)\n", "intent": "**Step 5:** Use the `.predict()` method of `model` to predict the cluster labels of `points`, assigning the result to `labels`.\n"}
{"snippet": "predictions = rtree.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "def mae_score(y_true, y_pred):\n    return mean_absolute_error(np.exp(y_true), np.exp(y_pred))\nmae_scorer = make_scorer(mae_score, greater_is_better=False)\n", "intent": "We'll also use a custom scorer which works with log-transformed values:\n"}
{"snippet": "def get_accuracy(ypred, ytest):\n    accuracy = accuracy_score(ytest, ypred)\n    return accuracy\n    raise NotImplementedError()\n", "intent": "Get the performance (accuracy) of your algorithm given ytest\n"}
{"snippet": "metrics.confusion_matrix(y, model1.predict(X))\n", "intent": "Compute confusion matrix.\n"}
{"snippet": "ten_fold_cv = ms.StratifiedKFold(n_splits=10, shuffle=True)\naucs = ms.cross_val_score(gtb1, X, y, scoring='roc_auc', cv=ten_fold_cv)\nnp.mean(aucs)\n", "intent": "Evaluate performance through cross-validation.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint (\"RMSE:\", math.sqrt(mean_squared_error(ys, predictions)))\nprint (\"MAE:\", mean_absolute_error(ys, predictions))\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "y_train_pred = clf.predict(X_train)\n", "intent": "Predict the training set.\n"}
{"snippet": "cv = ShuffleSplit(n_splits=5, test_size=.2)\ncross_val_score(classifier, X, y, cv=cv)\n", "intent": "You can use all of these cross-validation generators with the cross_val_score method:\n"}
{"snippet": "def train(self, training_data, epochs, mini_batch_size, learning_rate):\n    training_data = list(training_data)\n    for i in range(epochs):\n        mini_batches = self.create_mini_batches(training_data, mini_batch_size)\n        cost = sum(map(lambda mini_batch: self.update_params(mini_batch, learning_rate), mini_batches))\n        acc = self.evaluate(training_data)\n        print(\"Epoch {} complete. Total cost: {}, Accuracy: {}\".format(i, cost, acc))\n", "intent": "We shall implement backpropogration with stocastic mini-batch gradient descent to optimize out network.\n"}
{"snippet": "test_predictions = model.predict_proba(X_test).argmax(axis=-1)\ntest_answers = y_test.argmax(axis=-1)\ntest_accuracy = np.mean(test_predictions==test_answers)\nprint(\"\\nTest accuracy: {} %\".format(test_accuracy*100))\nassert test_accuracy>=0.92,\"Logistic regression can do better!\"\nassert test_accuracy>=0.975,\"Your network can do better!\"\nprint(\"Great job!\")\n", "intent": "So far, our model is staggeringly inefficient. There is something wrong with it. Guess, what?\n"}
{"snippet": "r50_predictions = [np.argmax(r50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(r50_predictions)==np.argmax(test_targets, axis=1))/len(r50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def ResidSE(x, y, n, lm):\n    return np.sqrt(np.sum((lm.predict(x) - y)**2)/(n-2))\ndef StandErr_b0(RSE, n, x):\n    xbar = np.mean(x)\n    return np.sqrt(RSE**2*(1/n + xbar**2/np.sum((x - xbar)**2)))\ndef StandErr_b1(RSE, n, x):\n    xbar = np.mean(x)\n    return np.sqrt(RSE**2*(1/np.sum((x - xbar)**2)))\n", "intent": "Now we'll define functions for the RSE and standard errors:\n"}
{"snippet": "X_real, X_fake = np.array([next(data_point) for _ in range(batch_size)]).reshape(-1,1), G.predict(z)\n", "intent": "- Evaluate $\\mathcal{D}$'s accuracy on the fake images $\\mathcal{G}$ produces from `Z_`\n- Save them into two python variables `loss` and `acc`\n"}
{"snippet": "predictions = nb.predict(X_teste)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "metrics.silhouette_score(data, kmeans.labels_, metric='euclidean')\n", "intent": "Compute the silhoutte coefficient to see how consistent our data is within the clusters\n"}
{"snippet": "print(y[:30])\nprint(model.predict(X)[:30])\n", "intent": "It turns out that, for this dataset, the classifier is pretty much perfect:\n"}
{"snippet": "auc = roc_auc_score( subreddit_id_binary[test_idx], y_score[:,1])\nprint(\"{0:.2f}\".format(auc))\n", "intent": "To evalute how are classifer had done we find the AUC (Area Under Curve) of a ROC Curve and plot a precision recall curve. \n"}
{"snippet": "prediction_rfc = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h5>4.3.1.3.2. Incorrectly Classified point</h5>\n"}
{"snippet": "y_predict = knn_1.predict(X)\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "lasso = Lasso(alpha=optimal_lasso.alpha_)\nlasso_scores = cross_val_score(lasso, Xs, Y.values.ravel(), cv=10)\n", "intent": "---\nIs it better than the Linear regression? Is it better than Ridge? What do the differences in results imply about the issues with the dataset?\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccs = cross_val_score(knn, X, y, cv=10)\nprint(accs)\nprint(np.mean(accs))\n", "intent": "Use 10 folds. How does the mean performace across folds compare to the baseline accuracy?\n"}
{"snippet": "def predict(clf, timeline):\n    return y_pred\n", "intent": "- Use the SVM classifier to classify each tweet in timeline as a positive tweet or a negative tweet.\n"}
{"snippet": "print(\"R-squared value of this fit:\",round(metrics.r2_score(y_train,train_pred),3))\n", "intent": "**R-square of the model fit**\n"}
{"snippet": "prediction = classifier.predict(X_test)\n", "intent": "We can then apply the model to unseen data and use the model to predict the estimated outcome using the ``predict`` method:\n"}
{"snippet": "model.load_weights('imdb.model.best.hdf5')\nscore = model.evaluate(x_test, y_test, verbose=1)\nprint(\"\\nAccuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_train_pred = GR.predict(X_train)\ny_test_pred  = GR.predict(X_test)\ntrain_test_gr_error = pd.Series({'train': mean_squared_error(y_train, y_train_pred),\n                                         'test':  mean_squared_error(y_test, y_test_pred)},\n                                          name='MSE').to_frame().T\ntrain_test_gr_error\n", "intent": "The error on train and test iris_data sets. Since this is continuous, we will use mean squared error.\n"}
{"snippet": "pred_sem3_train = model_semantic3.predict(designmat_sem3_train)\npred_acc_sem3_train = correlate(data_train, pred_sem3_train)\n", "intent": "Predict the training data and correlate those predictions with the real response amplitudes.\n"}
{"snippet": "y_train_pred_gr = GR.predict(X_train)\ny_test_pred_gr = GR.predict(X_test)\ntrain_test_gr_error = pd.concat([measure_error(y_train, y_train_pred_gr, 'train'), \n                                 measure_error(y_test, y_test_pred_gr, 'test')], \n                                 axis = 1)\ntrain_test_gr_error\n", "intent": "The number of nodes is 119 and the maximum depth of the tree is 9. \n"}
{"snippet": "print logreg.predict(X_test)[0:10]\nprint logreg.predict_proba(X_test)[0:10, :] \nlogreg.predict_proba(X_test)[0:10,1]\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n", "intent": "Now we will look into how we can MODIFY THE performance of a classifier by adjusting the classification threshold. \n"}
{"snippet": "predictions = model.predict(X)\nprint(f\"True output: {y[0]}\")\nprint(f\"Predicted output: {predictions[0]}\")\nprint(f\"Prediction Error: {predictions[0]-y[0]}\")\n", "intent": "We can use our model to make predictions.\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(df['Cluster'],kmean_clustering.labels_))\nprint('\\n')\nprint(classification_report(df['Cluster'],kmean_clustering.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "clf, ts_preds = classify_topics(\n    nmf, nmf.transform(train_data), train['target'], test_data, check_random_state(0)\n    )\nprint(classification_report(test['target'], ts_preds, target_names=test['target_names']))\n", "intent": "The resulting classification report and confusion matrix are shown to demonstrate the quality of this classification method.\n"}
{"snippet": "train_and_evaluate(clf, X_train, X_test, y_train, y_test)\n", "intent": "Now, lets fit our modelfrom the train set and test it against the test set. Explain why our test set contains 25 measurements.\n"}
{"snippet": "def precision(actual, preds):\n    return None \nprint(precision(y_test, preds_nb))\nprint(precision_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def recall(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    act_pos = (actual==1).sum()\n    return float(tp)/act_pos\nprint(recall(y_test, preds_nb))\nprint(recall_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "from sklearn.metrics import f1_score\ndef my_f1_score(trained_model):\n    return None\nassert my_f1_score(trained_knn) == f1_score(trained_knn['y_test'],\n                                            trained_knn['y_test_pred']), \\\n         'Those are not the same'\n", "intent": "Complete the following \"roll your own\" method for calculating F1-Score.\n"}
{"snippet": "sp_ypred =  spatial.predict(coordinates[test,:])\natt_ypred = attribute.predict(X[test,:])\nboth_ypred = both.predict(np.hstack((X,coordinates))[test,:])\n", "intent": "To score them, I'm going to grab their out of sample prediction accuracy and get their % explained variance:\n"}
{"snippet": "print(TP/float(TP + FP))\nprint(metrics.precision_score(y_test, y_pred_class))\n", "intent": "<b>6. Precision: </b> When a positive value is predicted, how often is the prediction correct?\n"}
{"snippet": "y_pred = regression.predict(X_test)\n", "intent": "$MSPE(\\{x_i, y_i\\}_{i=1}^l, \\, w) = \\frac{1}{l}\\sum_{i=1}^l \\left( \\frac{y_i - \\langle w, x_i \\rangle }{y_i} \\right)^2$\n"}
{"snippet": "y_pred = knn.predict(X_train)\n", "intent": "```\ny_pred = knn.predict(X_train)```\n"}
{"snippet": "pred = pipeline.predict([text])\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "def predict(clf, timeline):\n    return y_pred\n", "intent": "- Use the RandomForestClassifier to classify each tweet in timeline as a positive tweet or a negative tweet.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n", "intent": "Now we can check the accuracy of our model:\n"}
{"snippet": "predict(image_path=image_paths_train[1])\n", "intent": "We can try it for another image in our new training-set and the VGG16 model is still confused.\n"}
{"snippet": "shutil.rmtree(path = \"babyweight_trained_wd\", ignore_errors = True) \ntrain_and_evaluate(\"babyweight_trained_wd\")\n", "intent": "Finally, we train the model!\n"}
{"snippet": "metrics.mean_squared_error([1, 2, 3, 4, 5], [1, 2, 3, 4, 5])\n", "intent": "For example, if we to compare two arrays of the same values, we would expect a mean squared error of 0:\n"}
{"snippet": "predicted_values = knm.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.10f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "reconstruction=model.predict(x_test)\n", "intent": "Now we want to get the target of the AE - meaning the reconstructed image.\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(x_test) - y_test) ** 2))\n", "intent": "The plot shows our model fits the data well.\n"}
{"snippet": "print(\"Graph A: learning rate = 0.0001\")\nplot_error(.0001)\n", "intent": "Plot the error as a function of the number of iterations for various learning rates. Choose the rates\nso that it tells a story.\n"}
{"snippet": "def get_accuracy(ypred, ytest):\n    return 100*accuracy_score(ypred,ytest)\n", "intent": "Get the performance (accuracy) of your algorithm given ytest\n"}
{"snippet": "print 'Estimated number of clusters: %d' % n_clusters_\nprint \"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels)\nprint \"Completeness: %0.3f\" % metrics.completeness_score(y, labels)\nprint \"V-measure: %0.3f\" % metrics.v_measure_score(y, labels)\n", "intent": "**7.2 Check the homogeneity, completeness, and V-measure against the stored rank `y`**\n"}
{"snippet": "print \"Accuracy = %.3f\" % (metrics.accuracy_score(decision_tree.predict(X), Y))\n", "intent": "If we use this as our decision tree, how accurate is it?\n"}
{"snippet": "fit <- lm(assists ~ fgMade, data=train)\npredictions <- predict(fit, test)\n", "intent": "Let's try doing some predictions based on linear regression. Say we want to predict assists in terms of field goals.\n"}
{"snippet": "tuned_forest_predictions = locally_best_forest.predict(X_test) \naccuracy_score(y_test,tuned_forest_predictions)\n", "intent": "Make a prediction for test data.\n"}
{"snippet": "y_pred = model.predict(X_test)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100 * np.sum(np.array(Xception_predictions) == np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = model.predict(X_test)\nprint(\"KNN Test Accuracy is {0:.2f}\".format(accuracy_score(y_test, y_pred) * 100))\n", "intent": "Finally let's see how our estimator performs on the hold-out test set\n"}
{"snippet": "preds_train = model.predict(X_train, verbose=0)\npreds_test = model.predict(X_test, verbose=0)\npreds_train = (preds_train > 0.5).astype(np.int)\npreds_test = (preds_test > 0.5).astype(np.int)\n", "intent": "Do a prediction for both train and test\n"}
{"snippet": "print(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(X_test) - y_test) ** 2))\nprint('Variance score: %.2f' % regr.score(X_test, y_test))\n", "intent": "e) Evalute the R^2 on training data. Is this good? Bad? Why?\n"}
{"snippet": "import copy\ndef predict(model, train_set, width, nof_points):\n    prediction = []\n    x_test = copy.deepcopy(train_set[-width : ]) \n    for i in range(nof_points):\n        prediction.append(model.predict(x_test.reshape((1, -1))))\n        x_test[0 : -1] = x_test[1 : ]\n        x_test[-1] = prediction[-1]\n    return np.array(prediction)\n", "intent": "Now change the ```width``` parameter to see if you can get a better score.\nNow execute the code below to see the prediction of this model.\n"}
{"snippet": "action_layer.epsilon.set_value(0.05)\nuntrained_reward = np.mean(pool.evaluate(save_path=\"./records\",\n                                         record_video=True))\n", "intent": "Play one full game with untrained network, record video\n"}
{"snippet": "pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "predictions = lr.predict(new_data_reindexed)\n", "intent": "We can now make predictions using our model.\n"}
{"snippet": "models=[tree,svr,MLP,neigh,randomforest,GBR,knn,ada_neigh]\nrss_list=[]\nfor model in models:\n    rss_list.append(sum((y_test-model.predict(X_test_pca))**2))\n", "intent": "Residual Sum of Square of each model in comparison list\n"}
{"snippet": "print \"Class predictions according to Sklearn:\" \nprint sentiment_model.predict(sample_test_matrix)\n", "intent": "Run the following code to verify that the class predictions obtained by your calculations are the same as that obtained from GraphLab Create.\n"}
{"snippet": "y_estimado = fit.predict(X)\nprint(\"Reales   :\", y[0:25])\nprint(\"Estimados:\", y_estimado[0:25])\n", "intent": "Se pueden evaluar las clases estimadas por el modelo vs. las clases reales\n"}
{"snippet": "from sklearn.metrics import r2_score\ndef print_r2_score():\n    r2 = \n    print(\"R2 score: {:.2f}\".format(r2))\n", "intent": "Write a function that:\n* Takes y_test en y_pred as parameters.\n* Calculates the $R^2$ score.\n* Prints the result.\n"}
{"snippet": "preds = model.predict(test_sample)\nerrors = [i for i in xrange(0, len(test_sample)) if preds[i] != test_labels_sample[i]]\nfor i in errors:\n    query_img = test_sample[i]\n    _, result = model.kneighbors(query_img, n_neighbors=4)\n    show(query_img)\n    show(train[result[0],:], len(result[0]))\n", "intent": "* Next, visualize the nearest neighbors of cases where the model makes erroneous predictions\n"}
{"snippet": "cm = sklearn.metrics.confusion_matrix(y_train,clf.predict(X_train))\ncm_norm = cm/cm.sum(axis=1)[:, np.newaxis]\ncm_norm\n", "intent": "Plot the (normalised) confusion matrix for the training data. Label axes appropriately.\n"}
{"snippet": "bank_features_predicted = grid_search.best_estimator_.predict(bank_features_test)\ntuned_score = f1_score(bank_labels_test, bank_features_predicted)\n", "intent": "Use the best estimator from your grid search. Score it using the function from problem 6. Save your answer in `tuned_score`.\n"}
{"snippet": "print \"Root Mean Squared Logarithmic Error Train: \", rmsele(rf_op.predict(xtrain), ytrain)\nprint \"Root Mean Squared Logarithmic Error Test: \", rmsele(rf_op.predict(xtest), ytest)\nprint \"Training accuracy: %0.2f%%\" % (100*rf_op.score(xtrain, ytrain))\nprint \"Test accuracy: %0.2f%%\" % (100*rf_op.score(xtest, ytest)) + \"\\n\"\n", "intent": "It is clear from the above plot that month 9 has the most effect on the outcome.\n"}
{"snippet": "print(classification_report(y_test,preds))\nprint(confusion_matrix(y_test,preds))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "y_pred = linreg.predict(X)\nglass.loc[:, 'y_pred'] = y_pred\n", "intent": "- Add a column `y_pred` to `glass` that stores the model's fitted values for the refractice index.\n"}
{"snippet": "knr.predict(dp_scaled)\n", "intent": "Let's use our trained KNeighborsRegressor to predict the value for our scaled datapoint:\n"}
{"snippet": "y_predict_full = model.predict(X)\nmodel.score(X, y)\n", "intent": "+ Is this error reliable? \n+ What could we do to make it better?\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nyhat_train = linregpipe.predict(X_train)\nyhat_valid = linregpipe.predict(X_valid)\nmse_train = mean_squared_error(yhat_train, y_train)\nmse_valid = mean_squared_error(yhat_valid, y_valid)\nprint(\"Training MSE:   {:.3f}\".format(mse_train))\nprint(\"Validation MSE: {:.3f}\".format(mse_valid))\nxplot = np.linspace(-60,60,100).reshape(-1,1)\nyplot = linregpipe.predict(xplot) \ndam_plot([(X_train, y_train, \"training\"), (X_valid, y_valid, \"validation\")], [(xplot, yplot, \"model\")])\n", "intent": "**Part B**: Next we'll make predictions on the training and validation sets and evaluate the mean-squared error. \n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"RMSE:\", mean_squared_error(ys, predictions)**.5\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "test_wf         = vectorizer.transform(test['description'])\ntest_prediction = nb_classifier.predict(test_wf)\n", "intent": "Precision: % of selected items that are correct \nRecall: % of correct items that are selected\n"}
{"snippet": "best = clf.best_estimator_\nprint('Best estimator: ', best)\nscores = cross_validation.cross_val_score(best, X_test, y_test, cv=10)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "Then we can check the accuracy on the **Test Set**:\n"}
{"snippet": "def predict(model, train_set, width, nof_points):\n    prediction = []\n    x_test = copy.deepcopy(train_set[-width : ]) \n    for i in range(nof_points):\n        prediction.append(model.predict(x_test.reshape((1, -1))))\n        x_test[0 : -1] = x_test[1 : ]\n        x_test[-1] = prediction[-1]\n    return np.array(prediction)\n", "intent": "Now change the ```width``` parameter to see if you can get a better score.\nNow execute the code below to see the prediction of this model.\n"}
{"snippet": "scores = cross_val_score(su, X_cross, Y_cross, cv=10)  \nprint(\"Accuracy :\", scores)\n", "intent": "Due to lack of computational power I am using 1/10 fraction of train.csv dataset for cross validation.\n"}
{"snippet": "import numpy as np\nxinput = np.array([[3.0, 5.0, 4.1, 2.0]])\npred_class_number = knn.predict(xinput)\nprint( iris.target_names[pred_class_number] )\n", "intent": "We can test it out by making a prediction for a new input example described by 4 feature values:\n"}
{"snippet": "from sklearn.metrics import classification_report\nclass_rep_tree = classification_report(test_labels,pred_labels_tree)\nclass_rep_log = classification_report(test_labels,pred_labels_logit)\nprint(\"Decision Tree: \\n\", class_rep_tree)\nprint(\"Logistic Regression: \\n\", class_rep_log)\n", "intent": "This is better but as in our project for the course, let's use more metrics to evaluate our models.\n"}
{"snippet": "ex_1d_x = np.array([50, 150], dtype=np.float32)\npredict = k_means_estimator.predict(input_fn=lambda: input_fn_1d(ex_1d_x), as_iterable=False)\n", "intent": "*Note that the predict() function expects the input exactly like how we specified the feature vector*\n"}
{"snippet": "split = 0.5\nX_train, X_test, y_train, y_test = load_dataset(split)\nprint('Training set: {0} samples'.format(X_train.shape[0]))\nprint('Test set: {0} samples'.format(X_test.shape[0]))\nk = 3\ny_pred = predict(X_train, y_train, X_test, k)\naccuracy = compute_accuracy(y_pred, y_test)\nprint('Accuracy = {0}'.format(accuracy))\n", "intent": "Should output an accuracy of 0.9473684210526315.\n"}
{"snippet": "y_pred = model.predict(X_test)\nprint(confusion_matrix(y_actual, y_pred))\n", "intent": "Let's first print the confusion matrix as we usually do.\n"}
{"snippet": "loss, acc = new_model.evaluate(test_images, test_labels)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n", "intent": "Check its accuracy:\n"}
{"snippet": "    predictions = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "preds = predict(X_test, 0.8, model)\n", "intent": "Make predictions on testing data. Store predictions in preds variable.\n"}
{"snippet": "def predict(model, ts_data, width, nof_points):\n    prediction = []\n    x_test = np.copy(ts_data[-width : ])\n    for i in range(nof_points):\n        prediction.append(model.predict(x_test.reshape((1, -1)))[0])\n        x_test[0 : -1] = x_test[1 : ]\n        x_test[-1] = prediction[-1]\n    return np.array(prediction)\n", "intent": "Now we're going to visualize our prediction by repeating the one-step-ahead prediction. \n"}
{"snippet": "test_files = [im for im in os.listdir(TEST_DIR)]\ntest = np.ndarray((len(test_files), ROWS, COLS, CHANNELS), dtype=np.uint8)\nfor i, im in enumerate(test_files): \n    test[i] = read_image(TEST_DIR+im)\ntest_preds = model.predict(test, verbose=1)\n", "intent": "Finishing off with predictions on the test set.\n"}
{"snippet": "docs_new = ['God is love', 'OpenGL on the GPU is fast']\nX_new_counts = count_vect.transform(docs_new)\nX_new_tfidf = tfidf_transformer.transform(X_new_counts)\npredicted = clf.predict(X_new_tfidf)\nfor doc, category in zip(docs_new, predicted):\n    print('%r => %s' % (doc, twenty_train.target_names[category]))\n", "intent": "Predicting on a new document\n"}
{"snippet": "pred = svc1.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "print acc_score(y_test, y_pred)\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\npredictions = svc.predict(X_test)\nprint(confusion_matrix(y_test, predictions))\nprint('\\n')\nprint(classification_report(y_test, predictions))\n", "intent": "This might not go the way you expect it to.\n"}
{"snippet": "n = len(y)\ncv = sklearn.cross_validation.StratifiedShuffleSplit(n, n_iter=10, test_size=.25)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "Of course, you can also do a stratified version of this:\n"}
{"snippet": "cv = sklearn.cross_validation.StratifiedShuffleSplit(y, n_iter=10, test_size=.25)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "Of course, you can also do a stratified version of this:\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "**Root mean squared error (RMSE)** is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncvs = cross_val_score(enet, X, y, cv =5)\ncvs\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "score = model.evaluate(testvectors, Y_test)\nprint(\"Test loss\", score[0])\nprint(\"Test accuracy\", score[1])\n", "intent": "An overall accuracy measure can also be obtained by means of the **evaluate** method of the model\n"}
{"snippet": "predicted = result.predict(X_test)\nprobs = result.predict_proba(X_test)\nprint metrics.accuracy_score(y_test, predicted)\n    print metrics.roc_auc_score(y_test, probs[:, 1])\n", "intent": "Answer: A one unit increase in GPA doubles the odds of being admitted.  \n"}
{"snippet": "predictions = lmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "def recall(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1))) \n    actual_pos = np.sum(actual==1) \n    return tp/(actual_pos)\nprint(recall(y_test, preds_rf))\nprint(recall_score(y_test, preds_rf))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "cv_scores = []\nkf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 2016)\nfor dev_index, val_index in kf.split(range(train_X.shape[0])):\n        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n        dev_y, val_y = train_y[dev_index], train_y[val_index]\n        preds, model = runXGB(dev_X, dev_y, val_X, val_y)\n        cv_scores.append(log_loss(val_y, preds))\n        print(cv_scores)\n        break\n", "intent": "Without CV statistic,to score get 0.5480 by SRK. And CV statistic get 0.5346 In fact ,you \nneed to turn down the learning rate and turn up run_num\n"}
{"snippet": "EXAMPLES = ['the fifty day of 2015', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "pred_dtree = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "preds_binary = rtf.predict(X_test)\npreds_prob = rtf.predict_proba(X_test)\nabs_diff = abs(preds_binary - y_test)\nwrong_instances = X_test[abs_diff==1,:]\nright_instances = X_test[abs_diff==0,:]\n", "intent": "Extract the wrongly classified instances (False positive and false negative instances)\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "preds = log_model.predict(X= train_features)\npd.crosstab(preds,df[\"Survived\"])\n", "intent": "Next, let's make class predictions using this model and then compare the predictons to the actual values:\n"}
{"snippet": "r2_score(ytest, ytest_model)\n", "intent": "Compute and print the training and test $R^2$ score here: \n"}
{"snippet": "accuracy_score(Y_test,Y_pred)\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "def predict(ratings, similarity, type='item'):\n    if type == 'item':\n        print np.shape(ratings)\n        print np.shape(similarity)\n        result = np.dot(ratings,similarity)\n    if type == 'user':\n        rating_mean = np.mean(ratings,axis=1)\n        rating_diff = (ratings - rating_mean[:,np.newaxis])\n        result = rating_mean[:,np.newaxis] + np.dot(similarity,rating_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n    return result\n", "intent": "<img src=\"user_predict.gif\">\n<img src=\"item_predict.gif\">\n"}
{"snippet": "newdata = test_rgb.reshape(M * N, -1)[:,:]\ncluster = gmm.predict(newdata)\ncluster = cluster.reshape(M, N)\n", "intent": "Apply to the test image and plot\n"}
{"snippet": "prediction = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print('RMSE Train', sqrt(mean_squared_error(y_train_pred, y_train)))\nprint('RMSE Test' , sqrt(mean_squared_error(y_test_pred, y_test)))\n", "intent": "The below calculation shows that RMSE value for both training and test data is similar and less too. It revals that model is a good fit.\n"}
{"snippet": "preds = model.predict(Xr)\npreds\n", "intent": "Visualize predictions\n"}
{"snippet": "preds = model.predict(X)\nsum(preds == y)/float(X.shape[0])\n", "intent": "This is what's known as the accuracy score or the percentage of prediction we got correct.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "**Wow we got 91% accuracy in test set using OvO**\n"}
{"snippet": "print(TP / float(TP + FN))\nprint(metrics.recall_score(y_test, y_pred))\n", "intent": "How sensitive is the classifier to detecting positive instances?\nAKA \"True Positive Rate\" or \"Recall\"\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\npreds = model.predict(test_tensor)\npreds = np.argmax(preds, axis=1)\nY_test_binary = np.argmax(Y_test,axis=1)\nconfusion_matrix = confusion_matrix(Y_test_binary, preds)\nconfusion_matrix\n", "intent": "In this case adding more epochs ends making the model overfit (increasing the test loss), so I keep 20 epochs.\n"}
{"snippet": "lambda_ = T_distill * T_distill\ntrain_loss = get_cross_entropy_loss(logits=predictions_student, labels=batch_train_labels)\ntrain_loss += lambda_ * distill_kl_loss\n", "intent": "**Define the joint training loss**\n"}
{"snippet": "rf_pred = rfmodel.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "predictionTree = dtree.predict(X_test)\n", "intent": "Let's evaluate our decision tree.\n"}
{"snippet": "y_pred = best_sclf.predict(X_test)\nprint(\"Lift Score for Stacking Classifier:\")\nlift_score(y_test, best_sclf.predict(X_test))\n", "intent": "Tuning for lift yields the same Best Parameter as before and thus the same model\n"}
{"snippet": "input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={'images': mnist.test.images}, y=mnist.test.labels,\n    batch_size=batch_size, shuffle=False)\nmodel.evaluate(input_fn)\n", "intent": "Evaluate the Model (**mnist.test.images**). Define the input function for evaluating.\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "Lasst uns den Entscheidungsbaum auswerten.\n"}
{"snippet": "metrics.silhouette_score(y, labels, metric='euclidean')\n", "intent": "Compute the silhoutte coefficient to see how consistent our data is within the clusters\n"}
{"snippet": "print(confusion_matrix(y_test, predictions))\nprint('\\n')\nprint(classification_report(y_test, predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100.0*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Test accuracy: %.2f%%\" % (scores[1]*100))\n", "intent": "Once we found our best model, we can evaluate its performance on the test set. In our case, we would like to compute its accuracy:\n"}
{"snippet": "def calculate_model_error_accuracy(X_train, y_train, y_test,predictions):\n    trainScore = mean_squared_error(X_train, y_train)\n    print('Train Score: %.4f MSE (%.4f RMSE)' % (trainScore, math.sqrt(trainScore)))\n    testScore = mean_squared_error(predictions, y_test)\n    print('Test Score: %.8f MSE (%.8f RMSE)' % (testScore, math.sqrt(testScore)))\n", "intent": "   **Calculate the model error **\n"}
{"snippet": "rasnet50_predictions = [np.argmax(rasnet50_dog_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\ntest_accuracy = 100*np.sum(np.array(rasnet50_predictions)==np.argmax(test_targets, axis=1))/len(rasnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "metrics.accuracy_score(y, predY)\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "pred_RF = RF_clf.predict(X_test)\n", "intent": "Predicting the Random Forest Classifier on test set\n"}
{"snippet": "res_predictions = [np.argmax(res_model.predict(np.expand_dims(feature, axis=0))) for feature in test_res]\ntest_accuracy = 100*np.sum(np.array(res_predictions)==np.argmax(test_targets, axis=1))/len(res_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import metrics\ndt_predicted = dt_classifier.predict(X_test)\nprint(\"Classification report:\")\nprint(metrics.classification_report(y_test, dt_predicted)) \n", "intent": "Scikit-learn will can print out the **Recall** and **Precision** scores for a classification model by using `metrics.classification_report()`.\n"}
{"snippet": "part_test = np.array(['57111'])\ncomplaint_test = np.array(['BRAKE PEDAL FEELS HARD'])\nX_new_part_labelencoded = enc_label.transform(part_test)\nX_new_part_onehot = enc_onehot.transform(X_new_part_labelencoded.reshape(-1,1))\nX_new_complaint_counts = count_vect.transform(complaint_test)\nX_new_complaint_tfidf = tfidf_transformer.transform(X_new_complaint_counts)\nX_new_combined_tfidf = sparse.hstack((X_new_part_onehot, X_new_complaint_tfidf), format='csr')\npredicted = clf.predict(X_new_combined_tfidf)\nprint(predicted)\n", "intent": "This should return 1:\n"}
{"snippet": "yhat=lr.predict(Z_test)\nyhat[0:4]\n", "intent": " We can make a prediction:\n"}
{"snippet": "prediction_train = lm.predict(X=X_train)\nprint('Prediction of training data:',prediction_train)\nprice_predict = prediction_train  \n", "intent": "By looking into the attributes of your model, write down an equation for predicting the price of a car given the engine-power.\n"}
{"snippet": "[model.predict(np.expand_dims(validation_inputs[i], 0)) for i in range(10)]\n", "intent": "Let's print the predicted and actual labels for the first 10 observations.\n"}
{"snippet": "constant = np.ones(len(X_train))[np.newaxis].T\nXandBias = np.hstack((constant, X_train ))\nw = np.array([lrModel.intercept_, lrModel.coef_[0]])[np.newaxis].T\ndef myPredict(table, biasAndweights):\n    return table.dot(biasAndweights)\nmyPredictions = myPredict(XandBias, w).T[0]\nmodelPredictions = lrModel.predict(X_train)\nprint \"our equation results in exactly the same results as the predict function of the linear model for all inputs\"\nnp.all(myPredictions == modelPredictions)\n", "intent": "By looking into the attributes of your model, write down an equation for predicting the price of a car given the engine-power.\n"}
{"snippet": "predictions = knn.predict(test_x)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "To predict the label probabilites of the test set using this model we can use the following code.\n"}
{"snippet": "print \"accuracy  {:.3}\".format(accuracy_score(ytest, ypred))\nprint \"precision {:.3}\".format(precision_score(ytest, ypred))\nprint \"recall    {:.3}\".format(recall_score(ytest, ypred))\n", "intent": "Results from testing the model on the unbalanced data \nThese are the definitive metrics that inform us about how well this classifier will perform.\n"}
{"snippet": "my_model = my_model_1\nmy_preds = my_model.predict(test_data)\npreds = my_preds\nmost_likely_labels = decode_predictions(preds, top=1, class_list_path='../input/resnet50/imagenet_class_index.json')\nfor i, img_path in enumerate(img_paths[:2]):\n    display(Image(img_path))\n    print(most_likely_labels[i])\n", "intent": "**Method 1: Results**\n"}
{"snippet": "print('MAE: ',metrics.mean_absolute_error(y_test, pred))\nprint('MSE: ', metrics.mean_squared_error(y_test, pred))\nprint('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, pred)))\n", "intent": "** Calculating Errors **\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nrf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\nsvc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\nprint(\"AUC for Random Forest: {:.3f}\".format(rf_auc))\nprint(\"AUC for SVC: {:.3f}\".format(svc_auc))\n", "intent": "- A good summary measure is the area under the ROC curve (AUROC or AUC)\n- Compute using the `roc_auc_score` \n    - Don't use `auc`\n"}
{"snippet": "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(estimator, X, Y_enc, cv=kfold)\nprint \"Baseline Acc: %.2f%% (%.2f%%)\"%(results.mean()*100, results.std()*100)\n", "intent": "Evaluate baseline model\n"}
{"snippet": "print(classification_report(y_test, y_predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "samples = 100\npredicted_vals = [model.predict(np.expand_dims(validation_inputs[i], 0)) for i in range(samples)]\npredicted = [np.rint(x[0][0]) for x in predicted_vals]\nactual = validation_labels[:samples]\nsum(abs(predicted-actual) == 0)/len(predicted) * 100\n", "intent": "As an additional check, let's compute the validation accuracy for the first 100 samples.\n"}
{"snippet": "X_more = np.random.uniform(X.min(), X.max(), size=(10, 2))\nX_more\nmore_cluster_labels = kmeans.predict(X_more)\n", "intent": "KMeans can generalize, SpectralClustering can not\n-----------------------------------------------------------\nKMeans has a ``predict`` function.\n"}
{"snippet": "from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\ncross_val_score()\n", "intent": "Test the performance of the AdaBoost and GradientBoostingClassifier models on the car dataset. Use the code you developed above as a starter code.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\npredicted_salaries = model.predict(X_test)\nprint (mean_absolute_error(y_test, predicted_salaries))\n", "intent": "As mentioned, our evaluation criteria is going to be MAE - Mean Absolute Error, we want the lowest average error of salary prediction  \n"}
{"snippet": "class ContentLoss(nn.Module):\n    def __init__(self, target,):\n        super(ContentLoss, self).__init__()\n        self.target = target.detach()\n    def forward(self, input):\n        self.loss = F.mse_loss(input, self.target)\n        return input\n", "intent": "- image X\n- content image C\n- FXL  of a layer L\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom math import sqrt\ndef rmse(y_true, y_pred):\n    rmse = sqrt(mean_squared_error(y_true, y_pred))\n    return rmse\n", "intent": "Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between log(predicted value) and log(observed values) of sales price.\n"}
{"snippet": "cross_val_score(est, X, y)\n", "intent": "And we can use the `cross_val_score` function to check the predictive accuracy:\n"}
{"snippet": "X_test = full_matrix[idx_split:,:]\ntest_pred = lrfinal.predict_proba(X_test)[:, 1]\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "But cross-validation comes at the cost of training\nthe model several times, so it is not always possible.\n"}
{"snippet": "predictions = model.predict(test_x)\n", "intent": "---\n<a id=\"predictions\"></a>\n"}
{"snippet": "activations2 = model_diagnostic.predict(np.expand_dims(number_to_input_example(4), 0))\n", "intent": "Next, let's look at the activations for a number that's not divisible by 3 and plot the same results.\n"}
{"snippet": "def precision(actual, preds):\n    tp = len(np.intersect1d(np.where(actual == 1), np.where(preds == 1)))\n    pp = (preds == 1).sum()\n    return  tp/pp\nprint(precision(y_test, preds_nb))\nprint(precision_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "custom_tree=AC209Tree.build_tree(data_missing_dt_train.as_matrix(), max_depth=5)\nyhat_custom_train = AC209Tree.predict(custom_tree, X_dt_train.as_matrix())\ntrain_score = AC209Tree.accuracy_metric(yhat_custom_train,y_dt_train.as_matrix())\nyhat_custom_test = AC209Tree.predict(custom_tree, X_dt_test.as_matrix())\ntest_score = AC209Tree.accuracy_metric(yhat_custom_test,y_dt_test.as_matrix())\nprint('Optimal Custom Decision Tree Depth: 5')\nprint ('Train score:', train_score)\nprint ('Test score:', test_score)\n", "intent": "**From CV it looks like the optimal max_tree_depth for Custom Decision Tree is 5**\n"}
{"snippet": "y_pred = my_tree.predict(X_train)\naccuracy = metrics.accuracy_score(y_train, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_train, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_train), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the decision tree on the training set\n"}
{"snippet": "p_test = np.zeros_like(y_test)\nfor flip in [False, True]:\n    temp_x = x_test\n    if flip:\n        temp_x = img.flip_axis(temp_x, axis=2)\n    p_test += 0.5 * np.reshape(model.predict(temp_x, verbose=1), y_test.shape)\n", "intent": "Little bit of TTA, predict on both horisontal orientations.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccs = cross_val_score(knn, X, y, cv=10)\nprint accs\nprint np.mean(accs)\n", "intent": "Use 10 folds. How does the performace compare to the baseline accuracy?\n"}
{"snippet": "from sklearn.metrics import r2_score\nimport numpy as np\nx= np.array(trainX)\ny = np.array(trainY)\np = np.poly1d(np.polyfit(x,y,4))\nr2 = r2_score(y,p(x))\nprint \"For training set, R2 is \", r2\nr2 = r2_score(np.array(testY),p(np.array(testX)))\nprint \"For test set, R2 is \", r2\n", "intent": "Try measuring the error on the test data using different degree polynomial fits. What degree works best?\n"}
{"snippet": "predictions = rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "predictions=clf.predict(X_test)\n", "intent": "Testing the outcome with various performance metrics\n"}
{"snippet": "from sklearn import metrics\ny_predict = knn1.predict(X)\nprint(y_predict)\nprint(metrics.accuracy_score(y, y_predict))\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "def predict(model, ts_data, width, nof_points):\n    prediction = []\n    x_test = np.copy(ts_data[-width : ])\n    for i in range(nof_points):\n        prediction.append(model.predict(x_test.reshape((1, -1)))[0])\n        x_test[0 : -1] = x_test[1 : ]\nraise NotImplementedError()\n    return np.array(prediction)\n", "intent": "Now we're going to visualize our prediction by repeating the one-step-ahead prediction. \n"}
{"snippet": "train_preds = meta_model.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2, train_preds)\ntest_preds = meta_model.predict(stacked_test_pred)\nr2_test_stacking = r2_score(y_test, test_preds)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "knn.predict([3, 5, 4, 2])\n", "intent": "In order to **make a prediction**, the new observation must have the **same features as the training observations**, both in number and meaning.\n"}
{"snippet": "def predict(n):\n", "intent": "and as you can see, the network's performance has shot up significantly - despite only making a few changes. \n"}
{"snippet": "def train_classifier(clf, X_train, y_train):\n    start = time()\n    y_pred = clf.predict(features)\n    end = time()    \n    return y_pred\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n", "intent": "Defining functions to train models, predict labels and show metric results: accuracy and f1-score.\n"}
{"snippet": "rf_predictions = rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(\"****** Test Data ********\")\npred = model.predict_classes(test)\nprint(metrics.classification_report(test_labels, pred))\nprint(\"Confusion Matrix\")\nprint(metrics.confusion_matrix(test_labels, pred))\n", "intent": "Use the test dataset to evaluate the model\n"}
{"snippet": "y_pred = my_tuned_model.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluate the perofmrance of the model selected by the grid search on a hold-out dataset\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfor clf in ??:\n    clf.??\n    y_pred = ??\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n", "intent": "Predict 5 classifiers trained above and calculate accuracy using 'accuracy_score'\n"}
{"snippet": "print metrics.classification_report(y,labels)\n", "intent": "Check the labels of the clusters\n"}
{"snippet": "clf2, y_pred2 = cv_svc_pipe(X_train, y_train, X_test, random_state=check_random_state(0))\nscore2 = accuracy_score(y_pred2, y_test)\nprint(\"SVC prediction accuracy = {0:3.1f}%\".format(100.0 * score2))\n", "intent": "- Now we'll build a pipeline curing `CountVectorizer` and `LinearSVC`\n"}
{"snippet": "rmse = np.sqrt(mean_squared_error(b, multib_pred))\nprint (\"The Root mean squared error is : {}\" .format(rmse))\nprint (\"The Mean absolute error is : {}\" .format(mean_absolute_error(b, multib_pred)))\nprint (\"The Correlation Coefficient is : {}\" .format((np.corrcoef(b, multib_pred))[0][1]))\n", "intent": "<font color='red'>TASK MARK: 2</font>\n<br>\n<font color='green'>COMMENT:  Avoid spurious precision. </font>\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "new_model.compile(optimizer='adam', \n              loss=tf.keras.losses.sparse_categorical_crossentropy,\n              metrics=['accuracy'])\nloss, acc = new_model.evaluate(test_images, test_labels)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n", "intent": "Run the restored model.\n"}
{"snippet": "logreg.predict_proba(0)[:, 1]\n", "intent": "**Interpretation:** A 1 unit increase in 'diabetesMed' is associated with a 0.214549 unit increase in the log-odds of 'assorted'.\n"}
{"snippet": "err(y_train, m_gb.predict(X_train))\n", "intent": "What are the errors for both the train and test sets?\n"}
{"snippet": "X16=df2015a[['Q1 Sales','Volume Sold(L)']]\npredictions16 = lm.predict(X16)\npredictions16b = lm1.predict(X16)\nSales2016=sum(predictions16)\nSales2015=df2015['Sales'].sum()\nprint 'The Total sales for 2015: $%d' %Sales2015\nprint \"The Projected Sales for 2016: $%d\" %Sales2016\n", "intent": "Our Adjusted R^2 Value(0.990) is quite good, according to this model, we should be able to predict 2016 sales based on q1 sales.\n"}
{"snippet": "pred = dtree.predict(X_test)\n", "intent": "Let's evaluate our decision tree.\n"}
{"snippet": "print (classification_report(y_test, predicted_y_rf))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "mse= np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint(mse)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "clf3, y_pred3 = cv_svc_pipe_sw(X_train, y_train, X_test, random_state=check_random_state(0))\nscore3 = accuracy_score(y_pred3, y_test)\nprint(\"SVC prediction accuracy = {0:3.1f}%\".format(100.0 * score3))\n", "intent": "- Finally, we'll add English stop words.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df1['Cluster'],kmeans.labels_))\nprint(classification_report(df1['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "- Use the ``accuracy_score`` utility to see how well we did.\n"}
{"snippet": "y_pred = regressor.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred = lr.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred = clf_cv.predict(Xtestlr)\naccuracy_score(y_pred,ytestlr)\n", "intent": "***best C = 0.001 value obtained with scikit_learn.GridSearchCV differs from manually calculated best C : [0.1, 1.0, 10.0, 100.0]***\n"}
{"snippet": "mnb_true = y\nmnb_pred = mnb_classifier.predict(x)\nconfusion_matrix(mnb_true, mnb_pred)\n", "intent": "**d)** Compute the resulting confusion_matrix by using the builtin scikit-learn class and display the result. \n"}
{"snippet": "hide_code\nmodel.load_weights('weights.best.model.hdf5')\nscore = model.evaluate(x_test, y_test)\nscore\n", "intent": "We should have an accuracy greater than 50%\n"}
{"snippet": "from lib import gfx  \nfrom sklearn import metrics\nfrom math import sqrt\ny_pred = xg_reg.predict(test_dmatrix)\nrmse_score = sqrt(metrics.mean_squared_error(y_test, y_pred))\nprint('\\n\ngfx.plot_actual_vs_predicted(y, y_test, y_pred)\n", "intent": "Evaluates the model on the holdout set:\n"}
{"snippet": "def display_prediction(idx):\n    print(predict(W, X_test[idx, :]).argmax())\n    return display_image(X_test[idx].reshape(28, 28))\n", "intent": "Nice, with 10 epochs we get an accuracy of ~89%.\n"}
{"snippet": "trees, trees_weights = adaboost(X, Y, 266)\nYhat_test = adaboost_predict(X_test, trees, trees_weights)\nerr_test = 1-accuracy(Y_test, Yhat_test)\n", "intent": "When numTrees = 266, validation accuracy is the highest and has a value of 0.9542. Corresponding train accuracy is 0.9465.\n"}
{"snippet": "for i in range(2):\n    tau, _ = stats.kendalltau(\n        ridge.predict(X_test[b_test == i]), y_test[b_test == i])\n    print('Kendall correlation coefficient for block %s: %.5f' % (i, tau))\n", "intent": "- and the [Kendall tau](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) for the test data is:\n"}
{"snippet": "print classification_report(y_test, pred)\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions = nb.predict(X_test)\n", "intent": "Define the target and feature set for the test data\n"}
{"snippet": "threshold = 0.5\npreds = predict(X_test,threshold,model)\n", "intent": "Make predictions on testing data. Store predictions in preds variable.\n"}
{"snippet": "def anomaly_score(error, dist):\n    delta = np.abs(error - dist.mean())\n    return dist.cdf(dist.mean() + delta)\n", "intent": "We could also come up with a continuous scoring system using the cumulative density function.\n"}
{"snippet": "test_probs = lr.predict_proba(X_test)[:,1]\nfpr, tpr, thres = roc_curve(y_test, test_probs)\n", "intent": "Onto plotting the ROC curve\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nfrom keras.optimizers import RMSprop\nmodel = Sequential()\nmodel.add(LSTM(128, input_shape=(max_length, voc_size)))\nmodel.add(Dense(voc_size, activation='softmax'))\noptimizer = RMSprop(lr=0.01)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy')\n", "intent": "Let's build a first model and train it on a very small subset of the data to check that it works as expected:\n"}
{"snippet": "print (\"Logistic regression model \")\nlreg1 = smf.logit(formula = 'RESULT ~ RADIUS_MN + TEXTURE_MN + PERIMETER_MN + AREA_MN + SMOOTHNESS_MN + COMPACTNESS_MN + CONCAVITY_MN + CONCAVE_MN + SYMMETRY_MN + FRACTAL_MN ', data = data).fit()\nprint (lreg1.summary())\n", "intent": "** Explore Correlation between house value  and AGE**\n"}
{"snippet": "scaler.fit(banknotes.drop('Class',axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "sess.close()\ntf.reset_default_graph()\n", "intent": "now that we understand this better let's try to use it to predict something the next term in the sequence above.\n"}
{"snippet": "from sklearn.linear_model import Ridge\nridge = Ridge(alpha=0.1).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\n", "intent": "`Ridge` can also be found in `sklearn.linear_model`.\n"}
{"snippet": "CONV_SIZE = face_img.shape\ndef calc_auc_conv2d(rcoefs):\n    coefs = rcoefs.reshape(CONV_SIZE)/rcoefs.sum()\n    score_image = relu(convolve(grey_img,coefs))\n    score_value = score_image.flatten()\n    fpr2, tpr2, _ = roc_curve(ground_truth_labels,score_value)\n    return {'fpr': fpr2, 'tpr': tpr2, 'auc': auc(fpr2,tpr2), 'gimg': score_image}\n", "intent": "Using the RGB instead of the gray value for the CNN\n"}
{"snippet": "subset = 3000\nZ_train_subset = Z_train[:subset]\ny_train_subset = y_train[:subset]\nmodel = SGDRegressor(random_state=42, tol=1e-3, max_iter=1000, verbose=True)\nmodel.fit(Z_train_subset, y_train_subset)\nprint(model.score(Z_test, y_test))\n", "intent": "Looking at the learning curve, the model performs worse as training progresses.\nTherefore, use a subset of training data to stop training early.\n"}
{"snippet": "logmodel = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier\n", "intent": "Create a Logistic Regression Model\n"}
{"snippet": "studA_days = students['A']['days']\nstudA_mor = students['A']['morale']\nAmod = LinearRegression()\nAmod.fit(studA_days[:, np.newaxis], studA_mor)\n", "intent": "---\nI decide to measure the morale with a linear regression, predicting it from the number of days. \nMy model is:\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(features_train, t_train)\n", "intent": "A Naive Bayes classifier is fit on the training set.\n"}
{"snippet": "d_tree = DecisionTreeClassifier(criterion=\"gini\")\nd_tree.fit(train_data, train_data.index) \n", "intent": "Vamos ao que interessa...\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB(alpha=0.1)\nclf\n", "intent": "We can now train a classifier, for instance a Multinomial Naive Bayesian classifier:\n"}
{"snippet": "reg = linear_model.LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))\n", "intent": "Run the following cell to create your model.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=32,\n          epochs=2,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def softmax(inputs):\n    exp_inputs = np.exp(inputs)\n    out = np.divide (exp_inputs, exp_inputs.sum())\n    return out\nsoftmax([1, 0.5, 2])\n", "intent": "- Sigmoid\n$$\\phi(x) = \\frac{1}{1 + \\exp(-x)}$$\n- Softmax\n$$\\phi(x)_k = \\frac{\\exp(- x_k)}{\\sum_{i=1}^K \\exp(-x_i)}$$\n- Linear\n$$\\phi(x) = x$$\n"}
{"snippet": "x = tf.Variable(2.0)\nf = tf.exp(x)\ngrads = tf.gradients(f,[x])\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer()) \n    print(grads[0].eval())\n", "intent": "**Exercise:** Use TensorFlow to compute the derivative of $f(x) = e^x$ at $x=2$.\n"}
{"snippet": "classifier.fit(X_train, y_train)\n", "intent": "Fit (train) or model using the training data\n"}
{"snippet": "nn = Sequential()\n", "intent": "Initialise neural network.\n"}
{"snippet": "lr_kidney = LogisticRegression()\nlr_kidney.fit(X_kidney_train_std, y_kidney_train)\nprint('Train accuracy: {:.5f}'.format(lr_kidney.score(X_kidney_train_std, y_kidney_train)))\nprint('Test accuracy: {:.5f}'.format(lr_kidney.score(X_kidney_test_std, y_kidney_test)))\n", "intent": "4 - Fit a Logistic Regression model to the data, report the training and testing accuracies, and comment on your results.\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nforest = RandomForestRegressor( )\nparams = {'max_depth':[3,4,5],  \n          'max_leaf_nodes':[5,6,7], \n          'min_samples_split':[3,4],\n          'n_estimators': [100]\n         }\nestimator_rfr = GridSearchCV(forest, params, n_jobs=-1,  cv=5,verbose=1)\n", "intent": "---\nInclude a gridsearch \n"}
{"snippet": "df_duplicate = df[df['is_duplicate'] == 1]\ndfp_nonduplicate = df[df['is_duplicate'] == 0]\np = np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()\nn = np.dstack([dfp_nonduplicate[\"question1\"], dfp_nonduplicate[\"question2\"]]).flatten()\nprint (\"Number of data points in class 1 (duplicate pairs) :\",len(p))\nprint (\"Number of data points in class 0 (non duplicate pairs) :\",len(n))\nnp.savetxt('train_p.txt', p, delimiter=' ', fmt='%s')\nnp.savetxt('train_n.txt', n, delimiter=' ', fmt='%s')\n", "intent": "- Creating Word Cloud of Duplicates and Non-Duplicates Question pairs\n- We can observe the most frequent occuring words\n"}
{"snippet": "_X = np.array([[1,2,3], [4,5,6]])\nX = tf.convert_to_tensor(_X)\nout = tf.zeros_like(X)\nprint(out.eval())\nassert np.allclose(out.eval(), np.zeros_like(_X))\n", "intent": "Q2. Let X be a tensor of [[1,2,3], [4,5,6]]. <br />Create a tensor of the same shape and dtype as X with all elements set to zero.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Q10. Apply 5 kernels of height 3, stride 2, and valid padding to x.\n"}
{"snippet": "reg_fake = LinearRegression()\nreg_fake.fit(fake_x.reshape(-1,1), fake_y.reshape(-1,1))\nfake_slope_estimated = reg_fake.coef_[0,0]\nfake_slope_estimated\n", "intent": "And now let's calculate the estimated slope for this fake data\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "print(loss_func(model(xb), yb))\n", "intent": "Let's double-check that our loss has gone down:\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(class_weight='balanced')\nclassifier\n", "intent": "Create a Logistic Regression Model\n"}
{"snippet": "model.fit(x_train, y_train, batch_size = 32, epochs = 10, validation_data = (x_test, y_test), verbose = 2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "he_init = tf.variance_scaling_initializer()\ndef dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None,\n        activation=tf.nn.elu, initializer=he_init):\n    with tf.variable_scope(name, \"dnn\"):\n        for layer in range(n_hidden_layers):\n            inputs = tf.layers.dense(inputs, n_neurons, activation=activation,\n                                     kernel_initializer=initializer,\n                                     name=\"hidden%d\" % (layer + 1))\n        return inputs\n", "intent": "We will need similar DNNs in the next exercises, so let's create a function to build this DNN:\n"}
{"snippet": "start_fc = time.time()\ntrace = trace_callback()\nhistory_fc = model_fc.fit(X_train, y_train,\n                          epochs=100, \n                          batch_size=32,\n                          validation_data=(X_valid, y_valid), \n                          verbose=0, \n                          callbacks=[trace])\nend_fc = time.time()\n", "intent": "Train model. Here we use simple callback (defined before) to trace how many epochs have been done (default output is too long for 100 epochs)\n"}
{"snippet": "loss_func(model(xb), yb), accuracy(model(xb), yb)\n", "intent": "Note that we no longer call `log_softmax` in the `model` function. Let's confirm that our loss and accuracy are the same as before:\n"}
{"snippet": "kmeans2 = KMeans(3, random_state=rng).fit(df[df.columns[:-1]].values)\n", "intent": "minimize the distance between the cluster and a data point in that cluster.  \n"}
{"snippet": "def most_similar(search):\n    title_vecs = np.array([nlp(i).vector for i in news_data['clean_title']])\n    nn = NearestNeighbors(n_neighbors=5, n_jobs=-1)\n    nn.fit(title_vecs)\n    distances, indices = nn.kneighbors(nlp(search).vector.reshape(1,-1))\n    print(news_data.ix[indices[0]])\n", "intent": "* Test this function on a few search terms of your own\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers.core import Dense\nfrom keras.optimizers import SGD\nnp.random.seed(0)\nmodel = Sequential()\nmodel.add(Dense(5, input_dim=784, activation=\"sigmoid\"))\nmodel.add(Dense(10, activation=\"sigmoid\"))\nmodel.compile(optimizer=SGD(lr=0.1), loss='mean_squared_error', metrics=[\"accuracy\"])\n", "intent": " - Dense   < - >  Convolution\n -Fully\n -Connected\n -Network\n"}
{"snippet": "clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y, cv_x_responseCoding, cv_y, clf)\n", "intent": "<h3>5.2.2. Testing the model with best hyper paramters</h3>\n"}
{"snippet": "writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\nwriter.close()\n", "intent": "Step 2: Write the graph info into that directory\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nreg=LinearRegression()\nfit=reg.fit(X_train,y_train)\naccuracy=reg.score(X_test,y_test)\naccuracy=accuracy*100\naccuracy = float(\"{0:.4f}\".format(accuracy))\nprint('Accuracy is:',accuracy,'%')\n", "intent": "Coefficient of Determination used in the score function : $$ 1-\\frac{\\sum(y_{true}-y_{pred})^{2} }{\\sum (y_{true}-y_{mean,true})^{2}} $$\n"}
{"snippet": "def weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n", "intent": "Watchout the ReLU activation functions, hence the need to carefully initialize the weights (dead neurons)\n"}
{"snippet": "g = load_graph(graph)\n", "intent": "Load the graph (model) and the classes into the workspace\n"}
{"snippet": "import xgboost\nclxg = xgboost.XGBRegressor(max_depth=10, learning_rate=0.3, n_estimators=50)\nclxg.fit(X_train, y_train)\n", "intent": "[XGBRegressor](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/sklearn_examples.py)\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "The same linear regression in SKLearn\n"}
{"snippet": "logreg_parameters = {\n    'penalty':['l1','l2'],\n    'C': [0.1, .975, .98, .99, 1.0, 1.01, 1.02, 5],\n    'solver':['liblinear']}\ngs = GridSearchCV(logreg, logreg_parameters, verbose=False, cv=5)\ngs.fit(X, y)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nmodel_c = GridSearchCV(tree.DecisionTreeClassifier(), param_grid)\nmodel_c.fit(X_train, y_train)\n", "intent": "Then we can implement the grid search and fit our model according to the best parameters.\n"}
{"snippet": "dtree=DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "feature_cols = ['TV', 'radio', 'newspaper', 'size_large']\nX = data[feature_cols]\ny = data.sales\nlm2 = LinearRegression()\nlm2.fit(X, y)\nzip(feature_cols, lm2.coef_)\n", "intent": "Let's redo the multiple linear regression and include the **size_large** feature:\n"}
{"snippet": "sess = tf.Session()\n", "intent": "A session places a graph onto a computation device and provides methods to execute it.\n"}
{"snippet": "a = tf.constant(2, name=\"Pedro\")\nb = tf.constant(2, name=\"Maria\")\nx = tf.add(a,b, name=\"add\")\nwith tf.Session() as sess:\n    writer = tf.summary.FileWriter(\"./graphs\", sess.graph)\n    print(sess.run(x))\n    writer.close()\n", "intent": "We can visualize our session using tensorboard\n"}
{"snippet": "model = LeNetOneShot()\ndef eval_func(input_data, output, loss):\n    _, _, target = input_data\n    output1, output2 = output\n    euclidean_distance = F.pairwise_distance(output1, output2)\n    predict = (euclidean_distance > 1.2).float()\n    return predict.eq(target.squeeze()).sum().item()\nlearner = Learner(model, data_loader_train, data_loader_test)\nlearner.fit(model_loss_func, 0.002, num_epochs=100, eval_func=eval_func)\n", "intent": "***BE CAREFUL***\nThe accuracy depends on threshold. Right now it's unreliable. We will find the best threshold later\n"}
{"snippet": "_X = np.diag((1, 2, 3))\nX = tf.convert_to_tensor(_X, tf.float32)\ne= tf.linalg.eigh(X)\nprint(e[0].eval())\nprint(e[1].eval())\n", "intent": "Q12. Compute the eigenvalues and eigenvectors of X.\n"}
{"snippet": "diffed_fare = smt.ARIMA(fare[\"amount\"], order=(0,2,0)).fit()\n", "intent": "We'll start by forecasting revenue.  And then we'll look at costs.\n"}
{"snippet": "with tf.name_scope(\"Layer_1_Conv\"):\n    W1 = tf.Variable(tf.truncated_normal([6,6,1,sizeLayerOne], stddev=0.1), name=\"Weights\")\n    b1 = tf.Variable(tf.ones([sizeLayerOne])/10, name=\"Bias\")\n    Y1 = tf.nn.relu(tf.nn.conv2d(X_img, W1, strides=stridesL1, padding='SAME') + b1)\n", "intent": "Straturile retelei : \n"}
{"snippet": "svc = LinearSVC(C = 0.01)\nt=time.time()\nsvc.fit(X_train, y_train)\nt2 = time.time()\nprint(round(t2-t, 2), 'Seconds to train SVC...')\nprint('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n", "intent": "The best C parameter was found to be 4 since it resulted in the fastest training time.\n"}
{"snippet": "a=[ [0.1, 0.2,  0.3  ],\n    [20,  2,       3   ] ]\nb = tf.Variable(a,name='b')\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.argmax(b,1))\n", "intent": "Gets you the maximum value from a tensor along the specified axis.\n"}
{"snippet": "weights = tf.Variable(tf.random_normal(shape=(X.shape[1],1), stddev=0.01, dtype=tf.float32))\nb = tf.Variable(0.0, dtype=tf.float32)\n", "intent": "__Your code goes here.__ For the training and testing scaffolding to work, please stick to the names in comments.\n"}
{"snippet": "m_km = KMeans(n_clusters=4, random_state=2017).fit(dfn)\nm_km\n", "intent": "Last big drop at 3-4, so we pick 4. Note that this isn't an exact method.\n"}
{"snippet": "tf.reset_default_graph()\nparam_x = tf.placeholder(dtype=tf.float32, name='x')\nparam_y = tf.placeholder(dtype=tf.float32, name='y')\nsum_params = param_x + param_y\n", "intent": "Placeholders can be created with the `tf.placeholder` method:\n"}
{"snippet": "reset_graph()\n", "intent": "implement l1 regularization manually\n"}
{"snippet": "g = Graph()\ng.set_as_default()\n", "intent": "z = Ax + b\nA = 10\nb = 1\nz = 10x + 1\n"}
{"snippet": "clear_session()\nmodel = Sequential(\n    [Dropout(0, input_shape=resnet.output_shape[1:]),\n        Dense(2, activation='softmax')])\nmodel.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Define Simple Model\n"}
{"snippet": "smaller_model = models.Sequential()\nsmaller_model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\nsmaller_model.add(layers.Dense(4, activation='relu'))\nsmaller_model.add(layers.Dense(1, activation='sigmoid'))\nsmaller_model.compile(optimizer='rmsprop',\n                      loss='binary_crossentropy',\n                      metrics=['acc'])\n", "intent": "Now let's try to replace it with this smaller network:\n"}
{"snippet": "from sklearn.svm import SVC\nsvm_clf = SVC()\nsvm_clf.fit(X_train, y_train)\n", "intent": "We are now ready to train a classifier. Let's start with an `SVC`:\n"}
{"snippet": "reset_graph()\ndef relu(X):\n    with tf.name_scope(\"relu\"):\n        w_shape = (int(X.get_shape()[1]), 1)                          \n        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")    \n        b = tf.Variable(0.0, name=\"bias\")                             \n        z = tf.add(tf.matmul(X, w), b, name=\"z\")                      \n        return tf.maximum(z, 0., name=\"max\")                          \n", "intent": "Even better using name scopes:\n"}
{"snippet": "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\ny = tf.get_default_graph().get_tensor_by_name(\"y:0\")\naccuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\ntraining_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")\n", "intent": "Once you know which operations you need, you can get a handle on them using the graph's `get_operation_by_name()` or `get_tensor_by_name()` methods:\n"}
{"snippet": "hidden = tf.layers.dense(dnn_outputs, units=10, activation=tf.nn.elu, kernel_initializer=he_init)\nlogits = tf.layers.dense(hidden, units=1, kernel_initializer=he_init)\ny_proba = tf.nn.sigmoid(logits)\n", "intent": "Now lets add an extra hidden layer with just 10 neurons, and the output layer, with a single neuron:\n"}
{"snippet": "with tf.Session():\n    print(T.eval(feed_dict={y: np.array([0, 1, 2, 3, 9])}))\n", "intent": "A small example should make it clear what this does:\n"}
{"snippet": "prepare_log_dir()\nwriter = tf.summary.FileWriter(log_dir, tf.get_default_graph())\nwriter.close()\n", "intent": "When visualizing the graph, placeholders are depicted as follows:\n"}
{"snippet": "def make_keras_estimator(output_dir):\n  from tensorflow import keras\n  model = keras.models.Sequential()\n  model.add(keras.layers.Dense(32, input_shape=(N_INPUTS,), name=TIMESERIES_INPUT_LAYER))\n  model.add(keras.layers.Activation('relu'))\n  model.add(keras.layers.Dense(1))\n  model.compile(loss = 'mean_squared_error',\n                optimizer = 'adam',\n                metrics = ['mae', 'mape']) \n  return keras.estimator.model_to_estimator(model)\n", "intent": "You can also invoke a Keras model from within the Estimator framework by creating an estimator from the compiled Keras model:\n"}
{"snippet": "def try_out():\n  with tf.Session() as sess:\n    fn = read_dataset(tf.estimator.ModeKeys.EVAL, \n                    {'input_path': 'data', 'batch_size': 8, 'nitems': 5668, 'nusers': 82802})\n    feats, _ = fn()\n    print feats['input_rows'].eval()\ntry_out()\n", "intent": "This code is helpful in developing the input function. You don't need it in production.\n"}
{"snippet": "model = LogisticRegression()\nclf = sklearn.model_selection.GridSearchCV(model, param_grid={'C': Cs})\nclf.fit(Xlr, ylr)\nprint(\"C={}\".format(clf.best_estimator_.C), \"Score={}\".format(clf.best_score_))\n", "intent": "<div class=\"span5 alert alert-success\">\n<h4>SOLUTIONS: Exercise Set IV</h4>\n    </div>\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nsvc = svm.SVC()\nC_range = 10.0 ** np.arange(-2, 5)\ngamma_range = 10.0 ** np.arange(-4, 5)\ndegree_range=[2,3,4]\nkernels=['linear', 'poly', 'rbf']\nparam_grid = dict(C=C_range, gamma=gamma_range, degree=degree_range, kernel=kernels)\ngrid = GridSearchCV(svc, param_grid, scoring='accuracy')\ngrid.fit(X, y)\n", "intent": "Try out different things that we did above, try a grid search for the optimal model hyperparameters.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "base_classifier = DecisionTreeClassifier()\nbase_classifier.fit(train_X, train_y)\n", "intent": "Let's use Decision Tree classifier first. As mentioned, it can process multiclass data by design.\n"}
{"snippet": "from sklearn.cluster import DBSCAN\ndbscn = DBSCAN(eps = 3, min_samples = 3)\ndbscan.fit(Xs)\n", "intent": "Remember to pass an `eps` and `min_samples` of your choice.\n"}
{"snippet": "x = tf.constant(1.0, name = 'input')\nw = tf.Variable(0.8, name = 'weight')\ny = tf.mul(w, x, name = 'output')\n", "intent": "Tensorboard reads the name field that is stored inside each operation. \n"}
{"snippet": "tf.reset_default_graph()\nn_inputs = 28*28\nX = tf.placeholder(tf.float32, shape=[None, n_inputs])\nhidden1 = tf.nn.elu(tf.matmul(X, W1) + b1)\nhidden2 = tf.nn.elu(tf.matmul(hidden1, W2) + b2)\nhidden3 = tf.nn.elu(tf.matmul(hidden2, W3) + b3)\noutputs = tf.matmul(hidden3, W4) + b4\n", "intent": "Finally, we can create a Stacked Autoencoder by simply reusing the weights and biases from the Autoencoders we just trained:\n"}
{"snippet": "with tf.Session() as sess:\n    result = sess.run(sum_params, feed_dict={param_x: 3, param_y: 2})\nprint(result)\n", "intent": "To provide values for the placeholders upon execution, a `feed_dict` has to be provided:\n"}
{"snippet": "import tensorflow as tf\nsoftmax_data = [0.7, 0.2, 0.1]\none_hot_data = [1.0, 0.0, 0.0]\nsoftmax = tf.placeholder(tf.float32)\none_hot = tf.placeholder(tf.float32)\ncross_entropy = - tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\nwith tf.Session() as session:\n    result = session.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data})\n    print(result)\n", "intent": "tf.reduce_sum() function takes an array of numbers and sums them together\ntf.log() is the natural log\n"}
{"snippet": "nl1 = BatchNormalization()\nnl2 = BatchNormalization()\n", "intent": "Now we're ready to create and insert our layers just after each dense layer.\n"}
{"snippet": "fn = theano.function(all_args, error, updates=upd, allow_input_downcast=True)\n", "intent": "We're finally ready to compile the function!\n"}
{"snippet": "def relu(x): return Activation('relu')(x)\ndef dropout(x, p): return Dropout(p)(x) if p else x\ndef bn(x): return BatchNormalization(mode=0, axis=-1)(x)\ndef relu_bn(x): return relu(bn(x))\n", "intent": "These components should all be familiar to you:\n* Relu activation\n* Dropout regularization\n* Batch-normalization\n"}
{"snippet": "correct,total = 0,0\nfor data in testloader:\n    images, labels = data\n    outputs = net(Variable(images).cuda())\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels.cuda()).sum()\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n", "intent": "The results seem pretty good. Let us look at how the network performs on the whole dataset.\n"}
{"snippet": "xt, yt = next(dl)\ny_pred = net2(Variable(xt).cuda())\n", "intent": "First, we will do a **forward pass**, which means computing the predicted y by passing x to the model.\n"}
{"snippet": "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\ndense_1 = lasagne.layers.DenseLayer(input_layer,num_units=50,\n                                   nonlinearity = lasagne.nonlinearities.sigmoid,\n                                   name = \"hidden_dense_layer\")\ndense_output = lasagne.layers.DenseLayer(dense_1,num_units = 10,\n                                        nonlinearity = lasagne.nonlinearities.softmax,\n                                        name='output')\n", "intent": "Defining network architecture\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\nmodel.summary()\n", "intent": "Take couple of minutes and try to play with the number of layers and the number of parameters in the layers to get the best results. \n"}
{"snippet": "print(np.sum(softmax([10, 2, -3])))\n", "intent": "Probabilities should sum to 1:\n"}
{"snippet": "with tf.Session() as sess:\n    result = sum_params.eval(session=sess, feed_dict={param_x: 1, param_y: 2})\nprint(result)\n", "intent": "This is also possible when using the `eval` method:\n"}
{"snippet": "num_epochs = 3\nfor epoch in range(num_epochs):\n    epoch_loss = 0.\n    for x, _ in train_loader:\n        x = Variable(x.view(-1, 784))\n        epoch_loss += svi.step(x)\n    normalizer = len(train_loader.dataset)\n    print(\"[epoch %03d]  average training ELBO: %.4f\" % (epoch, -epoch_loss / normalizer))\n", "intent": "Let's do 3 epochs of training and report the ELBO averaged per data point for each epoch (note that this can be somewhat slow on Azure ML)\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(penalty='l1', C=1.0)\nlr.fit(X_train_std, y_train)\nprint('Training accuracy:', lr.score(X_train_std, y_train))\nprint('Test accuracy:', lr.score(X_test_std, y_test))\n", "intent": "Applied to the standardized Wine data ...\n"}
{"snippet": "spikes = tf.Variable([False]*8, name='spikes')\n", "intent": "Create a boolean vector called `spikes` of the same dimensions as before:\n"}
{"snippet": "w = tf.Variable([0.] * num_coeffs, name=\"parameters\")\ny_model = model(X, w)\ncost = tf.div(tf.add(tf.reduce_sum(tf.square(Y-y_model)),\n                     tf.multiply(reg_lambda, tf.reduce_sum(tf.square(w)))),\n              2*x_train.size)\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n", "intent": "Define the regularized cost function\n"}
{"snippet": "x = tf.placeholder(tf.float32, [None, 24 * 24])\ny = tf.placeholder(tf.float32, [None, len(names)])\nW1 = tf.Variable(tf.random_normal([5, 5, 1, 64]))\nb1 = tf.Variable(tf.random_normal([64]))\nW2 = tf.Variable(tf.random_normal([5, 5, 64, 64]))\nb2 = tf.Variable(tf.random_normal([64]))\nW3 = tf.Variable(tf.random_normal([6*6*64, 1024]))\nb3 = tf.Variable(tf.random_normal([1024]))\nW_out = tf.Variable(tf.random_normal([1024, len(names)]))\nb_out = tf.Variable(tf.random_normal([len(names)]))\n", "intent": "Define the placeholders and variables for the CNN model:\n"}
{"snippet": "ranks = [5, 5, 5]\ncore = Variable(tl.tensor(rng.random_sample(ranks)), requires_grad=True)\nfactors = [Variable(tl.tensor(rng.random_sample((tensor.shape[i], ranks[i]))),\n                 requires_grad=True) for i in range(tl.ndim(tensor))]\n", "intent": "Initialise a random Tucker decomposition of that tensor\n"}
{"snippet": "def net(X):\n    y_linear = nd.dot(X, W) + b\n    yhat = softmax(y_linear)\n    return yhat\n", "intent": "Now we're ready to define our model\n"}
{"snippet": "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), \n              metrics=['accuracy'])\nnetwork_history = model.fit(X_train, Y_train, batch_size=128, \n                            epochs=4, verbose=1, validation_data=(X_val, Y_val))\n", "intent": "Try increasing the number of epochs (if you're hardware allows to)\n"}
{"snippet": "LassoMd = lasso.fit(final_train_df.values,y_train)\nENetMd = ENet.fit(final_train_df.values,y_train)\nKRRMd = KRR.fit(final_train_df.values,y_train)\nGBoostMd = GBoost.fit(final_train_df.values,y_train)\n", "intent": "**Fit the training dataset on every model**\n"}
{"snippet": "with tf.Session() as sess:\n    result = sum_params.eval(session=sess, feed_dict={param_x: [1,2,3], param_y: 3})\nprint(result)\n", "intent": "Instead of scalars, one can also provide vectors (or other arrays) for the placeholders:\n"}
{"snippet": "def cluster_plotter(eps=1.0, min_samples=5):\n    dbkk = DBSCANKK(eps=eps, min_samples=min_samples)\n    dbkk.fit(X)\n    plot_clusters(X, dbkk.point_cluster_labels)\n", "intent": "---\nDon't pass `X` in to the function. We will just use the \"global\" X defined in the jupyter notebook earlier.\n"}
{"snippet": "import tensorflow as tf\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\nsession = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=True))\n", "intent": "---\nIn this notebook, we train a CNN to classify images from the CIFAR-10 database.\n"}
{"snippet": "clf_model_c = MultinomialNB()\n", "intent": "* model c - train - [performance measures][0:4]\n* model c - test - [performance measures][0:4]\n"}
{"snippet": "lm = LogisticRegression()\nlm.fit(X1,y)\nprint lm.coef_\nprint lm.intercept_\n", "intent": "Run your regression line on X1 and interpret your MOMMY AND DADMY coefficients.\n"}
{"snippet": "clf_iris_pr = KNeighborsClassifier()\nclf_iris_pr.fit(X_ir[:,:2], y_ir)\nplot_Kneig(clf_iris_pr, X_ir[:,:2], y_ir)\n", "intent": "Classes are pretty well separated. We can check how the our model would work if we take two other features.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.01))\n    embedded = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn import discriminant_analysis\nLDA = discriminant_analysis.LinearDiscriminantAnalysis()\nLDA.fit(x_tm, y_tm)\n", "intent": "- Build the LDA model on the tumor data\n"}
{"snippet": "plotModel(svm_model, iris.data[:, 2], iris.data[:, 3], iris.target)\npl.xlabel('Petal Length')\npl.ylabel('Petal Width')\n", "intent": "Visualize the result:\n"}
{"snippet": "kmeans.set_params(n_clusters=3)\nkmeans.fit(card)\nlabel = kmeans.labels_\nlabel\n", "intent": "There are no obvious boundaries to split this data set.\nGroup these consumers into three clusters.\n"}
{"snippet": "tf.reset_default_graph()\nvar_theta = tf.Variable(3, name=\"theta\")\nvar_rand = tf.Variable(tf.random_normal(shape = []))\n", "intent": "Upon creation, an initial value has to be provided:\n* in practice, this often is some random number\n"}
{"snippet": "model.fit(X, Y)\n", "intent": "Now that you've initialized your model, it's time to train it.\n"}
{"snippet": "class ConvLayer(nn.Module):\n    def __init__(self, in_channels=3, out_channels=256, kernel_size=9):\n        super(ConvLayer, self).__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels,\n                               out_channels=out_channels,\n                               kernel_size=kernel_size,\n                               stride=1\n                             )\n    def forward(self, x):\n        return F.relu(self.conv(x))\n", "intent": "This is a usual convolution layer that extracts basic features from images.\n"}
{"snippet": "with tf.name_scope('model'):\n    m = tf.Variable(tf.random_normal([1]), name='m')\n    b = tf.Variable(tf.random_normal([1]), name='b')\n    y = m * x_placeholder + b\n", "intent": "Step 3) Define our model.\nHere, we'll use a linear model: *y = mx + b*\n"}
{"snippet": "session = tf.Session(graph=model.graph)\n", "intent": "We need a TensorFlow session to execute the graph.\n"}
{"snippet": "import numpy as np\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\nwith tf.Session(\"grpc://tensorflow3.pipeline.io:8888\", graph=graph, config=sess_config) as session:\n    result = session.run(final_result, feed_dict={ input_array: np.ones([1000]) }, options=run_options)\n    print(result)\n", "intent": "We can now run the graph \n"}
{"snippet": "import numpy as np\nimport tensorflow as tf\nm1 = np.array([[1., 2.], [3., 4.], [5., 6.], [7., 8.]], dtype=np.float32)\nm1_input = tf.placeholder(tf.float32, shape=[4, 2])\nm2 = tf.Variable(tf.random_uniform([2, 3], -1.0, 1.0))\nm3 = tf.matmul(m1_input, m2)\nm3 = tf.Print(m3, [m3], message=\"m3 is: \")\ninit = tf.initialize_all_variables()\n", "intent": "A jupyter notebook version of the simple 'starter' example.\nFirst, define a constant, and define the TensorFlow graph.\n"}
{"snippet": "images = get_next_image_files(batch_size=3)\ndisplay_augment_pipeline(images)\n", "intent": "Visualize each of the data augmentation operation\n=================================================\n"}
{"snippet": "lm1 = smf.ols('DomesticTotalGross ~ ones', data=df)\nfit1 = lm1.fit()\nfit1.summary()\n", "intent": "The results of this model predicts the mean of the outcome variable.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./rnn_time_series_model__testing\")\n    zero_seq_seed = [0. for i in range(num_time_steps)]\n    for iteration in range(len(ts_data.x_data) - num_time_steps):\n        X_batch = np.array(zero_seq_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n        y_pred = sess.run(outputs, feed_dict={X: X_batch})\n        zero_seq_seed.append(y_pred[0, -1, 0])\n", "intent": "** Note: Can give wacky results sometimes, like exponential growth**\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(\"f({0}) = {1}, f'({0})={2}\".format(*sess.run((x,f,gradient), feed_dict={c: 4.2})))\n", "intent": "The gradient can be evaluated like every other tensor:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded_input = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0))\n    return tf.nn.embedding_lookup(embedded_input, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    return tf.contrib.layers.embed_sequence(input_data, \n                                            vocab_size=vocab_size, \n                                            embed_dim=embed_dim)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "test_input = [[0.25, 0.15]]\ngraph=tf.Graph()\nwith tf.Session() as session:\n    tf.global_variables_initializer().run(session=session)\n    output = session.run(out, feed_dict=feed_dict) \n    print(output[0]) \n", "intent": "As with the previous example, we can use `session.run()` to execute this computation graph, and use a `feed_dict` to feed in our input:\n"}
{"snippet": "svc.fit(data,labels)\n", "intent": "Fit SVC with the same training data and predict the same point\n"}
{"snippet": "def get_data(sz, bs):\n    tfms = tfms_from_model(arch, sz, aug_tfms=transforms_side_on, max_zoom=1.1)\n    data = ImageClassifierData.from_csv(PATH, 'train', f'{PATH}labels.csv', test_name='test', \n                                        num_workers=4, val_idxs=val_idxs, suffix='.jpg', \n                                        tfms=tfms, bs=bs)\n    return data if sz > 300 else data.resize(340, 'tmp')\n", "intent": "starting w/ small images, large batch sizes to train model v.fast in beginning; increase image size and decrease batch-size as go along.\n"}
{"snippet": "graph_in = Input((vocab_size, 50))\nconvs = [ ]\nfor fsz in xrange(3, 6):\n    x = Convolution1D(64, fsz, border_mode='same', activation='relu')(graph_in)\n    x = MaxPooling1D()(x)\n    x = Flatten()(x)\n    convs.append(x)\nout = Merge(mode='concat')(convs)\ngraph = Model(graph_in, out)\n", "intent": "We use the functional API to create multiple ocnv layers of different sizes, and then concatenate them.\n"}
{"snippet": "model.fit([conv_feat, trn_sizes], trn_labels, batch_size=batch_size, nb_epoch=3,\n             validation_data=([conv_val_feat, val_sizes], val_labels))\n", "intent": "And when we train the model, we have to provide all the input layers' data in an array:\n"}
{"snippet": "model = RandomForestRegressor(n_jobs=-1)\nmodel.fit(df, y)\nmodel.score(df, y)\n", "intent": "Now we have something we can pass into a Random-Forest Regressor.\n"}
{"snippet": "torch.tensor([[1, 2, 3]],dtype=torch.int32)\n", "intent": "*umm....*\n*...*\nSo it's `torch.tensor` not `torch.Tensor`? Got a lot of errors trying to specify a datatype with capital T. Alright then.\n"}
{"snippet": "classifier.fit(X_train, z_train)\n", "intent": "Fit (train) or model using the training data\n"}
{"snippet": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nn_folds = 5\nparameters = {'max_features': [4, 8, 14, 20, 24]}\nrf = RandomForestClassifier(max_depth=4)\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)\n", "intent": "Let's see how the model performance varies with ```max_features```, which is the maximum numbre of features considered for splitting at a node.\n"}
{"snippet": "np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n", "intent": "https://goo.gl/forms/EeadABISlVmdJqgr2 \n"}
{"snippet": "compiled_expr = theano.function([x, y, alpha], z)\n", "intent": "`theano.function`\nReturns a callable object that will calculate outputs from inputs\n"}
{"snippet": "model.fit(X, y, cat_features=categorical_features_indices)\n", "intent": "Now we would re-train our tuned model on all train data that we have\n"}
{"snippet": "tree_mod = DecisionTreeRegressor()\nrf_mod = RandomForestRegressor()\nada_mod = AdaBoostRegressor()\nreg_mod = LinearRegression()\n", "intent": "> **Step 3:** Now that you have imported the 4 models that can be used for regression problems, instantate each below.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim)))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "y = data.label\nmodel = LogisticRegression() \nmodel.fit(X, y) \nprint model.score(X, y)\nexamine_coefficients(model, X).head()\n", "intent": "- Change the `C` parameter\n    - how do the coefficients change? (use `examine_coeffcients`)\n    - how does the model perfomance change (using AUC)\n"}
{"snippet": "with tf.name_scope('model'):\n    m = tf.Variable(tf.random_normal([1]), name='m')\n    b = tf.Variable(tf.random_normal([1]), name='b')\n    y = m * x_placeholder + b\n", "intent": "Here, we'll use a linear model (e.g., *y = mx + b*)\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def split_Xy(data):\n    return data[:, [0,1,2,3]], data[:, [4]].flatten()\n    raise NotImplementedError()\n", "intent": "Split the data into the features `X` and the labels `y`. The labels are last column of the data\n"}
{"snippet": "X = torch.Tensor([[1,4],[3,1]])\nprint(\"X=\", X)\nsorted_tensor, sorted_indices = X.sort(1)\nprint(\"sorted tensor=\", sorted_tensor)\nprint(\"sorted indices=\", sorted_indices)\nassert np.array_equal(sorted_tensor.numpy(), np.sort(X.numpy(), axis=1))\nassert np.array_equal(sorted_indices.numpy(), np.argsort(X.numpy(), axis=1))\n", "intent": "Q22. Sort X along the second dimension.\n"}
{"snippet": "x = torch.Tensor([1, 2, 3])\ny = torch.Tensor([-1, -2, -3])\nz = x.add(y)\nprint(z)\nassert np.array_equal(z.numpy(), np.add(x.numpy(), y.numpy()))\n", "intent": "Q11. Add x and y element-wise.\n"}
{"snippet": "X = torch.Tensor( [[-1, -2, -3], [0, 1, 2]] )\nY = X.ne(0)\nprint(Y)\nassert np.allclose(Y.numpy(), np.not_equal(X.numpy(), 0))\n", "intent": "Q22. Return 0 if an element of X is 0, otherwise 1.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(...)\n", "intent": "Q8. Return the cumulative product of all elements along the second axis in X.\n"}
{"snippet": "x = torch.Tensor([-3, -2, -1, 1, 2, 3])\ny = 2.\nz = ...\nprint(z)\n", "intent": "Q18. Compute the remainder of x / y element-wise.\n"}
{"snippet": "x = torch.Tensor([1, 2])\ny = torch.Tensor([3, 4])\nz = x.matmul(y)\nprint(z)\nassert z==x.dot(y)\n", "intent": "Q1. Compute the inner product of two vectors x and y.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\ny = torch.Tensor([3, 4])\nz = ...\nprint(z)\n", "intent": "Q3. Compute a matrix-vector product of matrix X and vector y.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nclfs = {'lr': LogisticRegression(random_state=0),\n        'mlp': MLPClassifier(random_state=0),\n        'rf': RandomForestClassifier(random_state=0)}\n", "intent": "In the dictionary:\n- the key is the acronym of the classifier\n- the value is the classifier (with random_state=0)\n"}
{"snippet": "from keras.models import load_model\nG = load_model('G.h5') \nD = load_model('D.h5') \n", "intent": "- Load $\\mathcal{G}$ and $\\mathcal{D}$ into memory\n- Save them into python variables named `G` and `D` respectively\n"}
{"snippet": "model_2 = ensemble.RandomForestClassifier(n_estimators=500, max_depth=12, random_state=0)\nmodel_2.fit(X_train1, y_train1)\n", "intent": "Provo ad aumentare la profondita' del Random Forest per aumentare la ROC:\n"}
{"snippet": "elastic = sklearn.linear_model.ElasticNetCV()\nelastic.fit(weather[non_syd_col], weather.SYDNEY)\nprint elastic.score(weather[non_syd_col], weather.SYDNEY)\nprint elastic.intercept_\nzip(non_syd_col, elastic.coef_)\n", "intent": "The cross validation (CV) has sorted out the alpha, correct?\n"}
{"snippet": "from IPython.display import display\nfrom keras.preprocessing.image import array_to_img\nimport numpy as np\ndigit_idxs = [np.argwhere(y_mnist == d).flatten()[0] for d in range(10)]\nX_train = X_mnist[digit_idxs]\nfor x in X_train:\n    img = array_to_img(np.expand_dims(x, axis=0), data_format='channels_first')\n    display(img)\nX_train.shape\n", "intent": "The following code creates a training set consisting of one image digit per class.\n"}
{"snippet": "model.fit(X_train,Y_train,validation_data=(X_val,Y_val), epochs=20)\n", "intent": "- Optimize `model` on `X_train` and `Y_train`\n"}
{"snippet": "data_vectors = np.array([get_phrase_embedding(l) for l in data])\n", "intent": "Finally, let's build a simple \"similar question\" engine with phrase embeddings we've built.\n"}
{"snippet": "model.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n", "intent": "Let's now compile and train the model.\n"}
{"snippet": "def conv_block(x, num_filters, filter_size, stride=(2,2), mode='same', act=True):\n    x = Convolution2D(num_filters, filter_size, filter_size, subsample=stride, border_mode=mode)(x)\n    x = BatchNormalization(mode=2)(x)\n    return Activation('relu')(x) if act else x\ndef res_block(initial_input, num_filters=64):\n    x = conv_block(initial_input, num_filters, 3, (1,1))\n    x = conv_block(x, num_filters, 3, (1,1), act=False)\n    return merge([x, initial_input], mode='sum')\n", "intent": "ConvBlock  \nResBlock\n    - no activation on the last layer\n"}
{"snippet": "inp = Input(shape=(SEQ_LEN,))\nemb = Embedding(VOCAB_SIZE, EMBEDDING_LEN_50, input_length=SEQ_LEN)(inp)\nlstm = LSTM(100)(emb)\npreds = Dense(2, activation = 'softmax')(lstm)\nsimple_lstm_model = Model(inputs=inp, outputs=preds)\nsimple_lstm_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\nsimple_lstm_model.summary()\n", "intent": "First, let us try to use a very simple model using the LSTM layer. We will NOT be working with the pre-trained embeddings for now\n"}
{"snippet": "ones = torch.FloatTensor([1]).cuda()\n", "intent": "Lets create labels for real images\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.regularizers import l1\nENCODING_DIM = 32\nencoder = Sequential(name='encoder')\nencoder.add(Dense(units=ENCODING_DIM, activity_regularizer=l1(1e-5), input_shape=[784] ))\nencoder.summary()\n", "intent": "- Define an encoder which maps a flattened MNIST image to a vector of size `ENCODING_DIM`\n- Use a keras sequential model\n- Do not compile your model!\n"}
{"snippet": "def logistic_regression_grad(X, y, theta):\n    grad = (sigmoid(X @ theta) - y) @ X \n    return grad\ntheta = [0, 1]\nsimple_X_1 = np.hstack([np.arange(10)/10, np.arange(10)/10 + 0.75])\nsimple_X = np.vstack([np.ones(20), simple_X_1]).T\nsimple_y = np.hstack([np.zeros(10), np.ones(10)])\nlinear_regression_grad(simple_X, simple_y, theta)\n", "intent": "And then complete the gradient function. You should get a gradient of about $[0.65, 0.61]$ for the given values $\\theta$ on this example dataset.\n"}
{"snippet": "regr_test = LinearRegression()\nregr_test.fit(X_reduced, y)\nregr_test.coef_\n", "intent": "The above plot indicates that the lowest training MSE is reached when doing regression on 18 components.\n"}
{"snippet": "db2 = DBSCAN(eps=1, min_samples=10)\ndb2.fit(iris_data_scaled)\ndb2.labels_\n", "intent": "What happens as we alter either $\\epsilon$ or min_samples?\nLet's first alter min_samples:\n"}
{"snippet": "dt = DecisionTreeRegressor(random_state=seed)\n", "intent": "Define a decision tree. ([DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n"}
{"snippet": "encoding_dim = 32\nimage_size = mnist.train.images.shape[1]\ninputs_ = tf.placeholder(tf.float32, (None, image_size), name='inputs')\ntargets_ = tf.placeholder(tf.float32, (None, image_size), name='targets')\nencoded = tf.layers.dense(inputs_, encoding_dim, activation=tf.nn.relu)\nlogits = tf.layers.dense(encoded, image_size, activation=None)\ndecoded = tf.nn.sigmoid(logits, name='output')\nloss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\ncost = tf.reduce_mean(loss)\nopt = tf.train.AdamOptimizer(0.001).minimize(cost)\n", "intent": "> name parameter in tensor\n> image_size = mnist.train.images.shape[1]\n"}
{"snippet": "class Length(layers.Layer):\n    def call(self, inputs, **kwargs):\n        return K.sqrt(K.sum(K.square(inputs), -1))\n    def compute_output_shape(self, input_shape):\n        return input_shape[:-1]\n", "intent": "Definitions will emerge here for all parts of Capsule Layer\n- Class Length\n- Class Mask\n- Squashing Function\n- Class Capsule Layer\n"}
{"snippet": "mean_knn_n2 = KNeighborsClassifier(n_neighbors=1,\n                              weights='uniform',\n                              p=2,\n                              metric='minkowski')\naccuracy_crossvalidator(X, Y, mean_knn_n2, cv_indices)\n", "intent": "As you can see the mean cross-validated accuracy is very high with 5 neighbors. \nLet's see what it's like when we use only 1 neighbor:\n"}
{"snippet": "print('Gradient Boost with 1200 trees, learning rate of .2')\ngb5 = GradientBoostingRegressor(learning_rate=.2,n_estimators=1200)\nstage_score_plot(gb5, X_train, y_train, X_test, y_test, .2)\n", "intent": "- Looking better, let's try even smaller.\n"}
{"snippet": "for i in range(10,90,10):\n    print('Gamma :{}'.format(i))\n    svm_imbalanced = SVC(kernel='rbf',gamma=i).fit(X_small_ann,y_small_ann)\n    decision_boundary(svm_imbalanced ,X_small_ann,y_small_ann)\n", "intent": "- No decision boundaries. Try changing the parameters.\n"}
{"snippet": "decoder = Sequential(name='decoder')\ndecoder.add(Dense(units=64, activation='relu', input_shape=[ENCODING_DIM]))\ndecoder.add(Dense(units=128, activation='relu'))\ndecoder.add(Dense(units=784, activation='sigmoid'))\ndecoder.summary()\n", "intent": "- Define a decoder which maps a compressed version of an image back to a flattened image\n- Use a keras sequential model\n- Do not compile your model!\n"}
{"snippet": "from keras.optimizers import SGD\nmodel1.compile(loss='categorical_crossentropy',\n               optimizer=SGD(lr=0.04),\n               metrics=['accuracy'])\nmodel1.fit(X_train, y_train, nb_epoch=20, batch_size=16)\n", "intent": "Compile and fit this smaller model. You should get very similar accuracy on the test data as we saw above.\n"}
{"snippet": "poly=LinearRegression()\npoly.fit(x_train_pr,y_train)\n", "intent": "Now let's create a linear regression model \"poly\" and train it.\n"}
{"snippet": "MaxPooling2D(pool_size=2, strides=1)\n", "intent": "If we would instead like to use a stride of 1, but still keep the size of the window at 2x2, then we would use:\n"}
{"snippet": "net = SingleHiddenLayerNetwork()\nopt = tf.train.AdamOptimizer(learning_rate=0.01)\nacc_history = np.zeros(50)\nfor epoch in range(50):\n  accuracy = tfe.metrics.Accuracy()\n  for (xb, yb) in tfe.Iterator(train_dataset.shuffle(1000).batch(32)):\n    opt.apply_gradients(loss_grad(net, xb, yb))\n    accuracy(tf.argmax(net(tf.constant(xb)), axis=1), tf.argmax(tf.constant(yb), axis=1))\n  acc_history[epoch] = accuracy.result().numpy()\n", "intent": "Let us rewrite the optimization code, this time by accumulating the training accuracy at each epoch:\n"}
{"snippet": "x = tf.Variable(5)\n", "intent": "Coding the Linear Function in Tensorlow\n"}
{"snippet": "n_hidden_layer = 256\nweights = {'hidden_layer':tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n           'output_layer':tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))}\nbiases = {'hidden_layer':tf.Variable(tf.random_normal([n_hidden_layer])),\n          'output_layer':tf.Variable(tf.zeros([n_classes]))}\n", "intent": "* Network architecture\n"}
{"snippet": "model = NearestNeighbors(metric = 'cosine', algorithm = 'brute')\nmodel.fit(interactions_mtx_knn)\n", "intent": "And run knn in two lines\n"}
{"snippet": "logit_data = [2.0, 1.0, 0.1]\nlogits = tf.placeholder(tf.float32)\nsoftmax = tf.nn.softmax(logits)    \nwith tf.Session() as sess:\n    output = sess.run(softmax, feed_dict={logits: logit_data} )\n    print(output)\n", "intent": "Easy as that! ```tf.nn.softmax()``` implements the softmax function for you. It takes in logits and returns softmax activations.\n"}
{"snippet": "model.fit(X_train, y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "autoencoder.fit(X_train,X_train)\n", "intent": "- Optimize your autoencoder on the training data\n"}
{"snippet": "with tf.Session() as session:\n    result = session.run(c)\n    print \"c =: {}\".format(result)\n", "intent": "<div align=\"right\">\n<a href=\"\n</div>\n<div id=\"operations\" class=\"collapse\">\n```\nc=tf.sin(a)\n```\n</div>\n"}
{"snippet": "for k in range(1, 30)[::-1]:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    print(k, knn.score(X_test, y_test))\n", "intent": "Are our choice of hyper-parameters and features the same as they were when we were validating based on a training set alone?\n"}
{"snippet": "universe_end_date = pd.Timestamp('2016-01-05', tz='UTC')\nuniverse_tickers = engine\\\n    .run_pipeline(\n        Pipeline(screen=universe),\n        universe_end_date,\n        universe_end_date)\\\n    .index.get_level_values(1)\\\n    .values.tolist()\nuniverse_tickers\n", "intent": "With the pipeline engine built, let's get the stocks at the end of the period in the universe we're using.\n"}
{"snippet": "model.fit(x_train,y_train,\n          epochs = 10,\n          batch_size = int(x_train.shape[0]*0.1),\n          verbose = 0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.ensemble import ExtraTreesClassifier\net = ExtraTreesClassifier(n_estimators=1000, class_weight='balanced')\n", "intent": "Probemos ahora un Extra Tree.\n"}
{"snippet": "model.fit(points)\n", "intent": "**Step 4:** Use the `.fit()` method of `model` to fit the model to the array of points `points`.\n"}
{"snippet": "parameters = {'C' : [0.001, 0.1, 1, 10, 100], 'penalty' : ['l1', 'l2']}\nlr_model = model_performance(GridSearchCV(LogisticRegression(random_state=42), param_grid=parameters, cv=skf), cv=True)\n", "intent": "Without any optimization, we can predict patients who will convert to AD with 89% accuracy. Now, the classifier will be optimized.\n"}
{"snippet": "model_b = sm.OLS.from_formula(\"SalePrice ~ scale(OverallQual) + scale(OverallCond) + scale(GrLivArea) + scale(I(GrLivArea**2)) + scale(I(GrLivArea**3)) + scale(KitchenQual) + scale(GarageCars) + scale(BsmtQual) + scale(YearBuilt) + C(Neighborhood) + C(MSZoning)\", data=df)\nresult_b = model_b.fit()\nprint(result_b.summary())\n", "intent": "$$ y = OQ + OC + GA + GA^2 + GA^3 + KQ  + GC + BQ + YB + Category $$\n"}
{"snippet": "kmeans.fit(college_data.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "model.fit(X_train,Y_train,validation_data=(X_val,Y_val), epochs=20)\n", "intent": "- Optimize `model` on `X_train` and `Y_train`\n"}
{"snippet": "if USE_PRETRAINED:\n    with open('pretrained/xgb_grid_search_eta_50.pkl', 'rb') as f:\n        grid = pickle.load(f)\nelse:      \n    xgb_param_grid = {'eta':[0.5,0.4,0.3,0.2,0.1,0.075,0.05,0.04,0.03]}\n    grid = GridSearchCV(XGBoostRegressor(num_boost_round=50, gamma=0.2, max_depth=8, min_child_weight=6,\n                                         colsample_bytree=0.6, subsample=0.9),\n                        param_grid=xgb_param_grid, cv=5, scoring=mae_scorer)\n    grid.fit(train_x, train_y.values)\n", "intent": "First, we plot different learning rates for a simpler model (50 trees):\n"}
{"snippet": "g = Graph()\n", "intent": "<p>\n<strong>z = Ax +b </strong><br/>\nA = 10 <br/>\nb = 1 <br/>\nz = 10x + 1 where x is a placeholder <br/>\n</p>\n"}
{"snippet": "session = tf.Session(graph=linreg_graph)\nsession.run(init)\n", "intent": "We start off by initializeing the Variables:\n"}
{"snippet": "train_model(\n    learning_rate=0.00002,\n    steps=500,\n    batch_size=5\n)\n", "intent": "Click below for one possible solution.\n"}
{"snippet": "hidden_neurons = 200\nmodel = Sequential()\nmodel.add(LSTM(hidden_neurons, input_shape=(input_len, num_chars)))\nmodel.add(Dense(num_chars, activation='softmax'))\nprint(model.summary())\n", "intent": "Here we will make the RNN model.\n"}
{"snippet": "activations = model.run_graph([image], [\n    (\"input_image\",        model.keras_model.get_layer(\"input_image\").output),\n    (\"res4w_out\",          model.keras_model.get_layer(\"res4w_out\").output),  \n    (\"rpn_bbox\",           model.keras_model.get_layer(\"rpn_bbox\").output),\n    (\"roi\",                model.keras_model.get_layer(\"ROI\").output),\n])\n", "intent": "In some cases it helps to look at the output from different layers and visualize them to catch issues and odd patterns.\n"}
{"snippet": "sz=375\ntfms = tfms_from_model(arch, sz, aug_tfms=augs, crop_type=CropType.NO)\ndata = ImageClassifierData.from_csv(PATH, 'train_crop', f'{PATH}train_int.csv', test_name='test',\n                                      val_idxs=val_idxs, tfms=tfms, bs=bs)\n", "intent": "Now another training iteration with 340x340 images\n"}
{"snippet": "model2_ridge = lm.Ridge(fit_intercept=False).fit(X, y)\n", "intent": "Fit the model (by default, $\\alpha$ = 1).\n"}
{"snippet": "tree2 = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=50)\ntree2.fit(X, y)\n", "intent": "Train a decision tree by limiting:\n* the maximum number of questions (depth)\n* the minimum number of samples in each leaf\n"}
{"snippet": "model = get_model(sentences)\n", "intent": "The following code cell takes a while to complete.\n"}
{"snippet": "logreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\nmodel_gridsearch = LogisticRegression()\nselector_gridsearch = GridSearchCV(model_gridsearch, logreg_parameters, cv = 5)\nselector_gridsearch.fit(X_train2, y_train)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "naive_bayes_model = naive_bayes.MultinomialNB()\n", "intent": "Instantiate the Naive Bayes predictor from scikit-learn with the training data. \n"}
{"snippet": "tf.logging.set_verbosity(tf.logging.INFO)\nshutil.rmtree('taxi_trained', ignore_errors=True) \nmodel = tf.contrib.learn.DNNRegressor(hidden_units=[32, 8, 2],\n      feature_columns=make_feature_cols(), model_dir='taxi_trained')\nmodel.fit(input_fn=make_input_fn(df_train), steps=100);\n", "intent": "<h3> Deep Neural Network regression </h3>\n"}
{"snippet": "l = lbl.fit(train['disable_communication'])\ntrain['disable_communication'] = l.transform(train['disable_communication'])\ntest['disable_communication'] = l.transform(test['disable_communication'])\n", "intent": "disable_communication is a categorical variable and so we are encode it\n"}
{"snippet": "model_1Dconv_w_d = Sequential()\nks = 5\nmodel_1Dconv_w_d.add(Convolution1D(filters=32, kernel_size=ks, padding='causal', dilation_rate=1, input_shape=(128, 1)))\nmodel_1Dconv_w_d.add(Convolution1D(filters=32, kernel_size=ks, padding='causal', dilation_rate=2))\nmodel_1Dconv_w_d.add(Convolution1D(filters=32, kernel_size=ks, padding='causal', dilation_rate=4))\nmodel_1Dconv_w_d.add(Convolution1D(filters=32, kernel_size=ks, padding='causal', dilation_rate=8))\nmodel_1Dconv_w_d.add(Dense(1))\nmodel_1Dconv_w_d.add(Lambda(slice, arguments={'slice_length':look_ahead}))\nmodel_1Dconv_w_d.compile(optimizer='adam', loss='mean_squared_error')\nmodel_1Dconv_w_d.summary()\n", "intent": "Here we define a Neural network with 1D convolutions and \"causal\" padding, this time with dilation rate, so we are able to look back longer in time.\n"}
{"snippet": "svc = svm.SVC(kernel='poly',C=1, degree=2, probability=True).fit(X_training,y_training)\nsvc.score(X_training,y_training)\n", "intent": "To show that Support Vector are actually quality data. We turn back to Polynomial Kernel degree 2.\n"}
{"snippet": "test_input = [[0.25, 0.15]]\ngraph=tf.Graph()\nwith tf.Session() as session:\n    tf.global_variables_initializer().run(session=session)\n    output = \n    print(output[0]) \n", "intent": "As with the previous example, we can use `session.run()` to execute this computation graph, and use a `feed_dict` to feed in our input:\n"}
{"snippet": "add = tf.add(5, 2) \nsub = tf.sub(10, 4) \nmul = tf.mul(2, 5)  \ndiv = tf.div(10, 5) \nwith tf.Session() as sess:\n    output = [sess.run(add), sess.run(sub), sess.run(mul), \n              sess.run(div)]\n    print(output)\n", "intent": "It also works if you feed it only `{a: 'hi'}`, i.e. the relevant placeholder value(s).\n"}
{"snippet": "best_et_reg = ExtraTreesRegressor(n_estimators=75,\n                                  max_depth=max_depth,\n                                  n_jobs=-1,\n                                  random_state=18)\n", "intent": "We recall that we did this tuning with `n_estimators=75`.\n"}
{"snippet": "deep_model = Sequential()\ndeep_model.add(Dense(units=6, activation='relu', input_dim=32))\ndeep_model.add(Dense(units=6, activation='relu'))\ndeep_model.add(Dense(units=2, activation='softmax'))\n", "intent": "For this network, we simply add an additional hidden layer of 6 nodes\n"}
{"snippet": "lr.fit(pm2_regr.dropna().iloc[:, :-1], pm2_regr.dropna().iloc[:, -1])\nlr.score(pm2_regr.dropna().iloc[:, :-1], pm2_regr.dropna().iloc[:, -1])\n", "intent": "Dropping all rows with at least 1 NA:\n"}
{"snippet": "bagg_lr = BaggingClassifier(lr)\n", "intent": "There is no improvement and the results are way worse than before, let's try bagging:\n"}
{"snippet": "reset_graph()\nmy_second_graph = tf.get_default_graph()\nx = tf.Variable(3, name=\"x\")\ny = tf.Variable(4, name=\"y\")\nf = x*x*y + y + 2\nwith tf.Session() as sess:\n    x.initializer.run()\n    y.initializer.run()\n    result = f.eval()\nprint(result)\n", "intent": "Now try this second graph:\n"}
{"snippet": "n = _\nkmeans = KMeans(n_clusters=n)\n", "intent": "Agora podemos iniciar o algoritmo de clustering.\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.neighbors import KNeighborsRegressor\nk = 25 \nneigh = KNeighborsRegressor(n_neighbors=k)\n", "intent": "Now let's build the KNN model.  \n"}
{"snippet": "grid_search = GridSearchCV(svc,param_grid=param_grid,n_jobs=-1,verbose = 3,refit=True)\ngrid_search.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "b_fit = cluster.KMeans(n_clusters=2).fit(b_data)\nplot_clusters(b_data, b_fit)\n", "intent": "Let's use KMeans to fit the three other (b,c,d) 2D datasets with `n_clusters=2` and generate similar plots. Which fits give the expected results?\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nmodel = Sequential()\nmodel.add(Dense(20, input_dim=1000))\nmodel.add(Activation('relu'))\nmodel.add(Dense(2))\nmodel.add(Activation('sigmoid'))\nmodel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "deep_model = Sequential()\ndeep_model.add(Dense(units=6, activation='relu', input_dim=4))\ndeep_model.add(Dense(units=6, activation='relu'))\ndeep_model.add(Dense(units=2, activation='softmax'))\n", "intent": "For this network, we simply add an additional hidden layer of 6 nodes\n"}
{"snippet": "scaler.fit(data.drop('TARGET CLASS',axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "lr.fit(titanic_temp[['Pclass']], titanic_temp.Survived)\n", "intent": "Errm, predicting based on age doesn't work\n"}
{"snippet": "Xtrain = X[:1000,:]\nytrain = y[:1000]\nXtest = X[1000:,:]\nytest = y[1000:]\ndigits_rf = RandomForestClassifier(n_estimators=200, criterion='entropy')\ndigits_rf.fit(Xtrain,ytrain);\n", "intent": "Let's identify the misclassified images.\n"}
{"snippet": "x = tf.placeholder(tf.float32, [None, 784]) \ny = tf.placeholder(tf.float32, [None, 10]) \nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\npred = tf.nn.softmax(tf.matmul(x, W) + b) \n", "intent": "Network definition.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed     = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "knn =KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=500)\nrfc.fit(X_train, y_train)\n", "intent": "Now its time to train our model!\n**Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.**\n"}
{"snippet": "model_sigmoid = Sequential()\nmodel_sigmoid.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))\nmodel_sigmoid.add(Dense(128, activation='sigmoid'))\nmodel_sigmoid.add(Dense(output_dim, activation='softmax'))\nmodel_sigmoid.summary()\n", "intent": " <h3>  MLP + Sigmoid activation + SGDOptimizer </h3>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Text Classification with Universal Sentence Encoder-localhost</H1></u></center>\n"}
{"snippet": "deep_model = Sequential()\ndeep_model.add(Dense(units=6, activation='relu', input_dim=20))\ndeep_model.add(Dense(units=6, activation='relu'))\ndeep_model.add(Dense(units=2, activation='softmax'))\n", "intent": "For this network, we simply add an additional hidden layer of 6 nodes\n"}
{"snippet": "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=100)\n", "intent": "Use your X_test, y_test from the `train_test_split` step for the `validation_data` parameter.\n"}
{"snippet": "lr = LinearRegression()\nlr.fit(Xstd, baseball.age.values)\nprint 'Intercept:', lr.intercept_\nprint 'Coefs:', lr.coef_\n", "intent": "**Build a linear regression predicting age from the standardized height and weight data. Interpret the coefficients.**\n"}
{"snippet": "weights = {\n        'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n        'wd1': tf.Variable(tf.random_normal([7*7*128, 1024])),\n        'out': tf.Variable(tf.random_normal([1024, n_classes]))\n        }\nbiases = {\n        'bc1': tf.Variable(tf.random_normal([32])),\n        'bd1': tf.Variable(tf.random_normal([1024])),\n        'out': tf.Variable(tf.random_normal([n_classes]))\n        }\n", "intent": "<a id=\"define-network-paramters-that-will-be-fit\"></a>\n"}
{"snippet": "ag = AgglomerativeClustering(n_clusters=2)\nag.fit(X)\npredicted_labels = ag.labels_\n", "intent": "Next we'll ask `AgglomerativeClustering` to return back two clusters for Iris:\n"}
{"snippet": "toyregr_skl = linear_model.LinearRegression()\nresults_skl = toyregr_skl.fit(x_train,y_train)\nbeta0_skl = toyregr_skl.intercept_\nbeta1_skl = toyregr_skl.coef_[0]\nprint(\"(beta0, beta1) = (%f, %f)\" %(beta0_skl, beta1_skl))\n", "intent": "Below is the code for sklearn.\n"}
{"snippet": "epochs = 10\nbatch_size = 512\nmodel.fit(x_train, y_train, \n          epochs=epochs, \n          batch_size=batch_size,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "m.fit(df_raw.drop('SalePrice', axis=1), df_raw.SalePrice)\n", "intent": "Below `m.fit` will fail with \n*ValueError: could not convert string to float: 'Conventional'*\nas the data is not yet ready...\n"}
{"snippet": "net = nn.Sequential(\n    nn.Linear(28*28, 100),\n    nn.ReLU(),\n    nn.Linear(100, 100),\n    nn.ReLU(),\n    nn.Linear(100, 10),\n    nn.LogSoftmax()\n).cuda()\n", "intent": "We will begin with the highest level abstraction: using a neural net defined by PyTorch's `Sequential` class.  \n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n", "intent": "Then we can built our regression model:\n"}
{"snippet": "from torchvision import datasets, transforms\nfrom torch import nn, optim\ntrain_data = datasets.MNIST('../data', train=True, download=True,\n                   transform=transforms.ToTensor())\ntrain_loader = torch.utils.data.DataLoader(train_data,\n                                           batch_size=batch_size, shuffle=True, **{})\nVAE_MNIST = VAE(fc1_dims=fc1_dims, fc21_dims=fc21_dims, fc22_dims=fc22_dims, fc3_dims=fc3_dims, fc4_dims=fc4_dims)\noptimizer = optim.Adam(VAE_MNIST.parameters(), lr=lr)\nfor epoch in range(1, epochs + 1):\n    train(epoch, train_loader, VAE_MNIST, optimizer)\n", "intent": "Run the box below to train the model using the hyperparameters you entered above.\n"}
{"snippet": "_x = np.array([1, 2, 3], np.int32)\n_y = np.array([4, 5, 6], np.int32)\n_z = np.array([7, 8, 9], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nz = tf.convert_to_tensor(_y)\n", "intent": "Q8. Add x, y, and z element-wise.\n"}
{"snippet": "_x = np.array([1., 2., 3.], np.float32)\nx = tf.convert_to_tensor(_x)\n", "intent": "Q17. Compute $e^x$, element-wise.\n"}
{"snippet": "_x = np.array([10, 20, 30], np.int32)\n_y = np.array([2, 3, 7], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.mod(x, y)\nout2 = x % y\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n_out = _x % _y\nassert np.array_equal(out1.eval(), _out)\n", "intent": "Q6. Get the remainder of x / y element-wise.\n"}
{"snippet": "_x = np.array([[1, 2], [3, 4]])\n_y = np.array([[1, 2], [1, 2]])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout = tf.pow(x, y)\nprint(out.eval())\n_out = np.power(_x, _y)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q17. Compute $x^y$, element-wise.\n"}
{"snippet": "_X= np.random.rand(1, 2, 3, 4)\nX = tf.convert_to_tensor(_X)\n", "intent": "Q6. Transpose the last two dimensions of X.\n"}
{"snippet": "_X = np.array(\n[[1, 0, 0, 0],\n [0, 2, 0, 0],\n [0, 0, 3, 0],\n [0, 0, 0, 4]])\nX = tf.convert_to_tensor(_X)\nout = tf.diag_part(X)\nprint(out.eval())\n_out = np.diag(_X)\nassert np.array_equal(out.eval(), _out)\n", "intent": "Q2. Extract the diagonal of X.\n"}
{"snippet": "_X = np.array([[1,2,3], [4,5,6]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q1. Compute the cumulative sum of X along the second axis.\n"}
{"snippet": "_X = np.arange(1, 10).reshape(3, 3)\nX = tf.convert_to_tensor(_X)\n", "intent": "Q11. Return the elements of X, if X < 4, otherwise X*10.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [-1,-2,-3,-4], \n     [-10,-20,-30,-40],\n     [10,20,30,40]])\nX = tf.convert_to_tensor(_X)\nout = tf.unsorted_segment_sum(X, [1, 0, 1, 0], 2)\nprint(out.eval())\n", "intent": "Q8. Compute the sum along the second and fourth and \nthe first and third elements of X separately in the order.\n"}
{"snippet": "from keras.models import Sequential\nmodel = Sequential()\n", "intent": "We first need to create a sequential model\n"}
{"snippet": "tf.reset_default_graph()\nx = tf.constant([1, 1, 2, 2, 2, 3], tf.float32)\n", "intent": "Q06. Compute half the L2 norm of `x` without the sqrt.\n"}
{"snippet": "tf.reset_default_graph()\nx = tf.constant([0, 2, 1, 3, 4], tf.int32)\nembedding = tf.constant([0, 0.1, 0.2, 0.3, 0.4], tf.float32)\noutput = tf.nn.embedding_lookup(embedding, x)\nwith tf.Session() as sess:\n    print(sess.run(output))\n", "intent": "Q9. Map tensor `x` to the embedding.\n"}
{"snippet": "_X = np.arange(1, 11).reshape([2, 5])\nX = tf.convert_to_tensor(_X)\nout = tf.split(X, 5, axis=1) \nprint([each.eval() for each in out])\ncomp = np.array_split(_X, 5, 1) \nassert np.allclose([each.eval() for each in out], comp)\n", "intent": "Q16. Let X be a tensor of<br/>\n[[ 1  2  3  4  5]<br />\n [ 6  7  8  9  10]].<br />\nSplit X into 5 same-sized tensors along the second dimension.\n"}
{"snippet": "V = tf.Variable(tf.truncated_normal([1, 10]))\nW = ...\ninit_op = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init_op) \n    _V, _W = sess.run([V, W])\n    print(_V)\n    print(_W)\n    assert np.array_equiv(_V * 2.0, _W)\n", "intent": "Q3-4. Complete this code.\n"}
{"snippet": "w1 = tf.Variable(1.0)\nw2 = tf.Variable(2.0)\nw3 = tf.Variable(3.0)\nout = w1 + w2 + w3\ninit_op = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init_op) \n    print(sess.run(out))\n", "intent": "Q2. Complete this code.\n"}
{"snippet": "with tf.Session() as sess:\n    ones = tf.ones(shape=[2,3])\n    print(sess.run(ones))\n    assert np.allclose(sess.run(ones), np.ones([2, 3]))\n", "intent": "Q3. Create a tensor of shape [2, 3] with all elements set to one.\n"}
{"snippet": "y_pred = tf.nn.softmax(layer_fc2)\n", "intent": "Apply softmax to the final fully connected layer.\n"}
{"snippet": "old_alex_graph = tf.Graph()\nwith old_alex_graph.as_default():\n    saver = tf.train.import_meta_graph(\"../week_06/saved_models/alexnet.meta\")\n", "intent": "Now, let's bring back in the AlexNet we (possibly struggled) to create last week.\n"}
{"snippet": "fake_breakout_x,fake_breakout_y = create_fake_linear(100, 3.2, 4.2, 10)\n", "intent": "1\\. Create a new pair of fake $x$ and $y$ that is 100 long called `fake_breakout_x` and `fake_breakout_y`, with `a=3.2`, `b=4.2` and `noise_sd=10`. \n"}
{"snippet": "from keras.layers import Dense\nnumber_inputs = 3\nnumber_hidden_nodes = 4\nmodel.add(Dense(units=number_hidden_nodes,\n                activation='relu', input_dim=number_inputs))\n", "intent": "Next, we add our first layer. This layer requires you to specify both the number of inputs and the number of nodes that you want in the hidden layer.\n"}
{"snippet": "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=100)\n", "intent": "Use your X_test, y_test from the `train_test_split` step for the `validation_data` parameter.\n"}
{"snippet": "value = torch.Tensor(np.array(train_w2v_set))\ntext = Variable(value)   \nencoder_values, decoder_values = model(text)\n", "intent": "We will convert training dataset first into a torch tensor, and form a differentiable Variable.\n"}
{"snippet": "architecture = resnet34\ntransformer = tfms_from_model(architecture, sz=32, max_zoom=1.1)\ndata = ImageClassifierData.from_arrays(path=data_path, \n                                       trn=(X_train, y_train), \n                                       val=(X_val, y_val),\n                                       test=(X_test),\n                                       bs=64,\n                                       classes=class_names,\n                                       tfms=transformer)\n", "intent": "Basic model, built from lesson 1. Names and arguments are explicit to facilitate understanding\n"}
{"snippet": "def activation(X):\n    return np.tanh(X)\n", "intent": "We'll use $\\tanh$ activations for our hidden units, so let's define that real quick:\n"}
{"snippet": "param_test4 = {\n    'subsample':[i/10.0 for i in range(6,10)],\n    'colsample_bytree':[i/10.0 for i in range(6,10)]\n}\ngsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n                                        min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n                       param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch4.fit(train[predictors],train[target])\n", "intent": "Tune subsample and colsample_bytree\n"}
{"snippet": "model.fit(X, y)\nprint(model)\n", "intent": "Next, we fit the model to our data using the fit method.\n"}
{"snippet": "criterion = ClassNLLCriterion()\nnet = Sequential()\nnet.add(Linear(2, 4))\nnet.add(ReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\nprint net\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "model.fit(x_train, y_train, epochs=1000, batch_size=50, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from keras import regularizers\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000, kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "deep_model = Sequential()\ndeep_model.add(Dense(units=6, activation='relu', input_dim=2))\ndeep_model.add(Dense(units=6, activation='relu'))\ndeep_model.add(Dense(units=2, activation='softmax'))\n", "intent": "For this network, we simply add an additional hidden layer of 6 nodes\n"}
{"snippet": "def init_weights(shape):\n    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(init_random_dist)\n", "intent": "Function to help intialize random weights for fully connected or convolutional layers.\n"}
{"snippet": "from tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\ndef create_model():\n    model = Sequential()\n    model.add(Dense(6, input_dim=4, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n", "intent": "Now set up an actual MLP model using Keras:\n"}
{"snippet": "mlpc = MLPClassifier([30, 10], max_iter = 1000)\nparameters = {'activation':['relu', 'logistic', 'identity']}\ngcnnc = GridSearchCV(mlpc, parameters, cv=4)\ngcnnc.fit(...)\nresults = gcnnc.cv_results_\nprint(results['mean_test_score'])\n", "intent": "*First, Lets confirm which activation function works the best for classification. Is the difference as significant as for the regression problem?*\n"}
{"snippet": "from keras.models import load_model\nmodel.save('my_model.h5')\nmodel2 = load_model('my_model.h5')\n", "intent": "It is really important to be able to reload the model after you've been training it for hours on end (usually). So save the model.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=3)\n", "intent": "Then, we can create a KNN model and specify the parameter `k`. Create a model with `k = 3`.\n"}
{"snippet": "detection_graph = load_graph(SSD_GRAPH_FILE)\nimage_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\ndetection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\ndetection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\ndetection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n", "intent": "Below we load the graph and extract the relevant tensors using [`get_tensor_by_name`](https://www.tensorflow.org/api_docs/python/tf/Graph\n"}
{"snippet": "result = smf.logit(formula='default_yes ~ balance + income', data=default).fit()\nresult.summary()\n", "intent": "Computing stand errors of coefficents of logistic regression using bootstrap\n"}
{"snippet": "grid_search = GridSearchCV(lr, parameters, n_jobs=-1, scoring='roc_auc', cv=skf)\ngrid_search = grid_search.fit(X, y)\ngrid_search.best_estimator_\n", "intent": "**Answer:** 2.\n**Solution:**\n"}
{"snippet": "knn.fit(X, y)\n", "intent": "<b>Step3:</b> Fit the model with the data (aka 'model training')\n- Model is learning the relationship between X and y\n- Occours in-place\n"}
{"snippet": "multi_lin_model = sklearn.linear_model.LinearRegression()\n", "intent": "We start again by creating a [```LinearRegression```](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n"}
{"snippet": "net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n", "intent": "We now do the max-pooling on the output of the convolutional layer.\n"}
{"snippet": "logreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\ngs = GridSearchCV(lr_model, logreg_parameters, verbose=False, cv=5)\ngs.fit(X, y)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "kmeans.fit(df.drop('Private', axis=1)) \n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense\nmodel = Sequential()\nmodel.add(Embedding(10000,64))\nmodel.add(LSTM(64))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n", "intent": "i) [2 point] Now built a LSTM model by replacing the simple RNN layter in the above model with a LSTM layer. Print a summary of the LSTM model.\n"}
{"snippet": "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\nclf.fit(X_train, y_train)\n", "intent": "Any difference if we use entropy instead of gini as the split criteria?\n"}
{"snippet": "encoder_embedding = Embedding(input_dim=num_words,\n                              output_dim=embedding_size,\n                              name='encoder_embedding')\n", "intent": "This is the embedding-layer.\n"}
{"snippet": "import tensorflow as tf\nconfig = tf.ConfigProto()\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nprint(config,\"\\n\",sess)\n", "intent": "* Use *log_device_placement=True*. This tells placer to log msg whenever a node is \"placed\".\n"}
{"snippet": "def linear_model(img):\n    return ylogits, NCLASSES\n", "intent": "Let's start with a very simple linear classifier. All our models will have this basic interface -- they will take an image and return logits.\n"}
{"snippet": "knn = KNeighborsRegressor(n_neighbors=3, weights='distance')\nknn_bag = BaggingRegressor(knn, n_estimators=20)\nknn_boost = AdaBoostRegressor(knn, n_estimators=100)\n", "intent": "Armed with this we can now try bagging and boosting. For this we need the Bagging regressor - I take the very simple approach used earlier.\n"}
{"snippet": "from keras.layers import Dense\nnumber_inputs = 3\nnumber_hidden_nodes = 4\nmodel.add(Dense(units=number_hidden_nodes,\n                activation='relu',\n                input_dim = number_inputs))\n", "intent": "Next, we add our first layer. This layer requires you to specify both the number of inputs and the number of nodes that you want in the hidden layer.\n"}
{"snippet": "regr = linear_model.LinearRegression()\nregr.fit(X_tr,y_tr)\n", "intent": "Fit the model on the training data.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ngrid = {'n_neighbors': np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=3) \nknn_cv.fit(x,y)\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))\n", "intent": "We only need is one line code that is GridSearchCV\ngrid: K is from 1 to 50(exclude)\n"}
{"snippet": "gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, loss='huber', random_state=5)\nscore = cv_evaluation(gboost)\nprint('Gradient Boosting Score: {:.4f}'.format(score.mean()))\n", "intent": "* **Gradient Boosting Regression**:\n"}
{"snippet": "model_random_forest_100trees.fit(X_text_train_tfidf, yelp_data_final_update['Review_category_cool_notcool'])\n", "intent": "Random Forest for cool not cool:\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nnew_docs = ['He watches basketball and baseball', 'Julie likes to play basketball', 'Jane loves to play baseball']\nnew_term_freq_matrix = tfidf_vectorizer.transform(new_docs)\nprint tfidf_vectorizer.vocabulary_\nprint new_term_freq_matrix.todense()\n", "intent": "And we can fit new observations into this vocabulary space like so:\n"}
{"snippet": "train_sizes, train_scores, valid_scores = learning_curve(LogisticRegression(), X4, y4,)\n", "intent": "Age and positive nodes are both negatively correlated with survival while operation year is positively correlate.\n"}
{"snippet": "from sklearn.svm import SVC \nclf = SVC(kernel='linear')\nclf.fit(X, y)\n", "intent": "Now we'll fit a Support Vector Machine Classifier to these points:\n"}
{"snippet": "lstm_size = 128\nnum_layers = 2\nlstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\ndrop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=0.5)\nenc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n", "intent": "This is just going to be a regular LSTM cell with some dropout. Also, we'll be using the embedding from above to feed the input to this unit\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, '/tmp/mlp_regression.ckpt')\n    preds = sess.run(y_pred, feed_dict={X: X_test})\n    print(sess.run(loss, feed_dict={X: X_test, y: y_test}))\n", "intent": "Load the saved model and make predictions on the test set.\n"}
{"snippet": "deep_model = Sequential()\ndeep_model.add(Dense(units = 6,\n                     activation = 'relu',\n                     input_dim = 2))\ndeep_model.add(Dense(units = 6,\n                     activation = 'relu'))\ndeep_model.add(Dense(units = 2,\n                activation = 'softmax'))\n", "intent": "For this network, we simply add an additional hidden layer of 6 nodes\n"}
{"snippet": "svm1 = SVC( kernel = \"linear\", C = 1.0, random_state = 0 )\nsvm2 = SGDClassifier( loss = \"hinge\" ) \nsvm1.fit( x_train_sd, y_train )\n", "intent": "Use `SGDClassifier` for online learning, which scales better with large dataset.\n"}
{"snippet": "logmodel = LogisticRegression()\nlogmodel.fit(X_train, y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "age=baseball.age.values\nfrom sklearn import linear_model\nclf = linear_model.LinearRegression()\nclf.fit(Xstd,baseball.age.values)\nprint 'Intercept:', clf.intercept_\nprint 'Coefs:', clf.coef_\n", "intent": "**Build a linear regression predicting age from the standardized height and weight data. Interpret the coefficients.**\n"}
{"snippet": "lda = models.LdaModel(corpus, num_topics=6, \n                            update_every=2,\n                            id2word=dictionary, \n                            chunksize=15, \n                            passes=10)\nlda.show_topics()\n", "intent": "The LDA model can be built immediately:\n"}
{"snippet": "x = T.vector(\"x\")\ny = T.vector(\"y\")\nw = theano.shared(1., name=\"w\")\nb = theano.shared(0., name=\"b\")\nprint(\"Initial model:\", w.get_value(), b.get_value())\n", "intent": "We first declare Theano symbolic variables:\n"}
{"snippet": "missing = np.isnan(y)\nmod = LogisticRegression()\nmod.fit(X[~missing], y[~missing])\n", "intent": "Next, we create a `LogisticRegression` model, and fit it using the non-missing observations.\n"}
{"snippet": "cifar_model.add(Conv2D(filters=64, kernel_size=(5,5), activation='relu'))\ncifar_model.add(Activation('relu'))\n", "intent": "**(f)** Add another ``Conv2D`` layer identical to the others except with 64 filters instead of 32. Add another ``relu`` activation layer.\n"}
{"snippet": "def conv2d(x,W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n", "intent": "There are already a function in TF to perform the convolution.  \nWe are going to create a wrapper aourn it that sets the parameters for us\n"}
{"snippet": "X_extra = np.hstack((X, np.sqrt(X[:, [0]]**2 + X[:, [1]]**2)))\nplot_learning_curve(LinearSVC(C=0.25), \"LinearSVC(C=0.25) + distance feature\", \n                    X_extra, y, ylim=(0.8, 1.2),\n                    train_sizes=np.linspace(.1, 1.0, 5))\n", "intent": "Ways to decrease underfitting:\n * **use more or better features** (the distance from the origin should help!)\n"}
{"snippet": "model.fit(...\n", "intent": "Then we fit our model using the the total request rate and cpu.\n"}
{"snippet": "regression = linear_model.LinearRegression()\n", "intent": "LinearRegression object of Sklearn is capable of creating Polynomial functions\n"}
{"snippet": "ridge = Ridge(random_state=17, alpha=0.1)\n", "intent": "**Train the same Ridge with all available data, make predictions for the test set and form a submission file.**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(10, input_dim=4, activation='relu'))\nmodel.add(Dense(20, activation='relu'))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "X = tf.placeholder(tf.float32, shape=(None,), name=\"x\")\nY = tf.placeholder(tf.float32, shape=(None,), name=\"y\")\nw = tf.Variable([0., 0.], name=\"parameter\", trainable=True)\ny_model = tf.sigmoid(-(w[1] * X + w[0]))\n", "intent": "Create our parameters and placeholders for X and Y to feed them with the data above:\n"}
{"snippet": "def cluster_posts(weights, n_clusters=5):\n    laplacian = sparse.csgraph.laplacian(weights, normed=True, use_out_degree=True)\n    laplacian = sparse.csr_matrix(laplacian)\n    eigenvalues, eigenvectors = sparse.linalg.eigsh(laplacian, k=n_clusters, which = 'SA')\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(eigenvectors)\n    cluster_labels = kmeans.labels_\n    return [cluster_labels, eigenvectors[:,1:3], eigenvalues]\n", "intent": "The following function clusters topics using Laplacian Graph Eigendecomposition:\n"}
{"snippet": "G = graphs.Graph(W_sparse)\nG.compute_laplacian('normalized')\nlaplacian = G.L\n", "intent": "Compute the graph Laplacian\n"}
{"snippet": "plot_model(model_mlp_multi, X_train, y_train, (X_valid, y_valid))\n", "intent": "The learning procedure gitters a lot.\nLet's look at the decision surface\n"}
{"snippet": "net = ModelTorch()\nnet.add(Linear(X.shape[1], 64))\nnet.add(BatchMeanSubtraction())\nnet.add(ELU())\nnet.add(Linear(64, y.shape[1]))\nnet.add(SoftMax())\nnet.compile(ClassNLLCriterion(), nesterov, {\"learning_rate\": 0.05, \"momentum\": 0.6}, accuracy=True)\nnp.random.seed(21)\nhist_BN = net.fit(X, y, iteration_hist=True, epochs=5)\n", "intent": "Batch Mean Subtraction\n"}
{"snippet": "inp_bench = Input(shape=(X_train.shape[1],), name=\"input_a\")\ndnn_bench = Dense(**config)(inp_bench)\ndnn_bench = Dense(**config)(dnn_bench)\ndnn_bench = Dense(**config)(dnn_bench)\ndnn_bench = Dense(**config)(dnn_bench)\ndnn_bench = Dense(**config)(dnn_bench)\ndnn_bench = Dense(units=10, activation=\"softmax\")(dnn_bench)\n", "intent": "Let's benchmark our model against the same model but trained from scratch on the same 500 examples per class dataset\n"}
{"snippet": "multi_lin_model.fit(\n", "intent": "Next fit the model on the data:\n"}
{"snippet": "model_column_twolayers.summary()\n", "intent": "Let\"s take a look at our final model now:\n"}
{"snippet": "model_lstm = Sequential()\nmodel_lstm.add(LSTM(6, input_shape=(look_back, 1)))\nmodel_lstm.add(Dense(horizont))\n", "intent": "We have very few data. So we need to build very simple model not tot overfit\n"}
{"snippet": "sess = tf.Session()\nprint(sess.run(node3))\n", "intent": "Values are now available:\n"}
{"snippet": "mglearn.plots.plot_dbscan()\n", "intent": "* Density-Based Spatial Clustering of Applications with Noise.\n* It does not require the user to set the number of clusters a priori.\n"}
{"snippet": "from sklearn import tree\ndtc = tree.DecisionTreeClassifier(max_depth = 5)\ndtc.fit(X_train,y_train)\ndtc.score(X_test,y_test)\n", "intent": "7) Train a Decision Tree classifier with maximum depth 5 and plot the decision tree. How does performance compare?\n"}
{"snippet": "theta_suff_avp,_ =\\\nstructure_perceptron.estimate_perceptron(training_set,\n                                         features.word_suff_feats,\n                                         tagger_base.classifier_tagger,\n                                         20,\n                                         all_tags)\n", "intent": "The cell below takes 50 seconds to run on my laptop.\n"}
{"snippet": "L = tf.nn.conv2d(X, W, strides=[1,2,2,1], padding='SAME')\nL\n", "intent": "* Must have strides[0] = strides[3] = 1\n* 28+2(Padding)-3(kernel size)+1 = 28\n* 32 = the number of filters\n"}
{"snippet": "shared_weights = theano.shared(np.random.rand(64))\ninput_X = T.matrix()\ninput_y = T.ivector()\n", "intent": "Firstly, let's try initialize the weights randomly.\n"}
{"snippet": "model = Sequential()\nmodel.add(LSTM(50, activation='relu', \n               input_shape=(n_steps, n_features) \n              ))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n", "intent": "A Vanilla LSTM is an LSTM model that has a single hidden layer of LSTM units, and an output layer used to make a prediction.\n"}
{"snippet": "simple_lin_model.fit(total_page_views_M, cpu_usage)\n", "intent": "Then we fit our model using the the total request rate and cpu. The coefficients found are automatically stored in the ```simple_lin_model``` object.\n"}
{"snippet": "learning_curve = trainModel('lenet_dropout_075', {'X_train' : preProcessDataSet(X_train), 'y_train' : y_train, \n                            'X_valid' : preProcessDataSet(X_valid), 'y_valid' : y_valid }, \n           epochs = 10, batch_size = 128, train_keep_prob_fc = 0.75, \n           rate = 0.001, model_func = Net_1)\nbuildLearningCurve(learning_curve)\n", "intent": "Lets try to add dropout to default LeNet model and see if accuracy will increase. Keep propapility is 0.75\n"}
{"snippet": "km2 = KMeans(n_clusters=2,init='k-means++',n_init=10,max_iter=300,random_state = 0)\nfit =km2.fit(set1)\n", "intent": "------------------------------------------------\n"}
{"snippet": "knn = KNeighborsClassifier()\nparam_dict = {\n    'n_neighbors':range(1,11),\n    'weights':['distance', 'uniform'],\n}\ngsk = GridSearchCV(knn, param_dict, cv=3).fit(X_train, Y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "loader = DataLoader()\ny = np.zeros([len(loader.yes_image_name) + len(loader.no_image_name)])\ny[0:len(loader.yes_image_name)] = 1\nallfeatures = None\nwith open ('allfeatures.file','rb') as file:\n    allfeatures = pickle.load(file)\nclassifier = svm.SVC(probability=True) \nclassifier.fit(allfeatures, y)\nprint(\"Training SVM Done.\")\n", "intent": "You may skip the following two steps if you have downloaded svmmodel.file\n"}
{"snippet": "import tensorflow as tf\nhello_constant = tf.constant('Hello World!')\nwith tf.Session() as sess:\n    output = sess.run(hello_constant)\n    print(output)\n", "intent": "Simple \"Hello World!\" TensorFlow test\n"}
{"snippet": "def try_out():\n  with tf.Session() as sess:\n    fn = read_dataset(tf.estimator.ModeKeys.EVAL, \n                    {'input_path': 'data', 'batch_size': 4, 'nitems': 5668, 'nusers': 82802})\n    feats, _ = fn()\n    print(feats['input_rows'].eval())\n    print(feats['input_rows'].eval())\ntry_out()\n", "intent": "This code is helpful in developing the input function. You don't need it in production.\n"}
{"snippet": "model = DecisionTreeClassifier()\ndepth_parm = np.arange(1, 12, 1)\nnum_samples_parm = np.arange(5,95,10)\nparameters = {'max_depth' : depth_parm,\n             'min_samples_leaf' : num_samples_parm}\nclf = GridSearchCV(model, parameters, cv=10)\nclf.fit(X_train,y_train)\nprint(clf.score(X_test, y_test))\n", "intent": "First, consider a decision tree, doing a grid search over hyperparameters.\n"}
{"snippet": "y_1 = activation(torch.mm(features, W1) + B1)\ny_2 = activation(torch.mm(y_1, W2) + B2)\nprint(y_2)\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "def gen(Z, w,  w2, w3, wx):\n    h = relu(batchnorm(T.dot(Z,w)))\n    h2 = relu(batchnorm(T.dot(h,w2)))\n    h2 = h2.reshape((h2.shape[0], ngf*2, 7, 7))\n    h3 = relu(batchnorm(trconv(h2,w3, output_shape=(None,None,7,7),filter_size=(5,5),subsample=(2, 2), border_mode=(2, 2))))\n    x = sigmoid(trconv(h3,wx, output_shape=(None, None, 14, 14),\n                                   filter_size=(5, 5), subsample=(2, 2), border_mode=(2, 2)))\n    return x\n", "intent": "**Problem 4 (1pt)** Implement generator by plugging in the hidden variables and weights. Hint: if the sizes mismatched, theano will complain.\n"}
{"snippet": "m_true = 0.5\nb_true = 0.25\nnp.random.seed(23)\ndef theModel(xmin=0, xmax = 1, num=20):\n    sigma = 0.1\n    x = np.linspace(xmin, xmax, num)\n    y = b_true + m_true * x - sigma * np.random.randn(len(x))\n    return(x, y)\nx, y = theModel(num = 50)\n", "intent": "Just as in the $\\texttt{intro2pp-bm}$ notebook we will first generate some data from a linear model.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=4, algorithm='brute').fit(train, train_labels)\n", "intent": "Next, as before, we fit a model to the training data:\n"}
{"snippet": "W = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n", "intent": "<center><h2>Variables</h2></center>\n"}
{"snippet": "log.fit(X_train,y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "import numpy as np\nrnd_data = np.random.multivariate_normal([0, 0], [[3, 4], [5, 6]], 1000)\nprint(rnd_data.shape)\nwhitened_data, whitening_matrix = whiten(rnd_data)\nassert(np.allclose(np.identity(2), np.cov(whitened_data)))\nprint(\"Passed\")\n", "intent": "Identity covariance test:\n"}
{"snippet": "detection_graph = load_graph(FASTER_RCNN_GRAPH_FILE)\nimage_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\ndetection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\ndetection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\ndetection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n", "intent": "Below we load the graph and extract the relevant tensors using [`get_tensor_by_name`](https://www.tensorflow.org/api_docs/python/tf/Graph\n"}
{"snippet": "def make_model(y=0.):\n    gamma = 1.\n    theta = pm.Normal('$\\theta$', mu=0., tau=1., value=0.)\n    @pm.stochastic(observed=True)\n    def likelihood(value=y, gamma=gamma, theta=theta):\n        return gamma * pm.normal_like(value, theta, 1.)\n    return locals()\n", "intent": "All, right. Now, let's program this thing in pysmc and compare the results.\nWe start with the model:\n"}
{"snippet": "scaler.fit(dataframe.drop('TARGET CLASS',axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nX = mammals.loc[:, ['body']]\npf = PolynomialFeatures(degree=3, include_bias=False)\npf.fit(X)\npf.transform(X)\n", "intent": "**sklearn has a \"transformer\" that generates polynomial terms**\n"}
{"snippet": "test_acc = []\nfor i in range(1, X_tsc.shape[0]+1):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_tsc, y_train)\n    test_acc.append(knn.score(X_vsc, y_val))\n", "intent": "- Store the test accuracy in a list.\n- Plot the test accuracy vs. the number of neighbors.\n"}
{"snippet": "model.fit(\n    trainvectors, \n    Y_train, \n    batch_size=128, \n    nb_epoch=20, \n    verbose=2 \n)\n", "intent": "Now we can invoke the **fit** method of the network, which will perform the training process. It is done as follows\n"}
{"snippet": "X = df.values\nkm = KMeans(n_clusters=15).fit(X)\nkm.labels_\n", "intent": "Cool we ended up with 15 dark spots.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded_input = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=.01))\n    embedded_input = tf.nn.embedding_lookup(embedded_input, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\n", "intent": "Create an empty MLP, i.e. empty linear stack of layers\n"}
{"snippet": "vgg_model.add(Dense(1000, activation='softmax'))\n", "intent": "Finally a softmax layer to predict the categories. There are 1000 categories and hence 1000 neurons.\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-softmax'))\n    feed = {x: train_x,\n            y: train_y}\n    out = sess.run(tf.nn.softmax(logits), feed_dict=feed)\n    out_train_y =sess.run(tf.argmax(out, axis=1))\n    label_train_y = sess.run(tf.argmax(train_y, axis=1))\n    test_los,test_acc = sess.run([cost,accuracy], feed_dict=feed)\n    print(\"Train accuracy: {:.4f}%\".format(test_acc*100))\n", "intent": "show the lowest confidence for correct output\nshow the highest confidence for wrong output\nBUT keep the number of correct prediction at 95%\n"}
{"snippet": "def SpectralClustering(X):\n    return \n", "intent": "**Part C**: Complete the function `SpectralClustering` to return an indicator vector `z` corresponding to the cluster assignments. \n"}
{"snippet": "outputs, state = tf.nn.dynamic_rnn(cells, x_one_hot, initial_state=initial_state)\n", "intent": "* **Output**: This is the actual output from LSTM cell.\n* **State**: This is a output from LSTM cell that pass to next time step.\n"}
{"snippet": "from keras.models import load_model\nmodel = load_model('cats_and_dogs_small_2.h5')\nmodel.summary()  \n", "intent": "Intermediate activations\n"}
{"snippet": "clf = sklearn.neighbors.KNeighborsClassifier(gs.best_params_['n_neighbors'])\nclf.fit(X_train, Y_train)\nprint clf.score(X_train, Y_train)\nprint clf.score(X_test, Y_test)\n", "intent": "Test the performance of our tuned KNN classifier on the test set.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(784,)))\nmodel.add(Activation('relu'))\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n", "intent": "This boils down to defining the network as\n"}
{"snippet": "sparse_matrix = tf.sparse.SparseTensor(dense_shape=[3,3], indices=[[0,0],[1,1],[2,2]], values=[1,1,1])\nsparse_matrix.values\n", "intent": "** tf.SparseTensor ** - A sparse representation Tensor\nFor sparse Tensors, a more efficient representation is `tf.SparseTensor`.\n"}
{"snippet": "lr_atemp = LinearRegression()\nX = bikes.loc[:, ['atemp']]\nlr_atemp.fit(X, y)\n", "intent": "- Create another `LinearRegression` instance that is fit using `atemp` only, and print the coefficients.\n"}
{"snippet": "model = LogisticRegression()\nmodel.fit(X, y)\nprint model.coef_\n", "intent": "__What can we do with an estimator?__ \nInference!\n"}
{"snippet": "from neon.models import Model\nmlp = Model(layers=layers)\nfrom neon.callbacks.callbacks import Callbacks\ncallbacks = Callbacks(mlp, train_set, output_file=args.output_file,\n                      progress_bar=True)\nmlp.fit(train_set, optimizer=optimizer, num_epochs=args.epochs, cost=cost,\n        callbacks=callbacks)\n", "intent": "Model class contains all aspect of the neural network, architecture, cost function, optimizers; it exposes a `fit` that is used to train the network.\n"}
{"snippet": "gridsearchcv = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "svc2 = svm.LinearSVC(C=100, loss='hinge', max_iter=1000)\nsvc2.fit(data[['X1', 'X2']], data['y'])\nsvc2.score(data[['X1', 'X2']], data['y'])\n", "intent": "It appears that it mis-classified the outlier.  Let's see what happens with a larger value of C, again using `.score`:\n"}
{"snippet": "def inception(image, reuse):\n    preprocessed = tf.multiply(tf.subtract(tf.expand_dims(image, 0), 0.5), 2.0)\n    arg_scope = nets.inception.inception_v3_arg_scope(weight_decay=0.0)\n    with slim.arg_scope(arg_scope):\n        logits, _ = nets.inception.inception_v3(\n            preprocessed, 1001, is_training=False, reuse=reuse)\n        logits = logits[:,1:] \n        probs = tf.nn.softmax(logits) \n    return logits, probs\nlogits, probs = inception(image, reuse=False)\n", "intent": "Next, we load the Inception v3 model.\n"}
{"snippet": "su =  SuperLearnerClassifier()\nsu.fit(X_train, y_train)\n", "intent": "Train a Super Learner Classifier using the prepared dataset\n"}
{"snippet": "import numpy as np\nnp.random.seed(0)\nx = np.random.random(size=(15, 1))\ny = 3 * x.flatten() + 2 + np.random.randn(15)\ny.shape\n", "intent": "To demonstrate the use of simple linear regression with sci-kit learn, we will first create sample data in the form of NumPy arrays.\n"}
{"snippet": "model = make_pipeline(PolynomialFeatures(degree=5), RandomForestRegressor(n_estimators=60, min_samples_split=1))\nmodel.fit(Xtrain, ytrain)\n", "intent": "Create, fit, tune and predict using the `RandomForestRegression` model here:\n"}
{"snippet": "model_param_values = pickle.load(open('blvc_googlenet.pkl'))['param values']\nlasagne.layers.set_all_param_values(cnn_output_layer, model_param_values)\n", "intent": "Load the pretrained weights into the network\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 4))\nnet.add(ReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "def negative_log_likelihood(X, y, w):\n    m = sigmoid(X.dot(w))\n    nll = -1*(np.dot(y, np.log(m+1e-15))+np.dot(1-y,np.log(1-m+1e-15)))\n    return nll\n", "intent": "As defined in Eq. 33\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    inputs = tf.contrib.layers.embed_sequence(input_data,vocab_size,embed_dim)\n    return inputs\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "W_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n", "intent": "In order to build a deep network, we stack several layers of this type. The\nsecond layer will have 64 features for each 5x5 patch.\n"}
{"snippet": "polynomial_features = PolynomialFeatures(degree=15)\nlinear_regression = LinearRegression()\nlinear_regression = ElasticNet(alpha=0.001)\npipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                     (\"linear_regression\", linear_regression)])\npipeline.fit(X[:, np.newaxis], y)\n", "intent": "Here, we're building linear polynomial models of degrees 2, 4 and 15.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nclf= DecisionTreeClassifier(random_state=42)\nclf.fit(training_inputs, training_classes)\n", "intent": "Now create a DecisionTreeClassifier and fit it to your training data.\n"}
{"snippet": "tf.reset_default_graph()\nbatch_size = 32\nwith tf.device('/cpu:0'):\n    real_images, _, _ = data_provider.provide_data(\n        'train', batch_size, MNIST_DATA_DIR)\ncheck_real_digits = tfgan.eval.image_reshaper(\n    real_images[:20,...], num_cols=10)\nvisualize_digits(check_real_digits)\n", "intent": "<a id='unconditional_input'></a>\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.metrics import roc_auc_score\nparams = {'n_neighbors': arange(1, 200, 5)}\ngrid_searcher = GridSearchCV(KNN(),\\\n                             params, cv=5, scoring='roc_auc', n_jobs=3)\ngrid_searcher.fit(X_train, y_train)\nprint grid_searcher.best_score_\nprint grid_searcher.best_estimator_\n", "intent": "Use 5-fold crossvalidation for finding optimal K in KNN\n"}
{"snippet": "model = make_pipeline(PolynomialFeatures(degree=1), Ridge(alpha=0.001))\nmodel.fit(Xtrain, ytrain)\n", "intent": "Create, fit, tune and predict using the `Ridge` model here:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_normal([vocab_size, embed_dim], stddev = 0.1), name = 'embedding')\n    embed = tf.nn.embedding_lookup(embedding, input_data, name = 'embed')\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "lm = LinearRegression()\nlm.fit(X_train, y_train)\n", "intent": "d) Train model on training data, and make predictions on testing data\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform([vocab_size, embed_dim]))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "housingModel = createCompileFitModel(X_train, y_train, learning_rate=0.008)\n", "intent": "Let's try different learning rate\n"}
{"snippet": "feature_columns = boston.feature_names\nregressor = learn.DNNRegressor( feature_columns=None,\n                               hidden_units=[13, 13, 10],\n                              model_dir = '/tmp/tf/')\nregressor.fit(X_train, y_train, steps=5000, batch_size=1)\n", "intent": "<h1>Building a [13,13,10] Neural Net</h1>\n<br />\n"}
{"snippet": "result = tf.nn.in_top_k([[0.1, 0.4, 0.3, 0.2, 0.5]], [4], 1)\nwith tf.Session() as s:\n    print(result.eval())\n", "intent": "**Specifying how to evaluate the model**\n"}
{"snippet": "from skater.model import InMemoryModel\nmlp_model = InMemoryModel(model.predict_proba, input_formatter = texts_to_vectors)\nmlp_model(['hello'])\n", "intent": "* We'll use Skater/Lime with ipywidgets to explore how our model generates predictions.\n* Model needs to output probabilities to get good results.\n"}
{"snippet": "from sklearn.svm import SVC\nsvc1 = SVC()\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "X=tf.placeholder(shape=(4,2),dtype=tf.float32,name='input')\ny=tf.placeholder(shape=(4,1),dtype=tf.float32,name='output')\nW=tf.Variable([[1],[1]],dtype=tf.float32,name='weights')\nb=tf.Variable([0],dtype=tf.float32,name='bias')\n", "intent": "we create our objects\n"}
{"snippet": "def PolynomialRegression(model, degree=2, **kwargs):\n    return make_pipeline(PolynomialFeatures(degree), model(**kwargs))\nlinear_param_grid = {\n    'polynomialfeatures__degree': np.arange(4),\n    'linearregression__fit_intercept': [True, False],\n    'linearregression__normalize': [True, False]\n}\nlinear_grid = GridSearchCV(PolynomialRegression(LinearRegression), linear_param_grid, cv=7)\nlinear_grid.fit(X, y)\nprint(linear_grid.best_params_)\n", "intent": "Create, fit, tune and predict using the `LinearRegression` model here:\n"}
{"snippet": "sess = tf.Session()\nsess.run(tf.global_variables_initializer())\ntrain_summary_writer = tf.summary.FileWriter('./summaries/', sess.graph)\n", "intent": "Running the session\n"}
{"snippet": "km = KMeans(random_state = 324,n_clusters = 2)\nres_g = km.fit(data3_g)\ndata3_g.groupby(res_g.labels_).count()\n", "intent": "** From the result above, I will use KMeans into k = 2 clusters**\n"}
{"snippet": "biases = {\n    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n", "intent": "Each of the multiple hidden layers require its own weight and bias.\n"}
{"snippet": "NB_credit_mod = GaussianNB() \nNB_credit_mod.fit(X_train, y_train)\n", "intent": "The code in the cell below defines a naive Bayes model object and then fits the model to the training data. Execute this code:\n"}
{"snippet": "c = sklearn.ensemble.BaggingClassifier(base_estimator=sklearn.tree.DecisionTreeClassifier(max_depth=1))\ncv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True, random_state=123456)\n", "intent": "What about with \"decision stumps\" (aka one-R)?\n"}
{"snippet": "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)\n", "intent": "Fit model on training data\n"}
{"snippet": "x = torch.tensor([1., 2., 3.])\ny = torch.tensor([4., 5., 6.])\nz = x + y\nprint(z)\n", "intent": "Operations with Tensors\n~~~~~~~~~~~~~~~~~~~~~~~\nYou can operate on tensors in the ways you would expect.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ngrid_dt = GridSearchCV(estimator=DecisionTreeClassifier(),\n                    param_grid={'min_samples_leaf': [3, 4, 5],\n                                'max_depth': [None, 1,3,5,7,9,10,11,12,13]\n                               },\n                    scoring=\"accuracy\",\n                    cv=10,\n                    return_train_score=True)\ngrid_dt.fit(X_train, y_train);\n", "intent": "<b> 1.4.3 Decision Tree Classifier\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=1)\n", "intent": "Next we choose a model and hyperparameters:\n"}
{"snippet": "rforest_param_grid = {\n    'polynomialfeatures__degree': np.arange(4),\n    'randomforestregressor__n_estimators': np.arange(50, 100, 5)\n}\nrforest_grid = GridSearchCV(PolynomialRegression(RandomForestRegressor), rforest_param_grid, cv=5,\\\n                            verbose=1, n_jobs=3)\nrforest_grid.fit(X, y)\nprint(rforest_grid.best_params_)\n", "intent": "Create, fit, tune and predict using the `RandomForestRegression` model here:\n"}
{"snippet": "sagemaker = boto3.Session().client(service_name='sagemaker') \njob_name = tuner.latest_tuning_job.job_name\nprint(job_name)\nsagemaker.describe_hyper_parameter_tuning_job(\n    HyperParameterTuningJobName=job_name)['HyperParameterTuningJobStatus']\n", "intent": "Let's just run a quick check of the tuning job status to make sure it started successfully. \n"}
{"snippet": "lin_reg = LinearRegression()\nlin_reg.fit(train_set, train_labels)\n", "intent": "Now that we have our data in the correct form, we pass in the `train_set` and `train_labels` into the `fit` method to train the model.\n"}
{"snippet": "ols.fit(X_train, y_train)\nprint(\"R^2 for training set:\", ols.score(X_train, y_train))\nprint('-'*50)\nprint(\"R^2 for test set:\", ols.score(X_test, y_test))\n", "intent": "- Do multiple linear regression with new data set.\n- Report the coefficient of determination from the training and testing sets.\n"}
{"snippet": "scaler.fit(X)\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "def conv_block(x, filters, k_size, strides=(1,1), padding='same', act='relu'):\n    x = l.Conv2D(filters, k_size, strides=strides, padding=padding, activation=act)(x)\n    return l.BatchNormalization()(x)\n", "intent": "First, we'll build a conv block. This is simply a Conv2D layer, followed by bn and a relu activation.\n"}
{"snippet": "gradient_boosting_regressor = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1,max_depth=1,\n                                                        random_state=0,\n                                loss='ls',max_features=1,subsample=.5)\n", "intent": "Gradient Boosting Model\n---\n"}
{"snippet": "rforest = RandomForestClassifier(n_estimators=1000)\n", "intent": "Now its time to train our model!\n**Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    return tf.nn.embedding_lookup(embedding,input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embedding ,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "ridge_param_grid = {\n    'polynomialfeatures__degree': np.arange(4),\n    'ridge__fit_intercept': [True, False],\n    'ridge__normalize': [True, False],\n    'ridge__alpha': np.linspace(0.01, 0.5, num=5)\n}\nridge_grid = GridSearchCV(PolynomialRegression(Ridge), ridge_param_grid, cv=7, verbose=1)\nridge_grid.fit(X, y)\nprint(ridge_grid.best_params_)\n", "intent": "Create, fit, tune and predict using the `Ridge` model here:\n"}
{"snippet": "rfc = RandomForestClassifier()\n", "intent": "Now its time to train our model!\n**Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.**\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nn_estimators = range(90,100)\nmax_depth = [1, 2, 3, 4, 5]\nsubsample = [0.8, 0.9, 1.0]\ngbc = GradientBoostingClassifier()\nparam_grid = dict(n_estimators=n_estimators, max_depth=max_depth, subsample=subsample)\ngrid = GridSearchCV(gbc, param_grid, cv=cv)\ngrid.fit(X, y)\n", "intent": "Take the best model and try to improve it using grid search.\n"}
{"snippet": "model = Sequential([\n    Embedding(vocab_size, 32, input_length=seq_len),\n    Flatten(),\n    Dense(100, activation='relu'),\n    Dropout(0.7),\n    Dense(1, activation='sigmoid')])\n", "intent": "Note:\n    - Input: Review containing 500 words, output: 1/0 (positive/negative)\n    - Input layer = Embedding: IP = 500, OP = 500x32\n"}
{"snippet": "plot_KMeans(X, centroids, labels)\n", "intent": "  * Plot de data, centroids en labels.\n  * Wat gebeurt er als je het algoritme meerdere keren draait? Worden altijd dezelfde zes clusters gevonden?\n"}
{"snippet": "perceptron.fit(X_pairs,y_and)\n", "intent": "  * Train de perceptron op `X_pairs` met `y_and` als target.\n"}
{"snippet": "noClusters = 30\nkm = KMeans(n_clusters=noClusters, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=0)\n", "intent": "Uit de plot blijkt niet een direct een knik. Een hogere waarde voor k lijkt een beter resultaat op te leveren. Vanwege rekentijd kies ik k=30.\n"}
{"snippet": "rn = sklearn.neighbors.RadiusNeighborsRegressor()\nrn.fit(table[['sq__ft']],table['price'])\n", "intent": "Radius Neighbors Regressor\n"}
{"snippet": "pf = PolynomialFeatures(degree=7, include_bias=False)\npf.fit(X)\nX7 = pf.transform(X)\nlr_boston7 = LinearRegression()\nlr_boston7.fit(X7, y)\n", "intent": "- Create a linear regression model for MEDV against DIS with polynomial terms for DIS up to and including degree seven.\n"}
{"snippet": "gs3 = GridSearchCV(LogisticRegression(),\n                  logreg_parameters,\n                  cv=5,\n                  scoring='average_precision')\ngs3.fit(X_train, y_train)\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "model.fit(x,y,epochs=50);\n", "intent": "Now that we have specified the model and how we want to train it, calling the [`fit`](https://keras.io/models/sequential/\n"}
{"snippet": "def conv_layer(prev_layer, layer_depth, is_training):\n    strides = 2 if layer_depth % 3 == 0 else 1\n    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', activation=None, use_bias=False)\n    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n    conv_layer = tf.nn.relu(conv_layer)\n    return conv_layer\n", "intent": "Modified `conv_layer` to add batch normalization to the convolutional layers it creates. \n"}
{"snippet": "nb.fit(X_test,y_test)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "model.fit(X_train_indices, Y_train_oh, epochs=50, batch_size=32, shuffle=True)\n", "intent": "Fit the Keras model on `X_train_indices` and `Y_train_oh`. We will use `epochs = 50` and `batch_size = 32`.\n"}
{"snippet": "indeces = [2,27, 64]\nlogistic = LogisticRegression().fit(training_data[:,indeces],training_data[:,-1])\nvalidation_error = logistic.score(validation_data[:,indeces], validation_data[:,-1]) \ntest_error = logistic.score(test_data[:,indeces], test_data[:,-1]) \ndisplay(str.format(\"{} training samples : validation error = {}, test error = {}\",training_data.shape[0], validation_error, test_error))\n", "intent": "ind_var5 -> 27 <br>\nind_var30 -> 64 <br>\nvar15 -> 2 (categorical?)\n"}
{"snippet": "km4 = KMeans(n_clusters=4)\nkm4.fit(data)\nlabels = km4.labels_\ncents4 = km4.cluster_centers_\npd.value_counts(labels)\n", "intent": "What cluster value should we set?\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape=(n_cols,)))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer=\"adam\", loss = \"mean_squared_error\")\n", "intent": "Restart the model from the beginning.\n"}
{"snippet": "V = df[[\"valence\"]]\nlr_V = LogisticRegression()\nlr_V.fit(V, y);\n", "intent": "Train model using one feature: \"valence\"\n"}
{"snippet": "test_models = [LinearRegression(), Ridge(alpha=10), Lasso(alpha=10)]\nscores = [analyze_performance(my_model, X, y_sample) for my_model in test_models]\n", "intent": "Let's incorpoate polynomial degrees a few degrees with the regularized models.\n"}
{"snippet": "sm = pystan.StanModel(model_code=LDA_STAN)\nfit = sm.vb(data=data, iter=10000, algorithm=\"meanfield\", elbo_samples=100, grad_samples=20, seed=42, verbose=True)\nprint(fit)\n", "intent": "Run the STAN model now. The VB version will be faster, as usual, and if you're running this in the class, this may be important. \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1, input_dim=1, kernel_initializer='zeros', bias_initializer='zeros'))\nmodel.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.1))\nprint('Training...')\nhistory = saveWeightsCallback()\nmodel.fit(x,y,epochs=100, callbacks=[history], verbose=0, batch_size=32);\nprint('Done!')\n", "intent": "Now we train the model, passing the newly created callback as argument to the `fit` method.\n"}
{"snippet": "def conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n", "intent": "I will explain what the next functions are doing.\n"}
{"snippet": "v = []\nfor i in range(len(x1)):\n    vi = []\n    for j in range(len(w)):\n        vij = pulp.LpVariable('v_%d%d' % (i, j), cat=pulp.LpBinary)\n        vi.append(vij)\n    v.append(vi)\n", "intent": "Secondary decision variables = do we associate point $i$ with cluster $j$?\n"}
{"snippet": "pipeline.fit(X_train,y_train)\n", "intent": "**Now fit the pipeline to the training data. we cant use the same training data as the data is being vectorised\n"}
{"snippet": "conv_layer = tf.nn.conv2d(input,weight,strides=[1,2,2,1],padding=\"SAME\")\nconv_layer = tf.nn.bias_add(conv_layer,bias)\nconv_layer = tf.nn.relu(conv_layer)\nconv_layer = tf.nn.max_pool(conv_layer,ksize=[1,2,2,1],\n                           strides = [1,2,2,1],padding=\"SAME\")\n", "intent": "* Function -- **tf.nn.max_pool()**\n"}
{"snippet": "model.fit(x_train, y_train, epochs=100, batch_size=50)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "activations = tf.nn.relu(pre_activations)\noutputs = snt.Linear(output_size=1)(activations)\noutputs\n", "intent": "To complete our model, we apply a ReLU non-linearity and add a final linear layer with just 1 output.\n"}
{"snippet": "from keras.layers import Dense\nnumber_inputs = 3\nnumber_hidden_nodes = 4\nmodel.add(Dense(units=number_hidden_nodes,\n                activation='relu',\n                input_dim = number_inputs))\n", "intent": "Next, we add our first layer. This layer requires you to specify both the number of inputs and the number of nodes that you want in the hidden layer.\n"}
{"snippet": "def conv_net(x, dropout):\n    conv1 = conv2d(x, wc1, bc1, SC1, PC1)\n    conv1 = max_pool2d(conv1, P1, SP1)\n    conv2 = conv2d(conv1, wc2, bc2, SC2, PC2)\n    conv2 = max_pool2d(conv2, P2, SP2)\n    fc1 = tf.reshape(conv2, [-1, FC1_in])\n    fc1 = affine_relu_dropout(fc1, wfc1, bfc1, dropout)\n    out = tf.add(tf.matmul(fc1, wout), bout)\n    return out\n", "intent": "We are ready to build our first conv-net\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "model = base_model()\ntb = TensorBoard(log_dir='./logs/initial_setting')\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=1024, callbacks=[tb])\n", "intent": "Now that we defined the architecture, we can train it with the following cell.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nmodel = linear_model.LinearRegression()\nparameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\ngrid = GridSearchCV(model,parameters, cv=None)\ngrid.fit(X_train, y_train)\n", "intent": "GridSearch Visualization\n"}
{"snippet": "knn.fit(np.array(X_train), np.ravel(y_train))\n", "intent": "Based on the above, this model is showing bias and could be improved with feature engineering \n"}
{"snippet": "final_estimator = SGDClassifier(alpha=0.75, average=False, class_weight='balanced',\n       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=100,\n       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n       power_t=0.5, shuffle=True, tol=0.001,\n       validation_fraction=0.1, verbose=1, warm_start=False, random_state = 22)\n", "intent": "Creating Final Model \n"}
{"snippet": "model1 = load_model(save_model_name, custom_objects={'my_iou_metric': my_iou_metric})\ninput_x = model1.layers[0].input\noutput_layer = model1.layers[-1].input\nmodel = Model(input_x, output_layer)\nc = optimizers.Adam(lr = 0.005)\nmodel.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric_2])\n", "intent": "Then the default threshod for pixel prediction is 0 instead of 0.5, as in my_iou_metric_2.\n"}
{"snippet": "model.fit(df[iris.feature_names], df.species)\n", "intent": "Fit the classifier with the iris data:\n"}
{"snippet": "from sklearn.dummy import DummyClassifier\ndcl= DummyClassifier('most_frequent')\ndcl.fit(X_tr,y_tr)\nprint('The baseline classifier (predict most frequent class) would achieve a \\\nclassification accuracy score of: {:.3f}'.format(dcl.score(X_val, y_val)))\n", "intent": "*Your answer goes here*  \n"}
{"snippet": "def h_of_theta(theta, X):\n    transposed_theta = theta[:, None]\n    return sigmoid(X.dot(transposed_theta))\n", "intent": "$g(\\theta^Tx)$ expressed as:\n"}
{"snippet": "scaler.fit(dataframe.drop('TARGET CLASS', axis = 1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "dtree = DecisionTreeClassifier()\n", "intent": "**Erstelle eine Instanz des DecisionTreeClassifier() namens dtree und fitte die Trainingsdaten darauf.**\n"}
{"snippet": "model = base_model()\ntb = TensorBoard(log_dir='./logs/final_model')\nestop = EarlyStopping(monitor='val_acc', patience=5)\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=1024, callbacks=[tb, estop])\n", "intent": "Choose the best one and train longer. Here we also use [early stopping](https://en.wikipedia.org/wiki/Early_stopping\n"}
{"snippet": "probs = np.array([0.2, 0.5, 0.8, 0.9, 0.1, 0.75])\npreds = threshold(probs, thres=.7)\npreds\n", "intent": "Here's an example of using the threshold and profit_functions\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(train_df[colnames], train_df['target'])\nnb.score(test_df[colnames], test_df['target'])\n", "intent": "Exercise: Build a GaussianNB model using sklearn, and estimate the accuracy on the test set.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nparamgrid=dict(C=[0.001, 0.1, 1, 10, 100])\ncf=LogisticRegression()\ngrid = GridSearchCV(cf, paramgrid,cv=10)\ngrid.fit(Xlr, ylr)\n", "intent": "<h1><center>GridSearchCV!</center></h1>\n"}
{"snippet": "y_pred = tf.layers.dense(out, n_classes, activation=None)\n", "intent": "Finally we pass this output through another layer with output size `n_classes`. This step is necessary to get our predictions.\n"}
{"snippet": "reset_tf()\nN_PIXELS= 28 * 28\nBATCH_SIZE = 100\nLEARNING_RATE = 0.5\nx = tf.placeholder(tf.float32, [None, N_PIXELS], name=\"pixels\")\ny_labels = tf.placeholder(tf.float32, [None, 10], name=\"labels\")\nW = tf.Variable(tf.zeros([N_PIXELS, 10]), name=\"weights\")\nb = tf.Variable(tf.zeros([10]), name=\"biases\")\ny = tf.matmul(x, W) + b\n", "intent": "The linear model is very similar to before, but it now has an output of dimension 10, instead of 1.\n"}
{"snippet": "skipgram = Sequential()\nskipgram.add(Embedding(input_dim=V, output_dim=dim, init='glorot_uniform', input_length=1))\nskipgram.add(Reshape((dim, )))\nskipgram.add(Dense(input_dim=dim, output_dim=V, activation='softmax'))\nSVG(model_to_dot(skipgram, show_shapes=True).create(prog='dot', format='svg'))\n", "intent": "- Lastly, we create the (shallow) network!\n"}
{"snippet": "ln.fit(X_train, y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "with tf.variable_scope('layer1'):\n    w = tf.get_variable('v', shape=[2, 3], initializer=my_initializer)\n    print w.name\nwith tf.variable_scope('layer2'):\n    w = tf.get_variable('v', shape=[2, 3], initializer=my_initializer)\n    print w.name\n", "intent": "`tf.variable_scope(scope_name)` manages namespaces for names passed to `tf.get_variable`.\n"}
{"snippet": "IcebergModel = Iceberg_model((75,75,3))\n", "intent": "For  creating the model we provide the shape of each training data (75,75,3) .\n"}
{"snippet": "p = Perceptron()\np.train(X_shuffled,y_shuffled,10000,verbose=False)\n", "intent": "Try to get as high an accuracy as possible.\n"}
{"snippet": "model = DecisionTreeClassifier()\npredictor_var = ['Credit_History','Gender','Married','Education']\nclassification_model(model,df,predictor_var,outcome_var)\n", "intent": "https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/\n"}
{"snippet": "def build_basic_model(shape, neurons):\n    model = Sequential()\n    model.add(LSTM(neurons[0], input_shape=(shape[0], shape[1]), return_sequences=True))\n    model.add(LSTM(neurons[1], input_shape=(shape[0], shape[1]), return_sequences=False))\n    model.add(Dense(neurons[2],activation='linear'))\n    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n    model.summary()\n    return model\n", "intent": "**Design a basic LSTM model**\n"}
{"snippet": "def build_linear_regression_model_basic(X, y):\n    linear_mdl = linear_model.LinearRegression()  \n    linear_mdl.fit(X, y)  \n    return linear_mdl\n", "intent": "   **Design the Benchmark Regression Model **\n"}
{"snippet": "iso = manifold.Isomap(n_neighbors=6, n_components=2)\niso.fit(X)\nmanifold_data = iso.transform(X)\nplot_clustering(manifold_data[indices], X[indices], y_num, title='ISOMAP')\n", "intent": "**Excerise 3:** Apply ISOMAP on the data\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), minval=-1, maxval=1)) \n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model.fit(X_train, y_train)\n", "intent": "You're now ready to train the model. Write and run the following line:\n`model.fit(X_train, y_train)`\n"}
{"snippet": "linreg.fit(x_train,y_train)\n", "intent": "Now, apply the training set to the predictive model using the fit() function.\n"}
{"snippet": "lr = hmm.MultinomialHMM(n_components=2) \nX = shampoo_transitions.Direction.values.reshape(-1,1) \nlr.fit(X)\n", "intent": "Next we create a Multinomial HMM. This one is used for strictly categorical variables, we you don't have a continous distribution.\n"}
{"snippet": "model.fit(x_train, y_train, batch_size=100, epochs=10, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "DIVISIBILITY_NUMBER = 7\nmodel_div7 = build_model()\nmodel_div7.summary()\n", "intent": "Let's try divisibility by 7 first.\n"}
{"snippet": "clf_petal = cluster.KMeans(init='k-means++', n_clusters=3, random_state=33)\nclf_petal.fit(X_train4[:,2:4])\n", "intent": "Repeat the experiment, using petal dimensions\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nmodel_dt = GridSearchCV(dt_classifier, param_grid, cv=3, iid=False, return_train_score=True)\nmodel_dt.fit(X_train, y_train);\n", "intent": "Then we can implement the grid search and fit our model according to the best parameters.\n"}
{"snippet": "clf = LogisticRegression().fit(bX_train, by_train)\nprint('Balanced training accuracy:', clf.score(bX_test, by_test))\nprint('Balanced testing accuracy:', clf.score(bX_test, by_test))\nprint('Overall test accuracy:', clf.score(X_test, y_test))\n", "intent": "**Now to see how the data will fit using logistic regression using all the provided features**\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'polynomialfeatures__degree': np.arange(5),\n    'linearregression__fit_intercept': [True, False],\n    'linearregression__normalize': [True, False]\n}\ngrid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)\ngrid.fit(X, y)\ngrid.best_params_\n", "intent": "We can use grid search to find a good model; we can vary a couple of other hyperparameters too:\n"}
{"snippet": "Feature_Name=[]\nFeature_Rsq=[]\nfor name in list(X):\n    lr.fit(X_train[[name]], y_train)\n    lr.score(X_test[[name]], y_test)\n    Feature_Rsq.append(lr.score(X_test[[name]], y_test))\n", "intent": "We can calculate the R^2 for every feature:   \n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n", "intent": "Train the logistic regression model\n"}
{"snippet": "dummy_classifier = DummyClassifier(strategy=\"most_frequent\")\ndummy_classifier.fit(X_tr,y_tr )\nprint('The baseline classifier (most frequent) would achieve a classification accuracy score of: {:.3f}'.\n      format(dummy_classifier.score(X_val,y_val)))\n", "intent": "*Your answer goes here*\nAssuming that all points were labelled as 0 in \"is_person\" is the simplest classification way.\n"}
{"snippet": "print(x_train.shape)\nmodel.fit(x_train ,y_train, epochs = 10, batch_size = 18, verbose = 2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "sess = tf.Session()\n", "intent": "A session places a graph onto a computation device and provides methods to execute it.\n"}
{"snippet": "def sampling(args):\n    z_mean, z_log_var = args\n    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n                              mean=0., stddev=1.)\n    return z_mean + K.exp(z_log_var) * epsilon\nz = layers.Lambda(sampling)([z_mean, z_log_var])\n", "intent": "We'll do our sampling in the latent space via a special 'Lambda' layer, which is Keras' general layer for housing a function.\n"}
{"snippet": "tree_mod = DecisionTreeClassifier(max_depth=6, min_samples_leaf=5)\ntree_mod.fit(X_train, y_train)\nprint('Training Accuracy:', tree_mod.score(X_train, y_train))\nprint('Testing Accuracy:', tree_mod.score(X_test, y_test))\n", "intent": "You will be asked to specifty two parameters for the decision tree model: `max_depth` and `min_samples_leaf`. These values will be provided to you.\n"}
{"snippet": "dummyInput = np.ones(len(y))[np.newaxis].T\nlr = LinearRegression().fit(dummyInput, y)\n", "intent": "<font color=\"red\"> Good </font>\n"}
{"snippet": "model.fit(x_train, y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nprint('Processing GridSearch. Please hold for the next available set of outputs.\\n')\ngd_parameters = {   }\nrf = RandomForestClassifier(random_state=42)\ngd_model = GridSearchCV(rf, gd_parameters, n_jobs = -1, cv=10)\nprint(gd_model.best_params_)\nprint(gd_model.best_score_)\n", "intent": "You can find parameter options here: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"}
{"snippet": "lr = LogisticRegression(penalty='l2')\ncv = cross_validation(lr, X_under, Y_under , n_splits=5,init_chunk_size = 100, chunk_spacings = 10, average = \"binary\")\ncv.validate_for_holdout_set(X_holdout, Y_holdout)\ncv.plot_learning_curve()\n", "intent": "Regularization: Rigde \n"}
{"snippet": "sm = SMOTE(kind='regular')\nX_smote, Y_smote= sm.fit_sample(X_train, Y_train)\nlr = LogisticRegression()\ncv = cross_validation(lr, X_smote, Y_smote , n_splits=5, init_chunk_size = 100, chunk_spacings = 10, average = \"binary\")\ncv.validate_for_holdout_set(X_holdout, Y_holdout)\ncv.plot_learning_curve()\n", "intent": "Regularization: Ridge\n"}
{"snippet": "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(train_X, train_y, early_stopping_rounds=5, \n             eval_set=[(test_X, test_y)], verbose=False)\n", "intent": "*Step 4: Evaluate Regressors*\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm_out = KMeans(n_clusters = 2, n_init = 20, random_state = 2).fit(x)\n", "intent": "We now perform K-means clustering with `K = 2`:\n"}
{"snippet": "param_grid_nc = {'metric': ['euclidean','manhattan'],'shrink_threshold':[0.05,0.1,0.5,0.75]}\ngrid_nc_kfold_sfl = GridSearchCV(NearestCentroid(), param_grid_nc,cv=cv_sfl,return_train_score=True)\ngrid_nc_kfold_sfl.fit(x_train, y_train)\n", "intent": "**Comparison of best parameter values with/without random seed:**\n"}
{"snippet": "multi_lin_model = linear_model.LinearRegression()\n", "intent": "We start again by creating a [```LinearRegression```](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n"}
{"snippet": "weightdistribution(net.state_dict()['layer2.0.weight'].cpu().numpy().flatten())\nweightdistribution(net.state_dict()['fc1.weight'].cpu().numpy().flatten())\nweightdistribution(net.state_dict()['fc2.weight'].cpu().numpy().flatten())\n", "intent": "<b>Visualizing the weight distributions with the regularized loss</b>\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nfrom sklearn.cross_validation import KFold\npredictors = [\"Pclass\", \"Sex\", \"Age\",\"SibSp\", \"Parch\", \"Fare\",\n              \"Embarked\",\"NlengthD\", \"FsizeD\", \"Title\",\"Deck\"]\ntarget=\"Survived\"\nalg = LinearRegression()\nkf = KFold(titanic.shape[0], n_folds=3, random_state=1)\npredictions = []\n", "intent": "Predict Survival\n================\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dense\ndef create_fc_nn():\n    model = Sequential()\n    model.add(Dense(num_pixels, input_dim=num_pixels, init='normal', activation='relu'))\n    model.add(Dense(num_classes, init='normal', activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\nmodel = create_fc_nn()\nmodel.fit(X_train, Y_train, validation_data=(X_val, Y_val), nb_epoch=10, batch_size=200, verbose=2)\n", "intent": "Use Fc NN as a baseline model\n"}
{"snippet": "model=Sequential([\n        GRU(n_hidden, return_sequences=True, input_shape=(cs, vocab_size),\n                  activation='relu'),\n        TimeDistributed(Dense(vocab_size, activation='softmax')),\n    ])\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam())\n", "intent": "Identical to the last keras rnn, but a GRU!\n"}
{"snippet": "lm_inp2 = Input(shape=(2048,))\nlm2 = Model(lm_inp2, Dense(ndim)(lm_inp2))\n", "intent": "To improve things, let's fine tune more layers.\n"}
{"snippet": "from keras.models import Model\ninputs = Input(shape=(1000,))\nx = Dense(512, activation='relu')(inputs)\nx = Dropout(.5)(x)\npredictions = Dense(2, activation='softmax')(x)\nmodel = Model(inputs=inputs, outputs=predictions)\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "sess = tf.Session(config=config)\n", "intent": "$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\nExample 1: find root for \n$$f(x) = x^2 - sin(x)$$\n$$f'(x) = 2x - cos(x)$$\n"}
{"snippet": "def _sigmoid(x):\n    return 1.0/(1.0+np.exp(-x))\n", "intent": "**This basic neural net framework can learn MNIST**\n"}
{"snippet": "P = _sigmoid(C)\n", "intent": "**Step 5: Feed this through a sigmoid:**\n"}
{"snippet": "simple_lin_model.fit(total_request_rate_M, cpu)\n", "intent": "Then we fit our model using the the total request rate and cpu. The coefficients found are automatically stored in the ```simple_lin_model``` object.\n"}
{"snippet": "tf.reset_default_graph()\nn_his = 400\nn_vis = 4\ntf.reset_default_graph()\nvar_history = tf.get_variable(\"var_history\", \n                              dtype=np.float32, \n                              initializer=np.array([range(0,400)], \"float32\")) \nvar_history_shift = tf.get_variable(\"var_history_shift\", \n                              dtype=np.float32, shape=(1, n_his-n_vis)) \n", "intent": "running tf.variable.assign several times is quite ineficcient since every time tf_variable.assign is called the graph grows.\n"}
{"snippet": "knn = KNeighborsClassifier()\nX = fruit['color_score'].reshape(-1,1)\ny = fruit['fruit_name']\nk = [i for i in range(2, 10, 2)]\nparams = [{'n_neighbors': k }]\ngrid_search = GridSearchCV(lasso, params, cv = 5)\ngrid_search.fit(X, y)\n", "intent": "Generate a model to classify the fruit data using our train/test split protocol and a grid search for the appropriate number of $k$ means.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(50, activation='relu', input_dim=3000))\nmodel.add(Dropout(.5))\nmodel.add(Dense(5, activation='relu', input_dim=3000))\nmodel.add(Dropout(.5))\nmodel.add(Dense(2, activation='softmax'))\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "knn.fit(X_train,y_train)\n", "intent": "Fit this KNN model to the training data.\n"}
{"snippet": "max_degree = 11\nnoise_level = 0.5\nlinearized_model_dataframe = models.random_linearized_model(noise_level,max_degree,\n                                               x_range=[-1,1],\n                                               x_mode='linspace', N=1000)\nsns.lmplot(data=linearized_model_dataframe,\n           x='x',y='y',\n            lowess=True,\n           line_kws={'color':'k','linewidth':4});\n", "intent": "The cells below will generate data from linearized and nonlinear models. Run them both several times.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\ngbt = GradientBoostingClassifier()\ngbt.fit(X_train, y_train)\n", "intent": "Gradient Boosted Trees\n"}
{"snippet": "def plot_top_k(alpha, label='pos', k=10):\n    positive_words = [w for (w,y) in alpha.keys() if y == label]\n    sorted_positive_words = sorted(positive_words, key=lambda w:-alpha[w,label])[:k]\n    util.plot_bar_graph([alpha[w,label] for w in sorted_positive_words],sorted_positive_words,rotation=45)\n", "intent": "The per-class word distributions $\\balpha$:\n"}
{"snippet": "clf1 = LogisticRegression(intercept_scaling=1).fit(X, y)\n", "intent": "2\\. Fit `LogisticRegression` with `gym_hours` and `email_hours` as features and `data_scientist` as the response.\n"}
{"snippet": "simple_lin_model.fit(total_page_views_2D, cpu_usage)\n", "intent": "Then we fit our model using the the total request rate and cpu. The coefficients found are automatically stored in the ```simple_lin_model``` object.\n"}
{"snippet": "model = RandomForestClassifier().fit(features, target)\n", "intent": "Train the model using the best data available (in the case of the Titanic you'd use all the data)\n"}
{"snippet": "df_train = X_train.copy()\ndf_train[\"log_marketcap\"] = y_train\ndf_train.head()\nmod = smf.ols(formula='log_marketcap ~ log_revenue + log_employees  + log_assets', data=df_train).fit()\nprint(\"log_revenue \tlog_employees \tlog_assets \")\nprint(mod.params.values[1:])\n", "intent": "**Linear regression (to compare)**\n"}
{"snippet": "lm = smf.ols(formula='speed ~ temp', data = df).fit()\nprint (lm.summary())\n", "intent": "The R-squared is 0.011 which is small, so it seems 'Visibility' has no correlation to 'speed'.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, activation='tanh', input_shape=(x_train.shape[1],)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlgr=LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "input_text = Input(shape=(None,),dtype='int32', name='text')\nembeded_text = layers.Embedding(64, text_vocab_size)(input_text)\nencoded_text = layers.LSTM(32)(embeded_text)\n", "intent": "**Defining input for input text**\n"}
{"snippet": "full_drop = tf.nn.dropout(full_layer,keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "_x = np.array([1, 3, 0, -1, -3])\nx = tf.convert_to_tensor(_x)\ntf.sign(x).eval()\n", "intent": "Q11. Compute an element-wise indication of the sign of x, element-wise.\n"}
{"snippet": "_x = np.array([-np.pi, np.pi, np.pi/2])\nx = tf.convert_to_tensor(_x)\nprint(tf.sin(x).eval())\nprint(tf.cos(x).eval())\nprint(tf.tan(x).eval())\n", "intent": "Q21. Compuete the sine, cosine, and tangent of x, element-wise.\n"}
{"snippet": "optimized_model = grid_search.best_estimator_.fit(bank_features_train, bank_labels_train)\n", "intent": "Use the best estimator from your grid search. Score it using the function from problem 6. Save your answer in `tuned_score`.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [-1,-2,-3,-4], \n     [-10,-20,-30,-40],\n     [10,20,30,40]])\nX = tf.convert_to_tensor(_X)\ntf.unsorted_segment_sum(X,[0,1,0,1],2).eval()\n", "intent": "Q8. Compute the sum along the second and fourth and \nthe first and third elements of X separately in the order.\n"}
{"snippet": "def dense_to_sparse(tensor):\n    indices = tf.where(tf.not_equal(tensor, 0))\n    return tf.SparseTensor(indices=indices,\n                           values=tf.gather_nd(tensor, indices)-1,  \n                           dense_shape=tf.to_int64(tf.shape(tensor)))\nprint(dense_to_sparse(x).eval())\n", "intent": "Q3. Let's write a custom function that converts a SparseTensor to Tensor. Complete it.\n"}
{"snippet": "sess= tf.Session()\nprint(sess.run(z))\n", "intent": "Now let's create a session and let's evaluate the graph\n"}
{"snippet": "def run_evaluation(x1_, x2_):\n    sess = tf.Session()\n    print(sess.run(result, feed_dict = {x1: x1_, x2: x2_}))\n    sess.close()\n", "intent": "Or you can write a function that creates a session, evaluates a node, and then close it.\n"}
{"snippet": "sess4, ch4 = run_linear_model(1e-2, 15000, x, y, True)\n", "intent": "Graphically you cannot see any difference... Let's try to train the network longer.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(256, activation='relu', input_shape = (1000,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(2, activation = 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "log_model = LogisticRegression(random_state = 17)\nstart = time.time()\nlog_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(log_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"log_model\"></a>\n"}
{"snippet": "model = RandomForestRegressor()\nmodel.fit(train_x[:10000], train_y[:10000])\nNUM_FEATS = 3000\nfeats = sorted(list(zip(model.feature_importances_, train_x.columns)))\nfeats = list(list(zip(*feats[-NUM_FEATS:]))[1])\ntrain_x_f = train_x[feats]\ntest_x_f  = test_x[feats]\n", "intent": "FIXME: run PCA here\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(features_array, target)\n", "intent": "Let's try `LogisticRegression` from sklearn:\n"}
{"snippet": "torch.manual_seed(123)\nbaseline = torch.nn.Sequential(\n    torch.nn.Linear(1, 20),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20, 25),\n    torch.nn.ReLU(),\n    torch.nn.Linear(25, 1),\n)\ntorch.save(baseline.state_dict(), 'baseline.pth')\n", "intent": "Create (and save) the baseline model to use:\n"}
{"snippet": "lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n", "intent": "Lets define a lstm cell with tensorflow\n"}
{"snippet": "cur_w = np.zeros([visibleUnits, hiddenUnits], np.float32)\ncur_vb = np.zeros([visibleUnits], np.float32)\ncur_hb = np.zeros([hiddenUnits], np.float32)\nprv_w = np.zeros([visibleUnits, hiddenUnits], np.float32)\nprv_vb = np.zeros([visibleUnits], np.float32)\nprv_hb = np.zeros([hiddenUnits], np.float32)\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n", "intent": "We also have to initialize our variables. Thankfully, NumPy has a handy `zeros` function for this. We use it like so:\n"}
{"snippet": "rfc = RandomForestClassifier()\nrfc.fit(X_tsvd, y_train)\n", "intent": "At this point, we can move forward with modeling on our new sparse matrix of data:\n"}
{"snippet": "lm=LinearRegression()\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "bos_2 = bos[(bos['PRICE'] < 50) & (bos['CRIM'] < 40) & (bos['RM'] > 4)]\nbos_3_mod2 = ols('PRICE ~ CRIM + RM + PTRATIO', bos_2).fit()\nprint(bos_3_mod2.summary())\n", "intent": "**Remove the outliers and high leverage points from your model and run the regression again. How do the results change?**\n"}
{"snippet": "param_grid = [\n               {'n_neighbors': list(range(1, 50, 5))}\n]\nmy_tuned_model = GridSearchCV(neighbors.KNeighborsClassifier(), param_grid, cv=cv_folds, verbose = 2)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nmodel_tuned_params_list[\"Tuned kNN\"] = my_tuned_model.best_params_\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "from sklearn import linear_model\nlr = linear_model.LogisticRegression()\nlr.fit(X_train, Y_train)\n", "intent": "You can import a Logistic Regression Classifier by using the following codes:\n"}
{"snippet": "with pm.Model() as model:\n", "intent": "[**PYMC3**](https://pymc-devs.github.io/pymc3/api.html) docs will be useful throughout this lab.\n"}
{"snippet": "model2 = Sequential()\nmodel2.add(Dense(512, activation='relu', input_dim=1000))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(num_classes, activation='softmax'))\nmodel2.summary()\nmodel2.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Below is the model that Udacity provided that solves the threshold that they set, which was accuracy > 85% of the testing set. \n"}
{"snippet": "n_hidden = 25\ngen = np.random.RandomState(seed=123)\nW1 = Tensor(gen.normal(size=(1, n_hidden)))\nb1 = Tensor(gen.normal(size=n_hidden))\nW2 = Tensor(gen.normal(size=(n_hidden, 1)))\nb2 = Tensor(gen.normal(size=(1,)))\n", "intent": "Next declare the model parameters, with values initialized from a unit Gaussian distribution:\n"}
{"snippet": "nsample=2000\nwith pm.Model() as model:\n    pm.glm.GLM.from_formula('y ~ x', data) \n    start = pm.find_MAP() \n    step = pm.NUTS(scaling=start) \n    trace = pm.sample(nsample, step, start=start, progressbar=True, njobs=4) \nlines = {var:trace[var].mean() for var in trace.varnames}\npm.traceplot(trace, lines=lines)\n", "intent": "Repeat the earlier analysis and learn the variance of the data\n"}
{"snippet": "y = df['Class']\nX = df.drop('Class', axis = 1)\nscaler.fit(X)\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "mush_X = mush_df.iloc[:,1:]\nmush_y = mush_df.class_p\nmush_dt = tree.DecisionTreeClassifier()\nmush_dt.fit(mush_X, mush_y)\nmush_dt.score(mush_X, mush_y)\n", "intent": "* Create a decision tree to model whether a mushroom is poisonous. What is the score?\n* What are the most important features?\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_matrix = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    word_emb = tf.nn.embedding_lookup(embedding_matrix, input_data)\n    return word_emb\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "import sklearn.cross_validation\nkf = sklearn.cross_validation.KFold(n=len(irisdf), n_folds=3, shuffle=True, random_state=1234)\nfor train, test in kf:\n    clf3 = lm.LogisticRegression(penalty='l1', C=1e100).fit(irisdf.iloc[train][['sep_wid', 'pet_wid']], irisdf.iloc[train]['target'])\n    print clf3.coef_, clf3.score(irisdf.iloc[test][['sep_wid', 'pet_wid']], irisdf.iloc[test]['target'])\n", "intent": "Accuracy measure number correctly predicted over total number of observations.\n"}
{"snippet": "linereg = LinearRegression()\nlinereg.fit(Xn, shots)\ny = shots\nlinereg.score(Xn,y)\n", "intent": "---\nCross-validate the $R^2$ of a linear regression model with 10 cross-validation folds.\nHow does it perform?\n"}
{"snippet": "m = ols('PRICE ~ PTRATIO',bos).fit()\nprint('Coefficient:', lm.coef_)\nprint('Intercept:', lm.intercept_)\n", "intent": "<b> Fit a linear regression model using only the 'PTRATIO'\n"}
{"snippet": "xgb_rmodel = xgb.XGBRegressor().fit(boston['data'], boston['target'])\n", "intent": "Zbudujmy prosty model XGBoosta.\n"}
{"snippet": "valid = df.dropna().copy()\nnumeric_features = valid.select_dtypes(include=[np.number]).columns.tolist()\nfit = cluster.KMeans(n_clusters=3).fit(valid[numeric_features])\nvalid['cluster'] = fit.labels_\nfor i in range(fit.n_clusters):\n    print(f'Assigned {np.count_nonzero(fit.labels_ == i)} / {len(valid)} samples to cluster {i}.')\n", "intent": "Find the best assignment to 3 clusters using Kmeans (or change the value of `n_clusters` below to something more appropriate):\n"}
{"snippet": "train_model(X_train, y_train, X_val, y_val)\nmake_predictions(X_iris, y_iris)\n", "intent": "Lets run our train our model and test it out using the `train_model` and `make_predictions` functions we defined above:\n"}
{"snippet": "model_RAND   = compile_keras_sequential_model(model_layers_RAND, \"RAND\")\nmodel_LAST   = compile_keras_sequential_model(model_layers_LAST, \"LAST\")\nmodel_LAST2  = compile_keras_sequential_model(model_layers_LAST2, \"LAST2\")\nmodel_LINEAR = compile_keras_sequential_model(model_layers_LINEAR, \"LINEAR\")\nmodel_DNN    = compile_keras_sequential_model(model_layers_DNN, \"DNN\")\nmodel_CNN    = compile_keras_sequential_model(model_layers_CNN, \"CNN\")\n", "intent": "<a name=\"benchmark\"></a>\nBenchmark all the algorithms. This takes a while (approx. 10 min).\n"}
{"snippet": "k = 2 \nkmeans = KMeans(n_clusters = k) \nkmeans.fit(X);\ncentroids = kmeans.cluster_centers_ \nlabels = kmeans.labels_ \n", "intent": "k-means divides the data points into k clusters. We begin with k=2.\n"}
{"snippet": "a = torch.Tensor([[1, 2, 3], [4, 5, 6]])\nb = torch.Tensor([[1, 2, 3], [4, 5, 6]]).double()\ntry:\n    print(a + b)\nexcept Exception as e:\n    print(type(e))\n    print(e)\n", "intent": "`pytorch` is **really** strict about using the right data type:\n"}
{"snippet": "X_train_var = Variable(torch.Tensor(X_train).type(dtype))\nY_train_var = Variable(torch.Tensor(Y_train).type(dtype).long())\nX_test_var = Variable(torch.Tensor(X_test).type(dtype))\nY_test_var = Variable(torch.Tensor(Y_test).type(dtype).long())\nX_train_var.size()\n", "intent": "Finally, let's wrap the data into PyTorch Variables:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    input_embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev = 0.01))\n    embedded_sequence = tf.nn.embedding_lookup(input_embedding, input_data) \n    return embedded_sequence\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model_gbrA = ensemble.GradientBoostingRegressor()\nmodel_gbrB = ensemble.GradientBoostingRegressor()\nmodel_gbrC = ensemble.GradientBoostingRegressor()\nmodel_gbrD = ensemble.GradientBoostingRegressor()\nmodel_gbrE = ensemble.GradientBoostingRegressor()\n", "intent": "- Gradient Boosting Regressor\n"}
{"snippet": "rf = RandomForestRegressor(n_estimators=10, n_jobs=-1, min_samples_leaf=100)\n", "intent": "The minimum number of sample is each leaf.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 100)\nrfc.fit(X_train, y_train)\n", "intent": "Now let's compare the decision tree model to a random forest.\n"}
{"snippet": "RF = RandomForestClassifier(n_jobs = -1, n_estimators = 10, min_samples_leaf=10, max_depth=10)\n", "intent": "Fitting optimal Random Forest:\n"}
{"snippet": "reset_graph()\nsaver = ??\ntheta = ??\nwith tf.Session() as sess:\n    saver.restore(sess, \"./tmp/my_model_final.ckpt\")  \n    best_theta_restored = theta.eval() \nbest_theta_restored\n", "intent": "Restoring a model without the graph\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=20)\nkm.fit(fit_vect)\n", "intent": "Start with KMeans clustering, but feel free to try out other clustering methods afterwards\n"}
{"snippet": "errors = ??\nbst_n_estimators = np.??(errors)\ngbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators, random_state=42)\ngbrt_best.fit(X_train, y_train)\n", "intent": "make an errors list containing mean squared error at each stage of training (use gbrt.staged_predict())\n"}
{"snippet": "from tensorflow.python.framework import ops\nops.reset_default_graph()\ng = tf.get_default_graph()\n[op.name for op in tf.get_default_graph().get_operations()]\nX = tf.placeholder(tf.float32, name='X')\nh = linear(X, 2, 10)\n[op.name for op in tf.get_default_graph().get_operations()]\n", "intent": "Let's now take a look at what the tensorflow graph looks like when we create this type of connection:\n"}
{"snippet": "nb_utils.show_graph(net['graph_def'])\n", "intent": "Let's take a look at the network:\n"}
{"snippet": "cells = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_cells, state_is_tuple=True)\n", "intent": "Now we'll create our recurrent layer composed of LSTM cells.\n"}
{"snippet": "for input_example_batch, target_example_batch in dataset.take(1): \n  example_batch_predictions = model(input_example_batch)\n  print(example_batch_predictions.shape, \"\n", "intent": "We can also quickly check the dimensionality of our output, using a sequence length of 100. Note that the model can be run on inputs of any length.\n"}
{"snippet": "model.fit(X, y)\n", "intent": "Now lets apply the model to our data, we can do this by calling model.fit() \n"}
{"snippet": "rf_best = RandomForestClassifier(criterion='gini', class_weight='balanced', n_jobs=-1, \n                                 n_estimators=rf_cv.best_params_['n_estimators'],\n                                 min_samples_split=rf_cv.best_params_['min_samples_split'], \n                                 max_depth=rf_cv.best_params_['max_depth'])\nrf_best.fit(x_train, y_train)Now, we can find the score in the test dataset.\n", "intent": "With the best parameters, we can re-fit another RF object with those values.\n"}
{"snippet": "def gen_model(data, formula):\n    with pm.Model() as model_glm:\n        pm.glm.GLM.from_formula(formula, data)\n    return model_glm\n", "intent": "Complete the function *gen_model* by creating general linear model (GLM) using the formula and data passed in.\n"}
{"snippet": "km = KMeans(n_clusters = 5, random_state = 1337) \nkm.fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "def gaussian_rbf(us, lam=1):\n    return lambda x: np.array([np.exp(-np.linalg.norm(x - u)**2 / lam**2) for u in us])\nnum_basis = 20\nnp.random.seed(42)\nrbf_features = gaussian_rbf(Xmc[np.random.choice(range(len(Xmc)), num_basis, replace=False),:])\nPhimc = np.array([rbf_features(x) for x in Xmc])\nlr_rbf = linear_model.LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\")\nlr_rbf.fit(Phimc,Ymc)\n", "intent": "To capture the non-linear structure we randomly sample 20 data points and create an RBF feature centered at that point.\n"}
{"snippet": "def build_cell(num_units):     \n    lstm = tf.contrib.rnn.BasicLSTMCell(num_units)     \n    return lstm  \n", "intent": "* We construct two layer RNN with LSTM cells\n"}
{"snippet": "loss_value, grads = grad(model, features, labels)\nprint(\"Step: {}, Initial Loss: {}\".format(global_step.numpy(),\n                                          loss_value.numpy()))\noptimizer.apply_gradients(zip(grads, model.trainable_variables), global_step)\nprint(\"Step: {},         Loss: {}\".format(global_step.numpy(),\n                                          loss(model, features, labels).numpy()))\n", "intent": "We'll use this to calculate a single optimization step:\n"}
{"snippet": "logit_batch = model(image_batch).numpy()\nprint(\"min logit:\", logit_batch.min())\nprint(\"max logit:\", logit_batch.max())\nprint()\nprint(\"Shape:\", logit_batch.shape)\n", "intent": "Now it produces outputs of the expected shape:\n"}
{"snippet": "param_grid = {'C': [0.001,0.01,1,10, 100]}\nlogistic = GridSearchCV(LogisticRegression(),param_grid, cv=10)\n", "intent": "Logistic_regression is linear model and widely used in classification problems. This is our first model.\n"}
{"snippet": "NB1 = GaussianNB()\nX1 = news_A.drop(['class'],axis=1)\ny1  = news_A['class']\nNB1.fit(X1,y1)\nNB1.score(X1,y1)\n", "intent": "Fit a Gaussian Naive Bayes model to the original dataset A (including the outliers). Display the classification accuracy on the training dataset.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nctree = tree.DecisionTreeClassifier(random_state=1, max_depth=2)\n", "intent": "** metrics.roc_auc_score(y_test, probs)\n"}
{"snippet": "m_svm = SVR()\nm_svm.fit(X=X, y=sample['charges'])\n", "intent": "Fit support vector regression as before.\n"}
{"snippet": "clf = LogisticRegression(random_state=123, tol=1e-8).fit(X_train, y_train)\n", "intent": "Let's fit the Logistic Regression on our training data.\n"}
{"snippet": "batch_size = 100\nn_epochs = 10\ntraining = model.fit(traindata,trainlabels,\n                     nb_epoch=n_epochs,\n                     batch_size=batch_size,\n                      validation_data=(testdata, testlabels),\n                     verbose=1)\n", "intent": "In this step we will train the network and also define the number of epochs and batch size for training.\n"}
{"snippet": "from sklearn.neural_network import MLPClassifier\nprint MLPClassifier()\n", "intent": "<center>\n<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2016/03/2.-ann-structure.jpg\" width=\"500\">\n</center>\n"}
{"snippet": "kM = KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "estim_array = np.array( range(10,150, 1 ))\nscores = np.zeros(estim_array.shape)\nfor i , estim in enumerate(estim_array):\n    bagg = ensemble.BaggingClassifier(n_estimators = estim , oob_score = True)\n    model = bagg.fit(xTrain, yTrain)\n    scores[i] = model.oob_score_\n", "intent": "<font color= 'blue'>\n</font>\n<br>\n<br>\n** 1. Bagging:**\n<br>\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1000))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.summary()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "gird_search = GridSearchCV(SVC(),param_grid, verbose = 1, refit= True)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "lm.fit(X, bos.PRICE)\nprint(lm.intercept_)\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "from distutils.version import LooseVersion\nimport warnings\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nif not tf.test.gpu_device_name():\n    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\nelse:\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\nassert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\nprint('TensorFlow Version: {}'.format(tf.__version__))\n", "intent": "This will check to make sure you have the correct version of TensorFlow and access to a GPU\n"}
{"snippet": "b_size = [32,64,128]\nhistory_runs = []\nfor b in b_size:\n    print(f'Training on batchsize {b}')\n    history_runs.append(model.fit(X_train, Y_train, epochs=20, batch_size=b, \n                                  shuffle=True, validation_data=(X_test,Y_test), callbacks=[early_stopping]))\n", "intent": "Now to to see how different hyperparameters affect the network:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "D = tf.convert_to_tensor(np.array([[1., 2., 3.], \n                                   [-3., -7., -1.], \n                                   [0., 5., -2.]]))\nprint(sess.run(D))\n", "intent": "Create matrix from np array:\n"}
{"snippet": "naiveB = KNeighborsRegressor(n_neighbors=50)\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "print(scaler.fit(df.drop('TARGET CLASS', axis=1)))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "model.fit(df.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "model = baseline_model()\n", "intent": "Let us build the model.\n"}
{"snippet": "model = models.ldamodel.LdaModel(corpus = corpus, num_topics = 20, id2word = id2word, passes = 10)\n", "intent": "(Check https://radimrehurek.com/gensim/models/ldamodel as needed)\n"}
{"snippet": "model = keras.Sequential()\nmodel.add(keras.layers.Conv1D(filters=18, kernel_size=3, strides=1, padding='same', activation='relu', input_shape=X_train[0].shape))\nmodel.add(keras.layers.MaxPooling1D(pool_size=2, padding='valid'))\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(y_train[0].shape[0], activation='relu'))\nprint(model.summary())  \n", "intent": "You will want to use\nSee for examples on creating Keras modelshttps://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=100,\n          epochs=50,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "x = tf.placeholder(shape=[None,13],dtype=tf.float32, name='x-input')\nx_n = tf.layers.batch_normalization(x, training=True)\ny_ = tf.placeholder(shape=[None],dtype=tf.float32, name='y-input')\n", "intent": "Define input data placeholders\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=32,\n          epochs=10,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "features_flat = features.flatten()\nleakyrelu = np.zeros(features_flat.shape)\nfor xi in range(0,len(features_flat)):\n    if features_flat[xi] <=0:\n        leakyrelu[xi] = 0.01*features_flat[xi]\n    else:\n        leakyrelu[xi]=features_flat[xi]\nleakyrelu = np.reshape(leakyrelu,features.shape)\nprint('Leaky ReLU activation maps\\n',leakyrelu)\n", "intent": "leaky ReLu with negative slope coefficient = 0.01\n"}
{"snippet": "lr = LinearRegression()\ntype(lr)\n", "intent": "**Step 2:** \"Instantiate\" the \"estimator\"\n- \"Estimator\" is scikit-learn's term for \"model\"\n- \"Instantiate\" means \"make an instance of\"\n"}
{"snippet": "model_svc = grid.best_estimator_\nmodel_svc.fit(X_train, y_train)\n", "intent": "Tomemos el mejor estimador y re-entremos el modelo en el dataset completo:\n"}
{"snippet": "model.fit(x_train, y_train, epochs=5, batch_size=100, verbose=2, validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn import svm, datasets\nC = 1.0\nsvc = svm.SVC(kernel='poly', C=C).fit(X, y)\n", "intent": "Now we'll use linear SVC to partition our graph into clusters:\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(features_train, target_train)\n", "intent": "Let's start with a simple model from sklearn, namely `LogisticRegression`:\n"}
{"snippet": "model = keras.models.load_model('../data/keras_cnn_model.h5')\nwith open(\"../data/keras_cnn_history.p\", \"rb\") as f:\n    history = pickle.load(f)\n", "intent": "Load model and history:\n"}
{"snippet": "combined = keras.models.Model([noise, img_class], [valid, labels])\ncombined.compile(\n    loss=['binary_crossentropy', 'sparse_categorical_crossentropy'],\n    optimizer=optimizer\n)\n", "intent": "The combined model  (stacked generator and discriminator) takes\nnoise as input => generates images => determines validity \n"}
{"snippet": "for _ in range(4):\n    with tf.Session(config=tf.ConfigProto(device_count = {'GPU': 0})) as sess:\n        mdl.init.run()\n        with tf.variable_scope(\"H\", reuse=True):\n            w = tf.get_variable(\"kernel\")\n            print(w.eval(session=sess))\n", "intent": "We can evaluate a tensor with multiple init just to see the reproductibility\n"}
{"snippet": "with tf.Session() as sess:\n    result = sess.run(matrix_multi)\n    print(result)\n", "intent": "Now run the session to perform the Operation:\n"}
{"snippet": "attn_repeat_layer = RepeatVector(max_len_input, name='attn_repeat_layer')\nattn_concat_layer = Concatenate(axis=-1, name='attn_concat_layer')\nattn_dense1 = Dense(10, activation='tanh', name='attn_weights_dense_1')\nattn_dense2 = Dense(1, activation=softmax_over_time, name='attn_weights_dense_2')\nattn_dot = Dot(axes=1,name='attn_dot_layer')\n", "intent": "<img src='../images/attention_computation.png' style=\"width:500px;height:500px;\"/>\n"}
{"snippet": "0.125*np.identity(3)\n", "intent": "Matrix objects act sensibly when multiplied by scalars:\n"}
{"snippet": "print(np.identity(16)[0:1]) \n", "intent": "state : np.identity(16)[s1:s1+1] \n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nX_test=np.array([90, 106, 105, 115, 113]).reshape(-1,1)\nY_test=np.array([103, 131, 85, 99, 144]).reshape(-1,1)\nmodel=LinearRegression()\nfitted_model=model.fit(X_train,Y_train)\nr_square = fitted_model.score(X_test,Y_test)\nprint ('The R-squared metric is',r_square)\n", "intent": "|Xi|Yi|\n|:--:|:-------------------------------:|\n|90|103|\n|106|131|\n|105|85|\n|115|99|\n|113|144|\n"}
{"snippet": "from sklearn.cluster import KMeans\nn_clusters=3\nkm = KMeans(n_clusters,random_state=1)\nkm.fit(features[['mpg','disp']])\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "model=DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "from keras.layers import Dropout\nmodel_reg = Sequential()\nmodel_reg.add(Dense(10, activation='relu', input_shape=(dimData,)))\nmodel_reg.add(Dropout(0.5))\nmodel_reg.add(Dense(10, activation='relu'))\nmodel_reg.add(Dropout(0.5))\nmodel_reg.add(Dense(nClasses, activation='softmax'))\n", "intent": "Temos um caso claro de Overfitting. Para combater, aplicaremos **dropout**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogres = LogisticRegression()\nlogres.fit(x_train,y_train)\nprint(\"Best score: {:.2f}\".format(logres.score(x_test,y_test)))\n", "intent": "<b>Logistic Regression</b>\nAt first, logistic regression is shown. This algorithm showed a relatively low score of 0.75 of the tests to be correct.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded_input = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.01))\n    return tf.nn.embedding_lookup(embedded_input, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "scaler.fit(data.drop('Class',axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "dt = tree.DecisionTreeClassifier()\ndt\n", "intent": "<img src=\"images/iris_scatter.png\" height=\"500\" width=\"500\">\n"}
{"snippet": "approx = pm.fit(model=model, n=5000)\n", "intent": "We can use the same model with ADVI as follows. \n"}
{"snippet": "best_model_name = None\nLR_model = models[best_model_name].fit(X_train,y_train)\nprint(\"Train score: \", LR_model.score(X_train,y_train))\nprint(\"Test score: \", LR_model.score(X_test, y_test))\n", "intent": "After chossing the best scoring CV model we fit it on the entire training set and then evaluate it on the test set.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100)\n", "intent": "Create the random forest object:\n"}
