{"snippet": "print(\"Accuracy: \", imdb_model.evaluate(x_test_proc, y_test)[1])\n", "intent": "After training we can evaluate our model on the test set.\n"}
{"snippet": "predTree = drugTree.predict(X_testset)\n", "intent": "Let's make some <b>predictions</b> on the testing dataset and store it into a variable called <b>predTree</b>.\n"}
{"snippet": "print(classification_report(y_test, grid_predictions))\n", "intent": "NOW that's something\n"}
{"snippet": "import pylab \nimport scipy.stats as stats\nresids = X-model._raw_predict(ts.reshape(-1,1))[0][:,0]\nstats.probplot(resids, dist=\"norm\", plot=pylab)\npylab.show()\n", "intent": "After some tuning of parameteres I chose to fix noise variance. As result we get normal residuals.\n"}
{"snippet": "SGD_score_on_training = metrics.accuracy_score(y_train_1, SGD_prediction_on_training)\nSGD_precision_on_training = metrics.precision_score(y_train_1, SGD_prediction_on_training, pos_label=\"REAL\")\nSGD_recall_on_training = metrics.recall_score(y_train_1, SGD_prediction_on_training, pos_label=\"REAL\")\nSGD_f1_on_training = metrics.f1_score(y_train_1, SGD_prediction_on_training, pos_label=\"REAL\")\nprint(\"accuracy for SGD on training dataset1:   %0.3f\" % SGD_score_on_training)\nprint(\"precision for SGD on training dataset1:   %0.3f\" % SGD_precision_on_training)\nprint(\"recall for SGD on training dataset1:   %0.3f\" % SGD_recall_on_training)\nprint(\"f1 for SGD on training dataset1:   %0.3f\" % SGD_f1_on_training)\n", "intent": "**Prediction of Labels(Fake/Real)**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_predicted = net.forward(X_test)\ny_predicted = np.argmax(y_predicted, axis=1)\naccuracy_score(y_predicted, Y_test)\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "print(np.sum((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint(\"Test accuracy: {:.2f}\".format(test_accuracy) + \"%\")\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "clf, y_pred = fit_and_predict(X_train_scaled, y_train, X_test_scaled, 'rbf')\n", "intent": "Run the following cell to actually train on the flights data. It might take a while.\n"}
{"snippet": "print(classification_report(predicted_y,y_test))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nmean_absolute_error(ytest, rf1.predict(Xtest))\n", "intent": "Since our response is very skewed, we may want to suppress outliers by using the `mean_absolute_error` instead. \n"}
{"snippet": "model.predict(X[0:2,:])\n", "intent": "Let's predict probabilities for the first two examples.\n"}
{"snippet": "r_predictions = rdtree.predict(X_test)\n", "intent": "Vamos prever os valores do y_test e avaliar o nosso modelo.\n** Preveja a classe de not.fully.paid para os dados X_test. **\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "Create new predictions based on our model, using X_test\n"}
{"snippet": "print(\"precesion for BOW with alpha 0.001 is\",precision_score(test_label_y, predicted, average=\"macro\"))\nprint(\"Recall for BOW with alpha 0.001 is\",recall_score(test_label_y, predicted, average=\"macro\"))  \n", "intent": "<b>f1 score for bag of words is 0.923</b>\n"}
{"snippet": "model.predict(input_test)                \n", "intent": "We can then use our model to predict the outputs to the test set. Again this is the same syntax no matter what the model.\n"}
{"snippet": "theta_min = np.matrix(result2[0])\npredictions = predict(theta_min, X2)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y2)]\naccuracy = (sum(map(int, correct)) % len(correct))\nprint \"accuracy = {0}%\".format(accuracy)\n", "intent": "Finally, we can use the prediction function from part 1 to see how accurate our solution is on the training data.\n"}
{"snippet": "medae = metrics.median_absolute_error(y_test, predictions)\nd_medae = metrics.median_absolute_error(y_test, d_predictions)\nprint \"MedAE / LR = {0:.4}, Dummy = {1:0.4}\".format(medae, d_medae)\n", "intent": "$$ MedAE(y, \\hat y) = median( | y_1 - \\hat y_1 |, ..., | y_n - \\hat y_n | ) $$\n"}
{"snippet": "print(accuracy_score(y_test, y_pred))\n", "intent": "Let's see what the accuracy score is.\n"}
{"snippet": "y_pred = nb.predict(X_test)\nf1_score(y_test, y_pred, average='macro')\n", "intent": "  * Classificeer de test data met behulp van de getrainde Naive Bayes.\n  * Bereken de gemiddelde F1-score (average='macro')\n"}
{"snippet": "pred = svm.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "print(\"The Coefficient of Determination (R^2) is: \\n{:.5f}\".format(r2_score(y_mlr,result_mlr)))\nprint(\"The Root Mean Squared Error (RMSE) is: \\n{:.5f}\".format(np.sqrt(mean_squared_error(y_mlr,result_mlr))))\nprint(\"The Mean Absolute Error (MAE) is: \\n{:.5f}\".format(mean_absolute_error(y_mlr,result_mlr)))\nprint(\"The Correlation Coefficient (CC) is: \\n{}\".format(np.corrcoef(result_mlr,y_mlr)))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "guesswho = model.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "pred_rf = rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "y_pred = cross_val_predict(regr, X_pca, y, cv=5)\n", "intent": "* Worse than Cross validation with all 8 attributes\n"}
{"snippet": "user_prediction = predict(data_matrix, user_similarity, type='user')\nitem_prediction = predict(data_matrix, item_similarity, type='item')\n", "intent": "Finally, we will make predictions based on user similarity and item similarity.\n"}
{"snippet": "log_reg_pred = log_reg.predict_proba(test)[:, -1]\n", "intent": "Next step is predictions and selects the correct column:\n"}
{"snippet": "pred = clf.predict(transformed[:, :-222])\nis_anomaly = pred != selected['geoNetwork.country'].cat.codes\n", "intent": "We set suspect anomaly cases as those whose predicted *country* value does not match up with the value in data:\n"}
{"snippet": "exam_result_predicted = np.array([predict(a, b, time) for time in all_times])\nprint(\"Predicted:\", exam_result_predicted)\nprint(\"Actual:   \", exam_result)\n", "intent": "Let's now call `predict()` for every input value and compare our outputs to the original ones.\n"}
{"snippet": "print(linreg.predict(100))\n", "intent": "Let's predict how much is the average Sales of a company that spends 100,000 dollars in TV ads. \n"}
{"snippet": "metrics.silhouette_score(X_scaled, labels, metric='euclidean')\n", "intent": "And to compute the clusters' silhouette coefficient:\n"}
{"snippet": "logreg.predict_proba([1, 0, 29, 1])[:, 1]\n", "intent": "Predict probability of survival for **Susan**: same as Adam, except female.\n"}
{"snippet": "X_pred = autoencoder.predict([X_train[0:5]])\npl.imshow(X_pred[0].squeeze(), interpolation='nearest', cmap=cm.binary)\npl.colorbar()\n", "intent": "A theano function to get the representation of a given input (without reconstructing it)\n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import mean_squared_error\nlin_mse = mean_squared_error(y_pred, y_test)\nlin_rmse = np.sqrt(lin_mse)\nprint('Liner Regression RMSE: %.4f' % lin_rmse)\n", "intent": "Calculate root-mean-square error (RMSE)\n"}
{"snippet": "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3, n_jobs=-1)\nf1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\n", "intent": "**Warning**: the following cell may take a very long time (possibly hours depending on your hardware).\n"}
{"snippet": "y_pred = dnn_clf.predict(X_test1)\naccuracy_score(y_test1, y_pred)\n", "intent": "Let's check that we do indeed get 99.32% accuracy on the test set:\n"}
{"snippet": "def print_rmse(model, df):\n  metrics = model.evaluate(input_fn = make_eval_input_fn(df))\n  print('RMSE on dataset = {}'.format(np.sqrt(metrics['average_loss'])))\nprint_rmse(model, df_valid)\n", "intent": "Evaluate on the validation data (we should defer using the test data to after we have selected a final model).\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\nnp.sqrt(metrics.mean_squared_error(y_test, preds))\n", "intent": "Take a minute to discuss Root Mean Squared Error [RMSE](https://www.kaggle.com/wiki/RootMeanSquaredError)\n"}
{"snippet": "r2_lr = r2_score(ytest, ypred)\nprint(r2_lr)\n", "intent": "Compute and print the training and test $R^2$ score here: \n"}
{"snippet": "rms = sqrt(mean_squared_error(test.Weekly_Sales, y_hat_avg.moving_avg_forecast))\nprint(rms)\n", "intent": "We chose the data from the last eight weeks. We will now calculate RMSE to check to accuracy of our model.\n"}
{"snippet": "y_pred = LogReg1.predict(x_test1)\n", "intent": "**Use the Model to predict on x_test and evaluate the model using metric(s) of Choice.**\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "Evaluate the different classifiers with cross validation\n"}
{"snippet": "print('Model Parameters:\\n')\npprint(best_grid.get_params())\nprint('\\n')\nevaluate(best_grid, test_features, test_labels)\n", "intent": "The final model from hyperparameter tuning is as follows.\n"}
{"snippet": "preds = predict(net2, md.val_dl).argmax(1)\nplots(x_imgs[:8], titles=preds[:8])\n", "intent": "Let's look at our predictions on the first eight images:\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(ypred, ytest))\n", "intent": "We can take a look at the classification report for this classifier:\n"}
{"snippet": "prediction_maps = fully_conv_ResNet.predict(np.random.randn(1, 200, 300, 3))\nprediction_maps.shape\n", "intent": "You can use the following random data to check that it's possible to run a forward pass on a random RGB image:\n"}
{"snippet": "from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, y_score)\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\n", "intent": "Compute the average precision score\n...................................\n"}
{"snippet": "logodds = 0.64722323 + 4.1804038614510901\nodds = np.exp(logodds)\nprob = odds/(1 + odds)\nprint('Probability Calculated for al=3: ', prob)\nprint('Probability computed by model for al=3: ', logreg.predict_proba(3)[:, 1])\n", "intent": "<b>Interpretation:</b> A 1 unit increase in 'al' is associated with a 4.18 unit increase in the log-odds of 'household'.\n"}
{"snippet": "r2_rf = r2_score(ytest, ypred)\nprint(r2_rf)\n", "intent": "Compute and print the training and test $R^2$ score here: \n"}
{"snippet": "def pos_tag(sentence):\n    print('checking...')\n    tagged_sentence = []\n    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n    return zip(sentence, tags)\nimport platform\nprint(platform.python_version())\n", "intent": "for results the link of pos_tags:\nhttps://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n"}
{"snippet": "logit_1.predict_proba([[3], [4]])\n", "intent": "- We can access the probability to be in each class predicted by the logistic regression as well:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(colleges['Cluster'], kmeans.labels_))\nprint(classification_report(colleges['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "train_preds = lr.predict(X_train_level2) \nr2_train_stacking = r2_score(y_train_level2, train_preds) \ntest_preds = lr.predict(X_test_level2) \nr2_test_stacking = r2_score(y_test, test_preds) \nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "def predict(x_in): return sess.run(y, feed_dict={x: [x_in]})\n", "intent": "Demonstrate saving and restoring a model\n"}
{"snippet": "print(classification_report(y_pca_test, y_pred_et_smote_pca))\n", "intent": "** Classification Report **\n___\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "dtree_score = cross_val_score(dtree, features, labels, n_jobs=-1).mean()\nprint(\"{0} -> DTree: {1})\".format(columns, dtree_score))\n", "intent": "Notice gender has a fairly important level of importance.\nLet's test the accuracy again:\n"}
{"snippet": "inp = np.expand_dims(conv_val_feat[0],0)\nnp.round(lrg_model.predict(inp)[0], 2)\n", "intent": "We have to add an extra dimension to our input since the CNN expects a 'batch' (even if it's just a batch of one).\n"}
{"snippet": "r2_ridge = r2_score(ytest, ypred)\nprint(r2_ridge)\n", "intent": "Compute and print the training and test $R^2$ score here: \n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print (metric)\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=skf_seed) \n    NNF = NearestNeighborsFeats.NearestNeighborsFeats(n_jobs=4, k_list=k_list, metric=metric)\n    preds = cross_val_predict(NNF, X, Y, cv=skf) \n    np.save('data/knn_feats_%s_train.npy' % metric, preds)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint('The training accuracy is', train_accuracy)\nprint('The test accuracy is', test_accuracy)\n", "intent": "Now, let's see how our model does, let's calculate the accuracy over both the training and the testing set.\n"}
{"snippet": "class_pred = model.predict(images_test, batch_size=32)\nprint(class_pred[0])\n", "intent": "Predict class for test set images\n"}
{"snippet": "logreg.predict([[1.67]])[0]\n", "intent": "Scikit-learn also has an in-built `.predict()` function for our regression model which will output either `0` or `1` for a given argument:\n"}
{"snippet": "print('R-squared value is', metrics.r2_score(y, intercept + slope*x))\nprint('MAE is', metrics.mean_absolute_error(y, intercept + slope*x))\nprint('Mean and std are', df['Data'].mean(), df['Data'].std())\n", "intent": "The metrics for the whole set are:\n"}
{"snippet": "for_pred = dfor.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "roc_auc_score(df['admit'], lm.predict(feature_set))\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "y_test_6 = model_5.predict(X_test1)\ny_test_6_prob = model_5.predict_proba(X_test1)\nmetrics.roc_auc_score(db_result['FLG_DEF_6M'].tolist(), y_test_6_prob[:,1])\n", "intent": "Valuto la bonta' del risultato:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(bos.PRICE, lm.predict(X))\nprint(mse)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "print('Model Parameters:\\n')\npprint(best_grid.get_params())\nprint('\\n')\npredictions = evaluate(best_grid, test_features, test_labels)\n", "intent": "The final model from hyperparameter tuning as follows\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import GroupKFold\nkfold = GroupKFold(n_splits=7)\ny_predict = cross_val_predict(model, X_train, y_train,cv=kfold,groups=train['SentenceId'])\nprint(y_predict.shape)\ny_predict[0:10]\n", "intent": "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"}
{"snippet": "predictions_1 = model_bag.predict(testing_data)\npredictions_2 = model_forest.predict(testing_data)\npredictions_3 = model_boost.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "score = model.evaluate(X_train, y_train)\nprint(\"\\n Training Accuracy:\", score[1])\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"\\n Testing accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model. \n"}
{"snippet": "y_pred = linreg.predict(X)   \nmetrics.r2_score(y, y_pred)  \n", "intent": "Let's confirm the R-squared value for our simple linear model using `scikit-learn's` prebuilt R-squared scorer:\n"}
{"snippet": "print(\"Classification Report:\\n\", metrics.classification_report(y_test,y_test_pred))\n", "intent": "Or we can compute the full classification report, which will give us precision/recall per-feature:\n"}
{"snippet": "def gbm_predict(X, base_algorithms_list, coefficients_list):\n    return [sum([coeff * algo.predict([x])[0] for algo, coeff in zip(base_algorithms_list, coefficients_list)]) for x in X.values]\n", "intent": "$L'(y,z) = 2(y - z) = c*(y - z)$\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_vgg19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions) == np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predicted_labels = classifier.predict( testing_data )\n", "intent": "That's it! **The classifier is now trained on the training data!**\nNow predict the value of the digit on the second half:\n"}
{"snippet": "yhat=lm.predict(new_input)\nyhat[0:5]\n", "intent": "Produce a prediction \n"}
{"snippet": "y_pred = fit_decision(X_train, y_train, X_test, random_state=check_random_state(0))\naccuracy = accuracy_score(y_test, y_pred)\nprint('The accuracy score is {:0.2f}.'.format(accuracy))\n", "intent": "- Function `fit_decision()` trains a **Decision Trees** model.\n"}
{"snippet": "y_pred = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "y_pred_class = knn.predict(X)\nfrom sklearn import metrics\nprint metrics.accuracy_score(y, y_pred_class)\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "leo = np.array([[3, 22, 0]])\nkate = np.array([[1, 25, 1]])\nprint(m.predict(leo))\nprint(m.predict(kate))\n", "intent": "There is (at least) one error in the definition of the data for prediction. Can you find and fix it?\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_modelD.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50d]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = svc_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "metrics.silhouette_score(X_scale, labels, metric='euclidean') \n", "intent": "Compute the Silhoutte Score\n"}
{"snippet": "labels = model.predict(points)\n", "intent": "**Step 5:** Use the `.predict()` method of `model` to predict the cluster labels of `points`, assigning the result to `labels`.\n"}
{"snippet": "predictions = rtree.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "def mae_score(y_true, y_pred):\n    return mean_absolute_error(np.exp(y_true), np.exp(y_pred))\nmae_scorer = make_scorer(mae_score, greater_is_better=False)\n", "intent": "We'll also use a custom scorer which works with log-transformed values:\n"}
{"snippet": "def get_accuracy(ypred, ytest):\n    accuracy = accuracy_score(ytest, ypred)\n    return accuracy\n    raise NotImplementedError()\n", "intent": "Get the performance (accuracy) of your algorithm given ytest\n"}
{"snippet": "metrics.confusion_matrix(y, model1.predict(X))\n", "intent": "Compute confusion matrix.\n"}
{"snippet": "ten_fold_cv = ms.StratifiedKFold(n_splits=10, shuffle=True)\naucs = ms.cross_val_score(gtb1, X, y, scoring='roc_auc', cv=ten_fold_cv)\nnp.mean(aucs)\n", "intent": "Evaluate performance through cross-validation.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint (\"RMSE:\", math.sqrt(mean_squared_error(ys, predictions)))\nprint (\"MAE:\", mean_absolute_error(ys, predictions))\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "y_train_pred = clf.predict(X_train)\n", "intent": "Predict the training set.\n"}
{"snippet": "cv = ShuffleSplit(n_splits=5, test_size=.2)\ncross_val_score(classifier, X, y, cv=cv)\n", "intent": "You can use all of these cross-validation generators with the cross_val_score method:\n"}
{"snippet": "def train(self, training_data, epochs, mini_batch_size, learning_rate):\n    training_data = list(training_data)\n    for i in range(epochs):\n        mini_batches = self.create_mini_batches(training_data, mini_batch_size)\n        cost = sum(map(lambda mini_batch: self.update_params(mini_batch, learning_rate), mini_batches))\n        acc = self.evaluate(training_data)\n        print(\"Epoch {} complete. Total cost: {}, Accuracy: {}\".format(i, cost, acc))\n", "intent": "We shall implement backpropogration with stocastic mini-batch gradient descent to optimize out network.\n"}
{"snippet": "test_predictions = model.predict_proba(X_test).argmax(axis=-1)\ntest_answers = y_test.argmax(axis=-1)\ntest_accuracy = np.mean(test_predictions==test_answers)\nprint(\"\\nTest accuracy: {} %\".format(test_accuracy*100))\nassert test_accuracy>=0.92,\"Logistic regression can do better!\"\nassert test_accuracy>=0.975,\"Your network can do better!\"\nprint(\"Great job!\")\n", "intent": "So far, our model is staggeringly inefficient. There is something wrong with it. Guess, what?\n"}
{"snippet": "r50_predictions = [np.argmax(r50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(r50_predictions)==np.argmax(test_targets, axis=1))/len(r50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def ResidSE(x, y, n, lm):\n    return np.sqrt(np.sum((lm.predict(x) - y)**2)/(n-2))\ndef StandErr_b0(RSE, n, x):\n    xbar = np.mean(x)\n    return np.sqrt(RSE**2*(1/n + xbar**2/np.sum((x - xbar)**2)))\ndef StandErr_b1(RSE, n, x):\n    xbar = np.mean(x)\n    return np.sqrt(RSE**2*(1/np.sum((x - xbar)**2)))\n", "intent": "Now we'll define functions for the RSE and standard errors:\n"}
{"snippet": "X_real, X_fake = np.array([next(data_point) for _ in range(batch_size)]).reshape(-1,1), G.predict(z)\n", "intent": "- Evaluate $\\mathcal{D}$'s accuracy on the fake images $\\mathcal{G}$ produces from `Z_`\n- Save them into two python variables `loss` and `acc`\n"}
{"snippet": "predictions = nb.predict(X_teste)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "metrics.silhouette_score(data, kmeans.labels_, metric='euclidean')\n", "intent": "Compute the silhoutte coefficient to see how consistent our data is within the clusters\n"}
{"snippet": "print(y[:30])\nprint(model.predict(X)[:30])\n", "intent": "It turns out that, for this dataset, the classifier is pretty much perfect:\n"}
{"snippet": "auc = roc_auc_score( subreddit_id_binary[test_idx], y_score[:,1])\nprint(\"{0:.2f}\".format(auc))\n", "intent": "To evalute how are classifer had done we find the AUC (Area Under Curve) of a ROC Curve and plot a precision recall curve. \n"}
{"snippet": "prediction_rfc = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h5>4.3.1.3.2. Incorrectly Classified point</h5>\n"}
{"snippet": "y_predict = knn_1.predict(X)\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "lasso = Lasso(alpha=optimal_lasso.alpha_)\nlasso_scores = cross_val_score(lasso, Xs, Y.values.ravel(), cv=10)\n", "intent": "---\nIs it better than the Linear regression? Is it better than Ridge? What do the differences in results imply about the issues with the dataset?\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccs = cross_val_score(knn, X, y, cv=10)\nprint(accs)\nprint(np.mean(accs))\n", "intent": "Use 10 folds. How does the mean performace across folds compare to the baseline accuracy?\n"}
{"snippet": "def predict(clf, timeline):\n    return y_pred\n", "intent": "- Use the SVM classifier to classify each tweet in timeline as a positive tweet or a negative tweet.\n"}
{"snippet": "print(\"R-squared value of this fit:\",round(metrics.r2_score(y_train,train_pred),3))\n", "intent": "**R-square of the model fit**\n"}
{"snippet": "prediction = classifier.predict(X_test)\n", "intent": "We can then apply the model to unseen data and use the model to predict the estimated outcome using the ``predict`` method:\n"}
{"snippet": "model.load_weights('imdb.model.best.hdf5')\nscore = model.evaluate(x_test, y_test, verbose=1)\nprint(\"\\nAccuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_train_pred = GR.predict(X_train)\ny_test_pred  = GR.predict(X_test)\ntrain_test_gr_error = pd.Series({'train': mean_squared_error(y_train, y_train_pred),\n                                         'test':  mean_squared_error(y_test, y_test_pred)},\n                                          name='MSE').to_frame().T\ntrain_test_gr_error\n", "intent": "The error on train and test iris_data sets. Since this is continuous, we will use mean squared error.\n"}
{"snippet": "pred_sem3_train = model_semantic3.predict(designmat_sem3_train)\npred_acc_sem3_train = correlate(data_train, pred_sem3_train)\n", "intent": "Predict the training data and correlate those predictions with the real response amplitudes.\n"}
{"snippet": "y_train_pred_gr = GR.predict(X_train)\ny_test_pred_gr = GR.predict(X_test)\ntrain_test_gr_error = pd.concat([measure_error(y_train, y_train_pred_gr, 'train'), \n                                 measure_error(y_test, y_test_pred_gr, 'test')], \n                                 axis = 1)\ntrain_test_gr_error\n", "intent": "The number of nodes is 119 and the maximum depth of the tree is 9. \n"}
{"snippet": "print logreg.predict(X_test)[0:10]\nprint logreg.predict_proba(X_test)[0:10, :] \nlogreg.predict_proba(X_test)[0:10,1]\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n", "intent": "Now we will look into how we can MODIFY THE performance of a classifier by adjusting the classification threshold. \n"}
{"snippet": "predictions = model.predict(X)\nprint(f\"True output: {y[0]}\")\nprint(f\"Predicted output: {predictions[0]}\")\nprint(f\"Prediction Error: {predictions[0]-y[0]}\")\n", "intent": "We can use our model to make predictions.\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(df['Cluster'],kmean_clustering.labels_))\nprint('\\n')\nprint(classification_report(df['Cluster'],kmean_clustering.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "clf, ts_preds = classify_topics(\n    nmf, nmf.transform(train_data), train['target'], test_data, check_random_state(0)\n    )\nprint(classification_report(test['target'], ts_preds, target_names=test['target_names']))\n", "intent": "The resulting classification report and confusion matrix are shown to demonstrate the quality of this classification method.\n"}
{"snippet": "train_and_evaluate(clf, X_train, X_test, y_train, y_test)\n", "intent": "Now, lets fit our modelfrom the train set and test it against the test set. Explain why our test set contains 25 measurements.\n"}
{"snippet": "def precision(actual, preds):\n    return None \nprint(precision(y_test, preds_nb))\nprint(precision_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def recall(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    act_pos = (actual==1).sum()\n    return float(tp)/act_pos\nprint(recall(y_test, preds_nb))\nprint(recall_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "from sklearn.metrics import f1_score\ndef my_f1_score(trained_model):\n    return None\nassert my_f1_score(trained_knn) == f1_score(trained_knn['y_test'],\n                                            trained_knn['y_test_pred']), \\\n         'Those are not the same'\n", "intent": "Complete the following \"roll your own\" method for calculating F1-Score.\n"}
{"snippet": "sp_ypred =  spatial.predict(coordinates[test,:])\natt_ypred = attribute.predict(X[test,:])\nboth_ypred = both.predict(np.hstack((X,coordinates))[test,:])\n", "intent": "To score them, I'm going to grab their out of sample prediction accuracy and get their % explained variance:\n"}
{"snippet": "print(TP/float(TP + FP))\nprint(metrics.precision_score(y_test, y_pred_class))\n", "intent": "<b>6. Precision: </b> When a positive value is predicted, how often is the prediction correct?\n"}
{"snippet": "y_pred = regression.predict(X_test)\n", "intent": "$MSPE(\\{x_i, y_i\\}_{i=1}^l, \\, w) = \\frac{1}{l}\\sum_{i=1}^l \\left( \\frac{y_i - \\langle w, x_i \\rangle }{y_i} \\right)^2$\n"}
{"snippet": "y_pred = knn.predict(X_train)\n", "intent": "```\ny_pred = knn.predict(X_train)```\n"}
{"snippet": "pred = pipeline.predict([text])\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "def predict(clf, timeline):\n    return y_pred\n", "intent": "- Use the RandomForestClassifier to classify each tweet in timeline as a positive tweet or a negative tweet.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n", "intent": "Now we can check the accuracy of our model:\n"}
{"snippet": "predict(image_path=image_paths_train[1])\n", "intent": "We can try it for another image in our new training-set and the VGG16 model is still confused.\n"}
{"snippet": "shutil.rmtree(path = \"babyweight_trained_wd\", ignore_errors = True) \ntrain_and_evaluate(\"babyweight_trained_wd\")\n", "intent": "Finally, we train the model!\n"}
{"snippet": "metrics.mean_squared_error([1, 2, 3, 4, 5], [1, 2, 3, 4, 5])\n", "intent": "For example, if we to compare two arrays of the same values, we would expect a mean squared error of 0:\n"}
{"snippet": "predicted_values = knm.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.10f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "reconstruction=model.predict(x_test)\n", "intent": "Now we want to get the target of the AE - meaning the reconstructed image.\n"}
{"snippet": "print('Coefficients: \\n', regr.coef_)\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(x_test) - y_test) ** 2))\n", "intent": "The plot shows our model fits the data well.\n"}
{"snippet": "print(\"Graph A: learning rate = 0.0001\")\nplot_error(.0001)\n", "intent": "Plot the error as a function of the number of iterations for various learning rates. Choose the rates\nso that it tells a story.\n"}
{"snippet": "def get_accuracy(ypred, ytest):\n    return 100*accuracy_score(ypred,ytest)\n", "intent": "Get the performance (accuracy) of your algorithm given ytest\n"}
{"snippet": "print 'Estimated number of clusters: %d' % n_clusters_\nprint \"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels)\nprint \"Completeness: %0.3f\" % metrics.completeness_score(y, labels)\nprint \"V-measure: %0.3f\" % metrics.v_measure_score(y, labels)\n", "intent": "**7.2 Check the homogeneity, completeness, and V-measure against the stored rank `y`**\n"}
{"snippet": "print \"Accuracy = %.3f\" % (metrics.accuracy_score(decision_tree.predict(X), Y))\n", "intent": "If we use this as our decision tree, how accurate is it?\n"}
{"snippet": "fit <- lm(assists ~ fgMade, data=train)\npredictions <- predict(fit, test)\n", "intent": "Let's try doing some predictions based on linear regression. Say we want to predict assists in terms of field goals.\n"}
{"snippet": "tuned_forest_predictions = locally_best_forest.predict(X_test) \naccuracy_score(y_test,tuned_forest_predictions)\n", "intent": "Make a prediction for test data.\n"}
{"snippet": "y_pred = model.predict(X_test)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100 * np.sum(np.array(Xception_predictions) == np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = model.predict(X_test)\nprint(\"KNN Test Accuracy is {0:.2f}\".format(accuracy_score(y_test, y_pred) * 100))\n", "intent": "Finally let's see how our estimator performs on the hold-out test set\n"}
{"snippet": "preds_train = model.predict(X_train, verbose=0)\npreds_test = model.predict(X_test, verbose=0)\npreds_train = (preds_train > 0.5).astype(np.int)\npreds_test = (preds_test > 0.5).astype(np.int)\n", "intent": "Do a prediction for both train and test\n"}
{"snippet": "print(\"Residual sum of squares: %.2f\"\n      % np.mean((regr.predict(X_test) - y_test) ** 2))\nprint('Variance score: %.2f' % regr.score(X_test, y_test))\n", "intent": "e) Evalute the R^2 on training data. Is this good? Bad? Why?\n"}
{"snippet": "import copy\ndef predict(model, train_set, width, nof_points):\n    prediction = []\n    x_test = copy.deepcopy(train_set[-width : ]) \n    for i in range(nof_points):\n        prediction.append(model.predict(x_test.reshape((1, -1))))\n        x_test[0 : -1] = x_test[1 : ]\n        x_test[-1] = prediction[-1]\n    return np.array(prediction)\n", "intent": "Now change the ```width``` parameter to see if you can get a better score.\nNow execute the code below to see the prediction of this model.\n"}
{"snippet": "action_layer.epsilon.set_value(0.05)\nuntrained_reward = np.mean(pool.evaluate(save_path=\"./records\",\n                                         record_video=True))\n", "intent": "Play one full game with untrained network, record video\n"}
{"snippet": "pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "predictions = lr.predict(new_data_reindexed)\n", "intent": "We can now make predictions using our model.\n"}
{"snippet": "models=[tree,svr,MLP,neigh,randomforest,GBR,knn,ada_neigh]\nrss_list=[]\nfor model in models:\n    rss_list.append(sum((y_test-model.predict(X_test_pca))**2))\n", "intent": "Residual Sum of Square of each model in comparison list\n"}
{"snippet": "print \"Class predictions according to Sklearn:\" \nprint sentiment_model.predict(sample_test_matrix)\n", "intent": "Run the following code to verify that the class predictions obtained by your calculations are the same as that obtained from GraphLab Create.\n"}
{"snippet": "y_estimado = fit.predict(X)\nprint(\"Reales   :\", y[0:25])\nprint(\"Estimados:\", y_estimado[0:25])\n", "intent": "Se pueden evaluar las clases estimadas por el modelo vs. las clases reales\n"}
{"snippet": "from sklearn.metrics import r2_score\ndef print_r2_score():\n    r2 = \n    print(\"R2 score: {:.2f}\".format(r2))\n", "intent": "Write a function that:\n* Takes y_test en y_pred as parameters.\n* Calculates the $R^2$ score.\n* Prints the result.\n"}
{"snippet": "preds = model.predict(test_sample)\nerrors = [i for i in xrange(0, len(test_sample)) if preds[i] != test_labels_sample[i]]\nfor i in errors:\n    query_img = test_sample[i]\n    _, result = model.kneighbors(query_img, n_neighbors=4)\n    show(query_img)\n    show(train[result[0],:], len(result[0]))\n", "intent": "* Next, visualize the nearest neighbors of cases where the model makes erroneous predictions\n"}
{"snippet": "cm = sklearn.metrics.confusion_matrix(y_train,clf.predict(X_train))\ncm_norm = cm/cm.sum(axis=1)[:, np.newaxis]\ncm_norm\n", "intent": "Plot the (normalised) confusion matrix for the training data. Label axes appropriately.\n"}
{"snippet": "bank_features_predicted = grid_search.best_estimator_.predict(bank_features_test)\ntuned_score = f1_score(bank_labels_test, bank_features_predicted)\n", "intent": "Use the best estimator from your grid search. Score it using the function from problem 6. Save your answer in `tuned_score`.\n"}
{"snippet": "print \"Root Mean Squared Logarithmic Error Train: \", rmsele(rf_op.predict(xtrain), ytrain)\nprint \"Root Mean Squared Logarithmic Error Test: \", rmsele(rf_op.predict(xtest), ytest)\nprint \"Training accuracy: %0.2f%%\" % (100*rf_op.score(xtrain, ytrain))\nprint \"Test accuracy: %0.2f%%\" % (100*rf_op.score(xtest, ytest)) + \"\\n\"\n", "intent": "It is clear from the above plot that month 9 has the most effect on the outcome.\n"}
{"snippet": "print(classification_report(y_test,preds))\nprint(confusion_matrix(y_test,preds))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "y_pred = linreg.predict(X)\nglass.loc[:, 'y_pred'] = y_pred\n", "intent": "- Add a column `y_pred` to `glass` that stores the model's fitted values for the refractice index.\n"}
{"snippet": "knr.predict(dp_scaled)\n", "intent": "Let's use our trained KNeighborsRegressor to predict the value for our scaled datapoint:\n"}
{"snippet": "y_predict_full = model.predict(X)\nmodel.score(X, y)\n", "intent": "+ Is this error reliable? \n+ What could we do to make it better?\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nyhat_train = linregpipe.predict(X_train)\nyhat_valid = linregpipe.predict(X_valid)\nmse_train = mean_squared_error(yhat_train, y_train)\nmse_valid = mean_squared_error(yhat_valid, y_valid)\nprint(\"Training MSE:   {:.3f}\".format(mse_train))\nprint(\"Validation MSE: {:.3f}\".format(mse_valid))\nxplot = np.linspace(-60,60,100).reshape(-1,1)\nyplot = linregpipe.predict(xplot) \ndam_plot([(X_train, y_train, \"training\"), (X_valid, y_valid, \"validation\")], [(xplot, yplot, \"model\")])\n", "intent": "**Part B**: Next we'll make predictions on the training and validation sets and evaluate the mean-squared error. \n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"RMSE:\", mean_squared_error(ys, predictions)**.5\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "test_wf         = vectorizer.transform(test['description'])\ntest_prediction = nb_classifier.predict(test_wf)\n", "intent": "Precision: % of selected items that are correct \nRecall: % of correct items that are selected\n"}
{"snippet": "best = clf.best_estimator_\nprint('Best estimator: ', best)\nscores = cross_validation.cross_val_score(best, X_test, y_test, cv=10)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "Then we can check the accuracy on the **Test Set**:\n"}
{"snippet": "def predict(model, train_set, width, nof_points):\n    prediction = []\n    x_test = copy.deepcopy(train_set[-width : ]) \n    for i in range(nof_points):\n        prediction.append(model.predict(x_test.reshape((1, -1))))\n        x_test[0 : -1] = x_test[1 : ]\n        x_test[-1] = prediction[-1]\n    return np.array(prediction)\n", "intent": "Now change the ```width``` parameter to see if you can get a better score.\nNow execute the code below to see the prediction of this model.\n"}
{"snippet": "scores = cross_val_score(su, X_cross, Y_cross, cv=10)  \nprint(\"Accuracy :\", scores)\n", "intent": "Due to lack of computational power I am using 1/10 fraction of train.csv dataset for cross validation.\n"}
{"snippet": "import numpy as np\nxinput = np.array([[3.0, 5.0, 4.1, 2.0]])\npred_class_number = knn.predict(xinput)\nprint( iris.target_names[pred_class_number] )\n", "intent": "We can test it out by making a prediction for a new input example described by 4 feature values:\n"}
{"snippet": "from sklearn.metrics import classification_report\nclass_rep_tree = classification_report(test_labels,pred_labels_tree)\nclass_rep_log = classification_report(test_labels,pred_labels_logit)\nprint(\"Decision Tree: \\n\", class_rep_tree)\nprint(\"Logistic Regression: \\n\", class_rep_log)\n", "intent": "This is better but as in our project for the course, let's use more metrics to evaluate our models.\n"}
{"snippet": "ex_1d_x = np.array([50, 150], dtype=np.float32)\npredict = k_means_estimator.predict(input_fn=lambda: input_fn_1d(ex_1d_x), as_iterable=False)\n", "intent": "*Note that the predict() function expects the input exactly like how we specified the feature vector*\n"}
{"snippet": "split = 0.5\nX_train, X_test, y_train, y_test = load_dataset(split)\nprint('Training set: {0} samples'.format(X_train.shape[0]))\nprint('Test set: {0} samples'.format(X_test.shape[0]))\nk = 3\ny_pred = predict(X_train, y_train, X_test, k)\naccuracy = compute_accuracy(y_pred, y_test)\nprint('Accuracy = {0}'.format(accuracy))\n", "intent": "Should output an accuracy of 0.9473684210526315.\n"}
{"snippet": "y_pred = model.predict(X_test)\nprint(confusion_matrix(y_actual, y_pred))\n", "intent": "Let's first print the confusion matrix as we usually do.\n"}
{"snippet": "loss, acc = new_model.evaluate(test_images, test_labels)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n", "intent": "Check its accuracy:\n"}
{"snippet": "    predictions = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "preds = predict(X_test, 0.8, model)\n", "intent": "Make predictions on testing data. Store predictions in preds variable.\n"}
{"snippet": "def predict(model, ts_data, width, nof_points):\n    prediction = []\n    x_test = np.copy(ts_data[-width : ])\n    for i in range(nof_points):\n        prediction.append(model.predict(x_test.reshape((1, -1)))[0])\n        x_test[0 : -1] = x_test[1 : ]\n        x_test[-1] = prediction[-1]\n    return np.array(prediction)\n", "intent": "Now we're going to visualize our prediction by repeating the one-step-ahead prediction. \n"}
{"snippet": "test_files = [im for im in os.listdir(TEST_DIR)]\ntest = np.ndarray((len(test_files), ROWS, COLS, CHANNELS), dtype=np.uint8)\nfor i, im in enumerate(test_files): \n    test[i] = read_image(TEST_DIR+im)\ntest_preds = model.predict(test, verbose=1)\n", "intent": "Finishing off with predictions on the test set.\n"}
{"snippet": "docs_new = ['God is love', 'OpenGL on the GPU is fast']\nX_new_counts = count_vect.transform(docs_new)\nX_new_tfidf = tfidf_transformer.transform(X_new_counts)\npredicted = clf.predict(X_new_tfidf)\nfor doc, category in zip(docs_new, predicted):\n    print('%r => %s' % (doc, twenty_train.target_names[category]))\n", "intent": "Predicting on a new document\n"}
{"snippet": "pred = svc1.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "print acc_score(y_test, y_pred)\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\npredictions = svc.predict(X_test)\nprint(confusion_matrix(y_test, predictions))\nprint('\\n')\nprint(classification_report(y_test, predictions))\n", "intent": "This might not go the way you expect it to.\n"}
{"snippet": "n = len(y)\ncv = sklearn.cross_validation.StratifiedShuffleSplit(n, n_iter=10, test_size=.25)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "Of course, you can also do a stratified version of this:\n"}
{"snippet": "cv = sklearn.cross_validation.StratifiedShuffleSplit(y, n_iter=10, test_size=.25)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "Of course, you can also do a stratified version of this:\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "**Root mean squared error (RMSE)** is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncvs = cross_val_score(enet, X, y, cv =5)\ncvs\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "score = model.evaluate(testvectors, Y_test)\nprint(\"Test loss\", score[0])\nprint(\"Test accuracy\", score[1])\n", "intent": "An overall accuracy measure can also be obtained by means of the **evaluate** method of the model\n"}
{"snippet": "predicted = result.predict(X_test)\nprobs = result.predict_proba(X_test)\nprint metrics.accuracy_score(y_test, predicted)\n    print metrics.roc_auc_score(y_test, probs[:, 1])\n", "intent": "Answer: A one unit increase in GPA doubles the odds of being admitted.  \n"}
{"snippet": "predictions = lmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "def recall(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1))) \n    actual_pos = np.sum(actual==1) \n    return tp/(actual_pos)\nprint(recall(y_test, preds_rf))\nprint(recall_score(y_test, preds_rf))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "cv_scores = []\nkf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 2016)\nfor dev_index, val_index in kf.split(range(train_X.shape[0])):\n        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n        dev_y, val_y = train_y[dev_index], train_y[val_index]\n        preds, model = runXGB(dev_X, dev_y, val_X, val_y)\n        cv_scores.append(log_loss(val_y, preds))\n        print(cv_scores)\n        break\n", "intent": "Without CV statistic,to score get 0.5480 by SRK. And CV statistic get 0.5346 In fact ,you \nneed to turn down the learning rate and turn up run_num\n"}
{"snippet": "EXAMPLES = ['the fifty day of 2015', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "pred_dtree = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "preds_binary = rtf.predict(X_test)\npreds_prob = rtf.predict_proba(X_test)\nabs_diff = abs(preds_binary - y_test)\nwrong_instances = X_test[abs_diff==1,:]\nright_instances = X_test[abs_diff==0,:]\n", "intent": "Extract the wrongly classified instances (False positive and false negative instances)\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "preds = log_model.predict(X= train_features)\npd.crosstab(preds,df[\"Survived\"])\n", "intent": "Next, let's make class predictions using this model and then compare the predictons to the actual values:\n"}
{"snippet": "r2_score(ytest, ytest_model)\n", "intent": "Compute and print the training and test $R^2$ score here: \n"}
{"snippet": "accuracy_score(Y_test,Y_pred)\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "def predict(ratings, similarity, type='item'):\n    if type == 'item':\n        print np.shape(ratings)\n        print np.shape(similarity)\n        result = np.dot(ratings,similarity)\n    if type == 'user':\n        rating_mean = np.mean(ratings,axis=1)\n        rating_diff = (ratings - rating_mean[:,np.newaxis])\n        result = rating_mean[:,np.newaxis] + np.dot(similarity,rating_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n    return result\n", "intent": "<img src=\"user_predict.gif\">\n<img src=\"item_predict.gif\">\n"}
{"snippet": "newdata = test_rgb.reshape(M * N, -1)[:,:]\ncluster = gmm.predict(newdata)\ncluster = cluster.reshape(M, N)\n", "intent": "Apply to the test image and plot\n"}
{"snippet": "prediction = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print('RMSE Train', sqrt(mean_squared_error(y_train_pred, y_train)))\nprint('RMSE Test' , sqrt(mean_squared_error(y_test_pred, y_test)))\n", "intent": "The below calculation shows that RMSE value for both training and test data is similar and less too. It revals that model is a good fit.\n"}
{"snippet": "preds = model.predict(Xr)\npreds\n", "intent": "Visualize predictions\n"}
{"snippet": "preds = model.predict(X)\nsum(preds == y)/float(X.shape[0])\n", "intent": "This is what's known as the accuracy score or the percentage of prediction we got correct.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "**Wow we got 91% accuracy in test set using OvO**\n"}
{"snippet": "print(TP / float(TP + FN))\nprint(metrics.recall_score(y_test, y_pred))\n", "intent": "How sensitive is the classifier to detecting positive instances?\nAKA \"True Positive Rate\" or \"Recall\"\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\npreds = model.predict(test_tensor)\npreds = np.argmax(preds, axis=1)\nY_test_binary = np.argmax(Y_test,axis=1)\nconfusion_matrix = confusion_matrix(Y_test_binary, preds)\nconfusion_matrix\n", "intent": "In this case adding more epochs ends making the model overfit (increasing the test loss), so I keep 20 epochs.\n"}
{"snippet": "lambda_ = T_distill * T_distill\ntrain_loss = get_cross_entropy_loss(logits=predictions_student, labels=batch_train_labels)\ntrain_loss += lambda_ * distill_kl_loss\n", "intent": "**Define the joint training loss**\n"}
{"snippet": "rf_pred = rfmodel.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "predictionTree = dtree.predict(X_test)\n", "intent": "Let's evaluate our decision tree.\n"}
{"snippet": "y_pred = best_sclf.predict(X_test)\nprint(\"Lift Score for Stacking Classifier:\")\nlift_score(y_test, best_sclf.predict(X_test))\n", "intent": "Tuning for lift yields the same Best Parameter as before and thus the same model\n"}
{"snippet": "input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={'images': mnist.test.images}, y=mnist.test.labels,\n    batch_size=batch_size, shuffle=False)\nmodel.evaluate(input_fn)\n", "intent": "Evaluate the Model (**mnist.test.images**). Define the input function for evaluating.\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "Lasst uns den Entscheidungsbaum auswerten.\n"}
{"snippet": "metrics.silhouette_score(y, labels, metric='euclidean')\n", "intent": "Compute the silhoutte coefficient to see how consistent our data is within the clusters\n"}
{"snippet": "print(confusion_matrix(y_test, predictions))\nprint('\\n')\nprint(classification_report(y_test, predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100.0*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Test accuracy: %.2f%%\" % (scores[1]*100))\n", "intent": "Once we found our best model, we can evaluate its performance on the test set. In our case, we would like to compute its accuracy:\n"}
{"snippet": "def calculate_model_error_accuracy(X_train, y_train, y_test,predictions):\n    trainScore = mean_squared_error(X_train, y_train)\n    print('Train Score: %.4f MSE (%.4f RMSE)' % (trainScore, math.sqrt(trainScore)))\n    testScore = mean_squared_error(predictions, y_test)\n    print('Test Score: %.8f MSE (%.8f RMSE)' % (testScore, math.sqrt(testScore)))\n", "intent": "   **Calculate the model error **\n"}
{"snippet": "rasnet50_predictions = [np.argmax(rasnet50_dog_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\ntest_accuracy = 100*np.sum(np.array(rasnet50_predictions)==np.argmax(test_targets, axis=1))/len(rasnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "metrics.accuracy_score(y, predY)\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "pred_RF = RF_clf.predict(X_test)\n", "intent": "Predicting the Random Forest Classifier on test set\n"}
{"snippet": "res_predictions = [np.argmax(res_model.predict(np.expand_dims(feature, axis=0))) for feature in test_res]\ntest_accuracy = 100*np.sum(np.array(res_predictions)==np.argmax(test_targets, axis=1))/len(res_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import metrics\ndt_predicted = dt_classifier.predict(X_test)\nprint(\"Classification report:\")\nprint(metrics.classification_report(y_test, dt_predicted)) \n", "intent": "Scikit-learn will can print out the **Recall** and **Precision** scores for a classification model by using `metrics.classification_report()`.\n"}
{"snippet": "part_test = np.array(['57111'])\ncomplaint_test = np.array(['BRAKE PEDAL FEELS HARD'])\nX_new_part_labelencoded = enc_label.transform(part_test)\nX_new_part_onehot = enc_onehot.transform(X_new_part_labelencoded.reshape(-1,1))\nX_new_complaint_counts = count_vect.transform(complaint_test)\nX_new_complaint_tfidf = tfidf_transformer.transform(X_new_complaint_counts)\nX_new_combined_tfidf = sparse.hstack((X_new_part_onehot, X_new_complaint_tfidf), format='csr')\npredicted = clf.predict(X_new_combined_tfidf)\nprint(predicted)\n", "intent": "This should return 1:\n"}
{"snippet": "yhat=lr.predict(Z_test)\nyhat[0:4]\n", "intent": " We can make a prediction:\n"}
{"snippet": "prediction_train = lm.predict(X=X_train)\nprint('Prediction of training data:',prediction_train)\nprice_predict = prediction_train  \n", "intent": "By looking into the attributes of your model, write down an equation for predicting the price of a car given the engine-power.\n"}
{"snippet": "[model.predict(np.expand_dims(validation_inputs[i], 0)) for i in range(10)]\n", "intent": "Let's print the predicted and actual labels for the first 10 observations.\n"}
{"snippet": "constant = np.ones(len(X_train))[np.newaxis].T\nXandBias = np.hstack((constant, X_train ))\nw = np.array([lrModel.intercept_, lrModel.coef_[0]])[np.newaxis].T\ndef myPredict(table, biasAndweights):\n    return table.dot(biasAndweights)\nmyPredictions = myPredict(XandBias, w).T[0]\nmodelPredictions = lrModel.predict(X_train)\nprint \"our equation results in exactly the same results as the predict function of the linear model for all inputs\"\nnp.all(myPredictions == modelPredictions)\n", "intent": "By looking into the attributes of your model, write down an equation for predicting the price of a car given the engine-power.\n"}
{"snippet": "predictions = knn.predict(test_x)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "To predict the label probabilites of the test set using this model we can use the following code.\n"}
{"snippet": "print \"accuracy  {:.3}\".format(accuracy_score(ytest, ypred))\nprint \"precision {:.3}\".format(precision_score(ytest, ypred))\nprint \"recall    {:.3}\".format(recall_score(ytest, ypred))\n", "intent": "Results from testing the model on the unbalanced data \nThese are the definitive metrics that inform us about how well this classifier will perform.\n"}
{"snippet": "my_model = my_model_1\nmy_preds = my_model.predict(test_data)\npreds = my_preds\nmost_likely_labels = decode_predictions(preds, top=1, class_list_path='../input/resnet50/imagenet_class_index.json')\nfor i, img_path in enumerate(img_paths[:2]):\n    display(Image(img_path))\n    print(most_likely_labels[i])\n", "intent": "**Method 1: Results**\n"}
{"snippet": "print('MAE: ',metrics.mean_absolute_error(y_test, pred))\nprint('MSE: ', metrics.mean_squared_error(y_test, pred))\nprint('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, pred)))\n", "intent": "** Calculating Errors **\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nrf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\nsvc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\nprint(\"AUC for Random Forest: {:.3f}\".format(rf_auc))\nprint(\"AUC for SVC: {:.3f}\".format(svc_auc))\n", "intent": "- A good summary measure is the area under the ROC curve (AUROC or AUC)\n- Compute using the `roc_auc_score` \n    - Don't use `auc`\n"}
{"snippet": "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(estimator, X, Y_enc, cv=kfold)\nprint \"Baseline Acc: %.2f%% (%.2f%%)\"%(results.mean()*100, results.std()*100)\n", "intent": "Evaluate baseline model\n"}
{"snippet": "print(classification_report(y_test, y_predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "samples = 100\npredicted_vals = [model.predict(np.expand_dims(validation_inputs[i], 0)) for i in range(samples)]\npredicted = [np.rint(x[0][0]) for x in predicted_vals]\nactual = validation_labels[:samples]\nsum(abs(predicted-actual) == 0)/len(predicted) * 100\n", "intent": "As an additional check, let's compute the validation accuracy for the first 100 samples.\n"}
{"snippet": "X_more = np.random.uniform(X.min(), X.max(), size=(10, 2))\nX_more\nmore_cluster_labels = kmeans.predict(X_more)\n", "intent": "KMeans can generalize, SpectralClustering can not\n-----------------------------------------------------------\nKMeans has a ``predict`` function.\n"}
{"snippet": "from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\ncross_val_score()\n", "intent": "Test the performance of the AdaBoost and GradientBoostingClassifier models on the car dataset. Use the code you developed above as a starter code.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\npredicted_salaries = model.predict(X_test)\nprint (mean_absolute_error(y_test, predicted_salaries))\n", "intent": "As mentioned, our evaluation criteria is going to be MAE - Mean Absolute Error, we want the lowest average error of salary prediction  \n"}
{"snippet": "class ContentLoss(nn.Module):\n    def __init__(self, target,):\n        super(ContentLoss, self).__init__()\n        self.target = target.detach()\n    def forward(self, input):\n        self.loss = F.mse_loss(input, self.target)\n        return input\n", "intent": "- image X\n- content image C\n- FXL  of a layer L\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom math import sqrt\ndef rmse(y_true, y_pred):\n    rmse = sqrt(mean_squared_error(y_true, y_pred))\n    return rmse\n", "intent": "Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between log(predicted value) and log(observed values) of sales price.\n"}
{"snippet": "cross_val_score(est, X, y)\n", "intent": "And we can use the `cross_val_score` function to check the predictive accuracy:\n"}
{"snippet": "X_test = full_matrix[idx_split:,:]\ntest_pred = lrfinal.predict_proba(X_test)[:, 1]\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)\n", "intent": "But cross-validation comes at the cost of training\nthe model several times, so it is not always possible.\n"}
{"snippet": "predictions = model.predict(test_x)\n", "intent": "---\n<a id=\"predictions\"></a>\n"}
{"snippet": "activations2 = model_diagnostic.predict(np.expand_dims(number_to_input_example(4), 0))\n", "intent": "Next, let's look at the activations for a number that's not divisible by 3 and plot the same results.\n"}
{"snippet": "def precision(actual, preds):\n    tp = len(np.intersect1d(np.where(actual == 1), np.where(preds == 1)))\n    pp = (preds == 1).sum()\n    return  tp/pp\nprint(precision(y_test, preds_nb))\nprint(precision_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "custom_tree=AC209Tree.build_tree(data_missing_dt_train.as_matrix(), max_depth=5)\nyhat_custom_train = AC209Tree.predict(custom_tree, X_dt_train.as_matrix())\ntrain_score = AC209Tree.accuracy_metric(yhat_custom_train,y_dt_train.as_matrix())\nyhat_custom_test = AC209Tree.predict(custom_tree, X_dt_test.as_matrix())\ntest_score = AC209Tree.accuracy_metric(yhat_custom_test,y_dt_test.as_matrix())\nprint('Optimal Custom Decision Tree Depth: 5')\nprint ('Train score:', train_score)\nprint ('Test score:', test_score)\n", "intent": "**From CV it looks like the optimal max_tree_depth for Custom Decision Tree is 5**\n"}
{"snippet": "y_pred = my_tree.predict(X_train)\naccuracy = metrics.accuracy_score(y_train, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_train, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_train), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the decision tree on the training set\n"}
{"snippet": "p_test = np.zeros_like(y_test)\nfor flip in [False, True]:\n    temp_x = x_test\n    if flip:\n        temp_x = img.flip_axis(temp_x, axis=2)\n    p_test += 0.5 * np.reshape(model.predict(temp_x, verbose=1), y_test.shape)\n", "intent": "Little bit of TTA, predict on both horisontal orientations.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccs = cross_val_score(knn, X, y, cv=10)\nprint accs\nprint np.mean(accs)\n", "intent": "Use 10 folds. How does the performace compare to the baseline accuracy?\n"}
{"snippet": "from sklearn.metrics import r2_score\nimport numpy as np\nx= np.array(trainX)\ny = np.array(trainY)\np = np.poly1d(np.polyfit(x,y,4))\nr2 = r2_score(y,p(x))\nprint \"For training set, R2 is \", r2\nr2 = r2_score(np.array(testY),p(np.array(testX)))\nprint \"For test set, R2 is \", r2\n", "intent": "Try measuring the error on the test data using different degree polynomial fits. What degree works best?\n"}
{"snippet": "predictions = rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "predictions=clf.predict(X_test)\n", "intent": "Testing the outcome with various performance metrics\n"}
{"snippet": "from sklearn import metrics\ny_predict = knn1.predict(X)\nprint(y_predict)\nprint(metrics.accuracy_score(y, y_predict))\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "def predict(model, ts_data, width, nof_points):\n    prediction = []\n    x_test = np.copy(ts_data[-width : ])\n    for i in range(nof_points):\n        prediction.append(model.predict(x_test.reshape((1, -1)))[0])\n        x_test[0 : -1] = x_test[1 : ]\nraise NotImplementedError()\n    return np.array(prediction)\n", "intent": "Now we're going to visualize our prediction by repeating the one-step-ahead prediction. \n"}
{"snippet": "train_preds = meta_model.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2, train_preds)\ntest_preds = meta_model.predict(stacked_test_pred)\nr2_test_stacking = r2_score(y_test, test_preds)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "knn.predict([3, 5, 4, 2])\n", "intent": "In order to **make a prediction**, the new observation must have the **same features as the training observations**, both in number and meaning.\n"}
{"snippet": "def predict(n):\n", "intent": "and as you can see, the network's performance has shot up significantly - despite only making a few changes. \n"}
{"snippet": "def train_classifier(clf, X_train, y_train):\n    start = time()\n    y_pred = clf.predict(features)\n    end = time()    \n    return y_pred\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n", "intent": "Defining functions to train models, predict labels and show metric results: accuracy and f1-score.\n"}
{"snippet": "rf_predictions = rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(\"****** Test Data ********\")\npred = model.predict_classes(test)\nprint(metrics.classification_report(test_labels, pred))\nprint(\"Confusion Matrix\")\nprint(metrics.confusion_matrix(test_labels, pred))\n", "intent": "Use the test dataset to evaluate the model\n"}
{"snippet": "y_pred = my_tuned_model.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluate the perofmrance of the model selected by the grid search on a hold-out dataset\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfor clf in ??:\n    clf.??\n    y_pred = ??\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n", "intent": "Predict 5 classifiers trained above and calculate accuracy using 'accuracy_score'\n"}
{"snippet": "print metrics.classification_report(y,labels)\n", "intent": "Check the labels of the clusters\n"}
{"snippet": "clf2, y_pred2 = cv_svc_pipe(X_train, y_train, X_test, random_state=check_random_state(0))\nscore2 = accuracy_score(y_pred2, y_test)\nprint(\"SVC prediction accuracy = {0:3.1f}%\".format(100.0 * score2))\n", "intent": "- Now we'll build a pipeline curing `CountVectorizer` and `LinearSVC`\n"}
{"snippet": "rmse = np.sqrt(mean_squared_error(b, multib_pred))\nprint (\"The Root mean squared error is : {}\" .format(rmse))\nprint (\"The Mean absolute error is : {}\" .format(mean_absolute_error(b, multib_pred)))\nprint (\"The Correlation Coefficient is : {}\" .format((np.corrcoef(b, multib_pred))[0][1]))\n", "intent": "<font color='red'>TASK MARK: 2</font>\n<br>\n<font color='green'>COMMENT:  Avoid spurious precision. </font>\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "new_model.compile(optimizer='adam', \n              loss=tf.keras.losses.sparse_categorical_crossentropy,\n              metrics=['accuracy'])\nloss, acc = new_model.evaluate(test_images, test_labels)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n", "intent": "Run the restored model.\n"}
{"snippet": "logreg.predict_proba(0)[:, 1]\n", "intent": "**Interpretation:** A 1 unit increase in 'diabetesMed' is associated with a 0.214549 unit increase in the log-odds of 'assorted'.\n"}
{"snippet": "err(y_train, m_gb.predict(X_train))\n", "intent": "What are the errors for both the train and test sets?\n"}
{"snippet": "X16=df2015a[['Q1 Sales','Volume Sold(L)']]\npredictions16 = lm.predict(X16)\npredictions16b = lm1.predict(X16)\nSales2016=sum(predictions16)\nSales2015=df2015['Sales'].sum()\nprint 'The Total sales for 2015: $%d' %Sales2015\nprint \"The Projected Sales for 2016: $%d\" %Sales2016\n", "intent": "Our Adjusted R^2 Value(0.990) is quite good, according to this model, we should be able to predict 2016 sales based on q1 sales.\n"}
{"snippet": "pred = dtree.predict(X_test)\n", "intent": "Let's evaluate our decision tree.\n"}
{"snippet": "print (classification_report(y_test, predicted_y_rf))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "mse= np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint(mse)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "clf3, y_pred3 = cv_svc_pipe_sw(X_train, y_train, X_test, random_state=check_random_state(0))\nscore3 = accuracy_score(y_pred3, y_test)\nprint(\"SVC prediction accuracy = {0:3.1f}%\".format(100.0 * score3))\n", "intent": "- Finally, we'll add English stop words.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df1['Cluster'],kmeans.labels_))\nprint(classification_report(df1['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "- Use the ``accuracy_score`` utility to see how well we did.\n"}
{"snippet": "y_pred = regressor.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred = lr.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred = clf_cv.predict(Xtestlr)\naccuracy_score(y_pred,ytestlr)\n", "intent": "***best C = 0.001 value obtained with scikit_learn.GridSearchCV differs from manually calculated best C : [0.1, 1.0, 10.0, 100.0]***\n"}
{"snippet": "mnb_true = y\nmnb_pred = mnb_classifier.predict(x)\nconfusion_matrix(mnb_true, mnb_pred)\n", "intent": "**d)** Compute the resulting confusion_matrix by using the builtin scikit-learn class and display the result. \n"}
{"snippet": "hide_code\nmodel.load_weights('weights.best.model.hdf5')\nscore = model.evaluate(x_test, y_test)\nscore\n", "intent": "We should have an accuracy greater than 50%\n"}
{"snippet": "from lib import gfx  \nfrom sklearn import metrics\nfrom math import sqrt\ny_pred = xg_reg.predict(test_dmatrix)\nrmse_score = sqrt(metrics.mean_squared_error(y_test, y_pred))\nprint('\\n\ngfx.plot_actual_vs_predicted(y, y_test, y_pred)\n", "intent": "Evaluates the model on the holdout set:\n"}
{"snippet": "def display_prediction(idx):\n    print(predict(W, X_test[idx, :]).argmax())\n    return display_image(X_test[idx].reshape(28, 28))\n", "intent": "Nice, with 10 epochs we get an accuracy of ~89%.\n"}
{"snippet": "trees, trees_weights = adaboost(X, Y, 266)\nYhat_test = adaboost_predict(X_test, trees, trees_weights)\nerr_test = 1-accuracy(Y_test, Yhat_test)\n", "intent": "When numTrees = 266, validation accuracy is the highest and has a value of 0.9542. Corresponding train accuracy is 0.9465.\n"}
{"snippet": "for i in range(2):\n    tau, _ = stats.kendalltau(\n        ridge.predict(X_test[b_test == i]), y_test[b_test == i])\n    print('Kendall correlation coefficient for block %s: %.5f' % (i, tau))\n", "intent": "- and the [Kendall tau](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) for the test data is:\n"}
{"snippet": "print classification_report(y_test, pred)\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions = nb.predict(X_test)\n", "intent": "Define the target and feature set for the test data\n"}
{"snippet": "threshold = 0.5\npreds = predict(X_test,threshold,model)\n", "intent": "Make predictions on testing data. Store predictions in preds variable.\n"}
{"snippet": "def anomaly_score(error, dist):\n    delta = np.abs(error - dist.mean())\n    return dist.cdf(dist.mean() + delta)\n", "intent": "We could also come up with a continuous scoring system using the cumulative density function.\n"}
{"snippet": "test_probs = lr.predict_proba(X_test)[:,1]\nfpr, tpr, thres = roc_curve(y_test, test_probs)\n", "intent": "Onto plotting the ROC curve\n"}
