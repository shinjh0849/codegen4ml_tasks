{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "Split it into a training set and a test set:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n", "intent": "Don't forget to scale the data:\n"}
{"snippet": "df = pd.read_csv('/home/data_scientist/data/misc/sim.reg.data')\ndf.head()\n", "intent": "The cell below reads in a simulated dataset where y (labels) is an unknown function of a,b, and c (your features).\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "b. Split it into a training set and a test set using `train_test_split()`.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX2D = pca.fit_transform(X)\n", "intent": "With Scikit-Learn, PCA is really trivial. It even takes care of mean centering for you:\n"}
{"snippet": "X3D_inv = pca.inverse_transform(X2D)\n", "intent": "Recover the 3D points projected on the plane (PCA 2D subspace).\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=0.95)\nX_train_reduced = pca.fit_transform(X_train)\n", "intent": "*Exercise: Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%.*\n"}
{"snippet": "from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, random_state=42)\nX_reduced = tsne.fit_transform(X)\n", "intent": "Now let's use t-SNE to reduce dimensionality down to 2D so we can plot the dataset:\n"}
{"snippet": "idx = (y == 2) | (y == 3) | (y == 5) \nX_subset = X[idx]\ny_subset = y[idx]\ntsne_subset = TSNE(n_components=2, random_state=42)\nX_subset_reduced = tsne_subset.fit_transform(X_subset)\n", "intent": "Let's see if we can produce a nicer image by running t-SNE on these 3 digits:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_housing_data = scaler.fit_transform(housing.data)\nscaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n", "intent": "Gradient Descent requires scaling the feature vectors first. We could do this using TF, but let's just use Scikit-Learn for now.\n"}
{"snippet": "from sklearn.datasets import make_moons\nm = 1000\nX_moons, y_moons = make_moons(m, noise=0.1, random_state=42)\n", "intent": "First, let's create the moons dataset using Scikit-Learn's `make_moons()` function:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(data[:100])\nX_test = scaler.transform(data[100:])\n", "intent": "Normalize the data:\n"}
{"snippet": "df2 = pd.read_csv('/home/data_scientist/data/misc/sim.reg.w4p3.data')\nX = df2.drop('y', axis=1)\ny = df2['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\ndf2.head()\n", "intent": "The cell below reads in a more complex simulated dataset, where y is some unknown function of all (or a subset of) the features a,b,c,d,e, and f.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler_all = StandardScaler()\ndf_data_scaled = scaler_all.fit_transform(df_data)\n", "intent": "DBSCAN does not have the concept of 'predict'. \nWe'll use the \"full\" dataset (first 300 values), without splitting train and test.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../data/research-and-development-expenditure-by-type-of-cost/research-and-development-expenditure-by-type-of-cost.csv',\n                 usecols=['sector', 'type_of_expenditure', 'type_of_cost', 'rnd_expenditure'])\ndf.head()\n", "intent": "1. Encode string labels to numbers\n2. Ensure dataset is balanced\n"}
{"snippet": "dimensions = [2, 3, 5, 7]\nfor n in dimensions:\n    print('========= Projection into %d dimensions =========' % n)\n    pca = KernelPCA(n_components=n)\n    train_X_pca = pca.fit(train_X).transform(train_X)\n    test_X_pca = pca.transform(test_X)\n    print('PCA explained variance ratio: %s' % str(pca.explained_variance_ratio_))\n    TrainRFClassifier(train_X_pca, train_y, test_X_pca, test_y)        \n", "intent": "The code below performs PCA projections before fitting the RandomForestClassifier.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX = df.loc[:, ['quota', 'premium']]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n", "intent": "1. Scale the features\n"}
{"snippet": "from sklearn.datasets import make_classification\nX, y = make_classification(\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n    n_informative=3, n_redundant=1, flip_y=0,\n    n_features=20, n_clusters_per_class=1,\n    n_samples=100, random_state=10\n)\ndf = pd.DataFrame(X)\ndf['target'] = y\ndf.target.value_counts().plot(kind='bar', title='Count (target)');\n", "intent": "For ease of visualization, let's create a small unbalanced sample dataset using the <code>make_classification</code> method:\n"}
{"snippet": "df = pd.read_csv('D:/tmp/graduate-employment-survey-ntu-nus-sit-smu-sutd/graduate-employment-survey-ntu-nus-sit-smu-sutd.csv',\n                 encoding='ISO-8859-1',\n                 usecols=['university', 'employment_rate_overall', 'gross_monthly_median'])\ndf = df.loc[df.university == 'National University of Singapore']\ndf.columns\n", "intent": "The first step is to inspect the data to see what transformation/cleaning is needed.\n"}
{"snippet": "data = {\n    'gross_monthly_median': pd.to_numeric(df.gross_monthly_median, errors='coerce'),\n    'employment_rate_overall': pd.to_numeric(df.employment_rate_overall, errors='coerce')\n}\ndf_dataset = pd.DataFrame(data).dropna()\ndf_dataset.head()\n", "intent": "The `gross_monthly_median` column is handled the same way.\nLet's now perform the data transformation and cleaning.\n"}
{"snippet": "df = pd.read_csv('D:/tmp/poker-hand/poker-hand-training-true.data',\n                 names=['S1', 'C1', 'S2', 'C2', 'S3', 'C3', 'S4', 'C4', 'S5', 'C5', 'CLASS'])\ndf.head()\n", "intent": "1. read_csv for both training and test set\n"}
{"snippet": "df = pd.read_csv('D:/tmp/beijing-pmi/PRSA_data_2010.1.1-2014.12.31.csv')\ndf.head()\n", "intent": "Check:\n- Did the data import correctly?\n- What are the types of each column?\n- How large is the dataset?\n"}
{"snippet": "df = pd.read_csv('/home/data_scientist/data/misc/sim.data')\ndf.head()\n", "intent": "The cell below reads in a simulated dataset where y(labels) are a unknown function of a, b, and c.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n", "intent": "4. train-test split\n"}
{"snippet": "with open(anchors_path, 'r') as f:\n    print(f.read())\n", "intent": "These are the pre-defined anchors, chosen to be representative of the ground truth detection boxes.\nThese are found using K-means clustering.\n"}
{"snippet": "with open(classes_path, 'r') as f:\n    print(f.read())\n", "intent": "These are the classes of objects that the detector will recognize.\n"}
{"snippet": "encoder = LabelEncoder()\ncategory_enc = encoder.fit_transform(df.Category)\ndf['y'] = category_enc\ndf\n", "intent": "Label encode the 'Category' column into numbers to form our output (Y).\n"}
{"snippet": "df = pd.read_csv('d:/tmp/news-article/Full-Economic-News-DFE-839861.csv', encoding='latin1',\n                usecols=['relevance', 'text'])\ndf.head()\n", "intent": "1. Load the data into pandas\n2. Check for NaNs (if any) and decide what you would do with them\n"}
{"snippet": "def play_midi(filename):\n    from music21 import midi\n    mf = midi.MidiFile()\n    mf.open(filename)\n    mf.read()\n    mf.close()\n    stream = midi.translate.midiFileToStream(mf)\n    stream.show('midi')\nplay_midi('Classical-Piano-Composer/test_output.mid')\n", "intent": "This produces the following file:\n`test_output.mid`\n"}
{"snippet": "df_train = pd.read_csv('fifa_processed_train.csv')\ndf_test = pd.read_csv('fifa_processed_test.csv')\n", "intent": "Load pre-processed data from Day 9 workshop.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n   'This is the first document.',\n   'This document is the second document.',\n   'And this is the third one.',\n   'Is this the first document?',\n]\ncv = CountVectorizer()\nresult = cv.fit_transform(corpus)\nprint(result) \n", "intent": "* Words that are used more frequently get a higher count\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus = [\n   'This is the first document.',\n   'This document is the second document.',\n   'And this is the third one.',\n   'Is this the first document?',\n]\ntfidf = TfidfVectorizer()\nresult = tfidf.fit_transform(corpus)\nresult.todense()\n", "intent": "- TF: Term Frequency: rewards words commonly used in a document\n- IDF: Inverse Document Frequency: penalises words commonly used in all documents\n"}
{"snippet": "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=.7, random_state=0)\n", "intent": "The code cell below creates a validation set.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n", "intent": "- Train test split\n- Iteratively:\n - Select model parameters\n - Cross validate\n - Plot learning curve\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nencoders = dict()\nfor f in features:\n    if (df[f].dtypes == 'object'):\n        encoder = LabelEncoder()\n        encoder.fit(df[f])        \n        df[f] = encoder.transform(df[f])\n        encoders[f] = encoder\n", "intent": "For simplicity, let's label encode these categorical features.\nHow to one-hot encode, for reference:\n```\npd.get_dummies(df['marital'])\n```\n"}
{"snippet": "Z_train, Z_test, y_train, y_test = train_test_split(Z_scaled, y)\n", "intent": "- Train/test split\n- Train a Decision Tree Classifier\n- Evaluation Metrics\n- Visualise the decision tree\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../data/imdb-review-dataset/imdb_master.csv',\n                 encoding='latin_1', index_col=0)\ndf1 = df.loc[df.label != 'unsup']\ndf1.label.value_counts() \n", "intent": "Dataset: https://www.kaggle.com/utathya/imdb-review-dataset/version/1\n"}
{"snippet": "df = pd.read_csv('./bitcoin/bitcoin/bitflyerJPY_1-min_data_2018-06-01_to_2018-06-27.csv')\ndf.head()\n", "intent": "```\npd.read_csv()\n```\n"}
{"snippet": "df1 = df.loc[df.Timestamp < '2018-06-02 00:00']\ndf1.to_csv('bitcoin_June1_2018.csv', index=False) \ndf1.head()\n", "intent": "```\ndf.to_csv()\n```\n"}
{"snippet": "array1 = df['Close'].values\nprint(array1)\ndf1 = pd.DataFrame(array1)\ndf1.head()\n", "intent": "Create Dataframe from Numpy array\n```\ndf = pd.DataFrame(numpy_array)\n```\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n", "intent": "We will perform feature scaling.\nBefore features can be scaled, we need to holdout (separate out) the test dataset from the training dataset. \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n", "intent": "No imbalance, go ahead and train a decision tree classifier\n"}
{"snippet": "df = pd.read_csv('/home/data_scientist/data/misc/sim.data')\ndf.drop('y', axis=1, inplace=True)\ndf.head()\n", "intent": "The cell below reads in a simulated dataset with the features a, b, and c.\n"}
{"snippet": "df_kmeans = pd.concat([pd.DataFrame(X, columns=['sepal length (cm)', 'sepal width (cm)',\n                                                'petal length (cm)', 'petal width (cm)']),\n                       pd.DataFrame(cluster_kmeans, columns=['cluster_id'])], axis=1)\n", "intent": "Let's explore the clusters using scatter matrix to see if we discover patterns.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, cluster_kmeans, random_state=42)\n", "intent": "Train a decision tree using any set of clusters from earlier workshop.\n"}
{"snippet": "df.to_csv('./dataset1_dataset2_solution.csv',index=False)\n", "intent": "Since there are no more duplicates, we can output the file\n"}
{"snippet": "_, bp = df4.drop(columns=['id','date','yr_renovated','waterfront','lat','long']).boxplot(figsize=(15,10),return_type='both');\noutliers = []\nfor flier in bp[\"fliers\"]:\n    outliers.append(list(flier.get_ydata()))\noutlier_no = []\nfor l in outliers:\n    outlier_no.append(len(l))\npd.DataFrame(np.array(outlier_no).reshape(1,13),columns=df4.drop(columns=['id','date','yr_renovated','waterfront','lat','long']).columns)    \n", "intent": "After the deletion, we check the number of outliers once more\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(input = 'content', analyzer = 'word')\ntfidf_vectors = tfidf_vectorizer.fit_transform(sents_words)\ntfidf_vectors.shape\n", "intent": "Write your code below to generate TF-IDF vector.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(boston_df.iloc[:,:-1],boston_df.iloc[:,-1:], random_state = 1)\n", "intent": "we will select a feature to degrade (remove some values as missing data)\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(boston_df.iloc[:,:-1],boston_df.iloc[:,-1:],random_state=111)\n", "intent": "\\begin{equation}\ny_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\n\\end{equation}\n"}
{"snippet": "CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\nFEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS) - 1]\nLABEL = CSV_COLUMNS[0]\ndf_train = pd.read_csv('./taxi-train.csv', header = None, names = CSV_COLUMNS)\ndf_valid = pd.read_csv('./taxi-valid.csv', header = None, names = CSV_COLUMNS)\ndf_test = pd.read_csv('./taxi-test.csv', header = None, names = CSV_COLUMNS)\n", "intent": "Read data created in the previous chapter.\n"}
{"snippet": "def get_distinct_values(column_name):\n    return bq.Query(sql).execute().result().to_dataframe()\n", "intent": "Let's write a query to find the unique values for each of the columns and the count of those values.\n"}
{"snippet": "df = pd.read_csv('exercise_dataset_SLU13.csv')\nprint('Shape:', df.shape)\ndf.head()\n", "intent": "First, let us load some data to fit into a classifier.\n"}
{"snippet": "stationsdf = bq.Query(stations).execute().result().to_dataframe()\n", "intent": "Let's explore the embeddings for some stations. Let's look at stations with overall similar numbers of trips. Do they have similar embedding values?\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('{}/batch_predict/predictions-00000-of-00001.csv'.format(OUTDIR), names=('key','true_cost','predicted_cost'))\ndf['true_cost'] = df['true_cost'] * 20000\ndf['predicted_cost'] = df['predicted_cost'] * 20000\ndf.head()\n", "intent": "<h3> Prediction </h3>\n"}
{"snippet": "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\nprint(\"Done\")\n", "intent": "Let's download  Fashion MNIST data and examine the shape. Take note of the numbers you will get. You will use them throughout this notebook.\n"}
{"snippet": "df = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep=\",\")\n", "intent": "Next, we'll load our data set.\n"}
{"snippet": "df = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep = \",\")\n", "intent": "Next, we'll load our data set.\n"}
{"snippet": "import shutil\nshutil.rmtree('data/sines', ignore_errors=True)\nos.makedirs('data/sines/')\nnp.random.seed(1) \nfor i in range(0,10):\n  to_csv('data/sines/train-{}.csv'.format(i), 1000)  \n  to_csv('data/sines/valid-{}.csv'.format(i), 250)\n", "intent": "<h3> Cloud ML Engine </h3>\nNow to train on Cloud ML Engine with more data.\n"}
{"snippet": "import os, shutil\nDATADIR='data/txtcls'\nshutil.rmtree(DATADIR, ignore_errors=True)\nos.makedirs(DATADIR)\ntraindf.to_csv( os.path.join(DATADIR,'train.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')\nevaldf.to_csv( os.path.join(DATADIR,'eval.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')\n", "intent": "Finally we will save our data, which is currently in-memory, to disk.\n"}
{"snippet": "import shutil\nshutil.rmtree('data/sines', ignore_errors=True)\nos.makedirs('data/sines/')\nfor i in range(0,10):\n  to_csv('data/sines/train-{}.csv'.format(i), 1000)  \n  to_csv('data/sines/valid-{}.csv'.format(i), 250)\n", "intent": "<h3> Cloud ML Engine </h3>\nNow to train on Cloud ML Engine with more data.\n"}
{"snippet": "categories_list = open(\"categories.txt\").read().splitlines()\nauthors_list = open(\"authors.txt\").read().splitlines()\ncontent_ids_list = open(\"content_ids.txt\").read().splitlines()\nmean_months_since_epoch = 523\n", "intent": "To start, we'll load the list of categories, authors and article ids we created in the previous **Create Datasets** notebook.\n"}
{"snippet": "import keras\nfrom keras.datasets import cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n", "intent": "---\nIn this notebook, we train a CNN on augmented images from the CIFAR-10 database.\n"}
{"snippet": "test_set_df = bq.Query(sql).execute().result().to_dataframe()\ntest_set_df.to_csv('test_set.csv', header=False, index=False, encoding='utf-8')\ntest_set_df.head()\n", "intent": "Repeat the query as above but change outcome of the farm fingerprint hash to collect the remaining 10% of the data for the test set.\n"}
{"snippet": "CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\nFEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS) - 1]\nLABEL = CSV_COLUMNS[0]\ndf_train = pd.read_csv('./taxi-train.csv', header = None, names = CSV_COLUMNS)\ndf_valid = pd.read_csv('./taxi-valid.csv', header = None, names = CSV_COLUMNS)\n", "intent": "Read data created in the previous chapter.\n"}
{"snippet": "style_image = scipy.misc.imread(\"images/monet_800600.jpg\")\nimshow(style_image)\n", "intent": "For our running example, we will use the following style image: \n"}
{"snippet": "content_image = scipy.misc.imread(\"images/louvre_small.jpg\")\ncontent_image = reshape_and_normalize_image(content_image)\n", "intent": "Let's load, reshape, and normalize our \"content\" image (the Louvre museum picture):\n"}
{"snippet": "style_image = scipy.misc.imread(\"images/monet.jpg\")\nstyle_image = reshape_and_normalize_image(style_image)\n", "intent": "Let's load, reshape and normalize our \"style\" image (Claude Monet's painting):\n"}
{"snippet": "customer_log_df = np.log(customer_features)\nscaler.fit(customer_log_df)\ncustomer_log_sc = scaler.transform(customer_log_df)\ncustomer_log_sc_df = pd.DataFrame(customer_log_sc, columns=customer_features.columns)\n", "intent": "Many times the skew of data can be easily removed by taking the log of the data. Let's do so here.\nWe will then scale the data after deskewing.\n"}
{"snippet": "scaler = StandardScaler()\ncustomer_sc = scaler.fit_transform(customer_features)\ncustomer_sc_df = pd.DataFrame(customer_sc, columns=customer_features.columns)\nsc_stats = customer_features.describe().T\nsc_stats['skew'] = st.skew(customer_features)\nsc_stats['kurt'] = st.kurtosis(customer_features)\ndisplay(stats)\ndisplay(sc_stats)\n", "intent": "$$Z = \\frac{X-\\mu}{\\sigma}$$\n"}
{"snippet": "customer_log_df = np.log(1+customer_features)\nscaler.fit(customer_log_df)\ncustomer_log_sc = scaler.transform(customer_log_df)\ncustomer_log_sc_df = pd.DataFrame(customer_log_sc, columns=customer_features.columns)\n", "intent": "Many times the skew of data can be easily removed by taking the log of the data. Let's do so here.\nWe will then scale the data after deskewing.\n"}
{"snippet": "import keras\nfrom keras.datasets import cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n", "intent": "---\nIn this notebook, we train a CNN to classify images from the CIFAR-10 database.\n"}
{"snippet": "column_names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\", \n                \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\ndata = pd.read_csv(\"abalone.data\", names=column_names)\nprint(\"Number of samples: %d\" % len(data))\ndata.head()\n", "intent": "There are no column labels in the data, so we copy them from the documentation and use `pandas` to read and print few lines of the dataset.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\n_, i_test = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[i_test] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "merged = pd.merge(df_transactions, df_offers)\npivoted = merged.pivot_table(index='customer_name', columns='offer_id', values='n', fill_value=0)\npivoted\n", "intent": "<div class=\"span5 alert alert-success\">\n<h4>SOLUTIONS: Exercise Set I</h4>\n    </div>\n"}
{"snippet": "golf_df = pd.read_csv(\"golf_data.csv\")\ngolf_df\n", "intent": "<b>Naive Bayesian Classifier</b><br>\nHere, we'll train a Naive Bayesian classifier to make a prediction as to whether or not we'll play golf.\n"}
{"snippet": "golf_df[\"Outlook\"] = LabelEncoder().fit_transform(golf_df[\"Outlook\"])\ngolf_df[\"Wind\"] = LabelEncoder().fit_transform(golf_df[\"Wind\"])\ngolf_df[\"Play\"] = LabelEncoder().fit_transform(golf_df[\"Play\"])\ngolf_df\n", "intent": "The first thing we have to do is encode the catagorical and binary variables\n"}
{"snippet": "X_Train, X_Test, Y_Train, Y_Test = train_test_split(golf_df[[\"Outlook\", \"Temperature\", \"Humidity\", \"Wind\"]], \n                                                    golf_df[\"Play\"], test_size = 0.3, random_state = 0)\n", "intent": "Next we split the data\n"}
{"snippet": "df=pd.DataFrame({\"age\": x_data, \"bmi\": y_data})\nmodel = ols(\"bmi ~ age\", data=df).fit()\nprint(model.summary())\n", "intent": "Next we use <a href=\"http://www.statsmodels.org/stable/index.html\" target=\"\n"}
{"snippet": "mk_df = pd.read_csv(\"mariokart.csv\")\n", "intent": "<b>MarioKart</b><br>\nThe MarioKart dataset contains data from 141 online auctions, some of games in new condition, others in used condition.\n"}
{"snippet": "mk_df = pd.read_csv(\"MarioKart.csv\")\nmk_df.head()\n", "intent": "<b>Example 1: Mariokart</b><br>\nHere we'll build a multiple regression model using the Mariokart data that we first saw last class.\n"}
{"snippet": "import keras\nfrom keras.datasets import cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the CIFAR-10 database.\n"}
{"snippet": "diabetes = datasets.load_diabetes()\ndiabetes_df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ndiabetes_df.head()\ndiabetes_target = diabetes.target\ndiabetes_target_binary = (diabetes_target > 140).astype(int)\n", "intent": "Using the diabetes data, build a logistic regression model to predict diabetes (defined clinically as blood glucose above 140)\n"}
{"snippet": "weights = pandas.read_table(\"/home/skipper/school/talks/538model/\"\n                            \"data/pollster_weights.csv\")\n", "intent": "These are old weights obtained from the 538 web site. New weights are not published anywhere to my knowledge.\n"}
{"snippet": "pvi = pandas.read_csv(\"/home/skipper/school/talks/538model/data/partisan_voting.csv\")\npvi.set_index(\"State\", inplace=True);\npvi.PVI = pvi.PVI.replace({\"EVEN\" : \"0\"})\npvi.PVI = pvi.PVI.str.replace(\"R\\+\", \"-\")\npvi.PVI = pvi.PVI.str.replace(\"D\\+\", \"\")\npvi.PVI = pvi.PVI.astype(float)\npvi.PVI\n", "intent": "Partisan voting index\n"}
{"snippet": "party_affil = pandas.read_csv(\"/home/skipper/school/talks/538model/\"\n                              \"data/gallup_electorate.csv\")\nparty_affil.Democrat = party_affil.Democrat.str.replace(\"%\", \"\").astype(float)\nparty_affil.Republican = party_affil.Republican.str.replace(\"%\", \"\").astype(float)\nparty_affil.set_index(\"State\", inplace=True);\nparty_affil.rename(columns={\"Democrat Advantage\" : \"dem_adv\"}, inplace=True);\nparty_affil[\"no_party\"] = 100 - party_affil.Democrat - party_affil.Republican\nparty_affil[[\"dem_adv\", \"no_party\"]]\n", "intent": "Gallup party affiliation (Poll Jan.-Jun. 2012)\n"}
{"snippet": "pvi = pandas.read_csv(\"/home/skipper/school/seaboldgit/talks/pydata/data/partisan_voting.csv\")\n", "intent": "Partican Voting Index data obtained from [Wikipedia](http://en.wikipedia.org/wiki/Cook_Partisan_Voting_Index)\n"}
{"snippet": "party_affil = pandas.read_csv(\"/home/skipper/school/seaboldgit/talks/pydata/data/gallup_electorate.csv\")\n", "intent": "Party affliation of electorate obtained from [Gallup](http://www.gallup.com/poll/156437/Heavily-Democratic-States-Concentrated-East.aspx\n"}
{"snippet": "obama_give = pandas.read_csv(\"/home/skipper/school/seaboldgit/talks/pydata/data/obama_indiv_state.csv\", \n                             header=None, names=[\"State\", \"obama_give\"])\nromney_give = pandas.read_csv(\"/home/skipper/school/seaboldgit/talks/pydata/data/romney_indiv_state.csv\",\n                             header=None, names=[\"State\", \"romney_give\"])\n", "intent": "Campaign Contributions from FEC.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.text, df.sentiment, test_size=0.3)\n", "intent": "Now that we have the 1000 labeled reviews, let's split into a 70/30 train/test set with `sklearn`:\n"}
{"snippet": "data = pd.read_csv(\"https://raw.githubusercontent.com/pburkard88/DS_BOS_07/master/Data/Spam%20Classification/sms.csv\")\ndata.head()\n", "intent": "Now read in the data with `pandas` `read_csv()` and check it out with `head()`:\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint(\"The MNIST database has a training set of %d examples.\" % len(X_train))\nprint(\"The MNIST database has a test set of %d examples.\" % len(X_test))\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the MNIST database.\n"}
{"snippet": "df = pd.read_csv(\"~/git/GA/DS_BOS_07/Data/Spam Classification/sms.csv\")\ndf.head(10)\n", "intent": "Now read in the data with `pandas` `read_csv()` and check it out with `head()`:\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer(decode_error = 'ignore')\nvect.fit(X_train)\nvect.get_feature_names()\n", "intent": "Use `sklearn.feature_extraction.text.CountVectorizer` on the training set to create a vectorizer called `vect`.\n"}
{"snippet": "tdf = pd.read_csv(\"https://raw.githubusercontent.com/pburkard88/DS_BOS_07/master/Data/Titanic/titanic.csv\")\ntdf.head()\n", "intent": "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle.\n"}
{"snippet": "titanic = pd.read_csv('~/git/GA/DS_BOS_07/Data/Titanic/titanic.csv')\ntitanic.head(10)\n", "intent": "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle.\n"}
{"snippet": "df = pd.read_csv('~/git/GA/DS_BOS_07/Data/Heart Disease/heart_disease.csv', names=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num'])\n", "intent": "Note: You'll have to manually add column labels\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) \n", "intent": "Use `train_test_split()` to generate a 70/30 train/test split and store this in `X_train`, `X_test`, `y_train`, `y_test`.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3)\n", "intent": "Use `train_test_split()` to generate a 70/30 train/test split and store this in `X_train`, `X_test`, `y_train`, `y_test`.\n"}
{"snippet": "df_wide = df_wide.fillna(0)\n", "intent": "Set Nans to zero with the `fillna()` function.\n"}
{"snippet": "ratings = pd.read_table('../Data/movielens/ratings.dat', sep='::', names= ['UserID','MovieID','Rating','Timestamp'])\n", "intent": "Load the ratings.dat data into a `ratings` variable with the same separator, and the column names UserID, MovieID, Rating, Timestamp.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\ndata\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "df = pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "df = pd.read_csv('College_Data')\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df_scaled = pd.DataFrame(data=scaled_features,columns=df.columns[:-1])\ndf_scaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df = pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nX_full, y_full = make_classification(n_samples=2400, n_features=20, n_informative=18,\n                                     n_redundant=0, n_classes=2, random_state=2319)\n", "intent": "In this assignment you are asked to apply different classifiers to the dataset:\n"}
{"snippet": "from sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nX, y = make_moons(n_samples=300, shuffle=True, noise=0.05, random_state=1011)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1011)\n", "intent": "Let's generate a toy dataset for classification.\n"}
{"snippet": "cars_info = pd.read_csv('data/cars.csv', index_col=0, dtype=np.float)\nX = cars_info.speed.values.reshape(-1, 1)\n", "intent": "For our experiments we will use \"Cars\" dataset, which contains information about braking distances for several cars from the 1920s.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_scaler = StandardScaler()\nX_train = X_scaler.fit_transform(X)\ny_train = np.array(cars_info.dist)\n", "intent": "Normalization leads to better convergence\n"}
{"snippet": "y_train = train_data['human-generated']\ncontextVectorizer = CountVectorizer().fit(train_data['context'])\nx_context = contextVectorizer.transform(train_data['context'])\nresponseVectorizer = CountVectorizer().fit(train_data['response'])\nx_response = responseVectorizer.transform(train_data['response'])\nx_train = sparse.hstack([x_context,x_response])\n", "intent": "Generate feature matricies for the context and the response. Fill free to create additional\nfeature extraction objects if you like.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer()\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "kpcaTransform = skdec.KernelPCA(n_components=2,kernel=\"rbf\",gamma=0.023)\nkpcaTransform.fit(X)\nXkpca = kpcaTransform.transform(X)\n", "intent": "<br/> <!--Intentionally left blank-->\n"}
{"snippet": "kpcaTransformCircles=skdec.KernelPCA(n_components=1,kernel=\"rbf\",gamma=1.65)\nkpcaTransformCircles.fit(X)\nXkpca = kpcaTransformCircles.transform(X)\n", "intent": "After PCA we still have not separable sets.\n"}
{"snippet": "split = train_test_split(X, y, test_size=0.5,\n                         random_state=42, stratify=y)\ntrain_X, test_X, train_y, test_y = split\n", "intent": "Split data into training sample and test sample\n"}
{"snippet": "X, y = load_iris(return_X_y=True)\nX_train_non_normalized = X[-100:, [1, 2]]\ny_train = y[-100:]\ny_train[y_train == 1] = 1\ny_train[y_train == 2] = -1\nX_train = scale(X_train_non_normalized)\nn_dim = X_train.shape[1]\nn_elements = X.shape[0]\n", "intent": "Out of box sklearn supports various techniques for data preprocessing.\nhttp://scikit-learn.org/stable/modules/preprocessing.html\n"}
{"snippet": "features = features.fillna(0.0)\nfeatures_kaggle = features_kaggle.fillna(0.0) \ndisplay(features.head())\n", "intent": "And now we'll fill in any blanks with zeroes.\n"}
{"snippet": "transformations_pipeline = FeatureUnion(transformer_list=[\n        (\"cap_gain_loss_pipeline\", cap_gain_loss_pipeline),\n        (\"num_pipeline\", num_pipeline),\n        (\"cat_pipeline\", append_categoricals),\n    ])\n", "intent": "Below is the pipeline for combining all of the other pipelines\n"}
{"snippet": "tpm = pd.read_csv('bmdc-data/lps_tpm.csv', index_col=0)\n", "intent": "Included in this repo is a TPM table which only contains expression from ~300 cells which was part of the time course we are interested in.\n"}
{"snippet": "sample_info = pd.read_csv('bmdc-data/sample_info_lps.csv', index_col=0)\n", "intent": "We also have meta data about these cells\n"}
{"snippet": "gene_annotation = pd.read_csv('bmdc-data/mouse_annotation.csv', index_col=0)\ngene_annotation.head()\n", "intent": "Let's load some gene annotations as well, since our TPM table is just indexed by indecipherable Ensembl IDs.\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "**Create a StandardScaler() object called scaler.**\n"}
{"snippet": "cluster_membership = pd.DataFrame({'cluster': np.argmax(mohgp.phi,1)}, index=varying)\n", "intent": "We can extract the genes belonging to the clusters. Here we will look at a few of them.\n"}
{"snippet": "path = './data/vehicles_test.csv'\ntest = pd.read_csv(path)\ntest['vtype'] = test.vtype.map({'car':0, 'truck':1})\ntest\n", "intent": "<a id=\"testing-preds\"></a>\n"}
{"snippet": "path = './data/titanic.csv'\ntitanic = pd.read_csv(path)\ntitanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\ntitanic.Age.fillna(titanic.Age.median(), inplace=True)\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\nembarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\ntitanic = pd.concat([titanic, embarked_dummies], axis=1)\ntitanic.head()\n", "intent": "We'll build a classification tree using the Titanic survival data set:\n"}
{"snippet": "import pandas as pd\npath = './data/vehicles_train.csv'\ntrain = pd.read_csv(path)\ntrain['vtype'] = train.vtype.map({'car':0, 'truck':1})\ntrain\n", "intent": "<a id=\"manual-bagged\"></a>\n"}
{"snippet": "df = pd.read_csv(\"assets/datasets/iris.csv\")\nprint df['Name'].value_counts()\ndf.head(5)\n", "intent": "Let's do some clustering with the iris dataset.\n"}
{"snippet": "import pandas as pd\nurl = './data/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer\n", "intent": "<a id=\"k-means-demo\"></a>\n---\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "Both sklearn and seaborn have ways to import the iris data:\n- `sklearn.datasets.load_iris()`\n- `sns.load_dataset(\"iris\")`\nThe seaborn way is easier.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(y)\nle.classes_\n", "intent": "- Compare the predicted labels vs. the actual labels.\n"}
{"snippet": "Xs = StandardScaler().fit_transform(X)\nXs = pd.DataFrame(Xs, columns=X.columns)\n", "intent": "Standardize the data and compare at least one of the scatterplots for the scaled data to unscaled above.\n"}
{"snippet": "scaled_features = scaler.fit_transform(data.drop('Class',axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "import pandas as pd\nurl = '../data/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer\n", "intent": "<a id=\"k-means-demo\"></a>\n---\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\ndf = pd.read_csv('assets/datasets/titanic.csv')\ninclude = ['Pclass', 'Sex', 'Age', 'Fare', 'SibSp', 'Survived']\ndf['Sex'] = df['Sex'].apply(lambda x: 0 if x == 'male' else 1)\ndf = df[include].dropna()\nX = df[['Pclass', 'Sex', 'Age', 'Fare', 'SibSp']]\ny = df['Survived']\nPREDICTOR = RandomForestClassifier(n_estimators=100).fit(X, y)\n", "intent": "Our classifier algorithm will be a random forest, which as you know is relatively slow to train.\n"}
{"snippet": "import sklearn.model_selection\n(Xtrain, Xtest, Ytrain, Ytest) = sklearn.model_selection.train_test_split(salaries[['AnnualSalary']],\n                                                                          salaries.GrossPay)\n", "intent": "It seems like there is a linear relationship in there, but it's obscured by a lot of noise.\nSplit the data into training and testing data sets.\n"}
{"snippet": "pd.DataFrame(sanders).head()\n", "intent": "> *Hint: this is as easy as passing it to the DataFrame constructor!*\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)\n", "intent": "Notice that we create the train/test split first. This is because we will reveal information about our testing data if we standardize right away.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXs = ss.fit_transform(X)\n", "intent": "- Plot the standardized mean cross-validated accuracy against the unstandardized. Which is better?\n- Why?\n"}
{"snippet": "ss = StandardScaler()\nXs = ss.fit_transform(X)\n", "intent": "**10.C: Standardize the predictor matrix.**\n"}
{"snippet": "url = './data/bikeshare.csv'\nbikes = pd.read_csv(url, index_col='datetime', parse_dates=True)\n", "intent": "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n"}
{"snippet": "y = kobe.SHOTS_MADE.values\nX = kobe.iloc[:,1:]\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXs = ss.fit_transform(X)\n", "intent": "---\nWhy is normalization necessary for regularized regressions?\nUse the `sklearn.preprocessing` class `StandardScaler` to standardize the predictors.\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features,columns=data.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "table = pd.DataFrame({'probability':[0.1, 0.2, 0.25, 0.5, 0.6, 0.8, 0.9]})\ntable['odds'] = table.probability / (1 - table.probability)\ntable\n", "intent": "**As an example we can create a table of probabilities vs. odds, as seen below.**\n"}
{"snippet": "X = admissions[['gre']]\ny = admissions['admit']\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=46)\nlogit_simple = linear_model.LogisticRegression(C=1e9).fit(X_train, y_train)\n", "intent": "**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**\n"}
{"snippet": "bank_md1 = pd.get_dummies(bank_a[['age','job','education','day_of_week','y']], drop_first = True)\nbank\nLogReg1 = LogisticRegression()\nX1 = bank_md1.drop('y', axis =1)\ny1 = bank_md1['y']\nx_train1, x_test1, y_train1, y_test1 = train_test_split(X1,y1, random_state =42)\nLogReg1.fit(x_train1, y_train1)\n", "intent": "**Build a Model**  \n*Model 1, using `age`, `job`, `education`, and `day_of_week`*\n"}
{"snippet": "name = bank_md1.columns.drop('y')\ncoef = LogReg1.coef_[0]\npd.DataFrame([name,coef],index = ['Name','Coef']).transpose()\n", "intent": "**Get the Coefficient for each feature.**\n- Be sure to make note of interesting findings.\n*Seems like `job_entrepreneur` carries that largest coef.*\n"}
{"snippet": "yelp = pd.read_csv(csv_file)\nyelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\nX = yelp_best_worst.text\ny = yelp_best_worst.stars\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n", "intent": "- Select only 5-star and 1-star reviews.\n- The text will be the features, the stars will be the target.\n- Create a train-test split.\n"}
{"snippet": "vect = TfidfVectorizer(stop_words='english')\ndtm = vect.fit_transform(yelp.text)\nfeatures = vect.get_feature_names()\ndtm.shape\n", "intent": "> **Note:** Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF!\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nX_train, X_test, y_train, y_test = train_test_split(X[predictors], y, train_size=0.7, random_state=8)\nlr2 = LinearRegression()\nlr2.fit(X_train,y_train)\nlr2.score(X_test, y_test)\n", "intent": "- Score and plot.  \n- How do your metrics change?\n"}
{"snippet": "from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.5, random_state =8 ) \n", "intent": "Score and plot your predictions.\n"}
{"snippet": "college = pd.read_csv('./College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "loans = pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "ad = pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "data = pd.read_csv('./yelp.csv')\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "c = CountVectorizer().fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "from keras.datasets import mnist\ndata_dir=\"C:/Training/udacity/DeepLearningNanoDegree/Projects/mnist-mlp/data/\"\n(X_train, y_train), (X_test, y_test) = mnist.load_data(path=data_dir+\"mnist.npz\")\nprint(\"The MNIST database has a training set of %d examples.\" % len(X_train))\nprint(\"The MNIST database has a test set of %d examples.\" % len(X_test))\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the MNIST database.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX = vote_features\ny = vote_target\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                   test_size = 0.3,\n                                                   random_state = 4444)\n", "intent": "Split the data into test and training sets.\nUse this function: `from sklearn.cross_validation import train_test_split`\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data(path=\"C:/Training/udacity/AI_NanoDegree/Term2/3.Convolutional Neural Networks Videos/datasets/mnist.npz\")\nprint(\"The MNIST database has a training set of %d examples.\" % len(X_train))\nprint(\"The MNIST database has a test set of %d examples.\" % len(X_test))\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the MNIST database.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nLabel_X_1 = LabelEncoder()\nX.iloc[:,1] =  Label_X_1.fit_transform(X.iloc[:,1])\nLabel_X_2 = LabelEncoder()\nX.iloc[:,2] =  Label_X_2.fit_transform(X.iloc[:,2])\n", "intent": "Encode the categorical data\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.1, stratify=y, random_state=0)\n", "intent": "Given that the dependent variable is not uiform, we will use the stratified split\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "Feature scaling, critical for training the deep neural network, even for 0/1 categorical variables\n"}
{"snippet": "df = pd.read_csv('College_Data',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n", "intent": "b. Split it into a training set and a test set using `train_test_split()`.\n"}
{"snippet": "from sklearn.datasets import make_moons\nm = 1000\nX_moons, y_moons = make_moons(m, noise=0.1)\n", "intent": "First, let's create the moons dataset using Scikit-Learn's `make_moons()` function:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\n", "intent": "Normalize the data:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(train_reduced)\nX_test_scaled = scaler.transform(test_reduced)\n", "intent": "Here we will try to use an artificial neural network to predict\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n", "intent": "Here we will use the backward elimination, which is the fastest one\n"}
{"snippet": "data = pd.read_csv('Position_Salaries.csv')\n", "intent": "The basic idea is to reduce the informatio entropy. The algorithm looks for the best split to ...?\n"}
{"snippet": "filename = '/data/flights/On_Time_On_Time_Performance_2014_1.csv'\ndata_fl = pd.read_csv(filename)\n", "intent": "First, we will take an example table and inspect the memory usage for each data type: float, int and object\n"}
{"snippet": "x = np.array(data_pred_delay)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True, random_state=1)\n", "intent": "From the correlation coefficient, we could see that features are not correlated between each pair.\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "without using Scikit-Learn\n"}
{"snippet": "df = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "mat = pd.DataFrame(\n    [ df[f].value_counts() for f in list(cat_features) ],\n    index=list(cat_features)\n    ).stack()\npd.DataFrame(mat.values, index=mat.index)\n", "intent": "Results in a data frame:\n"}
{"snippet": "import scipy\npca_data = doPCA(test_data, 11)\nprint(test_data.shape)\nprint(pca_data.shape)\n", "intent": "To apply PCA-based dimensionality reduction to the letter data we can now type:\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nfrom utils import load_wine\nX, y = load_wine()\nclf = RandomForestClassifier()\nclf.fit(X[3:], y[3:])\n", "intent": "---\nIndividual trees output the class index, `RandomForestClassifier` has already\ntranslated back to class label. This is a bit confusing.\n"}
{"snippet": "pca = PCA(n_components=1)\npca.fit(X)\nX_pca = pca.transform(X)\nprint('shape of transformed data:', X_pca.shape)\nprint('First five data points after transform:')\nprint(X_pca[:5])\n", "intent": "---\nWe can use PCA to reduce the number of dimensions of our data by setting the smallest components to zero.\n"}
{"snippet": "import shutil\nshutil.rmtree('data/sines', ignore_errors=True)\nos.makedirs('data/sines/')\nfor i in xrange(0,10):\n  to_csv('data/sines/train-{}.csv'.format(i), 1000)  \n  to_csv('data/sines/valid-{}.csv'.format(i), 250)\n", "intent": "<h3> Cloud ML Engine </h3>\nNow to train on Cloud ML Engine with more data.\n"}
{"snippet": "def cleanup_nulls(df, variablename):\n  df2 = df.pivot_table(variablename, 'date', 'stationid', fill_value=np.nan)\n  print('Before: {} null values'.format(df2.isnull().sum().sum()))\n  df2.fillna(method='ffill', inplace=True)\n  df2.fillna(method='bfill', inplace=True)\n  df2.dropna(axis=1, inplace=True)\n  print('After: {} null values'.format(df2.isnull().sum().sum()))\n  return df2\n", "intent": "One way to fix this is to do a pivot table and then replace the nulls by filling it with nearest valid neighbor\n"}
{"snippet": "df_std = pd.DataFrame(std, columns=df.columns[:-1])\ndf_std.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\n(training_inputs,\n testing_inputs,\n training_classes,\n testing_classes) = train_test_split(all_inputs, all_classes, train_size=0.75, random_state=1)\n", "intent": "Now our data is ready to be split.\n"}
{"snippet": "train = pd.read_csv('./data/train.csv')\ntest = pd.read_csv('./data/test.csv')\n", "intent": "* Exploratory Data Analysis\n* How to deal with data that has anonymous features\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "cars_cancel_test = pd.read_csv('./data/Kaggle_YourCabs_score.csv', index_col='id')\ncars_cancel_test.head()\n", "intent": "[[ go back to the top ]](\n"}
{"snippet": "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\ntext = open(path).read()\nprint('corpus length:', len(text))\n", "intent": "We're going to download the collected works of Nietzsche to use as our data for this class.\n"}
{"snippet": "tables = [pd.read_csv(fname+'.csv', low_memory=False) for fname in table_names]\n", "intent": "We're going to go ahead and load all of our csv's as dataframes into a list `tables`.\n"}
{"snippet": "joined.CompetitionOpenSinceYear = joined.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)\njoined.CompetitionOpenSinceMonth = joined.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)\njoined.Promo2SinceYear = joined.Promo2SinceYear.fillna(1900).astype(np.int32)\njoined.Promo2SinceWeek = joined.Promo2SinceWeek.fillna(1).astype(np.int32)\n", "intent": "Next we'll fill in missing values to avoid complications w/ na's.\n"}
{"snippet": "joined.to_csv('joined.csv')\n", "intent": "We'll back this up as well.\n"}
{"snippet": "joined = pd.read_csv('joined.csv', index_col=0)\njoined[\"Date\"] = pd.to_datetime(joined.Date)\njoined.columns\n", "intent": "We now have our final set of engineered features.\n"}
{"snippet": "train = pd.DataFrame(utils.load_array(data_path+'train/train_features.bc'),columns=['TRIP_ID', 'CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID',\n       'TIMESTAMP', 'DAY_TYPE', 'MISSING_DATA', 'POLYLINE', 'LATITUDE', 'LONGITUDE', 'DAY_OF_WEEK',\n                            'QUARTER_HOUR', \"WEEK_OF_YEAR\", \"TARGET\", \"COORD_FEATURES\"])\n", "intent": "Meanshift clustering as performed in the paper\n"}
{"snippet": "train = pd.DataFrame(utils.load_array(data_path+'train/train_features.bc'),columns=['TRIP_ID', 'CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID',\n       'TIMESTAMP', 'DAY_TYPE', 'MISSING_DATA', 'POLYLINE', 'LATITUDE', 'LONGITUDE', 'TARGET',\n                            'COORD_FEATURES', \"DAY_OF_WEEK\", \"QUARTER_HOUR\", \"WEEK_OF_YEAR\"])\n", "intent": "Load training data and cluster centers\n"}
{"snippet": "from sklearn import model_selection\nfr_train, fr_test, en_train, en_test = model_selection.train_test_split(\n    fr_padded, en_padded, test_size=0.1)\n[o.shape for o in (fr_train, fr_test, en_train, en_test)]\n", "intent": "And of course we need to separate our training and test sets...\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "veczr =  CountVectorizer(ngram_range=(1,2), tokenizer=tokenize)\ntrn_term_doc = veczr.fit_transform(trn)\nval_term_doc = veczr.transform(val)\n", "intent": "Similar to the model before but with bigram features.\n"}
{"snippet": "df_raw = pd.read_csv(f'{PATH}Train.csv', low_memory=False, \n                     parse_dates=[\"saledate\"])\n", "intent": "*Question*\nWhat stands out to you from the above description?  What needs to be true of our training and validation sets?\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_words,test_words = train_test_split(all_words,test_size=0.1,random_state=42)\n", "intent": "We hold out 10% of all words to be used for validation.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_words,test_words = train_test_split(all_words,test_size=0.1,random_state=42)\n", "intent": "We hold out 20% of all words to be used for validation.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.images.shape\n", "intent": "We'll use Scikit-Learn's data access interface and take a look at this data:\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "Let's demonstrate the naive approach to validation using the Iris data, which we saw in the previous section.\nWe will start by loading the data:\n"}
{"snippet": "categories = ['talk.religion.misc', 'soc.religion.christian',\n              'sci.space', 'comp.graphics']\ntrain = fetch_20newsgroups(subset='train', categories=categories)\ntest = fetch_20newsgroups(subset='test', categories=categories)\n", "intent": "For simplicity here, we will select just a few of these categories, and download the training and testing set:\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nx = np.array([2, 3, 4])\npoly = PolynomialFeatures(3, include_bias=False)\npoly.fit_transform(x[:, None])\n", "intent": "This polynomial projection is useful enough that it is built into Scikit-Learn, using the ``PolynomialFeatures`` transformer:\n"}
{"snippet": "from pandas.tseries.holiday import USFederalHolidayCalendar\ncal = USFederalHolidayCalendar()\nholidays = cal.holidays('2012', '2016')\ndaily = daily.join(pd.Series(1, index=holidays, name='holiday'))\ndaily['holiday'].fillna(0, inplace=True)\n", "intent": "Similarly, we might expect riders to behave differently on holidays; let's add an indicator of this as well:\n"}
{"snippet": "df = pd.read_csv(\"Classified Data\",index_col=0)\n", "intent": "Set index_col=0 to use the first column as the index.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n                                                random_state=42)\n", "intent": "For the sake of testing our classifier output, we will split the data into a training and testing set:\n"}
{"snippet": "digits_new = pca.inverse_transform(data_new)\nplot_digits(digits_new)\n", "intent": "Finally, we can use the inverse transform of the PCA object to construct the new digits:\n"}
{"snippet": "X = vectorizer.fit_transform(corpus)\nX\n", "intent": "Now we have the training corpus defined as a list of raw text documents.  We can pass this to our vectorizer to build our bag of words matrix.\n"}
{"snippet": "path = os.getcwd() + '\\data\\ex2data2.txt'\ndata2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])\ndata2.head()\n", "intent": "Similar to part 1, let's start by visualizing the data.\n"}
{"snippet": "def read_data(filename):\n  f = zipfile.ZipFile(filename)\n  for name in f.namelist():\n    return tf.compat.as_str(f.read(name)).split()\n  f.close()\nwords = read_data(filename)\nprint('Data size %d' % len(words))\n", "intent": "Read the data into a string.\n"}
{"snippet": "import pandas as pd\nfile_path = 'train.csv'\ndf = pd.read_csv(file_path) \ny = df.SalePrice\n", "intent": "The varialbe we want to predict is the SalePrice. So:\n"}
{"snippet": "import pandas as pd\nfile_path = 'train.csv'\ndf = pd.read_csv(file_path) \ny = df.SalePrice\npredictors = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', \n                        'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = df[predictors]\nfrom sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\n", "intent": "Let's load the data again and split it:\n"}
{"snippet": "sub = pd.DataFrame()\nsub['ImageId'] = new_test_ids\nsub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\nsub.to_csv('sub-dsbowl2018-1.csv', index=False)\n", "intent": "... and then finally create our submission!\n"}
{"snippet": "from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\ndata = np.asarray(digits.data, dtype='float32')\ntarget = np.asarray(digits.target, dtype='int32')\nX_train, X_test, y_train, y_test = train_test_split(\n    data, target, test_size=0.15, random_state=37)\nscaler = preprocessing.StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "- normalization\n- train/test split\n"}
{"snippet": "ad_data = pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\ndata = np.asarray(digits.data, dtype='float32')\ntarget = np.asarray(digits.target, dtype='int32')\nX_train, X_test, y_train, y_test = train_test_split(\n    data, target, test_size=0.15, random_state=37)\nscaler = preprocessing.StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "- Normalization\n- Train / test split\n"}
{"snippet": "import pandas as pd\nraw_ratings = pd.read_csv(op.join(ML_100K_FOLDER, 'u.data'), sep='\\t',\n                      names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\nraw_ratings.head()\n", "intent": "Each line contains a rated movie: \n- a user\n- an item\n- a rating from 1 to 5 stars\n"}
{"snippet": "from sklearn.manifold import TSNE\nitem_tsne = TSNE(perplexity=30).fit_transform(item_embeddings)\n", "intent": "- we use scikit learn to visualize items embeddings\n- Try different perplexities, and visualize user embeddings as well\n- What can you conclude ?\n"}
{"snippet": "import pandas as pd\nall_ratings = pd.read_csv(op.join(ML_100K_FOLDER, 'u.data'), sep='\\t',\n                          names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\nall_ratings.head()\n", "intent": "Each line contains a rated movie: \n- a user\n- an item\n- a rating from 1 to 5 stars\n"}
{"snippet": "names = [\"name\", \"date\", \"genre\", \"url\"]\nnames += [\"f\" + str(x) for x in range(19)]  \nitems = pd.read_csv(op.join(ML_100K_FOLDER, 'u.item'), sep='|', encoding='latin-1',\n                    names=names)\nitems.fillna(value=\"01-Jan-1997\", inplace=True)\nitems.head()\n", "intent": "The item metadata file contains metadata like the name of the movie or the date it was released\n"}
{"snippet": "from sklearn.manifold import TSNE\nimg_emb_tsne = TSNE(perplexity=30).fit_transform(out_tensors)\n", "intent": "Let's find a 2D representation of that high dimensional feature space using T-SNE:\n"}
{"snippet": "from keras.utils.data_utils import get_file\nURL = \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\ncorpus_path = get_file('nietzsche.txt', origin=URL)\ntext = open(corpus_path).read().lower()\nprint('Corpus length: %d characters' % len(text))\n", "intent": "Let's use some publicly available philosopy:\n"}
{"snippet": "idx = 0\nwith open(filenames_train[idx], 'rb') as f:\n    print(\"class:\", target_names[target_train[idx]])\n    print()\n    print(f.read().decode('latin-1')[:500] + '...')\n", "intent": "Let's check that text of some document have been loaded correctly:\n"}
{"snippet": "texts_train = [open(fn, 'rb').read().decode('latin-1') for fn in filenames_train]\ntexts_test = [open(fn, 'rb').read().decode('latin-1') for fn in filenames_test]\n", "intent": "This dataset is small so we can preload it all in memory once and for all to simplify the notebook.\n"}
{"snippet": "ad_data=pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "PATH = 'data/dbpedia/'\nURL_BASE = 'http://downloads.dbpedia.org/3.5.1/en/'\nfilenames = [\"redirects_en.nt.bz2\", \"page_links_en.nt.bz2\"]\nfor filename in filenames:\n    if not os.path.exists(PATH+filename):\n        print(\"Downloading '%s', please wait...\" % filename)\n        open(PATH+filename, 'wb').write(urlopen(URL_BASE+filename).read())\n", "intent": "Note: this takes a while\n"}
{"snippet": "from sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, random_state=123)\nalphas, lambdas = stepwise_kpca(X, gamma=15, n_components=1)\n", "intent": "Now, let's make a new half-moon dataset and project it onto a 1-dimensonal subspace using the RBF kernel PCA:\n"}
{"snippet": "from sklearn.decomposition import PCA as sklearnPCA\nsklearn_pca = sklearnPCA(n_components=2)\nY_sklearn = sklearn_pca.fit_transform(X_std)\n", "intent": "For educational purposes, we went a long way to apply the PCA to the Iris dataset. But luckily, there is already implementation in scikit-learn. \n"}
{"snippet": "tfidf = TfidfTransformer(use_idf=True, smooth_idf=False, norm='l2')\ntfidf.fit_transform(tf).toarray()[-1]\n", "intent": "And finally, we compare the results to the results that the `TfidfTransformer` returns.\n"}
{"snippet": "tfidf = TfidfTransformer(use_idf=True, smooth_idf=True, norm=None)\ntfidf.fit_transform(tf).toarray()[-1][:3]\n", "intent": "To confirm that we understand the `smooth_idf` parameter correctly, let us walk through the 3-word example again:\n"}
{"snippet": "import pandas as pd\ndf = pd.DataFrame([\n            ['green', 'M', 10.1, 'class1'], \n            ['red', 'L', 13.5, 'class2'], \n            ['blue', 'XL', 15.3, 'class1']])\ndf.columns = ['color', 'size', 'prize', 'class label']\ndf\n", "intent": "First, let us create a simple example dataset with 3 different kinds of features:\n- color: nominal\n- size: ordinal\n- prize: continuous\n"}
{"snippet": "class_le.inverse_transform(df['class label'])\n", "intent": "The class labels can be converted back from integer to string via the `inverse_transform` method:\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndvec = DictVectorizer(sparse=False)\nX = dvec.fit_transform(df.transpose().to_dict().values())\nX\n", "intent": "Now, we can use the `DictVectorizer` to turn this\nmapping into a matrix:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3, \n                     stratify=y,\n                     random_state=0)\n", "intent": "Splitting the data into 70% training and 30% test subsets.\n"}
{"snippet": "train = pd.read_csv('titanic_train.csv')\ntest = pd.read_csv('titanic_test.csv')\n", "intent": "Let's start by reading in the titanic_train.csv file into a pandas dataframe.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\nlr = LogisticRegression()\nlr = lr.fit(X_train_pca, y_train)\n", "intent": "Training logistic regression classifier using the first 2 principal components.\n"}
{"snippet": "df.to_csv('movie_data.csv', index=False, encoding='utf-8')\n", "intent": "Optional: Saving the assembled data as CSV file:\n"}
{"snippet": "with open('./sckit-model-to-json/params.json', 'r', encoding='utf-8') as infile:\n    print(infile.read())\n", "intent": "When we read the file, we can see that the JSON file is just a 1-to-1 copy of our Python dictionary in text format:\n"}
{"snippet": "with open('./sckit-model-to-json/attributes.json', 'r', encoding='utf-8') as infile:\n    print(infile.read())\n", "intent": "If everything went fine, our JSON file should look like this -- in plaintext format:\n"}
{"snippet": "if Version(sklearn_version) < '0.18':\n    from sklearn.cross_validation import train_test_split\nelse:\n    from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0)\n", "intent": "Splitting data into 70% training and 30% test data:\n"}
{"snippet": "if Version(sklearn_version) < '0.18':\n    from sklearn.cross_validation import train_test_split\nelse:\n    from sklearn.model_selection import train_test_split\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3, random_state=0)\n", "intent": "Splitting the data into 70% training and 30% test subsets.\n"}
{"snippet": "df.to_csv('./movie_data.csv', index=False)\n", "intent": "Optional: Saving the assembled data as CSV file:\n"}
{"snippet": "X, y = make_hastie_10_2(n_samples=8000, random_state=42)\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\ngs = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                  param_grid={'min_samples_split': range(2, 403, 10)},\n                  scoring=scoring, cv=5, refit='AUC')\ngs.fit(X, y)\nresults = gs.cv_results_\n", "intent": "Running ``GridSearchCV`` using multiple evaluation metrics\n----------------------------------------------------------\n"}
{"snippet": "from IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()\n", "intent": "1. [The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3](http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/) by Thomas Wiecki\n"}
{"snippet": "train = pd.read_csv('titanic_train.csv')\n", "intent": "Let's start by reading in the titanic_train.csv file into a pandas dataframe.\n"}
{"snippet": "filenames = tf.train.match_filenames_once('./audio_dataset/*.wav')\ncount_num_files = tf.size(filenames)\nfilename_queue = tf.train.string_input_producer(filenames)\nreader = tf.WholeFileReader()\nfilename, file_contents = reader.read(filename_queue)\nchromo = tf.placeholder(tf.float32)\nmax_freqs = tf.argmax(chromo, 0)\n", "intent": "Select the location for the audio files:\n"}
{"snippet": "import pandas as pd\ndf = pd.DataFrame({'feature': x, 'label': y})\nprint(df)\n", "intent": "```pandas``` is the essential package for working with dataframes.  Make a convenient dataframe for using our feature to predict our label.\n"}
{"snippet": "all_X = all_X.fillna(all_X.mean())\n", "intent": "We can approximate the missing values by the mean values of the current feature.\n"}
{"snippet": "(doremi_sample_rate, doremi) = scipy.io.wavfile.read(\"audio_files/do-re-mi.wav\")\n", "intent": "* x-axis: time\n* y-axis: frequency\n* z-axis (color): strength of each frequency\n"}
{"snippet": "df = pd.read_csv(\"data/ebaytitles.csv\")\ndf = df.sample(frac=0.1) \ndf.head()\n", "intent": "Read csv file into a dataframe\n"}
{"snippet": "X = df.title.values\ny = df.category_name.values\nX_tr, X_te, y_tr, y_te = train_test_split(X, \n                                          y,\n                                          test_size=0.1,\n                                          random_state=0)\n", "intent": "Split the data into train and test observations - there is a column\n"}
{"snippet": "df = pd.read_csv(\"data/ebaytitles.csv\")\ndf = df.iloc[:100000,:]\ndf.head()\n", "intent": "Read csv file into a dataframe\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\nimputer = imputer.fit(X[:, 1:3])\nX[:, 1:3] = imputer.transform(X[:, 1:3])\n", "intent": "X is matrix of Features\nY dependent vector\n"}
{"snippet": "combined_data = pd.DataFrame()\ncombined_data = combined_data.append(titanic_train_org)\ncombined_data = combined_data.append(titanic_test_org)\ncombined_data.drop(['PassengerId'], axis=1, inplace=True)\ncombined_data.reset_index(drop=True, inplace=True)\ntrain_idx = len(titanic_train_org)\ntest_idx = len(combined_data) - len(titanic_test_org)\nprint('Combined dataset dimension: {} rows, {} columns'.format(combined_data.shape[0], combined_data.shape[1]))\ncombined_data.head()\n", "intent": "- Imputing null values\n- Transform categorical features\n- Identify new features based on existing features\n"}
{"snippet": "messages = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t',\n                           names=[\"label\", \"message\"])\nmessages.head()\n", "intent": "We'll use **read_csv** and make note of the **sep** argument, we can also specify the desired column names by passing in a list of *names*.\n"}
{"snippet": "numeric_feats = all_df.dtypes[all_df.dtypes != \"object\"].index\nskewed_feats = all_df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nsns.barplot(skewness.index,skewness.Skew);\n", "intent": "**Lets see the highly skewed features we have**\n"}
{"snippet": "regressor = LinearRegression()  \nregressor.fit(X_train, y_train)  \ncoeff_df = pd.DataFrame(regressor.coef_, X.columns, columns=['Coefficient'])  \ncoeff_df  \n", "intent": "Let's build & train the linear regressor model with mutliple features and label.\n"}
{"snippet": "films = { 'title': titles, 'movie_idx': movie_idx, 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\nframe = pd.DataFrame(films, index = [clusters] , columns = ['movie_idx', 'title', 'cluster', 'genre'])\nprint(frame['cluster'].value_counts()) \ngrouped = frame['movie_idx'].groupby(frame['cluster']) \nprint(grouped.mean()) \n", "intent": "Now, we can create a dictionary of titles, ranks, the synopsis, the cluster assignment, and the genre\n"}
{"snippet": "def compute_tf_idf(tf, idf):\n    tf_idf = dict.fromkeys(tf.keys(), 0)\n    for word, v in tf.items():\n        tf_idf[word] = v * idf[word]\n    return tf_idf\ntf_idf_A = compute_tf_idf(tf_A, idf)\ntf_idf_B = compute_tf_idf(tf_B, idf)\ntf_idf_C = compute_tf_idf(tf_C, idf)\nprint('TF-IDF bag of words:')\npd.DataFrame([tf_idf_A, tf_idf_B, tf_idf_C])\n", "intent": "Now, tf-idf is the product of tf to idf. For our python example, tf-idf is dictionary with the corresponding products.\n"}
{"snippet": "import pandas as pd\npath = '../data/yelp.csv'\nyelp = pd.read_csv(path)\n", "intent": "Read **`yelp.csv`** into a Pandas DataFrame and examine it.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\n", "intent": "Use CountVectorizer to create **document-term matrices** from X_train and X_test.\n"}
{"snippet": "import pandas as pd\npath = '../data/yelp.csv'\nyelp = pd.read_csv(path)\n", "intent": "- \"corpus\" = collection of documents\n- \"corpora\" = plural form of corpus\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape\n", "intent": "- **lowercase:** boolean, True by default\n    - Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\ntokenize_test(vect)\n", "intent": "**Approach 3:** Don't convert to lowercase\n"}
{"snippet": "yelp = pd.read_csv('yelp.csv')\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer()\nvect\n", "intent": "[TfidfVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values=1, strategy='median')\n", "intent": "[Imputer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html)\n"}
{"snippet": "origdf = df.merge(df_transactions, on='customer_name').merge(df_offers, on='offer_id')\nwinesdf = origdf[['cluster_id', 'varietal', 'past_peak']]\nwinesdf.pivot_table(index=['cluster_id'], columns=['varietal'], aggfunc='count')\n", "intent": "It looks like 3 is the best value for K, which was the same value suggested by the elbow method.\n"}
{"snippet": "df_d4_predicted = pd.read_csv(\n    os.path.join(data_path,'output_deg_4',\n    'bp-GDbDfIfQef8-fit_degree_4_example_test30.csv.gz'))\ndf_d4_predicted.columns = [\"Row\",\"y_predicted\"]\n", "intent": "<h4>Model with degree 4 features</h4>\n"}
{"snippet": "df_d15_predicted = pd.read_csv(\n    os.path.join(data_path,'output_deg_15',\n    'bp-wr0EvVL9UA5-fit_degree_15_example_test30.csv.gz'))\ndf_d15_predicted.columns = [\"Row\",\"y_predicted\"]\n", "intent": "<h4>Model with degree 15 features</h4>\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "Scikit-learn has a bunch of example datasets. Here, we'll use the iris dataset: which contains data about different species of plants. \n"}
{"snippet": "sales = pd.read_csv('/Users/kiefer/Desktop/Iowa_Liquor_Sales_reduced.csv')\n", "intent": "Load your data from project 3\n"}
{"snippet": "df = pd.DataFrame()\ndf['size'] = np.random.choice([\"big\", \"med\", \"large\", \"gigantic\"], size=500)\ndf['price'] = np.random.choice(np.arange(100, 1000, 1), size=500)\ndf['baths'] = np.random.choice(np.arange(1, 4, 0.5), size=500)\n", "intent": "With 2-3 continious variables, and one categorical.\n"}
{"snippet": "df = pd.DataFrame()\ndf['size'] = np.random.choice([\"big\", \"med\", \"large\", \"giantic\"], size=500)\ndf['baths'] = np.random.choice(np.arange(1, 4, 0.5), size=500)\n", "intent": "With 2-3 continious variables, and one categorical.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "bcw = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-2/datasets/breast_cancer_wisconsin/wdbc.data', \n                  header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "bcw = pd.read_csv('/Users/alex/Desktop/DSI-SF-2-akodate/datasets/breast_cancer_wisconsin/wdbc.data', \n                  header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nconfusion = pd.DataFrame(confmat, index=['is_over_200k', 'is_not_over_200k'],\n                         columns=['predicted_is_over_200k','predicted_is_not_over_200k'])\nprint(confusion)\n", "intent": "What do these mean?\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"../../../datasets/sacramento_real_estate/Sacramentorealestatetransactions.csv\")\n", "intent": "We did this in our previous lab.\n"}
{"snippet": "ss = StandardScaler()\nXn = ss.fit_transform(X)\nprint Xn.shape\n", "intent": "---\nAlways a necessary step when performing regularization.\n"}
{"snippet": "df = pd.read_csv('/Users/alex/Desktop/DSI-SF-2-akodate/datasets/breast_cancer_wisconsin/breast_cancer.csv', na_values='?')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "df = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-2/datasets/breast_cancer_wisconsin/breast_cancer.csv', na_values='?')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "grad_test = pd.read_csv('/Users/alex/Desktop/DSI-SF-2-akodate/datasets/data_for_diplomas/grad_test.csv')\n", "intent": ".\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "data = pd.read_csv('~/Desktop/DSI-SF-2/datasets/401_k_abadie/401ksubs.csv')\ndata.head(2)\n", "intent": "1. Read the data into Pandas.\n2. Explore the data by sorting, plotting, group_by, and any other ideas/techniques you have been using.\n"}
{"snippet": "X = cv.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "sstrain = StandardScaler()\nsstest = StandardScaler()\nlr_sep = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\nXtrain_n = sstrain.fit_transform(Xtrain)\nXtest_n = sstest.fit_transform(Xtest)\n", "intent": "For the sake of example, standardize the Xtrain and Xtest separately and show that their normalization parameters differ.\n"}
{"snippet": "Xn = ss.fit_transform(X)\n", "intent": "Import the LogisticRegression and StandardScaler classes.\n"}
{"snippet": "docs = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\ndocs\n", "intent": "What is being counted?\n_Warning: big ugly sparse matrix ahead._\n"}
{"snippet": "data = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-2/datasets/adult_salary/adult.csv')\n", "intent": "Gradient boosting vs. AdaBoost: 7\nHinge Loss: 2\nPipelines: 5\nFeature importances: 0\nLDA: 2\nNLP: 2\n"}
{"snippet": "csv_file = \"https://vincentarelbundock.github.io/Rdatasets/csv/cluster/pluton.csv\"\ndf = pd.read_csv(csv_file)\ndf.head(5)\n", "intent": "We have a nice [data dictionary](https://vincentarelbundock.github.io/Rdatasets/doc/cluster/pluton.html)\n"}
{"snippet": "stats_pcs = pd.DataFrame(stats_pcs, columns=['PC'+str(i) for i in range(1,8)])\nstats_pcs['athlete'] = hep.iloc[:,0]\nstats_pcs['score'] = hep.score\n", "intent": "---\nAdd back in the athelete and score columns from the original data.\n"}
{"snippet": "wine_pca = PCA().fit(wine_cont_n)\nwine_pcs = wine_pca.transform(wine_cont_n)\nwine_pcs = pd.DataFrame(wine_pcs, columns=['PC'+str(i) for i in range(1, wine_pcs.shape[1]+1)])\nwine_pcs['red_wine'] = wine.red_wine\n", "intent": "---\nCreate a new dataframe with the principal components and the `red_wine` column added back in from the original data.\n"}
{"snippet": "cv = CountVectorizer(ngram_range=(1,2), max_features=2500, binary=True, stop_words='english')\nwords = cv.fit_transform(rt.quote)\n", "intent": "---\nIt is up to you what ngram range you want to select. **Make sure that `binary=True`**\n"}
{"snippet": "Xtrain, Xtest, ytrain, ytest = train_test_split(words.values, rt.fresh.values, test_size=0.25)\n", "intent": "---\nYou should keep 25% of the data in the test set.\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "cvt = CountVectorizer(stop_words=\"english\", ngram_range=(2,3))\nmatrix = cvt.fit_transform(insults_df[\"Comment\"])\nfreqs = [(word, matrix.getcol(idx).sum()) for word, idx in cvt.vocabulary_.items()]\nprint sorted (freqs, key = lambda x: -x[1])[:75]\n", "intent": "Display the top 75 ngrams with frequencies.\n"}
{"snippet": "cvt = CountVectorizer()\nX_all = cvt.fit_transform(insults_df[\"Comment\"])\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "iowa_file = '/Users/alex/Desktop/DSI-SF-2-akodate/datasets/iowa_liquor/Iowa_Liquor_sales_sample_10pct.csv'\niowa = pd.read_csv(iowa_file)\n", "intent": "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n---\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size = 0.2, random_state = 7)\n", "intent": "Batch size of 6 was chosen!\n"}
{"snippet": "df_d4_predicted = pd.read_csv(\n    os.path.join(data_path,'output_deg_4',\n    'bp-W4oBOhwClbH-fit_degree_4_example_test30.csv.gz'))\ndf_d4_predicted.columns = [\"Row\",\"y_predicted\"]\n", "intent": "<h4>Model with degree 4 features</h4>\n"}
{"snippet": "df_d15_predicted = pd.read_csv(\n    os.path.join(data_path,'output_deg_15',\n    'bp-rBWxcnPN3zu-fit_degree_15_example_test30.csv.gz'))\ndf_d15_predicted.columns = [\"Row\",\"y_predicted\"]\n", "intent": "<h4>Model with degree 15 features</h4>\n"}
{"snippet": "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()\n", "intent": "Loading training and testing datasets.\n"}
{"snippet": "data = pd.read_csv(\"./data/monthly-milk-production.csv\", index_col = 'Month')\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "effect_size(pd.DataFrame([1, 2, 3, 4]), pd.DataFrame([3, 3, 1, 2]))\n", "intent": "It is calculated with delta degree of freedom = 1!\n"}
{"snippet": "dfDelayData = pd.read_csv('airlineData/flightDelays2.csv') \n", "intent": "Due to the size of my original dataset, I utilized a slice of that which included only the columns that I needed for my below predictions. \n"}
{"snippet": "from sklearn import preprocessing\nstd_scale = preprocessing.StandardScaler().fit(df[['Alcohol', 'Malic acid']])\ndf_std = std_scale.transform(df[['Alcohol', 'Malic acid']]) \n", "intent": "We use sklearn linrary for standardise data (mean=0, SD=1). \n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(x_cols)\nx_2 = pca.transform(x_cols)\ndf_x_2 = pd.DataFrame(x_2, columns=['C1','C2'])\ndf_x_2.head()\n", "intent": "Use PCA  for dimension reduction to 2 components\n"}
{"snippet": "flights_df['carrier_flight_dest'] = flights_df.carrier + flights_df.flight.astype(str) + \"  \"\\\n                                + flights_df.dest\ntotal = pd.DataFrame(flights_df.groupby('carrier_flight_dest').count())\ntotal.reset_index(level=0, inplace=True)\nprint(total.head())\nflights_365 = total[total['year']==365]['carrier_flight_dest']\nprint(flights_365)\n", "intent": "Which flights (i.e. carrier + flight + dest) happen every day? Where do they fly to?\n"}
{"snippet": "weather_df = pd.read_csv('../Problem_Set_1/weather.csv.bz2')\nweather_df.head()\n", "intent": "What weather conditions are associated with flight delays leaving NYC? Use graphics to explore.\n"}
{"snippet": "flights_df3=pd.DataFrame()\nflights_df3['count_delayed']=flights_df.groupby('date').apply( lambda x:x[x['dep_delay']>0]['dep_delay'].count())\nprint flights_df3.sort(columns='count_delayed',ascending=False).head(n=10)\nflights_df3_temp=flights_df3.reset_index()\n", "intent": "(b) What was the worst day to fly out of NYC in 2013 if you dislike delayed flights?\n"}
{"snippet": "flights_df4=pd.DataFrame()\nflights_df4['avg_dep_delay']=flights_df1.groupby('month')['dep_delay'].mean()\nprint flights_df4.shape\nflights_df4\n", "intent": "(c) Are there any seasonal patterns in departure delays for flights from NYC?\n"}
{"snippet": "flights_df5=pd.DataFrame()\nflights_df5['avg_dep_delay']=flights_df1.groupby('hour')['dep_delay'].mean()\nprint flights_df5.shape\nflights_df5\n", "intent": "(d) On average, how do departure delays vary over the course of a day?\n"}
{"snippet": "flights_df6=pd.DataFrame()\nflights_df6=flights_df.groupby(['carrier','flight','dest']).size().reset_index()\nflights_df6.columns=['carrier','flight','dest','count']\nflights_df6\nprint flights_df6[flights_df6['count']==365]\n", "intent": "Which flights (i.e. carrier + flight + dest) happen every day? Where do they fly to?\n"}
{"snippet": "flights_df= pd.read_csv('C:\\\\Users\\\\root\\\\Desktop\\\\Winter2015\\\\CoreMethods\\\\PS1\\\\flights.csv')\nweather_df= pd.read_csv('C:\\\\Users\\\\root\\\\Desktop\\\\Winter2015\\\\CoreMethods\\\\PS1\\\\weather.csv')\n", "intent": "What weather conditions are associated with flight delays leaving NYC? Use graphics to explore.\n"}
{"snippet": "df08 = pd.read_csv('airlineData/DelayedFlights.csv') \n", "intent": "In the section below, I am looking at the impact of \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfeatures = [item for item in data.columns[:-1]]\ndata = data.replace({'g':1, 'b': 0})\nX = data[features]\ny = data[\"target\"]\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nlen(x_train)\n", "intent": "In order to check the effect of regularization on test data, split the data into train and test using sklearn.\n"}
{"snippet": "print(\"TFIDFVectorizer is being applied...\")\ntfidf_vectorizer = TfidfVectorizer(stop_words='english')\ntfidf_train = tfidf_vectorizer.fit_transform(X_train_1)\nprint(X_train_1.shape)\nprint(tfidf_train.shape)\ntfidf_test = tfidf_vectorizer.transform(X_test_1)\n", "intent": "**Create Vectorizers, I will be using only TFIDFVectorizer.**\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = random_state)\nnn.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)\n", "intent": "**Training the model 1**\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = random_state)\nnn.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)\n", "intent": "**Training the model 2**\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = random_state)\nnn.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)\n", "intent": "**Training the model 3**\n"}
{"snippet": "df = pd.read_csv('/Users/Carsten/GitRepos/NLP-LAB/Carsten_Solutions/sets/fact checking/fake_or_real_news.csv')\n", "intent": "Read in File fake_or_real_news.csv\n"}
{"snippet": "bin_count_vectorizer = CountVectorizer(stop_words='english')\nbin_count_train = bin_count_vectorizer.fit_transform(bin_X_train)\nbin_count_test = bin_count_vectorizer.transform(bin_X_test)\n", "intent": "generate two different vectorizers\n"}
{"snippet": "mul_count_vectorizer = CountVectorizer(stop_words='english')\nmul_count_train = mul_count_vectorizer.fit_transform(mul_X_train)\nmul_count_test = mul_count_vectorizer.transform(mul_X_test)\nmul_count_valid = mul_count_vectorizer.transform(mul_X_valid)\n", "intent": "* same again\n* only additional transform for validation set\n"}
{"snippet": "concat_count_vectorizer = CountVectorizer(stop_words='english')\nconcat_count_train = concat_count_vectorizer.fit_transform(concat_X_train)\nconcat_count_test = concat_count_vectorizer.transform(concat_X_test)\n", "intent": "* again the same ...\n"}
{"snippet": "func = lambda x: 2 + 0.5 * x + 3 * x ** 2 + 5 * stats.norm.rvs(0, 10)\ndf = pd.DataFrame()\ndf[\"x\"] = list(range(0, 30))\ndf[\"y\"] = map(func, df[\"x\"])\ndf.plot.scatter(x='x', y='y')\n", "intent": "Next we look at a data set that needs a quadratic fit. Let's do both a linear and quadratic fit and compare.\n"}
{"snippet": "size=10000\nX, y = transform_to_dataset(training_sentences)\nclf = Pipeline([\n    ('vectorizer', DictVectorizer(sparse=False)),\n    ('classifier', LogisticRegression())\n])\nclf.fit(X[:size], y[:size])\nprint('training OK')\nX_test, y_test = transform_to_dataset(test_sentences)\nperformances['performancesTreebank'].update({'Classifier': clf.score(X_test, y_test)})\n", "intent": "<h4>Task 1.1: Train classifier with the extracted features on the treebank corpus</h4>\n"}
{"snippet": "size=10000\nX, y = transform_to_dataset(training_sentences)\nclf = Pipeline([\n    ('vectorizer', DictVectorizer(sparse=False)),\n    ('classifier', LogisticRegression())\n])\nclf.fit(X[:size], y[:size])\nprint('training OK')\nX_test, y_test = transform_to_dataset(test_sentences)\nperformancesAlpino.update({'Classifier': clf.score(X_test, y_test)})\n", "intent": "<h4>Task 2.1: Train classifier on the extracted features</h4>\n"}
{"snippet": "ds1 = pd.read_csv('../fake_or_real_news.csv', sep=',', usecols=['title','text','label'])\nds1['claim'] = ds1[['title', 'text']].apply(lambda x: '. '.join(x), axis=1)\ndel ds1['title']\ndel ds1['text']\nds1.rename(index=str, columns={'label': 'y'}, inplace=True)\nds1['y'] = np.where(ds1['y'] == 'FAKE', 'false', 'true')\nds1.head()\n", "intent": "<h4>Read and preprocess datasets</h4>\n"}
{"snippet": "df = pd.read_csv('./task2_datasets1_2/fake_or_real_news.csv')\n", "intent": "only FAKE and REAL labels\n"}
{"snippet": "path='./task2_datasets1_2/liar_liar_paints_on_fire/liar_dataset/'\nheaders=['id','label','statement','subject','speaker','speakerstitle','stateinfo','party','barely-true','false','half-true','mostly-true','pants-on-fire','context']\nds2_train_df = pd.read_csv(path+'train.tsv',sep='\\t', names=headers)\nds2_test_df = pd.read_csv(path+'test.tsv',sep='\\t', names=headers)\nds2_valid_df = pd.read_csv(path+'valid.tsv',sep='\\t',names=headers)\n", "intent": "labels of a different range: pants-fire,barely-\ntrue,  false, half-true, mostly-true, and true\n"}
{"snippet": "count_vector = TfidfVectorizer(stop_words='english')\ncount_vector.fit_transform(df['text'])\nds1_x = count_vector.transform(df['text'])\nds1_y = df.label\n", "intent": "Clf1: Simple MultonomialNB after this [article](https://github.com/kjam/random_hackery/blob/master/Attempting%20to%20detect%20fake%20news.ipynb)\n"}
{"snippet": "print(length)\nsteps = ['original', 'dropped NA 1', 'dropped NA 2', 'dropped too short paragrphs', 'merged overlapping paragraphs']\ndataframe = pd.DataFrame(\n    {'length': length,\n     'steps': steps}, \n    index = steps)\ndataframe.plot(kind='bar');\n", "intent": "The plot shows that there was a considerable amount of NAs as well as a considerable amount of overlapping paragraphs.\n"}
{"snippet": "b = pd.DataFrame()\nfor att in X:\n    a = []\n    a = hypothesisTest(df, att, 'TREAT')\n    b = pd.concat([b,a],0)\nb\n", "intent": "We can carry out t-tests to evaluate whether these means are statistically distinguishable:\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.keys()\n", "intent": "We previously saw the **hand-written digits** data. Let's use that here to test the efficacy of the SVM and Random Forest classifiers.\n"}
{"snippet": "sub.to_csv('submissions/lgbm_{:.5f}_{:.5f}.csv'.format(val_mean, val_std), index=False)\n", "intent": "Now let's save the submission. I like annotating the submission with the local score to compare it with the leaderboard score.\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nknn_pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\nknn_params = {'knn__n_neighbors': range(1, 10)}\nknn_grid = GridSearchCV(knn_pipe, knn_params,\ncv=5, n_jobs=-1,\nverbose=True)\nknn_grid.fit(X_train, y_train)\nknn_grid.best_params_, knn_grid.best_score_\n", "intent": "Now, let's tune the number of neighbors $k$ for k-NN:\n"}
{"snippet": "time_df = pd.DataFrame(index=train_df.index)\ntime_df['target'] = train_df['target']\ntime_df['min'] = train_df[times].min(axis=1)\ntime_df['max'] = train_df[times].max(axis=1)\ntime_df['seconds'] = (time_df['max'] - time_df['min']) / np.timedelta64(1, 's')\ntime_df.head()\n", "intent": "Now, let us look at the timestamps and try to characterize sessions by timeframes:\n"}
{"snippet": "def fill_nan(table):\n    for col in table.columns:\n        table[col] = table[col].fillna(table[col].median())\n    return table   \n", "intent": "Let's write the function that will replace *NaN* values with a median for each column.\n"}
{"snippet": "PATH_TO_DATA = ('../../data')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'websites_train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'websites_test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "train_target = pd.read_csv('../data/train_log1p_recommends.csv', \n                           index_col='id')\ny_train = train_target['log_recommends'].values\n", "intent": "**Read train target and split data for validation.**\n"}
{"snippet": "PATH_TO_DATA = ('../../../data')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "from sklearn import datasets\ncancer = datasets.load_breast_cancer()\nX_can = cancer.data\ny_can = cancer.target\nprint(X_can.shape)\nprint(y_can.shape)\n", "intent": "Let's try to import sklearn and check some available datasets. \n"}
{"snippet": "pca16 = PCA(n_components=16).fit(X_dig)\nX_reduced16 =pca16.transform(X_dig)\nprint(X_reduced16.shape)\n", "intent": "Let's try to repeat K-means analysis taking 16 dimensions/components:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfmri_tr, fmri_ts, cond_tr, cond_ts = train_test_split(fmri_masked_2lb, conditions_2lb)\nsvc.fit(fmri_tr, cond_tr)\n", "intent": "Let's split our data and fit the model using the training set:\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"e:/sundog-consult/udemy/datascience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "from sklearn.decomposition import PCA\niris = datasets.load_iris()\nX_ir = iris.data\ny_ir = iris.target\nn_components = 2\npca = PCA(n_components=n_components)\nX_pca = pca.fit_transform(X_ir)\n", "intent": "Let's recreate X_pca from our previous notebook:\n"}
{"snippet": "X, y = get_dataset2(shift=0)\ndf = pd.DataFrame(np.hstack((X, y[:, np.newaxis])), columns=['x0', 'x1', 'x2', 'y'])\nsns.pairplot(df, hue='y', vars=['x0', 'x1', 'x2'])\n", "intent": "Let us make the last component fully irrelevant and try a L1-based method again\n"}
{"snippet": "from sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np\niris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf.head()\n", "intent": "In this example we will use dataset and the random forest classifer from Scikit-learn.\nExample from: Chris Albon: https://github.com/chrisalbon/notes\n"}
{"snippet": "data.Age = ____\ndata.Fare = ____\ndata['Embarked'] = data['Embarked'].fillna('S')\ndata.info()\n", "intent": "* Impute missing values:\n"}
{"snippet": "df_data = pd.read_csv(os.path.join(workspace_path, 'data/census.csv'), dtype=str)\nprint '%d rows' % len(df_data)\ndf_data.head()\n", "intent": "Its a good idea to load data and inspect it to build an understanding of the structure, as well as preparation steps that will be needed.\n"}
{"snippet": "import pandas as pd\nadver = pd.read_csv('data/adver.csv', index_col=0)\nadver.head()\n", "intent": "<p><a name=\"ex2\"></a></p>\nThe Advertising data includes Sales vs TV, Radio and Newspaper.\nWe start with importing the data and visualizing it.\n"}
{"snippet": "colnames = df.columns\nresult = pd.DataFrame(ols.coef_).transpose()\nresult.columns = colnames.tolist()\nresult['intercept'] = ols.intercept_ \nresult = result.transpose()\nresult.columns = ['coefficient']\nresult\n", "intent": "Below we list the coefficients for each variable.\n"}
{"snippet": "path_to_file = \"./data/pml2tumor.csv\"\ndata = pd.read_csv(path_to_file)\nx_tm = data[[\"Size\"]]\ny_tm = data[\"Malignant\"]\nx_tm2 = np.copy(x_tm)\nx_tm2[-3,0] = 13\nx_tm2[-1,0] = 14\n", "intent": "We demonstrate **LDA** in `sklearn` with the tumor data.\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\npair = (2,3)\nxlabel = iris.feature_names[pair[0]]\nylabel = iris.feature_names[pair[1]]\niris_x = iris.data[:,pair]\niris_y = iris.target\n", "intent": "<p><a name=\"ex3\"></a></p>\nWe will work on the `iris` data\n"}
{"snippet": "X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\nclf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)   \n", "intent": "A single train/test split is made easy with the train_test_split function in the cross_validation library:\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = pd.read_csv('data/data.csv')\nprint data.head()\nX = np.array(data[['x1', 'x2', 'x3', 'x4', 'x5', 'x6']])\ny = np.array(data[['y']])\n", "intent": "<p><a name=\"ex3\"></a></p>\nRun the follwing code to obatian the data:\n"}
{"snippet": "best2 = fs.SelectKBest(fs.chi2, k=2).fit_transform(iris.data, iris.target)\nbest2.shape\n", "intent": "We use the function **SelectKBest** to select the best 2 features using the chi-square test:\n"}
{"snippet": "percent80 = fs.SelectPercentile(fs.chi2, 80).fit_transform(iris.data, iris.target) \npercent80.shape\n", "intent": "We can see here: compared to the original predictors, the selected features are the last two features.\n"}
{"snippet": "import numpy as np\nnp.random.seed(1)\nm = 100\nx = np.random.randn(m) \ny = x + 2 * x**2 - 3 * x**3 + np.random.randn(m)\nX = np.column_stack((x, x**2, x**3, x**4, x**5, x**6))\ndata = pd.DataFrame(np.column_stack((X, y)))\ndata.columns = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'y']\ndata.to_csv('data/data_new.csv', index=False)\n", "intent": "** Appendix ** Here is how the data is generated:\n"}
{"snippet": "import numpy as np\nfrom sklearn import datasets\niris = datasets.load_iris()\nindex = range(100)\niris.x = iris.data[index, :]\niris.y = iris.target[index]\nsvm_model.fit(iris.x[:, 0:2], iris.y)\n", "intent": "<p><a name=\"1case1\"></a></p>\nWe demonstrate the `svm` model on the first 100 observation of the iris data, which include two species.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ntrain = pd.read_csv('data/spam_train.csv')\ntest = pd.read_csv('data/spam_test.csv')\nx_train = np.array(train.iloc[:, 0:57])\ny_train = np.ravel(train.iloc[:, -1])\nx_test = np.array(test.iloc[:, 0:57])\ny_test = np.ravel(test.iloc[:, -1])\n", "intent": "<p><a name=\"2case2\"></a></p>\nIn this case we try to fit a decision tree on the spam data set.\n"}
{"snippet": "import pandas as pd\nimport sklearn.cross_validation as cv\nspam_train = pd.read_csv('./data/spam_train.csv')\nspam_test = pd.read_csv('./data/spam_test.csv')\nx_train = np.array(spam_train.iloc[:, :57])\ny_train = np.array(spam_train.iloc[:, -1])\nx_test = np.array(spam_test.iloc[:, :57])\ny_test = np.array(spam_test.iloc[:, -1])\n", "intent": "<p><a name=\"3case2\"></a></p>\nIn this case we fit a random forest on the spam data. We first prepare the data:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA()\niris.data2 = pca.set_params(n_components = 2).fit_transform(iris.data)\n", "intent": "We can also fit the principal components to the KMeans algorithm. \n- First transform the data to principal components\n"}
{"snippet": "from sklearn import datasets\niris= datasets.load_iris()\nhier.set_params(n_clusters = 3)\nhier.fit(iris.data)\n", "intent": "<p><a name=\"hcase1\"></a></p>\nLet's try to fit the hierarchical clustering model with the iris data.\n"}
{"snippet": "with open('anna.txt', 'r') as f:\n    text=f.read()\nvocab = set(text)\nvocab_to_int = {c: i for i, c in enumerate(vocab)}\nint_to_vocab = dict(enumerate(vocab))\nchars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n", "intent": "First we'll load the text file and convert it into integers for our network to use.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA()\n", "intent": "<p><a name=\"pca-sklearn\"></a></p>\nIn order to implement PCA in Python, import the class `PCA` from *sklearn.decomposition*.\n"}
{"snippet": "pca.set_params(n_components = 2)\niris.data2 = pca.fit_transform(iris.data)\n", "intent": "- Fit `pca` with the data:  \n"}
{"snippet": "dataframe  = pd.read_csv('../data/projects.csv')\nprint dataframe.shape[0]\n", "intent": "First, check the size of the datasets:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.20, random_state=17)\n", "intent": "Split the data into a training set and a test set.\n"}
{"snippet": "if os.path.exists('data_september_2015.csv'): \n    data = pd.read_csv('data_september_2015.csv')\nelse: \n    url = \"https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2015-09.csv\"\n    data = pd.read_csv(url)\n    data.to_csv(url.split('/')[-1])\nprint \"Number of rows:\", data.shape[0]\nprint \"Number of columns: \", data.shape[1]\n", "intent": "***Let's first download the dataset and print out the its size***\n"}
{"snippet": "colleges = pd.read_csv('College_Data')\ncolleges.set_index('Unnamed: 0', inplace=True)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "secret = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaled_df = pd.DataFrame(data=scaled_features, columns = secret.columns[:-1])\nscaled_df.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df = pd.read_table('data/winequality-red.csv', sep=';')\nfeatures = df.columns - ['quality']\ntarget = 'quality'\n", "intent": "We are going to briefly revisit the dataset used for the \"wine regression\" exercise to show the built-in parallel processing capabilities of sklearn.\n"}
{"snippet": "csv_url = \"http://www.stat.berkeley.edu/~ledell/data/eeg_eyestate_splits.csv\"\ndata = pd.read_csv(csv_url)\n", "intent": "Let's import the same dataset directly with pandas\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(\n    stop_words='english', lowercase=True, binary=False, min_df=0.01)\nfeatures_train = vectorizer.fit_transform(reviews_train)\n", "intent": "To extract features from the reviews, we are going to use word counts after normalizing to lowercase and removing stopwords.\n"}
{"snippet": "from sklearn import preprocessing\nscaler = preprocessing.StandardScaler()\nscaler.fit(x_train)\nx_test = scaler.transform(x_test)\nx_train = scaler.transform(x_train)\n", "intent": "Now we're ready to put together our regression workflow. Let's start by rescaling the data set so that all variables have the same range.\n"}
{"snippet": "daily_vol_df = ticker_daily_vol.to_pandas_dataframe()\ndaily_vol_df.plot()\n", "intent": "**Plot daily volatility in stocks over time.**\n"}
{"snippet": "most_volatile_on_ave_stocks_set = set([x[0] for x in most_volatile_on_ave_stocks])\nticker_pandas_df = ticker_daily_vol.filter(lambda x: x[0] in most_volatile_on_ave_stocks_set) \\\n    .to_pandas_dataframe()\nticker_pandas_df.plot()\n", "intent": "** Plot stocks with the highest average daily volatility over time. **\n"}
{"snippet": "from io import StringIO  \nmovie_txt = requests.get('https://????/movies.dat').text\nmovie_file = StringIO(movie_txt) \nmovies = pd.read_csv(movie_file, delimiter='\\t')\nmovies\n", "intent": "Here's a chunk of the MovieLens Dataset:\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ntwenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\nprint type(twenty_train)\ntwenty_train['data']\n", "intent": "We can now load the list of files matching those categories as follows:\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(['Data programming is cool', 'If you and only you pass the course!'])\ncount_vect.get_feature_names()\n", "intent": "Now back to our text data example\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ntwenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\ntype(twenty_train.data[0])\n", "intent": "We can now load the list of files matching those categories as follows:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = store_spend[['APR_SPEND', 'MAY_SPEND']]\ny = store_spend['JUNE_SPEND']\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n", "intent": "First, separate the response and the features into their own NumPy arrays.\n"}
{"snippet": "pca = H2OPCA(k=5)\npca.fit(X_train_norm)\nX_train_norm_pca = pca.transform(X_train_norm)\nX_test_norm_pca  = pca.transform(X_test_norm)\n", "intent": "Then, we can apply PCA and keep the top 5 components. A user warning is expected here.\n"}
{"snippet": "pd.DataFrame(train.groupby('gender').stroke.value_counts())\n", "intent": "** Some numerical EDAs to quench my curiosity**\n"}
{"snippet": "electoral_votes = pd.read_csv(\"data/electoral_votes.csv\").set_index('State')\nelectoral_votes.head(5)\n", "intent": "*As a matter of convention, we will index all our dataframes by the state name*\n"}
{"snippet": "from io import StringIO  \nmovie_txt = requests.get('https://raw.github.com/cs109/cs109_data/master/movies.dat').text\nmovie_file = StringIO(movie_txt) \nmovies = pd.read_csv(movie_file, delimiter='\\t')\nprint movies[['id', 'title', 'imdbID', 'year']].iloc[0]\nmovies.head(3)\n", "intent": "Here's a chunk of the MovieLens Dataset:\n"}
{"snippet": "from io import StringIO  \nmovie_txt = requests.get('https://raw.github.com/cs109/cs109_data/master/movies.dat').text\nmovie_file = StringIO(movie_txt) \nmovies = pd.read_csv(movie_file, delimiter='\\t')\nprint movies[['id', 'title', 'imdbID', 'year']].iloc[0]\nmovies\n", "intent": "Here's a chunk of the MovieLens Dataset:\n"}
{"snippet": "im = io.imread('../images/chapel_floor.png')\nim_lab = color.rgb2lab(im)\ndata = np.array([im_lab[..., 0].ravel(),\n                 im_lab[..., 1].ravel(),\n                 im_lab[..., 2].ravel()])\nkmeans = KMeans(n_clusters=4, random_state=0).fit(data.T)\nsegmentation = kmeans.labels_.reshape(im.shape[:-1])\n", "intent": "Of course we can generalize this method to more than two clusters.\n"}
{"snippet": "with open(model_location, 'rb') as f:\n    ciphered_model = f.read()\n", "intent": "We will now read the encrypted model from the file Flor has stored it in:\n"}
{"snippet": "data = None\nHTML(open(\"input.html\").read())\n", "intent": "**Exercise**: Try to write a digit into the box below. This will automatically save your input in a variable `data` behind the scenes.\n"}
{"snippet": "with open('text8') as f:\n    words = f.read().split()\nn_words = len(words)\nprint('Number of words in the dataset: {}'.format(n_words))\n", "intent": "Read the dataset and split it into words\n"}
{"snippet": "def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\nmatrix = fill_na(matrix)\n", "intent": "Producing lags brings a lot of nulls.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\nclf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)   \n", "intent": "A single train/test split is made easy with the train_test_split function in the cross_validation library:\n"}
{"snippet": "sales = pd.read_csv('../readonly/final_project_data/sales_train.csv.gz')\nshops = pd.read_csv('../readonly/final_project_data/shops.csv')\nitems = pd.read_csv('../readonly/final_project_data/items.csv')\nitem_cats = pd.read_csv('../readonly/final_project_data/item_categories.csv')\n", "intent": "Let's load the data from the hard drive first.\n"}
{"snippet": "y_val_pred_inversed = mlb.inverse_transform(y_val_predicted_labels_tfidf)\ny_val_inversed = mlb.inverse_transform(y_val)\nfor i in range(3):\n    print('Title:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n        X_val[i],\n        ','.join(y_val_inversed[i]),\n        ','.join(y_val_pred_inversed[i])\n    ))\n", "intent": "Now take a look at how classifier, which uses TF-IDF, works for a few examples:\n"}
{"snippet": "sample_size = 200000\ndialogue_df = pd.read_csv('data/dialogues.tsv', sep='\\t').sample(sample_size, random_state=0)\nstackoverflow_df = pd.read_csv('data/tagged_posts.tsv', sep='\\t').sample(sample_size, random_state=0)\n", "intent": "Now, load examples of two classes. Use a subsample of stackoverflow data to balance the classes. You will need the full data later.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"https://www.dropbox.com/s/1k9cgsd7bzce0yk/housing-data.csv?dl=1\")\nprint df\n", "intent": "https://www.dropbox.com/s/1k9cgsd7bzce0yk/housing-data.csv?dl=1\n"}
{"snippet": "cvec = CountVectorizer()\ncvec.fit(data_train['data'])\n", "intent": "Initialize a standard CountVectorizer and fit the training data\n"}
{"snippet": "dfv = pd.read_csv('../assets/datasets/votes.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "xStand = StandardScaler().fit_transform(x)\n", "intent": "use a map to get the values into 0 and 1.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"/Users/Debjani/Downloads/IMDB.csv\")\n", "intent": "Source of the data: [IMBD movies data](https://dpalit.github.io/)\n"}
{"snippet": "data = pd.read_csv('./bank-additional/bank-additional-full.csv', sep=';')\npd.set_option('display.max_columns', 500)     \npd.set_option('display.max_rows', 20)         \ndata\n", "intent": "Now lets read this into a Pandas data frame and take a look.\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nyencode = LabelEncoder().fit(dataset.target)\ncensus = Pipeline([\n        ('encoder',  EncodeCategorical(dataset.categorical_features.keys())),\n        ('imputer', ImputeCategorical(['GRINST','MIGMTR3','MIGMTR4','MIGSAME','NOEMP','PEMNTVTY','PENATVTY','PRCITSHP'])),\n        ('classifier', SVC())\n    ])\ncensus.fit(dataset.data, yencode.transform(dataset.target))\n", "intent": "Model comparison using Support Vector Machine\n"}
{"snippet": "pca = PCA(n_components=2)\n", "intent": "Create a new PCA-object and set the target array-length to 2.\n"}
{"snippet": "transfer_values_reduced = pca.fit_transform(transfer_values)\n", "intent": "Use PCA to reduce the transfer-value arrays from 2048 to 2 elements.\n"}
{"snippet": "log_q_values.read()\nlog_reward.read()\n", "intent": "We can now read the logs from file:\n"}
{"snippet": "continuous_feature_assembler = VectorAssembler(inputCols=continuous_features, outputCol=\"unscaled_continuous_features\")\ncontinuous_feature_scaler = StandardScaler(inputCol=\"unscaled_continuous_features\", outputCol=\"scaled_continuous_features\", \\\n                                           withStd=True, withMean=False)\n", "intent": "**Step 4**: Continous Feature Pipeline\n"}
{"snippet": "categorical_feature_indexers = [StringIndexer(inputCol=x, \\\n                                              outputCol=\"{}_index\".format(x)) \\\n                                for x in categorical_features]\ncategorical_feature_one_hot_encoders = [OneHotEncoder(inputCol=x.getOutputCol(), \\\n                                                      outputCol=\"oh_encoder_{}\".format(x.getOutputCol() )) \\\n                                        for x in categorical_feature_indexers]\n", "intent": "**Step 5**: Categorical Feature Pipeline\n"}
{"snippet": "n_samples = 2000\nrandom_state=42\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05, random_state=random_state)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05, random_state=random_state)\nblobs = datasets.make_blobs(n_samples=n_samples, centers=2, random_state=random_state)\n", "intent": "Generate three different datasets, with two classes\n"}
{"snippet": "def get_distinct_values(column_name):\n  return bq.Query(sql).execute().result().to_dataframe()\n", "intent": "Let's write a query to find the unique values for each of the columns and the count of those values.\n"}
{"snippet": "CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\nFEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS)-1]\nTARGET = CSV_COLUMNS[0]\ndf_train = pd.read_csv('../datasets/taxi-train.csv', header=None, names=CSV_COLUMNS)\ndf_valid = pd.read_csv('../datasets/taxi-valid.csv', header=None, names=CSV_COLUMNS)\n", "intent": "Read data created in the previous chapter.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\n_, itest = train_test_split(range(critics.shape[0]), train_size=0.7,random_state=42)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itest] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "scaler = StandardScaler()\nscaler.fit(X_train)\nX_scaled_train = scaler.transform(X_train)\nX_scaled_train = pd.DataFrame(X_scaled_train)\nX_scaled_train.columns = list(df.columns)[1:]\ndf3 = X_scaled_train.join(y_train.reset_index())\n", "intent": "`2.` Now, use [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n"}
{"snippet": "student_data = pd.read_csv(\"student-data.csv\")\ntarget = student_data['passed']\nprint \"Student data read successfully!\"\ndata = pd.read_csv('student-data.csv', header=0)\noutcome_passed = data['passed']\noutcome_passed_num = outcome_passed.replace(['yes', 'no'], [1,0])\nfeatures = data.drop('passed', axis = 1)\nfeatures_num = preprocess_features(features)\n", "intent": "2) Load the student data from a CSV file and save as new dataframe @student_data\n"}
{"snippet": "user_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\nusers = pd.read_table('data/u.user', sep='|', header=None, names=user_cols)\n", "intent": "Documentation for [**`read_table`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html)\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX = representatives[range(1,17)]\nY = representatives[0]\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=.30, random_state=4444)\n", "intent": "Split the data into a test and training set. But this time, use this function: from sklearn.cross_validation import train_test_split\n"}
{"snippet": "movies = pd.read_csv('2013_movies.csv')\n", "intent": "For your movie classifiers, calculate the precision and recall for each class.\n"}
{"snippet": "movies = pd.read_csv('../../challenges_data/2013_movies.csv')\n", "intent": "Fit and evaluate a decision tree classifier for your movie dataset. Examine the rules your tree uses.\n"}
{"snippet": "movie=pd.read_csv('../../../challenges/challenges_data/2013_movies.csv')\nmovie=movie.dropna()\nmovie.describe()\n", "intent": "The error term must have constant variance\n"}
{"snippet": "max_features = 2000\n(X_train, y_train), (X_test, y_test) = reuters.load_data(\n    num_words=max_features)\nmaxlen = 10\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nnb_epoch = 20\n", "intent": "- reuter newswire ~ loads the Reuters newswire classification dataset\n- 46 classes\n"}
{"snippet": "from keras.datasets import cifar10\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n", "intent": "- The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes\n<img src ='../imgs/cifar.png'/>\n"}
{"snippet": "max_features = 2000\n(X_train, y_train), (X_test, y_test) = reuters.load_data(\n    num_words=max_features)\nmaxlen = 10\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nnb_epoch = 20\n", "intent": "- We'll work with reuter newswire to classification dataset.   \n- 46 classes\n- We will model this using RNNs, but first let's try with an ANN. \n"}
{"snippet": "train_x, train_y, test_x, test_y = load_data(\"../data/iris.data\")\n", "intent": "Note that we need the data in a different format. Each instance is a list of values (not a dict, as before).\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ntext = ['That is should come to this!', 'This above all: to thine own self be true.', 'Something is rotten in the state of Denmark.']\nvectorizer = CountVectorizer(ngram_range=(1,2))\nvectorizer.fit(text)\nprint(vectorizer.get_feature_names())\nx = vectorizer.transform(text)\n", "intent": "CountVectorizer:  Convert a collection of text documents to a matrix of token counts\nThis implementation produces a sparse representation.\n"}
{"snippet": "pd.DataFrame(dtm_lsa.round(5), index = example, columns = [\"component_1\",\"component_2\" ])\n", "intent": "Each document is a linear combination of the LSA components\n"}
{"snippet": "similarity = np.asarray(numpy.asmatrix(dtm_lsa) * numpy.asmatrix(dtm_lsa).T) \npd.DataFrame(similarity.round(6),index=(range(7)), columns=(range(7))).head(10)\n", "intent": "Document similarity using LSA\n"}
{"snippet": "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball']\nng_train = datasets.fetch_20newsgroups(subset='train', categories=categories, \n                                      remove=('headers', 'footers', 'quotes'))\n", "intent": "Let's retain only a subset of the 20 categories in the original 20 Newsgroups Dataset.\n"}
{"snippet": "ratings = pd.read_table('~/data/movielens/ratings.dat', sep='::', names= ['UserID','MovieID','Rating','Timestamp'])\n", "intent": "Load the ratings.dat data into a `ratings` variable with the same separator, and the column names UserID, MovieID, Rating, Timestamp.\n"}
{"snippet": "maxlen=100\nX_train=sequence.pad_sequences(X_train,maxlen=maxlen)\nX_test=sequence.pad_sequences(X_test,maxlen=maxlen)\n", "intent": "**Overfit?** Yes, but perhaps even more importantly: our loss rate \nis increasing     \nTry again\n[ref](http://cs231n.github.io/neural-networks-3/)\n"}
{"snippet": "from sklearn.feature_selection import RFE\nselect = RFE(LinearRegression(n_jobs=-1), n_features_to_select=4)\nselect.fit(X_train, y_train)\nX_train_rfe= select.transform(X_train)\nprint('X_train.shape: {}'.format(X_train.shape))\nprint('X_train_l1.shape: {}'.format(X_train_rfe.shape))\nX_test_rfe= select.transform(X_test)\nmodel = LinearRegression(n_jobs=-1).fit(X_train_rfe, y_train)\nprint('Test score: {:.3f}'.format(model.score(X_test_rfe, y_test)))\n", "intent": "And also some recursive feature elimination:\n"}
{"snippet": "def log_transform(arr):\n    return np.log(arr)\n", "intent": "**Logarithmic model**\n"}
{"snippet": "y=df.republican\nX=df.drop('republican',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=4444)\n", "intent": "Split the data into a test and training set. Use this function:\nfrom sklearn.cross_validation import train_test_split\n"}
{"snippet": "N = 200\npoints, clusters = make_blobs(n_samples=N, centers=3, n_features=2, cluster_std=0.8, random_state=0)\npoints = np.array(points).tolist()\n", "intent": "Generate random data points\n"}
{"snippet": "df = pd.read_csv('2013_movies.csv')\ndel df['Title']\nfrom datetime import datetime\ndf['ReleaseDate'] = df.apply(lambda x: (datetime.now() \n                                        - datetime.strptime(x['ReleaseDate'],'%Y-%m-%d %H:%M:%S'))\n                                         .total_seconds(),\n                             axis=1)\ndf = pd.get_dummies(df, columns = ['Director'], drop_first=True)\ndf.dropna(inplace=True)\n", "intent": "* Movie Dataset\n* Bar Graph of each Rating\n* KNN/LogReg Prediction Accuracies\n* Stupid Predictor Accuracy\n* Comparison\n* LogReg Coefficients\n"}
{"snippet": "X = congress_votes_df.iloc[:, 1:].copy()\ny = congress_votes_df.iloc[:, 0].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "**Challenge 2**\nSplit the data into a test and training set. Use this function:\n```\nfrom sklearn.cross_validation import train_test_split\n```\n"}
{"snippet": "X = haberman_df.iloc[:, :-1].copy()\ny = haberman_df.iloc[:, -1].copy()\ny[y == 2] = 0\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "The most recent year of surgery in the dataset is 1969.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nc = {'logit__C': np.arange(.01, 2.02, .05)}\npipe = Pipeline([('scaler', StandardScaler()),\n                 ('logit', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))])\ngrid = GridSearchCV(pipe, param_grid=c, cv=4, return_train_score=True)\ngrid.fit(X_train, y_train)\nprint('Highest cross-validation accuracy: {:0.3f}'.format(grid.best_score_))\nprint('Test set score: {:0.3f}'.format(grid.score(X_test, y_test)))\nprint('Optimal regularization strength: {}'.format(grid.best_params_['logit__C']))\n", "intent": "<font color=\"blue\">\nMake a similar model but with `LogisticRegression` instead, calculate test accuracy.\n</font>\n"}
{"snippet": "neighbors = {'knn__n_neighbors': np.arange(1, 21, dtype=int)}\npipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])\ngrid = GridSearchCV(pipe, param_grid=neighbors, cv=4, return_train_score=True)\ngrid.fit(X_train, y_train)\nn = grid.best_params_['knn__n_neighbors']  \n", "intent": "<font color=\"blue\">\nDraw the learning curve for KNN with the best k value as well.\n</font>\n"}
{"snippet": "pipe = Pipeline([('scaler', StandardScaler()), ('clf', GaussianNB())])\npipe.fit(X_train, y_train)\nprint('Gaussian Naive Bayes test score: {:0.3f}'.format(pipe.score(X_test, y_test)))\n", "intent": "Lastly, let's also try GaussianNB.  We excluded it from the grid search because we don't really have any parameters over which to search.\n"}
{"snippet": "param_grid = dict(\n    max_depth=range(2, 10, 2),\n    max_features=range(1, df.shape[1] - 1, 2)\n    )\ngrid = GridSearchCV(RandomForestClassifier(n_estimators=1000,\n                                           n_jobs=-1, random_state=RANDOM_STATE), \n                    param_grid, cv=4, return_train_score=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    df, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\ngrid.fit(X_train, y_train)\n", "intent": "Let's give this a try for random forest classificatoin with varying tree depth and features.\n"}
{"snippet": "movie_df=pd.read_csv('2013_movies.csv')\n", "intent": "For your movie classifiers, calculate the precision and recall for each class.\n"}
{"snippet": "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data'\ndf = pd.read_csv(url, names = ['Class Name','handicapped-infants','water-project-cost-sharing',\n                               'adoption-of-the-budget-resolution','physician-fee-freeze','el-salvador-aid',\n                              'religious-groups-in-schools','anti-satellite-test-ban','aid-to-nicaraguan-contras',\n                              'mx-missile','immigration','synfuels-corporation-cutback','education-spending',\n                              'superfund-right-to-sue','crime','duty-free-exports','export-administration-act-south-africa'])\n", "intent": "For the house representatives data set, fit and evaluate a decision tree classifier. Examine the rules your tree uses.\n"}
{"snippet": "N = 300\npoints, clusters = make_blobs(n_samples=N, centers=5, n_features=2, cluster_std=0.8, random_state=0)\npoints = np.array(points).tolist()  \nclusters = np.array(clusters).tolist()  \n", "intent": "Generate random data points\n"}
{"snippet": "url2 = 'https://storage.googleapis.com/kaggle-competitions-data/kaggle/3136/train.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1519266715&Signature=QAmWZ44uU2dmteoEgWcV%2BRLslJrMwCknx5I4Kk8evvjI69ajUWimlwojf5hEe8HTavVeEUdvYqqrXIJZpSB4wHahv%2FiIshNigAim2iV4GnNEro9NwQUh2HapuOt1l%2FfdRkFdLFO66mXVA5EhY4NghBXNvsY2TOKDpOZecFKWRuIwv0jdBHcttcCZS%2F5xjJf9IC1qRdLUc2TAFRzAL08BssfpdV%2F86QVhZrzKIq6bBwJeyOZmMeG0twqUuhXnfy0MnOuzUbUyJBlV3TyEeEHfq6vgdW%2BPCFxi6XI%2FGkgUtuXxMJMviWAFDOFsqt4PKQ20CBOFfSnlv8iU0FlQBEU2dg%3D%3D'\ndf3 = pd.read_csv(url2)\ndf3.head()\n", "intent": "**Challenge Number 3**\n"}
{"snippet": "import pandas as pd\nimport numpy as np\npath='https://ibm.box.com/shared/static/q6iiqb1pd7wo8r3q28jvgsrprzezjqk3.csv'\ndf = pd.read_csv(path)\n", "intent": "<p></p>\n<li>Model Evaluation</li>\n<li>Over-fitting, Under-fitting and Model Selection</li>\n<li>Ridge Regression</li>\n<li>Grid Search</li>\n<p></p>\n"}
{"snippet": "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\nnames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\ndataset = pandas.read_csv(url, names=names)\n", "intent": "> Iris Path: Dataset is available on many sites but we are taking it from UCI ML\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "plyfile = PlyData.read('Datasets/stanford_armadillo.ply')\narmadillo = pd.DataFrame({\n  'x':plyfile['vertex']['z'][::reduce_factor],\n  'y':plyfile['vertex']['x'][::reduce_factor],\n  'z':plyfile['vertex']['y'][::reduce_factor]\n})\n", "intent": "Load up the scanned armadillo:\n"}
{"snippet": "pca = do_PCA(armadillo, 'full')\n", "intent": "Let's see how long it takes PCA to execute:\n"}
{"snippet": "city = df.groupby('City')['Zip'].count().reset_index().sort_values('Zip', ascending=False).reset_index(drop=True)\ncity_community = df[df['Community School?'] == 'Yes'].groupby('City')['Zip'].count().reset_index().sort_values('Zip', ascending=False).reset_index(drop=True)\ncity_merge = pd.merge(city, city_community, how='left', on='City')\ncity_merge.fillna(0, inplace=True)\ncity_merge['Zip_y'] = city_merge['Zip_y'].astype(int)\ntop_10_city = city_merge.iloc[:10,]\ntop_10_city = top_10_city.rename(columns={\"Zip_x\":'Total Count', \"Zip_y\":'Community Count'})\ntop_10_city\n", "intent": "<a id='school_per_city'></a>\n***\nThe **Top 3 Cities** with the most schools are:\n1. **Brooklyn**: 411\n2. **Bronx**: 297\n3. **New York **: 232\n"}
{"snippet": "registration_per_year = pd.DataFrame(shsat.groupby('Year of SHST')['Number of students who registered for the SHSAT'].sum()).reset_index()\nregistration_per_year\n", "intent": "<a id='registration_per_year'></a>\nYear **2014** has the highest amount of registrations at **838**, but then it falls off in the following years.\n"}
{"snippet": "data = np.loadtxt('house_pricing.csv')\nscaler = StandardScaler()\nX = scaler.fit_transform(data[:, :-1])\ny = data[:, -1:]\ny_log = np.log(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n", "intent": "Let's load house pricing data again.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nspam_data = pd.read_csv('spam.csv')\nspam_data['target'] = np.where(spam_data['target']=='spam',1,0)\nspam_data.head(10)\n", "intent": "In this assignment you will explore text message data and create models to predict if a message is spam or not. \n"}
{"snippet": "milk_prod = pd.read_csv('monthly-milk-production.csv', index_col='Month')\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", \\\n                   quoting=3 )\nprint test.shape\nnum_reviews = len(test[\"review\"])\n", "intent": "Gets over 90% on the test data\n"}
{"snippet": "import pandas as pd\npath = 'data/yelp.csv'\nyelp = pd.read_csv(path)\n", "intent": "Read **`yelp.csv`** into a pandas DataFrame and examine it.\n"}
{"snippet": "n=len(df) \nm=len(df.iloc[0,:])\ndf_matrix = df.as_matrix()\nnx = df_matrix[:,0:(m-1)]\nny = df_matrix[:,m-1]\nny = ny[:,np.newaxis] \nW1 = np.linalg.inv(np.dot(nx.T,nx)) \nW2 = np.dot(nx.T,ny)\nW = np.dot(W1,W2)\nweights_df = pd.DataFrame(W,index=['beta0','beta1'])\n", "intent": "Matrix multiplication Linear Regression to obtain the weights\n$ W = (X^T X)^{-1} X^T Y $\n$ W_1 = (X^T X)^{-1} $\n$ W_2 = X^T Y $\n"}
{"snippet": "d_train.Embarked = d_train.Embarked.fillna(\"None\")\nd_train.Cabin = d_train.Cabin.fillna(\"None\")\nd_train.Age = d_train.Age.fillna(d_train.Age.mean())\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nd_train['Sex'] = encoder.fit_transform(d_train.Sex)\nd_train['Cabin'] = encoder.fit_transform(d_train.Cabin)\nd_train['Embarked'] = encoder.fit_transform(d_train.Embarked)\nd_train.head()\n", "intent": "**Transformar dados categoricos e limpando algumas colunas**\n"}
{"snippet": "df = pd.read_csv(\"insurance.csv\", header=None, names=['n_rei', 'pgtTot'])\n", "intent": "ALGORITMO BASELINE:\n"}
{"snippet": "data = datasets.load_boston() \n", "intent": "Loads Boston dataset from datasets library:\n"}
{"snippet": "df = pd.DataFrame(data.data, columns=data.feature_names) \n", "intent": "Define the data/predictors as the pre-set feature names:\n"}
{"snippet": "target = pd.DataFrame(data.target, columns=[\"MEDV\"]) \n", "intent": "Put the target (housing value -- MEDV) in another DataFrame:\n"}
{"snippet": "from IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()\n", "intent": "- Email: <kevin@dataschool.io>\n- Website: http://dataschool.io\n- Twitter: [@justmarkham](https://twitter.com/justmarkham)\n"}
{"snippet": "target = pd.DataFrame(data.target, columns=[\"MEDV\"])\n", "intent": "Put the target (housing value -- MEDV) in another DataFrame:\n"}
{"snippet": "import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\ndata = datasets.load_iris()\n", "intent": "> Put your code here\n"}
{"snippet": "log_model.score(X = pd.DataFrame(encoded_sex) ,\n                y = df[\"Survived\"])\n", "intent": "We can also get the accuracy of a model using the scikit-learn model.score() function:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nmodel2 = LogisticRegression()\nmodel2.fit(X_train, y_train)\n", "intent": "So far, we have trained and tested on the same set. Let's instead split the data into a training set and a testing set.\n"}
{"snippet": "loans = pd.read_csv(\"/Users/racheldyap/Desktop/BTS 1st Semester/CDA/Session 8/loan_data.csv\")\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "conf_mat = confusion_matrix(y_test, y_hat_test)\nconf_df = pd.DataFrame(conf_mat, columns = ['y_hat=0', 'y_hat = 1'], index = ['y=0', 'y=1'])\nconf_df\n", "intent": "**3\\. Compute the confusion table for both the fitted classifier and the classifier that predicts all 0's.**\n"}
{"snippet": "conf_mat_1 = confusion_matrix(y_test, y_hat_test)\nconf_df_1 = pd.DataFrame(conf_mat_1, columns = ['y_hat=0', 'y_hat = 1'], index = ['y=0', 'y=1'])\nconf_df_1\n", "intent": "We now create the confusion matrix.\n"}
{"snippet": "conf_mat_2 = confusion_matrix(y_test2, y_hat_test2)\nconf_df_2 = pd.DataFrame(conf_mat_2, columns = ['y_hat=0', 'y_hat = 1'], index = ['y=0', 'y=1'])\nconf_df_2\n", "intent": "We now create the confusion matrix for Problem 2. \n"}
{"snippet": "X_train_quad = PolynomialFeatures(2).fit_transform(X_train)\nX_test_quad = PolynomialFeatures(2).fit_transform(X_test)\ny_train.shape, X_train_quad.shape, y_test.shape, X_test_quad.shape\n", "intent": "> \n> Both Multinomial & OvR implemented for reference.\n"}
{"snippet": "import pandas as pd\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data'\ncol_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\npima = pd.read_csv(url, header=None, names=col_names)\n", "intent": "[Pima Indian Diabetes dataset](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) from the UCI Machine Learning Repository\n"}
{"snippet": "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train,\n                                                stratify = labels_train, random_state = 123)\n", "intent": "Train/Validation Split\n"}
{"snippet": "Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(Xtrain, Ytrain,\n                                                  stratify=Ytrain, random_state=123)\n", "intent": "Train/Validation Split\n"}
{"snippet": "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train, \n                                                stratify = labels_train,\n                                                random_state = 123)\n", "intent": "Train/Validation Split\n"}
{"snippet": "car = pd.read_csv('../datasets/car_evaluation/car.csv')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "np.random.seed(101)\nfrom sklearn.model_selection import train_test_split\ndatatrain, datatest = train_test_split(data2.ix[comind&demoind], test_size=0.5)\n", "intent": "Now split the dataset, train the model over one half and apply to the other\n"}
{"snippet": "df = pd.read_csv('HtWt.csv')\ndf.head()\n", "intent": "We have observations of height and weight and want to use a logistic model to guess the sex.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = deputes[['place_en_hemicycle']]\ny = deputes['groupe_sigle']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25)\n", "intent": "Which [algorithm](http://scikit-learn.org/stable/tutorial/machine_learning_map/) should I use ? \n"}
{"snippet": "from sklearn.datasets import fetch_lfw_people\nfaces = fetch_lfw_people(min_faces_per_person=60)\nprint(faces.target_names)\nprint(faces.images.shape)\n", "intent": "This example applies a PCA projection to facial image data \nusing the Labeled Faces in the Wild dataset made available through Scikit-Learn:\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.keys()\n", "intent": "We previously looked at the hand-written digits data, and we use that again here to see how the random forest classifier can be used in this context.\n"}
{"snippet": "iris = datasets.load_iris()\nx_vals = np.array([x[3] for x in iris.data])\ny_vals = np.array([y[0] for y in iris.data])\n", "intent": "Next we load the Iris data from the Scikit-Learn library.\n"}
{"snippet": "from pandas import DataFrame\nlabels = [\"hot\", \"warm\", \"cool\", \"cold\"] * 25\nrandom.shuffle(labels)\nN = 100\ndata = DataFrame(np.random.randint(0, 100, size=(N, 2)), columns = [\"x\", \"y\"], index = labels)\ndata.count\n", "intent": "Create the training data\n"}
{"snippet": "test_data = replace_strings(pd.read_csv('/root/labs/demos/titanic/test.csv'))\ntest_data\n", "intent": "Not bad! We have some test data to run it against in train.csv\n"}
{"snippet": "x, y = datasets.make_classification(n_samples=20000, n_features=20, n_classes=2, n_informative=15)\nprint numpy.percentile(x[y == 1 ],  [0, 25, 50, 75, 100])\nprint numpy.percentile(x[y == 0 ],  [0, 25, 50, 75, 100])\n", "intent": "**Let us now build a much larger matrix for use in our classification initiatives.**\n"}
{"snippet": "train = pd.read_csv('data/titanic_train.csv')\ntest = pd.read_csv('data/titanic_test.csv')\n", "intent": "2. We load the train and test datasets with Pandas.\n"}
{"snippet": "(X_train, X_test, \n y_train, y_test) = cv.train_test_split(X, y, test_size=.05)\n", "intent": "6. Let's try to train a `LogisticRegression` classifier. We first need to create a train and a test dataset.\n"}
{"snippet": "submission = generate_submission(threshold=0.04)\nsubm_name = \"FADL1-L3CA-submission-RN34-06\"\nsubmission.to_csv(f'{SUBM}{subm_name}.csv.gz', compression='gzip', index=False)\nFileLink(f'{SUBM}{subm_name}.csv.gz')\n", "intent": "This scores: **0.90181 -- 517/938** Private, when using a threshold of `0.035` and Log predictions.\n"}
{"snippet": "df = pd.read_csv(path + 'train.csv')\nlen(df['id'])\n", "intent": "Getting length of dataset. Dataset is my custom-built set for my G-LOC-Detector.\n"}
{"snippet": "PATH = 'data/nietzsche/'\nget_data(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", f'{PATH}nietzsche.txt')\ntext = open(f'{PATH}nietzsche.txt').read()\nprint('corpus length:', len(text))\n", "intent": "> We're going to download the collected works of Nietzsche to use as our data for this class.\n"}
{"snippet": "movie_names = pd.read_csv(path+'movies.csv').set_index('movieId')['title'].to_dict()\n", "intent": "Just for display purposes, let's read in the movie names too:\n"}
{"snippet": "iris = datasets.load_iris()\nx_vals = np.array([x[3] for x in iris.data])\ny_vals = np.array([y[0] for y in iris.data])\n", "intent": "Now we load the Iris data.\n"}
{"snippet": "centers = [[1,1], [6,6]]\nX, _ = make_blobs(n_samples=400, n_features=2, centers=centers, cluster_std=3.)\nrun_MS();\n", "intent": "However, if there's an outlier point that doesn't converge with the rest, Mean Shift will make it it's own cluster.\n"}
{"snippet": "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntrain = pd.read_csv(comp_path/TRAIN_DATA_FILE)\n", "intent": "Labels are already binary encoded, so no need to numericalize. Therefore `use_vocab=False`.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df, y, random_state=42)\n", "intent": "**We won't be scaling the data for this model, because it's not required here.**\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nprint(digits.DESCR)\n", "intent": "Let's load a simple dataset of 8x8 gray level images of handwritten digits (bundled in the sklearn source code):\n"}
{"snippet": "from sklearn.decomposition import RandomizedPCA\npca = RandomizedPCA(n_components=2)\nX_pca.shape\n", "intent": "Let's visualize the dataset on a 2D plane using a projection on the first 2 axis extracted by Principal Component Analysis:\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits_split_filenames = persist_cv_splits(digits.data, digits.target,\n    name='digits', random_state=42)\ndigits_split_filenames\n", "intent": "Let's try it on the digits dataset, we can run this from the :\n"}
{"snippet": "TfidfVectorizer()\n", "intent": "The text vectorizer has many parameters to customize it's behavior, in particular how it extracts tokens:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntext_train_small, text_validation, target_train_small, target_validation = train_test_split(\n    text_train_all, target_train_all, test_size=.5, random_state=42)\n", "intent": "Let's split the training CSV file into a smaller training set and a validation set with 100k random tweets each:\n"}
{"snippet": "from sklearn.feature_extraction.text import HashingVectorizer\nh_vectorizer = HashingVectorizer(charset='latin-1')\ndv['h_vectorizer'] = h_vectorizer\ndv['read_csv'] = read_csv\ndv['training_csv_file'] = training_csv_file\ndv['n_partitions'] = len(client)\n", "intent": "Let's send all we need to the engines\n"}
{"snippet": "iris = datasets.load_iris()\nx_vals = np.array([x[3] for x in iris.data]) \ny_vals = np.array([y[0] for y in iris.data]) \n", "intent": "We load the iris data.\n"}
{"snippet": "pca.fit(_90s_z)\n_90s_existing_nd = pca.transform(_90s_z)\n_90s_existing_df_nd = pd.DataFrame(_90s_existing_nd)\n_90s_existing_df_nd.index = _90s.index\n_90s_existing_df_nd.columns = _90s.columns\n_90s_existing_df_nd.head()\n", "intent": "Repeat the same steps for the other dataset.\n"}
{"snippet": "c1means = pd.DataFrame(cluster1.mean())\nc2means = pd.DataFrame(cluster2.mean())\nc3means = pd.DataFrame(cluster3.mean())\nc4means = pd.DataFrame(cluster4.mean())\nc5means = pd.DataFrame(cluster5.mean())\nc6means = pd.DataFrame(cluster6.mean())\nc7means = pd.DataFrame(cluster7.mean())\n", "intent": "Now, lets look at the characteristics of each cluster by calculating means of each clusters.\n"}
{"snippet": "pca.fit(_10s_z)\n_10s_existing_nd = pca.transform(_10s_z)\n_10s_existing_df_nd = pd.DataFrame(_10s_existing_nd)\n_10s_existing_df_nd.index = _10s.index\n_10s_existing_df_nd.columns = _10s.columns\n_10s_existing_df_nd.head()\n", "intent": "Repeat the same steps for the other dataset.\n"}
{"snippet": "from sklearn.datasets import load_digits\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nX, y = load_digits(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n", "intent": "<h3>Two Layer Fully Connected Neural Net with SGD</h3>\n"}
{"snippet": "trainDF = pd.read_csv('train.csv')\ntrainDF = trainDF.dropna(how=\"any\").reset_index(drop=True)\ntrainDF.ix[:7,3:]\nprint trainDF.shape\n", "intent": "Load data and show some samples of the data\n-------------------------------------------\n"}
{"snippet": "submissionName = 'shallowBenchmark2'\nsubmission = pd.DataFrame()\nsubmission['test_id'] = testDF['test_id']\nsubmission['is_duplicate'] = testPredictions\nsubmission.to_csv(submissionName + '.csv', index=False)\n", "intent": "Create a Submission\n-------------------\n"}
{"snippet": "trainDF = pd.read_csv('train.csv')\ntrainDF = trainDF.dropna(how=\"any\").reset_index(drop=True)\ntrainDF.ix[:7,3:]\n", "intent": "Load data and show some samples of the data\n-------------------------------------------\n"}
{"snippet": "submissionName = 'shallowBenchmark'\nsubmission = pd.DataFrame()\nsubmission['test_id'] = testDF['test_id']\nsubmission['is_duplicate'] = testPredictions\nsubmission.to_csv(submissionName + '.csv', index=False)\n", "intent": "Create a Submission\n-------------------\n"}
{"snippet": "masked_epi = masker.inverse_transform(samples)\nmax_zscores = nl.image.math_img(\"np.abs(img).max(axis=3)\", img=masked_epi)\nnl.plotting.plot_stat_map(max_zscores, bg_img=anat, dim=-.5)\n", "intent": "To recover the original data shape (giving us a masked and z-scored BOLD series), we simply use the masker's inverse transform:\n"}
{"snippet": "(x_vals, y_vals) = datasets.make_circles(n_samples=350, factor=.5, noise=.1)\ny_vals = np.array([1 if y==1 else -1 for y in y_vals])\nclass1_x = [x[0] for i,x in enumerate(x_vals) if y_vals[i]==1]\nclass1_y = [x[1] for i,x in enumerate(x_vals) if y_vals[i]==1]\nclass2_x = [x[0] for i,x in enumerate(x_vals) if y_vals[i]==-1]\nclass2_y = [x[1] for i,x in enumerate(x_vals) if y_vals[i]==-1]\n", "intent": "For this example, we will generate fake non-linear data.  The data we will generate is concentric ring data.\n"}
{"snippet": "import google.datalab.bigquery as bq\nsql = \"SELECT image_url, short_label, label FROM demos.cervical_truncated_images\"\ndf = bq.Query(sql).execute().result().to_dataframe()\nprint 'Have a total of {} labeled images'.format(len(df))\ndf.tail()\n", "intent": "This is what the labeled dataset looks like (To train on a different set of images, start with a BigQuery table that has these two columns).\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer \ncount_vector = CountVectorizer()\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "data = pd.read_csv('311-service-requests.csv', parse_dates=['Created Date'], low_memory=False)\n", "intent": "Check first that you have the CSV in the same folder as the .ipynb notebook.\n"}
{"snippet": "data = pandas.read_csv('datasets/training.csv')\ndata_correlation = pandas.read_csv('datasets/check_correlation.csv')\n", "intent": "Download \n* `training.csv`, \n* `check_correlation.csv`, \nto the folder `datasets/` from https://www.kaggle.com/c/flavours-of-physics/data\n"}
{"snippet": "df2 = pd.DataFrame({'A' : 1.,\n                  'B' : pd.Timestamp('20130102'),\n                  'C' : pd.Series(1,index=list(range(4)),dtype='float32'),\n                  'D' : np.array([3] * 4,dtype='int32'),\n                  'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]),\n                  'F' : 'foo' })\ndf2\n", "intent": "Creating a DataFrame by passing a dict of objects that can be converted to series-like.\n"}
{"snippet": "df1.fillna(value=5)\n", "intent": "Filling missing data\n"}
{"snippet": "events = pd.read_csv(\"data/train.csv\")\n", "intent": "Now let's plot some events.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve\ndata = pandas.read_csv('datasets/training.csv')\n", "intent": "Compare ROC curve stability for simple Tree and for any ensemble method. Do they have different confidence intervals for ROC curves and AUC indeed?\n"}
{"snippet": "from sklearn.datasets import fetch_covtype\ncovtype = fetch_covtype()\n", "intent": "http://habrahabr.ru/post/249759/\nhttp://nbviewer.ipython.org/github/aguschin/kaggle/blob/master/forestCoverType_featuresEngineering.ipynb\n"}
{"snippet": "print('Getting Test Set Accuracy For {} Sentences.'.format(len(texts_test)))\ntest_acc_all = []\nfor ix, t in enumerate(vocab_processor.fit_transform(texts_test)):\n    y_data = [[target_test[ix]]]\n    if (ix+1)%50==0:\n        print('Test Observation \n    [[temp_pred]] = sess.run(prediction, feed_dict={x_data:t, y_target:y_data})\n    test_acc_temp = target_test[ix]==np.round(temp_pred)\n    test_acc_all.append(test_acc_temp)\nprint('\\nOverall Test Accuracy: {}'.format(np.mean(test_acc_all)))\n", "intent": "Now that we have a logistic model, we can evaluate the accuracy on the test dataset.\n"}
{"snippet": "data_agreement = pandas.read_csv('datasets/check_agreement.csv')\ndata_MC = pandas.concat([data_agreement[data_agreement.signal == 1], data[data.signal == 1]])\ndata_MC['signal'] = numpy.array([0] * sum(data_agreement.signal.values == 1) + [1] * sum(data.signal.values == 1))\n", "intent": "Use U-test to compare different ND pdfs\n"}
{"snippet": "columns = ['hSPD', 'pt_b', 'pt_phi', 'vchi2_b']\noriginal = root_numpy.root2array('datasets/MC_distribution.root', branches=columns)\ntarget = root_numpy.root2array('datasets/RD_distribution.root', branches=columns)\noriginal = pandas.DataFrame(original)\ntarget = pandas.DataFrame(target)\noriginal_weights = numpy.ones(len(original))\n", "intent": "Pay attention that here we work with `.root` files and `root_numpy` can help\n"}
{"snippet": "train_index, test_index = train_test_split(range(len(data)))\ntrain = data.iloc[train_index, :]\ntest = data.iloc[test_index, :]\n", "intent": "The worst features are `SPDhits`, `FlightDistance`, `IP_p1p2`\n"}
{"snippet": "PATH_TO_DATA = ('./data')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "ohe = OneHotEncoder()\nohe_columns = ['weekday'] + hour\nohe.fit(X[ohe_columns])\nX_ohe = ohe.transform(X[ohe_columns])\ntest_ohe = ohe.transform(test[ohe_columns])\n", "intent": "Scale this features and combine then with Tf-Idf based on sites (you'll need `scipy.sparse.hstack`)\n"}
{"snippet": "x_train_multi, x_test_multi, y_train_multi, y_test_multi = Multi_log_reg.split(x_multi,y_multi,rand=None)\nnot_bi = Multi_log_reg.not_bi(x_multi)\nscaler = StandardScaler()\nscaler.fit(x_train_multi[not_bi]) \nx_train_scaled_multi=x_train_multi\nx_test_scaled_multi=x_test_multi\nx_train_scaled_multi[not_bi] = scaler.transform(x_train_multi[not_bi])\nx_test_scaled_multi[not_bi]  = scaler.transform(x_test_multi[not_bi])\nmodel_multi = Multi_log_reg.reg(x_train_scaled_multi,y_train_multi)\nprint(\"Mean accuracy score is %f\" % model_multi.score(x_test_scaled_multi,y_test_multi,sample_weight=None))\n", "intent": "Run Multi-Class(Ridge)\n"}
{"snippet": "def pca_k(k):\n    pca = PCA(n_components=k)\n    principalComponents = pca.fit_transform(X)\n    pca.explained_variance_ratio_\n    print(\"The percentage of Variance retained by K = \",k, \" : \", sum(pca.explained_variance_ratio_))\n", "intent": "   - My first attempt with K = 30 yield 0.761394, which is not a very good result.\n   - When K goes up to 70 The result seems promising.\n"}
{"snippet": "from sklearn.model_selection import train_test_split , cross_val_score\ndef split(x,y,rand=0):\n        y = np.ravel(y)\n        x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.25,random_state=rand)\n        return x_train, x_test, y_train, y_test \n", "intent": "Manually generate validation dataset (in case of clf is not useful...)\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X.reshape([len(X),-1]),\n                                                 y.reshape([len(y),-1]),\n                                                 test_size=0.05,random_state=42)\nprint(\"X_train.shape=\",X_train.shape)\nprint(\"X_test.shape=\",X_test.shape)\n", "intent": "Now let's get through the usual pipeline: split data between train and test; fit linear regression\n"}
{"snippet": "def tokenizer(text):\n    words = nltk.word_tokenize(text)\n    return words\ntfidf = TfidfVectorizer(tokenizer=tokenizer, stop_words='english', max_features=max_features)\nsparse_tfidf_texts = tfidf.fit_transform(texts)\n", "intent": "Define tokenizer function and create the TF-IDF vectors with SciKit-Learn.\n"}
{"snippet": "from sklearn.random_projection import GaussianRandomProjection\nXrp = GaussianRandomProjection(n_components=2).fit_transform(X)\n", "intent": " * Pick several random axes from normal distribution\n * Projects data to these axis\n * Mostly useless for our task.\n"}
{"snippet": "from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=2)\nXsvd = svd.fit_transform(X)\n", "intent": "* Idea: try to compress the data in a way that you can then restore it\n* Equivalent to minimizing MSE: $|| X  - U \\cdot \\Sigma \\cdot V^T ||$\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(X.values, y.values,\n                                                                train_size=0.85, random_state=1234)\n", "intent": "First of all, lets split our train data to train and validation sets:\n"}
{"snippet": "import pandas\ndf2 = pandas.DataFrame(data=memo_time)\ndf2 = df2.set_index(\"legend\").sort_values(\"average\")\ndf2\n", "intent": "C'est beaucoup plus rapide.\n"}
{"snippet": "iris = datasets.load_iris()\nX_iris = iris.data\ny_iris = iris.target\n", "intent": "Nous partirons classiquement du dataset Iris (classification de 3 fleurs sur la base de certaines de leurs mesures) :\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "``scikit-learn`` embeds a copy of the iris CSV file along with a helper function to load it into numpy arrays:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(faces.data,\n        faces.target, random_state=0)\nprint(X_train.shape, X_test.shape)\n", "intent": "We'll perform a Support Vector classification of the images.  We'll\ndo a typical train-test split on the images to make this happen:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n", "intent": "Simple [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n"}
{"snippet": "from pyensae.datasource import download_data\nif False:\n    file = download_data(\"Divvy_Trips_2016_Q3Q4.zip\",\n                         url=\"https://s3.amazonaws.com/divvy-data/tripdata/\")\nelse:\n    file = download_data(\"Divvy_Trips_2016_Q3.zip\")    \n", "intent": "Une carte des stations un jour de semaine.\n"}
{"snippet": "original_image = scipy.misc.imread(original_image_file)\nstyle_image = scipy.misc.imread(style_image_file)\ntarget_shape = original_image.shape\nstyle_image = scipy.misc.imresize(style_image, target_shape[1] / style_image.shape[1])\n", "intent": "Read in the images.\n"}
{"snippet": "train_df[\"avg_item_cat_price\"] = train_df[\"avg_item_cat_price\"].fillna(train_df[\"avg_all_item_cat_price\"]) \ntrain_df[\"avg_all_item_price\"] = train_df[\"avg_all_item_price\"].fillna(train_df[\"avg_all_item_cat_price\"]) \ntrain_df[\"avg_item_price\"] = train_df[\"avg_item_price\"].fillna(train_df[\"avg_all_item_price\"]) \ntrain_df[\"avg_shop_price\"] = train_df[\"avg_shop_price\"].fillna(train_df[\"avg_all_shop_price\"]) \ntrain_df[\"avg_all_shop_item_price\"] = train_df[\"avg_all_shop_item_price\"].fillna(train_df[\"avg_all_item_price\"]) \n", "intent": "Some items in the test dataset do not exist in train dataset. Imputation is needed. \n"}
{"snippet": "from itertools import product\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = []\nfor block_num in train_df['date_block_num'].unique():\n    cur_shops = train_df.loc[train_df['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = train_df.loc[train_df['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n", "intent": "Create grid using all the shop_id, item_id combiation for each date_block_num\n"}
{"snippet": "for df in train_df, test_df:\n    for feat in df.columns[4:]:\n        if 'sales' in feat:\n            df[feat]=df[feat].fillna(0)            \n        else:\n            df[feat]=df[feat].fillna(df[feat].median())           \n", "intent": "Some items in the test dataset do not exist in train dataset. Imputation is needed. \n"}
{"snippet": "index_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\ngb = sales.groupby(index_cols, as_index=False)['item_cnt_day'].agg('sum').rename(columns={'sum':'target'})\n", "intent": "This is necessary because shop and items could be different from one month to the next\n"}
{"snippet": "sales_submission_prev_month = pd.merge(sales_test, grid.loc[grid.date_block_num == 33], how='left', on=['shop_id', 'item_id'])\nsales_submission_prev_month = sales_submission_prev_month.loc[:,['ID', 'item_cnt_day']].fillna(0).rename(columns={'item_cnt_day':'item_cnt_month'})\nsales_submission_prev_month.to_csv('../readonly/sales_submission_prev_month.csv.gz', index=False, compression='gzip')\n", "intent": "Submit the sales for 2015 October (date_block_num = 33)\n"}
{"snippet": "index_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n", "intent": "We now need to prepare the features. This part is all implemented for you.\n"}
{"snippet": "summary_hour_duration = pd.DataFrame(train_data.groupby(['day_of_week','hour'])['trip_duration'].mean())\nsummary_hour_duration.reset_index(inplace = True)\nsummary_hour_duration['unit']=1\nsns.set(style=\"white\", palette=\"muted\", color_codes=False)\nsns.set_context(\"poster\")\nsns.tsplot(data=summary_hour_duration, time=\"hour\", unit = \"unit\", condition=\"day_of_week\", value=\"trip_duration\")\nsns.despine(bottom = False)\n", "intent": "- Simple lineplots can explain how the trip duration is changing with time for different days of week\n- very easy to interpret\n"}
{"snippet": "test_df = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv')\ntest_fr = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_test.csv')\ntest_fr_new = test_fr[['id', 'total_distance', 'total_travel_time', 'number_of_steps']]\ntest_df = pd.merge(test_df, test_fr_new, on = 'id', how = 'left')\ntest_df.head()\n", "intent": " 1. Loading test data from competition and OSRM features for test data\n"}
{"snippet": "train = pd.read_csv('../data/train.csv')\ntest = pd.read_csv('../data/test.csv')\n", "intent": "<h2>1.3 Loading the Data</h2><br>\n<p>Load the data using the Pandas `read_csv` function:</p>\n"}
{"snippet": "cols = train.columns.values\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n", "intent": "Create a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n", "intent": "`TfidfVectorizer` = `CountVectorizer` + `TfidfTransformer`\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(documents, lowercase = True, token_pattern = '(?u)\\\\b\\\\w\\\\w+\\\\b' )\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "features = features.fillna(0.0)\ndisplay(features.head())\n", "intent": "And now we'll fill in any blanks with zeroes.\n"}
{"snippet": "df_mm = p.MinMaxScaler().fit_transform(df) \n", "intent": "`3.` Now it's your turn.  Try fitting the **MinMaxScaler** transformation to this dataset. You should be able to use the previous example to assist.\n"}
{"snippet": "pca, X_pca = do_pca(15, X)\n", "intent": "Now let's fit PCA with 15 components, and take a look at a few of the main features that live on the pca object we get back.\n"}
{"snippet": "df_train['Embarked'] = df_train['Embarked'].fillna(\"\")\nembarked_locs = sorted(df_train['Embarked'].unique())\nembarked_locs_mapping = dict(zip(embarked_locs, \n                                 range(0, len(embarked_locs) + 1)))\nembarked_locs_mapping\n", "intent": "Prepare to map Embarked from a string to a number representation:\n"}
{"snippet": "df_test = pd.read_csv('data/test.csv')\ndf_test.head()\n", "intent": "Read the test data:\n"}
{"snippet": "df_test['Survived'] = test_y\ndf_test[['PassengerId', 'Survived']] \\\n    .to_csv('data/results-rf.csv', index=False)\n", "intent": "Create a DataFrame by combining the index from the test data with the output of predictions, then write the results to the output:\n"}
{"snippet": "dataset = pandas.read_csv(\"tennis.csv\")\n", "intent": "** Loading data, generated locally for example 'John plays tennis?' **\n"}
{"snippet": "train_x, train_y, test_x, test_y = load_data(\"data/basketball.train.csv\")\n", "intent": "And then we can use it to load the data.\n"}
{"snippet": "import pandas\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = pandas.read_csv(url, names=names)\ndataframe.head()\n", "intent": "Now let's have a look at the dataset.\n"}
{"snippet": "import pandas\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = pandas.read_csv(url, names=names)\ndataframe.head()\n", "intent": "Pima Indians Diabetes Data Set: https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes\n"}
{"snippet": "messages = pandas.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t',\n                           names=[\"label\", \"message\"])\nmessages.head()\n", "intent": "We'll use **read_csv** and make note of the **sep** argument, we can also specify the desired column names by passing in a list of *names*.\n"}
{"snippet": "columns = ['id number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', \n           'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class']\ndf = pd.DataFrame(data, columns = columns)\ndf.head(10)\n", "intent": "Take a look at the dataset.\n"}
{"snippet": "dataset = pandas.read_csv(\"Iris.csv\")\ndataset.drop(columns=['Id'], inplace =True)\ndataset.head()\n", "intent": "** Loading data, generated locally for example 'John plays tennis?' **\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntrain, test = train_test_split(dataset, train_size = 0.7)\nfeature_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n", "intent": "** Train / Test set split: Dividing sub-set of test and training **\n"}
{"snippet": "columns = ['id number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', \n           'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class']\ndf = pd.DataFrame(cancer_data, columns = columns)\ndf.head()\n", "intent": "Let's have a look at the dataset.\n"}
{"snippet": "column = ['Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', \n           'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class']\ndf = pd.DataFrame(cancer_data, columns = column)\ndf.head()\n", "intent": "Print the dataset again to confirm the id number column is deleted.\n"}
{"snippet": "data = pd.read_csv('Advertising.csv', index_col=0)\ndata.head()\n", "intent": "Read data into a DataFrame\n"}
{"snippet": "dicts_test_x = []\nvectorizer_test = DictVectorizer()\nvec_test_x = \n", "intent": "We do similarly for vectorizing `test_x`.\n"}
{"snippet": "model = NMF(init=\"nndsvd\",\n            n_components=4,\n            max_iter=200)\n", "intent": "Apply NMF with SVD-based initialization to the document-term matrix $\\text{A}$ generate 4 topics.\n"}
{"snippet": "W = model.fit_transform(vectorized)\nH = model.components_\n", "intent": "Get the factors $\\text{W}$ and $\\text{H}$ from the resulting model.\n"}
{"snippet": "train = pd.read_csv('./input_bikesharing/train.csv',  parse_dates=['datetime'])\nprint(train.shape)\nprint(train.columns)\nprint(train.info())\ntrain.head()\n", "intent": "1. Data Load\n=============\n"}
{"snippet": "submit = pd.read_csv('./input_bikesharing/sampleSubmission.csv')\nprint(submit.shape)\nsubmit.head()\n", "intent": "4. Submit\n=========\n"}
{"snippet": "train_df = pd.read_csv(DATA_PATH + TRAIN_CSV)\ntest_df = pd.read_csv(DATA_PATH + TEST_CSV)\nstops = set(stopwords.words('english'))\ndef text_to_word_list(text):\n", "intent": "Create Embedding Matrix\n"}
{"snippet": "df_train = pd.read_csv(DATA_PATH + 'train.csv')\ndf_train.head()\n", "intent": "Set Training Data\n=================\n"}
{"snippet": "train = pd.read_csv('./input/train.csv', index_col='PassengerId')\nprint(train.shape)\ntrain.head(2)\n", "intent": "1. Load Dataset\n===============\n"}
{"snippet": "import pandas as pd\ntrain= pd.read_csv('./input/train.csv')\ntest = pd.read_csv('./input/test.csv')\n", "intent": "**load train, test dataset using Pandas**\n"}
{"snippet": "def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived, dead])\n    df.index = ['Survived', 'Dead']\n    df.plot(kind='bar', stacked=True, figsize=(10,5))\n", "intent": "* Pclass\n* Sex\n* Sibsp(\n* Parch(\n* Embarked\n* Cabin\n"}
{"snippet": "digits = datasets.load_digits()\n", "intent": "Load a sample dataset\n"}
{"snippet": "train[\"Embarked\"].fillna('S', inplace=True)\ntest[\"Embarked\"].fillna('S', inplace=True)\n", "intent": "**fill out missing embark with S embark**\n"}
{"snippet": "matchedData = cleanData[(cleanData['PurchasedUnits'] == 1) & (cleanData['DeliveredUnits'] == 1)]\nmatchedData.ProfitabilityScore.fillna(matchedData.ProfitabilityScore.mean(), inplace=True)\nmatchedData.ClearanceScore.fillna(matchedData.ProfitabilityScore.mean(), inplace=True)\n", "intent": "Matched Dataset\n===============\nCreate the matched subset of the data  \nClean `ProfitabilityScore` and `ClearanceScore`\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndv = DictVectorizer()\nX = dv.fit_transform(data[['Category']].T.to_dict().values())\nprint X, X.shape\n", "intent": "Another option which we haven't seen before is to use another feature extraction tool from sklearn, the DictVectorizer\n"}
{"snippet": "drinks = pd.read_csv('http://bit.ly/drinksbycountry')\ndrinks.loc[0:2]\ndrinks.iloc[0:2,0:2]\nufo.fillna(method='bfill').tail()\nufo.fillna(method='ffill').tail()\n", "intent": "    drinks.loc[[0:2,9:10],:]\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.20, random_state=17)\n", "intent": "Split the data into a training set and a test set.\n"}
{"snippet": "iris = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\n", "intent": "https://en.wikipedia.org/wiki/Iris_flower_data_set\n"}
{"snippet": "import pandas as pd\ncrime = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data', header=None, na_values=['?'])\ncrime = crime.iloc[:, 5:]\ncrime.dropna(inplace=True)\ncrime.head()\nX = crime.iloc[:, :-1]\ny = crime.iloc[:, -1]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n", "intent": "This data set contains data on violent crimes within a community.\n"}
{"snippet": "import statsmodels.api as sm\ntarget = pd.DataFrame(df_rookie_averages['Salary_2018'])\nindependent_vars = df_rookie_averages.loc[:,['PTS','DWS','Age']]\nindependent_vars.astype(float)\nindependent_vars.dropna(inplace=True)\nindependent_vars.describe()\nindependent_vars = sm.add_constant(independent_vars)\ngen_model = sm.OLS(list(target['Salary_2018'].astype(float)),independent_vars.astype(float)).fit()\ngen_model.summary()\n", "intent": "This is the part of the ensemble model that is not position specific.  It does not use variables used by the position specific sub models.\n"}
{"snippet": "data = pd.read_csv('../../DSI-CHI-1/lessons/week-04/3.2-logistic-regression-lab/assets/datasets/train.tsv', sep='\\t', na_values={'is_news' : '?'}).fillna(0)\ndata['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\ndata['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n", "intent": "- These are websites that always relevant like recipes or reviews (as opposed to current events)\n- Look at some examples\n"}
{"snippet": "df = pd.read_csv('HtWt.csv')\ndf.head()\n", "intent": "We will look at the effect of strongly correlated variabels using a data set from Kruschke's book.\n"}
{"snippet": "data = pd.read_csv('../../DSI-CHI-1/lessons/week-04/4.2-sklearn-regularization-logistic-lab/assets/datasets/train.tsv', sep='\\t', na_values='?')\ndata['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', '')).fillna('')\ndata['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', '')).fillna('')\n", "intent": "- These are websites that always relevant like recipies or reviews (as opposed to current events)\n- Look at some examples\n"}
{"snippet": "dimensions = ['mean', 'se', 'worst']\nattributes = ['radius', 'texture', 'perimeter', 'area', 'smoothness', 'compactness',\n              'concavity', 'concave_points', 'symmetry', 'fractal_dimension']\nattribute_names = ['{}-{}'.format(x, y) for x in attributes for y in dimensions]\ncell_data_filepath = 'https://s3-us-west-2.amazonaws.com/ga-dat-2015-suneel/datasets/breast-cancer.csv'\ncol_names = ['id', 'diagnosis'] + attribute_names\ncell_df = pd.read_csv(cell_data_filepath, header=None, names=col_names)\ncell_df.head()\n", "intent": "Return to the Wisconsin breast cancer data. Clean it up as we did before.\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(features_df, target_df, test_size=0.33, random_state=5)\n", "intent": "Split into 66% training set and 33% testing set\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconmat = np.array(confusion_matrix(Y_test, Y_pred))\nconfusion = pd.DataFrame(conmat, index=['is_healthy', 'has_cancer'],\n                         columns=['predicted_healthy', 'predicted_cancer'])\nprint(confusion)\n", "intent": "Look at the confusion matrix\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(dummies)\nX.shape\n", "intent": "Cluster the Data to our our target groups. **BONUS**: Perform multiple clustering tests on various indicator variables\n"}
{"snippet": "df = pd.read_csv('../../DSI-CHI-1/lessons/week-07/1.3-tuning-clusters/assets/datasets/iris.csv')\ndf.head()\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_standard = StandardScaler().fit_transform(X)\n", "intent": "Run the clustering analysis using scikit-learn.\n*Hint*: Estimate the number of clusters, k, based on your visual examination of the distributions.\n"}
{"snippet": "df = pd.read_csv('../../DSI-CHI-1/lessons/week-07/1.3-tuning-clusters/assets/datasets/wine.csv')\ndf.head()\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "df = pd.read_csv('../../DSI-CHI-1/lessons/week-07/2.3-pca-lab-1/assets/datasets/votes.csv', index_col=0)\nprint df.shape\ndf.head()\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "standardizer = StandardScaler(withMean=True, withStd=True, \n                              inputCol='features', \n                              outputCol='std_features')\nmodel = standardizer.fit(output)\noutput = model.transform(output)\n", "intent": "Scale features to have zero mean and unit standard deviation\n"}
{"snippet": "X_standard = StandardScaler().fit_transform(X)\nX_standard.shape\n", "intent": "Next, create the covariance matrix from the standardized x-values and decompose these values to find the eigenvalues and eigenvectors\n"}
{"snippet": "pca = PCA(n_components=8)\npca_x = pca.fit_transform(X_standard)\npca_x[0:5]\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "X_standard = StandardScaler().fit_transform(X)\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "pca = PCA(n_components=4)\npca_x = pca.fit_transform(X_standard)\npca_x[0:5]\n", "intent": "Finally, conduct the PCA - use the results about to guide your selection of \"n\" componants\n"}
{"snippet": "df_unseen = pd.read_csv('../data/bank-marketing-data/bank-unseen-data.csv')\n", "intent": "Load new data from '../data/bank-marketing-data/bank-unseen-data.csv'\n"}
{"snippet": "df_unseen = pd.read_csv('../data/bank-marketing-data/bank-unseen-data.csv', index_col=0)\n", "intent": "Load new data from '../data/bank-marketing-data/bank-unseen-data.csv'\n"}
{"snippet": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nnb_samples = 100  \nnb_features = 20  \nbatch_size = 16\nX = np.random.random((nb_samples, nb_features))\ny = np.random.randint(0, 2, nb_samples)  \nX_train, X_test, y_train, y_test = train_test_split(X, y)\ny_train.shape, X_test.shape\n", "intent": "Scikit-learn has a `train_test_split` function.\n"}
{"snippet": "df = pd.read_csv(\"./College_Data\",index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df = pd.read_csv(\"advertising.csv\")\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "from sklearn import svm, grid_search, datasets\nfrom spark_sklearn import GridSearchCV\niris = datasets.load_iris()\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\nsvr = svm.SVC()\nclf = GridSearchCV(sc, svr, parameters)\nclf.fit(iris.data, iris.target)\n", "intent": "Install if necessary\n```\n```\n"}
{"snippet": "X = bow_transformer.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "n_samples = 100\nrandom_state = 0\ndatasets = collections.OrderedDict([\n    ('Blobs', make_blobs(n_samples=n_samples, centers=2, cluster_std=0.5, random_state=random_state)),\n    ('Circles', make_circles(n_samples=n_samples, factor=.5, noise=.03, random_state=random_state)),\n    ('Moons', make_moons(n_samples=n_samples, noise=.03, random_state=random_state))\n])\n", "intent": "Create a few toy datasets for binary classification. 'Blobs' is linearly seperable, the others are not.\n"}
{"snippet": "hours = [0.0,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0]\nph = pd.DataFrame({'Hours Researched':hours,'Prob. of Hiring':[special.expit(a*x+b) for x in hours]})\nph\n", "intent": "We can use this function to predict the *probability* that a given amount of research time will result in being hired.\n"}
{"snippet": "le = LabelEncoder()\nle.fit([\"red\", \"orange\", \"yellow\"])\n", "intent": "We make an instance of the `preprocessing.LabelEncoder()` class, and then fit a given data set.\n"}
{"snippet": "list(le.inverse_transform([2, 2, 1]))\n", "intent": "This operation can easily be undone with `.inverse_transform()`:\n"}
{"snippet": "fit.apply(lambda x: d[x.name].inverse_transform(x))\n", "intent": "And then can reverse the encoding:\n"}
{"snippet": "test = pd.DataFrame(columns = c.columns)\ntest.loc[0] = ['red','square']\ntest\n", "intent": "We can then use the dictionary to label future data `df` of the correct layout with the code `df.apply(lambda x: d[x.name].transform(x))`:\n"}
{"snippet": "an = pd.read_csv('animals.csv',index_col=0)\nan\n", "intent": "Let's look at the following data about animals:\n"}
{"snippet": "user_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\nusers = pd.read_table('http://bit.ly/movieusers', sep='|', header=None, names=user_cols)\n", "intent": "Documentation for [**`read_table`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html)\n"}
{"snippet": "from sklearn import model_selection, metrics, linear_model, datasets, feature_selection, tree\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\nd = defaultdict(LabelEncoder)\nanimals = tree.DecisionTreeClassifier()\nle = LabelEncoder()\ntn1 = (train_c[['Cold or Warm Blooded','Covering','Aquatic','Aerial','Lays Eggs']]).apply(lambda x: d[x.name].fit_transform(x))\ntrain_encoded = pd.concat([train_c['Legs'],tn1],axis=1)\ntrain_encoded\n", "intent": "Now we will use the LabelEncoder on each, and create a model on the training set.\n"}
{"snippet": "iris = sklearn.datasets.load_iris()\n", "intent": "For this part, we'll use the Iris dataset, and we'll train a random forest. \n"}
{"snippet": "lfw_people = datasets.fetch_lfw_people(min_faces_per_person=50, \n                resize=0.4, data_home='../../data/faces')\nprint('%d objects, %d features, %d classes' % (lfw_people.data.shape[0],\n      lfw_people.data.shape[1], len(lfw_people.target_names)))\nprint('\\nPersons:')\nfor name in lfw_people.target_names:\n    print(name)\n", "intent": "Let's load a dataset of peoples' faces and output their names. (This step requires stable, fast internet connection.)\n"}
{"snippet": "boston = datasets.load_boston()\nX = boston.data\n", "intent": "For the next question, load the housing prices dataset:\n"}
{"snippet": "sales = pd.read_csv('Iowa_Liquor_sales_sample_10pct.csv')\n", "intent": "Load your data from project 3\n"}
{"snippet": "bcw = pd.read_csv('../assets/datasets/breast-cancer-wisconsin.csv',\n                 names=column_names, na_values=['?'])\n", "intent": "We can see that there are '?' fields in column 6 that are making some columns strings. Reload the dataset specifying na_values.\n"}
{"snippet": "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", header=None)\n", "intent": "Read in Wisconsin Breast Cancer Dataset\n"}
{"snippet": "sac = pd.read_csv('../../assets/datasets/Sacramentorealestatetransactions.csv')\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "X = pd.DataFrame(data=sX,columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "ufo = pd.read_csv('http://bit.ly/uforeports')\nufo.columns\n", "intent": "**Question:** When reading from a file, how do I read in only a subset of the columns?\n"}
{"snippet": "from sklearn import preprocessing\ndf_numerical = df.copy()\nencoder_dict = {}\nfor col in string_variables:\n    le = preprocessing.LabelEncoder()\n    le.fit(df[col])\n    encoder_dict[col] = le\n    df_numerical[col] = le.transform(df[col])\ndf_numerical.head()\n", "intent": "Learn about label encoders here: http://scikit-learn.org/stable/modules/preprocessing.html\n"}
{"snippet": "df_additional_full = pd.read_csv('homework/Hw3/bank-additional-full.csv', sep = ';')\n", "intent": "Extend the analysis and cross-validation to bank-additional-full.csv. How does the performance change?\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features_array, target, test_size=0.20, random_state=0)\n", "intent": "Let's take the 80% of the data for training a first model and keep 20% for computing is generalization score:\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\nX = iris['data']\nNames = iris['feature_names']\ntarget_names = iris['target_names']\ny = iris['target']\n", "intent": "Import the data from the iris dataset in sklearn, generate X and y arrays\n"}
{"snippet": "oos = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT4/master/data/used_vehicles_oos.csv')\noos['type'] = oos.type.map({'car':0, 'truck':1})\noos\n", "intent": "How accurate is scikit-learn's regression tree at predicting the out-of-sample data?\n"}
{"snippet": "titanic = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT4/master/data/titanic.csv')\ntitanic.head(10)\n", "intent": "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle.\n"}
{"snippet": "pd.DataFrame(x_back, columns=vectorizer.get_feature_names())\n", "intent": "If we wanted the extra overhead, we could use the feature names as columns to a dataframe:\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(min_df = 1)\ntfidf_matrix = tfidf_vectorizer.fit_transform(mydoclist)\nprint tfidf_matrix.todense()\n", "intent": "In fact, you can do this just by combining the steps into one: the TfidfVectorizer\n"}
{"snippet": "allbeers = pd.read_csv(\"beer_reviews.tar.gz\", compression='gzip')\n", "intent": "Import data in a pandas dataframe called \"allbeers\". Use the compression keyword\n"}
{"snippet": "ufo = pd.read_csv('http://bit.ly/uforeports', nrows=3)\nufo\n", "intent": "**Question:** When reading from a file, how do I read in only a subset of the rows?\n"}
{"snippet": "movies = pd.read_table('movielens/movies.dat', sep='::', names= ['ITEMID', 'Title', 'Genres'], index_col= 'ITEMID')\n", "intent": "Import 'movies.dat' to a 'movies' pandas dataframe. Make sure you name the columns, use the correct separator and define the index.\n"}
{"snippet": "ratings = pd.read_table('movielens/ratings.dat', sep='::', names= ['UserID','MovieID','Rating','Timestamp'])\n", "intent": "Import 'ratings.dat' to a 'ratings' pandas dataframe. Make sure you name the columns, use the correct separator.\n"}
{"snippet": "svd.load_data(filename='./movielens/ratings.dat', sep='::', format={'col':0, 'row':1, 'value':2, 'ids': int})\n", "intent": "Populate it with the data from the ratings dataset, using the built in load_data method\n"}
{"snippet": "def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "tsne = TSNE(random_state=17)\ntsne_representation = tsne.fit_transform(X_scaled)\n", "intent": "Now, build a t-SNE representation:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_train_all=train[:1460]\nfeature_test=train[1460:]\ntarget_data=np.log(target_data)\nX_train, X_test, y_train, y_test = train_test_split(feature_train_all,target_data, test_size = 0.2, random_state = 0)\n", "intent": "4.Shuffle and Split Data\n"}
{"snippet": "from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ncategorical=list(raw_data.select_dtypes(include=['object']).columns.values)\nprint(categorical)\nfor value in categorical:\n    le.fit(raw_data[value])\n    raw_data[value]=le.transform(raw_data[value])\nraw_data.head()\n", "intent": "3.Converting a categorical feature with labelEncoder\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_train_all=raw_data[:1460]\nfeature_test=raw_data[1460:]\nX_train, X_test, y_train, y_test = train_test_split(feature_train_all,target_data, test_size = 0.2, random_state = 0)\n", "intent": "4.Shuffle and Split Data\n"}
{"snippet": "from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ncategorical=list(raw_data.select_dtypes(include=['object']).columns.values)\nprint(categorical)\nfor value in categorical:\n    le.fit(raw_data[value])\n    raw_data[value]=le.transform(raw_data[value])\nraw_data.head()\n", "intent": "3.Converting a categorical feature\n"}
{"snippet": "drinks = pd.read_csv('http://bit.ly/drinksbycountry')\ndrinks.dtypes\n", "intent": "**Question:** How do I drop all non-numeric columns from a DataFrame?\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_train_all=raw_data_pca[:1460]\nfeature_test=raw_data_pca[1460:]\nX_train, X_test, y_train, y_test = train_test_split(feature_train_all,target_data, test_size = 0.2, random_state = 0)\n", "intent": "4.Shuffle and Split Data\n"}
{"snippet": "from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ncategorical=list(raw_data.select_dtypes(include=['object']).columns.values)\nprint(categorical)\nfor value in categorical:\n    le.fit(raw_data[value])\n    raw_data[value]=le.transform(raw_data[value])\nraw_data.head()\n", "intent": "3.Converting the categorical features\n"}
{"snippet": "from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ncategorical=list(raw_data.select_dtypes(include=['object']).columns.values)\nprint(categorical)\nfor value in categorical:\n    le.fit(raw_data[value])\n    raw_data[value]=le.transform(raw_data[value])\nraw_data.head()\n", "intent": "Converting a categorical feature with labelEncoder\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_train_all=raw_data[:4209]\nfeature_test=raw_data[4209:]\nX_train, X_test, y_train, y_test = train_test_split(feature_train_all,target_data, test_size = 0.2, random_state = 0)\n", "intent": "Shuffle and Split Data\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_train_all=raw_data_pca[:4209]\nfeature_test=raw_data_pca[4209:]\nX_train, X_test, y_train, y_test = train_test_split(feature_train_all,target_data, test_size = 0.2, random_state = 0)\n", "intent": "Shuffle and Split Data\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_train_all=raw_data_ica[:4209]\nfeature_test=raw_data_ica[4209:]\nX_train, X_test, y_train, y_test = train_test_split(feature_train_all,target_data, test_size = 0.2, random_state = 0)\n", "intent": "Shuffle and Split Data\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n", "intent": "1. randomly choose 30% of the data for testing\n2. encode the class\n3. standaridize the features\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test = train_test_split(X, test_size=0.3, random_state=0, stratify=X)\n", "intent": "1. randomly choose 30% of the data for testing\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nStdScaler = StandardScaler()\nX_train_std_sklearn = StdScaler.fit_transform(X_train)\nX_test_std_sklearn = StdScaler.transform(X_test)\n", "intent": "Here we compare the performance of our standard scaler with the StandardScaler in sklearn\n"}
{"snippet": "drinks = pd.read_csv('http://bit.ly/drinksbycountry', dtype={'beer_servings':float})\ndrinks.dtypes\n", "intent": "Documentation for [**`astype`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html)\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nStdScaler = StandardScaler()\nX_train_std_sklearn = StdScaler.fit_transform(X_train)\nX_test_std_sklearn = StdScaler.transform(X_test)\n", "intent": "Here we compare the performance of our StandardScaler with the StandardScaler in sklearn\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"c:\\pydat\\PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\n", "intent": "Find the best model to classify the digits dataset.\nUse a 10-fold cross-validation for the model selection phase.\n"}
{"snippet": "cross_validation.train_test_split(X, y, test_size=0.3)\n", "intent": "Find the best model to classify the digits dataset.\nUse a 10-fold cross-validation for the model selection phase.\n"}
{"snippet": "from sklearn import datasets\nX, _ = datasets.make_blobs(n_samples=150, random_state=8, centers=4)\nplot(X[:, 0], X[:, 1], 'bx')\n", "intent": "Prepare a sinthetic dataset for this example.\n"}
{"snippet": "from sklearn import datasets\nX, _ = datasets.make_circles(n_samples=150, factor=.5, noise=.05)\nplot(X[:, 0], X[:, 1], 'bx')\n", "intent": "Test the different clustering approaches with a \"circles\" dataset.\n"}
{"snippet": "from sklearn import decomposition\nlda = decomposition.LatentDirichletAllocation(n_topics=6)  \nlda.fit(tfidf)\n", "intent": "Compare the results of Nonnegative Matrix Factorization (NMF) with Latent Dirichlet Allocation (LDA).\n"}
{"snippet": "X_train, X_test, y_train, y_test =cross_validation.train_test_split(X,y,test_size = 0.3)\n", "intent": "Preparazione dei test e dei training set\n"}
{"snippet": "db_train = pd.read_csv('CONTEST_TRAINING_SET_PUBBLICO.CSV')\n", "intent": "Importo il dataset di training:\n"}
{"snippet": "ufo['Shape Reported'].fillna(value='VARIOUS', inplace=True)\n", "intent": "Documentation for [**`value_counts`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html)\n"}
{"snippet": "pd.DataFrame(metrics.confusion_matrix(db_result['FLG_DEF_6M'].tolist(),y_test1))\n", "intent": "Provo la bonta' del modello attraverso la confusion matrix:\n"}
{"snippet": "training_data = pd.read_csv(\"CONTEST_TRAINING_SET_PUBBLICO.CSV\",delimiter=\",\",decimal=\".\")\ntest_data = pd.read_csv(\"CONTEST_TEST_SET_PUBBLICO.csv\",delimiter=\",\",decimal=\".\")\nresults_data = pd.read_csv(\"CONTEST_TEST_RESULTS_AN_PRIVATO.csv\",delimiter=\",\",decimal=\".\")\n", "intent": "CARICO I DATASET IMPOSTANDO LE OPZIONI SUL DELIMITER E SUL SEPARATORE DECIMALE \n"}
{"snippet": "digits = datasets.load_digits()\n", "intent": "We will use the digits dataset from [scikit-learn](http://scikit-learn.org/stable/).\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\nprint('Train size: {}'.format(len(X_train)))\nprint('Test size: {}'.format(len(X_test)))\n", "intent": "In order to be able to measure the performance of an estimator, we need to split the data into train and test data sets. Shuffling is not necessary.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_all = data[['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit','temp', 'atemp', 'hum', 'windspeed']]\ny_all = data['cnt']\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, \n    y_all,\n    random_state=1,\n    test_size=0.2)\nprint('Train size: {}'.format(len(X_train)))\nprint('Test size: {}'.format(len(X_test)))\n", "intent": "Prepare train and test data sets.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_all = data[['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit','temp', 'atemp', 'hum', 'windspeed', 'hist']]\ny_all = data['cnt']\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, \n    y_all,\n    random_state=1,\n    test_size=0.2)\nprint('Train size: {}'.format(len(X_train)))\nprint('Test size: {}'.format(len(X_test)))\n", "intent": "Prepare train and test data sets.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "Standardize the features\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_all = data[['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit','temp', 'atemp', 'hum', 'windspeed', 'hist']]\ny_all = data['cnt']\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, \n    y_all,\n    random_state=1,\n    test_size=0.2)\nprint('Train size: {}'.format(len(X_train)))\nprint('Test size: {}'.format(len(X_test)))\n", "intent": "Prepare train and test data sets.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\ntrain_shape = X_train.shape\ntest_shape = X_test.shape\nscaler = StandardScaler()\nscaler.fit(X_train.reshape(train_shape[0]*train_shape[1], train_shape[2]))\nX_train = scaler.transform(X_train.reshape(train_shape[0]*train_shape[1], train_shape[2])).reshape(train_shape)\nX_test = scaler.transform(X_test.reshape(test_shape[0]*test_shape[1], test_shape[2])).reshape(test_shape)\n", "intent": "Standardize the features\n"}
{"snippet": "df = pd.DataFrame({'ID':[100, 101, 102, 103], 'quality':['good', 'very good', 'good', 'excellent']})\ndf\n", "intent": "The **category** data type should only be used with a string Series that has a **small number of possible values**.\n"}
{"snippet": "import lda\nvect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \nsentences_train = vect.fit_transform(sentences)\n", "intent": "What:  Way of automatically discovering topics from sentences\nWhy:   Much quicker than manually creating and identifying topic clusters\n"}
{"snippet": "import pandas\nwheel = pandas.read_csv('wheel.csv')\n", "intent": "The data is in wheel.csv.\nWe would like to understand the relationship between _seconds_ and _signal_\n"}
{"snippet": "confusion_matrix = np.array(confusion_matrix(test_y, Y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(confusion_matrix, index=['over_200k', 'under_200k'],\n                         columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "fare_pipe = make_pipeline(ColumnSelector('Fare'),\n                          StandardScaler())\nfare_pipe.fit_transform(df.head())\n", "intent": "The `Fare` attribute can be scaled using one of the scalers from the preprocessing module. \n"}
{"snippet": "union = make_union(age_pipe,\n                   one_hot_pipe,\n                   gender_pipe,\n                   fare_pipe)\nunion.fit_transform(df.head())\n", "intent": "Use the `make_union` function from the `sklearn.pipeline` module to combine all the pipes you have created.\n"}
{"snippet": "df = pd.DataFrame(data=adult, columns=['workclass', 'education-num', 'hours-per-week', 'income'])\ndf.head(n=5)\n", "intent": "Convert the data to a Pandas dataframe\n"}
{"snippet": "df_new = pd.DataFrame(data=df, columns=['workclass_num', 'education-num', 'hours-per-week', 'income_num'])\ndf_new2 = pd.DataFrame(data=df, columns=['workclass_num', 'education-num'])\ndf_new.head(n=5)\n", "intent": "Create a New Dataframe with just numerical data\n"}
{"snippet": "iris = pd.read_csv('../../assets/datasets/iris.csv')\n", "intent": "First, let's import our data. We'll be using *Iris* again for this exercise. \n"}
{"snippet": "x_standard = StandardScaler().fit_transform(X)\n", "intent": "First, let's standardize the data\n"}
{"snippet": "test = pd.read_csv('http://bit.ly/kaggletest')\ntest.head()\n", "intent": "**Video series:** [Introduction to machine learning with scikit-learn](https://www.youtube.com/playlist?list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A)\n"}
{"snippet": "iris = pd.read_csv('iris.csv')\n", "intent": "First, let's import our data. We'll be using *Iris* again for this exercise. \n"}
{"snippet": "PCA_set = PCA(n_components=5)\nY = PCA_set.fit_transform(xStand)\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "votes = pd.read_csv('../assets/datasets/votes.csv', index_col=0)\nvotes.fillna('n', inplace=True)\nvotes\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "X_standard = StandardScaler().fit_transform(X)\n", "intent": "Next, create the covariance matrix from the standardized x-values and decompose these values to find the eigenvalues and eigenvectors\n"}
{"snippet": "pcask = PCA(n_components=5)\ny_sk = pcask.fit_transform(X_standard)\ny_sk\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "airports = pd.read_csv('../../assets/datasets/airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "xStand = StandardScaler().fit_transform(x)\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "Ydf = pd.DataFrame(Y, columns=[\"PC1\", \"PC2\"])\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "airports = pd.read_csv('../../assets/datasets/airport_operations.csv')\nairports.head()\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "pd.DataFrame({'PassengerId':test.PassengerId, 'Survived':new_pred_class}).set_index('PassengerId').head()\n", "intent": "Documentation for the [**`DataFrame`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) constructor\n"}
{"snippet": "airports = pd.read_csv('airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "We're going to load the iris data from the scikit \"datasets\" package\n"}
{"snippet": "X, y = load_iris().data, load_iris().target\n", "intent": "Define your \"X\" and \"y\" variables for the analysis\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_stand, y, test_size=0.3, random_state = 42)\n", "intent": "Split the data in the ordinary way, making sure you have a 70/30 split.\n"}
{"snippet": "the_normalizer(v_merge_re['comments'][5])\n", "intent": "Test out your normalizer on a comment to ensure it's working. \n"}
{"snippet": "feature_set = []\nlen1 = v_merge_re[\"comments\"].size\nfor i in xrange(0,len1):\n    feature_set.append(the_normalizer(v_merge_re['comments'][i]))\n", "intent": "Use the normalizer that you created to build a 'stripped' down comments array/vector/list (however you did it)\n"}
{"snippet": "normalizer(comment)\n", "intent": "Test out your normalizer on a comment to ensure it's working. \n"}
{"snippet": "iris_train_1_2 = iris_train.loc[iris_train['Species'].isin(['versicolor','virginica'])]\nx_1_2 = iris_train_1_2.iloc[:,:4]\ny_1_2 = iris_train_1_2.iloc[:,4]\nx_1_2 = StandardScaler().fit_transform(x_1_2)\n", "intent": "**Fitting Fisher projection by discriminating classes 1 and 2**\n"}
{"snippet": "x_test = StandardScaler().fit_transform(iris_test.iloc[:,:4])\ny_test_3_4 = iris_test.apply(lambda row : return_class(row),axis=1)\n", "intent": "**Projecting test data to above two projections**\n"}
{"snippet": "ufo = pd.read_csv('http://bit.ly/uforeports')\nufo.head()\n", "intent": "**Question:** What is the difference between **`ufo.isnull()`** and **`pd.isnull(ufo)`**?\n"}
{"snippet": "np.sum(img_to_array(img0)* img_to_array(img0_))\n", "intent": "- Compute these quantities to verify your answer\n"}
{"snippet": "import numpy as np\nfrom keras.datasets import mnist\nfrom keras.preprocessing.image import array_to_img, img_to_array\nimport keras as keras\n[X, y], _ = mnist.load_data()\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nX = np.expand_dims(X, axis=1).astype(np.float)\nimgs = [array_to_img(x, data_format='channels_first') for x in X]\nX = np.array([img_to_array(img, data_format='channels_last') for img in imgs]) / 255. \ny = np.array([img_to_array(img, data_format='channels_last') for img in imgs]) / 255\n", "intent": "Since tensorflow expects images with shape `(height, width, channels)` we need to convert the dimensions of the MNIST images.\n"}
{"snippet": "import numpy as np\nfrom keras.datasets import mnist\nfrom keras.preprocessing.image import array_to_img, img_to_array\n[X_mnist, y], _ = mnist.load_data()\nX_mnist = X_mnist.astype(np.float) / 255.\nX = np.expand_dims(X_mnist, axis=-1)\nX.shape\n", "intent": "Since tensorflow expects images with shape `(height, width, channels)` we need to convert the dimensions of the MNIST images.\n"}
{"snippet": "import pandas as pd\nY = np.zeros([nb_train, 10])\nfor i, (d1, d2) in enumerate(y):\n    Y[[i, i], [d1, d2]] = 1\nX_train, Y_train = X[:-nb_train//10], Y[:-nb_train//10]\nX_val, y_val = X[-nb_train//10:], y[-nb_train//10:]\nhistory = model.fit(X_train, Y_train, callbacks=[ThreeAccuracyCallback(X_val, y_val)])\npd.DataFrame(history.history).plot();\n", "intent": "The following code fits a multi-label logistic regression model to the multi-digit MNIST images and uses an `AccuracyCallback` for evaluation.\n"}
{"snippet": "import numpy as np\nfrom keras.datasets import mnist\nfrom keras.preprocessing.image import array_to_img, img_to_array\n[X, y], _ = mnist.load_data()\nX = np.expand_dims(X, axis=1).astype(np.float)\nimgs = [array_to_img(x, data_format='channels_first') for x in X]\nX = np.array([img_to_array(img, data_format='channels_last') for img in imgs]) / 255.\n", "intent": "Since tensorflow expects images with shape `(height, width, channels)` we need to convert the dimensions of the MNIST images.\n"}
{"snippet": "data_raw = pd.read_csv('data/adult.data.txt')\ndata_raw.head(20)\n", "intent": "Let us import and look at the data.\n"}
{"snippet": "display(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2']))\n", "intent": "Let's now see how the log-transformed sample data has changed after having a PCA transformation applied to it using only two dimensions.\n"}
{"snippet": "from sklearn import cross_validation\nfrom sklearn.linear_model import LinearRegression\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size = 0.2)\nregr = LinearRegression()\nregr.fit(X_train, y_train)\naccuracy = regr.score(X_test, y_test)\naccuracy\n", "intent": "Here we will partition up the training and testing sets as well as fit our model.\n"}
{"snippet": "count_vectorizer_100 = CountVectorizer(max_features=100, stop_words='english')\n", "intent": "Okay, it's **far** too big to even look at. Let's try to get a list of features from a new `CountVectorizer` that only takes the top 100 words.\n"}
{"snippet": "train = pd.read_csv('http://bit.ly/kaggletrain')\ntrain.head()\n", "intent": "Documentation for [**`concat`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html)\n"}
{"snippet": "idf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\nTop_100_tokens_idf = idf_vectorizer.fit_transform(All_speeches)\nidf_df = pd.DataFrame(Top_100_tokens_idf.toarray(), columns=idf_vectorizer.get_feature_names())\nidf_df['china trade'] = idf_df['china'] + idf_df['trade']\n", "intent": "Now what if I'm using a `TfidfVectorizer`?\n"}
{"snippet": "meta = pd.read_csv('./data_v_7_stc/meta/meta.txt', sep='\\t', header=None, names=['name', 'class'], usecols=[0, 4], index_col=0)\nle = LabelEncoder()\nmeta['class'] = le.fit_transform(meta['class'])\nmeta.head()\n", "intent": "Load train database\n"}
{"snippet": "training_set = pd.read_csv('./Boltzmann_Machines/ml-100k/u1.base',\n                           delimiter='\\t', names=['UserID','MovieID','Rating','Timestamp'])\nprint training_set.shape\ntraining_set.head()\n", "intent": "* from 100k dataset\n"}
{"snippet": "training_set = pd.read_csv('./ml-100k/u1.base',\n                           delimiter='\\t', names=['UserID','MovieID','Rating','Timestamp'])\nprint training_set.shape\ntraining_set.head()\n", "intent": "* from 100k dataset\n"}
{"snippet": "df = pd.read_csv('predict/bidder1_gender_predict.csv')\nprint df.shape\ndone1 = df['file']\ndf2 = pd.read_csv('predict/bidder1_gender_diff_predict_1.csv')\nprint df2.shape\ndone2 = df2['file']\nshould = pd.read_fwf('bidder1.txt',names =['url'])\nwanted=should['url']\ndiff = list(set(wanted)-set(done1)-set(done2))\nlen(diff)\n", "intent": "CPU times: user 2min 18s, sys: 17 s, total: 2min 35s\nWall time: 1h 34min 33s\n"}
{"snippet": "df = pd.read_csv('bidder1_gender_predict.csv')\ndf.shape\n", "intent": "CPU times: user 2min 18s, sys: 17 s, total: 2min 35s\nWall time: 1h 34min 33s\n"}
{"snippet": "df = pd.read_csv('boss1_gender_predict.csv')\ndf.shape\n", "intent": "CPU times: user 2min 18s, sys: 17 s, total: 2min 35s\nWall time: 1h 34min 33s\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(df, mark, test_size=0.25, random_state=0)\ndim(X_train, X_test, y_train, y_test)\n", "intent": "* train on 75% of and test on the remaining 25% of the data\n"}
{"snippet": "cols = ['f1', 'f2', 'f3', 'f4', 'species']\ndata = pd.read_csv('../data/iris.csv', names=cols)\n", "intent": "This portion of the notebook is a carry over from the \"example1_\" notebook.\n"}
{"snippet": "pd.DataFrame([[100, 'red'], [101, 'blue'], [102, 'red']], columns=['id', 'color'])\n", "intent": "Documentation for [**`DataFrame`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n"}
{"snippet": "f = z.open('ml-1m/movies.dat')\ntmp = f.read()\ntmp = tmp.decode('latin')\nmovies = np.array([np.array(t.split('::')) for t in tmp.splitlines()])\nf.close()\n", "intent": "Movies' information\n* MovieID, Title, Genres\n"}
{"snippet": "f = z.open('ml-1m/ratings.dat')\ntmp = f.read()\ntmp = tmp.decode()\nratings = np.array([np.array(t.split('::')) for t in tmp.splitlines()])\nratings = ratings.astype(np.float)\nratings[:, 2] /= 5\nf.close()\n", "intent": "Ratings' information\n* UserID, MovieID, Rating, Timestamp\n"}
{"snippet": "print('\nnow = datetime.now()\noof_result = pd.DataFrame(avreal, columns=['target'])\noof_result['prediction'] = avpred\noof_result['id'] = avids\noof_result.sort_values('id', ascending=True, inplace=True)\noof_result = oof_result.set_index('id')\nsub_file = 'train_5fold-keras-run-01-v1-oof_' + str(score) + '_' + str(now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\nprint('\\n Writing out-of-fold file:  %s' % sub_file)\noof_result.to_csv(sub_file, index=True, index_label='id')\n", "intent": "Save the file with out-of-fold predictions. For easier book-keeping, file names have the out-of-fold gini score and are are tagged by date and time.\n"}
{"snippet": "result = pd.DataFrame(mpred, columns=['target'])\nresult['id'] = te_ids\nresult = result.set_index('id')\nprint('\\n First 10 lines of your 5-fold average prediction:\\n')\nprint(result.head(10))\nsub_file = 'submission_5fold-average-keras-run-01-v1_' + str(score) + '_' + str(now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\nprint('\\n Writing submission:  %s' % sub_file)\nresult.to_csv(sub_file, index=True, index_label='id')\n", "intent": "Save the final prediction. This is the one to submit.\n"}
{"snippet": "data = pd.read_csv('../../data/aquastat/aquastat.csv.gzip', compression='gzip')\ndata.region = data.region.apply(lambda x: simple_regions[x])\ndata = data.loc[~data.variable.str.contains('exploitable'),:]\ndata = data.loc[~(data.variable=='national_rainfall_index')]\n", "intent": "http://www.fao.org/nr/water/aquastat/data/query/index.html\n"}
{"snippet": "from sklearn.datasets import load_digits\nimport seaborn as sns\ndataset = load_digits()\nX, y = dataset.data, dataset.target\n", "intent": "What are the difficult digits to see?\n"}
{"snippet": "x = df.CRIM.values.reshape(-1,1)\ny = y\nX_train, X_test, y_train, y_test = train_test_split(x, y)\n", "intent": "- `train_test_split`\n- `mean_squared_error`\n- `cross_val_score`\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\ndf = pd.read_csv('data/titanic.csv')\ninclude = ['cclass', 'sex', 'age', 'fare', 'sibsp', 'survived']\ndf['sex'] = df['sex'].apply(lambda x: 0 if x == 'male' else 1)\ndf = df[include].dropna()\nX = df[['pclass', 'sex', 'age', 'fare', 'sibsp']]\ny = df['survived']\nPREDICTOR = RandomForestClassifier(n_estimators=100).fit(X, y)\n", "intent": "Our classifier algorithm will be a random forest, which as you know is relatively slow to train.\n"}
{"snippet": "import pandas as pd\nyelp = pd.read_csv('../data/yelp.csv')\nyelp.head(3)\n", "intent": "Read `yelp.csv` into a DataFrame.\n"}
{"snippet": "for num_feats in np.arange(1,65, dtype = int):\n    pca = PCA(n_components = num_feats)\n    pca.fit(digits.data)\n    variance_explained = sum(pca.explained_variance_ratio_)\n    if variance_explained >= .90:\n        break\nprint(\"{:d} features are needed to explain 90% of the variance\".format(num_feats))\n", "intent": "**Problem 2e** How many components are needed to explain 90% of the variance in the data.\n"}
{"snippet": "training_data, validation_data, test_data = load_data()\n", "intent": "Let's see how the data looks:\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n", "intent": "Let's load the MNIST dataset from `keras.datasets`. The download may take a few minutes.\n"}
{"snippet": "from sklearn.feature_selection import RFE\nclf_rf_3 = RandomForestClassifier()      \nrfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)\nrfe = rfe.fit(x_train, y_train)\n", "intent": "Like previous method, we will use 5 features. However, which 5 features will we use ? We will choose them with RFE method.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=27)\npca.fit(X_train)\n", "intent": "There are 784 features, it will be difficult and time consuming task to reduce the features using p-value and VIF score.\nSo let's use PCA to do that.\n"}
{"snippet": "pcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':X_colnames})\npcs_df.head()\n", "intent": "- We'll plot original features on the first 2 principal components as axes\n"}
{"snippet": "del X\nX= train[:6000].values\ndel train\nX_std = StandardScaler().fit_transform(X)\npca = PCA(n_components=5)\npca.fit(X_std)\nX_5d = pca.transform(X_std)\nTarget = target[:6000]\n", "intent": "Now using the Sklearn toolkit, we implement the Principal Component Analysis algorithm as follows:\n"}
{"snippet": "pca = PCA(svd_solver='randomized', random_state=27)\npca.fit(X_train)\n", "intent": "There are 154 features, it will be difficult and time consuming task to reduce the features using p-value and VIF score.\nSo let's use PCA to do that.\n"}
{"snippet": "pca_final = IncrementalPCA(n_components=19)\nX_train_pca = pca_final.fit_transform(X_train)\nX_test_pca = pca_final.transform(X_test)\nX_train_pca.shape\n", "intent": "- We'll choose 19 components for our modeling\nActual Dimensions 89, after PCA, 19\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(twenty_train.data)\nX_train_counts.shape\n", "intent": "CountVectorizer segment each text file into words (for English splitting by space), and count \n"}
{"snippet": "X_pca_train = pca.fit_transform(X_train)\nX_pca_test = pca.transform(X_test)\nRFmod = RandomForestClassifier()\npars = {\"n_estimators\": [10, 100, 300],\n        \"max_features\": [1, 2], \n        \"min_samples_leaf\": [1,10]}\ngrid_results = GridSearchCV(RandomForestClassifier(), \n                            pars,\n                            cv = 5)\ngrid_results.fit(X_pca_train, y_train)\n", "intent": "**Exercise 5**: Re-do the classification on the PCA components instead of the original features.\n"}
{"snippet": "scaler_a = p.StandardScaler()\nscaler_c = p.StandardScaler()\nscaler_a.fit(azdias_10)\nscaler_c.fit(customers_10)\nazdias_ss = pd.DataFrame(scaler_a.transform(azdias_10), columns = azdias_10.columns.values)\ncustomers_ss = pd.DataFrame(scaler_c.transform(customers_10), columns = customers_10.columns.values)\n", "intent": "Use StandardScaler module scales each feature to mean 0 and standard deviation 1.\n"}
{"snippet": "var_list = list()\ncomp_list= list()\nfor _ in [100, 150, 200, 250, 300, 350, 400, 450,  500, 550, 600, 700, 800, 860]:\n    pca = PCA(_)\n    X_pca = pca.fit_transform(customers_ss)\n    var = np.sum(pca.explained_variance_ratio_)\n    var_list.append(var)\n    comp_list.append(_)\n", "intent": "Main idea is to find balance between reduced number of features and remaining variance.\n"}
{"snippet": "centers = pca_a.inverse_transform(kmeans.cluster_centers_)\ndf_temp = pd.DataFrame(centers[8,:])\ndf_temp.columns= [\"Weight\"]\ndf_temp[\"Column_name\"] = azdias_ss.columns.values\ndf_temp.nlargest(10, 'Weight')\n", "intent": "This cluster indicates that customers of a mail-order sales company are:\n* `LP_LEBENSPHASE_FEIN_income_high` wealthy people,\n*\n"}
{"snippet": "centers = pca_a.inverse_transform(kmeans.cluster_centers_)\ndf_temp = pd.DataFrame(centers[14,:])\ndf_temp.columns= [\"Weight\"]\ndf_temp[\"Column_name\"] = azdias_ss.columns.values\ndf_temp.nlargest(10, 'Weight')\n", "intent": "This cluster indicates that customers of a mail-order sales company are:\n* \n"}
{"snippet": "df_attr = pd.read_csv('info_attributes_processed.csv', sep=',', low_memory=False, index_col=0)\ndf_attr.head()\n", "intent": "Helper DataFrame with attribute descriptions\n"}
{"snippet": "ord_col_names = [_[0] for _ in pd.read_csv(\"ordinal_names.csv\", header=None, index_col=0).values]\nord_col_names = set(ord_col_names)\n", "intent": "Intentify columns with ordinal data using information from part 1.\n"}
{"snippet": "bi_cat_col_names = [_[0] for _ in pd.read_csv(\"binary_categorical_names.csv\", header=None, index_col=0).values]\nbi_cat_col_names = set(bi_cat_col_names)\nbi_cat_col_names.remove(\"KBA05_SEG6\")\n", "intent": "First encode binary categorical data.\n"}
{"snippet": "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\ntrain_values = imp.fit_transform(X_train)\ntest_values = imp.fit_transform(X_test)\n", "intent": "Because less than 1% of data is missing, I decided to simply impute missing values with averages for each column with missing values.\n"}
{"snippet": "scaler = p.StandardScaler()\nscaler.fit(X_train)\nX_train_ss = pd.DataFrame(scaler.transform(X_train), columns = col_order)\nX_test_ss = pd.DataFrame(scaler.transform(X_test), columns = col_order)\n", "intent": "Use StandardScaler module scales each feature to mean 0 and standard deviation 1.\n"}
{"snippet": "from sklearn.pipeline import FeatureUnion\ntransformers = [(\"pca\", PCA(n_components=2)),\n                (\"pt\", PSFMagThreshold(p=1.45))]\nfeat_union = FeatureUnion(transformers)\nX_transformed = feat_union.fit_transform(X_train)\n", "intent": "Now let's make a feature set that combines this feature with the PCA features:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_col_names = ['num_preg', 'glucose_conc', 'diastolic_bp', 'thickness', 'insulin', 'bmi', 'diab_pred', 'age']\npredicted_class_names = ['diabetes']\nX = df[feature_col_names].values     \ny = df[predicted_class_names].values \nsplit_test_size = 0.30\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_test_size, random_state=42) \n", "intent": "70% for training, 30% for testing\n"}
{"snippet": "df_mm = p.MinMaxScaler().fit_transform(df)\n", "intent": "`3.` Now it's your turn.  Try fitting the **MinMaxScaler** transformation to this dataset. You should be able to use the previous example to assist.\n"}
{"snippet": "X = tfidf_vect.fit_transform(dta.violations)\n", "intent": "Notice here that we can combine the fitting and the transformation by taking advantage of the `fit_transform` method.\n"}
{"snippet": "movie_df = pd.read_csv('imdb.csv', delimiter='\\t')\nmovie_df.head()\n", "intent": "Import the IMDb data.\n"}
{"snippet": "df = pd.DataFrame(X_r, columns=['PC1', 'PC2'])\ndf.head()\ndf['species'] = Y\ndf.head()\n", "intent": "Now we can assemble the two components and the `species` column into a DataFrame.\n"}
{"snippet": "n_components=6\npca = PCA(n_components=n_components)\n", "intent": "Set the number of components to 6:\n"}
{"snippet": "(input_train, input_test, labels_train, labels_test) = train_test_split(input_, labels_, test_size=0.1)\n", "intent": "Now, let us split our data into train and test set.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\ndata.head()\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"/Users/christyhe/Documents/Data_Scientist/DataScience-Python3/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.ensemble import RandomForestClassifier\niris = datasets.load_iris()\nRFclf = RandomForestClassifier().fit(iris.data, iris.target)\n", "intent": "To test the slides on your local machine you need to run the following: \n    ipython nbconvert Session2FinalProject.ipynb --to slides --post serve\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(kick[numerical_columns], response, test_size = 0.3, random_state=465)\n", "intent": "Train test split after only considering the columns extracted above\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(kick[numerical_columns],\n                                                    response, random_state=143)\n", "intent": "Train test split after only considering the columns extracted above\n"}
{"snippet": "titanic_df = pd.read_csv('/Users/avkashchauhan/learn/seattle-workshop/titanic_list.csv')\n", "intent": "http://www.titanicfacts.net/titanic-passengers.html\nTotal Passangers: 1317\nDetails:\nhttps://blog.socialcops.com/engineering/machine-learning-python/\n"}
{"snippet": "titanic_df = pd.read_csv('/Users/avkashchauhan/learn/comcast/titanic_list.csv')\n", "intent": "http://www.titanicfacts.net/titanic-passengers.html\nTotal Passangers: 1317\nDetails:\nhttps://blog.socialcops.com/engineering/machine-learning-python/\n"}
{"snippet": "data = pd.read_csv('train.csv')\nX = data[['X']].as_matrix()\ny = data['y'].as_matrix()\nX.shape, y.shape\n", "intent": "Let's load in the data:\n"}
{"snippet": "df=pd.read_csv(\"religion.csv\")\ndf.head()\n", "intent": "Let us assume that we have a \"population\" of 200 counties $x$:\n"}
{"snippet": "def make_features(train_set, test_set, degrees):\n    traintestlist=[]\n    for d in degrees:\n        traintestdict={}\n        traintestdict['train'] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n        traintestdict['test'] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n        traintestlist.append(traintestdict)\n    return traintestlist\n", "intent": "Lets put this alltogether. Below we create multiple datasets, one for each polynomial degree:\n"}
{"snippet": "CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\nFEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS)-1]\nTARGET = CSV_COLUMNS[0]\ndf_train = pd.read_csv('./taxi-train.csv', header=None, names=CSV_COLUMNS)\ndf_valid = pd.read_csv('./taxi-valid.csv', header=None, names=CSV_COLUMNS)\n", "intent": "Read data created in the previous chapter.\n"}
{"snippet": "advertising = pd.read_csv('Data/Advertising.csv', usecols=[1,2,3,4])\nadvertising.info()\n", "intent": "Datasets available on http://www-bcf.usc.edu/~gareth/ISL/data.html\n"}
{"snippet": "train_df = pd.read_csv(\"training_sources.csv\")\nX_train = np.array(train_df[[\"mean\", \"nobs\", \"duration\"]])\ny_train = np.array(train_df[\"Class\"])\n", "intent": "**Problem 2a**\nRead in the training set file, and create a feature vector `X` and label array `y`.\n"}
{"snippet": "print(X_reduced.shape)\npd.DataFrame(X_reduced).loc[:4,:5]\n", "intent": "The above loadings are the same as in R.\n"}
{"snippet": "df = pd.read_csv('Data/Wage.csv')\ndf.head(3)\n", "intent": "Using write.csv in R, I exported the dataset from package 'ISLR' to a csv file.\n"}
{"snippet": "X1 = PolynomialFeatures(1).fit_transform(df.age.values.reshape(-1,1))\nX2 = PolynomialFeatures(2).fit_transform(df.age.values.reshape(-1,1))\nX3 = PolynomialFeatures(3).fit_transform(df.age.values.reshape(-1,1))\nX4 = PolynomialFeatures(4).fit_transform(df.age.values.reshape(-1,1))\nX5 = PolynomialFeatures(5).fit_transform(df.age.values.reshape(-1,1))\ny = (df.wage > 250).map({False:0, True:1}).values\nprint('X4:\\n', X4[:5])\nprint('y:\\n', y[:5])\n", "intent": "Create polynomials for 'age'. These correspond to those in R, when using raw=TRUE in poly() function.\n"}
{"snippet": "df = pd.read_csv('Data/Hitters.csv').dropna()\ndf.info()\n", "intent": "In R, I exported the dataset from package 'ISLR' to a csv file.\n"}
{"snippet": "df2 = pd.read_csv('Data/Heart.csv').drop('Unnamed: 0', axis=1).dropna()\ndf2.info()\n", "intent": "Dataset available on http://www-bcf.usc.edu/~gareth/ISL/data.html\n"}
{"snippet": "df3 = pd.read_csv('Data/Carseats.csv').drop('Unnamed: 0', axis=1)\ndf3.head()\n", "intent": "In R, I exported the dataset from package 'ISLR' to a csv file.\n"}
{"snippet": "boston_df = pd.read_csv('Data/Boston.csv')\nboston_df.info()\n", "intent": "In R, I exported the dataset from package 'MASS' to a csv file.\n"}
{"snippet": "X_train = pd.read_csv('Data/Khan_xtrain.csv').drop('Unnamed: 0', axis=1)\ny_train = pd.read_csv('Data/Khan_ytrain.csv').drop('Unnamed: 0', axis=1).as_matrix().ravel()\nX_test = pd.read_csv('Data/Khan_xtest.csv').drop('Unnamed: 0', axis=1)\ny_test = pd.read_csv('Data/Khan_ytest.csv').drop('Unnamed: 0', axis=1).as_matrix().ravel()\n", "intent": "In R, I exported the dataset from package 'ISLR' to csv files.\n"}
{"snippet": "labels = pd.read_csv(\"b37d3960-6909-472b-9ce1-c33b07dbdb66.csv\")\nids = labels.id.tolist()\nlabels = labels.genus.tolist()\nfilenames = [\"%s.jpg\" % id for id in ids]\nX = load_bee_images(\"data/train\", filenames)\nX = prep_images(X)\n", "intent": "Load images and labels for model training\n"}
{"snippet": "test_df = pd.read_csv(\"test_sources.csv\")\nX_test = np.array(test_df[[\"mean\", \"nobs\", \"duration\"]])\ny_test = np.array(test_df[\"Class\"])\n", "intent": "You can load the test set using the commands below.\n"}
{"snippet": "def convert_timedelta_to_float(df, feature):\n    lst = []\n    for i in df[feature]:\n        lst.append(i.total_seconds())\n    new_feature = pd.DataFrame(lst, index=df.index).rename(columns={0:feature})\n    return new_feature\nfor col in tdeltas:\n    df[col] = convert_timedelta_to_float(df, col)\ndf.head()\n", "intent": "First I'll need to convert the timedeltas to a float so I can further analyze the timestamp data.\n"}
{"snippet": "scaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df_2)\n", "intent": "We need to normalize data first\n"}
{"snippet": "PATH_TO_DATA = ('/Users/owner/data/project_alice')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "vcrz = CountVectorizer(ngram_range=(1,3), max_features=100000)\ncounts_train = vcrz.fit_transform(full_df['str0'][:idx_train].ravel(), y) \ncounts_test = vcrz.fit_transform(full_df['str0'][idx_train:].ravel(), y) \n", "intent": "Build Tf-Idf features based on sites. You can use `ngram_range`=(1, 3) and `max_features`=100000 or more\n"}
{"snippet": "pipeline = Pipeline([(\"vectorize\", TfidfVectorizer(ngram_range=(1, 3), max_features=100000)), (\"tfidf\", TfidfTransformer())])\npipeline.fit(full_df['str'][:idx].ravel(),y)\nX_train = pipeline.transform(full_df['str'][:idx].ravel())\nX_test = pipeline.transform(full_df['str'][idx:].ravel())\nfor C in range(3,6):\n    print(C, get_auc_lr_valid(X_train, y, C=C))\n", "intent": "Perform cross-validation with logistic regression.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nstandardScaler = StandardScaler()\nX = standardScaler.fit_transform(X[:,1:])  \ny = standardScaler.fit_transform(y)\n", "intent": "Whoa!  What happened??\nThis illustrates an issue with Gradient Descent.  Not scaling of variables can wreak havoc on the optimization\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nstandardScaler = StandardScaler()\nX = standardScaler.fit_transform(X[:,1:])\nX = np.matrix(X)\nX = sma.add_constant(X)\nlogistic_regression = GradientDescent(BCE, tolerance=1e-7)\nb = logistic_regression.optimize(X,g,learning_rate=10) \nprint(\"iterations: {}\".format(logistic_regression.iter))\nprint(logistic_regression.beta.T)\nBCE.valueAt(X,g,b)\n", "intent": "That's alot of iterations!  We can improve it by standardizing the features and playing with the learning rate\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nstandardScaler = StandardScaler()\nX = standardScaler.fit_transform(X[:,1:])  \ny = standardScaler.fit_transform(y)\n", "intent": "Estimate the regression with regularization\n"}
{"snippet": "X = standardScaler.fit_transform(X[:,1:])\nX = np.matrix(X)\nX = sma.add_constant(X)\nlogistic_regression = GradientDescent(BCE, tolerance=1e-7)\nb = logistic_regression.optimize(X,g,learning_rate=10) \nprint(\"iterations: {}\".format(logistic_regression.iter))\nprint(logistic_regression.beta.T)\nBCE.valueAt(X,g,b)\n", "intent": "That's alot of iterations!  We can improve it by standardizing the features and playing with the learning rate\n"}
{"snippet": "(x_train_temp, y_train_temp), (x_test_temp, y_test_temp) = mnist.load_data()\n", "intent": "(ooh look it's all stored on Amazon's AWS!)\n(pssst, we're in the cloooud, it's the future!)\n"}
{"snippet": "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.4,random_state=20)\n", "intent": "Before we build our model, lets generate train/test splits of our data:\n"}
{"snippet": "cement_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/slump_test.data\",index_col=0)\ncement_data.columns = cement_data.columns.str.lower().str.replace(\" \",\"_\").str.replace(\".\",\"\")\n", "intent": "Lets get the data directly off the web this time:\n"}
{"snippet": "special_missing_category = kidney_data[kidney_columns[14:-1]].fillna(\"missing\")\nspecial_missing_category.head()\n", "intent": "Now let's do **2.** (Which is much easier):\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target  \n", "intent": "- 50 samples of 3 different species of iris (150 samples total)\n- Measurements: sepal length, sepal width, petal length, petal width\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "- 50 samples of 3 different species of iris (150 samples total)\n- Measurements: sepal length, sepal width, petal length, petal width\n"}
{"snippet": "sc = StandardScaler()\nhousing_data_scaled  = housing_data.copy()\nhousing_data_scaled[housing_columns[:-1]]  = sc.fit_transform(housing_data[housing_columns[:-1]])\nprint(housing_data_scaled.shape)\nhousing_data_scaled.head()\n", "intent": "And normalizing it per-column:\n"}
{"snippet": "sc = StandardScaler()\nhousing_data_scaled  = housing_data.copy()\nhousing_data_scaled[housing_columns[:-1]]  = sc.fit_transform(housing_data[housing_columns[:-1]])\n", "intent": "And normalizing it per-column:\n"}
{"snippet": "vectorizer = CountVectorizer(ngram_range=(1,6))\ntransformed_text = vectorizer.fit_transform(some_text.text)\nprint(transformed_text)\n", "intent": "Think of it as large-scale one-hot encoding with some catches:\n"}
{"snippet": "vectorizer = CountVectorizer(ngram_range=(1,6))\ntransformed_text = vectorizer.fit_transform(some_text.text)\n", "intent": "Think of it as large-scale one-hot encoding with some catches:\n"}
{"snippet": "(x_train_temp, y_train_temp), (x_test_temp, y_test_temp) = mnist.load_data()\n", "intent": "(ooh look it's all stored on Amazon's AWS!)\n(pssst, we're in the cloooud)\n"}
{"snippet": "scaled = pd.DataFrame(scaled,columns=df.columns.values[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data = pd.read_csv('College_Data',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "final_zips = final_filtered.sort_values(\"zip\").zip.unique()\nfinal_kmeans_labels = final_kmeans.labels_\nfinal_dbscan_labels = final_dbscan.labels_\nto_merge = pd.DataFrame({\"ZIPCODE\": final_zips, \"kmeans\": final_kmeans_labels, \"dbscan\": final_dbscan_labels})\nzips_with_labels = zipcodes.merge(to_merge, on=\"ZIPCODE\").drop_duplicates(subset=[\"ZIPCODE\"])\nax = zips_with_labels.plot('kmeans', figsize=(16, 9), legend=True,categorical=True)\nax.get_xaxis().set_visible(False)\nax.get_yaxis().set_visible(False)\n", "intent": "    5. overlay your data on a NYC map: you can use shapefiles for the zip codes and different colors for different clusters\n"}
{"snippet": "final_zips = final_filtered.sort_values(\"zip\").zip.unique()\nfinal_kmeans_labels = final_kmeans.labels_\nfinal_dbscan_labels = final_dbscan.labels_\nto_merge = pd.DataFrame({\"ZIPCODE\": final_zips, \"kmeans\": final_kmeans_labels, \"dbscan\": final_dbscan_labels})\nzips_with_labels = zipcodes.merge(to_merge, on=\"ZIPCODE\").drop_duplicates(subset=[\"ZIPCODE\"])\nax = zips_with_labels.plot('kmeans', cmap='coolwarm', figsize=(16, 9), legend=True,categorical=True)\nax.get_xaxis().set_visible(False)\nax.get_yaxis().set_visible(False)\n", "intent": "    5. overlay your data on a NYC map: you can use shapefiles for the zip codes and different colors for different clusters\n"}
{"snippet": "dir_test = 'datasets/energy_efficiency_test.csv'\ndf_test = pd.read_csv(dir_test, index_col=False)\n", "intent": "Run the following to load the test set into a pandas dataframe.\n"}
{"snippet": "df_training = pd.read_csv(dir_training)\n", "intent": "First let's load the dataset into a pandas dataframe. [(Hint)](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)\n"}
{"snippet": "df = pandas.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "df_feat = pandas.DataFrame(scaled_features,columns = df.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "ad_data = pandas.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "from astropy.table import Table\nTable.read( \n", "intent": "**Problem 1b**\nUse [`Table.read()`](http://docs.astropy.org/en/stable/api/astropy.table.Table.html\n"}
{"snippet": "advert = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\nadvert.head()\n", "intent": "Let's compute these evaluation metrics for the linear model we have developed for our advertising dataset\n"}
{"snippet": "hr = pd.read_csv(\"data/HR_comma_sep.csv\")\nhr.head()\n", "intent": "We will revisit the HR dataset and see if we can use KNN to predict whether an employee is likely to leave the company\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"}
{"snippet": "of_df = pd.read_csv(\"data/old-faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "data = pd.read_csv(\"../../lesson-11/code/data/stumbleupon.tsv\", sep='\\t', encoding=\"utf-8\")\ndata['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\ndata['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n", "intent": "Importing the stumbleupon dataset from last week\n"}
{"snippet": "titles = data['title'].fillna('')\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features = 1000, \n                             ngram_range=(1, 2), \n                             stop_words='english',\n                             binary=False)\nvectorizer.fit(titles)\nX = vectorizer.transform(titles)\n", "intent": " We previously used the Count Vectorizer to extract text features for this classification task\n"}
{"snippet": "df = pd.read_csv(\"../assets/titanic.csv\")\n", "intent": "- Include visualizations, descriptive statistics, etc.\n"}
{"snippet": "scaler = preprocessing.StandardScaler()\n", "intent": "- Depending on the classification model we're using, we may want to scale our features (in my case, only Age)\n"}
{"snippet": "import pandas as pd\nfrom sklearn import datasets\niris = datasets.load_iris()\nprint(iris.data.shape)\nprint(iris.target.shape)\n", "intent": "<b>Shape and representation<b>\n"}
{"snippet": "sdss = pd.read_csv(\"DSFP_SDSS_spec_train.csv\")\nsdss[:5]\n", "intent": "**Problem 4a**\nDownload and read in the [training set](https://northwestern.box.com/s/sjegm0tx62l2i8dkzqw22s4gmq1a9sg1) for the model. \n"}
{"snippet": "with open('./tweets.pickle', 'rb') as f:\n    char2int, int2char = pickle.load(f)\njson_file = open('./architecture.trump.json', 'rt')\narchitecture = json_file.read()\njson_file.close()\nmodel2 = model_from_json(architecture)\nmodel2.load_weights('./weights.trump.h5')\n", "intent": "To load the model run the following:\n"}
{"snippet": "index_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n", "intent": "We now need to prepare the features. This part is all implemented for you.\n"}
{"snippet": "def query_to_df(session,query):\n    result = session.execute(query)\n    d = DataFrame(result.fetchall())\n    d.columns = result.keys()\n    return d\n", "intent": " Load data via sqlalchemy from the sql lite database forjar.db\n ===================================================================================\n"}
{"snippet": "pca_model = PCA(n_components=n_latent, whiten=True)\npsth_pca_latent = pca_model.fit_transform(psth)\n", "intent": "Below, we'll calculate the latent structure present in the data with a few algorithms.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42)\n", "intent": "Split into a training set and a test set using a stratified k fold\n"}
{"snippet": "import pandas as pd       \ntrain = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, \\\n                    delimiter=\"\\t\", quoting=3)\n", "intent": "from https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n"}
{"snippet": "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", header=None)\ndf.head()\n", "intent": "Read in Wisconsin Breast Cancer Dataset\n"}
{"snippet": "california_housing_dataframe = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep=\",\")\n", "intent": "Next, we'll load our data set.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\n    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n    header=None, \n    sep=',')\ndf.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\ndf.dropna(how=\"all\", inplace=True) \ndf.tail()\n", "intent": "In order to load the Iris data directly from the UCI repository, we are going to use the [pandas](http://pandas.pydata.org) library.\n"}
{"snippet": "from astropy.table import Table\nTable.read('irsa_catalog_WISE_iPTF14jg_search_results.tbl', format='ipac')\n", "intent": "**Problem 1b**\nUse [`Table.read()`](http://docs.astropy.org/en/stable/api/astropy.table.Table.html\n"}
{"snippet": "from sklearn import datasets\ndiabetes = datasets.load_diabetes()\nprostrate = pd.read_table(\"data/prostrate.data\").iloc[:,1:]\n", "intent": "We will be using a dataset of prostrate cancer occurances.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nimport matplotlib.pylab as pl\npl.gray();\n", "intent": "Import the dataset and use `pylab` to explore.\n"}
{"snippet": "pca = PCA(2)\n", "intent": "- Lower accuracy for k = 10\n"}
{"snippet": "X_train, X_test, y_train, y_test =train_test_split(reviews.data,reviews.target)\n", "intent": "What is the accuracy? \n<br>\n<details><summary>\nClick here for my accuracy...\n</summary>\n<br>\naccuracy = 82.00%  <br>\n</details>\n"}
{"snippet": "W = model.fit_transform(doc_term)\nH = model.components_\n", "intent": "Get the factors $\\text{W}$ and $\\text{H}$ from the resulting model.\n"}
{"snippet": "import pandas as pd\npath_data = '../../data/clean_json.json'\ndef load_json_data(path_to_file):\n    data_DF = pd.read_json(path_to_file,encoding='ascii')\n    data_DF['from'] = data_DF['from'].str.lower()\n    data_DF['body'] = data_DF['body'].apply(lambda x: \" \".join(str(x).split()))\n    return data_DF \n", "intent": "In the folder you will find a json file.\n"}
{"snippet": "import pandas as pd\ndef load_json_data(path_to_file):\n    data_DF = pd.read_json(path_to_file,encoding='ascii')\n    data_DF['from'] = data_DF['from'].str.lower()\n    data_DF['body'] = data_DF['body'].apply(lambda x: \" \".join(str(x).split()))\n    return data_DF\n", "intent": "In the folder you will find a json file.\n"}
{"snippet": "from scipy import misc, ndimage\nim = misc.imread(\"handwrite-small.png\")\nim = (255-im)/16.0\n", "intent": "<img src=\"handwrite-me.jpg\">\n<img src=\"handwrite-large.png\" width=\"50%\"> \n"}
{"snippet": "model = model_from_json(open('mnist_cnn.json').read())\nmodel.load_weights('mnist_cnn_weights.h5')\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "First we will load the MNIST model we trained beforehand.\n"}
{"snippet": "n = 10000\np = 2\nX, y = make_blobs(n_samples=n, n_features=p, centers=2, cluster_std=1.05, random_state=23)\nX = np.c_[np.ones(len(X)), X]\ny = y.astype('float')\n", "intent": "**Data set for classification**\n"}
{"snippet": "import pandas as pd\nimport numpy as np\npath = path='https://ibm.box.com/shared/static/q6iiqb1pd7wo8r3q28jvgsrprzezjqk3.csv'\ndf = pd.read_csv(path)\n", "intent": "<p></p>\n<li><a href=\"\n<li><a href=\"\n<li><a href=\"\n<li><a href=\"\n<p></p>\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=1)\nprint(\"number of test samples :\", x_test.shape[0])\nprint(\"number of training samples:\",x_train.shape[0])\n", "intent": " now we randomly split our data into training and testing data  using the function **train_test_split** \n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)\nprint(\"done\")\n", "intent": "Let's use 55 percent of the data for testing and the rest for training:\n"}
{"snippet": "pr=PolynomialFeatures(degree=5)\nx_train_pr=pr.fit_transform(x_train[['horsepower']])\nx_test_pr=pr.fit_transform(x_test[['horsepower']])\npr\n", "intent": "We will perform a degree 5 polynomial transformation on the feature **'horse power'**. \n"}
{"snippet": "def f(order,test_data):\n    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)\n    pr=PolynomialFeatures(degree=order)\n    x_train_pr=pr.fit_transform(x_train[['horsepower']])\n    x_test_pr=pr.fit_transform(x_test[['horsepower']])\n    poly=LinearRegression()\n    poly.fit(x_train_pr,y_train)\n    PollyPlot(x_train[['horsepower']],x_test[['horsepower']],y_train,y_test,poly,pr)\n", "intent": " The following function will be used in the next section; please run the cell.\n"}
{"snippet": "pr=PolynomialFeatures(degree=2)\nx_train_pr=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\nx_test_pr=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\n", "intent": " Let's perform a degree two polynomial transformation on our data. \n"}
{"snippet": "log_centers = pca.inverse_transform(centers)\ntrue_centers = scaler.inverse_transform(log_centers)\ntrue_centers = np.exp(true_centers)\nsegments = ['Segment {}'.format(i) for i in range(0,len(centers))]\ntrue_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())\ntrue_centers.index = segments\ndisplay(true_centers)\n", "intent": "\"If you do this, you will need to apply the following to transform your data back in Implementation: Data Recovery.\"\n"}
{"snippet": "subm = np.column_stack((np.asarray(test_data.index.values), np.asarray(test_data_pred, dtype=np.float32)))\nnp.savetxt('kaggle_submissions/sub_ridge_01.csv',subm, delimiter=',', comments='',  newline='\\n', fmt='%s', \n           header = 'id,trip_duration')\nsubmission = pd.read_csv('kaggle_submissions/sub_ridge_01.csv', index_col= 'id')\nprint(submission.shape)\nsubmission.head()\n", "intent": "Now we create the submission file.\n"}
{"snippet": "subm = np.column_stack((np.asarray(test_data.index.values), np.asarray(test_data_pred, dtype=np.float32)))\nnp.savetxt('kaggle_submissions/sub_rfr.csv',subm, delimiter=',', comments='',  newline='\\n', fmt='%s', \n           header = 'id,trip_duration')\nsubmission = pd.read_csv('kaggle_submissions/sub_rfr.csv', index_col= 'id')\nprint(submission.shape)\nsubmission.head()\n", "intent": "Now we create the submission file.\n"}
{"snippet": "standardizer = StandardScaler(withMean=True, withStd=True, \n                              inputCol='raw_features', \n                              outputCol='features')\nmodel = standardizer.fit(output)\noutput = model.transform(output)\n", "intent": "Scale features to have zero mean and unit standard deviation\n"}
{"snippet": "subm = np.column_stack((np.asarray(test_data.index.values), np.asarray(test_data_pred, dtype=np.float32)))\nnp.savetxt('kaggle_submissions/submission_grad_boost.csv',subm, delimiter=',', comments='',  newline='\\n', fmt='%s', \n           header = 'id,trip_duration')\nsubmission = pd.read_csv('kaggle_submissions/submission_grad_boost.csv', index_col= 'id')\nprint(submission.shape)\nsubmission.head()\n", "intent": "Creating the submission file.\n"}
{"snippet": "key_pts_frame = pd.read_csv('/data/training_frames_keypoints.csv')\nprint(key_pts_frame.shape)\nn = 5\nimage_name = key_pts_frame.iloc[n, 0]\nkey_pts = key_pts_frame.iloc[n, 1:].as_matrix()\nkey_pts = key_pts.astype('float').reshape(-1, 2)\nprint('Image name: ', image_name)\nprint('Landmarks shape: ', key_pts.shape)\nprint('First 4 key pts: {}'.format(key_pts[:4]))\n", "intent": "Then, let's load in our training data and display some stats about that data to make sure it's been loaded in correctly!\n"}
{"snippet": "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n", "intent": "The MNIST dataset contains thousands of grayscale images of handwritten digits.\n"}
{"snippet": "import helper\ndata_dir = './data/simpsons/moes_tavern_lines.txt'\ntext = helper.load_data(data_dir)\ntext = text[81:]\n", "intent": "Play around with `view_sentence_range` to view different parts of the data.\n"}
{"snippet": "boston = load_boston()\n", "intent": "Next we'll download the data set\n"}
{"snippet": "coeff_df = DataFrame(boston_df.columns)\ncoeff_df.columns = ['Features']\ncoeff_df[\"Coefficient Estimate\"] = pd.Series(lreg.coef_)\ncoeff_df\n", "intent": "What we'll do next is set up a DataFrame showing all the Features and their estimated coefficients obtained form the linear regression.\n"}
{"snippet": "df = pd.DataFrame()\ndf['ship_type'] = np.random.choice([\"romulan\", \"human\", \"klingon\", \"borg\", \"red_shirt\", \"ovid\"], size=500)\ndf['ship_value'] = np.random.randint(200000, 10000000, size=500)\ndf['ship_speed'] = np.random.randint(10, 60, size=500)\ndf['baths'] = np.random.choice(np.arange(1, 4, 0.5), size=500)\ny = df[['baths']]\nX = df[['ship_type','ship_value','ship_speed']]\n", "intent": "With 2-3 continious variables, and one categorical.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncolumns = ['ship_type','ship_value',]\nstand_columns = \"'stand_ship_type','stand_ship_value'\"\ndf['stand_ship_type','stand_ship_value'] = scaler.fit_transform(df[columns])\n", "intent": "More code will be written...\n"}
{"snippet": "my_alphas = []\nmy_score = []\nfor x in results.grid_scores_:\n    y = tuple(x)\n    my_score.append(y[1])\n    my_alphas.append(y[0]['alpha'])\nmy_df = pd.DataFrame(data=[my_alphas,my_score])\nmy_df = my_df.T\nmy_df.columns=[['Alphas','Score']]\nmy_df.plot(x= 'Alphas',y = 'Score' ,kind='scatter')\n", "intent": "Figure out how to plot the impact of your score given the range of parameters used.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\ndata = pd.read_csv(\"data.csv\") \nlabels = data['Dataset']  \ndata\n", "intent": "In this step, we will be analyzing the data given to us. It gives us an idea of what features are important to the determination of liver disease. \n"}
{"snippet": "cm = pd.DataFrame(confusion_matrix(y_test, predictions), columns=['predicted is cheap','predicted is expensive'],\n                 index = ['actual is cheap','actual is expensive'])\ncm\n", "intent": "What do these mean?\n"}
{"snippet": "iowa_file = '/Users/ryandunlap/Desktop/DSI-SF-2/datasets/iowa_liquor/Iowa_Liquor_sales_sample_10pct.csv'\niowa = pd.read_csv(iowa_file)\n", "intent": "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n---\n"}
{"snippet": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nairline_data = pd.read_csv('airline_data.csv', header=None, names=cols, low_memory=False)\nairline_data.fillna(airline_data.mean(axis=1))\nairline_data.drop[airline_data.index[4]]\n", "intent": "Other crash example\n"}
{"snippet": "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/titanic.csv'\ntitanic = pd.read_csv(url)\ntitanic.head()\n", "intent": "We'll deal with Titanic dataset again, this time with XGBClassifier\n"}
{"snippet": "import pandas as pd\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/titanic.csv'\ntitanic = pd.read_csv(url)\ntitanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\ntitanic.Age.fillna(titanic.Age.median(), inplace=True)\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\nembarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\ntitanic = pd.concat([titanic, embarked_dummies], axis=1)\ntitanic.head()\n", "intent": "We'll build a classification tree using the Titanic data:\n"}
{"snippet": "from sklearn.decomposition import PCA as sklearnPCA\nsklearn_pca = sklearnPCA(n_components=2)\nY_sklearn = sklearn_pca.fit_transform(X_std) \n", "intent": "For educational purposes, we went a long way to apply the PCA to the Iris dataset. But luckily, there is already implementation in scikit-learn. \n"}
{"snippet": "X, y = make_blobs(n_samples=10000, \n                  n_features=10, centers=100, random_state=0)\n", "intent": "X, y = make_blobs...\nfig = plt.figure(figsize=(10,8))\nax = fig.add_subplot(111)\nax.scatter...\n"}
{"snippet": "authorship = pd.read_csv(\"http://people.stern.nyu.edu/jsimonof/AnalCatData/Data/Comma_separated/authorship.csv\")\nprint authorship.columns\n", "intent": "- \"http://people.stern.nyu.edu/jsimonof/AnalCatData/Data/Comma_separated/authorship.csv\"\n- Print the columns, print df.head()\n"}
{"snippet": "le = preprocessing.LabelEncoder()\nle.fit(authors)\nauthorship['Author_num'] = le.transform(authorship['Author']) \nprint authorship['Author_num']\n", "intent": "Use the LabelEncoder to encode Authors to integers...\n1. What does the LabelEncoder do for us? \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val = train_test_split(new_data,labels,test_size=0.33,random_state=42)\n", "intent": "Q8. Create training and validation split on data. Check out train_test_split() function from sklearn to do this.\n"}
{"snippet": "import pandas as pd\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer\n", "intent": "1. K-means clustering\n2. Clustering evaluation\n3. DBSCAN clustering\n"}
{"snippet": "from sklearn.datasets import make_blobs\nnum_blobs = 8\nX, Y = make_blobs(centers=num_blobs, cluster_std=0.5, random_state=2)\n", "intent": "The $k$-Means Algorithm\n====================\nLet's start by generating some artificial blobs of data:\n"}
{"snippet": "X = pd.DataFrame(X, columns=df.drop('Class', axis=1).columns)\nX.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df = pd.read_csv('College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "mean_distances = np.apply_along_axis(np.mean, 1, dist_mtx)\ndf_valid_popularity = pd.DataFrame({'coupon_id_hash': coupons_valid_ids,\n    'popularity': 1-mean_distances})\ndf_valid_popularity.head()\n", "intent": "And finally the validation coupons \"popularity\", expressed as how similar are the validation coupons to the most popular coupons during training.\n"}
{"snippet": "coupons_train_feat_num = df_coupons_train_feat[num_cols].values\ncoupons_valid_feat_num = df_coupons_valid_feat[num_cols].values\nscaler = MinMaxScaler()\ncoupons_train_feat_num_norm = scaler.fit_transform(coupons_train_feat_num)\ncoupons_valid_feat_num_norm = scaler.transform(coupons_valid_feat_num)\n", "intent": "And the numeric ones\n"}
{"snippet": "ncomp = 50\nnmf_model = NMF(n_components=ncomp, init='random', random_state=1981)\nuser_factors = nmf_model.fit_transform(interactions_mtx)\nitem_factors = nmf_model.components_.T\n", "intent": "None negative matrix factorization with default values and n_comp (50 to start with) components/factors.\n"}
{"snippet": "mean_value = data['abdomo_protein'].mean()\ndata['abdomo_protein'] = data['abdomo_protein'].fillna(mean_value)\ndata['abdomo_protein']\n", "intent": "Q3. Fill all NaN values in column 'abdomo_protein' with the mean value of 'abdomo_protein'.\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "** Crie um objeto StandardScaler() chamado scaler. **\n"}
{"snippet": "yelp = pd.read_csv('yelp.csv')\n", "intent": "** Leia o arquivo yelp.csv e configure-o como um dataframe chamado yelp. **\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\n", "intent": "** Import CountVectorizer e crie um objeto CountVectorizer. **\n"}
{"snippet": "yelp = pd.read_csv(\"yelp.csv\")\n", "intent": "** Leia o arquivo yelp.csv e configure-o como um dataframe chamado yelp. **\n"}
{"snippet": "ad_data = pd.read_csv('advertising.csv')\n", "intent": "** Leia o arquivo advertising.csv e grave-o em um DataFrame chamado ad_data. **\n"}
{"snippet": "loans = pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas para ler loan_data.csv como um DataFrame chamado loans. **\n"}
{"snippet": "data_path_test = os.path.join(os.getcwd(), 'datasets', 'spambase_test.csv')\nspambase_test = pd.read_csv(data_path_test, delimiter = ',')\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure\n"}
{"snippet": "label_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(train_targets)\ny_valid = label_encoder.transform(valid_targets)\ny_test = label_encoder.transform(test_targets)\ny_train_1_hot = to_categorical(y_train, N_CLASSES)\ny_valid_1_hot = to_categorical(y_valid, N_CLASSES)\nwith open('label_encoder.p', 'wb') as f:\n    pickle.dump(label_encoder, f)\n", "intent": "    also convert targets to 1-hot format required by using categorical_crossentropy loss \n"}
{"snippet": "dfT = pd.read_csv('C:/Users/BeckyC/Desktop/Data Science - GA files/titanic.csv')\n", "intent": "Create a cluster based on Pclass and Age from the Titanic dataset.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()  \ndata['outcome'] = le.fit_transform(data['outcome'])  \n", "intent": "Q9. Using LabelEncoder, encode 'outcome'.\n"}
{"snippet": "from sklearn.model_selection import train_test_split, GridSearchCV\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n", "intent": "I will use a random forest classification to look for important features.\n"}
{"snippet": "fi = pd.DataFrame(rf.feature_importances_, columns = ['importance'])\nlabel = pd.DataFrame(X.columns, columns = ['feature'])\nfeature_imp = pd.concat([label, fi], axis = 1)\n", "intent": "The random forest model was able to predict the active status with an accuracy score of 78.3 %.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = 'data/iris.data'\niris = pd.read_csv(data)\n", "intent": "<a id=\"overview-of-the-iris-dataset\"></a>\n---\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = '../data/iris.data'\niris = pd.read_csv(data)\n", "intent": "<a id=\"overview-of-the-iris-dataset\"></a>\n---\n"}
{"snippet": "table = pd.DataFrame({'probability':[0.1, 0.2, 0.25, 0.5, 0.6, 0.8, 0.9]})\ntable['odds'] = table.probability/(1 - table.probability)\ntable\n", "intent": "**As an example we can create a table of probabilities vs. odds, as seen below.**\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nprint (cancer.feature_names)\nprint (cancer.DESCR)\n", "intent": "***\nA frequently used data set for ML is a data set for *breast cancer diagnosis*\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\nforest = RandomForestClassifier(n_estimators=100, random_state=0)\nforest.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))\n", "intent": "Effectively, boundaries are more complex\n***\n"}
{"snippet": "from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n", "intent": "Neural networks are quite sensitive to feature scaling, so let's try to scale the features.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.images.shape\n", "intent": "***\nAnother classic example case for ML is handwritten digits data.\nA suitable dataset is included with sklearn, first we look into it:\n"}
{"snippet": "import numpy as np\nimport pandas as pd\ndata = pd.read_csv('abalone.data',sep = ' ')\n", "intent": "In this lab, we will be faced with a regression problem. We have to guess the age of abalone(a type of marine snail) based on other given data. \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n", "intent": "***\nOf course what we not just clustering but classification, so let's try our two models we had used before also on digits:\n"}
{"snippet": "pca = PCA(n_components=2)\n", "intent": "What is `n_components`?\n"}
{"snippet": "use_cols = ['Cabin', 'Cabin_reduced', 'Sex']\nX_train, X_test, y_train, y_test = train_test_split(\n    data[use_cols], \n    data.Survived,  \n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "We reduced the number of different labels from 148 to 9.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data[['Age', 'Fare']].fillna(0),\n    data.Survived,\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "Age contains 20 % of missing data. For simplicity, I will fill the missing values with 0.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "coef = model.coef_.reshape(1,4)\npd.DataFrame(data=coef, columns=['Avg. Session Length', 'Time on App',\n       'Time on Website', 'Length of Membership'])\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "X = yelp['text']\ny = yelp['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=77)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "df = pd.read_csv('train.csv')\n", "intent": "Use `pandas` to load the file `train.csv`.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\n", "intent": "Split the data into a training and a test set:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.33,random_state=42)\n", "intent": "Q8. Split training and testing set.\n"}
{"snippet": "df = pd.read_csv(\"train806.csv\")\n", "intent": "Verwende `pandas`, um die Datei `train.csv` einzulesen.\n"}
{"snippet": "import pandas as pd\ndata = pd.DataFrame()\ndata['gender'] = ['male','male','male','male','female','female','female','female']\ndata['height'] = [6,5.92,5.58,5.92,5,5.5,5.42,5.75]\ndata['weight'] = [180,190,170,165,100,150,130,150]\ndata['shoe size'] = [12,11,12,10,6,8,7,9]\ndata\n", "intent": "Consider the following dataset:\n"}
{"snippet": "person = pd.DataFrame()\nperson['height'] = [6]\nperson['weight'] = [130]\nperson['shoe size'] = [8]\nperson\n", "intent": "Given this dataset, implement the naive bayes classification algorithm and use it classify the following person as either a male or female:\n"}
{"snippet": "import pandas as pd\ndata = pd.DataFrame()\ndata['gender'] = ['male','male','male','male','female','female','female','female']\ndata['height'] = [6,5.92,5.58,5.92,5,5.5,5.42,5.75]\ndata['weight'] = [180,190,170,165,100,150,130,150]\ndata['shoe size'] = [12,11,12,10,6,8,7,9]\ndata.head()\n", "intent": "Consider the following dataset:\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n", "intent": "Practice: Apply PCA to the iris dataset.\n"}
{"snippet": "labels =[\"Survived\", \"Perished\"]\ndef splitData(features):\n    titanic_predictors = titanic[features].as_matrix()\n    titanic_labels = titanic[\"Survived\"].as_matrix()\n    XTrain, XTest, yTrain, yTest = train_test_split(titanic_predictors, titanic_labels, random_state=1, test_size=0.5)\n    return XTrain, XTest, yTrain, yTest\n", "intent": "Here is some code that splits the data into training and test sets for cross-validation and selects features.\n"}
{"snippet": "contraception_df = pd.read_csv(\"cmc.csv\")\ncontraception_df.head()\n", "intent": "And then load and explore the dataset:\n"}
{"snippet": "features = contraception_df.columns[:-1]\ndef splitData(features):\n    contraception_labels = contraception_df[\"Contraceptive-method-used\"].as_matrix()\n    contraception_predictors =  contraception_df[features].as_matrix()\n    XTrain, XTest, yTrain, yTest = train_test_split(contraception_predictors, contraception_labels, \n                                                    random_state=1, test_size=0.5)\n    return XTrain, XTest, yTrain, yTest\n", "intent": "Here is some code that splits the data into training and test sets for cross-validation and selects features.\n"}
{"snippet": "XTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state=1, test_size=0.2)\nmodel.fit(XTrain, yTrain)\n", "intent": "So, let's see what our algorithm things about the Mod Squad! \n"}
{"snippet": "df=pd.read_csv(\"loan_data.csv\")\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nlist(digits.keys())\n", "intent": "Let's load a simple dataset of 8x8 gray level images of handwritten digits (bundled in the sklearn source code):\n"}
{"snippet": "from sklearn.decomposition import RandomizedPCA\npca = RandomizedPCA(n_components=2)\nX_pca = pca.fit_transform(X)\nX_pca.shape\n", "intent": "Let's visualize the dataset on a 2D plane using a projection on the first 2 axis extracted by Principal Component Analysis:\n"}
{"snippet": "from sklearn.feature_extraction.text import HashingVectorizer\nh_vectorizer = HashingVectorizer(encoding='latin-1')\ndv['h_vectorizer'] = h_vectorizer\ndv['names'] = names\ndv['training_csv_file'] = training_csv_file\ndv['n_partitions'] = len(client)\n", "intent": "Let's send all we need to the engines\n"}
{"snippet": "train = pd.DataFrame()\nfor i in range(5):\n    df = load_month(i, True)\n    train = pd.concat([train, df], axis=0)\ntest = load_month(5, True)\n", "intent": "Load 2015-01 - 2015-05 as training and test on 2015-06\n"}
{"snippet": "def encode_cat_data(df):\n    string_data = df.select_dtypes(include=[\"object\"])\n    for c in string_data.columns:\n        le = LabelEncoder()    \n        le.fit(df[c])\n        df[c] = le.transform(df[c])\n", "intent": "Encode categorical columns\n"}
{"snippet": "from datetime import datetime\nimport csv\nlogging.info('- Generate submission')\nsubmission_file = '../results/submission_' + \\\n                  str(datetime.now().strftime(\"%Y-%m-%d-%H-%M\")) + \\\n                  '.csv'\nsubmission.to_csv(submission_file, index=False, index_label=False)\n", "intent": "Get submission DataFrame and write csv file\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nll = 5\ntrain_df = pd.DataFrame({'ncodpers': range(ll) + range(ll), 'fecha_dato': [201501]*ll + [201502]*ll, 'a': range(2*ll), 'b': np.random.randn(2*ll)}, columns=['ncodpers', 'fecha_dato', 'a', 'b'])\ntrain_df\n", "intent": "Append last choice as columns\n"}
{"snippet": "data = pd.read_csv('TaiwaneseDefaultData.csv')\ndata.head()\n", "intent": "Note, the last column is the label\n"}
{"snippet": "scaler = StandardScaler().fit(features)\nscaled_features = scaler.transform(features)\nprint(scaled_features)\n", "intent": "Super, now we're finally ready for our Scikit-Learn magic; let's apply the `StandardScaler` to the feature set:\n"}
{"snippet": "df=pd.read_csv(\"College_Data\",index_col=0)\ndf.head()\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_norm =  StandardScaler().fit_transform(X)\nmodel.fit(X_norm, y)\ncoeffs = pd.DataFrame(model.coef_, columns = iris.feature_names, index =iris.target_names)\ncoeffs\n", "intent": "Let's normalize the data and repeat the exercise:\n> Student should do this:\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "Scikit Learn implements support vector machine models in the `svm` package.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfeatures = [c for c in df.columns if c != 'acceptability']\nfor c in df.columns:\n    df[c] = le.fit_transform(df[c])\nX = df[features]\ny = df['acceptability']\n", "intent": "Since most of the features are categorical text we will need to encode them as numbers using the `LabelEncoder`:\n"}
{"snippet": "feature_importances = pd.DataFrame(dt.feature_importances_,\n                                   index = X.columns,\n                                    columns=['importance']).sort_values('importance',\n                                                                        ascending=False)\nfeature_importances.head()\n", "intent": "Great, now we are ready to look at feature importances in our tree:\n"}
{"snippet": "cars=pd.DataFrame(mtcars)\n", "intent": "Convert to a Pandas Dataframe for our analysis\n"}
{"snippet": "iris=pd.read_csv(\"iris.csv\")\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "First, let's import our data. We'll be using *Iris* again for this exercise. \n"}
{"snippet": "x = pd.DataFrame(iris.data)\nx.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\ny = pd.DataFrame(iris.target)\ny.columns = ['Targets']\n", "intent": "As in 2.1, let's split the set into two parts. \"X\" will be the data and \"Y\" will be the class labels.\n"}
{"snippet": "x = pd.DataFrame(iris.data)\nx.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\ny = pd.DataFrame(iris.target)\ny.columns = ['Targets']\n", "intent": "Define your \"X\" and \"y\" variables for the analysis\n"}
{"snippet": "df=pd.read_csv(\"KNN_Project_Data\")\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "data = datasets.load_boston()\nprint data.DESCR \n", "intent": "**Load the boston housing data with the `datasets.load_boston()` function.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size= 0.25)\n", "intent": "---\n- Make predictor matrix `X` and target variable `y`\n- Split your data into a validation set using `train_test_split`\n"}
{"snippet": "test_df = test_df.drop('PassengerId', axis=1)\ntest_df.to_csv('new_test_data.csv', index=False)\ntest_df\n", "intent": "And the test dataset.\n"}
{"snippet": "A.fillna('missing data')\n", "intent": "That actually works with any data type.\n"}
{"snippet": "A.fillna(A.mean())\n", "intent": "We can use this functionality to fill in the gaps with the average value computed across the non-missing values.\n"}
{"snippet": "print(B)\nprint()\nprint(B.fillna(method='pad'))\n", "intent": "You can fill values forwards and backwards with the flags *pad* / *ffill* and *bfill* / *backfill*\n"}
{"snippet": "B.fillna(method='bfill', limit=1)\n", "intent": "We can set a limit if we only want to replace consecutive gaps.\n"}
{"snippet": "np.random.seed(2)\nser = pd.Series(np.arange(1, 10.1, .25)**2 + np.random.randn(37))\nbad = np.array([4, 13, 14, 15, 16, 17, 18, 20, 29])\nser[bad] = np.nan\nmethods = ['linear', 'nearest', 'quadratic']\ndf = pd.DataFrame({m: ser.interpolate(method=m) for m in methods})\ndf.plot()\n", "intent": "Below we compare three different methods.\n"}
{"snippet": "file_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\nnames = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Name']\ndf_iris = pd.read_csv(file_url, names=names, header=None)\nprint(df_iris.head())\n", "intent": "Now let's load and take a look at the Iris dataset again.\n"}
{"snippet": "ss=StandardScaler()\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "cdf = pd.DataFrame(lm.coef_, X.columns,columns=['Coeffecient'])\n", "intent": "**Interpretting the coefficients and answering the question at hand.**\n"}
{"snippet": "loans_df = pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "loans_df = pd.read_csv('loan_data.csv')\n", "intent": "** Check out the info(), head(), and describe() methods on loans.**\n"}
{"snippet": "fscaled_df = pd.DataFrame(feats_scaled, columns=feats.columns)\nfscaled_df.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "ad_df = pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer()\nprint(count_vector)\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "five_year_returns = \\\n    get_pricing(\n        data_portal,\n        trading_calendar,\n        universe_tickers,\n        universe_end_date - pd.DateOffset(years=5),\n        universe_end_date)\\\n    .pct_change()[1:].fillna(0)\nfive_year_returns\n", "intent": "Let's get returns data for our risk model using the `get_pricing` function. For this model, we'll be looking back to 5 years of data.\n"}
{"snippet": "num_factor_exposures = 20\npca = fit_pca(five_year_returns, num_factor_exposures, 'full')\npca.components_\n", "intent": "Let's see what the model looks like. First, we'll look at the PCA components.\n"}
{"snippet": "df1=pd.DataFrame(data=scaled_Value,columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "def factor_returns(pca, returns, factor_return_indices, factor_return_columns):\n    assert len(factor_return_indices.shape) == 1\n    assert len(factor_return_columns.shape) == 1\n    return pd.DataFrame(pca.transform(returns),  factor_return_indices, factor_return_columns)\nproject_tests.test_factor_returns(factor_returns)\n", "intent": "Implement `factor_returns` to get the factor returns from the PCA model using the returns data.\n"}
{"snippet": "def idiosyncratic_var_matrix(returns, factor_returns, factor_betas, ann_factor):\n    common_returns = pd.DataFrame(np.dot(factor_returns, factor_betas.T), returns.index, returns.columns)\n    s_returns = returns - common_returns\n    return pd.DataFrame(np.diag(np.var(s_returns))*ann_factor, returns.columns, returns.columns)\nproject_tests.test_idiosyncratic_var_matrix(idiosyncratic_var_matrix)\n", "intent": "Implement `idiosyncratic_var_matrix` to get the idiosyncratic variance matrix.\n"}
{"snippet": "def idiosyncratic_var_vector(returns, idiosyncratic_var_matrix):\n    return pd.DataFrame(np.diag(idiosyncratic_var_matrix), returns.columns)\nproject_tests.test_idiosyncratic_var_vector(idiosyncratic_var_vector)\n", "intent": "Implement `idiosyncratic_var_vector` to get the idiosyncratic variance Vector.\n"}
{"snippet": "all_weights = pd.DataFrame(np.repeat(1/len(universe_tickers), len(universe_tickers)), universe_tickers)\npredict_portfolio_risk(\n    risk_model['factor_betas'],\n    risk_model['factor_cov_matrix'],\n    risk_model['idiosyncratic_var_matrix'],\n    all_weights)\n", "intent": "Let's see what the portfolio risk would be if we had even weights across all stocks.\n"}
{"snippet": "ls_factor_returns = pd.DataFrame()\nfor factor, factor_data in clean_factor_data.items():\n    ls_factor_returns[factor] = al.performance.factor_returns(factor_data).iloc[:, 0]\n(1+ls_factor_returns).cumprod().plot()\n", "intent": "Let's view the factor returns over time. We should be seeing it generally move up and to the right.\n"}
{"snippet": "sector_lookup = pd.read_csv(\n    os.path.join(os.getcwd(), '..', '..', 'data', 'project_7_sector', 'labels.csv'),\n    index_col='Sector_i')['Sector'].to_dict()\nsector_lookup\nsector_columns = []\nfor sector_i, sector_name in sector_lookup.items():\n    secotr_column = 'sector_{}'.format(sector_name)\n    sector_columns.append(secotr_column)\n    all_factors[secotr_column] = (all_factors['sector_code'] == sector_i)\nall_factors[sector_columns].head()\n", "intent": "For the model to better understand the sector data, we'll one hot encode this data.\n"}
{"snippet": "ls_factor_returns = pd.DataFrame()\nfor factor_name, data in factor_data.items():\n    ls_factor_returns[factor_name] = al.performance.factor_returns(data).iloc[:, 0]\n(1 + ls_factor_returns).cumprod().plot()\n", "intent": "Let's view the factor returns over time. We should be seeing it generally move up and to the right.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X)\nprint(pca.components_)  \nprint(pca.explained_variance_)  \n", "intent": "[The inspiration](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)\n"}
{"snippet": "prices = pd.read_csv('stock_prices_advanced_optimization.csv', parse_dates=['date'], index_col=0)\n", "intent": "Load the data from the file `stock_prices_advanced_optimization.csv`.\n"}
{"snippet": "X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "rm = RiskModelPCA(2) \nrm.fit(rets) \n", "intent": "Let's fit the risk model with 2 factors (i.e., we'll keep 2 PCs).\n"}
{"snippet": "unemp_03_14 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/unemp_tot_03_14.csv')\nunemp_03_14 = unemp_03_14.iloc[:,1:]\npov_county_year_03_14 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/pov_county_year_03_14.csv')\npov_county_year_03_14 = pov_county_year_03_14.iloc[:,1:]\ncdc_03_14 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/cdc_03_14.csv')\ncdc_03_14 = cdc_03_14.iloc[:,2:]\ncdc_03_14.head(12)\n", "intent": "https://www.census.gov/geo/reference/county-changes.html\n"}
{"snippet": "wine = wine.drop('type', 1)\ndf_X = pd.DataFrame(wine, columns=wine.columns)\ndf_X = df_X.drop('quality', 1)\ndf_y = wine[\"quality\"]\nX_train, X_test, y_train, y_test = train_test_split(df_X,df_y, test_size=.4)\nprint \"       X Shape  Y Shape\"\nprint \"Train\", X_train.shape, y_train.shape\nprint \"Test \", X_test.shape, y_test.shape\n", "intent": "Using scikit-learn or statsmodels, build the necessary models for your scenario. Evaluate model fit.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(Q1_2015_X, Q234_2015_Y, test_size=.33)\nprint \"       X Shape  Y Shape\"\nprint \"Train\", X_train.shape, y_train.shape\nprint \"Test \", X_test.shape, y_test.shape\n", "intent": "Using scikit-learn or statsmodels, build the necessary models for your scenario. Evaluate model fit.\n"}
{"snippet": "df = pd.read_csv('assets/dataset/flight_delays.csv')\ndf.head(3)\n", "intent": "Using a dataset of flight delays let's:\n- try to predict whether a flight will be delayed by 15 minutes\n- visualize our predictions\n"}
{"snippet": "df = pd.read_csv('assets/dataset/collegeadmissions.csv')\ndf.head()\n", "intent": "Here's a dataset of grad school admissions, based on GPA, rank, and GRE score. In the admit column \"1\" is admit.\n"}
{"snippet": "sac = pd.read_csv('../assets/datasets/Sacramentorealestatetransactions.csv')\nsac.head()\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\ny = iris.target\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "df_new = pd.DataFrame(data=df, columns=['workclass_num', 'education-num', 'hours-per-week', 'income_num'])\ndf_new2 = pd.DataFrame(data=df, columns=['workclass_num', 'education-num'])\ndf_new.head(5)\n", "intent": "Create a New Dataframe with just numerical data\n"}
{"snippet": "df=pd.read_csv(\"advertising.csv\")\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "airports = pd.read_csv('/Users/HudsonCavanagh/Documents/airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "xStand = StandardScaler().fit_transform(x)\n", "intent": "Then, standardize the features for analysis\n"}
{"snippet": "PCdf = pd.DataFrame(xPC, columns=['PC1','PC2'])\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "air_pca_2_features = pd.DataFrame(air_pca_2_features)\nair_pca_2_features.head()\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "airports = pd.read_csv('./assets/datasets/airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "X_scaled = preprocessing.MinMaxScaler().fit_transform(df)\n", "intent": "Next, since each of our features have different units and ranges, let's do some preprocessing:\n"}
{"snippet": "from sklearn import preprocessing\nencode = preprocessing.LabelEncoder()\npre_poll['age'] = encode.fit_transform(pre_poll.age) \npre_poll['state'] = encode.fit_transform(pre_poll.state)\npre_poll['edu'] = encode.fit_transform(pre_poll.edu)\npre_poll.head()\n", "intent": "Some of our ML work will be better suited if the input data is contained in a numpy object.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split, cross_val_score\nX_train, X_test, y_train, y_test = train_test_split(poll_nd, bush_null, test_size=0.30, random_state=50)\n", "intent": "Split the data in the ordinary way, making sure you have a 70/30 split.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"data_training.csv\")\nimport numpy as np\nX = np.array(data[['x1', 'x2']])\ny = np.array(data['y'])\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, \n\t\t\t\t\t\t\t\t\t\t\t\t\ty, \n\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size = 0.25)\n", "intent": "Point of the exercise: it is not easy to fit these parameters manually.\n**Testing in sklearn**\n"}
{"snippet": "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=101)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline([('std_scaler', StandardScaler()),\n                    ('poly_features', PolynomialFeatures(degree = 2))])\nhousing_prep = pipeline.fit_transform(X)\n", "intent": "<b>SCALING FEATURES</b>\n"}
{"snippet": "customer_log_df = np.log(customer_df)\nscaler.fit(customer_log_df)\ncustomer_log_sc = scaler.transform(customer_log_df)\ncustomer_log_sc_df = pd.DataFrame(customer_log_sc, columns=customer_df.columns)\n", "intent": "Many times the skew of data can be easily removed by taking the log of the data. Let's do so here.\nWe will then scale the data after deskewing.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('car.csv') \ndf.head()\n", "intent": "Vamos a comenzar con la lectura del dataset de aceptabilidad de autos.\n"}
{"snippet": "subte = pd.read_csv('estaciones-de-subte.csv',delimiter=',')\nmetrobus = pd.read_csv('estaciones-de-metrobus.csv',delimiter=';')\nferrocarril = pd.read_csv('estaciones-de-ferrocarril.csv',delimiter=';')\n", "intent": "Datos de servicios de transporte por Buenos Aires Data:\n"}
{"snippet": "loans=pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "df=pd.read_csv('College_Data',index_col=0)\ndf.head()\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df=pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaler=StandardScaler()\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "scaled_features=pd.DataFrame(scaled_features,columns=df.columns.drop('TARGET CLASS'))\nscaled_features.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint('Size of training data: {}'.format(len(X_train)))\nprint('Size of testing data: {}'.format(len(X_test)))\n", "intent": "- From the dataset, we will split into training and testing data.\n"}
{"snippet": "from dopy.doplot.selection import add_log_to_dataframe, add_max_to_dataframe, add_min_to_dataframe\nadd_min_to_dataframe(real_dataframe_wrongPV, 'B0_FitDaughtersConst_KS0_min_PT', ['B0_FitDaughtersConst_KS0_P1_PT_flat', 'B0_FitDaughtersConst_KS0_P0_PT_flat'])\nadd_min_to_dataframe(signal_dataframe_wrongPV, 'B0_FitDaughtersConst_KS0_min_PT', ['B0_FitDaughtersConst_KS0_P1_PT_flat', 'B0_FitDaughtersConst_KS0_P0_PT_flat'])\nadd_min_to_dataframe(real_dataframe_wrongPV, 'test_IP', ['B0_FitDaughtersConst_KS0_P0_IPCHI2_flat', 'B0_FitDaughtersConst_KS0_P1_IPCHI2_flat'])\nadd_min_to_dataframe(signal_dataframe_wrongPV, 'test_IP', ['B0_FitDaughtersConst_KS0_P0_IPCHI2_flat', 'B0_FitDaughtersConst_KS0_P1_IPCHI2_flat'])\nadd_min_to_dataframe(real_dataframe_wrongPV, 'B0_FitDaughtersConst_J_psi_1S_min_PT', ['B0_FitDaughtersConst_J_psi_1S_P0_PT_flat', 'B0_FitDaughtersConst_J_psi_1S_P1_PT_flat'])\nadd_min_to_dataframe(signal_dataframe_wrongPV, 'B0_FitDaughtersConst_J_psi_1S_min_PT', ['B0_FitDaughtersConst_J_psi_1S_P0_PT_flat', 'B0_FitDaughtersConst_J_psi_1S_P1_PT_flat'])\nreal_dataframe_wrongPV['B0_FitPVConst_KS0_tau_dimless'] = real_dataframe_wrongPV['B0_FitPVConst_KS0_tau_flat']/real_dataframe_wrongPV['B0_FitPVConst_KS0_tauErr_flat']\nsignal_dataframe_wrongPV['B0_FitPVConst_KS0_tau_dimless'] = signal_dataframe_wrongPV['B0_FitPVConst_KS0_tau_flat']/signal_dataframe_wrongPV['B0_FitPVConst_KS0_tauErr_flat']\n", "intent": "Generate new features\n------------------------------------\n"}
{"snippet": "from dopy.doplot.selection import add_log_to_dataframe, add_max_to_dataframe, add_min_to_dataframe\nadd_min_to_dataframe(real_dataframe, 'B0_FitDaughtersConst_KS0_min_PT', ['B0_FitDaughtersConst_KS0_P1_PT', 'B0_FitDaughtersConst_KS0_P0_PT'])\nadd_min_to_dataframe(signal_dataframe, 'B0_FitDaughtersConst_KS0_min_PT', ['B0_FitDaughtersConst_KS0_P1_PT', 'B0_FitDaughtersConst_KS0_P0_PT'])\nadd_min_to_dataframe(real_dataframe, 'test_IP', ['B0_FitDaughtersConst_KS0_P0_IPCHI2', 'B0_FitDaughtersConst_KS0_P1_IPCHI2'])\nadd_min_to_dataframe(signal_dataframe, 'test_IP', ['B0_FitDaughtersConst_KS0_P0_IPCHI2', 'B0_FitDaughtersConst_KS0_P1_IPCHI2'])\nadd_min_to_dataframe(real_dataframe, 'B0_FitDaughtersConst_J_psi_1S_min_PT', ['B0_FitDaughtersConst_J_psi_1S_P0_PT', 'B0_FitDaughtersConst_J_psi_1S_P1_PT'])\nadd_min_to_dataframe(signal_dataframe, 'B0_FitDaughtersConst_J_psi_1S_min_PT', ['B0_FitDaughtersConst_J_psi_1S_P0_PT', 'B0_FitDaughtersConst_J_psi_1S_P1_PT'])\nreal_dataframe['B0_FitPVConst_KS0_tau_dimless'] = real_dataframe['B0_FitPVConst_KS0_tau']/real_dataframe['B0_FitPVConst_KS0_tauErr']\nsignal_dataframe['B0_FitPVConst_KS0_tau_dimless'] = signal_dataframe['B0_FitPVConst_KS0_tau']/signal_dataframe['B0_FitPVConst_KS0_tauErr']\n", "intent": "Generate new features\n------------------------------------\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../datasets/ch1ex1.csv')\npoints = df.values\nnew_df = pd.read_csv('../datasets/ch1ex2.csv')\nnew_points = new_df.values\n", "intent": "**Step 1:** Load the dataset _(written for you)_.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../datasets/ch1ex1.csv')\npoints = df.values\n", "intent": "**Step 1:** Load the dataset _(written for you)_.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../datasets/fish.csv')\ndel df['species']\n", "intent": "**Step 1:** Load the dataset _(this bit is written for you)_.\n"}
{"snippet": "import pandas as pd\nfn = '../datasets/company-stock-movements-2010-2015-incl.csv'\nstocks_df = pd.read_csv(fn, index_col=0)\n", "intent": "**Step 1:** Load the data _(written for you)_\n"}
{"snippet": "import pandas as pd\nfn = '../datasets/company-stock-movements-2010-2015-incl.csv'\nstocks_df = pd.read_csv(fn, index_col=0)\n", "intent": "**Step 1:** Load the data _(written for you)_.\n"}
{"snippet": "companies = list(stocks_df.index)\nmovements = stocks_df.values\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import make_pipeline\nnormalizer = Normalizer()\nkmeans = KMeans(n_clusters=14)\npipeline = make_pipeline(normalizer, kmeans)\npipeline.fit(movements)\n", "intent": "**Step 2:** Run your code from the previous exercise _(filled in for you)_.\n"}
{"snippet": "import pandas as pd\nseeds_df = pd.read_csv('../datasets/seeds.csv')\ndel seeds_df['grain_variety']\n", "intent": "**Step 1:** Load the dataset _(written for you)_.\n"}
{"snippet": "data_path = votre_path + \"features.txt\"\nfeatures_names =    pd.read_csv(data_path,delim_whitespace=True,header=None)\ndata_path = votre_path + \"train/X_train.txt\"\nactivity_features = pd.read_csv(data_path,delim_whitespace=True,header=None,names=features_names.values[:,1])\nactivity_features.head()\n", "intent": "Importation de la base d'apprentissage de donnees de features\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../datasets/fish.csv')\nspecies = list(df['species'])\ndel df['species']\n", "intent": "**Step 1:** Load the dataset, extracting the species of the fish as a list `species` _(done for you)_\n"}
{"snippet": "normalizer = Normalizer()\n", "intent": "**Step 3:** Create an instance of `Normalizer` called `normalizer`.\n"}
{"snippet": "client_df = pd.DataFrame(np.array(client_data).reshape(3,3), columns=['RM','LSTAT','PTRATIO'])\ncorr = client_df.corr()\nsns.heatmap(corr,annot=True,xticklabels=['RM','LSTAT','PTRATIO'], yticklabels =['RM','LSTAT','PTRATIO'])\n", "intent": "Heatmap of the client data shows the positive and negative correlations.\n"}
{"snippet": "import matplotlib.image as mpimg\nimage1 = mpimg.imread('external-data/right-of-way.png')\nimage2 = mpimg.imread('external-data/pedestrians_1.png')\nimage3 = mpimg.imread('external-data/general_caution_1.png')\nimage4 = mpimg.imread('external-data/children-crossing_1.png')\nimage5 = mpimg.imread('external-data/stop.png')\nimage6 = mpimg.imread('external-data/no-truck-passing.png')\nX_test_new = [image1, image2, image3, image4, image5, image6]\ny_test_new = [11,27,18,28,14,10]\n", "intent": "Load the new  images\n"}
{"snippet": "import pandas as pd\nfn = '../datasets/company-stock-movements-2010-2015-incl.csv'\nstocks_df = pd.read_csv(fn, index_col=0)\ncompanies = list(stocks_df.index)\nmovements = stocks_df.values\n", "intent": "**Step 1:** Load the data _(written for you)_\n"}
{"snippet": "import pandas as pd\nscores_df = pd.read_csv('../datasets/eurovision-2016-televoting.csv', index_col=0)\ncountry_names = list(scores_df.index)\n", "intent": "**Step 1:** Load the DataFrame _(written for you)_\n"}
{"snippet": "data = pd.read_csv('data/weather-non-agg-DFE.csv')\nprint(data.shape)\ndata.head()\n", "intent": "To begin with let's load the data into a format we can work with.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nsamples = x_train[:2]\nx_train_counts = ...\nprint(pd.DataFrame(x_train_counts.A, columns=count_vect.get_feature_names()).to_string())\n", "intent": "Now let's see how we can transform our tweets into vectors\n"}
{"snippet": "df_pivot = df.pivot(values='n', columns='offer_id', index='customer_name').fillna(0)\n", "intent": "All the value ranges look ok. Let's get the pivot table.\n"}
{"snippet": "data_path = votre_path + \"train/y_train.txt\"\nactivity  =    pd.read_csv(data_path,delim_whitespace=True,header=None)\nactivity  =  activity.values[:,0] -  1\nactivity_names = ['WALKING','WALKING_UPSTAIRS','WALKING_DOWNSTAIRS','SITTING','STANDING','LAYING']\n", "intent": "Recuperations des types d'activites\n"}
{"snippet": "X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(X_res, y_res,\n                                                    stratify=y_res, test_size=0.3, random_state=42)\n", "intent": "We can see that the resampled variables have increased in size. Let's check the class balance.\n"}
{"snippet": "col_names = ['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', \n        'drive-wheels', 'engine-location', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type', \n        'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-rate', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\ncars = pd.read_csv(\"/Users/chesterjohn/Desktop/dataquest/jupyter-1-ml-fundamentals/imports-85.data\",names=col_names)\nprint(cars.head(2))\n", "intent": "Looks like our data do not contain headers names, so we have to manually add them.\n"}
{"snippet": "dataset = pd.read_csv(\"train.csv\")\nX = dataset.iloc[:,:-1].values\nY=dataset.iloc[:,-1].values\n", "intent": "Import dataset to train model\n"}
{"snippet": "X_train, X_test, y_train ,y_test = train_test_split(X,Y,test_size = 0.2,random_state=0)\n", "intent": "Split X to Train and Test set by 80% and 20% respectively\n"}
{"snippet": "scaler = StandardScaler()\nscaler.fit(X)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "Perform feature scalling on features to improve the performance of model\n"}
{"snippet": "dataset = pd.read_csv('train.csv')\n", "intent": "Import dataset to train model\n"}
{"snippet": "dataset = dataset.fillna(-1)\nX = dataset.iloc[:,1:-1].values\ny = dataset.iloc[:,-1].values\n", "intent": "Fill NaN with -1 and Split dataset to X and y\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)\n", "intent": "Split X to Train and Test set by 80% and 20% respectively\n"}
{"snippet": "dataset = pd.read_csv('train.csv')\nX = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values\n", "intent": "Import dataset to train model and split dataset to X and y\n"}
{"snippet": "data_path = votre_path + \"train/subject_train.txt\"\nsujet =  pd.read_csv(data_path,delim_whitespace=True,header=None)\nsujet =sujet.values[:,0]\n", "intent": "Recuperations des sujets\n"}
{"snippet": "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n", "intent": "Split X to Train and Test set by 80% and 20% respectively\n"}
{"snippet": "from sklearn.datasets import fetch_california_housing\nhousing = fetch_california_housing()\n", "intent": "for california housing dataset\n"}
{"snippet": "data = pd.read_csv('bitcoin_price.csv', index_col='trading_day')\nnext_day = data['price_usd'].iloc[1:]\ndata = data.iloc[:-1,:]\ndata['next_day'] = next_day.values\nprint data.shape\n", "intent": "Create the outcome variable `next_day`.\n"}
{"snippet": "def merge_columns(main, other):\n    result = pd.merge(left=main,right=other, how='outer', left_on='date', right_on='date')\n    return result\ntrends = pd.read_csv('bitcoin_trends.csv')\ndata = merge_columns(data, trends)\n", "intent": "<img src='bitcoin_trend2.png' width=400px>\n"}
{"snippet": "features = list(set(selected_columns) - set(['next_day']))\ny = df['next_day']\nX = df[features]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "Split the data into training and test set.\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nmin_max_scaler = MinMaxScaler()\nlongitude_minmax = min_max_scaler.fit_transform(X['X'])\nlongitude_minmax_df = pd.DataFrame(longitude_minmax, columns=['scaled_longitude'])\npos_X = X.copy()\ndel pos_X['X']\npos_X = pd.concat([longitude_minmax_df, pos_X], axis=1)\npos_X.head()\n", "intent": "We can use MinMaxScaler() with a range of [0,1] to address this problem.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n", "intent": "Use `train_test_split()` to generate a 70/30 train/test split and store this in `X_train`, `X_test`, `y_train`, `y_test`.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "Use `train_test_split()` to generate a 70/30 train/test split and store this in `X_train`, `X_test`, `y_train`, `y_test`.\n"}
{"snippet": "scaled_features = pd.DataFrame(scaled_data, columns=['Image.Var', 'Image.Skew', 'Image.Curt', 'Entropy'])\nscaled_features.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data_path = votre_path + \"train/Inertial Signals/body_acc_x_train.txt\"\nacc_x = pd.read_csv(data_path,delim_whitespace=True,header=None).values\ndata_path = votre_path + \"train/Inertial Signals/body_acc_y_train.txt\"\nacc_y = pd.read_csv(data_path,delim_whitespace=True,header=None).values\ndata_path = votre_path + \"train/Inertial Signals/body_acc_z_train.txt\"\nacc_z = pd.read_csv(data_path,delim_whitespace=True,header=None).values\n", "intent": "Importation des donnees brute accelerometre\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "import pandas as pd\nimport glob\npath =r'/Users/mtlee/Code/DAT_mtl/nytfiles/' \nallFiles = glob.glob(path + \"/nyt*.csv\")\nframe = pd.DataFrame()\nlist_ = []\nfor file_ in allFiles:\n    df = pd.read_csv(file_,index_col=None, header=0)\n    list_.append(df)\nframe = pd.concat(list_, axis=0)\n", "intent": "Put together the NYT data from 30 different files into one file. Run df.shape and report the dimensions of your final dataframe.\n"}
{"snippet": "from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy = 'mean')\n", "intent": "[sklean impute docs](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n"}
{"snippet": "from sklearn.datasets import load_iris \niris = load_iris() \nnumSamples, numFeatures = iris.data.shape \n", "intent": "<h3>Dimensional Reduction for IRIS data set</h3>\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nX_r  = pca.fit_transform(X)\nprint(np.shape(X), np.shape(X_r))\n", "intent": "Tools for data mining and analysis. For now we will just focus on dimensionality reduction \nhttp://scikit-learn.org/stable/index.html\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nprint (digits.data.shape)\nimagea = digits.images\ntargeta = digits.target\nprint (imagea.shape, targeta.shape)\n", "intent": "<h3> Visualizing Handwritten Digits </h3>\nload the digit data set\n"}
{"snippet": "df = pd.read_csv('resources/train_house.csv')\ndf.head()\n", "intent": "Working with Real Estate Dataset\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_housing_data = scaler.fit_transform(housing.data)\nscaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n", "intent": "When using Gradient Descent, recall that it is important to first normalize the input feature vectors, or else training may be much slower.\n"}
{"snippet": "air_quality = pd.read_csv(fname, header=4, skipfooter=4, na_values='No data', engine='python')\n", "intent": "Let's try to read the data using `pandas.read_csv()` function.\n"}
{"snippet": "pca_features_myact = PCA(n_components=2).fit(preprocessing.scale(features_myact)).transform(features_myact)\npca_Comp0 = pca_features_myact[:,0]\npca_Comp1 = pca_features_myact[:,1]\npca_Comp0.shape\n", "intent": "Projection sur 2 axes de l'ACP\n"}
{"snippet": "age_gender_bkts_data = pd.read_csv(\"age_gender_bkts.csv\")\n", "intent": "This file contains the census data of age and gender distribution for the 10 destination countries as of 2015.\n"}
{"snippet": "countries_data = pd.read_csv(\"countries.csv\")\n", "intent": "This file contains the latitude, longitude, distance to US, area, and language of 10 destination countries.\n"}
{"snippet": "train.pivot_table(values='Loan_Status',index=['Credit_History'],aggfunc=lambda x: x.map({'Y':1,'N':0}).mean())\n", "intent": "Probability of getting loan for each Credit History class:\n"}
{"snippet": "ss = MinMaxScaler()\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "pca = PCA(n_components=X.shape[1])\npca.fit(X)\n", "intent": "Compute a PCA with the maximum number of components.\n"}
{"snippet": "print(pca.explained_variance_ratio_)\nK = 2\npca = PCA(n_components=X.shape[1])\npca.fit(X)\nPC = pca.transform(X)\n", "intent": "Retrieve the explained variance ratio. Determine $K$ the number of components.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('Advertising.csv', index_col=0)\ndata.rename(columns = {'radio':'Radio','newspaper':'Newspaper','sales':'Sales'},inplace = True)\ndata.head()\n", "intent": "The data set we are using is: Advertising.csv\n"}
{"snippet": "data = data.iloc[np.random.permutation(len(data))]\ndata.reset_index(drop = True, inplace = True)\nx = data.drop(['Target'], axis = 1)\ny = data.Target\ntrain_examples = 100000\nfrom sklearn.cross_validation import train_test_split\nx_train, x_test, y_train, y_test = train_test_split( x, y, train_size = train_examples )\n", "intent": "Recreating training and test sets:\n"}
{"snippet": "train = pd.read_csv('train.csv')\n", "intent": "Simple data loading and transformations:\n"}
{"snippet": "pca = PCA(n_components=3)\npca.fit(basket_data_num_nor)\npcaBasket3 = pca.transform(basket_data_num_nor)\nComp0 = pcaBasket3[:,0]\nComp1 = pcaBasket3[:,1]\nComp2 = pcaBasket3[:,2]\n", "intent": "Extraction des trois premieres composantes\n"}
{"snippet": "survived_sex = data[data['Survived']==1]['Sex'].value_counts()\ndead_sex = data[data['Survived']==0]['Sex'].value_counts()\ndf = pd.DataFrame([survived_sex,dead_sex])\ndf.index = ['Survived','Dead']\ndf.plot(kind='bar',stacked=True, figsize=(13,8))\n", "intent": "Perfect.\nLet's now make some charts.\nLet's visualize survival based on the gender.\n"}
{"snippet": "survived_embark = data[data['Survived']==1]['Embarked'].value_counts()\ndead_embark = data[data['Survived']==0]['Embarked'].value_counts()\ndf = pd.DataFrame([survived_embark,dead_embark])\ndf.index = ['Survived','Dead']\ndf.plot(kind='bar',stacked=True, figsize=(13,8))\n", "intent": "Let's now see how the embarkation site affects the survival.\n"}
{"snippet": "def recover_train_test_target():\n    global combined\n    train0 = pd.read_csv('../data/train.csv')\n    targets = train0.Survived\n    train = combined.ix[0:890]\n    test = combined.ix[891:]\n    return train,test,targets\n", "intent": "Recovering the train set and the test set from the combined dataset is an easy task.\n"}
{"snippet": "features = pd.DataFrame()\nfeatures['feature'] = train.columns\nfeatures['importance'] = clf.feature_importances_\n", "intent": "Let's have a look at the importance of each feature.\n"}
{"snippet": "def embarked_impute(train, test):\n    for i in [train, test]:\n        i['Embarked'] = i['Embarked'].fillna('S')\n    return train, test\n", "intent": "We fill the null values in the `Embarked` column with the most commonly occuring value, which is 'S.'\n"}
{"snippet": "train = pd.read_csv(\"train.csv\")\ntest    = pd.read_csv(\"test.csv\")\nfull = train.append( test , ignore_index = True )\ntitanic = full[ :891 ]\nprint train.shape\nprint test.shape\ndel train , test\nprint ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)\n", "intent": "Now that our packages are loaded, let's read in and take a peek at the data.\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "titanic = pd.read_csv(\"train.csv\")\ntitanic.head()\n", "intent": "Load train & test data\n======================\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\nlabelEnc=LabelEncoder()\ncat_vars=['Embarked','Sex',\"Title\",\"FsizeD\",\"NlengthD\",'Deck']\nfor col in cat_vars:\n    titanic[col]=labelEnc.fit_transform(titanic[col])\n    titanic_test[col]=labelEnc.fit_transform(titanic_test[col])\ntitanic.head()\n", "intent": "Convert Categorical variables into Numerical ones\n=================================================\n"}
{"snippet": "print gb_features\ncols = trainX.columns.values[1:]\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n", "intent": "Create a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\ndigits.images.shape\n", "intent": "We'll use scikit-learn's data access interface and take a look at this data:\n"}
{"snippet": "from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(numTrees=100, maxDepth=2, labelCol=\"label\", seed=42)\nrfModel = rf.fit(trainingData)\nrfFeatureImportance = pd.DataFrame([(name, rfModel.featureImportances[idx]) for idx, name in attrs],columns=['feature_name','feature_importance'])\nprint(rfFeatureImportance.sort_values(by = ['feature_importance'], ascending = False))\n", "intent": "1. Build and train a RandomForestClassifier and print out a table of feature importances from it.\n"}
{"snippet": "from sklearn import linear_model\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\nY = iris.target\nprint iris.DESCR\n", "intent": "Load the data set from Sci Kit Learn\n"}
{"snippet": "iris_data = DataFrame(X,columns=['Sepal Length','Sepal Width','Petal Length','Petal Width'])\niris_target = DataFrame(Y,columns=['Species'])\n", "intent": "Let's put the data into a pandas DataFrame.\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\nX = iris.data\nY = iris.target\nprint iris.DESCR\n", "intent": "First we'll start by importing the Data set we are already very familiar with, the Iris Data Set\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n", "intent": "Now we will split the data into a training set and a testing set and then train our model.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n", "intent": "Now that we have our model, we will continue by seperating into training and testing sets:\n"}
{"snippet": "train = read_csv(\"data/training.csv\")\nprint(train.count())\n", "intent": "Load the training data from the local file. This model will make use of a broken out dev set, so it won't need the test data.\n"}
{"snippet": "new_train = read_csv(\"data/training.csv\")\nall_data, all_labels = load(new_train, complete=False)\n", "intent": "Finding which images have points closest to the edges.\n"}
{"snippet": "PATH_TO_DATA = ('.')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n                                                random_state=2)\nprint(Xtrain.shape, Xtest.shape)\n", "intent": "Let's try a classification task on the digits. The first thing we'll want to do is split the digits into a training and testing sample:\n"}
{"snippet": "import pandas as pd\nbus = pd.read_csv(\"data/san_francisco/businesses.csv\", \n                             encoding='ISO-8859-1')\nins = pd.read_csv(\"data/san_francisco/inspections.csv\")\nvio = pd.read_csv(\"data/san_francisco/violations.csv\")\n", "intent": "One of the useful features of Pandas is its ability to load fairly messy \"CSV\" files:\n"}
{"snippet": "feature_weights = pd.DataFrame({\n        'features': training_data.drop('latest_score', axis=1).columns, \n        'weights': model.coef_})\nprint(feature_weights.sort_values('weights'))\n", "intent": "The most negative coefficient determine the the violations that most contributed to a reduction in score.\n"}
{"snippet": "import statsmodels.api as sm\nfrom statsmodels.formula.api import logit, glm, ols\ndat = pd.DataFrame(data, columns = ['Temperature', 'Failure'])\nlogit_model = logit('Failure ~ Temperature',dat).fit()\nprint logit_model.summary()\n", "intent": "Lets plot this data\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\nX = digits.data\ny = digits.target\n", "intent": "Load up the digits dataset, and create matrix X and vector y containing the predictor and response variables\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y)\n", "intent": "Break up the dataset into a training and testing set\n"}
{"snippet": "cv = CountVectorizer()\nX = cv.fit_transform(df.all_data)\n", "intent": "Now, lets try a basic CountVectorizer\n"}
{"snippet": "cv = CountVectorizer(stop_words='english')\nX = cv.fit_transform(df.all_data)\n", "intent": "We saw there was limited success there. We should enhance it by removing stop words\n"}
{"snippet": "cv = TfidfVectorizer(stop_words='english')\nX = cv.fit_transform(df.all_data)\n", "intent": "Perhaps we can try weighing words inversly by their frequency\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n", "intent": "Split the data to get going\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(xData, yData, test_size=0.3, random_state=0)\nmodel2 = LogisticRegression()\nmodel2.fit(X_train, y_train)\n", "intent": "So far, we have trained and tested on the same set. Let's instead split the data into a training set and a testing set.\n"}
{"snippet": "heroes[\"attack_type\"] = LabelEncoder().fit_transform(heroes[\"attack_type\"])\nheroes[\"primary_attr\"] = LabelEncoder().fit_transform(heroes[\"primary_attr\"])\nhero_categorical_features = [\"attack_type\", \"primary_attr\"]\n", "intent": "*Prepare categorical features*\n"}
{"snippet": "for i in range(1,6):\n    for team in [\"r\", \"d\"]:\n        for feature in [\"hero_id\"]:\n            fights[\"%s%s_%s\" % (team, i, feature)] = fights[\"%s%s_%s\" % (team, i, feature)].fillna(200)\n            fights[\"%s%s_%s\" % (team, i, feature)] = fights[\"%s%s_%s\" % (team, i, feature)].astype(\"uint8\")\n", "intent": "*Replace NaN and Float to Int32*\n"}
{"snippet": "for team in [\"radiant\", \"dire\"]:\n    player_features = [\"%s_%s_mean\" % (team, feature) for feature  in players_characteristics]\n    poly = PolynomialFeatures(2)\n    player_features_values = poly.fit_transform(fights[player_features].fillna(0))\n    player_features_names = [\"%s_feature_%s\" % (team, j) for j in range(player_features_values.shape[1])]\n    player_features_df = pd.DataFrame(player_features_values, columns=player_features_names)\n    fights = pd.concat([fights, player_features_df], axis=1)\n", "intent": "*Calculate polynom*\n"}
{"snippet": "for i in range(1,6):\n    for team in [\"r\", \"d\"]:\n        for feature in [\"hero\", \"items\", \"role\"]:\n            fights[\"%s%s_%s\" % (team, i, feature)] = fights[\"%s%s_%s\" % (team, i, feature)].fillna(200)\n            fights[\"%s%s_%s\" % (team, i, feature)] = fights[\"%s%s_%s\" % (team, i, feature)].astype(\"uint8\")\n", "intent": "*Replace NaN and Float to Int32*\n"}
{"snippet": "import numpy as np\nfrom sklearn import preprocessing\nlabels = np.array([1, 5, 3, 2, 1, 4, 2, 1, 3])\nlb = preprocessing.LabelBinarizer()\nlb.fit(labels)\nlb.transform(labels)\n", "intent": "Transforming your labels into one-hot encode vectors is pretty simple with scikit-learn.\n"}
{"snippet": "california_housing_dataframe = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\n", "intent": "Next, we'll load our data set.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n", "intent": "So, we want to have 2 datasets, train and test, each used for the named purpose exclusively.\n"}
{"snippet": "min_max_scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-1, 1))\nfeatures_scaled = min_max_scaler.fit_transform(features)\nprint(features_scaled.shape)\nprint(features_scaled.min(axis=0))\nprint(features_scaled.max(axis=0))\n", "intent": "Scale the features from -1 to 1\n"}
{"snippet": "print(\"Size of training set before: \", x_train.shape)\n(x_train, y_train), (x_test, y_test) = boston_housing.load_data(test_split=0.10)\nprint(\"Size of training set after: \", x_train.shape)\n", "intent": "You can also choose the proportion of training data you would like.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=1, stratify=y)\n", "intent": "Splitting data into 70% training and 30% test data:\n"}
{"snippet": "iris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\n", "intent": "First load the iris dataset into a pandas dataframe.\n"}
{"snippet": "pca = PCA(n_components=36)\npca.fit(X_train)\n", "intent": "It seems like the largest eigenvalues are roughly the first 40. We will use 36 prinicple components so the number is square like the original data.\n"}
{"snippet": "X_train_inv = pca.inverse_transform(X_train_pca)\n", "intent": "The data after PCA is not as interpretable because each\n"}
{"snippet": "specObj = Table.read(path.join(data_path, 'sdss', 'specObj-merged.hdf5'), \n                     path='specObj')\nspec_class = specObj['CLASS'].astype(str)\nspec_classes = np.unique(spec_class)\nfor cls in spec_classes:\n    print(cls, (spec_class == cls).sum())\n", "intent": "Let's take a look at how the spectral data was classified by SDSS into galaxies, QSOs, or stars\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\nnewsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\nnewsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)\n", "intent": "The dataset we'll use is the 20newsgroup dataset that is available from sklearn. This dataset has news articles grouped into 20 news categories\n"}
{"snippet": "import pandas as pd\nstemmer = SnowballStemmer(\"english\")\noriginal_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n           'traditional', 'reference', 'colonizer','plotted']\nsingles = [stemmer.stem(plural) for plural in original_words]\npd.DataFrame(data={'original word':original_words, 'stemmed':singles })\n", "intent": "Let's also look at a stemming example. Let's throw a number of words at the stemmer and see how it deals with each one:\n"}
{"snippet": "label_df = pd.read_csv(label_csv)\nlabel_df.head()\n", "intent": "In this data set the labels associated with each image are stored as a csv file\n"}
{"snippet": "label_df.pivot_table(index='breed', aggfunc=len).sort_values('id', ascending=False)\n", "intent": "Lets get an idea of class representation in the dataset\n"}
{"snippet": "label_df = pd.read_csv(f'{PATH}train.csv')\nlabel_df.head()\n", "intent": "The \"test\" and \"train\" folders contain images for the test set and training set. Lets look at the format the train.csv file:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n", "intent": "Standardizing the features:\n"}
{"snippet": "probs_embedded = TSNE(n_components=2).fit_transform(probs)\n", "intent": "Now to use TSNE to decompose the data into two dimensions\n"}
{"snippet": "test = pd.read_csv('test.csv')\ntrain = pd.read_csv('train.csv')\nfull_set = train.append(test, ignore_index=True, sort=True)\ndata = full_set.copy()\ndata = data.drop('PassengerId',1)\n", "intent": "Data is provided by Kaggle, available at https://www.kaggle.com/c/titanic/data\n"}
{"snippet": "family = pd.DataFrame()\nfamily[ 'FamilySize' ] = full_set[ 'Parch' ] + full_set[ 'SibSp' ] + 1\n", "intent": "Both these values refer to the family size of the passenger on board, so lets wrap them up into a single variable.\n"}
{"snippet": "movies = pd.read_csv(path+'movies.csv')\nmovies.head()\n", "intent": "We can also connect movie Ids to movie titles.\n"}
{"snippet": "pca2 = PCA(n_components=2)\nmovie_cluster = pca2.fit_transform(movie_emb)\nmovie_cluster.shape\n", "intent": "We can also use PCA to dimension reduce and cluster the movie values themselves instead of looking at principal components.\n"}
{"snippet": "probs_trans = manifold.TSNE(n_components=2, perplexity=15).fit_transform(preds)\n", "intent": "We can perform t-SNE on our model's output vectors. As these vectors are from the final classification, we would expect them to cluster well.\n"}
{"snippet": "df = pd.read_csv(path/'train.csv')\ndf.head()\n", "intent": "The dataset in question contains text quenstions from Quora. Questions are labeled as either sincere or insincere.\n"}
{"snippet": "class_desc = pd.read_csv(f'{PATH}/challenge-2018-class-descriptions-500.csv', header=None, names=['Code', 'Class'])\nclass_desc.head()\n", "intent": "Classes in this data set are all represented by a code. There are 500 classes in the entire data set.\n"}
{"snippet": "newDF = pd.DataFrame(scaled_features,columns=df.columns[:-1])\nnewDF.info()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df_new = pd.read_csv('./data/bank-marketing-data/bank-unseen-data.csv')\n", "intent": "Load new data from './data/bank-marketing-data/bank-unseen-data.csv'\n"}
{"snippet": "print(\"total variance:\", np.sum(np.var(X,0)))\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X)\nprint(\"variance explained via the first and second components:\\n\" , pca.explained_variance_)\nprint(\"principal components:\\n\", pca.components_)\n", "intent": "(in this particular example ~97.6% of the variance is preserved if we project down to the leading principal component)\n"}
{"snippet": "data=pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/Bonus/titanic3.csv\").dropna(subset=['age'])\ntarget = data['survived']\nX = data[['age', 'sex', 'pclass', 'sibsp', 'parch']]\nX = pd.get_dummies(X)\nX_train, X_test, target_train, target_test = train_test_split(\n    X, target, test_size=0.25, random_state=1\n)\n", "intent": "Consider the Titanic dataset below\n"}
{"snippet": "bcs = pd.read_csv('https://github.com/estimand/teaching-datasets/raw/master/british-crime-survey/bcs.csv')\n", "intent": "Read in the British Crime Survey 2007-2008 dataset.\n"}
{"snippet": "whites = pd.read_csv(WHITES_URL, sep=';')\n", "intent": "Read in the Wine Quality dataset.\n"}
{"snippet": "iris = pd.read_csv(IRIS_URL, header=None, names=var_names)\n", "intent": "Read in the Iris dataset.\n"}
{"snippet": "crypto = pd.read_csv('https://raw.githubusercontent.com/estimand/teaching-datasets/master/cryptocurrencies/cryptocurrencies.csv')\n", "intent": "Read in the cryptocurrencies dataset.\n"}
{"snippet": "crypto_close = crypto.pivot_table(values='close', index='date', columns='symbol')\n", "intent": "Pivot close prices.\n"}
{"snippet": "boroughs = pd.read_csv(BOROUGHS_URL, encoding='iso-8859-1')\n", "intent": "Read in the London Borough Profiles datasets.\n"}
{"snippet": "boroughs_mds = smds.fit_transform(boroughs)\n", "intent": "Two-dimensional projection ('embedding') of 'boroughs'\n"}
{"snippet": "url = 'https://gist.githubusercontent.com/podopie/5ea0c35ecc556d6cbae3/raw/c56f694bf4e7bbeeec92e24d33a8f49f7da37be8/mammals.csv'\nanimals = pd.read_csv(url)\nprint animals.describe()\n", "intent": "Here we'll work with a very simple data set of one input (animal body weight) to find the relationship with a response (animal brain weight)\n"}
{"snippet": "h2020 = pd.read_csv(H2020_URL, sep=';', decimal=',')\n", "intent": "Read in the H2020 dataset.\n"}
{"snippet": "vectorizer = fe.text.CountVectorizer(\n    stop_words='english',\n    ngram_range=(1, 2),\n    min_df=5\n)\n", "intent": "Count words and 2-grams (combinations of two words) in the 'objective', keeping only those that occur at least 5 times.\n"}
{"snippet": "importances = pd.DataFrame({\n    'variable': vectorizer.get_feature_names() + ['totalCost'] + list(country_dummies.columns),\n    'importance': rf1.feature_importances_\n})\nimportances.sort_values('importance', ascending=False, inplace=True)\nimportances.head(10)\n", "intent": "Extract variable importances and sort in descending order.\n"}
{"snippet": "X_tfidf = vectorizer.fit_transform(h2020.objective)\nX_tfidf = hstack([X_tfidf, np.asmatrix(h2020.totalCost).T, country_dummies])\n", "intent": "Prepare the data (as above).\n"}
{"snippet": "importances = pd.DataFrame({\n    'variable': vectorizer.get_feature_names() + ['totalCost'] + list(country_dummies.columns),\n    'importance': rf2.feature_importances_\n})\nimportances.sort_values('importance', ascending=False, inplace=True)\nimportances.head(10)\n", "intent": "Extract variable importances and sort in descending order.\n"}
{"snippet": "vectorizer = fe.text.CountVectorizer(\n    stop_words='english',\n    min_df=5\n)\nX = vectorizer.fit_transform(h2020.objective)\n", "intent": "Count words in the 'objective', keeping only those that occur at least 5 times.\n"}
{"snippet": "with ZipFile(io.BytesIO(requests.get(SMS_URL).content)) as zf:\n    sms = pd.read_table(zf.open('SMSSpamCollection'), names=['class', 'text'])\n", "intent": "Read in the SMS spam dataset.\n"}
{"snippet": "def pd_centers(featuresUsed, centers):\n    colNames = list(featuresUsed)\n    colNames.append('prediction')\n    Z = [np.append(A, index) for index, A in enumerate(centers)]\n    P = pd.DataFrame(Z, columns=colNames)\n    P['prediction'] = P['prediction'].astype(int)\n    return P\n", "intent": "Let us first create some utility functions which will help us in plotting graphs:\n"}
{"snippet": "train_data = pd.read_csv('./train.csv', parse_dates=['datetime'])\ntest_data = pd.read_csv('./test.csv', parse_dates=['datetime'])\ntest_count = pd.read_csv('./test_solution.csv', parse_dates=['datetime'])\ntrain_data.head()\n", "intent": "The bike dataset is devided into training and test sets. The former is used to learn the model, whereas the later is left for evaluation.\n"}
{"snippet": "url = 'https://gist.githubusercontent.com/podopie/5ea0c35ecc556d6cbae3/raw/c56f694bf4e7bbeeec92e24d33a8f49f7da37be8/mammals.csv'\nanimals = pd.read_csv(url)\nanimals['body'].values\n", "intent": "Here we'll work with a very simple data set of one input (animal body weight) to find the relationship with a response (animal brain weight)\n"}
{"snippet": "pcask = PCA(n_components=3)\nY = pcask.fit_transform(xStand) \nprint(pcask.fit(xStand).components_) \n", "intent": "Now that we have discovered the principal componants, we have an educated idea on how many componants to pass to the function.\n"}
{"snippet": "X_piv = pd.pivot_table(X_fin, index='full_name', aggfunc=np.mean).drop(['season', 'max_pt_season_bin'], axis=1)\ny_piv = pd.pivot_table(y, index='full_name', aggfunc=np.mean).drop(['season'], axis=1)\ndraft_eligible_17 = pd.pivot_table(draft_eligible_17, index='full_name', aggfunc=np.mean).drop(['season', 'max_pt_season_bin'], axis=1, errors='ignore')\nlen(X_piv), len(y_piv), len(draft_eligible_17)\n", "intent": "This will allow us to see a players full body of work and use an average of those metrics. As you can see it ends up to about \n"}
{"snippet": "df = pd.read_csv(\"../assets/datasets/votes.csv\")\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "df.to_csv(\"votes_corrected.csv\")\n", "intent": "Next, let's define the x and y variables: \n"}
{"snippet": "confusion_mat = np.array(confusion_matrix(y_test, predictions, labels = [1,0]))\nconfusion = pd.DataFrame(confusion_mat, index=['over_200k', 'under_200k'],\n                         columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "dataset = load_iris()\ndata =  pd.DataFrame(dataset.data, columns = dataset.feature_names)\ntarget =  pd.Series(dataset.target)\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "results = pd.DataFrame(data = [labels, target], index = [\"predicted\", \"actual\"]).T\nresults['difference'] = results['predicted'] - results['actual']\ncolored_error = results[results['difference'] != 0]['difference'].abs().map({1: 'b', 2: 'r'})\nerror_size = results['difference'].abs().map({0: 0, 1: 100, 2: 100})\n", "intent": "First, go ahead and plot the results of your clustering analysis\n"}
{"snippet": "def nb_cross_validation(x_data = X, y_data = y):\n    train_X, test_X, train_y, test_y = model_selection.train_test_split(X, y)\n    naive_bayes_model.fit(train_X, train_y)\n    return [naive_bayes_model.score(test_X, test_y), naive_bayes_model]\n", "intent": "Define the target and feature set for the test data\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nscaled_X = ss.fit_transform(X)\nprint X.shape\nscaled_X.shape\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\nlabel_encoder = enc.fit(titanic_X[:, 2])\nprint \"Categorical classes:\", label_encoder.classes_\ninteger_classes = label_encoder.transform(label_encoder.classes_)\nprint \"Integer classes:\", integer_classes\nt = label_encoder.transform(titanic_X[:, 2])\ntitanic_X[:, 2] = t\n", "intent": "Class and sex are categorical classes. Sex can be converted to a binary value (0=female,1=male):\n"}
{"snippet": "dir_vec = CountVectorizer(ngram_range=(2,3), strip_accents = 'ascii')\ndir_vec.fit(movies['Actors'])\ndirector_bigrams = pd.DataFrame(dir_vec.transform(movies['Director']).todense(),\n                      columns = dir_vec.get_feature_names())\n", "intent": "These are the most frequently occuring 20 actors to appear in the NYT top 1000 movies.\n"}
{"snippet": "rfecv = RFECV(estimator = DecisionTreeRegressor(), cv = 5, scoring = 'mean_squared_error')\nrfecv.fit(X,y)\nrfecv_cols = X.columns[rfecv.support_]\n", "intent": "To narrow down the number of features to consider when building a tree, I'll use cross-validated recursive feature elimination.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import train_test_split\nX = vectorizer.transform(tomatoes.Phrase)\ny = tomatoes.Sentiment\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5)\nmodel = LogisticRegression(multi_class='multinomial', solver='newton-cg')\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n", "intent": "Next, we transform the individual phrases using our vectorizer, create a train test split, and fit a logistic regression model.\n"}
{"snippet": "X_crime = pd.get_dummies(crime.DayOfWeek)\ny_crime = crime.Category\nX_crime_train, X_crime_test, y_crime_train, y_crime_test = \\\n    train_test_split(X_crime, y_crime, train_size=.05)\nmodel_crime = LogisticRegression(multi_class='multinomial', solver='newton-cg')\nmodel_crime.fit(X_crime_train, y_crime_train)\n", "intent": "Next, we'll do a quick spin through the SF Crime dataset to see another example of visualizing a learned model.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_with_num_words, y, train_size=0.5)\ntree_model_with_num_words = DecisionTreeClassifier()\ntree_model_with_num_words.fit(X_train, y_train)\ntree_model_with_num_words.score(X_test, y_test)\n", "intent": "This has been good to visualize what is going on with the decision tree, but let's see how well we can do if we don't limit the depth.\n"}
{"snippet": "scaler = StandardScaler()\nscaler.fit(customer_features)\ncustomer_sc = scaler.transform(customer_features)\ncustomer_sc_df = pd.DataFrame(customer_sc, columns=customer_features.columns)\nsc_stats = customer_sc_df.describe().T\nsc_stats['skew'] = st.skew(customer_features)\nsc_stats['kurt'] = st.kurtosis(customer_features)\ndisplay(stats)\ndisplay(sc_stats)\n", "intent": "$$Z = \\frac{X-\\mu}{\\sigma}$$\n"}
{"snippet": "iris = load_iris()\n", "intent": "In this step we will load and store the iris data set into a variable.\n"}
{"snippet": "data.pivot_table(index=['platform','release_year'],columns=['editors_choice'],values=['title'],aggfunc='count')\n", "intent": "Does number of games by a platform in a given year have any effect on these awards?\n"}
{"snippet": "data = data.fillna(data.dropna().median())\ndata.shape\n", "intent": "It delete all those rows which contain missing values. Now we take its median and impute values to the null\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(titanic_X, titanic_y, test_size=0.25, random_state=33)\n", "intent": "Separate training and test sets\n"}
{"snippet": "cdf = pd.DataFrame(lm.coef_[0], index=X_train.columns, columns=['coefficients'])\ncdf\n", "intent": "Create a table showing the coefficient (influence) of each of the columns\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n", "intent": "Now let us use scikit-learn to easily transform this dataset\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "df = pd.read_csv('data/Train.zip')\ndf.head(2)\n", "intent": "Putting it all together: creating a matrix of heterogeneous data types.\n"}
{"snippet": "df = pd.read_csv('data/train.csv')\n", "intent": "Load the data: train.csv\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "Create test and training set.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "Create test and training set.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(lowercase = True,token_pattern= \"(?u)\\\\b\\\\w\\\\w+\\\\b\")\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "df = pd.read_csv('College_Data.csv', index_col = 0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "pca = PCA()\npca.fit(data_transposed_scaled)\n", "intent": "Now that all the data has been generated, we can build the PCA model.\n"}
{"snippet": "rf.fit(train[features_to_use], train['final_status'])\nimportances = pd.DataFrame({'feature':train[features_to_use].columns,'importance':np.round(rf.feature_importances_,3)})\nimportances = importances.sort_values(by = 'importance', ascending=False)\nimportances\n", "intent": "Lets do feature selection using Random forest and we are going to choose 34 most important features\n"}
{"snippet": "tfidf = TfidfVectorizer(max_features=150)\n", "intent": "The maximum number of features for tfidf vectorizer would be taken as 150\n"}
{"snippet": "exercise = pd.read_csv('myfitnesspal/Exercise-Summary.csv', index_col = 0)\nmeasurement = pd.read_csv('myfitnesspal/Measurement-Summary.csv', index_col = 0)\nnutrition = pd.read_csv('myfitnesspal/Nutrition-Summary.csv', index_col = 0)\n", "intent": "First I will read the data I have into a pandas dataframes:\n"}
{"snippet": "measurement_agg['Height'] = 1.76  \nmeasurement_agg['BMI'] = np.round(measurement_agg['Weight'] / measurement_agg['Height']**2,2)\nmeasurement_agg['Age'] = np.round((measurement_agg.index - pd.Timestamp('1988-06-07')) / pd.Timedelta(days=365),1)\nmeasurement_agg['dW'] = measurement_agg['Weight'].diff(periods=1)\nmeasurement_agg.at[measurement_agg.index[0],'dW'] = 0\nmeasurement_agg.fillna(value=0, inplace=True)\nmeasurement_agg.head()\n", "intent": "Now let's fill in the data in the rest of the columns:\n"}
{"snippet": "itrain, itest = train_test_split(xrange(df.shape[0]), train_size=0.6)\nmask=np.ones(df.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\n", "intent": "In this lab we'll consider classification but Decision trees can be use for regression (prediction of continuous outcomes) as well.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = pd.read_csv(\"../data/house_prices.csv\")\ndata.head()\n", "intent": "***\n- Now, let's have a look at the data \n - Every row displays the Price and Area of each house\n"}
{"snippet": "labels_df = pd.read_csv(IMAGE_NET_LABELS_PATH, sep='\\\\t', header=None, names=['id','labels'])\nlabels_df.head(5)\n", "intent": "Lettura del file words di ImageNet come PandaDF. A ogni id (cartella che contiene immagini per le classi fornite) vengono assegnati i label\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\ndigits = datasets.load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n", "intent": "In this section we study how different estimators maybe be chained.\n"}
{"snippet": "from sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\npca = PCA(n_components=5)\npca.fit(X_train)\nX_train_reduced = pca.transform(X_train)\nX_test_reduced = pca.transform(X_test)\nlogistic = LogisticRegression()\nlogistic.fit(X_train_reduced, y_train)\nlogistic.score(X_test_reduced, y_test)\n", "intent": "You can apply the dimensionality reduction manually, like so:\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ncategories = ['alt.atheism',\n              'talk.religion.misc',\n              'comp.graphics',\n              'sci.space']\ntwenty_train_subset = fetch_20newsgroups(subset='train', categories=categories)\ntwenty_test_subset = fetch_20newsgroups(subset='test', categories=categories)\n", "intent": "We will take a look at some of the twenty newsgroups dataset, another common dataset for classification. Note that the data is fetched from.\n"}
{"snippet": "s4 = pandas.read_csv('data/spectra_4.csv')\nf = pandas.read_csv('data/freq.csv')\nc4 = s4['concentration']\nm4 = s4['molecule']\ns4 = s4['spectra']\n", "intent": "We are going to use `spectra_4.csv` for testing a model learned on the previous data\n"}
{"snippet": "def read_spectra(path_csv):\n    s = pandas.read_csv(path_csv)\n    c = s['concentration']\n    m = s['molecule']\n    s = s['spectra']\n    x = []\n    for spec in s:\n        x.append(numpy.fromstring(spec[1:-1], sep=','))\n    s = pandas.DataFrame(x)\n    return s, c, m\n", "intent": "We can define a function which will read the data and process them.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,train_size=0.7)\nx_train\n", "intent": "<ul>\n<li><code>train_size</code></li>\n<li><code>test_size</code></li>\n</ul>\n"}
{"snippet": "filedata='SVM_Dataset2.csv'\ndata2=pd.read_csv(filedata)\ndata2\n", "intent": "Now we upload the second dataset.\n"}
{"snippet": "import pandas as pd\npath = 'data/yelp.csv'\nreview = pd.read_csv(path, index_col = 0)\n", "intent": "Read **`yelp.csv`** into a pandas DataFrame and examine it.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nvect.fit(X_train)\n", "intent": "Use CountVectorizer to create **document-term matrices** from X_train and X_test.\n"}
{"snippet": "Y_pred = Y_pred_6\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('titanic.csv', index=False, columns=[\"Survived\", \"PassengerId\"])\n", "intent": "Here is our code to output our solution to a .csv file. Just modify the Y_pred variable to test each classifier. \n"}
{"snippet": "data['Cabin'].fillna('U0', inplace=True)\ndata['CabinSection'] = LabelEncoder().fit_transform(data['Cabin'].map(lambda x: x[0]))\ndata['CabinDistance'] = data['Cabin'].map(lambda x: x[1:])\ndata['CabinDistance'] = data['CabinDistance'].map(lambda x: x.split(' ')[0])\ndata['CabinDistance'].where(data['CabinDistance'] != '', '0', inplace=True)\ndata['CabinDistance'] = data['CabinDistance'].map(lambda x: int(x))\ndata.head()\n", "intent": "In this attempt we include all the cleaning inside one function and included comment to explain what we do in each step\n"}
{"snippet": "data.drop('PassengerId', axis=1, inplace=True)\ntraining_survived = data['Survived'].dropna()\ndata['Survived'].fillna(-1, inplace=True)\n", "intent": "This attemp fo similar data cleaning as the previous attemp. However, we introduce new columns and also standardizing almost all the column.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = CountVectorizer()\nX_train = vectorizer.fit_transform(twenty_train_subset.data)\n", "intent": "Here are some ways to generate features from the text:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0)\n", "intent": "Use different data sets for training and testing a model (generalization)\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n", "intent": "It is better to scale the data so that different features/channels have similar mean/std.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nclass_le = LabelEncoder()\ny = class_le.fit_transform(df['classlabel'].values)\ny\n", "intent": "We can use LabelEncoder in scikit learn to convert class labels automatically.\n"}
{"snippet": "import pandas as pd\ndf_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'\n                      'machine-learning-databases/wine/wine.data',\n                      header=None)\ndf_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n                   'Color intensity', 'Hue',\n                   'OD280/OD315 of diluted wines', 'Proline']\ndf_wine.head()\n", "intent": "Use the wine data set as it has 13 features for dimensionality reduction\n"}
{"snippet": "from distutils.version import LooseVersion as Version\nfrom sklearn import __version__ as sklearn_version\nif Version(sklearn_version) < '0.18':\n    from sklearn.cross_validation import train_test_split\nelse:\n    from sklearn.model_selection import train_test_split\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3, random_state=0)\n", "intent": "Splitting the data into 70% training and 30% test subsets.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA()\nX_train_pca = pca.fit_transform(X_train_std)\npca.explained_variance_ratio_\n", "intent": "PCA is actually part of scikit-learn, so we can use it directly instead of going through the code above.\n"}
{"snippet": "import pandas as pd\nwdbc_source = '../datasets/wdbc/wdbc.data'\ndf = pd.read_csv(wdbc_source, header=None)\n", "intent": "Malignant versus benign tumor cells based on 30 features (large enough for experiments in this chapter)\n"}
{"snippet": "import pandas as pd\nwdbc_source = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\ndf = pd.read_csv(wdbc_source, header=None)\n", "intent": "We will use the Wisconsin breast cancer dataset for the following questions\n"}
{"snippet": "if not os.path.isfile(csv_file):\n    df.to_csv(os.path.join(basepath, csv_filename), index=False, encoding='utf-8')\n", "intent": "Optional: Saving the assembled data as CSV file:\n"}
{"snippet": "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,5))\nX_train = vectorizer.fit_transform(twenty_train_subset.data)\n", "intent": "We can put this together with our other tricks as well.\n"}
{"snippet": "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True) \nraw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1] \nraw_tfidf \n", "intent": "As we can see, the results match the results returned by scikit-learn's `TfidfTransformer` (below).\n"}
{"snippet": "df = pd.read_csv('data/multiTimeline.csv', skiprows=1)\ndf.head()\n", "intent": "* Import data that you downloaded and check out first several rows:\n"}
{"snippet": "from keras.datasets import cifar10\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\nprint(\"Complete\")\n", "intent": "Start by importing the data from the pickle file.\n"}
{"snippet": "iris_data = pd.read_csv('./data/iris.csv') \n", "intent": "We can read the data with the help of pandas using the `read_csv` method\n"}
{"snippet": "f = open(\"data/brief_comments.txt\", \"r\") \ndogs = f.read() \nf.close() \n", "intent": "An alternative way of reading data from a file is to use the `with` statement.\n"}
{"snippet": "dfcars=pd.read_csv(\"data/mtcars.csv\")\ndfcars.head()\n", "intent": "Now let's read in some automatible data as a pandas *dataframe* structure.  \n"}
{"snippet": "dfcars=pd.read_csv(\"data/mtcars.csv\")\ndfcars.head()\n", "intent": "Now let's read in some automobile data as a pandas *dataframe* structure.  \n"}
{"snippet": "import pandas as pd\ndfcars = pd.read_csv(\"data/mtcars.csv\")\ndfcars = dfcars.rename(columns={\"Unnamed: 0\":\"car name\"})\ndfcars.head()\n", "intent": "We begin by loading up the `mtcars` dataset and cleaning it up a little bit.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntraindf, testdf = train_test_split(dfcars, test_size=0.2, random_state=42)\n", "intent": "Next, let's split the dataset into a training set and test set.\n"}
{"snippet": "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\nXtis = vectorizer.fit_transform(df['quote'])\nY = (df['fresh'] == 'fresh').values.astype(np.int8)\nxtraintis, xtesttis, ytraintis, ytesttis = train_test_split(Xtis, Y)\n", "intent": "Do both together help?\n"}
{"snippet": "sample_df = pd.DataFrame(dict(x=x[indexes],f=f[indexes],y=y[indexes])) \nsample_df.head()\n", "intent": "Moving on, let's get the $60$ random samples from our dataset.\n"}
{"snippet": "fold = 0\nfor train, valid in KFold(n_folds, shuffle=True).split(range(48)): \n    for d in degrees:\n        train_set = PolynomialFeatures(d).fit_transform(xtrain[train].reshape(-1,1))\n        valid_set = PolynomialFeatures(d).fit_transform(xtrain[valid].reshape(-1,1))\n        train_errors[d, fold], valid_errors[d, fold] = compute_MSE(train_set, ytrain[train], valid_set, ytrain[valid])\n    fold += 1\n", "intent": "Now let's try to run things and see what we get.\n"}
{"snippet": "wines_df = pd.read_csv(\"data/wines.csv\", index_col=0)\nwines_df.head()\n", "intent": "We do the usual read-in and verification of the data:\n"}
{"snippet": "from IPython.core.display import HTML\ndef css_styling(): styles = open(\"cs109.css\", \"r\").read(); return HTML(styles)\ncss_styling()\n", "intent": "**Harvard University**<br/>\n**Summer 2018**<br/>\n**Instructors**: Pavlos Protopapas, Kevin Rader\n<hr style=\"height:2pt\">\n"}
{"snippet": "dataDIR = './data'\nlogFile = os.path.join(dataDIR, 'driving_log.csv')\nraw_data=pd.read_csv(logFile)\nprint('Number of images per camera in the log file : {}'.format(raw_data.shape[0]))\nraw_data.head(5)\n", "intent": "First we load the log file with the steering angles\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nprint('X_train shape:', X_train.shape)\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\ninput_shape =  (3,64,64)\n", "intent": "I do not use a validation split, but I have to validate that the model works well in localization independently anyway.\n"}
{"snippet": "def get_russian_holidays(year):\n    url = f'https://www.timeanddate.com/holidays/russia/{year}'\n    html = requests.get(url).content\n    table_df = pd.read_html(html)[0]\n    table_df = table_df.rename(columns={'Date': 'date'})\n    holidays = pd.to_datetime(table_df['date'], format='%b %d')\n    holidays = holidays.apply(lambda x: x.replace(year=year))\n    return holidays\n", "intent": "We will here generate the number of holidays in the previous month, the current month and the next month\n"}
{"snippet": "col_names = [f'item_tf_idf_{i}' for i in range(tf_idf_item.shape[1])]\ntf_idf_item_df = pd.DataFrame(tf_idf_item, columns=col_names)\nitem_nlp = pd.concat([item_nlp, tf_idf_item_df], axis=1)\nitem_nlp.drop(['item_name', 'item_category_id', 'item_name_nlp'], axis=1, inplace=True)\n", "intent": "Combine the TF-IDF results with the corresponding data frames\n"}
{"snippet": "on = ['shop_id', 'item_id']\nid_df = pd.merge(sales_train.loc[:, on], sales_test, how='outer', on=['shop_id', 'item_id'])\nid_df.loc[:,'ID'].fillna(-1, inplace=True)\nid_df.loc[:,'ID'] = id_df.loc[:,'ID'].astype('int32')\n", "intent": "**NOTE**: We do an outer join here as some combinations of `shop_id` and `item_id` is only present in the test-set\n"}
{"snippet": "text, Y = zip(*corpus)\ntext, Y = [], []\nfor pair in corpus:\n    text.append(pair[0])\n    Y.append(pair[1])\ntext = tuple(text)\nY = tuple(Y)\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(text)\n", "intent": "Here, we convert the data that came out of my function into the vector representation that we introduced before.\n"}
{"snippet": "best_xg_prediction = test_dt.loc[:, ['ID']]\nbest_xg_prediction.loc[:, 'item_cnt_month'] = xg_lvl_1_pred.clip(0, 20)\nbest_xg_prediction.set_index('ID', inplace=True)\nbest_xg_prediction.to_csv(generated_data.joinpath('best_xg_prediction.csv'))\nbest_xg_prediction.loc[:, 'item_cnt_month'].describe()\n", "intent": "This gives a score of $1.06557$, which is sligthly worse then the previous neural network submission. It is likely that overfitting is the reason.\n"}
{"snippet": "data = pd.read_csv(\"adult_us_postprocessed.csv\")\ndata.head()\n", "intent": "Let's read the dataset. This is a post-processed version of the [UCI Adult dataset](http://archive.ics.uci.edu/ml/datasets/Adult).\n"}
{"snippet": "from IPython.display import clear_output\nfrom tqdm import trange\nfrom pandas import DataFrame\newma = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\nenv_batch = EnvBatch(10)\nbatch_states = env_batch.reset()\nrewards_history = []\nentropy_history = []\n", "intent": "Just the usual - play a bit, compute loss, follow the graidents, repeat a few million times.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nimgs_train, imgs_val, points_train, points_val = train_test_split(imgs, points, test_size=0.1)\n", "intent": "Run the following code to obtain train/validation split for training neural network.\n"}
{"snippet": "sample_size = 200000\ndialogue_df = pd.read_csv(Path('data', 'dialogues.tsv'), sep='\\t').sample(sample_size, random_state=0)\nstackoverflow_df = pd.read_csv(Path('data', 'tagged_posts.tsv'), sep='\\t').sample(sample_size, random_state=0)\n", "intent": "Now, load examples of two classes. Use a subsample of stackoverflow data to balance the classes. You will need the full data later.\n"}
{"snippet": "test = pandas.read_csv('test.csv.gz')\n", "intent": "Select your best classifier and prepare submission file.\n"}
{"snippet": "train_ada = pandas.read_csv('reference/training.csv', sep=',')\ntest_ada = pandas.read_csv('reference/test.csv', sep=',', index_col='id')\n", "intent": "`training.csv` is a mixture of simulated signal, real background.\nIt has the following columns.\n`test.csv` has the following columns:\n"}
{"snippet": "pd.DataFrame(data=forest.feature_importances_,\n             index=list(bcw_train.columns),\n             columns=['importance']).sort_values(by='importance', ascending=False)\n", "intent": "5 - Determine the most important three features in your model, and then fit a new model using only those three features.  Comment on your results.\n"}
{"snippet": "pd.DataFrame(data=treereg.feature_importances_,\n             index=list(Xtrain.columns),\n             columns=['importance']).sort_values(by='importance', ascending=False)\n", "intent": "4 - Determine the top three features, and fit a new decision tree regressor to the data.  Comment on your results.\n"}
{"snippet": "text, Y = zip(*corpus)\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(text)\n", "intent": "How are we doing now?\n"}
{"snippet": "pd.DataFrame(data=forestreg.feature_importances_,\n             index=list(Xtrain.columns),\n             columns=['importance']).sort_values(by='importance', ascending=False)\n", "intent": "8 - Determine the top three features, and fit a new random forest regressor to the data.  Comment on your results.\n"}
{"snippet": "errors = pd.read_csv('./data/errors.csv')\nerrors.head()\n", "intent": "I would use a binomial distribution with $p=0.005$.\n"}
{"snippet": "iris_train = pd.read_csv('./data/iris_train.csv')\niris_test = pd.read_csv('./data/iris_test.csv')\n", "intent": "This is a very common dataset (slightly modified) of three varieties of the iris flower; it will work well for clustering.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nheader = ['user_id', 'item_id', 'rating', 'timestamp']\ndata_movie_raw = pd.read_csv('../data/ml-100k/u.data', sep='\\t', names=header)\ndata_movie_raw.head()\n", "intent": "First familiarize yourself with the data you downloaded, and then import the `u.data` file and take a look at the first few rows.\n"}
{"snippet": "pm2_regr_imp = pm2_regr.dropna(subset=['year', 'month', 'day', 'hour', 'pm2'])\nimp = Imputer(strategy = 'median')\npm2_regr_imp = imp.fit_transform(pm2_regr_imp)\n", "intent": "I'm going to drop rows with NAs for the columns year, month and hour, pm2; I'm imputing the median for all other columns:\n"}
{"snippet": "mms = MinMaxScaler()\nXtrain_norm = mms.fit_transform(Xtrain)\nXtest_norm = mms.transform(Xtest)\nknn.fit(Xtrain_norm, ytrain)\nprint(knn.score(Xtrain_norm, ytrain))\nprint(knn.score(Xtest_norm, ytest))\n", "intent": "Normalized dataset:\n"}
{"snippet": "ssc = StandardScaler()\nXtrain_std = ssc.fit_transform(Xtrain)\nXtest_std = ssc.transform(Xtest)\nknn.fit(Xtrain_std, ytrain)\nprint(knn.score(Xtrain_std, ytrain))\nprint(knn.score(Xtest_std, ytest))\n", "intent": "Standardized dataset:\n"}
{"snippet": "pca = PCA(n_components=1)\npca.fit(X)\nX_pca = pca.transform(X)\nprint(X_pca.mean(axis=0))\nprint(X_pca.var(axis=0))\nprint(np.corrcoef(X_pca.T))\n", "intent": "The two variables are highly correlated, so almost all the variance is explained by the first principal component.\n"}
{"snippet": "pca = PCA(n_components=20)\npca.fit(communities_scaled[:, :-1])\nXtrainpca = pca.transform(Xtrain)\nXtestpca = pca.transform(Xtest)\n", "intent": "In the solutions he has the data scaled, all the features imputed and so he chooses 40 components.\nThe models chosen are linear regression and SVM.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test, images_train, images_test = train_test_split(\n        data, digits.target, digits.images,  test_size=0.25, random_state=42)\nn_samples, n_features = X_train.shape\nn_digits = len(np.unique(y_train))\nlabels = y_train\nprint_digits(images_train, y_train, max_n=20)\nprint_digits(images_test, y_test, max_n=20)\nprint(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n      % (n_digits, n_samples, n_features))\n", "intent": "Build training and test set\n"}
{"snippet": "from sklearn.manifold import Isomap\niso = Isomap(n_components=2)\nmadelon_iso = iso.fit_transform(madelon)\n", "intent": "3 - Perform variable reduction, to two components, via Isomap, and plot the data to show the latent clusters.  Comment on your results.\n"}
{"snippet": "from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2)\nmadelon_tsne = tsne.fit_transform(madelon)\n", "intent": "5 - Perform variable reduction, to two components, via TSNE, and plot the data to show the latent clusters.  Comment on your results.\n"}
{"snippet": "pca = PCA(n_components=2)\nmadelon_pca = pca.fit_transform(madelon)\n", "intent": "6 - Perform variable reduction, to two components, via PCA, and plot the data to show the latent clusters.  Comment on your results.\n"}
{"snippet": "le = LabelEncoder()\nle.fit([2, 4])\nbreast['class'] = le.transform(breast['class'])\n", "intent": "Also, I'm encoding the class column with 0s and 1s:\n"}
{"snippet": "scl = StandardScaler()\nsfm = SelectFromModel(LassoCV())\nXtrain_sc = scl.fit_transform(Xtrain)\nXtest_sc = scl.transform(Xtest)\nXtrain_lm = sfm.fit_transform(Xtrain_sc, ytrain)\nXtest_lm = sfm.transform(Xtest_sc)\ntrain_scores, test_scores = validation_curve(Ridge(), Xtrain_lm, ytrain,\n                                             param_name='alpha',\n                                             param_range=[0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000])\n", "intent": "In solution he uses LassoCV with SelectFromModel:\n"}
{"snippet": "player_batting_cols = ['ab', 'r', 'h', 'double', 'triple', 'hr', 'rbi', 'sb', 'cs', 'bb']\nplayer = pd.read_csv('player.csv')\nbatting = pd.read_csv('batting.csv')\nplayer_batting = pd.merge(left=player, right=batting, left_on='player_id', right_on='player_id')\nplayer_batting = player_batting.loc[player_batting.year >= 2004, ['name_given', 'name_last', 'year'] + player_batting_cols]\n", "intent": "**Correlation doesn't imply causation**\n"}
{"snippet": "person = pd.DataFrame()\nperson['Height'] = [72]\nperson['Weight'] = [188]\nperson\n", "intent": "create a new person for whom we know their feature values but not their gender. Our goal is to predict their gender.\n"}
{"snippet": "from sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import f_classif\nfeat_selector= SelectKBest(score_func=f_classif, k=3)\nX_new= feat_selector.fit_transform(X, y)\nprint(X_new.shape)\nprint(feat_selector.pvalues_)\nprint(feat_selector.scores_)\n", "intent": "(https://en.wikipedia.org/wiki/F-test)\n"}
{"snippet": "from sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import mutual_info_classif\nfeat_selector= SelectKBest(score_func=mutual_info_classif, k=4)\nX_new= feat_selector.fit_transform(X, y)\nprint(X_new.shape)\nprint(feat_selector.pvalues_)\nprint(feat_selector.scores_)\n", "intent": "(http://scikitlearn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html\n"}
{"snippet": "vectorizer = CountVectorizer(max_features=5000)\nXtis = vectorizer.fit_transform(df['quote'])\nY = (df['fresh'] == 'fresh').values.astype(np.int8)\nxtraintis, xtesttis, ytraintis, ytesttis = train_test_split(Xtis, Y)\n", "intent": "Do both together help?\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "Primeiro carregue o dataset. Vamos dar uma roubada e usar o dataset da biblioteca do sklearn.\n"}
{"snippet": "boston_data = pd.DataFrame(boston.data, columns=boston.feature_names)\n", "intent": "Jeito 2 - Com os nomes dos atributos\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = 42)\n", "intent": "Dividir o dataset em treino e teste\n"}
{"snippet": "Galton = pd.read_csv('C:/Users/crrodr/Documents/1. Conhecimento/Tera - Data Science/TURMA_4_201808/Regressao_Linear/Exemplo_Galton_simples/Galton.csv')\n", "intent": "importando dados do Galton\n"}
{"snippet": "x, y = utils.load_dataset('default')\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, stratify=y, random_state=1)\n", "intent": "Vamos usar o `VotingClassifier` do `sklearn` para compor alguns classificadores diferentes.\n"}
{"snippet": "xbase, xmeta, ybase, ymeta = train_test_split(xtrain, ytrain, test_size=0.5, random_state=1)\n", "intent": "Agora precisamos fazer um novo split no conjunto de treino para reservar dados para nosso meta-learner.\n"}
{"snippet": "lemmas_df = pd.DataFrame(lemmas, columns={'token'})\n", "intent": "Vamos construir um *dataframe* com os tokens *lemmetizados*.\n"}
{"snippet": "train_df, test_df = train_test_split(train_set, test_size=0.3)\n", "intent": "Vamos dividir nossa base em **treino** e **teste**.\n"}
{"snippet": "df_route = pd.read_csv(os.path.join(DATASET_FOLDER, 'route_clustering_elo7_dataset.csv'), sep=';')\ndf_route.head()\n", "intent": "Vamos tentar aplicar o mesmo algoritmo para o problema de cluster de frete.\n"}
{"snippet": "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\nXtis = vectorizer.fit_transform(df['quote'])\nY = (df['fresh'] == 'fresh').values.astype(np.int8)\nxtraintis, xtesttis, ytraintis, ytesttis = train_test_split(Xtis, Y)\n", "intent": "Do both together help?\n"}
{"snippet": "X = tfidf.fit_transform(df_reason['reason'].values)\n", "intent": "Cria a matriz de embeddings.\n"}
{"snippet": "df = pd.DataFrame({'reason': df_reason['reason'], 'labels': labels})\ndf.head()\n", "intent": "Vamos agora visualizar os clusters criados.\n"}
{"snippet": "pca = PCA()\n", "intent": "Vamos aplicar o PCA e verificar o resultado.\n"}
{"snippet": "scaled_feat = scaler.fit_transform(data.drop('Class', axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "scaled_data = pd.DataFrame(data=scaled_feat)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data = pd.read_csv('College_Data')\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "scaled_feat = scaler.fit_transform(data.drop('TARGET CLASS', axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "scaled_df = pd.DataFrame(scaled_feat, columns=data.columns.drop('TARGET CLASS'))\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X = countVec.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "ds = pd.read_csv('./train.csv')\ndata = ds.values\nprint data.shape\n", "intent": "1. GPU (Nvidia)\n2. Cuda (8.0 with ubuntu 16) [install from .run or .deb file]\n3. NVCC\n4. Theano\nwebsite: floydhub.com\n"}
{"snippet": "df = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n", "intent": "Let's create the DataFrame\n"}
{"snippet": "X = scaled_df\ny = cancer['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n", "intent": "Following we'll do the same train/test split for all the algorithms\n"}
{"snippet": "yelp = pd.read_csv('C://Users//Sai//Desktop//floyd//Natural-Language-Processing/yelp.csv')\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "env = pd.read_csv('data/ces3results_environment.csv')\ndemog = pd.read_csv('data/ces3results_demographics.csv')\n", "intent": "---\n<br>\nNow that we have a solid understanding of how SVM works, let's begin applying our classroom knowledge on real-life data. \n"}
{"snippet": "airquality_path = Path('annual_conc_by_monitor_2017.zip')\nzf = zipfile.ZipFile(airquality_path, 'r')\nf_name = 'annual_conc_by_monitor_2017.csv'\nwith zf.open(f_name) as fh:\n    annual_2017 = pd.read_csv(fh, low_memory=False)\nprint(annual_2017.columns)\nannual_2017\n", "intent": "Let's try to get a sense of what our data looks like. Run the next cell to see the 2017 dataset.\n"}
{"snippet": "pm25_ca = pd.read_csv('data/pm25_ca.csv', low_memory=False)\npm25_ca.shape\n", "intent": "This is what our resulting annual California PM2.5 dataset looks like.\n"}
{"snippet": "airquality_path = Path('annual_conc_by_monitor_2017.zip')\nzf = zipfile.ZipFile(airquality_path, 'r')\nf_name = 'annual_conc_by_monitor_2017.csv'\nwith zf.open(f_name) as fh:\n    annual_2017 = pd.read_csv(fh, low_memory=False)\nprint(annual_2017.columns)\n", "intent": "Let's try to get a sense of what our data looks like. Run the next cell to see the 2017 dataset.\n"}
{"snippet": "pm25_ca = pd.read_csv('pm25_ca.csv', low_memory=False)\npm25_ca.head()\n", "intent": "This is what our resulting annual California PM2.5 dataset looks like.\n"}
{"snippet": "import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', None)\nchurn_df = pd.read_csv('data/churn.all')\n", "intent": "<ul>\n<li>Data Source: https://www.sgi.com/tech/mlc/db/churn.all\n<li>Data info: https://www.sgi.com/tech/mlc/db/churn.names\n</ul>\n"}
{"snippet": "env = pd.read_csv('ces3results_environment.csv')\ndemog = pd.read_csv('ces3results_demographics.csv')\n", "intent": "Let's import the environmental and demographic datasets from CES:\n"}
{"snippet": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nm_train = pd.read_csv(\"mod_train.csv\", encoding = \"ISO-8859-1\", index_col=0)\nprint(m_train.dtypes,\"\\n\")\nprint(m_train.shape,\"\\n\")\nprint(m_train.label.value_counts(),\"\\n\")\n", "intent": "Lets recall the structure of our modified train data \n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nstandardize_data = StandardScaler().fit_transform(data)\nprint(standardize_data.shape)\n", "intent": "Before applying PCA we need to standardize the data set\n"}
{"snippet": "pd.Series(le.inverse_transform(y)).value_counts().sort_index()\n", "intent": "http://www.webgraphviz.com\n"}
{"snippet": "data = pd.read_csv( 'nba_2013.csv' )\ndata.head()\n", "intent": "pos:\n- SF small forward\n- C center\n- PF power forward\n- SG shooting guard\n- PG point guard\n- G guard\n- F forward\n"}
{"snippet": "from sklearn.svm import SVC\npoly_kernel_svm_clf = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n])\npoly_kernel_svm_clf.fit(X, y)\n", "intent": "$K(\\mathbf{a}, \\mathbf{b}) = (\\gamma\\mathbf{a}^T \\cdot \\mathbf{b} + r)^d$\n"}
{"snippet": "actual = [row[1:] for row in test]\nactual = inverse_transform(df, actual, scaler, n_test + 2)\n", "intent": "Get the actual test target and transform the data to their original scale\n"}
{"snippet": "import pandas as pd\nmovies_df = pd.DataFrame(movies)\nmovies_df['year'].hist()\n", "intent": "Now let's do this with pandas\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX = representatives[range(1,17)]\nY = representatives[0]\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=.30, random_state=4444)\n", "intent": "Split the data into a test and training set. Use this function: `from sklearn.cross_validation import train_test_split`\n"}
{"snippet": "from sklearn.feature_selection import RFE\nrfe_l1 = RFE(LRmodel_l1, n_features_to_select=1) \nrfe_l1.fit(X, y)\nprint(\"Logistic Regression (L1) RFE Result\")\ndf = pd.DataFrame({'name' :churn_feat_space.columns, 'rank' :rfe_l1.ranking_}, columns=['name', 'rank'])\ndf.sort_values(by='rank', ascending= True).reset_index()\n", "intent": "The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer \ncv = CountVectorizer()\ntrain_features = cv.fit_transform(posts_frame['text']).toarray() \ntrain_frame = posts_frame.join(pd.DataFrame(train_features, columns=cv.get_feature_names())) \ntrain_frame.drop(['likes_number','text'],inplace=True,axis=1,errors='ignore') \ntrain_frame\n", "intent": "Creating object-feature matrix\n"}
{"snippet": "train_frame.to_csv('traindata.csv')\n", "intent": "Saving train frame to file\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer \ncv = TfidfVectorizer(norm='l1', max_features = 200, analyzer = 'word', strip_accents='unicode', binary=True)\ntrain_features = cv.fit_transform(posts_frame['post_text']).toarray() \ntrain_frame = posts_frame.join(pd.DataFrame(train_features, columns=cv.get_feature_names())) \nall_data = train_frame.copy()\nvalue_frame = train_frame['likes_number']\ntrain_frame.drop(['likes_number','post_text'],inplace=True,axis=1,errors='ignore') \n", "intent": "Creating object-feature matrix\n"}
{"snippet": "scaler = StandardScaler()\ntrain_frame = scaler.fit_transform(train_frame)\n", "intent": "Scaling our features\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_frame, value_frame, test_size=0.3, random_state=42)\n", "intent": "Splitting into train and test samples\n"}
{"snippet": "X = data[\"review\"]\ny = data[\"label\"]\nclass_name = np.unique(y)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "You may use whatever settings you like. To compare your results to the solution notebook, use `test_size=0.33, random_state=42`\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\ntext_clf = Pipeline([(\"tfidf\",TfidfVectorizer()),(\"svc\",LinearSVC())])\ntext_clf.fit(X_train,y_train)\n", "intent": "You may use whatever model you like. To compare your results to the solution notebook, use `LinearSVC`.\n"}
{"snippet": "X_train,X_test,Y_train,Y_test = train_test_split(X,Y)\nlog_model2 = LogisticRegression()\nlog_model2.fit(X_train,Y_train)\n", "intent": "Testing and Training data\n"}
{"snippet": "df_scaled = pd.DataFrame(scaler.transform(df.drop(\"Class\",axis=1)),columns=df.drop(\"Class\",axis=1).columns)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.feature_selection import RFE\nrfe_l1 = RFE(LRmodel_l1, n_features_to_select=1) \nrfe_l1.fit(X, y)\nprint \"Logistic Regression (L1) RFE Result\"\nfor k,v in sorted(zip(map(lambda x: round(x, 4), rfe_l1.ranking_), churn_feat_space.columns)):\n    print v + \": \" + str(k)\n", "intent": "The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features.\n"}
{"snippet": "cm = pd.DataFrame(confusion_matrix(y_test,predicted_y),columns= class_names)\nprint(cm)\n", "intent": "**Show the Confusion Matrix for the predictions.**\n"}
{"snippet": "df = pd.read_csv(\"./KNN_Project_Data\")\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaler_feature = pd.DataFrame(scaler.transform(df.drop(\"TARGET CLASS\",axis=1)),columns=df.drop(\"TARGET CLASS\",axis=1).columns)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "yelp = pd.read_csv(\"./yelp.csv\")\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(yelp_class[\"text\"],yelp_class[\"stars\"], test_size=0.3,random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "user_engagement_df = pd.read_csv('takehome_user_engagement.csv',encoding=\"utf-8\")\nuser_engagement_df.head()\n", "intent": "We begin by reading in both datasets and displaying the first several rows of data.   \n"}
{"snippet": "car_gears = pd.DataFrame(car_gears)\ncar_gears.columns = ['vehicle_id','labels']\n", "intent": "Adding disturbing records\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\ndata.head(13)\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "gen = np.random.RandomState(seed=123)\ndata = pd.DataFrame()\ndata['x'] = gen.uniform(-1, +1, 50)\ndata['y'] = np.zeros_like(data['x'])\nsns.jointplot('x', 'y', data, stat_func=None, size=5);\n", "intent": "As an extreme example, consider the following 2D data which is effectively 1D since one feature has a constant value (zero):\n"}
{"snippet": "(feature_train, feature_test,\n target_train, target_test) = cross_validation.train_test_split(X, target, test_size=.25)\n", "intent": "We are going to use train_test_split to split the data up into training, testing, and validation sets\n"}
{"snippet": "df_img_train, df_img_test, nsrc_true_train, nsrc_true_test = model_selection.train_test_split(\n    df_img, nsrc_true, test_size=0.25, random_state=123)\ndf_iso_train, df_iso_test, e_true_train, e_true_test = model_selection.train_test_split(\n    df_iso, e_true, test_size=0.25, random_state=123)\n", "intent": "Split our image and label datasets using the sklearn default 25% test fraction. Note the use of `random_state` to ensure reproducible \"randomness\":\n"}
{"snippet": "assert np.allclose(e_true_test, e_scaler.inverse_transform(e_scaled_test))\n", "intent": "Note that a scaler transformation is invertible, which is useful for interpreting scaled predictions:\n"}
{"snippet": "iso_scaler = preprocessing.StandardScaler().fit(df_iso_train)\ndf_iso_train_scaled = iso_scaler.transform(df_iso_train)\ndf_iso_test_scaled = iso_scaler.transform(df_iso_test)\n", "intent": "Perform the same normalization on the subset of images with a single source:\n"}
{"snippet": "df.Age = df.Age.fillna(df.Title.map(title_to_age))\n", "intent": "with title_to_age value (the mean for that title)\n"}
{"snippet": "df_feat = pd.DataFrame(scaler_features,columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "OzoneImputeMean=OzoneData.copy()\nOzoneImputeMean['Ozone'].fillna(value = np.mean(OzoneImputeMean['Ozone']), inplace = True)\nOzoneImputeMean['Solar.R'].fillna(value = np.mean(OzoneImputeMean['Solar.R']), inplace = True)\n", "intent": "Hint: copy OzoneData this way:\nOzoneImputeMean = OzoneData.copy()\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler() \nXn = ss.fit_transform(X)\n", "intent": "Now we standardize our Xs\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(Xn, y, test_size=0.3, random_state=10)\nprint Xtrain.shape, Xtest.shape\nprint \"\\n======\\n\"\nprint ytrain.shape, ytest.shape\n", "intent": "Creating Train and Split Samples\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(Xn, y, test_size=0.3, random_state=10)\nprint X_train.shape, X_test.shape\nprint \"\\n======\\n\"\nprint y_train.shape, y_test.shape\n", "intent": "---\ntest size = 30% and random_state = 10\n"}
{"snippet": "(feature_train, feature_validate,\n target_train, target_validate) = cross_validation.train_test_split(feature_train, \n                                                                    target_train, \n                                                                    test_size=.33)\n", "intent": "We can now split the training data into training and validation sets \n"}
{"snippet": "Universities = pd.read_csv('College_Data',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "data_feat = pd.DataFrame(scaled_features,columns=data.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\ntext = open(path).read().lower()\nprint('corpus length:', len(text))\n", "intent": "Nietzsche's Complete works is publicly available on Amazon:\n"}
{"snippet": "pd.DataFrame(sorted(zip(Xt.columns, abs(estimator.coef_[0])), key=lambda (k, v): (abs(v), k), reverse=True)).plot(kind='bar')\n", "intent": "Use a bar chart to display the logistic regression coefficients. Start from the most negative on the left.\n"}
{"snippet": "df_new = pd.DataFrame(data=df, columns=['workclass_num', 'education-num', 'hours-per-week', 'income_num'])\ndf_new2 = pd.DataFrame(data=df, columns=['workclass_num', 'education-num'])\ndf_new.head(5)\n", "intent": "Create a New Dataframe with just numerical data for the analysis\n"}
{"snippet": "normalizer(big_table.Comments[4])\n", "intent": "Test out your normalizer on a comment to ensure it's working. \n"}
{"snippet": "fare_pipe = Pipeline([('selector', ColumnSelector('Fare')),\n                      ('scaler', StandardScaler())])\nfare_pipe.fit_transform(df)\n", "intent": "The `Fare` attribute can be scaled using one of the scalers from the preprocessing module. \n"}
{"snippet": "from sklearn.pipeline import make_union\nunion = make_union(age_pipe,\n                   one_hot_pipe,\n                   gender_pipe,\n                   fare_pipe)\nunion.fit_transform(df.head())\n", "intent": "Use the `make_union` function from the `sklearn.pipeline` modeule to combine all the pipes you have created.\n"}
{"snippet": "data, target = datasets.load_iris(return_X_y=True)\niris = datasets.load_iris()\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "* Let's look again at the Iris datasets\n"}
{"snippet": "sac = pd.read_csv('CSV/Sacramentorealestatetransactions.csv')\nsac = sac.copy()\nsac.head()\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, Y_pred))\nconfusion = pd.DataFrame(conmat, index=['under_200k', 'over_200k'],\n                         columns=['predicted_under_200k', 'predicted_over_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "import pandas\nwheel = pandas.read_csv('CSV/wheel.csv')\nwheel\n", "intent": "The data is in wheel.csv.\nWe would like to understand the relationship between _seconds_ and _signal_\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n", "intent": "Split the data in the ordinary way, making sure you have a 70/30 split.\n"}
{"snippet": "votes = pd.read_csv('../CSV/votes.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "PCA_sk = PCA(n_components=16)\nY_sk = PCA_sk.fit_transform(X_standard)\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "PCA_sk = PCA(n_components=7)\nY_sk = PCA_sk.fit_transform(X_standard)\n", "intent": "Finally, conduct the PCA - use the results about to guide your selection of \"n\" componants\n"}
{"snippet": "airports = pd.read_csv('../CSV/Airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "new_X = pd.DataFrame(Y_sk, columns=['PC1', 'PC2'])\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "scaler = preprocessing.StandardScaler().fit(data)\n", "intent": "* Doing the pre-processing this way allows the *same* transform to be applied to different data later\n"}
{"snippet": "X_standard = StandardScaler().fit_transform(X)\n", "intent": "Since we don't know what units our data was measured in, let's standardize \"x\"\n"}
{"snippet": "item_sum = pd.DataFrame(item_sum)\nitem_sum.index.name = ''\nitem_sum['item_id'] = item_sum.index.values\nitem_sum['First_letter'] = item_sum.index.to_series().map(items.item_name.apply(lambda x: x[0]))\n", "intent": "From .head() and .tail() we see that item id sorted by alphabet. What if it is depend on frequency of letters?\n"}
{"snippet": "data = {\n    'province': ['BXL', 'BXL', 'BXL', 'ANT', 'ANT', 'ANT'],\n    'year': [2013, 2014, 2015, 2013, 2014, 2015],\n    'customers': [150000, 200000, 250000, 200000, 150000, 100000]\n}\nframe = DataFrame(data, columns=['year', 'province', 'customers'])\nframe\n", "intent": "**Ex 1.4.2 Build a DataFrame that captures how the population in each  province of Flanders changed in the years 2013, 2014 and 2015.**\n"}
{"snippet": "names1880 = pd.read_csv('names/yob1880.txt', names=['name', 'sex', 'births'])\n", "intent": "**Ex 1.4.7 How many different male and female names were used in 1880?**\n"}
{"snippet": "def read_one_year(year):\n    df = pd.read_csv('names/yob{0}.txt'.format(year),\n                     names=['name', 'sex', 'births'])\n    df['year'] = int(year)\n    return df\nnames = pd.concat([read_one_year(y) for y in range(1880, 2014+1)],\n                 ignore_index=False)\n", "intent": "**Ex 1.4.8 Write a function to read all names data from 1880 to 2014 into a single `DataFrame`.  Store the result in `names`**\n"}
{"snippet": "total_births = top1000.pivot_table('births', index='year', columns='name',\n                                  aggfunc=sum)\ntotal_births.head()\n", "intent": "**Ex 1.4.11 Plot the proportion in each year of babies named John, Harry, Mary and Marilyn, all in one figure.**\n"}
{"snippet": "import load_spam\nspam_data = load_spam.spam_data_loader()\nspam_data.load_data()\n", "intent": "Let's put all this loading process into a separate file so that we can reuse it in other experiments.\n"}
{"snippet": "from sys import path\npath.append('../2 - Text data preprocessing')\nimport load_spam\nspam_data = load_spam.spam_data_loader()\nspam_data.load_data()\nprint(\"data loaded\")\nXtrain, ytrain, Xtest, ytest = spam_data.split(2000)\nXtrain.shape\n", "intent": "Let's illustrate that on the Ling-spam database.\n"}
{"snippet": "from keras.datasets import fashion_mnist\n(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n", "intent": "One nice thing is that Fashion MNIST has become part of the standard computer vision benchmarks and is included with most libraries, including Keras.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\ndata = iris.data\ntarget = iris.target\n", "intent": "* Fisher developed LDA to deal with the Iris dataset\n* So let's try that one\n* What do you get using the Iris data?\n"}
{"snippet": "data = pd.read_csv(\"data/mushrooms.csv\")\ndata.head(6)\n", "intent": "We now try to do binary classification on the Mushrooms data set loaded below:\n"}
{"snippet": "countvec = CountVectorizer()\nnovels_df = pandas.DataFrame(countvec.fit_transform(text_list).toarray(), columns=countvec.get_feature_names())\nnovels_df\n", "intent": "Create a DTM from these two novels, force it into a pandas DF, and inspect the output:\n"}
{"snippet": "import pandas\nimport nltk\nfrom nltk import word_tokenize\nimport string\ndf = pandas.read_csv(\"../Data/BDHSI2016_music_reviews.csv\", encoding='utf-8', sep = '\\t')\ndf\n", "intent": "First, read in our Music Reviews corpus as a Pandas dataframe.\n"}
{"snippet": "topic_dist_df = pandas.DataFrame(topic_dist)\ndf_w_topics = topic_dist_df.join(df_lit)\ndf_w_topics\n", "intent": "Merge back in with the original dataframe.\n"}
{"snippet": "churntrain, churntest = train_test_split(xrange(dfchurn.shape[0]), train_size=0.6)\nchurnmask=np.ones(dfchurn.shape[0], dtype='int')\nchurnmask[churntrain]=1\nchurnmask[churntest]=0\nchurnmask = (churnmask==1)\nchurnmask\n", "intent": "We havent made any calculations yet! Lets fix that omission and create our training and test sets.\n"}
{"snippet": "dfhw=pd.read_csv(\"https://dl.dropboxusercontent.com/u/75194/stats/data/01_heights_weights_genders.csv\")\nprint dfhw.shape\ndfhw.head()\n", "intent": "(I encountered this dataset in Conway, Drew, and John White. Machine learning for hackers. \" O'Reilly Media, Inc.\", 2012.)\n"}
{"snippet": "itrain, itest = train_test_split(xrange(df.shape[0]), train_size=0.6)\nmask=np.ones(df.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\nmask[:10]\n", "intent": "We split the data into training and test sets...\n"}
{"snippet": "from sklearn.decomposition import TruncatedSVD\nfrom sklearn.utils import as_float_array\ndata_centered = as_float_array(data)\nmean = np.mean(data_centered, axis=0)\ndata_centered -= mean\nU,S,V = np.linalg.svd(data_centered,full_matrices=False)\npd.DataFrame(U[:,:60] *S[:60]).head()\n", "intent": "<h1 ><font color='red'>SVD WORKS LIKE PCA ONLY IF YOU NORMALIZE THE DATA BEFORE !!!! (PCA does it automatically)</font></h1> \n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0)\n", "intent": "Splitting data into 70% training and 30% test data:\n"}
{"snippet": "y = pca.fit_transform(rvs)\n", "intent": "Reduce the dimension\n"}
{"snippet": "from sklearn.datasets import make_s_curve\ndata, colors = make_s_curve(n_samples=1000)\nprint(data.shape)\nprint(colors.shape)\n", "intent": "One dataset often used as an example of a simple nonlinear dataset is the S-cure:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(lfw_people.data, lfw_people.target, random_state=0)\nprint X_train.shape, X_test.shape\n", "intent": "We'll perform a Support Vector classification of the images.  We'll\ndo a typical train-test split on the images to make this happen:\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\n", "intent": "Apply PCA LocallyLinearEmbedding, and Isomap to project the data to two dimensions.\nWhich visualization technique separates the classes most cleanly?\n"}
{"snippet": "from sklearn import datasets, feature_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nnews = datasets.fetch_20newsgroups()\nX, y = news.data, news.target\nvectorizer = TfidfVectorizer()\nvectorizer.fit(X)\nvector_X = vectorizer.transform(X)\nprint vector_X.shape\n", "intent": "For some types of data, for instance text data, a feature extraction step must be applied to convert it to numerical features.\n"}
{"snippet": "novels_df = pandas.DataFrame(countvec.fit_transform(text_list).toarray(), columns=countvec.get_feature_names())\nnovels_df\n", "intent": "Creat a DTM from these two novels, force it into a pandas DF, and inspect the output:\n"}
{"snippet": "import pandas\nfrom sklearn.feature_extraction.text import CountVectorizer\ncon_score = pandas.read_csv('Concreteness_ratings_Brysbaert_et_al.csv')\nprint(con_score)\n", "intent": "* Do the additional exercises **without** copying and pasting code from lecture notebook.\n"}
{"snippet": "import pandas\nfrom sklearn.feature_extraction.text import CountVectorizer\ncon_score = pandas.read_csv('Concreteness_ratings_Brysbaert_et_al.csv')\nprint(con_score)\n", "intent": "* Do the additional exercises without copying and pasting code from lecture notebook.\n"}
{"snippet": "text_list = []\nmachiavelli_string = open('Machiavelli_ThePrince.txt', encoding='utf-8').read()\nmarx_string = open('Marx_CommunistManifesto.txt', encoding='utf-8').read()\ntext_list.append(machiavelli_string)\ntext_list.append(marx_string)\ncountvec = CountVectorizer(stop_words=\"english\")\nnovels_df = pandas.DataFrame(countvec.fit_transform(text_list).toarray(), columns=countvec.get_feature_names())\nnovels_df\n", "intent": "* open the **Machiavelli_ThePrince.txt** and **Marx_CommunistManifesto.txt**.\n* make a data frame that contains both of them.\n"}
{"snippet": "tfidfvec = TfidfVectorizer(stop_words = 'english', min_df = 1, binary=True)\ncountvec = CountVectorizer(stop_words = 'english', min_df = 1, binary=True)\ntraining_dtm_tf = countvec.fit_transform(df_train.body)\ntest_dtm_tf = countvec.transform(df_test.body)\ntraining_labels = df_train.label\ntest_labels = df_test.label\ntest_labels.value_counts()\n", "intent": "Next we need to create a dtm for each review, and an array containing the classification label for each review (for us, this is called 'label')\n"}
{"snippet": "X_pca = PCA().fit_transform(X)\n", "intent": "* PCA fails to linearly separate this data\n"}
{"snippet": "n_topics = 20\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nlda = LatentDirichletAllocation(n_topics=n_topics, \n                                max_iter=5,\n                                learning_method='online', \n                                learning_offset=50.,\n                                random_state=0)\n", "intent": "1. Fit the `lda` model below with the features you extracted above.\n"}
{"snippet": "start = time.time()\nlda = LatentDirichletAllocation( n_topics = N_TOPICS )\ndoctopic = lda.fit_transform( bag_of_words_all )\nend = time.time() \nprint(\"Processing took {}s\".format(end- start)) \n", "intent": "To create our topics we will use the LatentDirichletAllocation algorithm \n"}
{"snippet": "import statsmodels.api as sm\nfrom statsmodels.formula.api import logit, glm, ols\ndat = pd.DataFrame(data, columns = ['Temperature', 'Failure'])\nlogit_model = logit('Failure ~ Temperature',dat).fit()\nprint(logit_model.summary())\n", "intent": "Lets plot this data\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ndf_default = pd.read_excel('Data/Default.xlsx')\ndf_default.default= df_default.default.map({'No':0,'Yes':1})\ndf_default = df_default.iloc[:,[0,2,3]]\ndf_default.head()\ny = df_default.default.values\nX = df_default.iloc[:,[1,2]].values\nX_train, X_test, y_train,y_test = train_test_split(X,y,test_size =0.3, random_state =1,stratify =y)\n", "intent": "- We will be using the default data set to test\n"}
{"snippet": "print(\"\ntrain = pd.read_csv('numerai_datasets/numerai_training_data.csv', header=0)\ntournament = pd.read_csv('numerai_datasets/numerai_tournament_data.csv', header=0)\n", "intent": "We will be using the numerai data set for example\n"}
{"snippet": "df1 = pd.DataFrame(np.arange(24).reshape(4,6))\nnp.sin(df1)\n", "intent": "For element-wise function application, the most straightforward thing to do is to apply numpy functions to these objects:\n"}
{"snippet": "time_df = pd.DataFrame(np.random.rand(len(business_days)),\n                    index=business_days,\n                   columns=['random'])\ntime_df.head()\n", "intent": "This can in turn be used in as a DataFrame index:\n"}
{"snippet": "pd.pivot_table(df, index='State_Code', columns='County_Code',\n               values=['NUM_ALL', 'NUM_FHA'], aggfunc=np.sum).head()\n", "intent": "This can be done with one step with the `pivot_table()` function.\n"}
{"snippet": "df = pd.read_csv('HR_data.csv')\n", "intent": "***\nTenemos suerte y nos han pasado los datos en un csv. Importa el fichero como DataFrame de pandas y guardalo en la variable `df`\n"}
{"snippet": "kpca = KernelPCA(kernel=\"rbf\", fit_inverse_transform=True, gamma=10)\nX_kpca = kpca.fit_transform(X)\n", "intent": "* Kernel PCA, on the other hand\n"}
{"snippet": "from sklearn.datasets import load_boston\ndata = load_boston()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf['y'] = data.target\nprint(data.get('DESCR'))\n", "intent": "** Ejercicio 1: Carga el dataset de ejemplo por defecto de las casas de boston **\n"}
{"snippet": "df_cv_results = pd.DataFrame(gs_reg_DT.cv_results_)\ndf_cv_results.head()\n", "intent": "** Ejercicio 15: Guarda en un dataframe los resultados de las iteraciones. Los encontraras en .cv_results_ **\n"}
{"snippet": "from sklearn.datasets import load_iris\nimport numpy as np\ndata = load_iris()\nX = data.data\ny = data.target\nmask = np.random.choice([0,1], p=[0.99, 0.01], size=X.shape).astype(np.bool)\nX[mask] = np.nan\n", "intent": "Preparamos los datos con unos cuantos nan's...\n"}
{"snippet": "from sklearn.pipeline import FeatureUnion\ntransformer_list = [('num_pipe', num_pipe),\n                    ('cat_pipe', cat_pipe)]\nfull_pipe = FeatureUnion(transformer_list=transformer_list)\n", "intent": "** Join the two pipes using feature union **\n"}
{"snippet": "local_paths = ['data/othello.txt', 'data/mcbeth.txt']\ndocs = []\nfor path in local_paths:\n    with open(path, 'r') as f:\n        docs.append(f.read())\n", "intent": "O bien, desde local...\n"}
{"snippet": "pca_orig = PCA(n_components=2)\npca_orig.fit(original_imgs)\n", "intent": "Run PCA with 2 components on the original images\n"}
{"snippet": "pca_featurized = PCA(n_components=2)\npca_featurized.fit(featurized)\n", "intent": "Run PCA with 2 components on the featurizations\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])\ndf_feat.head(3)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nprint(boston.DESCR)\nboston_df = boston.data\n", "intent": "    from sklearn.datasets import load_boston\n    boston = load_boston()\n    print(boston.DESCR)\n    boston_df = boston.data\n"}
{"snippet": "pca = PCA().fit(X)\n", "intent": "* Say we want to know the five underlying factors\n"}
{"snippet": "from matplotlib.image import imread\ncustom_images = []\nfor i in range(15):\n    custom_images.append(imread(\"Captured-Image-Data/image\"+str(i)+\".jpg\",))\n", "intent": "* I load a set of **15** custom images from google that I resized to **32x32** by hand. (Loading JPEG images requires `pillow`).\n"}
{"snippet": "from urllib.request import urlretrieve\nfrom os.path import isfile\nfrom tqdm import tqdm\nfrom keras.datasets import cifar10\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\nprint('Training and Test data downloaded.')\n", "intent": "Download and load the Cifar10 dataset.\n"}
{"snippet": "df_bank = pd.DataFrame(data = bankf, columns = bank.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "** Use SciKit Learn to create training and testing sets of the data as we've done in previous lectures:**\n"}
{"snippet": "df = pd.read_csv('KNN_Project_Data')\ndf.head()\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features, columns = df.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "msg2_train, msg2_test, label2_train, label2_test = train_test_split(yelp_class['text'], yelp_class['stars'], test_size = 0.3, random_state = 101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"./PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "pca = UnsupervisedSpatialFilter(PCA(30), average=False)\npca_data = pca.fit_transform(X)\nev = mne.EvokedArray(np.mean(pca_data, axis=0),\n                     mne.create_info(30, epochs.info['sfreq'],\n                                     ch_types='eeg'), tmin=tmin)\nev.plot(show=False, window_title=\"PCA\")\n", "intent": "Transform data with PCA computed on the average ie evoked response\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\ndfp_subsampled = df[0:5000]\nX = MinMaxScaler().fit_transform(dfp_subsampled[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\ny = dfp_subsampled['is_duplicate'].values\n", "intent": "<h3>3.5.2 Visualization </h3>\n"}
{"snippet": "X_train,X_test, y_train, y_test = train_test_split(data, y_true, stratify=y_true, test_size=0.3)\n", "intent": "<h2> 4.3 Random train test split( 70:30) </h2>\n"}
{"snippet": "vectorizer = CountVectorizer(tokenizer = lambda x: x.split())\ntag_dtm = vectorizer.fit_transform(tag_data['Tags'])\n", "intent": "<h3> 3.2.1 Total number of unique tags </h3>\n"}
{"snippet": "start = datetime.now()\nvectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", \\\n                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))\nx_train_multilabel = vectorizer.fit_transform(x_train['question'])\nx_test_multilabel = vectorizer.transform(x_test['question'])\nprint(\"Time taken to run this cell :\", datetime.now() - start)\n", "intent": "<h2>4.3 Featurizing data </h2>\n"}
{"snippet": "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\nmultilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])\n", "intent": "__ Converting string Tags to multilable output variables __ \n"}
{"snippet": "start = datetime.now()\nvectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", \\\n                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))\nx_train_multilabel = vectorizer.fit_transform(x_train['question'])\nx_test_multilabel = vectorizer.transform(x_test['question'])\nprint(\"Time taken to run this cell :\", datetime.now() - start)\n", "intent": "<h3> 4.5.2 Featurizing data with TfIdf vectorizer </h3>\n"}
{"snippet": "import numpy as np\nimport pandas as pd\ndf = pd.DataFrame(data1.data, columns=data.feature_names)\ntarget = pd.DataFrame(data1.target, columns=[\"MEDV\"])\n", "intent": "First, we should load the data as a pandas data frame for easier analysis and set the median home value as our target variable\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nenc=OneHotEncoder(sparse=False)\nX_train_1=X_train\nX_test_1=X_test\ncolumns=['Gender', 'Married', 'Dependents', 'Education','Self_Employed',\n          'Credit_History', 'Property_Area']\nfor col in columns:\n     data=X_train[[col]].append(X_test[[col]])\n     enc.fit(data)\n", "intent": "One-Hot Encoding transforms each categorical feature with n possible values into n binary features, with only one active.\n"}
{"snippet": "y=pd.DataFrame(telco['Churn'])\ntelco=telco.drop('Churn',axis=1)\n", "intent": "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfitting.\n"}
{"snippet": "clf = LogisticRegression()\nscaler = StandardScaler()\nmodel = LinearModel(clf)\nX = scaler.fit_transform(meg_data)\nmodel.fit(X, labels)\nfor name, coef in (('patterns', model.patterns_), ('filters', model.filters_)):\n    coef = scaler.inverse_transform([coef])[0]\n    coef = coef.reshape(len(meg_epochs.ch_names), -1)\n    evoked = EvokedArray(coef, meg_epochs.info, tmin=epochs.tmin)\n    evoked.plot_topomap(title='MEG %s' % name)\n", "intent": "Decoding in sensor space using a LogisticRegression classifier\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(data, data.SalePrice, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "There are 4 variables with missing data.\n"}
{"snippet": "def impute_na(df, variable, median):\n    df[variable+'_NA'] = np.where(df[variable].isnull(), 1, 0)\n    df[variable].fillna(median, inplace=True)\n", "intent": "We observed that the numerical variables are not normally distributed. In particular, most of them apart from YearBuilt are skewed.\n"}
{"snippet": "def impute_na(df_train, df_test, variable):\n    most_frequent_category = df_train.groupby([variable])[variable].count().sort_values(ascending=False).index[0]\n    df_train[variable].fillna(most_frequent_category, inplace=True)\n    df_test[variable].fillna(most_frequent_category, inplace=True)\n", "intent": "Two of the variables have missing data, so let's replace by the most frequent category as we saw on previous lectures.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data[['Cabin', 'Survived']], data.Survived, test_size=0.3, random_state=0)\nX_train.shape, X_test.shape\n", "intent": "The ordering of the labels should be done considering the target ONLY on the training set, and then expanded it to the test set.\nSee below.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(data[['Cabin', 'Survived']], data.Survived, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "The risk factor should be calculated per label considering ONLY on the training set, and then expanded it to the test set.\nSee below.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(data[['Pclass', 'Age', 'Fare']],\n                                                    data.Survived, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "Age contains missing information, so I will fill those observations with the median in the next cell.\n"}
{"snippet": "scaler = StandardScaler() \nX_train_scaled = scaler.fit_transform(X_train) \nX_test_scaled = scaler.transform(X_test) \n", "intent": "StandardScaler from scikit-learn removes the mean and scales the data to unit variance. \n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(data[['Age', 'Fare', 'Survived']],\n                                                    data.Survived, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "The tree should be built using the training dataset, and then used to replace the same feature in the testing dataset, to avoid over-fitting.\n"}
{"snippet": "data = pd.read_csv('titanic.csv', usecols=['Cabin', 'Survived'])\ndata.Cabin.fillna('Missing', inplace=True)\ndata['Cabin'] = data['Cabin'].astype(str).str[0]\ndata.head()\n", "intent": "Discretisation using trees can also be used on categorical variables, to capture some insight into how well they predict the target.\n"}
{"snippet": "pca = UnsupervisedSpatialFilter(PCA(30), average=False)\npca_data = pca.fit_transform(X)\nev = mne.EvokedArray(np.mean(pca_data, axis=0),\n                     mne.create_info(30, epochs.info['sfreq'],\n                                     ch_types='eeg'), tmin=tmin)\nev.plot(show=False, window_title=\"PCA\", time_unit='s')\n", "intent": "Transform data with PCA computed on the average ie evoked response\n"}
{"snippet": "for df in [X_train, X_test, submission]:\n    for var in ['LotFrontage', 'GarageYrBlt']:\n        df[var+'_NA'] = np.where(df[var].isnull(), 1, 0)\n        df[var].fillna(X_train[var].median(), inplace=True) \nfor df in [X_train, X_test, submission]:\n    df.MasVnrArea.fillna(X_train.MasVnrArea.median(), inplace=True)\n", "intent": "- LotFrontage and GarageYrBlt: create additional variable with NA + median imputation\n- CMasVnrArea: median imputation\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfitting.\n"}
{"snippet": "data = pd.read_csv('santander.csv', nrows=50000)\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "In the following cells, I will show an alternative to the VarianceThreshold function of sklearn.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['target', 'ID'], axis=1),\n    data['target'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data[['Pclass', 'Sex', 'Embarked']],\n    data['Survived'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data[['Pclass', 'Sex', 'Embarked', 'Cabin', 'Survived']],\n    data['Survived'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.\n"}
{"snippet": "scaler = StandardScaler()\nscaler.fit(X_train.fillna(0))\ncoefs_df = []\nfor c in [1, 10, 100, 1000]:\n    logit = LogisticRegression(C=c, penalty='l2')\n    logit.fit(scaler.transform(X_train.fillna(0)), y_train)\n    coefs_df.append(pd.Series(logit.coef_.ravel()))\n", "intent": "Let's fit a few logistic regression models decreasing the penalty of the regularisation.\n"}
{"snippet": "data = pd.read_csv('training/training_variants')\nprint('Number of data points : ', data.shape[0])\nprint('Number of features : ', data.shape[1])\nprint('Features : ', data.columns.values)\ndata.head()\n", "intent": "<h3>3.1.1. Reading Gene and Variation Data</h3>\n"}
{"snippet": "clf = LogisticRegression()\nscaler = StandardScaler()\nmodel = LinearModel(clf)\nX = scaler.fit_transform(meg_data)\nmodel.fit(X, labels)\nfor name, coef in (('patterns', model.patterns_), ('filters', model.filters_)):\n    coef = scaler.inverse_transform([coef])[0]\n    coef = coef.reshape(len(meg_epochs.ch_names), -1)\n    evoked = EvokedArray(coef, meg_epochs.info, tmin=epochs.tmin)\n    evoked.plot_topomap(title='MEG %s' % name, time_unit='s')\n", "intent": "Decoding in sensor space using a LogisticRegression classifier\n"}
{"snippet": "y_true = result['Class'].values\nresult.Gene      = result.Gene.str.replace('\\s+', '_')\nresult.Variation = result.Variation.str.replace('\\s+', '_')\nX_train, test_df, y_train, y_test = train_test_split(result, y_true, stratify=y_true, test_size=0.2)\ntrain_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)\n", "intent": "<h4>3.1.4.1. Splitting data into train, test and cross validation (64:20:16)</h4>\n"}
{"snippet": "df = pd.read_csv('emails.csv')\n", "intent": "Load the file emails.csv\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n", "intent": "- Plot the standardized mean cross-validated accuracy against the unstandardized. Which is better?\n- Why?\n"}
{"snippet": "kmeans_df = pd.DataFrame(results_list_dict)\nkmeans_df.head()\n", "intent": "cluster_picker(X, range_setter)\n"}
{"snippet": "hsq = pd.read_csv('./datasets/hsq_data.csv')\n", "intent": "It is worth reading over the description of the data columns above for this.\n"}
{"snippet": "hsq = pd.read_csv('../datasets/hsq_data.csv')\n", "intent": "It is worth reading over the description of the data columns above for this.\n"}
{"snippet": "conmat = np.array(confusion_matrix(y_test, yhat_ridge, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['is_male', 'is_female'],\n                         columns=['predicted_male','predicted_female'])\nconfusion\n", "intent": "**9.C Construct the confusion matrix for the Ridge LR.**\n"}
{"snippet": "df['bias'] = df['bias'].apply(lambda x: 1 if x == 'partisan' else 0)\nX_train, X_test, y_train, y_test = train_test_split(df['text'].values,\n                                                   df['bias'].values)\n", "intent": "Please split the dataset into a training and test set and convert the `bias` feature into 0s and 1s.\n"}
{"snippet": "df = pd.read_csv('train.csv')\n", "intent": "Load `train.csv` from Kaggle into a pandas DataFrame.\n"}
{"snippet": "clf = make_pipeline(StandardScaler(), LogisticRegression(solver='lbfgs'))\ntime_gen = GeneralizingEstimator(clf, scoring='roc_auc', n_jobs=1,\n                                 verbose=True)\ntime_gen.fit(X=epochs['Left'].get_data(),\n             y=epochs['Left'].events[:, 2] > 2)\n", "intent": "We will train the classifier on all left visual vs auditory trials\nand test on all right visual vs auditory trials.\n"}
{"snippet": "test = pd.read_csv('test.csv')\n", "intent": "Be sure to do the **same** preprocessing you did for your training `X`.\n"}
{"snippet": "pd.DataFrame({\"ImageId\": list(range(1,len(Label)+1)), \"Label\": Label}).to_csv('predictions.csv', \\\n                                                                              index=False, header=True)\n", "intent": "Remember to set `index=False`!\n"}
{"snippet": "test = pd.read_csv('test.csv')\ntest = test / 255.\n", "intent": "Be sure to do the **same** preprocessing you did for your training `X`.\n"}
{"snippet": "test[['ImageId', 'Label']].to_csv('submission.csv', index=False)\n", "intent": "Remember to set `index=False`!\n"}
{"snippet": "data = load_iris()\n", "intent": "Scikit Learn datasets are actually functions that return an object containing the data we need.\n"}
{"snippet": "target_df = pd.DataFrame(data.target, columns=['species'])\ntarget_df.head()\n", "intent": "We'll use `data.target` to create our first dataframe, which is just a single column.\n"}
{"snippet": "sd = pd.read_csv('./datasets/speed_dating.csv')\n", "intent": "---\n- Remove columns with over 200 missing values.\n- Then, remove rows with missing values.\n- Verify no rows contain NaNs.\n"}
{"snippet": "sd = pd.read_csv('../datasets/speed_dating.csv')\n", "intent": "---\n- First, remove columns with over 200 missing values.\n- Then, remove rows with missing values.\n"}
{"snippet": "fu = FeatureUnion([\n    ('has_money_symbol_tf', has_money_symbol_tf),\n    ('has_number_large_number_tf', has_number_large_number_tf),\n    ('has_yelling_tf', has_yelling_tf),\n    ('is_exclaiming_tf', is_exclaiming_tf),\n    ('has_domain_name_tf', has_domain_name_tf),\n    ('has_special_characters_tf', has_special_characters_tf)\n])\n", "intent": "Combine all your function transformers into a feature union\n"}
{"snippet": "clf = LogisticRegression(solver='lbfgs')\nscaler = StandardScaler()\nmodel = LinearModel(clf)\nX = scaler.fit_transform(meg_data)\nmodel.fit(X, labels)\nfor name, coef in (('patterns', model.patterns_), ('filters', model.filters_)):\n    coef = scaler.inverse_transform([coef])[0]\n    coef = coef.reshape(len(meg_epochs.ch_names), -1)\n    evoked = EvokedArray(coef, meg_epochs.info, tmin=epochs.tmin)\n    evoked.plot_topomap(title='MEG %s' % name, time_unit='s')\n", "intent": "Decoding in sensor space using a LogisticRegression classifier\n"}
{"snippet": "df = pd.read_csv('../datasets/breast_cancer_wisconsin/breast_cancer.csv', na_values='?')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "scaler = preproc.StandardScaler()\nXs = scaler.fit_transform(X)\n", "intent": "- Plot the standardized mean cross-validated accuracy against the unstandardized. Which is better?\n- Why?\n"}
{"snippet": "df = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['MEDV'] = boston.target\n", "intent": "Load the Boston housing data.  Fix any problems, if applicable.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 142, test_size = 0.1)\n", "intent": "- Score and plot.  \n- How do your metrics change?\n"}
{"snippet": "wine = pd.read_csv('../datasets/winequality_merged.csv')\n", "intent": "**1. Load and examine the data.**\n"}
{"snippet": "ss = StandardScaler()\nmms = MinMaxScaler()\nXs = ss.fit_transform(X)\nXn = mms.fit_transform(X)\n", "intent": "**4. Create a standardized and normalized version of your predictor matrix.**\n"}
{"snippet": "boston_data = datasets.load_boston()\n", "intent": "**Load the boston housing data with the `datasets.load_boston()` function.**\n"}
{"snippet": "hsq = pd.read_csv('/Users/david.yan/hello_world/week-05/evaluation-classifiers_confusion_matrix_roc-lab/datasets/hsq_data.csv')\n", "intent": "It is worth reading over the description of the data columns above for this.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\nknn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(X_train, y_train)\n", "intent": "**Split the data into 66% training and 33% testing. Fit a KNN classifier with `n_neighbors=25` on the training data.**\n"}
{"snippet": "clf = make_pipeline(StandardScaler(),\n                    LinearModel(LogisticRegression(solver='lbfgs')))\ntime_decod = SlidingEstimator(clf, n_jobs=1, scoring='roc_auc', verbose=True)\ntime_decod.fit(X, y)\ncoef = get_coef(time_decod, 'patterns_', inverse_transform=True)\nevoked = mne.EvokedArray(coef, epochs.info, tmin=epochs.times[0])\njoint_kwargs = dict(ts_args=dict(time_unit='s'),\n                    topomap_args=dict(time_unit='s'))\nevoked.plot_joint(times=np.arange(0., .500, .100), title='patterns',\n                  **joint_kwargs)\n", "intent": "You can retrieve the spatial filters and spatial patterns if you explicitly\nuse a LinearModel\n"}
{"snippet": "df = pd.read_csv('./datasets/titanic_train.csv')\n", "intent": "We'll be working with the titanic datasets - go ahead and import it from the dataset folder (or query for it as described above). \n"}
{"snippet": "df = pd.read_csv('../datasets/titanic_train.csv')\n", "intent": "We'll be working with the titanic datasets - go ahead and import it from the dataset folder (or query for it as described above). \n"}
{"snippet": "prop = pd.read_csv('./datasets/assessor_sample.csv')\n", "intent": "Examine the columns.\n"}
{"snippet": "prop = pd.read_csv('../datasets/assessor_sample.csv')\n", "intent": "Examine the columns.\n"}
{"snippet": "Xs = StandardScaler().fit_transform(X)\n", "intent": "**10.C Standardize the predictor matrix.**\n"}
{"snippet": "sf_time = pd.DataFrame(sf_crime['Dates'].str.split(' ',1).tolist(),columns = ['date','time'])\nsf_date = pd.DataFrame(sf_time['date'].str.split('/').tolist(),columns = ['month','day','year'])\n", "intent": "> *Hint: `pd.to_datetime` may or may not be helpful.*\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=12)\n", "intent": "**Split data into training and testing with 50% in testing.**\n"}
{"snippet": "file_path = './datasets/breast_cancer_wisconsin/breast_cancer.csv'\ndf = pd.read_csv(file_path)\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "iris_data = datasets.load_iris()\n", "intent": "Both sklearn and seaborn have ways to import the iris data:\n- `sklearn.datasets.load_iris()`\n- `sns.load_dataset(\"iris\")`\nThe seaborn way is easier.\n"}
{"snippet": "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\ndata.head()\n", "intent": "Let's take a look at some data, ask some questions about that data, and then use linear regression to answer those questions!\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX = wine.iloc[:, :-1]\nXs = StandardScaler().fit_transform(X)\n", "intent": "**Select the other variables to use for clustering.**\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfeatures = [c for c in df.columns if c != 'acceptability']\nfor c in df.columns:\n    df[c] = le.fit_transform(df[c])\nX = df[features]\ny = df['acceptability']\n", "intent": "Since most of the features are categorical text we will need to encode them as numbers using the ***LabelEncoder***.\n"}
{"snippet": "sd = pd.read_csv('./datasets/speed_dating.csv')\nsd.info()\n", "intent": "---\n- Remove columns with over 200 missing values.\n- Then, remove rows with missing values.\n- Verify no rows contain NaNs.\n"}
{"snippet": "pd.DataFrame({'PC1': pc1_ev, 'PC2': pc2_ev},\n             index=EVENT_NAMES)\n", "intent": "---\nBased on how the original variables are weighted to calculate the components, how would you describe PC1 and PC2?\n"}
{"snippet": "lang = pd.read_csv(\"./datasets/lang.csv\")\nlang.head()\n", "intent": "We're going to be using **scipy** for our analysis. Let's load in the dataset using Pandas ```read.csv()``` and check the head to see it's structure\n"}
{"snippet": "cvt      =  CountVectorizer(strip_accents='unicode', ngram_range=(1,1))\nX_all    =  cvt.fit_transform(tweets_df['TEXT'])\ncolumns = np.array(cvt.get_feature_names())\nX_all\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "cvt      =  CountVectorizer(strip_accents='unicode', ngram_range=(1,1))\nX_all    =  cvt.fit_transform(tweets_df['TEXT'])\ncolumns  =  np.array(cvt.get_feature_names())          \nX_all\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "cvt = CountVectorizer(stop_words=\"english\", ngram_range=(2,4))\nX_all = cvt.fit_transform(tweets_df['TEXT'])\ncolumns  =  np.array(cvt.get_feature_names())\nfreq_words = get_freq_words(X_all, columns)\nfreq_words\n", "intent": "Display the top 75 ngrams with frequencies.\n"}
{"snippet": "from sklearn import naive_bayes\nimport numpy as np\nimport pandas as pd\ndata = pd.read_csv('./datasets/spam_base.csv')\n", "intent": "<a id=\"guided-practice-scikit-learn-implementation\"></a>\n"}
{"snippet": "X_new = pd.DataFrame({'TV': [50]})\nX_new.head()\n", "intent": "Thus, we would predict Sales of **9,409 widgets** in that market.\nOf course, we can also use Statsmodels to make the prediction:\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\n", "intent": "<a id=\"an-example-with-real-data\"></a>\n- We'll use the boston data set to implement multiple regression in TensorFlow\n"}
{"snippet": "data = pd.read_csv('assets/datasets/titanic_train.csv')\nX = data.drop('Survived', axis=1)\ny = data[['Survived']]\n", "intent": "<a id=\"load-in-the-titanic-data\"></a>\n"}
{"snippet": "class FeatureExtractor(TransformerMixin):\n    def __init__(self, column):\n        self.column = column\n    def fit(self, x, y=None):\n        return self\n    def transform(self, x, y=None):\n        return x[self.column].values.reshape(-1, 1)\nFeatureExtractor('Fare').fit_transform(X_train)[0:5]\n", "intent": "<a id=\"do-a-bit-of-data-cleaning\"></a>\n"}
{"snippet": "stocks = pd.DataFrame(stock_close)\n", "intent": "This converts the data into daily changes in stock price.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = pd.read_csv('assets/datasets/train.csv')\ndata.set_index('Date', inplace=True)\ndata.head()\n", "intent": "- Assemble observations and graphs as well as timeseries models in a notebook.\n"}
{"snippet": "pred_train, pred_test, tar_train, tar_test  =   train_test_split(predictors, targets, test_size=.3)\nprint( \"Predictor - Training : \", pred_train.shape, \"Predictor - Testing : \", pred_test.shape )\n", "intent": "We now split the model into training and testing data in the ratio of 70:30\n"}
{"snippet": "predictors = cleaned_data.drop(\"CLV\",axis=1)\ntargets = cleaned_data.CLV\npred_train, pred_test, tar_train, tar_test  =   train_test_split(predictors, targets, test_size=.1)\nprint( \"Predictor - Training : \", pred_train.shape, \"Predictor - Testing : \", pred_test.shape )\n", "intent": "Let us split the data into training and testing datasets in the ratio 90:10.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nnumeric_df_ss = pd.DataFrame(ss.fit_transform(numeric_df), index = numeric_df.index, columns= numeric_df.columns)\n", "intent": "The numeric_df looks to have missing values and we now have as sense of distributions. We can go ahead and standardize those values\n"}
{"snippet": "def cat_imputer(df):\n    for col in df.columns:\n        df[col].fillna(value=df[col].mode()[0], inplace=True)\n    return df\n", "intent": "let's impute the missing values for the categorcal with the mode \n"}
{"snippet": "X_new = pd.DataFrame({'TV': [data.TV.min(), data.TV.max()]})\nX_new.head()\n", "intent": "Let's make predictions for the **smallest and largest observed values of x**, and then use the predicted values to plot the least squares line:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconmat = confusion_matrix(y_test,y_pred)\nconfusion = pd.DataFrame(conmat, index = ['is_healthy', 'is_cancer'], columns =['predicted_healthy', 'predicted_cancer'])\nconfusion\n", "intent": "**Create the confusion matrix for your classfier's performance on the test set.**\n"}
{"snippet": "conmat = np.array(confusion_matrix(y_test, y_pred, labels=[0,1]))\nconfusion = pd.DataFrame(conmat, index=['is_healthy', 'is_cancer'],\n                         columns=['predicted_healthy', 'predicted_cancer'])\nconfusion\n", "intent": "**Create the confusion matrix for your classfier's performance on the test set.**\n"}
{"snippet": "selectkbest = SelectKBest(score_func=f_classif, k=5)\n", "intent": "Next, let's instantiate a `SelectKBest` object, using `f_classif` as our scoring function and a `k` of 5:\n"}
{"snippet": "non_quality_columns = [col for col in wine.columns if 'quality' not in col]\nX = wine[non_quality_columns].copy()\nss = StandardScaler()\nss.fit(X)\nstandard_X = ss.transform(X)\ny = wine['quality_0'].copy()\n", "intent": "Split up wine into your features (`X`) and target column (`y`), then standardize your features.\n"}
{"snippet": "df = pd.read_csv('datasets/boston.csv')\ndf.head()\n", "intent": "We'll be using the Boston housing dataset for this section. As a reminder, the first five rows are as follows:\n"}
{"snippet": "iris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf.head()\n", "intent": "First, let's import the `iris` dataset, loading it in from the `sklearn.datasets` module:\n"}
{"snippet": "df = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf.head()\n", "intent": "We'll show a brief example using the Iris dataset:\n"}
{"snippet": "ss = StandardScaler()\nss.fit(df)\nX = ss.transform(df)\n", "intent": "We'll begin by standardizing the data:\n"}
{"snippet": "cv = CountVectorizer()\ncv.fit(space_messages)\nstopped_words = pd.DataFrame(cv.transform(space_messages).todense(),\n                              columns=cv.get_feature_names())\nprint(stopped_words.sum().sort_values(ascending=False).head(10))\n", "intent": "We can also use `CountVectorizer` to remove common stopwords for us by setting the `stop_words` keyword argument to `english`\n"}
{"snippet": "def fill_nan(table):\n    for col in table.columns:\n        table[col] = table[col].fillna(table[col].median())\n    return table   \n", "intent": "Let's write the function that will replace *NaN* values with the median for each column.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(econ['text'].values, \n                                                   econ['relevance'].values,\n                                                   test_size=0.33,\n                                                   random_state=2017)\n", "intent": "And let's split into a training set and a test set, using the `text` of the document.\n"}
{"snippet": "cv = CountVectorizer(ngram_range=(1, 2))\nanswer = answer_df_questions(space_messages, cv)\n", "intent": "```\nInstructor answer:\nThis gives us all of the possible features from vectorizing the 100 messages\n```\n"}
{"snippet": "y = iris['species'].copy()\nX = iris[[col for col in iris.columns if col !='species']].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    random_state=2017)\n", "intent": "We'll split `species` into its own `y` variable and assign the remaining features to `X`, then train-test split (using 2017 as the `random_state`):\n"}
{"snippet": "econ = pd.read_csv('datasets/economic_news.csv',\n                  usecols=[14])\necon['text'] = econ['text'].apply(lambda x: x.replace('</br>', ''))\necon.head()\n", "intent": "First, let's reimport all of the economic news data instead of just the first 200 rows:\n"}
{"snippet": "cv = CountVectorizer(stop_words='english')\ncv.fit(econ['text'].values)\nX = cv.transform(econ['text'].values)\nX\n", "intent": "Next, we'll transform the data using `CountVectorizer` and removing stop words:\n"}
{"snippet": "from sklearn.decomposition import PCA\nX = demo_noage\npca = PCA()\npca = pca.fit(X)\nprint(pca.explained_variance_)\nprint(pca.components_)\n", "intent": "<a id=\"eigen\"></a>\n---\n"}
{"snippet": "Z = pca.transform(demo_noage)\nfeatures_pca = ['PC'+str(i+1) for i in range(pca.n_components_)]\nZ = pd.DataFrame(Z, columns=features_pca)\nZ.head(5)\n", "intent": "<a id=\"transformed\"></a>\n---\n"}
{"snippet": "age_imputer = Imputer(strategy='median')\nage_imputer.fit(df['age'].values.reshape(-1, 1))\nages = age_imputer.transform(\n    df['age'].values.reshape(-1, 1))\nprint(ages[0:5], ages.mean())\n", "intent": "Our results are as follows:\n- Mean: 16.65\n- Median: 17\n- Mode: 16.0\nWhat is most appropriate? Check in on Slack\n"}
{"snippet": "mm_scaler = MinMaxScaler()\nmm_scaler.fit(g1_g3)\nss_scaler = StandardScaler()\nss_scaler.fit(g1_g3)\nrb_scaler = RobustScaler()\nrb_scaler.fit(g1_g3)\n", "intent": "Let's use `MinMaxScaler`, `StandardScaler`, and `RobustScaler` to transform these values and see if visually, there is any difference:\n"}
{"snippet": "data = pd.read_csv('../../data/credit_scoring_sample.csv', sep =';')\ndata.head()\n", "intent": "Now, read the data:\n"}
{"snippet": "data = load_boston()\nprint(data.DESCR)\n", "intent": "Import data, take `TAX`, `CRIM`, and `PTRATIO` and assign to `X`. Assign target to `y`.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, boston_y, test_size=0.33)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n", "intent": "Create `X_train, X_test, y_train, y_test` using train-test split with a 33% holdout\n"}
{"snippet": "X = salary[['yd', 'yr']]\ny = salary['sl']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n", "intent": "Use `train_test_split()` to create a training set and a test set, split 50/50\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(boston, boston_y, test_size=0.33)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n", "intent": "Create `X_train, X_test, y_train, y_test` using train-test split with a 33% holdout\n"}
{"snippet": "def make_features(train_set, test_set, degrees):\n    train_dict = {}\n    test_dict = {}\n    for d in degrees:\n        traintestdict={}\n        train_dict[d] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n        test_dict[d] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n    return train_dict, test_dict\n", "intent": "We'll write a function to encapsulate what we learnt about creating the polynomial features.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ndatasize=sample_df.shape[0]\nitrain, itest = train_test_split(np.arange(60),train_size=0.8)\nxtrain= sample_df.x[itrain].values\nftrain = sample_df.f[itrain].values\nytrain = sample_df.y[itrain].values\nxtest= sample_df.x[itest].values\nftest = sample_df.f[itest].values\nytest = sample_df.y[itest].values\n", "intent": "We split the sample data into training and testing sets.\n"}
{"snippet": "from sklearn.datasets.california_housing import fetch_california_housing\ncal_housing = fetch_california_housing()\n", "intent": "First, the data. This one is built into sklearn, its a dataset about california housing prices. Its quite skewed as we shall see.\n"}
{"snippet": "train, test =  train_test_split(iris, test_size=.3)\nx_train, x_test = train[features_cols], test[features_cols]\ny_train, y_test = train['target'], test['target']\n", "intent": "We will now use the **sklearn** package to implement kNN:\nHere, we will split our data using the train_test_split function from sklearn.\n"}
{"snippet": "degree = 2\npoly = PolynomialFeatures(degree)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.fit_transform(x_test)\npd.DataFrame(x_train_poly).shape\n", "intent": "We first need to create the PolynomialFeatures object and specify to what degree we wish to take our polynomial to:\n"}
{"snippet": "lasso1_coef = pd.DataFrame({'coef': lasso1.coef_, 'coef_abs': np.abs(lasso1.coef_)},\n                          index=data.columns.drop('quality'))\nlasso1_coef.sort_values(by='coef_abs', ascending=False)\n", "intent": "**Which feature is the least informative in predicting wine quality, according to this LASSO model?**\n"}
{"snippet": "train, test =  train_test_split(crime_df, test_size=.2, random_state=123)\ntrain.shape,test.shape\n", "intent": "Now, let's split this dataset up into a testing and training set.\n"}
{"snippet": "X_test_numerical_powers = np.hstack((X_test_df[numerical_columns]**(i+1) for i in range(3)))\nX_test_np_powers = np.concatenate((X_test_numerical_powers,X_test_df.drop(numerical_columns, axis=1)),axis=1)\nX_test_df_powers = pd.DataFrame(X_test_np_powers)\nnewcolname = ['Temp', 'Dewpoint', 'Windspeed', 'Pressure', 'Precipitation', 'TMAX_C', 'TMIN_C', 'Temp^2', 'Dewpoint^2', 'Windspeed^2', 'Pressure^2', 'Precipitation^2', 'TMAX_C^2', 'TMIN_^2','Temp^3', 'Dewpoint^3', 'Windspeed^3', 'Pressure^3', 'Precipitation^3', 'TMAX_C^3', 'TMIN_C^3'] + list(X_train_df.drop(numerical_columns, axis=1))\nX_test_df_powers.columns = newcolname\nX_test_df_powers.head()\n", "intent": "We can do the same with the test set:\n"}
{"snippet": "all_poly_terms = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\nX_train_full_poly = all_poly_terms.fit_transform(X_train_df)\nX_test_full_poly = all_poly_terms.fit_transform(X_test_df)\nprint('number of total predictors', X_train_full_poly.shape[1])\n", "intent": "Let's now create a design matrix that includes all polynomial terms up to the third order, including all interactions. \n"}
{"snippet": "pca = PCA(n_components=5)\npca.fit(X_train_full_poly)\ntrain_pca = pca.transform(X_train_full_poly)\ntest_pca = pca.transform(X_test_full_poly)\nprint('Explained variance ratio:', pca.explained_variance_ratio_)\n", "intent": "Now we can fit our PCA model:\n"}
{"snippet": "X_test_numerical_powers = \nX_test_np_powers = np.concatenate((X_test_numerical_powers,X_test_df.drop(numerical_columns, axis=1)),axis=1)\nX_test_df_powers = pd.DataFrame(X_test_np_powers)\nnewcolname = ['Temp', 'Dewpoint', 'Windspeed', 'Pressure', 'Precipitation', 'TMAX_C', 'TMIN_C', 'Temp^2', 'Dewpoint^2', 'Windspeed^2', 'Pressure^2', 'Precipitation^2', 'TMAX_C^2', 'TMIN_^2','Temp^3', 'Dewpoint^3', 'Windspeed^3', 'Pressure^3', 'Precipitation^3', 'TMAX_C^3', 'TMIN_C^3'] + list(X_train_df.drop(numerical_columns, axis=1))\nX_test_df_powers.columns = newcolname\nX_test_df_powers.head()          \n", "intent": "We can do the same with the test set:\n"}
{"snippet": "x_train, x_test, y_train_val, y_test_val = train_test_split(x, y, test_size=0.6, random_state=42)\n", "intent": "Let's split our dataset into train and test.\n"}
{"snippet": "conf_mat_1 = confusion_matrix(y_test, y_hat_test)\nconf_df_1 = pd.DataFrame(conf_mat_1, columns = ['y_hat=0', 'y_hat = 1'], index = ['y=0', 'y=1'])\nconf_df_1\n", "intent": "We again rely on the confusion matrix to gain a clearer picture of how the classification did.\n"}
{"snippet": "conf_mat_2 = confusion_matrix(y_test2, y_hat_test2)\nconf_df_2 = pd.DataFrame(conf_mat_2, columns = ['y_hat=0', 'y_hat = 1'], index = ['y=0', 'y=1'])\nconf_df_2\n", "intent": "The test accuracy is better than before, but let us see how well it does for the cancer classification.\n"}
{"snippet": "poly_degree = 3\npoly = PolynomialFeatures(poly_degree, include_bias=False)\nX_train_poly_cubic = poly.fit_transform(X_train_2)\nX_test_poly_cubic = poly.fit_transform(X_test_2)\nlogregcv_cubic = LogisticRegressionCV()\nlogregcv_cubic.fit(X_train_poly_cubic, y_train_2)\nlogregcv_2_poly_cubic = LogisticRegressionCV(multi_class='multinomial')\nlogregcv_2_poly_cubic.fit(X_train_poly_cubic, y_train_2)\n", "intent": "Now, we show how to fit a Multiclass Logistic Regression model with cubic terms:\n"}
{"snippet": "lasso_cv_coef = pd.DataFrame({'coef': lasso_cv.coef_, 'coef_abs': np.abs(lasso_cv.coef_)},\n                          index=data.columns.drop('quality'))\nlasso_cv_coef.sort_values(by='coef_abs', ascending=False)\n", "intent": "**<font color='red'>Question 3:</font> Which feature is the least informative in predicting wine quality, according to the tuned LASSO model?**\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"G:\\DOCUMENTOS\\Data Science and Machine Learning with python\\DataScience-Python3/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\nclf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)\n", "intent": "A single train/test split is made easy with the train_test_split function in the cross_validation library:\n"}
{"snippet": "dataset = pd.read_csv(\"./data/dataset.csv\")\n", "intent": "Reading dataset with feature engineering done! Splitting dataset into train and test set.\n"}
{"snippet": "predictions = pd.DataFrame({\"PassengerId\":pd.concat([id_trainW,id_trainM,id_testW,id_testM], axis = 0),\n                            \"Survived\":list(pd.concat([pd.Series(train[train[\"Sex\"] == 1][\"Survived\"].astype(int)),\n                                                       pd.Series(train[train[\"Sex\"] == 0][\"Survived\"].astype(int)),\n                                                       pd.Series(predictions_cluster_1),\n                                                       pd.Series(predictions_cluster_2)],axis=0))})\n", "intent": "Sorting predictions according to delivery format\n"}
{"snippet": "train = pd.read_csv(\"./data/train.csv\")\n", "intent": "Let's get started!!!\n"}
{"snippet": "combined['Fare'].fillna(combined['Fare'].median(),inplace=True)\n", "intent": "We have replaced one missing Fare value by the median.\n"}
{"snippet": "train = pd.read_csv(\"./data/train.csv\")\n", "intent": "Reading and exploring data:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n", "intent": "**Create X and y train and test splits in one command using a split ratio and a random seed**\n"}
{"snippet": "df_2 = DataFrame(data_1, columns=['year', 'state', 'pop'])\ndf_2\n", "intent": "Create a DataFrame specifying a sequence of columns:\n"}
{"snippet": "data_demo = pd.read_csv('../../data/weights_heights.csv')\n", "intent": "Let's test out the algorithm on height/weight data. We will predict heights (in inches) based on weights (in lbs).\n"}
{"snippet": "data_2 = {'VA' : df_4['VA'][1:],\n          'MD' : df_4['MD'][2:]}\ndf_5 = DataFrame(data_2)\ndf_5\n", "intent": "Create a DataFrame from a dict of Series:\n"}
{"snippet": "df_13 = DataFrame({'foo' : [7, -5, 7, 4, 2, 0, 4, 7],\n                   'bar' : [-5, 4, 2, 0, 4, 7, 7, 8],\n                   'baz' : [-1, 2, 3, 0, 5, 9, 9, 5]})\ndf_13\n", "intent": "DataFrames can rank over rows or columns.\n"}
{"snippet": "df_14 = DataFrame(np.random.randn(5, 4),\n                  index=['foo', 'foo', 'bar', 'bar', 'baz'])\ndf_14\n", "intent": "Select DataFrame elements:\n"}
{"snippet": "data_2 = {'state' : ['NY', 'NY', 'NY', 'FL', 'FL'],\n          'year' : [2012, 2013, 2014, 2014, 2015],\n          'population' : [6.0, 6.1, 6.2, 3.0, 3.1]}\ndf_3 = DataFrame(data_2)\ndf_3\n", "intent": "Concatenate two DataFrames:\n"}
{"snippet": "df_1 = pd.read_csv(\"../data/ozone.csv\")\n", "intent": "Read data from a CSV file into a DataFrame (use sep='\\t' for TSV):\n"}
{"snippet": "df_1.to_csv('../data/ozone_copy.csv', \n            encoding='utf-8', \n            index=False, \n            header=False)\n", "intent": "Create a copy of the CSV file, encoded in UTF-8 and hiding the index and header labels:\n"}
{"snippet": "from sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=16)\n", "intent": "- We'll choose 16 components for our modeling\n"}
{"snippet": "ipl_data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings',\n         'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'],\n         'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2],\n         'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017],\n         'Points':[876,789,863,673,741,812,756,788,694,701,804,690]}\ndf = pd.DataFrame(ipl_data)\ndf.head()\n", "intent": "<a id='pandas-groupby'></a>\n"}
{"snippet": "cars=pd.read_csv('./EDA-getting&cleaningdata/CarPrice_Assignment.csv',encoding='iso-8859-1')\n", "intent": "[categorical encoding](http://pbpython.com/categorical-encoding.html)\n"}
{"snippet": "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                     test_size=0.3,\n                                                     random_state=17)\n", "intent": "Perform train/test split and scale data.\n"}
{"snippet": "ratings_data = pd.read_csv('C:/Users/xz2654/Downloads/ratings.csv')\nraw_movies_data = pd.read_csv('C:/Users/xz2654/Downloads/CleanedMovies.csv', encoding='latin-1')\nselected_movies=pd.read_csv('C:/Users/xz2654/Downloads/selectedmovies_infor.csv')\n", "intent": "We first read the files with pandas.\n"}
{"snippet": "genres_df=pd.read_csv('C:/Users/xz2654/Downloads/Selectedmoviesgenres.csv', encoding='latin-1')\nmovie_df=pd.read_csv('C:/Users/xz2654/Downloads/selectedmovies_infor.csv')\n", "intent": "First load the Selectedmovies and Selectedmoviesgenres datasets we got from previous data gathering and preparation process.\n"}
{"snippet": "col_names = ['labels','text']\ndf_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, columns=col_names)\ndf_val = pd.DataFrame({'text':val_texts, 'labels':val_labels}, columns=col_names)\n", "intent": "Fastai adopts a recent (LeCun paper) \"standard\" format for NLP datasets: label followed by text columns. \nFor IMDB, there is only one text column.\n"}
{"snippet": "import pandas as pd\nIMAGES,ANNOTATIONS,CATEGORIES = ['images', 'annotations', 'categories']            \ncategories = pd.DataFrame(trn_j[CATEGORIES])\nannotations = pd.DataFrame(trn_j[ANNOTATIONS])\nimages = pd.DataFrame(trn_j[IMAGES])\nlen(categories), len(images), len (annotations)\n", "intent": "We use pandas to organize the data. See the [Gist](https://gist.github.com/binga/1bc4ebe5e41f670f5954d2ffa9d6c0ed) for details. \n"}
{"snippet": "largest_bbox = data.pivot_table(index='file_name', values='area', aggfunc=max).reset_index()\nlargest_bbox = largest_bbox.merge(data[[AREA, BBOX, IMG_ID, FILE_NAME, NAME]], how='left')\nprint(\"If these are not equal, we may have duplicates\",  largest_bbox.shape[0], len(trn_fns)) \nlargest_bbox.head(1)\n", "intent": "**Best Practice:** Use pandas to create a CSV of the data to model, rather than trying to create a custom dataset. \n"}
{"snippet": "rf = path+'/ratings.csv'\nratings = pd.read_csv(rf)\nratings.head()\n", "intent": "The movielens data contains one user-rating-movie per row:\n"}
{"snippet": "mf = path+'/movies.csv'\nmovies = pd.read_csv(mf)\nmovies.head()\n", "intent": "Just for display purposes, let's read in the movie names too.\n"}
{"snippet": "param_test1 = {'n_estimators':range(120,201,10)}\ngs1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, \n                                                               min_samples_leaf=50,\n                                                               max_features=9,\n                                                               subsample=0.8,\n                                                               random_state=101010), \n                        param_grid = param_test1, \n                        scoring='roc_auc', cv=5)\ngs1.fit(train_set2[predictors3],train_set2[target])\npd.DataFrame(gs1.cv_results_)[cols].sort_values('rank_test_score').head(10)\n", "intent": "Its still increasting with n_estimators so we need to try more\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n     data.drop('subscribed', axis=1).values, \n     data['subscribed'].values,\n     stratify=data['subscribed'].values,\n     random_state=0)\n", "intent": "Let's now split our data into a train set and a test set.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(full_df,shuffle = True, test_size = 0.25, random_state=17)\ntrain_df=train_df.copy()\ntest_df=test_df.copy()\nprint(train_df.shape)\nprint(test_df.shape)\n", "intent": "Let's divide dataset into train (75%) and test (25%).\n"}
{"snippet": "dc_df = pd.get_dummies(c_df)\ndc_df.loc[dc_df['prev_days'] == 999, 'prev_days'] = np.nan\ndc_df = dc_df.drop(['subscribed_no'], axis=1)\nidc_df = pd.DataFrame(Imputer(strategy=\"mean\").fit_transform(dc_df.drop(['subscribed_yes'], axis=1).values))\nidc_df.columns = list(dc_df.drop(['subscribed_yes'], axis=1).columns)\nidc_df['subscribed'] = dc_df['subscribed_yes'].values\netrain_df = idc_df.iloc[:train_df.shape[0],:]\netest_df = idc_df.iloc[train_df.shape[0]:,:]\netrain_df.head()\n", "intent": "Now I get the dummies.\n"}
{"snippet": "vectorizer_ngrams = CountVectorizer(ngram_range=(1,10), stop_words=\"english\", lowercase=True, min_df=2)\nx_train_vectorized_ngrams = vectorizer_ngrams.fit_transform(X_train)\nx_test_vectorized_ngrams = vectorizer_ngrams.transform(X_test)\n", "intent": "Improve the model using more complex text features, including n-grams, character n-grams and possibly domain-specific features.\n"}
{"snippet": "Res = pd.DataFrame(x_16['Team 1'])\nRes['Team 2'] = x_16['Team 2']\nRes = Res.reset_index().drop(['Game ID'],axis=1)\nRes['Predicted_winner'] = y_pred['Predicted_winner']\nRes.head()\n", "intent": "Probability scores for both teams according to the predicted winner value are represented by probs1 and probs2\n"}
{"snippet": "td = pd.read_json('t.json.gz', orient='records', lines=True)\ntd['available_price'].fillna(td['available_price'].mean(),inplace=True)\ntd['mrp'].fillna(td['mrp'].mean(),inplace=True)\ntd.describe()\ntd.head()\n", "intent": "Reading both files and cleaning the columns which are required for now.\n"}
{"snippet": "t = td[['mrp']].values.astype(float)\nmin_max_scaler = MinMaxScaler()\nt_scaled = min_max_scaler.fit_transform(t)\nt_mrp_normalized = pd.DataFrame(t_scaled)\nt_mrp_normalized[0].replace((0,' '),np.nan,inplace=True)\nprint (\"The normalized mrp values are: \")\nt_mrp_normalized\n", "intent": "Normalization using SKLEARN library: for both files\n"}
{"snippet": "from sklearn import preprocessing\nscaler = preprocessing.StandardScaler()\nX2 = X.as_matrix().astype(np.float)\nX2 = scaler.fit_transform(X)\n", "intent": "In order to measure coefficients, recall that we must\n- a) Standarize coefficients\n- b) Remove collinear features\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('./data/hw2data.csv')\ndf.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "j = [10, 20, 30]\nfor s in j:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=s)\n    clf = LogisticRegression(penalty='l1', C=100)\n    clf.fit(X_train, y_train)\n    generate_ROCplot(fpr,tpr,'LR',roc_auc)\n    print \"The ROC curve for random state is:\", s\n", "intent": "For the different random state:10, 20 and 30. The roc_auc models are always keep consistent, therefore, the model is robust.\n"}
{"snippet": "def load_dataset(split):\n    dataset = datasets.load_iris()\n    X, y = dataset['data'], dataset['target']\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=123, test_size=(1 - split))\n    return X_train, X_test, y_train, y_test\n", "intent": "The iris data set (https://en.wikipedia.org/wiki/Iris_flower_data_set) is loaded and split into train and test parts by the function `load_dataset`.\n"}
{"snippet": "new_features_train_df=pd.DataFrame(index=train_df.index)\nnew_features_test_df=pd.DataFrame(index=test_df.index)\nnew_features_train_df['1/distance_to_SF']=1/(train_df['distance_to_SF']+0.001)\nnew_features_train_df['1/distance_to_LA']=1/(train_df['distance_to_LA']+0.001)\nnew_features_train_df['log_distance_to_SF']=np.log1p(train_df['distance_to_SF'])\nnew_features_train_df['log_distance_to_LA']=np.log1p(train_df['distance_to_LA'])\nnew_features_test_df['1/distance_to_SF']=1/(test_df['distance_to_SF']+0.001)\nnew_features_test_df['1/distance_to_LA']=1/(test_df['distance_to_LA']+0.001)\nnew_features_test_df['log_distance_to_SF']=np.log1p(test_df['distance_to_SF'])\nnew_features_test_df['log_distance_to_LA']=np.log1p(test_df['distance_to_LA'])\n", "intent": "Visually is not obvious so let's try to create a couple of new variables and check:\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nimport sklearn.linear_model as lm\ndf=pd.read_csv('/Users/apple/Documents/GitHub/APMAE4990-/data/hw2data.csv', index_col=0)\ndf=pd.DataFrame(df)\ndf.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "regr = Lasso(alpha=alpha_optim)\nregr.fit(X_train,y_train)\ndf_coeffs = pd.DataFrame({'coeffs':regr.coef_, 'name':X.columns.values})\ndf_coeffs=df_coeffs.sort_values(['coeffs'])\ndf_coeffs.iloc[0:110,:].plot(x='name',y='coeffs',kind='bar')\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntrain,test =train_test_split(df,test_size=0.2) \nprint len(train)\nprint len(test)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "kaggle_data = pd.DataFrame({'PassengerId':test.PassengerId, 'Survived':new_pred_class}).set_index('PassengerId')\nkaggle_data.to_csv('sub.csv')\n", "intent": "**Save DataFrame to csv**\n"}
{"snippet": "from sklearn.datasets import make_s_curve\ndata, colors = make_s_curve(n_samples=1000)\nprint(data.shape)\nprint(colors.shape)\n", "intent": "One dataset often used as an example of a simple nonlinear dataset is the S-curve:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n", "intent": "We split our data in a training and a test set again:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n", "intent": "To use a preprocessing method, we first import the estimator, here StandardScaler and instantiate it:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA()\n", "intent": "As always, we instantiate our PCA model. By default all directions are kept.\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42)\nX.shape\n", "intent": "Let's start of with a very simple and obvious example:\n"}
{"snippet": "new_features_train_df=pd.DataFrame(scaler.fit_transform(new_features_train_df),\n                            columns=new_features_train_df.columns, index=new_features_train_df.index)\nnew_features_test_df=pd.DataFrame(scaler.transform(new_features_test_df),\n                            columns=new_features_test_df.columns, index=new_features_test_df.index)\n", "intent": "And finally let's scale all this features\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    lfw_people.data, lfw_people.target, random_state=0)\nprint(X_train.shape, X_test.shape)\n", "intent": "We'll perform a Support Vector classification of the images.  We'll\ndo a typical train-test split on the images to make this happen:\n"}
{"snippet": "gram_vectorizer = CountVectorizer(ngram_range=(1, 2))\ngram_vectorizer.fit(X)\n", "intent": "Often we want to include unigrams (sigle tokens) and bigrams:\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nvectorizer = TfidfVectorizer()\nvectorizer.fit(text_train)\nX_train = vectorizer.transform(text_train)\nX_test = vectorizer.transform(text_test)\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n", "intent": "Previously, we applied the feature extraction manually, like so:\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\npipeline = make_pipeline(TfidfVectorizer(), LogisticRegression())\nparams = {'logisticregression__C': [.1, 1, 10, 100],\n          \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (2, 2)]}\ngrid = GridSearchCV(pipeline, param_grid=params, cv=5)\ngrid.fit(text_train, y_train)\nprint(grid.best_params_)\ngrid.score(text_test, y_test)\n", "intent": "Another benefit of using pipelines is that we can now also search over parameters of the feature extraction with ``GridSearchCV``:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntext_train_small, text_validation, target_train_small, target_validation = train_test_split(\n    text_train_all, np.array(target_train_all), test_size=.5, random_state=42)\n", "intent": "Let's split the training CSV file into a smaller training set and a validation set with 100k random tweets each:\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nprint digits.keys()\ndata         = digits[\"data\"]\nimages       = digits[\"images\"]\ntarget       = digits[\"target\"]\ntarget_names = digits[\"target_names\"]\n", "intent": "Explore the data and filter for instances of ones and sevens.\n"}
{"snippet": "test = test.fillna({'Fare' : test['Fare'].mean()}) \n", "intent": "As there is only one missing value, we can just input the mean of the column without any impact on the set.\n"}
{"snippet": "train = train.fillna({'Embarked' : 'S'})\n", "intent": "For the variable **Embarked**, as almost everyone came from Southampton we can directly put that.\n"}
{"snippet": "baja_si = df.loc[df['Baja'] == 'Baja SI', select]\nbaja_no = df.loc[df['Baja'] == 'Baja NO', select]\npca = PCA(n_components=3)\nprint(np.concatenate((baja_si, baja_no)).shape)\nPCA = pca.fit_transform(np.concatenate((baja_si,baja_no)))\n", "intent": "Using the elbow rule on the screeplot, it seems that 3 components might be enough to explain the most part of the information in the data.\n"}
{"snippet": "results_df=pd.DataFrame(columns=['model','CV_results', 'holdout_results'])\n", "intent": "Lets sum up the results of our project. We will compute RMSE on cross validation and holdout set and compare them.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.2, random_state=42)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata1=pd.read_csv(\"/home/zhejing/APMAE4990--master/data/hw2data.csv\")    \n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "regr = Lasso(alpha=opt_alpha)\nregr.fit(X_train, y_train)\ndf_coeffs = pd.DataFrame({'coeffs':regr.coef_, 'name':X.columns.values})\ndf_coeffs = df_coeffs.sort(columns=['coeffs'])\ndf_coeffs[::-1].head(50).plot(x='name', \n                              y='coeffs', \n                              kind='bar', \n                              title='First ten coefficients',\n                              figsize=(16, 8),\n                              fontsize=15)\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "X = df_final[['balance', 'income','student_Yes']]\ny = df_final.default_Yes\nseed = [10, 20, 30,40,50,60]\nfor s in seed:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=s)\n    clf = LogisticRegression(penalty='l1', C=100)\n    clf.fit(X_train, y_train)\n    plot_roc(clf, X_test, y_test, 'under random_state = %s' %s)\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "X = df_final[['balance', 'income','student_Yes']]\ny = df_final.default_Yes\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ntree = DecisionTreeClassifier(criterion='entropy',  max_depth=5)\ntree.fit(X_train, y_train)\nexport_graphviz(tree, out_file='tree.dot')                \n", "intent": "7) Train a Decision Tree classifier with maximum depth 5 and plot the decision tree. How does performance compare?\n"}
{"snippet": "from keras.preprocessing.sequence import pad_sequences\npad_tweet_list = pad_sequences(tokenized_tweet_list, maxlen=120)\n", "intent": "3\\. Use `pad_sequences` from `keras.preprocessing.sequence` to pad each sequence with zeros to **make the sequence length 120**. [2 pts]\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(pad_tweet_list, df['sentiments'][:5000], test_size=0.33, shuffle=True)\n", "intent": "4\\. Split the above data (the sequence and the label) into training (67%) and validation (33%) sets. [3 pts]\n"}
{"snippet": "df2015 = df.ix[(df['year']==2015)]\ndf2016 = df.ix[(df['year']==2016)]\ndf2015q1 = df2015.ix[(df2015['quarter']==1)]\ndf2015q234 = df2015.ix[(df2015['quarter']==2)|(df2015['quarter']==3)|(df2015['quarter']==4)]\ndf2015q1_s = df2015q1.pivot_table(index = 'store_number',aggfunc = sum)\ndf2015q234_s = df2015q234.pivot_table(index = 'store_number',aggfunc = sum)\ndf2016s = df2016.pivot_table(index = 'store_number',aggfunc = sum)\n", "intent": "Look for any statistical relationships, correlations, or other relevant properties of the dataset.\n"}
{"snippet": "import pandas as pd\ndata2=pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/ML_2017/Q2.csv\")\nfrom sklearn.cross_validation import train_test_split\ntrain,test=train_test_split(data2,random_state=100,test_size=0.4)\n", "intent": "In this question, we use data: \"Q2.csv\" for Bayesian Network Learning.\n"}
{"snippet": "train_part, test_part = train_test_split(data[['name','Manufacturer']], test_size=0.2, random_state=21, stratify=data[\"Manufacturer\"])\n", "intent": "At the first time let's build a model with only Description of Manufacturer as feature\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n", "intent": "Let's explore the dataset a little bit\n"}
{"snippet": "spam_np_vecs = np.zeros((len(spam_data1), 100))\nfor i, vec in enumerate(spam_data1.vecs):\n    spam_np_vecs[i, :] = vec\nspam_w2v_data = pd.concat([spam_data1.reset_index().label, pd.DataFrame(spam_np_vecs)], axis=1)\n", "intent": "Now let's just convert the format that we have into a final DataFrame with 100 features and 1 label for use in our document classification task:\n"}
{"snippet": "df_Beijing = pd.read_csv('../Data/FiveCitiesPM/Beijing.csv')\ndf_Beijing = df_Beijing[df_Beijing.year >= 2015]\ndf_Beijing.head(10)\n", "intent": "We'll start by working with Beijing data, and filter the dataset down to records from 2015. \n"}
{"snippet": "scaler = MinMaxScaler()\ndata = df.values\ndata = data.astype('float32')\nscaler.fit(data)\ndata = scaler.transform(data)\ntrain_size = int(len(data) * 0.67)\ntest_size = len(data) - train_size\ntrain, test = data[0:train_size,:], data[train_size:len(data),:]\nprint('Train Set contains:', len(train),'datapoints')\nprint('Test Set contains:', len(test),'datapoints')\n", "intent": "Use the MinMaxScaler to normalize the training and testing set betwee 0 and 1.  Name the normalized outputs norm_train and norm_test.\n"}
{"snippet": "df = df.fillna(df.median())\ndf.head()\n", "intent": "Fill the missing numbers with the median of the whole dataset! (hint: df.fillna?) Check to see if the counts for all columns is now 5000.\n"}
{"snippet": "from sklearn.preprocessing import LabelBinarizer\nlb = LabelBinarizer()\nfor col in ['churned']:\n    df[col] = lb.fit_transform(df[col])\nfrom sklearn.preprocessing import MinMaxScaler\nmsc = MinMaxScaler()\ndf = pd.DataFrame(msc.fit_transform(df),  \n                    columns=df.columns)\ny = df['churned']\nX = df[[col for col in df.columns if col!=\"churned\"]]\n", "intent": "Separate the data into the features and labels. Assign the features to variable X and labels to variable y.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.30, random_state=42)\nprint(\"Size of train dataset: {} rows\".format(X_train.shape[0]))\nprint(\"Size of test dataset: {} rows\".format(X_test.shape[0]))\n", "intent": "Split the data into 70% training set and 30% test set and assign them to variables named X_train, X_test, y_train, y_test (hint: train_test_split?)\n"}
{"snippet": "ames_df = pd.read_csv(\"http://www.amstat.org/publications/jse/v19n3/Decock/AmesHousing.txt\", delimiter=\"\\t\")\names_df.head()\n", "intent": "We'll be using the Ames Housing dataset during this exercise. Below we'll read the data in and display the first five rows.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ndata[float_columns] = sc.fit_transform(data[float_columns])\ndata.head(4)\n", "intent": "Perform feature scaling.\n"}
{"snippet": "columns_nans = ['race', 'diag_1', 'diag_2', 'diag_3']\nimp_most_frequent = SimpleImputer(missing_values='?', strategy='most_frequent', verbose=1)\nX_train_nan_most_frequent = pd.DataFrame(imp_most_frequent.fit_transform(X_train[columns_nans]),\n                                         columns=[el+'_mf' for el in columns_nans] )\nX_test_nan_most_frequent = pd.DataFrame(imp_most_frequent.transform(X_test[columns_nans]),\n                                         columns=[el+'_mf' for el in columns_nans] )\nX_train = pd.concat([X_train, X_train_nan_most_frequent], axis=1)\nX_test = pd.concat([X_test.reset_index(drop=True), X_test_nan_most_frequent], axis=1).set_index(X_test.index)\n", "intent": " - Baseline model: fill with most frequent value\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data, test_size=0.3, random_state=42)\n", "intent": "Next, split the data in train and test data sets.\n"}
{"snippet": "from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold=(.7 * (1 - .7)))\ndata2 = pd.concat([X_train,X_test])\ndata_new = pd.DataFrame(sel.fit_transform(data2))\ndata_y = pd.concat([y_train,y_test])\nfrom sklearn.model_selection import train_test_split\nX_new,X_test_new = train_test_split(data_new)\nY_new,Y_test_new = train_test_split(data_y)\n", "intent": " Identify highly correlated columns and drop those columns before building models\n"}
{"snippet": "skew = pd.DataFrame(data.skew())\nskew.columns = ['skew']\nskew['too_skewed'] = skew['skew'] > .75\nskew\n", "intent": "Notice that aside from the predictor variable, everything is float.\n"}
{"snippet": "print('Input features: \\n', datasets.load_iris().feature_names)\nprint()\nprint('Target classes: \\n', datasets.load_iris().target_names)\niris = datasets.load_iris().data\niris = preprocessing.scale(iris)\nspecies = datasets.load_iris().target\n", "intent": "* 4 dimensions is too many to plot\n"}
{"snippet": "data_path_train = os.path.join(os.getcwd(), 'datasets', 'splice_train.csv')\nsplice_train = pd.read_csv(data_path_train, delimiter = ',')\nprint('Number of instances: {}, number of attributes: {}'.format(splice_train.shape[0], splice_train.shape[1]))\nsplice_train.head(10)\n", "intent": "Load the `splice_train.csv` and `splice_test.csv` into two separate dataframes. Display the shape and first 10 instances for each dataframe.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nprint(digits.DESCR)\n", "intent": "Run the cell below to load the digits object and print its description.\n**Do not change any of the code in this question**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=42)\n", "intent": "Finally, let's make the train and test split. Again, don't alter this code.\n"}
{"snippet": "categories_df = pd.read_csv(\"categories.csv\", index_col=0)\ntrain_offsets_df = pd.read_csv(\"train_offsets.csv\", index_col=0)\ntrain_images_df = pd.read_csv(\"train_images.csv\", index_col=0)\nval_images_df = pd.read_csv(\"val_images.csv\", index_col=0)\n", "intent": "First load the lookup tables from the CSV files (you don't need to do this if you just did all the steps from part 1).\n"}
{"snippet": "from sklearn.datasets import fetch_lfw_people\nfaces = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n", "intent": "The data we'll use is a number of snapshots of the faces of world leaders. We'll fetch the data as follows:\n"}
{"snippet": "adf_kpss_returns = pd.DataFrame(index=asset_names, columns=['ADF','KPSS'])\nadf_returns = []\nkpss_returns = []\nfor i in range(len(asset_names)):\n    adf_returns.append(ts.adfuller(returns.iloc[:,i])[1])\n    kpss_returns.append(kpss(returns.iloc[:,i])[1])\n", "intent": "ACF and PACF demonstrate that time-series of returns should be stationary. Let's support this by performing ADF and KPSS tests.\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ntwenty_train = fetch_20newsgroups(subset='train',\n    categories=['comp.graphics', 'sci.med'], shuffle=True, random_state=0)\nprint(twenty_train.target_names)\n", "intent": "Working through an example.\n"}
{"snippet": "df=pd.read_csv('train.csv')\n", "intent": "Load `train.csv` from Kaggle into a pandas DataFrame.\n"}
{"snippet": "df = pd.read_csv('data/weather_2010-2016.csv');\n", "intent": "<h3>Time series prediction method</h3>\n"}
{"snippet": "df = pd.read_csv('data/weather_2010-2016.csv');\nfrom dateutil.parser import parse\ndf['Date'] = df['Date'].apply(lambda d: parse(d, dayfirst=True).date())\ndf = df.groupby('Date', as_index=False, sort=True)['Temp'].mean()\ndf['month'] = df['Date'].apply(lambda d: d.month)\ndf['day'] = df['Date'].apply(lambda d: d.day)\ndf.drop('Date', axis=1, inplace=True)\n", "intent": "<h3>Traditional method</h3>\n"}
{"snippet": "train_csv = pd.read_csv('data/train/train.csv')\nprint(train_csv.shape)\ntest_csv = pd.read_csv('data/test/test.csv')\nprint(test_csv.shape)\n", "intent": "Load dataset and check the size of train data and test data whichi is provided by the website.\n"}
{"snippet": "train_rows.to_csv('data/train_rows.csv', sep=',', index=False)\nvalid_rows.to_csv('data/valid_rows.csv', sep=',', index=False)\n", "intent": "For the convenience, both data frame are stored. They can be loaded in the future.\n"}
{"snippet": "train_input = train_rows.append(train_agument)\ntrain_input_stat = pd.DataFrame(train_input.landmark_id.value_counts())\ntrain_input_stat.reset_index(inplace=True)\ntrain_input_stat.columns = ['landmark_id','count']\ndisplay(train_input_stat.head())\ndisplay(train_input_stat.tail())\n", "intent": "After generating agumnet images, check if the train dataset is balance or not.\n"}
{"snippet": "train_reduce = pd.DataFrame()\nfor i in range(int(train_input.shape[0]/100)):\n    train_reduce = train_reduce.append(train_input.iloc[i*100:i*100+3])\ndisplay(train_reduce.shape)\ndisplay(train_reduce.head())\ndisplay(train_reduce.tail())\n", "intent": "There are too many train images so there are about 45000 images which will be used in this project.\n"}
{"snippet": "valid_reduce = pd.DataFrame()\nvalid_rows.sort_values(by=['landmark_id'])\ntmp_id = -1\nfor i in range(int(valid_rows.shape[0])):\n    if tmp_id != valid_rows.iloc[i]['landmark_id']:\n        valid_reduce = valid_reduce.append(valid_rows.iloc[i:i+1])\n        tmp_id = valid_rows.iloc[i]['landmark_id']\ndisplay(valid_reduce.shape)\ndisplay(valid_reduce.head())\n", "intent": "There are only two images for each lankmark to validate the model because the computer does not have enough memory.\n"}
{"snippet": "from sklearn.preprocessing import  StandardScaler\nscaler = StandardScaler()\n", "intent": "When we use linear model, **it's nesessary to standatrize** our data. Let's use **StandardScaler**:\n"}
{"snippet": "audio_file_path = '../Datasets'\nfilename = '/5e1b34a6_nohash_0.wav'\nsample_rate, samples = wavfile.read(str(audio_file_path) + filename)\n", "intent": "Let us read a sample audio file from this dataset: \n"}
{"snippet": "X = encode[:25]\ntsne_data = manifold.TSNE(n_components=2).fit_transform(X)\n", "intent": "We will take first 50 values to visualize the data.\n"}
{"snippet": "audio_file_path = '../Datasets'\nfilename = '/5e1b34a6_nohash_0.wav'\nsample_rate, samples = wavfile.read(str(audio_file_path) + filename)\nprint(sample_rate)\n", "intent": "Let us read a sample audio file from this dataset: \n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"Datasets/amazon.csv\")\nprint(data.describe())\ndata = data.dropna()\nprint(data.describe())\n", "intent": "**Objectives:** Create a linear regression based product rating solution.\n"}
{"snippet": "from sklearn import decomposition\npca = decomposition.PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X)\nprint(principalComponents.shape)\nplot_clustering(principalComponents,Y,Y,\"Distribution of clusters\")\n", "intent": "**Excerise 1:** Apply PCA on the data\n"}
{"snippet": "tsne_data = manifold.TSNE(n_components=2).fit_transform(X)\n", "intent": "**Excerise 4:** Apply t-SNE on the data\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"../Datasets/amazon_reviews.csv\")\nprint(data.describe())\ndata = data.dropna()\nprint(data.describe())\n", "intent": "**Objectives:** Create a non-linear regression based product rating solution.\n"}
{"snippet": "url = \"http://api.sdss3.org/spectrum?id=boss.3588.55184.511.v5_4_45&format=json\"\nresponse = urllib2.urlopen(url)\ndata = response.read()\ndata_dict = json.loads(data)\n", "intent": "With `urllib2`, have to format a string with some parameters, remember the ? and & in the URL, etc:\n"}
{"snippet": "print(\"total variance:{}\".format(np.sum(np.var(Xn,0))))\npca = PCA(2)\npca1 = pca.fit(Xn).transform(Xn)\nprint(\"variance explained via the first and second components:{}\\n\".format(pca.explained_variance_))\n", "intent": "Visualize the data using PCA (two dimensions). Color the points by the day of the week\n"}
{"snippet": "train_df, test_df, y_train, y_test = train_test_split(data_df.drop(['casual', 'registered', 'count'], axis=1), data_df[['casual', 'registered', 'count']], \n                                                      test_size=0.3, random_state=17, shuffle=True)\n", "intent": "I split data into train and hold-out samples \n"}
{"snippet": "print(\"total variance:{}\".format(np.sum(np.var(X,0))))\npca = PCA(2)\npca.fit(X)\nprint(\"variance explained via the first and second components:{}\\n\".format(pca.explained_variance_))\nprint(\"principal components:\\n{}\".format(pca.components_))\n", "intent": "(in this particular example ~97.6% of the variance is preserved if we project down to the leading principal component)\n"}
{"snippet": "admission = pd.read_csv('data/admission.csv')  \nadmission.head()\n", "intent": "Example - learning the probability of school admission based on two exams\n"}
{"snippet": "results.to_csv('./titanic_results.csv')\n", "intent": "To save your results (possibly for submission to a Kaggle competition), you can use the `.to_csv()` method to write the dataframe to a file.\n"}
{"snippet": "data = pd.read_csv('../data/housing-data.csv')\n", "intent": "Load a subset of the housing data\n"}
{"snippet": "forest_fires = pd.read_csv('../data/forestfires.csv')\nforest_fires.head()\n", "intent": "https://archive.ics.uci.edu/ml/datasets/Forest+Fires\n"}
{"snippet": "en = pd.read_csv('../data/ENB2012_data.csv',header=0)\nen.info()\n", "intent": "Back to our confusion matrix.  Let's refresh our data first.\n"}
{"snippet": "import pandas as pd\ntitanic = pd.read_csv('../data/titanic.csv')\ntitanic.head(10)\n", "intent": "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle.\n"}
{"snippet": "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\", header = None)\ndata.columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"]\ndata.head()\n", "intent": "Investigate the data and check for missing values - we've used .info() before:\n"}
{"snippet": "from sklearn.cross_validation import ShuffleSplit\ntitanic = pd.read_csv('../data/titanic.csv')\ntitanic.info()\n", "intent": "Random forests are a type of ensemble method (which we hinted at above) that build out groups of decision trees\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y, test_size=0.2, random_state=666)\n", "intent": "Split our train dataset to part, which we are going to train (X_train; same name), and to part with which we will check an error.\n"}
{"snippet": "mush_code = pd.DataFrame(mush['edible?'].map({'p':0,'e':1}))\nfor column in mush.columns:\n    if column == 'edible?':\n        continue\n    temp = pd.get_dummies(mush[column],prefix=column)\n    mush_code[temp.columns] = temp\nmush_code.head()\n", "intent": "For each new column, we'll preface it with the original column name\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "Load the sklearn `iris` dataset.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n", "intent": "So a concern if we're taking features away is are we losing too much information? And if we're losing information, are we gaining speed?\n"}
{"snippet": "data2 = pd.read_csv('C:\\Users\\Tam\\Google Drive\\Data\\GA\\Eviction_Notices.csv',header=0)\ndata2.info()\n", "intent": "Import Eviction Data\n"}
{"snippet": "all_data['GarageCars'] = all_data['GarageCars'].fillna((all_data['GarageCars'].mean()))\nall_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].fillna((all_data['TotalBsmtSF'].mean()))\n", "intent": "Fill missing values with the mean of the column.\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\ndegree = 20 \npf = PolynomialFeatures(degree)\nlr = LinearRegression() \nX_data = data[['x']]\nY_data = data['y']\nX_poly = pf.fit_transform(X_data)\n", "intent": "* Using the `PolynomialFeatures` class from Scikit-learn's preprocessing library, create 20th-order polynomial features \n"}
{"snippet": "data_path = ['C:/Users/japor/Desktop/Machine Learning 501 Nervana/Intel-ML101-Class4/data']\nfilepath = os.sep.join(data_path + ['Ames_Housing_Sales.csv'])\ndata = pd.read_csv(filepath, sep = ',')\n", "intent": "* Import the data with pandas, remove null values, and one-hot encode categoricals \n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data, test_size = 0.3, random_state = 42)\n", "intent": "Split the data into train and test data sets \n"}
{"snippet": "from sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold = 0.7 * (1 - 0.7))\ndata2 = pd.concat([X_train, X_test])\ndata_new = pd.DataFrame(sel.fit_transform(data2))\n", "intent": "Identify highly correlated columns and drop these columns before building models \n"}
{"snippet": "X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y, test_size=0.2, random_state=666)\n", "intent": "Split our train dataset again\n"}
{"snippet": "df = pd.DataFrame(zip(Y, labels), columns=['true','pred'])\ndf['correct'] = df.apply(lambda x: 1 if x['true'] == x['pred'] else 0, axis=1)\ndf\n", "intent": "Plot the predicted vs actual classifcations to see how our clustering analysis compares\n"}
{"snippet": "estimator3 = GradientBoostingRegressor()\nselector3 = RFECV(estimator3, step=1, cv=5)\nselector3 = selector3.fit(X, y)\nrfecv_columns3 = X.columns[selector3.support_]\nrfecv_columns3\n", "intent": "Variables relating to the Gradient Boosting Regressor will be marked either 'gbr' or '3'.\n"}
{"snippet": "estimator4 = ExtraTreesRegressor()\nselector4 = RFECV(estimator4, step=1, cv=5)\nselector4 = selector4.fit(X, y)\nrfecv_columns4 = X.columns[selector4.support_]\nrfecv_columns4\n", "intent": "Variables relating to the Extra Trees Regressor will be marked either 'etr' or '4'.\n"}
{"snippet": "train.workclass.value_counts(sort=True)\ntrain.workclass.fillna('Private', inplace=True)\n", "intent": "Fix missing values in 'workclass', 'occupation', and 'native country'\n"}
{"snippet": "housing_df = pd.read_csv('WestRoxbury.csv')\n", "intent": "Load the West Roxbury data set\n"}
{"snippet": "Amtrak_df = pd.read_csv('Amtrak.csv', squeeze=True)\nAmtrak_df['Date'] = pd.to_datetime(Amtrak_df.Month, format='%d/%m/%Y')\nridership_ts = pd.Series(Amtrak_df.Ridership.values, index=Amtrak_df.Date)\n", "intent": "Load the Amtrak data and convert them to be suitable for time series analysis\n"}
{"snippet": "universal_df = pd.read_csv('UniversalBank.csv')\nuniversal_df.plot.scatter(x='Income', y='CCAvg', \n                          c=['black' if c == 1 else 'grey' for c in universal_df['Securities Account']],\n                          ylim = (0.05, 10), alpha=0.2,\n                          logx=True, logy=True)\n", "intent": "Use `alpha` to add transparent colors \n"}
{"snippet": "import gmaps\nSCstudents = pd.read_csv('SC-US-students-GPS-data-2016.csv')\ngmaps.configure(api_key=os.environ['GMAPS_API_KEY'])\nfig = gmaps.figure(center=(39.7, -105), zoom_level=3)\nfig.add_layer(gmaps.symbol_layer(SCstudents, scale=2, \n                                 fill_color='red', stroke_color='red'))\nfig\n", "intent": "To run this example you need an API key for Google maps. Go to https://cloud.google.com/maps-platform/ to find out how to getone\n"}
{"snippet": "pd.DataFrame({'mean': bostonHousing_df.mean(),\n              'sd': bostonHousing_df.std(),\n              'min': bostonHousing_df.min(),\n              'max': bostonHousing_df.max(),\n              'median': bostonHousing_df.median(),\n              'length': len(bostonHousing_df),\n              'miss.val': bostonHousing_df.isnull().sum(),\n             })\n", "intent": "Compute mean, standard dev., min, max, median, length, and missing values for all variables\n"}
{"snippet": "X_train, X_train_test, y_train, y_train_test = train_test_split(df_train_store_mlc, y, \n                                                                test_size=0.2, random_state=17)\n", "intent": "> Now we will divide out train set on **80% training and 20% validation**.\n"}
{"snippet": "pcsSummary = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n                           'Proportion of variance': pcs.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\npcsSummary = pcsSummary.transpose()\npcsSummary.columns = ['PC1', 'PC2']\npcsSummary.round(4)\n", "intent": "The importance of components can be assessed using the explained variance.\n"}
{"snippet": "scores = pd.DataFrame(pcs.transform(cereals_df[['calories', 'rating']]), \n                      columns=['PC1', 'PC2'])\nscores.head()\n", "intent": "Use the `transform` method to get the scores.\n"}
{"snippet": "pcs = PCA()\npcs.fit(cereals_df.iloc[:, 3:].dropna(axis=0))\npcsSummary_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n                           'Proportion of variance': pcs.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\npcsSummary_df = pcsSummary_df.transpose()\npcsSummary_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_df.columns) + 1)]\npcsSummary_df.round(4)\n", "intent": "Perform a principal component analysis of the whole table ignoring the first three non-numerical columns.\n"}
{"snippet": "scaler = preprocessing.StandardScaler()\nscaler.fit(trainData[['Income', 'Lot_Size']])  \nmowerNorm = pd.concat([pd.DataFrame(scaler.transform(mower_df[['Income', 'Lot_Size']]), \n                                    columns=['zIncome', 'zLot_Size']),\n                       mower_df[['Ownership', 'Number']]], axis=1)\ntrainNorm = mowerNorm.iloc[trainData.index]\nvalidNorm = mowerNorm.iloc[validData.index]\nnewHouseholdNorm = pd.DataFrame(scaler.transform(newHousehold), columns=['zIncome', 'zLot_Size'])\n", "intent": "Initialize normalized training, validation, and complete data frames. Use the training data to learn the transformation.\n"}
{"snippet": "utilities_df = pd.read_csv('Utilities.csv')\nutilities_df.set_index('Company', inplace=True)\nutilities_df = utilities_df.apply(lambda x: x.astype('float64'))\nutilities_df.head()\n", "intent": "Load the data, set row names (index) to the utilities column (company) and remove it. Convert all columns to `float`\n"}
{"snippet": "d = pairwise.pairwise_distances(utilities_df, metric='euclidean')\npd.DataFrame(d, columns=utilities_df.index, index=utilities_df.index).head(5)\n", "intent": "Compute Euclidean distance matrix (to compute other metrics, change the name of `metric` argument)\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nbinary_vectorizer = CountVectorizer(binary=True)\nX = binary_vectorizer.fit_transform(corpus)\nprint X.todense()\n", "intent": "Now let's use scikit-learn to create our feature representations. `CountVectorizer` can be used to convert a corpus to a matrix of token counts.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=4)\n", "intent": "Train/Test split\nScikitlearn has its own methodology\nThere is a slightly criptics command that we need.\n"}
{"snippet": "path = os.getcwd() + '/data/ex2data2.txt'\ndata2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])\ndata2.head()\n", "intent": "Similar to part 1, let's start by visualizing the data.\n"}
{"snippet": "num_cols = key_cols+['prev_flux']\ntrain_ohe = pd.concat([pd.get_dummies(train_df['Year'], prefix = 'Year'), train_df[num_cols]], axis=1)\ntransformers = [('num', StandardScaler(), num_cols)]\nct = ColumnTransformer(transformers=transformers, remainder = 'passthrough' )\nlogit_pipe = Pipeline([('transform', ct), ('logit', LogisticRegression(C=0.01, class_weight='balanced', random_state=17))])\nlogit_pipe.fit(train_ohe.loc[:idx_split,:], train_df.loc[:idx_split,'bin_target'])\n", "intent": "Now time to fit model on the whole dataset and test it on the hold out\n"}
{"snippet": "chilledw_train = pd.DataFrame(data=normalized_chilledWater, index=np.arange('2012-01', '2014-01', dtype='datetime64[D]')).dropna()\nchilledw_test = pd.DataFrame(data=normalized_chilledWater, index=np.arange('2014-01', '2014-11', dtype='datetime64[D]')).dropna()\nXX_chilledw_train = chilledw_train.drop('chilledWater-TonDays', axis = 1).reset_index().drop('index', axis = 1)\nXX_chilledw_test = chilledw_test.drop('chilledWater-TonDays', axis = 1).reset_index().drop('index', axis = 1)\nYY_chilledw_train = chilledw_train['chilledWater-TonDays']\nYY_chilledw_test = chilledw_test['chilledWater-TonDays']\nprint XX_chilledw_train.shape, XX_chilledw_test.shape\n", "intent": "Analysis of chilled water data.\n"}
{"snippet": "steam_train = pd.DataFrame(data=normalized_steam, index=np.arange('2012-01', '2014-01', dtype='datetime64[D]')).dropna()\nsteam_test = pd.DataFrame(data=normalized_steam, index=np.arange('2014-01', '2014-11', dtype='datetime64[D]')).dropna()\nXX_steam_train = steam_train.drop('steam-LBS', axis = 1).reset_index().drop('index', axis = 1)\nXX_steam_test = steam_test.drop('steam-LBS', axis = 1).reset_index().drop('index', axis = 1)\nYY_steam_train = steam_train['steam-LBS']\nYY_steam_test = steam_test['steam-LBS']\nprint XX_steam_train.shape, XX_steam_test.shape\n", "intent": "Analysis of steam data.\n"}
{"snippet": "turnstile_data = pd.read_csv(\"turnstile_data_master_with_weather.csv\")\nturnstile_data.head()\n", "intent": "However, we're going to use the pandas library to make it much easier to analyse and chart the data contained in this CSV file.\n"}
{"snippet": "from sklearn.decomposition import PCA\nboth = [X[i] for i in range(len(y)) if y[i] == 0 or y[i] == 1]\nlabels = [y_ for y_ in y if y_ == 0 or y_ == 1]\npca = PCA(n_components=3)\nXproj3d = pca.fit_transform(both)\n", "intent": "Let's try to approximate the relative positions of our points in $\\mathbb{R}^3$, then:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(2)\nXproj = pca.fit_transform(X)\n", "intent": " How effective do you think logistic regression will be on the entire `digits` dataset?\n"}
{"snippet": "corpus_raw = u\"\"\nfor book_filename in book_filenames:\n    print(\"Reading '{0}'...\".format(book_filename))\n    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n        corpus_raw += book_file.read()\n    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n    print()\n", "intent": "**Combine the books into one string**\n"}
{"snippet": "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)\n", "intent": "**Train t-SNE, this could take a minute or two...**\n"}
{"snippet": "points = pd.DataFrame(\n    [\n        (word, coords[0], coords[1])\n        for word, coords in [\n            (word, all_word_vectors_matrix_2d[thrones2vec.vocab[word].index])\n            for word in thrones2vec.vocab\n        ]\n    ],\n    columns=[\"word\", \"x\", \"y\"]\n)\n", "intent": "**Plot the big picture**\n"}
{"snippet": "df = pd.read_csv('house-votes-84.data', header=None)\n", "intent": "Load the data into a pandas dataframe. \n"}
{"snippet": "raw_data = pd.read_csv(\"../../data/1987.csv.bz2\")\n", "intent": "Load dataset. Change path if needed.\n"}
{"snippet": "heart = pd.read_csv(\"./processed.cleveland.data.clean\", header=None)\n", "intent": "Use `pd.read_csv` to read in the file (add header=None).\nUse `pd.shape` to check dimensions and `pd.head()` to take a look at it.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX_scaled = pd.DataFrame(ss.fit_transform(X))\nX_scaled.columns = X.columns\nX = X_scaled\n", "intent": "Use scikit-learn `StandardScaler()` to scale all of the features and use `seaborn` to produce a pairplot and a heatmap of the correlctions.\n"}
{"snippet": "components = pd.DataFrame(pca.components_, \n                        columns = X.columns,\n                        index   = X.columns)\n", "intent": "Check out the `components` of the PCA object - what do they mean?\n"}
{"snippet": "X_pca = pd.DataFrame(pca.transform(X))\nX_pca.columns = ['PC' + str(i+1) for i in range(0,13)]\nX_pca = X_pca.join(y)\nX_pca.head()\n", "intent": "Plot the first two principal components and overlay the disease state as the color to see if they cluster:\n"}
{"snippet": "loans = pd.read_csv(\"loan_data.csv\")\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "college_data = pd.read_csv('College_Data')\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "ad_data = pd.read_csv(\"advertising.csv\")\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "X = cv.fit_transform(X) \n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "df = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n", "intent": "We have a small dataset but a large number of features. This is a good candidate for PCA - want to\nreduce these features to a more reasonable number\n"}
{"snippet": "raw_data.fillna(0, inplace=True)\n", "intent": "All columns associated with cancelled flights have a MaN in the some columns, like a 'Carrier Delay', or other delays. Filing it by zero.\n"}
{"snippet": "from sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nX, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4, bias=100.0)\nmodel = LinearRegression()\nmodel.fit(X, y)\n", "intent": "Create a model to quantify\n"}
{"snippet": "from sklearn.datasets import make_regression\nn_features = 3\nX, y = make_regression(n_samples=30, n_features=n_features, \n                       n_informative=n_features, random_state=42, \n                       noise=0.5, bias=100.0)\nprint(X.shape)\n", "intent": "Generate a linear dataset with 3 features\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nX_train.head()\n", "intent": "The first step is to split your data into Training and Testing using `train_test_split`.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n", "intent": "Step 2) Split data into training and testing data\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_scaler = StandardScaler().fit(X_train)\ny_scaler = StandardScaler().fit(y_train)\nX_train_scaled = X_scaler.transform(X_train)\nX_test_scaled = X_scaler.transform(X_test)\ny_train_scaled = y_scaler.transform(y_train)\ny_test_scaled = y_scaler.transform(y_test)\n", "intent": "Step 3) Scale or Normalize your data. Use StandardScaler if you don't know anything about your data.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n", "intent": "Split our data into training and testing\n"}
{"snippet": "from sklearn.decomposition import RandomizedPCA\nn_components = 64\npca = RandomizedPCA(n_components).fit(X_train_processed)\nX_train_pca = pca.transform(X_train_processed)\nX_test_pca = pca.transform(X_test_processed)\n", "intent": "Now we are going to apply **PCA** to obtain a dictionary of codewords. \n**`RamdomizedPCA`** class is what we need.\n"}
{"snippet": "dfIBM = pd.read_csv('./data/WA_Fn-UseC_-HR-Employee-Attrition.csv')\n", "intent": "Let's get the data, look at the first lines, check types and omissions\n"}
{"snippet": "df_test = pd.read_csv('titanic/test.csv')\ndf_test.head()\n", "intent": "Read the test data:\n"}
{"snippet": "df_test['Survived'] = test_y\ndf_test[['PassengerId', 'Survived']] \\\n    .to_csv('titanic/results-rf.csv', index=False)\n", "intent": "Create a DataFrame by combining the index from the test data with the output of predictions, then write the results to the output:\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_feat,columns=df.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.3,random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "pca = PCA(n_components=2, whiten=False)\npca.fit(X)\nY = pca.transform(X)\n", "intent": "Now use the first 2 `loadings` of the PCA and plot them in a scatter plot and color the by the lables `y`.\n"}
{"snippet": "scaled_bnote = scaler.fit_transform(bnote.drop('Class',axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "bnote_feat = pd.DataFrame(data = scaled_bnote, columns=bnote.columns[:-1])\nbnote_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "** Use SciKit Learn to create training and testing sets of the data as we've done in previous lectures:**\n"}
{"snippet": "cdf = pd.read_csv('College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "pd.concat([pd.DataFrame({'Unique Values': dfIBM.nunique().sort_values()}),\n           pd.DataFrame({'Type': dfIBM.dtypes})], axis=1, sort=False).sort_values(by='Unique Values')\n", "intent": "We can check count of unique values for all features\n"}
{"snippet": "ad_data = pd.read_csv('advertising.csv')\nad_data.head()\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler() \nnumerical = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\nfeatures_log_minmax_transformed = pd.DataFrame(data = features_log_transformed)\nfeatures_log_minmax_transformed[numerical] = scaler.fit_transform(features_log_transformed[numerical])\ndisplay(features_log_minmax_transformed.head(n = 5))\n", "intent": "Data normalization ensures that each feature is treated equally when applying supervised learners. \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features_log_minmax_transformed, \n                                                    outcome, \n                                                    test_size = 0.2, \n                                                    random_state = 0)\nprint (\"Training set has {} samples.\".format(X_train.shape[0]))\nprint (\"Testing set has {} samples.\".format(X_test.shape[0]))\n", "intent": "Data is splitted in 8:2 proportion to created training and testing set\n"}
{"snippet": "import pandas as pd\nfrom IPython.display import display \ndata = pd.read_csv('student_data.csv')\ndisplay(data.head(10))\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "train, test = train_test_split(df, test_size = 0.2, random_state = 0)\ny_train, y_test = train['loss'], test['loss']\ndel train['loss'], test['loss']\n", "intent": "This is equivalent to a single iteration of 5-fold CV\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(documents,lowercase=True,stop_words='english')\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "from keras.datasets import mnist\n(train_images, train_labels), (val_images, val_labels) = mnist.load_data()\n", "intent": "Load the MNIST data.\n"}
{"snippet": "from keras.datasets import fashion_mnist\n(train_images, train_labels), (val_images, val_labels) = fashion_mnist.load_data()\n", "intent": "Load the fashion MNIST data.\n"}
{"snippet": "from keras.datasets import mnist\n(train_images, train_labels), (val_images, val_labels) = mnist.load_data()\n", "intent": "First we load the MNIST data.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_sc = pd.DataFrame(sc.fit_transform(X_train[Categorical_with_order+Numeric]),\n                          columns=Categorical_with_order+Numeric, index=X_train.index)\nX_holdout_sc = pd.DataFrame(sc.transform(X_holdout[Categorical_with_order+Numeric]),\n                            columns=Categorical_with_order+Numeric, index=X_holdout.index)\n", "intent": "Now we can to scale all numerical features for use Logistic Regression.\n"}
{"snippet": "train_data, test_data,train_target, test_target = train_test_split(data, target, test_size=0.33, random_state=54)\n", "intent": "Splitt inn i trening og testdata med `train_test_split`\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_data, test_data,train_target, test_target = train_test_split(df2[['0','1']], df2['2'], test_size=0.33, random_state=54)\n", "intent": "Splitt inn i trening og testdata med `train_test_split`\n"}
{"snippet": "import pandas as pd\ndftrain = pd.read_csv('myimagery/traindata.csv')\ndftest = pd.read_csv('myimagery/testdata.csv')\nprint dftrain.head(5)\n", "intent": "Read in the CSV file as Pandas dataframes\n"}
{"snippet": "colnames=['Age','Workclass','Sector','Education','Education-num','Marital-Status','Occupation','Relationship','Race','Sex','Capital-Gain','Capital-Loss','Hours-Per-Week','Native-Country','y']\ndf =pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',names=colnames)\n", "intent": "We begin by reading in the data with the column names specified\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\niris.keys()\n", "intent": "<h3>Collecting Data</h3>\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df\nY = iris.target\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 0)\n", "intent": "<h3>Splitting the dataset into training and testing datasets</h3>\n"}
{"snippet": "import pandas as pd\ntrain_data = pd.read_csv('train.csv')\ntrain_data.head(4)\n", "intent": "<h3>Collecting Training Data</h3>\n"}
{"snippet": "df = pd.DataFrame()\ndf['ship_type']  =  np.random.choice([\"romulan\", \"human\", \"klingon\", \"borg\", \"red_shirt\", \"ovid\"], size=500)\ndf['ship_value'] =  np.random.randint(200000, 10000000, size=500)\ndf['ship_speed'] =  np.random.randint(10, 60, size=500)\ndf['baths']      =  np.random.choice(np.arange(1, 4, 0.5), size=500)\n", "intent": "With 2-3 continious variables, and one categorical.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler      =  StandardScaler()\ncolumns     =  ['ship_value', 'ship_speed']\ndf[columns] =  scaler.fit_transform(df[columns])\nformula  =  \"baths ~ ship_value + ship_speed\"\ny, X     =  patsy.dmatrices(formula, data=df, return_type=\"dataframe\")\nlm = LinearRegression()\nmodel = lm.fit(X, y)\nscore = model.score(X, y)\nprint \"R^2: \", score\n", "intent": "This is a very standard implementation of Patsy with feature scaling.\n"}
{"snippet": "pd.DataFrame({'Name': X_train_RF.columns.values,\n              'Coefficient': rf.feature_importances_}).sort_values(by='Coefficient',\n                                                                   ascending=False)\n", "intent": "Let's see features importances\n"}
{"snippet": "df = pd.read_csv('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/datasets/breast_cancer_wisconsin/wdbc.data', \n                  header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "b_cancer = pd.read_csv('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/datasets/breast_cancer_wisconsin/breast_cancer.csv')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "docs = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\ndocs.sum()\n", "intent": "What is being counted?\n_Warning: big ugly sparse matrix ahead._\n"}
{"snippet": "hep_pcs = pca.transform(Xn)\nhep_pcs = pd.DataFrame(pca.components_, columns=['PC'+str(i) for i in range(len(event_names))])\nhep_pcs['athlete'] = hep['Unnamed: 0']\nhep_pcs['score'] = hep['score']\nhep_pcs.head()\n", "intent": "---\nAdd back in the athelete and score columns from the original data.\n"}
{"snippet": "cvt      =  CountVectorizer(strip_accents='unicode', ngram_range=(2,4))\nX_all    =  cvt.fit_transform(insults_df['Comment'])\ncolumns  =  cvt.get_feature_names()\nx_df     =  pd.DataFrame(X_all.toarray(), columns=columns)\ntf_df    =  pd.DataFrame(x_df.sum(), columns=[\"freq\"])\ntf_df.sort_values(\"freq\", ascending=False).head(10)\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "cvt = CountVectorizer(stop_words=\"english\", ngram_range=(4,6))\nX_all = cvt.fit_transform(insults_df['Comment'])\nx_df     =  pd.DataFrame(X_all.toarray(), columns=cvt.get_feature_names())\ntf_df    =  pd.DataFrame(x_df.sum(), columns=[\"freq\"])\ntf_df.sort_values(\"freq\", ascending=False).head(10)\n", "intent": "Display the top 75 ngrams with frequencies.\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\nnewsgroups_train = fetch_20newsgroups(subset='train')\nnewsgroups_train.target_names\n", "intent": "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n"}
{"snippet": "import pandas as pd\nvehicles = pd.read_csv('used_vehicles.csv')\nvehicles\n", "intent": "In this example we are going to predict price of vehicles\n"}
{"snippet": "oos = pd.read_csv('used_vehicles_oos.csv')\noos['type'] = oos.type.map({'car':0, 'truck':1})\noos\n", "intent": "To test the models we will use a pre-prepared out-of-sample data set, which will need to apply the same preparations to.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=24)\nprint('Train size:', X_train.size)\nprint('Test size:', X_test.size)\n", "intent": "Split data into train/test with proportional 0.7/0.3 which is common split for such amount of data.\n"}
{"snippet": "stocks = pd.read_csv(\"../Data/stock_px.csv\", parse_dates = True, index_col=0)\n", "intent": "Let's load in some [stock data](../Data/stock_px.csv) and take a look at how we can use `Pandas` to plot some familiar quantities from that data.\n"}
{"snippet": "from sklearn.decomposition import PCA\nRANDOM_STATE=1234\npca = PCA(n_components=2, random_state=RANDOM_STATE) \niris_2d = pca.fit_transform(iris) \n", "intent": "We'll use [scikit-learn's PCA](http://scikit-learn.org/stable/modules/decomposition.html\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2) \niris_2d = pca.fit_transform(iris) \n", "intent": "We'll use [scikit-learn's PCA](http://scikit-learn.org/0.17/modules/decomposition.html\n"}
{"snippet": "hsq = pd.read_csv('./hsq_data.csv')\n", "intent": "It is worth reading over the description of the data columns above for this.\n"}
{"snippet": "tp = np.sum((y_test == 1) & (y_pred_cv == 1))\nfp = np.sum((y_test == 0) & (y_pred_cv == 1))\ntn = np.sum((y_test == 0) & (y_pred_cv == 0))\nfn = np.sum((y_test == 1) & (y_pred_cv == 0))\nprint(tp, fp, tn, fn)\ncm = np.array(confusion_matrix(y_test, y_pred_cv, labels=[1,0]))\nconfusion = pd.DataFrame(cm, index=['is_male', 'is-female'],\n                         columns=['predicted_male', 'predicted_female'])\nconfusion.T\n", "intent": "**9.C Construct the confusion matrix for the Ridge LR.**\n"}
{"snippet": "train_df = pd.read_csv(os.path.join(PATH, 'train_sample.csv'), nrows=200000)\ntrain_df = train_df.join(pd.read_csv(os.path.join(PATH,'mlboot_train_answers.tsv'), delimiter='\\t').set_index('cuid'), on='cuid', how='inner')\n", "intent": "** DictVectorizer **\n"}
{"snippet": "train_df = pd.read_csv(os.path.join(PATH, 'train_sample.csv'), nrows=200000)\ntrain_df = train_df.join(pd.read_csv(os.path.join(PATH,'mlboot_train_answers.tsv'), delimiter='\\t').set_index('cuid'), on='cuid', how='inner')\ntrain_df['cnt1'] = train_df['cnt1'].apply(lambda x: x[1:-1]+',' if len(x)>2 else '')\ntrain_df['cnt2'] = train_df['cnt2'].apply(lambda x: x[1:-1]+',' if len(x)>2 else '')\ntrain_df['cnt3'] = train_df['cnt3'].apply(lambda x: x[1:-1]+',' if len(x)>2 else '')\n", "intent": "** DictVectorizer2 **\n"}
{"snippet": "train_df = pd.read_csv(os.path.join(PATH, 'train_sample.csv'), nrows=700000)\ntrain_df = train_df.join(pd.read_csv(os.path.join(PATH,'mlboot_train_answers.tsv'), delimiter='\\t').set_index('cuid'), on='cuid', how='inner')\n", "intent": "** DictVectorizer **\n"}
{"snippet": "train_df = pd.read_csv(os.path.join(PATH, 'train_sample.csv'), nrows=700000)\ntrain_df = train_df.join(pd.read_csv(os.path.join(PATH,'mlboot_train_answers.tsv'), delimiter='\\t').set_index('cuid'), on='cuid', how='inner')\ntrain_df['cnt1'] = train_df['cnt1'].apply(lambda x: x[1:-1]+',' if len(x)>2 else '')\n", "intent": "** DictVectorizer2 **\n"}
{"snippet": "for column in X_train.columns:\n    X_train_sq, sq_attr = apply_cat_op(X_train, [column], sq_operation, 'sq_')\n    data = pd.concat([X_train, X_train_sq], axis=1)\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data)\n    grig_search = GridSearchCV(model, model_parameters, n_jobs=-1, cv=cv, scoring='accuracy')\n    grig_search.fit(data_scaled, y_train);\n    print('Column:', column, ' ', \n          'Accuracy:', grig_search.best_score_, ' ',\n          'Best params:', grig_search.best_params_)\n", "intent": "Create squared feature for each columns and test in with model:\n"}
{"snippet": "from sklearn.datasets import fetch_olivetti_faces\nfaces = fetch_olivetti_faces(shuffle=False)\nprint(faces.DESCR)\n", "intent": "Import the olivetti faces dataset.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer()\nprint(X[:2])\nprint()\nprint(vec.fit_transform(X).toarray()[:2])\nprint()\nprint(vec.get_feature_names())\n", "intent": "Note that we create our validation folds from our complete dataset as contrasted to creating folds from the training set in the SVM notebook ...\n"}
{"snippet": "file_path = 'shakespeare.txt'\nwith open(file_path,'r') as f:\n    data = f.read()\n    print(\"Data length:\", len(data))\ndata = data.lower()\n", "intent": "In this notebook we'll use shakespeare data, but you can basically choose any text file you like!\n"}
{"snippet": "test_predictions = \ntest_pred_inversed = mlb.inverse_transform(test_predictions)\ntest_predictions_for_submission = '\\n'.join('%i\\t%s' % (i, ','.join(row)) for i, row in enumerate(test_pred_inversed))\ngrader.submit_tag('MultilabelClassification', test_predictions_for_submission)\n", "intent": "When you are happy with the quality, create predictions for *test* set, which you will submit to Coursera.\n"}
{"snippet": "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_log1p_recommends.csv'), \n                           index_col='id')\n", "intent": "Read targets from file.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error, confusion_matrix\nfrom sklearn.grid_search import GridSearchCV\nAuto = pd.read_csv('Auto.csv', na_values='?').drop('name',axis = 1).dropna()\nAuto.head(5)\n", "intent": "Following code will load and clean the dataset and load some useful functions\n"}
{"snippet": "california_housing_dataframe = pd.read_csv(\"https://dl.google.com/mlcc/mledu-datasets/california_housing_train.csv\", sep=\",\")\n", "intent": "Next, we'll load our data set.\n"}
{"snippet": "from sklearn import preprocessing\nstd_scale = preprocessing.StandardScaler().fit(df[['SQFT', 'BDRMS', 'AGE']])\ndf_std = std_scale.transform(df[['SQFT', 'BDRMS', 'AGE']])\nminmax_scale = preprocessing.MinMaxScaler().fit(df[['SQFT', 'BDRMS', 'AGE']])\ndf_minmax = minmax_scale.transform(df[['SQFT', 'BDRMS', 'AGE']])\n", "intent": "Refer to this if you are confused as to the formula: [Standardization](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n"}
{"snippet": "class sklearn.preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)\n", "intent": "[StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n"}
{"snippet": "trainLabels = pd.read_csv('trainLabels.csv')\ntrainLabels.head()\n", "intent": "Train test has 35126 marked images\n"}
{"snippet": "df_demo = pd.read_csv('iowa_demographics.csv')\ndf_demo.head()\n", "intent": "[Source](https://en.wikipedia.org/wiki/List_of_counties_in_Iowa)\n"}
{"snippet": "df = pd.DataFrame(bos)\ndf.corr()\n", "intent": "**Your turn**: What are some other numeric variables of interest? Plot scatter plots with these variables and *PRICE*.\n"}
{"snippet": "MeanAge = TitanicTrain.Age.mean()\nMedianAge = TitanicTrain.Age.median()\nTitanic.Age = Titanic.Age.fillna(value=MedianAge)\nTitanic[\"AgeGroup\"] = Titanic.apply(lambda row: AgeGroup(row[\"Age\"]), axis=1)\nTitanic[\"Major\"] = Titanic.apply(lambda row: IsMajor(row[\"Age\"]), axis=1)\n", "intent": "**3.1 Impute missing values**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscale = StandardScaler().fit(Titanic[['Age', 'Fare']])\nTitanic[['Age', 'Fare']] = scale.transform(Titanic[['Age', 'Fare']])\n", "intent": "**3.3 Scaling numerical features**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state = 0)\n", "intent": "Separate the points into a training and a test datasets with `random_state = 0`.\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\n", "intent": "Now let's use a decision tree on a real dataset.\n"}
{"snippet": "import numpy as np\nlines = open(\"data_clustering.csv\").read().split()[1:]\nx = np.array([line.split(',')[0] for line in lines], \n             dtype=np.float32)\ny = np.array([line.split(',')[1] for line in lines], \n             dtype=np.float32)\n", "intent": "It seems that we have 2 columns, named V1 and V2. Let's load it into 2 ndarrays : x and y.\n"}
{"snippet": "z = pandas.DataFrame(wine_tree.feature_importances_,\n                     index=X_train.columns,\n                     columns=[\"Importance\"])\nz.sort_values(by=\"Importance\", ascending=False).head(3)\n", "intent": "**Answer**: With optimized hyperparameters, the test accuracy can reach 0.845.\n"}
{"snippet": "train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\ntrain.describe(include=\"all\")\n", "intent": "It's time to read in our training and testing data using `pd.read_csv`, and take a first look at the training data using the `describe()` function.\n"}
{"snippet": "cols_to_fill = [\n    'transactions_count', 'sum_transactions_count', \n    'mean_transactions_count', 'std_transactions_count',\n    'min_transactions_count', 'max_transactions_count',    \n]\ntrain[cols_to_fill] = train[cols_to_fill].fillna(0)\ntest[cols_to_fill] = test[cols_to_fill].fillna(0)\n", "intent": "Fill in the data on `card_id` that do not have transactions over the past three months.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)\n", "intent": "We will use part of our training data (22% in this case) to test the accuracy of our different models.\n"}
{"snippet": "models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)\n", "intent": "Let's compare the accuracies of each model!\n"}
{"snippet": "from skimage import io\npic = io.imread('data/bird_small.png') / 255.\nio.imshow(pic)\n", "intent": "http://scikit-image.org/\n"}
{"snippet": "Xval, Xtest, yval, ytest = train_test_split(mat.get('Xval'),\n                                            mat.get('yval').ravel(),\n                                            test_size=0.5)\n", "intent": "divide original validation data into validation and test set\n"}
{"snippet": "from sklearn.datasets import fetch_olivetti_faces, fetch_lfw_people\nfrom ipywidgets import interact\nimage_shape = (64, 64)\ndataset = fetch_olivetti_faces()\nfaces = dataset.data\n", "intent": "Next, we will take a look at what happens if we project some dataset consisting of human faces onto some basis we call\nthe \"eigenfaces\".\n"}
{"snippet": "lenses_df = pd.read_csv('../data/lenses.csv', index_col=0, \n                        names=['Age', 'Spectacle Prescription', 'Astigmatic', 'Tear Production Rate','Prescription Class'])\nlenses_df\n", "intent": "<img src='assets/images/lenses_dataset.png' width=700px>\n"}
{"snippet": "feature_matrix_train, \\\nfeature_matrix_test, \\\ntarget_vector_train, \\\ntarget_vector_test = train_test_split(istanbul_stocks_feature, \n                                      istanbul_stocks_target, \n                                      test_size=0.1,\n                                      random_state=11)\n", "intent": "We will use forward selection to develop our models.\n"}
{"snippet": "from time import time\nfrom sklearn import datasets\nX, y = datasets.make_regression(int(1e5), \n                                n_features=1000, \n                                noise=75.0)\n", "intent": "We will use it to construct a regression problem with 1000000 instances and 1000 features.\n"}
{"snippet": "df = pd.read_csv('./breast_cancer_wisconsin/wdbc.data', header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer as DV\nencoder = DV(sparse = False)\ncat_hot_x = encoder.fit_transform(X[categ2].T.to_dict().values())\n", "intent": "Let's apply OneHotEncoding for categorical features and scale numeric features\n"}
{"snippet": "def general_classifier(X, y, model, random_state):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state)\n    model.fit(X_train, y_train)\n    train_score = model.score(X_train, y_train) \n    test_score = model.score(X_test, y_test)\n    return {'model' : model,\n            'train_score' : train_score,\n            'test_score' : test_score}\n", "intent": "Complete this wrapper function\n"}
{"snippet": "ndata.fillna(0).tail(5)\n", "intent": "Set the **inplace = True** flag\n"}
{"snippet": "def bar_chart(feature):\n    survived = train[train['Survived'] == 1][feature].value_counts()\n    dead = train[train['Survived'] == 0][feature].value_counts()\n    df = pd.DataFrame([survived, dead])\n    df.index = ['Survived', 'Dead']\n    df.plot(kind='bar', stacked=True, figsize=(10, 5))\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "df_genes = pd.read_csv('data/nci60_data.csv', index_col=0)\n", "intent": "No longer interested in prediction - looking to discover underlying similarities in the data\n"}
{"snippet": "weekly = pd.read_csv('data/weekly.csv')\n", "intent": "a) Fit Logistic Regression with Lag1, Lag2\n"}
{"snippet": "train_X, test_X, train_y, test_y = train_test_split(X, y, \n                                                    train_size=0.5,\n                                                    test_size=0.5,\n                                                    random_state=123,\n                                                    stratify=y)\nprint('All:', np.bincount(y) / float(len(y)) * 100.0)\nprint('Training:', np.bincount(train_y) / float(len(train_y)) * 100.0)\nprint('Test:', np.bincount(test_y) / float(len(test_y)) * 100.0)\n", "intent": "So, in order to stratify the split, we can pass the label array as an additional option to the `train_test_split` function:\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(centers=2, random_state=0)\nprint('X ~ n_samples x n_features:', X.shape)\nprint('y ~ n_samples:', y.shape)\nprint('\\nFirst 5 samples:\\n', X[:5, :])\nprint('\\nFirst 5 labels:', y[:5])\n", "intent": "First, we will look at a two class classification problem in two dimensions. We use the synthetic data generated by the ``make_blobs`` function.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n", "intent": "Again, we start by splitting our dataset into a training (75%) and a test set (25%):\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42)\nX.shape\n", "intent": "Let's start by creating a simple, 2-dimensional, synthetic dataset:\n"}
{"snippet": "X_train,X_val,y_train,y_val = train_test_split(X.drop(['shop', 'y'],axis=1),y, random_state=42)\nX_train.shape,X_val.shape,y_train.shape,y_val.shape\n", "intent": "Usual approach for data split supposed to look like this. We won't forget to drop target variable\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nprint('CountVectorizer defaults')\nCountVectorizer()\n", "intent": "Now, we use the CountVectorizer to parse the text data into a bag-of-words model.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\npipeline = make_pipeline(TfidfVectorizer(), LogisticRegression())\nparams = {'logisticregression__C': [.1, 1, 10, 100],\n          \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (2, 2)]}\ngrid = GridSearchCV(pipeline, param_grid=params, cv=5)\ngrid.fit(text_train, y_train)\nprint(grid.best_params_)\ngrid.score(text_test, y_test)\n", "intent": "Another benefit of using pipelines is that we can now also search over parameters of the feature extraction with ``GridSearchCV``:\n"}
{"snippet": "from sklearn.manifold import TSNE\ntsne = TSNE(random_state=42)\ndigits_tsne = tsne.fit_transform(digits.data)\n", "intent": "Using a more powerful, nonlinear techinque can provide much better visualizations, though.\nHere, we are using the t-SNE manifold learning method:\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\n", "intent": "We will now apply the IsolationForest algorithm to spot digits written in an unconventional way.\n"}
{"snippet": "h_vec = HashingVectorizer(encoding='latin-1')\n", "intent": "Now, let's compare the computational efficiency of the `HashingVectorizer` to the `CountVectorizer`:\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nh_pipeline = Pipeline([\n    ('vec', HashingVectorizer(encoding='latin-1')),\n    ('clf', LogisticRegression(random_state=1)),\n])\nh_pipeline.fit(docs_train, y_train)\n", "intent": "Finally, let us train a LogisticRegression classifier on the IMDb training subset:\n"}
{"snippet": "vec = HashingVectorizer(encoding='latin-1')\nsgd.score(vec.transform(docs_test), y_test)\n", "intent": "Eventually, let us evaluate its performance:\n"}
{"snippet": "WXtable = pd.DataFrame(WX, columns=['lag_{}'.format(name) for name in Xnames])\n", "intent": "Then, we could fit a model using the neighbourhood average synthetic features as well:\n"}
{"snippet": "listings = pd.read_csv('./data/berlin-listings.csv.gz')\nlistings['geometry'] = listings[['longitude','latitude']].apply(shp.Point, axis=1)\nlistings = gpd.GeoDataFrame(listings)\nlistings.crs = {'init':'epsg:4269'}\nlistings = listings.to_crs(epsg=3857)\n", "intent": "First, we'll work with the point data to find regions where airbnbs colocate. \n"}
{"snippet": "df = pd.read_csv('ks-projects-201801.csv')\n", "intent": "So, now let's upload the data and look at it's contents. \n"}
{"snippet": "train_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train_df['comment'], train_df['label'], random_state=17)\n", "intent": "We split data into training and validation parts.\n"}
{"snippet": "X_train_texts = tf_idf_texts.fit_transform(train_texts)\nX_valid_texts = tf_idf_texts.transform(valid_texts)\n", "intent": "Do transformations separately for comments and subreddits. \n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nlr = LogisticRegression(C=0.001, random_state=5, class_weight='balanced')\nscal = StandardScaler()\nlr.fit(scal.fit_transform(X), y)\npd.DataFrame({'feat': independent_columns_names,\n              'coef': lr.coef_.flatten().tolist()}).sort_values(by='coef', ascending=False)\n", "intent": "**Answer:** 2.\n**Solution:**\n"}
{"snippet": "lr = LogisticRegression(C=0.001, random_state=5, class_weight='balanced')\nlr.fit(X, y)\npd.DataFrame({'feat': independent_columns_names,\n              'coef': lr.coef_.flatten().tolist()}).sort_values(by='coef', ascending=False)\n", "intent": "**Answer:** 2.\n**Solution:**\n"}
{"snippet": "pd.DataFrame({'feat': independent_columns_names,\n              'coef': rf_grid_search.best_estimator_.feature_importances_}).sort_values(by='coef', ascending=False)\n", "intent": "Rating of the feature importance:\n"}
{"snippet": "scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n", "intent": "Scale the sample using `StandardScaler` with default parameters.\n"}
{"snippet": "time_df = pd.DataFrame(index=train_df.index)\ntime_df['target'] = train_df['target']\ntime_df['min'] = train_df[times].min(axis=1)\ntime_df['max'] = train_df[times].max(axis=1)\ntime_df['seconds'] = (time_df['max'] - time_df['min']) / np.timedelta64(1, 's')\ntime_df.head()\n", "intent": "Now let us look at the timestamps and try to characterize sessions as timeframes:\n"}
{"snippet": "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_log1p_recommends.csv'), \n                           index_col='id')\ny_train = train_target['log_recommends'].values\n", "intent": "**Read train target and split data for validation.**\n"}
{"snippet": "def impute_nan_with_median(table):\n    for col in table.columns:\n        table[col]= table[col].fillna(table[col].median())\n    return table   \n", "intent": "Let us implement a function that will replace the NaN values by the median in each column of the table.\n"}
{"snippet": "scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n", "intent": "As we have very different scales of our variables, we need to rescale them.\n"}
{"snippet": "import pandas as pd\nurl = '../../data/titanic.csv'\ntitanic = pd.read_csv(url, index_col='PassengerId')\ntitanic.shape\n", "intent": "scikit-learn models expect that all values are **numeric** and **hold meaning**. Thus, missing values are not allowed by scikit-learn.\n"}
{"snippet": "import pandas as pd\nurl = '../../../../2_dataset/titanic.csv'\ntitanic = pd.read_csv(url, index_col='PassengerId')\ntitanic.shape\n", "intent": "scikit-learn models expect that all values are **numeric** and **hold meaning**. Thus, missing values are not allowed by scikit-learn.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n                                features,binarized_labels, test_size=0.33, random_state=42)\n", "intent": "- http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html\n"}
{"snippet": "vect = CountVectorizer(lowercase=True)\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape\nvect.get_feature_names()[1000:1010]\nX_train_dtm.shape\n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "import pandas as pd\nyelp = pd.read_csv('homework_data/yelp.csv')\nyelp.head(1)\n", "intent": "Read `yelp.csv` into a DataFrame.\n"}
{"snippet": "Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train[train['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train[train['Pclass']==3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))\n", "intent": "filling missing values\n"}
{"snippet": "of_df = pd.read_csv(\"../../data/old-faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "vect = TfidfVectorizer(ngram_range=(1,2), min_df=.01, stop_words='english')\nfit_vect = vect.fit_transform(X)\nkm = KMeans(n_clusters=20)\nkm.fit(fit_vect)\ndf['cluster_num'] = km.labels_\nreview_clusters(df)\n", "intent": "1. For TfidfVectorizer - modify ngram_range, min_df, and stop_words\n2. For KMeans - modify n_clusters\n"}
{"snippet": "X['CRFA_C'].fillna(chr(0), inplace=True)\nX['CRFA_C'] = X['CRFA_C'].apply(lambda x: ord(x))\n", "intent": "<b>Fixing 'CRFA_C'</b>\n"}
{"snippet": "currency_df = pd.read_csv('./data/usd_orig.csv')\ncurrency_df.drop([\"open\",\"max\",\"min\"], inplace=True, axis=1)\ncurrency_df['date'] = pd.to_datetime(currency_df['date'])\ncurrency_df['price'] = currency_df['price'].apply(lambda x: x.replace(',', '.'))\ncurrency_df['change%'] = currency_df['change%'].apply(lambda x: x.replace(',', '.').replace('%', ''))\ncurrency_df.to_csv('./data/usd.csv', index=False)\n", "intent": "The exchange rate needs a little preprocessing\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n", "intent": "- Logistic Regression\n"}
{"snippet": "from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nX_scaled = pd.DataFrame(data = X)\nX_scaled[X_numcols] = scaler.fit_transform(X[X_numcols])\nX_scaled.head()\n", "intent": "[3]. Applying RobustScaler to the data. Reference post:\nhttp://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=0)\n", "intent": "- Logistic Regression\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1) \n", "intent": "<b>Splitting X and y into training and testing sets</b>\n"}
{"snippet": "import pandas as pd\nloc=r\"C:\\Users\\anshul\\PycharmProjects\\scikit-learn\\scikit-learn-application\\pima-indians-diabetes.csv\"\ncol_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigre', 'age', 'label']\npima = pd.read_csv(loc, header=None, names=col_names)\n", "intent": " - Pima Indian Diabetes Dataset\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\nonehot_encoder = DictVectorizer()\nX = [\n    {'city': 'New York'},\n    {'city': 'San Francisco'},\n    {'city': 'Chapel Hill'}\n]\nprint(onehot_encoder.fit_transform(X).toarray())\n", "intent": "<b>One-hot-encoding</b>\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"C:\\\\Users\\\\anshul\\\\winequality-red.csv\", sep=';')\ndf.describe()\n", "intent": "The Wine Dataset - https://archive.ics.uci.edu/ml/datasets/wine\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"C:\\\\Users\\\\anshul\\\\PycharmProjects\\\\scikit-learn\\\\scikit-learn-application\\\\Mastering-Machine-Learning-with-scikit-learn-Second-Edition-master\\\\chapter06\\\\train.tsv\", header=0, delimiter='\\t')\nprint(df.count())\n", "intent": "- one-versus-all\n- one-versus-the-rest\n"}
{"snippet": "df = pd.read_csv(\"C:\\\\Users\\\\anshul\\\\PycharmProjects\\\\scikit-learn\\\\scikit-learn-application\\\\Mastering-Machine-Learning-with-scikit-learn-Second-Edition-master\\\\chapter07\\\\pima-indians-diabetes.data\", header=None)\ny = df[8]\nX = df[[0, 1, 2, 3, 4, 5, 6, 7]]\n", "intent": "Comparing the performance of Logistic Regression and naive Bayes classifiers on the Pima Indians Diabetes Database\n"}
{"snippet": "currency_df = pd.read_csv('./data/usd.csv', parse_dates=['date', ])\n", "intent": "Load currency exchange data\n"}
{"snippet": "boston = load_boston()\n", "intent": "First, let's have a look at the dataset from the sklearn library about house prices\n"}
{"snippet": "lb = LabelBinarizer()\nlb.fit(list('CHIMSVAGLPTRFYWDNEQK'))\nX_binarized = pd.DataFrame()\nfor col in X.columns:\n    binarized_cols = lb.transform(X[col])\n    for i, c in enumerate(lb.classes_):\n        X_binarized[str(col) + '_' + str(c)] = binarized_cols[:,i]\n", "intent": "Binarize each column into 1s and 0s representing whether an amino acid is present in that position. Run the cell below.\n"}
{"snippet": "scaled_features = scaler.fit_transform(data.drop('Class', axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features, columns=data.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df = pd.read_csv('College_Data', index_col=0)\ndf.head()\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = iris.drop('species', axis=1)   \ny = iris['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "** Split your data into a training set and a testing set.**\n"}
{"snippet": "labels = pd.DataFrame(list(zip(distances.index.values.tolist(), cluster.labels_)), columns = [\"id\", \"cluster\"])\nlabels.head(10)\n", "intent": "We will now take the results of the clustering and associate each of the data points into a cluster.\n"}
{"snippet": "from sklearn.datasets import *\ndata_boston = load_boston()\nprint data_boston.keys()\n", "intent": "We chose Boston Housing dataset, which contains information about the housing values in suburbs of Boston.\n"}
{"snippet": "pd.DataFrame(zip(boston.columns[:-1], lr.coef_), columns = [\"features\", \"coefficients\"])\n", "intent": "So, it means, that for each feature we have found one coefficient. Let's see\n"}
{"snippet": "y, X = full_df['change%'], full_df.drop('change%', axis=1)\nX['topic'] = X['topic'].fillna('Empty')\nX['tags'] = X['tags'].fillna('Empty')\n", "intent": "Fill N/A values and drop target variable\n"}
{"snippet": "df = pd.read_csv(\"Iris.csv\")\n", "intent": "```\ndf = pd.read_csv(\"Iris.csv\")```\n"}
{"snippet": "scaler = StandardScaler()\nscaler.fit(df_data)\n", "intent": "```\nscaler = StandardScaler()\nscaler.fit(df_data)```\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ndata = fetch_20newsgroups()\ndata.target_names\n", "intent": "```\nfrom sklearn.datasets import fetch_20newsgroups\ndata = fetch_20newsgroups()\ndata.target_names```\n"}
{"snippet": "train = pd.read_csv(\"../datasets/titanic/train.csv\")\ntest = pd.read_csv(\"../datasets/titanic/test.csv\")\n", "intent": "```\ntrain = pd.read_csv(\"../datasets/titanic/train.csv\")\ntest = pd.read_csv(\"../datasets/titanic/test.csv\")```\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"C:/Users/zliu/Documents/GitHub/udemy_dsmlwp/datascience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "airports = pd.read_csv('./airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"data/cars_sample.csv\")\n", "intent": "Run the Ordinary Least Square using the package sklearn\n"}
{"snippet": "pop = pd.read_csv('data/cars_small.csv')\n", "intent": "Run PCA with 2 dimensions on the cars dataset\n"}
{"snippet": "data = pd.read_csv('AmesHousing.txt', delimiter='\\t')\nnumeric_values = np.where(\n    (data.dtypes == np.dtype('int64'))\n    | (data.dtypes == np.dtype('float64'))\n)[0]\nX = data[numeric_values[2:-1]].values\ny = data['SalePrice'].values\nfeature_names = data.columns[numeric_values[2:-1]]\n", "intent": "We will use the Ames housing dataset from http://ww2.amstat.org/publications/jse/v19n3/decock.pdf.\n"}
{"snippet": "one_hot_topics = OneHotEncoder().fit_transform(X[['topic']])\none_hot_tags = OneHotEncoder().fit_transform(X[['tags']])\n", "intent": "Transform categorical features to dummy encoding\n"}
{"snippet": "data = DataFrame(np.column_stack((y,X)))\n", "intent": "Why is this what we want? Let's look at the pairwise correlations.\n"}
{"snippet": "arsenic_all = pd.read_stata('ARM_Data/arsenic/all.dta')\narsenic_wells = pd.read_csv('ARM_Data/arsenic/wells.dat', delimiter=' ')\n", "intent": "Exercise: (a) clean up the NaN, (b) extract only those that voted for republican or democrat. (c) solve a 2-class logistic regression.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\ntrainA = pd.read_csv(data_path, delimiter = ',')\ndata_path = os.path.join(os.getcwd(), 'datasets', 'train_20news_partB.csv')\ntrainB = pd.read_csv(data_path, delimiter = ',')\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "data_path_test = os.path.join(os.getcwd(), 'datasets', 'spambase_test.csv')\nspambase_test = pd.read_csv(data_path_test, delimiter = ',')\nspambase_test\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure\n"}
{"snippet": "forest_vect = CountVectorizer(\n    vocabulary=best_features, \n    max_features=2000\n)\nX_train_forest = forest_vect.fit_transform(X_train_text)\nX_test_forest = forest_vect.transform(X_test_text)\n", "intent": "_9.1 &emsp; Forest v2_\n"}
{"snippet": "X, y, coef = make_regression(n_samples=50,\n                       n_features=1,\n                       n_informative=1,\n                       noise=10.0,\n                       shuffle=False,\n                       coef=True)\n", "intent": "[sklearn.linear_model.LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n"}
{"snippet": "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\nX_poly2 = poly.fit_transform(X)\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_poly2, y, test_size=0.3)\nX_poly2[0:2,:]\n", "intent": "[sklearn.preprocessing.PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n"}
{"snippet": "X, y = make_circles(n_samples=100, noise=.3, factor=0.2)\n", "intent": "[sklearn.datasets.make_circles](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html)\n"}
{"snippet": "df_features = pd.DataFrame(scaled_features, columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "titles_normalized = pd.read_csv('./data/titles_normalized.csv', index_col='idx')\n", "intent": "Perform preprocessing of text features, eliminating various word forms\n"}
{"snippet": "X = CV.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,  random_state=101 )\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('forest-cover-type.csv')\ndf.head()\n", "intent": "First, let's load the dataset:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)\n", "intent": "Before applying PCA it is important to get all of the features onto the same scale by removing the mean and scaling to unit variance.\n"}
{"snippet": "data_stat_PR = pd.DataFrame(columns=['Price Range','number_of_samples'])\np=data.groupby(by='Price indi')\ni = 0\nfor grp, temp in p:\n    temp_data = [grp,len(temp)]\n    data_stat_PR.loc[i] = temp_data\n    i = i+1\n", "intent": "Mean rental fee near <b>EW14 Raffles Place MRT station</b> is the highest.\n"}
{"snippet": "val_X = poly.fit_transform(val_data[:,0:2])\nval_y = val_data[:,2][:,None]\n", "intent": "<hr>\nDo you think the validation error will be (almost) zero?\n"}
{"snippet": "df_mms = p.MinMaxScaler().fit_transform(df) \n", "intent": "`3.` Now it's your turn.  Try fitting the **MinMaxScaler** transformation to this dataset. You should be able to use the previous example to assist.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2)\ndrug_data_normalized_pca2d = pca.fit_transform(drug_data_normalized)\n", "intent": "Next we use PCA to bring the dimensionality of the data from 12 down to 2.\n"}
{"snippet": "print(sum(pca.inverse_transform([0, 1]))\n", "intent": "We can similarly figure out how the second principal component is obtained from the original 12 features:\n"}
{"snippet": "data_train = pd.read_csv('../data/mobile/train.csv')\ndata_test = pd.read_csv('../data/mobile/test.csv')\ndata_test.drop(columns='id', inplace=True)\n", "intent": "Let`s look at data:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_Valid, Y_train, Y_Valid = train_test_split(pad_seq, labels, test_size=0.33, shuffle=True)\n", "intent": "4\\. Split the above data (the sequence and the label) into training (67%) and validation (33%) sets. [3 pts]\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True)\n", "intent": "Hold out 25% of the data for testing.\n"}
{"snippet": "df = pd.read_csv(\"KNN_Project_Data\")\n", "intent": "** Leia o arquivo csv 'KNN_Project_Data' em um DataFrame **\n"}
{"snippet": "X = scale(digits.data)\ny = digits.target\nk = 10\nXTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state = 1, test_size=0.8)\nclf = KNeighborsClassifier(n_neighbors=k)\nclf.fit(XTrain, yTrain)\n", "intent": "Repeat task 1.1 using k-nearest neighbors (k-NN). In part 1, use k=10. In part 3, find the best value of k. \n"}
{"snippet": "newsgroups = fetch_20newsgroups(subset='all')\n", "intent": "**3.1.** Load the dataset.\n1. Print the exact number of news articles in the corpus.\n2. Print all 20 categories the news articles belong to.\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nx = np.array([2, 3, 4]).reshape(3,1)\npoly = PolynomialFeatures(3)\npoly.fit_transform(x)\n", "intent": "This polynomial projection is useful enough that it is built into Scikit-Learn, using the ``PolynomialFeatures`` transformer:\n"}
{"snippet": "label_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(train_targets)\ny_valid = label_encoder.transform(valid_targets)\ny_test = label_encoder.transform(test_targets)\ny_train_1_hot = to_categorical(y_train, N_CLASSES)\ny_valid_1_hot = to_categorical(y_valid, N_CLASSES)\nwith open('label_encoder.p', 'wb') as f:\n    pickle.dump(label_encoder, f)\n", "intent": ">Also convert targets into 1-hot format required by using categorical_crossentropy loss \n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "from the datasets load the iris data into a variable called iris\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=20)\n", "intent": "Let's train the classifier\n"}
{"snippet": "scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\ntsne2 = TSNE(random_state=17)\ntsne_representation2 = tsne2.fit_transform(X_scaled)\n", "intent": "Let's look at another representation of the `scaled data` colored by binary features:\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "Load the sklearn `iris` dataset.  This is one of the built-in datasets included in scikit-learn (and one we've seen before).\n"}
{"snippet": "X, y = digits.data, digits.target\npca = PCA()\nX_r = pca.fit(X).transform(X)\nratios = pca.explained_variance_ratio_\nprint ratios\n", "intent": "Was 2 a good choice - i.e. can we capture enough of the variance with just 2 components?  Let's see what the rest would have looked like:\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "Load the iris dataset:\n"}
{"snippet": "df = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf['species'] = iris.target\ndf.head()\n", "intent": "Create a new pandas dataframe:\n"}
{"snippet": "df = pd.read_csv(\"data/heart_disease.csv\")\ndf.head()\n", "intent": "Import the dataset into a pandas dataframe:\n"}
{"snippet": "df['thal'] = df['thal'].fillna(value=1.0)\ndf['thal'].values\n", "intent": "Ah - turns out we do have to deal with the NaN values\n"}
{"snippet": "vectorizer = CountVectorizer(stop_words=\"english\")\n", "intent": "That's right -- words like \"is\" and \"the\" are so common that they are not really helpful features. Let's remove them in the next step:\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ntrain_set = (\"The sky is blue.\", \"The sun is bright.\")\ntest_set = (\"The sun in the sky is bright.\", \"We can see the shining sun, the bright sun.\")\ncount_vectorizer = CountVectorizer(stop_words=\"english\")\nvocab_train = count_vectorizer.fit_transform(train_set)\nprint \"Vocabulary:\", count_vectorizer.vocabulary_\n", "intent": "The first step is to create our training and testing document set and computing the term frequency matrix:\n"}
{"snippet": "from sklearn import cross_validation\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(\n        wine.values, grape.values, test_size=0.4, random_state=0)\n", "intent": "Implementing cross-validation on our wine SVC is straightforward:\n"}
{"snippet": "X_train_part, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, stratify=y, random_state=17)\n", "intent": "Let's make a split into a train sample and hold-out sample:\n"}
{"snippet": "svd.load_data(filename='./data/movielens/ratings.dat', sep='::', format={'col':0, 'row':1, 'value':2, 'ids': int})\n", "intent": "Populate it with the data from the ratings dataset, using the built in load_data method\n"}
{"snippet": "data.fillna(grouped_by_age_pclass, inplace=True)\n", "intent": "Let's Keep It Simple\n"}
{"snippet": "titanic = pd.read_csv('../data/kaggle-titanic/train.csv')\n", "intent": "Today, we are going to Predict Whether a Passenger will Survive or not, given other attributes\n"}
{"snippet": "x_train_text, y_train = imdb.load_data(train=True)\nx_test_text, y_test = imdb.load_data(train=False)\n", "intent": "Load the training- and test-sets.\n"}
{"snippet": "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n                           padding=pad, truncating=pad)\ntokens_pad.shape\n", "intent": "To input texts with different lengths into the model, we also need to pad and truncate them.\n"}
{"snippet": "data_src = europarl.load_data(english=False,\n                              language_code=language_code)\n", "intent": "Load the texts for the source-language, here we use Danish.\n"}
{"snippet": "data_dest = europarl.load_data(english=True,\n                               language_code=language_code,\n                               start=mark_start,\n                               end=mark_end)\n", "intent": "Load the texts for the destination-language, here we use English.\n"}
{"snippet": "x_train_scaled = x_scaler.fit_transform(x_train)\n", "intent": "We then detect the range of values from the training-data and scale the training-data.\n"}
{"snippet": "from sklearn.preprocessing import LabelBinarizer\nencoder = LabelBinarizer()\nhousing_cat_1hot = encoder.fit_transform(housing_cat)\nhousing_cat_1hot\n", "intent": "- A shortcut (text categories => integer categories => one-hot vectors)\n"}
{"snippet": "scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_train_part_scaled, X_valid_scaled, y_train, y_valid = train_test_split(X_scaled, y,\\\n                                                        test_size=0.3, stratify=y, random_state=17)\n", "intent": "Some models should not be scaled, but for others it is necessary:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX2D = pca.fit_transform(X)\nprint(pca.components_[0])\nprint(pca.components_.T[:,0])\n", "intent": "* Uses SVD decomposition as before.\n* You can access each PC using *components_* variable. (\n"}
{"snippet": "pca = PCA()\npca.fit(X)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nd = np.argmax(cumsum >= 0.95) + 1\nprint(d)\n", "intent": "* No need to choose arbitrary \n"}
{"snippet": "from sklearn.decomposition import IncrementalPCA\nn_batches = 100\ninc_pca = IncrementalPCA(n_components=154)\nfor X_batch in np.array_split(X_mnist, n_batches):\n    print(\".\", end=\"\")\n    inc_pca.partial_fit(X_batch)\nX_mnist_reduced_inc = inc_pca.transform(X_mnist)\n", "intent": "* PCA normally requires entire dataset in memory for SVD algorithm.\n* **Incremental PCA (IPCA)** splits dataset into batches.\n"}
{"snippet": "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\nt1 = time.time()\nX_reduced = rnd_pca.fit_transform(X_mnist)\nt2 = time.time()\nprint(t2-t1, \"seconds\")\n", "intent": "* Stochastic algorithm, quickly finds approximation of 1st d components. Dramatically faster.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_housing_data = scaler.fit_transform(\n    housing.data)\nscaled_housing_data_plus_bias = np.c_[\n    np.ones((m, 1)), \n    scaled_housing_data]\nimport pandas as pd\npd.DataFrame(scaled_housing_data_plus_bias).info()\n", "intent": "* Could use TF; let's use Scikit first.\n"}
{"snippet": "le = LabelEncoder()\nle.fit(dicti)\n", "intent": "One Hot Encoding (OHE)\n"}
{"snippet": "df.fillna('-', inplace=True)\ntest.fillna('-', inplace=True)\n", "intent": "Wanted more clever way to dell and fill nan, but dropna or fillna, will work good.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nvec = TfidfVectorizer(max_features=4000, stop_words='english')\ntfidf = vec.fit_transform(bbc_news.text)\ntfidf.shape\n", "intent": "** TFIDF Count Vector **\n"}
{"snippet": "df = pd.read_csv(\"HousingAll.csv\")\ndf['Date'] = pd.to_datetime(df['Date'])\ndf = df.set_index('Date')\ndf.head()\n", "intent": "unsure about how it downsamples\n"}
{"snippet": "X2 = data_train2.drop(['price_range','inch'], axis=1)\nscaler2 = StandardScaler()\nX_scaled2, y2 = scaler2.fit_transform(X2), data_train2['price_range']\nX_train_part_scaled2, X_valid_scaled2, y_train2, y_valid2 = train_test_split\\\n                            (X_scaled2, y2, test_size=.3, stratify=y2, random_state=17)\n", "intent": "For `OneVsOneClassifier` with `LogisticRegression` unscaled matrix features:\n"}
{"snippet": "from google.cloud import bigquery\nbq = bigquery.Client(project=PROJECT)\nfor phase in [\"TRAIN\", \"VALID\", \"TEST\"]:\n    query_string = create_query(phase, \"5000\")\n    df = bq.query(query_string).to_dataframe()\n    df.to_csv(\"taxi-{}.csv\".format(phase.lower()), index_label = False, index = False)\n    print(\"Wrote {} lines to {}\".format(len(df), \"taxi-{}.csv\".format(phase.lower())))\n", "intent": "Now let's execute a query for train/valid/test and write the results to disk in csv format. We use Pandas's `.to_csv()` method to do so.\n"}
{"snippet": "df_train = pd.read_csv(filepath_or_buffer = \"./taxi-train.csv\")\ndf_valid = pd.read_csv(filepath_or_buffer = \"./taxi-valid.csv\")\ndf_test = pd.read_csv(filepath_or_buffer = \"./taxi-test.csv\")\nCSV_COLUMN_NAMES = list(df_train)\nprint(CSV_COLUMN_NAMES)\nFEATURE_NAMES = CSV_COLUMN_NAMES[1:] \nLABEL_NAME = CSV_COLUMN_NAMES[0] \n", "intent": "Because the files are small we can load them into in-memory Pandas dataframes.\n"}
{"snippet": "import shutil\nshutil.rmtree(path = \"data/sines\", ignore_errors = True)\nos.makedirs(\"data/sines/\")\nnp.random.seed(1) \nfor i in range(0,10):\n    to_csv(\"data/sines/train-{}.csv\".format(i), 1000)  \n    to_csv(\"data/sines/valid-{}.csv\".format(i), 250)\n", "intent": "<h3> Cloud ML Engine </h3>\nNow to train on Cloud ML Engine with more data.\n"}
{"snippet": "import shutil\nshutil.rmtree(path = \"data/sines\", ignore_errors = True)\nos.makedirs(\"data/sines/\")\nfor i in range(0,10):\n    to_csv(\"data/sines/train-{}.csv\".format(i), 1000)  \n    to_csv(\"data/sines/valid-{}.csv\".format(i), 250)\n", "intent": "<h3> Cloud ML Engine </h3>\nNow to train on Cloud ML Engine with more data.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"../data/yelp.csv\")\n", "intent": "Read **`yelp.csv`** into a Pandas DataFrame and examine it.\n"}
{"snippet": "combined = data[\"sat_results\"]\ncombined = combined.merge(data[\"ap_2010\"], on=\"DBN\", how=\"left\")\ncombined = combined.merge(data[\"graduation\"], on=\"DBN\", how=\"left\")\nto_merge = [\"class_size\", \"demographics\", \"survey\", \"hs_directory\"]\nfor m in to_merge:\n    combined = combined.merge(data[m], on=\"DBN\", how=\"inner\")\ncombined = combined.fillna(combined.mean())\ncombined = combined.fillna(0)\n", "intent": "Now the datasets can be combined in one by DBN column.\n"}
{"snippet": "pca = PCA(whiten=False, n_components=2)\n", "intent": "Above we got all components and then projected down. Here we have to specify the number of components up front.\n"}
{"snippet": "Xp_orig = scl.inverse_transform(Xp)\n", "intent": "If you want to go back to the original, non-standardized space, you need to `scl` object:\n"}
{"snippet": "pca2D = PCA(whiten=False, n_components=2)\nXtrans = pca2D.fit_transform(Xn)\nXp = pca2D.inverse_transform(Xtrans)\npca2D_std = PCA(whiten=False, n_components=2)\nXtrans_std = pca2D.fit_transform(Xs)\nXp_std = pca2D.inverse_transform(Xtrans_std)\n", "intent": "Again I'll do this for both standardised and non-standardised data.\n"}
{"snippet": "pca = PCA(n_components=0.9, random_state=17).fit(X2)\nX_pca = pca.transform(X2)\n", "intent": "Reduce the dimension while preserving the variance:\n"}
{"snippet": "pca = PCA(whiten=False, n_components=3)\npca.fit(X.T)\n", "intent": "In this case we are interested in the least important eigenvector as that should give the perpendicular to the plane.\n"}
{"snippet": "df = pd.DataFrame(X[0:1000, :], columns=['u-g', 'g-r', 'r-i', 'i-z', 'T'])\n", "intent": "Seaborn works a lot better with Pandas data frames so we create one here and then print the first three rows in the table.\n"}
{"snippet": "with open('data/track1/driving_log.csv', newline='') as f:\n    track_data = list(csv.reader(f, skipinitialspace=True, delimiter=',', quoting=csv.QUOTE_NONE))\ntrack_df = pd.read_csv('data/track1/driving_log.csv', header=0)\nprint('total records: ', len(track_data))\nprint('\\nfirst record:\\n', track_data[0])\nprint('\\nDataframe header:\\n')\ntrack_df.head()\n", "intent": "This is data recorded using the driving simulator. \n"}
{"snippet": "vectorizer = TfidfVectorizer()\nvectors = vectorizer.fit_transform(newsgroups.data)\n", "intent": "Now we compute the TF-IDF statistic for the newspaper dataset. To do so we use the `TfidfVectorizer()` function from the `sklearn` library.\n"}
{"snippet": "ref_df= pd.DataFrame(X, columns=['CreditScore', 'Geography',\n       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n       'IsActiveMember', 'EstimatedSalary'])\nref_df.head(2)\n", "intent": "- Convert countries to dummy variable\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_2 = LabelEncoder()\nX[:,2] = labelencoder_X_2.fit_transform(X[:,2])\n", "intent": "- Convert gender to dummy variables\n"}
{"snippet": "onehotencoder = OneHotEncoder(categorical_features=[1])\nX = onehotencoder.fit_transform(X).toarray()\n", "intent": "- Avoid multiple dummy variable by using onehotencoder \n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n", "intent": "- **Feature Scaling** is key due to its expensive calculations\n"}
{"snippet": "import statsmodels.api as sm\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nX = df[['Cylinder', 'Liter', 'Cruise']]\ny = df['Price']\nX[['Cylinder', 'Liter', 'Cruise']] = scale.fit_transform(X[['Cylinder', 'Liter', 'Cruise']].as_matrix())\nest = sm.OLS(y, X).fit()\nest.summary()\n", "intent": "Let's put these columns into multivariate regression.\n"}
{"snippet": "y=df['y'] \nscaler=MinMaxScaler()\ndf.iloc[:,5:10]=scaler.fit_transform(df.iloc[:,5:10]) \n", "intent": "We have gone from 20 features to 52 featues.\n"}
{"snippet": "banknotes_scaled = scaler.fit_transform(banknotes.drop('Class',axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "df_feat = pd.DataFrame(data=banknotes_scaled,columns=banknotes.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "num_col_fill_MostCom = [\"LotFrontage\", \"GarageYrBlt\"]\nnum_df[num_col_fill_MostCom] = num_df[num_col_fill_MostCom].apply(lambda x:x.fillna(x.value_counts().index[0]))\n", "intent": "Let's fill \"LotFrontage\", \"GarageYrBlt\" features with most common values.\n"}
{"snippet": "num_df = num_df.apply(lambda x:x.fillna(0))\n", "intent": "We fill the rest just with 0.\n"}
{"snippet": "fake = pd.DataFrame(ans_df, columns = ['target'])\nfake.to_csv('ridge2.csv', index = False)\n", "intent": "RMSE: 4.359570467231344\n"}
{"snippet": "def pca(n_components, X_train, X_test, whiten=True):\n    pca = PCA(n_components=n_components, whiten=whiten).fit(X_train)\n    return pca.transform(X_train), pca.transform(X_test)\n", "intent": "But here we see that Confusion matrix diagonal value is relevant only for 3rd class, so that we should balance our target.\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features, columns = df.columns[:-1]) \n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "loans =  pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "col = pd.read_csv(\"College_Data\", index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df = pd.read_csv('data/winemag-data-130k-v2.csv', index_col=0)\ndf.info(memory_usage='deep')\n", "intent": "Let's download the data from Kaggle, extract them into ```data``` folder and check the main properties of the resulting DataFrame:\n"}
{"snippet": "df =  pd.read_csv(\"Classified Data\", index_col=0)\n", "intent": "Set index_col=0 to use the first column as the index.\n"}
{"snippet": "import pandas as pd\nurl = './data/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer.head()\n", "intent": "<a id=\"k-means-demo\"></a>\n---\n"}
{"snippet": "df = pd.read_csv('./assets/data/haystack.csv')\n", "intent": "**Load the haystack data**\n"}
{"snippet": "pd.DataFrame(lm.coef_.T, X.columns)\n", "intent": "**View LASSO coefficients and identify which feature is irrelevant**\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nprint digits.keys()\n", "intent": "This does not load in as a dataframe like we are used to, but instead is a native format to sklearn called a \"Bunch\"\n"}
{"snippet": "pd.DataFrame(digits.data).head()\n", "intent": "We have 64 features of the data, one for each pixel in the 8x8 pixel image.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit(X_train).transform(X_train)\nX_test_scaled = scaler.fit(X_test).transform(X_test)\nmlp = MLPClassifier(activation='logistic',solver='lbfgs',random_state=42, hidden_layer_sizes=(3))  \nmlp.fit(X_train_scaled, y_train)\nprint('Accuracy on the training subset: {:.3f}'.format(mlp.score(X_train_scaled, y_train)))\nprint('Accuracy on the test subset: {:.3f}'.format(mlp.score(X_test_scaled, y_test)))\n", "intent": "Let's also standardize our features since Multilayer Perceptron is sensitive to feature scaling\n"}
{"snippet": "url = 'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data'\ndf = pd.read_csv(url, sep='\\t', header=0)\ndf = df.drop('Unnamed: 0', axis=1)   \n", "intent": "Our analysis begins by getting the data from Tibshirani's website.   \n"}
{"snippet": "df = pd.read_csv(\"College.csv\")\ndata = df.values\ndata1 = data[:,2:19]\nX = data1[:,0:16]\nY = data1[:,16]\nprint(X.shape)\n", "intent": "Import the data from the file College.csv\n"}
{"snippet": "ss = StandardScaler()\ndf_train.price = ss.fit_transform(df_train[['price']])\ndf_test.price = ss.transform(df_test[['price']])\n", "intent": "Our model is sensitive to non-centered numeric features, so we need to scale them:\n"}
{"snippet": "df = pd.read_csv(\"College.csv\")\ndata = df.values\ndata1 = data[:,2:19]\nX = data1[0:16]\nY = data1[17]\n", "intent": "Import the data from the file College.csv\n"}
{"snippet": "x_scale = preprocessing.scale(cancer.data)\ny = cancer.target\nx_train, x_test, y_train, y_test = train_test_split(x_scale, cancer.target, stratify= cancer.target, random_state=0)\n", "intent": "Before splitting the data into train and test- scale the data since we will be using gradient ascent\n"}
{"snippet": "X_scale = StandardScaler()\nX = X_scale.fit_transform(digits.data)\nX[0,:] \n", "intent": "The training features range from 0 to 15.  To help the algorithm converge, we will scale the data to have a mean of 0 and unit variance\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.25,\n                                                    random_state=1234,\n                                                    stratify=y)\n", "intent": "<img src=\"sklearn/figures/train_test_split_matrix.svg\" width=\"100%\">\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n", "intent": "Again, we start by splitting our dataset into a training (75%) and a test set (25%):\n"}
{"snippet": "for column_name in ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\\\n                   'PoolQC', 'Fence', 'MiscFeature', 'FireplaceQu']:\n    comb_data[column_name].fillna('Not', inplace=True)\n", "intent": "*Features that can be dropped - Alley', 'Fence', 'MiscFeature', 'PoolQC' [>80% missing data]*\n"}
{"snippet": "for column_name in ['GarageYrBlt']:\n    comb_data[column_name].fillna(1900, inplace=True)\n", "intent": "It shows that null values in 'GarageYrBlt' mean houses without garage. I am filling with 1900 to imply less value than others with garage.\n"}
{"snippet": "stumble_upon['title'] = stumble_upon.boilerplate.map(lambda x: json.loads(x).get('title', ''))\nstumble_upon['body'] = stumble_upon.boilerplate.map(lambda x: json.loads(x).get('body', ''))\ntitles = stumble_upon['title'].fillna('')\nbody = stumble_upon['title'].fillna('')\ntitles[0:5]\n", "intent": "This is a bit of the NLP we covered in the pipeline lecture!\n---\n"}
{"snippet": "def movie_parser(m):\n    return [m['num_votes'], m['rating'], m['tconst'], m['title'], m['year']]\nparsed = np.array([movie_parser(m) for m in top250])\nmovies = pd.DataFrame(parsed, columns = ['num_votes', 'rating', 'tconst', 'title', 'year'])\n", "intent": "Keep the fields:\n    num_votes\n    rating\n    tconst\n    title\n    year\nAnd discard the rest\n"}
{"snippet": "X_train_logreg_,X_test_logreg_,y_train_logreg,y_test_logreg=train_test_split(feats_logreg,\n                                        target,test_size=0.3,random_state=17,stratify=target)\n", "intent": "Now we'll split our data.  *stratify* used due to imbalance in classes.\n"}
{"snippet": "X_train_quadratic = quadratic_featurizer.fit_transform(X_train)\nX_test_quadratic = quadratic_featurizer.transform(X_test) \nprint X_train\nprint X_train_quadratic\n", "intent": "Now we will transform the training and test features.\n"}
{"snippet": "X_train_quadratic = quadratic_featurizer.fit_transform(X_train)\nX_test_quadratic = quadratic_featurizer.transform(X_test) \n", "intent": "Now we will transform the training and test features.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nbinary_vectorizer = CountVectorizer(binary=True)\nX = binary_vectorizer.fit_transform(corpus)\nprint binary_vectorizer.vocabulary_\nX.todense()\n", "intent": "Now let's use scikit-learn to create our feature representations. `CountVectorizer` can be used to convert a corpus to a matrix of token counts.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "Now let's create a KNN classifier with sklearn.\n"}
{"snippet": "data = pd.read_csv(\"sample_train.csv\")\n", "intent": "Load all the data set\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n", "intent": "Split X and y into training and test data sets\n"}
{"snippet": "X_train_df = pd.DataFrame(X_train, columns=X.columns)\ny_train_df = pd.DataFrame(y_train, columns=['trade_price'])\nX_train_df.describe()\n", "intent": "Get statistics on explanatory variables for train data set\n"}
{"snippet": "embed_df = pd.read_table(\"embeddings-scaled.EMBEDDING_SIZE=50.txt\", header=None, sep=' ', names=['word'] + range(1,51))\nembeddings = {row[0]: list(row[1:]) for row in embed_df.as_matrix()}\n", "intent": "(http://metaoptimize.com/projects/wordreprs/)\n"}
{"snippet": "binary_vectorizer = CountVectorizer(binary=True) \nX_train_v = binary_vectorizer.fit_transform(X_train)\nprint X_train_v.shape\nprint len(binary_vectorizer.vocabulary_)\n", "intent": "Fit and transform the training data\n"}
{"snippet": "X_train_tb,X_test_tb,y_train_tb,y_test_tb=train_test_split(feats_tb,\n                                        target,test_size=0.3,random_state=17,stratify=target)\n", "intent": "Let's split our data. *stratify* used due to imbalance in classes.\n"}
{"snippet": "content_image = scipy.misc.imread(\"images/my_content.jpg\")\ncontent_image = reshape_and_normalize_image(content_image)\n", "intent": "Let's load, reshape, and normalize our \"content\" image (the Louvre museum picture):\n"}
{"snippet": "from __future__ import print_function, division\nimport pandas as pd\ndf = pd.read_csv(\"data/Daily_Demand_Forecasting_Orders.csv\", delimiter=';')\ndf\n", "intent": "https://archive.ics.uci.edu/ml/datasets.html\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndarbe = pd.read_csv('darbe.csv')\ndarbe.info()\n", "intent": " - [darbe](https://trends.google.com.tr/trends/explore?date=today%205-y&geo=TR&q=darbe)\n"}
{"snippet": "sc = StandardScaler()\n", "intent": "**Create a StandardScaler() object called scaler.**\n"}
{"snippet": "sdf = pd.DataFrame(ScledData,columns=imagedf.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "cdf = pd.read_csv('College_Data',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "kdf = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scalar = StandardScaler()\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.3,random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train datasets shape:\", train_df.shape)\nprint(\"Test datasets shape:\", test_df.shape)\ntrain_df.head()\n", "intent": "**2. Primary data analysis**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(yelp_class['text'],yelp_class['stars'], test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "Time to standardize the variables.\n** Import StandardScaler from Scikit learn.**\n"}
{"snippet": "dfnew =pd.DataFrame(df_new,columns=df.columns[:-1])\ndfnew.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X=dfnew\ny=df['TARGET CLASS']\nX_Train,X_Test,y_Train,y_Test = train_test_split(X,y,test_size=0.3,random_state=101)\n", "intent": "**Use train_test_split to split your data into a training set and a testing set.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_Train,X_Test,y_Train,y_Test = train_test_split(X,y,test_size=0.3)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "ad_data =pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "centers = [[1, 1], [-2, -1], [1, -1]]\nX, _ = make_blobs(n_samples=1000, centers=centers, cluster_std=0.6)\n", "intent": "Generate some data.  We use sklearn.datasets.samples_generator to get some data to play with\n"}
{"snippet": "df_features = pd.DataFrame(scaled_features, columns=df.columns[:-1])\ndf_features.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "applications['location_country'] = applications['location_country'].fillna('n\\a')\napplications['location_city'] = applications['location_city'].fillna('n\\a')\n", "intent": "Check **location_country** and **location_city**:\n"}
{"snippet": "full_data['Functional'] = full_data['Functional'].fillna('Typ')\n", "intent": "* **Functional**: Home functionality rating. `NaN` means typical, so replace the missing values with `Typ`.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nfeatures = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'ExterQual', 'ExterCond', 'HeatingQC',\n            'PoolQC', 'KitchenQual', 'BsmtFinType1', 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish',\n            'LandSlope', 'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', \n            'OverallCond', 'YrSold', 'MoSold')\nfor feature in features:\n    le = LabelEncoder()\n    le.fit(list(full_data[feature].values))\n    full_data[feature] = le.transform(list(full_data[feature].values))\nfull_data.head(10)\n", "intent": "**Use `LabelEncoder` to split values of each feature into different categories.**\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndf = pd.read_csv('winequality-red.csv', sep=';')\n", "intent": "We take examples from the book \"Mastering Machine Learning with scikit-learn\" written by Gavin Hackeling.\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "Try https://medium.com/@haydar_ai/learning-data-science-day-9-linear-regression-on-boston-housing-dataset-cd62a80775ef\n"}
{"snippet": "train = pd.read_csv(\"train.csv\")\nprint(train.shape)\ntrain.head()\n", "intent": "<a id=\"1\"></a>\n* In this part we load and visualize the data.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=2)\nprint(\"x_train shape\",X_train.shape)\nprint(\"x_test shape\",X_val.shape)\nprint(\"y_train shape\",Y_train.shape)\nprint(\"y_test shape\",Y_val.shape)\n", "intent": "<a id=\"3\"></a>\n* We split the data into train and test sets.\n* test size is 10%.\n* train size is 90%.\n"}
{"snippet": "train = pd.read_csv(\"../input/train.csv\")\nprint(train.shape)\ntrain.head()\n", "intent": "<a id=\"1\"></a>\n* In this part we load and visualize the data.\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nageAndFare = scaler.fit_transform(ageAndFare)\nageAndFare = pd.DataFrame(ageAndFare, columns = [\"age\", \"fare\"])\nageAndFare.plot.scatter(x = \"age\", y = \"fare\")\n", "intent": "Scaling is needed\n> We also see that both variables have different scales.\n"}
{"snippet": "loans = pd.read_csv('../../../../../Documents/data/lending-club/accepted_2007_to_2018Q2.csv.gz', compression='gzip', low_memory=True)\n", "intent": "Read the data into a pandas dataframe:\n"}
{"snippet": "applications['doc_date_i'] = ((pd.to_datetime(applications['app_dt'], format = '%Y-%m-%d')-pd.to_datetime(applications['doc_date'], format = '%Y-%m-%d')).dt.days/365.25).fillna(0)\napplications['client_date_i'] = (pd.to_datetime(applications['app_dt'], format = '%Y-%m-%d')-pd.to_datetime(applications['client_date'], format = '%Y-%m-%d')).dt.days/365.25\n", "intent": "Add new features: time from client registration and from issued of document (for both than more then better)\n"}
{"snippet": "from sklearn.model_selection import train_test_split\n_, itest = train_test_split(range(critics.shape[0]), train_size=0.7, test_size=0.3)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itest] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "data = []\nwith open('/Users/yuka/Desktop/DataScience/Yelp Project/yelp_training_set/yelp_training_set_business.json') as f:\n    for line in f:\n        data.append(json.loads(line))\nyelp_training_set_business=pd.DataFrame(data)\nyelp_training_set_business.head()\n", "intent": "Data loading: Load the business data\n"}
{"snippet": "data = []\nwith open('/Users/yuka/Desktop/DataScience/Yelp Project/yelp_training_set/yelp_training_set_checkin.json') as f:\n    for line in f:\n        data.append(json.loads(line))\nyelp_training_set_checkin=pd.DataFrame(data)\nyelp_training_set_checkin.head()\n", "intent": "Data Loading: Load the Checkin Data\n"}
{"snippet": "data = []\nwith open('/Users/yuka/Desktop/DataScience/Yelp Project/yelp_training_set/yelp_training_set_review.json') as f:\n    for line in f:\n        data.append(json.loads(line))\nyelp_training_set_review=pd.DataFrame(data)\nyelp_training_set_review.head()\n", "intent": "Data loading: Load the review data\n"}
{"snippet": "data = []\nwith open('/Users/yuka/Desktop/DataScience/Yelp Project/yelp_training_set/yelp_training_set_user.json') as f:\n    for line in f:\n        data.append(json.loads(line))\nyelp_training_set_user=pd.DataFrame(data)\nyelp_training_set_user.head()\n", "intent": "Data Loading: Load the user data\n"}
{"snippet": "yelp_final_training_data_review_votes=pd.DataFrame()\nfor i in range(200473):\n    test = pd.DataFrame(data=yelp_final_training_data_set[\"review votes\"][i], index=[i])\n    yelp_final_training_data_review_votes=yelp_final_training_data_review_votes.append(test)\n", "intent": "Feature transform: As the reviews votes column in the yelp_final_training_data_set is a dictonary, we need to convert it to the data frame structure:\n"}
{"snippet": "yelp_review_votes.to_csv(\"/Users/yuka/Desktop/DataScience/Yelp Project/yelp_review_votes_master_data.csv\")\nyelp_data_final.to_csv(\"/Users/yuka/Desktop/DataScience/Yelp Project/yelp_data_final_master_data.csv\")\n", "intent": "Feature transform: Export the data set into the project folder so that we only need to refer to the exported csv files for further analysis.\n"}
{"snippet": "yelp_training_set_user_votes=pd.DataFrame()\nfor i in range(len(yelp_training_set_user)):\n    test = pd.DataFrame(data=yelp_training_set_user[\"votes\"][i], index=[i])\n    yelp_training_set_user_votes=yelp_training_set_user_votes.append(test)\n", "intent": "Convert the data dictionary \"user review votes\" to the user review votes data frame\n"}
{"snippet": "yelp_training_set_checkin_info=pd.DataFrame()\nfor i in range(len(yelp_training_set_checkin)):\n    test = pd.DataFrame(data=yelp_training_set_checkin[\"checkin_info\"][i], index=[i])\n    yelp_training_set_checkin_info=yelp_training_set_checkin_info.append(test)\n", "intent": "Convert the data dictionary \"checkin info\" to checkin info data frame:\n"}
{"snippet": "df_all = pd.read_csv(\"input/weatherAUS.csv\")\n", "intent": "Reading data into memory and creating a Pandas DataFrame object.\n"}
{"snippet": "X_test_train.to_csv(\"/Users/yuka/Desktop/DataScience/Yelp Project/X_test_train.csv\")\n", "intent": "Export the cleaned X_test_train data into the csv file:\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(ngram_range =(1,3), stop_words='english')\nX_train_countVectorizer = cv.fit_transform(X_text_train)\nX_train_countVectorizer.shape \n", "intent": "Use the count vectorizer transformer to convert the review text to the vectors\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_transformer = TfidfVectorizer(ngram_range =(1,3), stop_words='english', lowercase=True, min_df=2 )\nX_text_train_tfidf = tfidf_transformer.fit_transform(X_text_train)\nX_text_train_tfidf.shape \n", "intent": "Use the tfidf transformer to convert the review text to the vectors\n"}
{"snippet": "yelp_data_final_update.to_csv(\"/Users/yuka/Desktop/DataScience/Yelp Project/yelp_data_final_master_data_formodeling.csv\")\n", "intent": "Explore the final data set for modeling into the csv so that we can use it later:\n"}
{"snippet": "yelp_data_final_update=pd.read_csv(\"/Users/yuka/Desktop/DataScience/Yelp Project/yelp_data_final_master_data_formodeling.csv\")\n", "intent": "Import the analytical dataset:\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(ngram_range =(1,3), stop_words='english', min_df=50)\nX_train_countVectorizer = cv.fit_transform(X_text_train)\nX_train_countVectorizer.shape \n", "intent": "Use the count vectorizer transformer to convert the review text to the vectors\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_transformer = TfidfVectorizer(ngram_range =(1,3), stop_words='english', lowercase=True, min_df=50)\nX_text_train_tfidf = tfidf_transformer.fit_transform(X_text_train)\nX_text_train_tfidf.shape \n", "intent": "Use the tfidf transformer to convert the review text to the vectors\n"}
{"snippet": "titanic_train['Age'] = titanic_train['Age'].fillna(titanic_train['Age'].mean())\n", "intent": "Above, when we said:\n"}
{"snippet": "trainCleaned.to_csv('DATA/house-prices/train_cleaned.csv',index=False)\ntestCleaned.to_csv('DATA/house-prices/test_cleaned.csv',index=False)\n", "intent": "To save the dataframe as comma-separated values (CSV) file, run this line of code.\n"}
{"snippet": "from sklearn import preprocessing\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(df)\ndf = pd.DataFrame(scaler.transform(df), index=df.index, columns=df.columns)\ndf.iloc[4:10]\n", "intent": "Next step is to standardize our data - using MinMaxScaler\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\n", "intent": "Let us start loading the data set.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=33)\n", "intent": "Separate train and test\n"}
{"snippet": "from sklearn.feature_selection import *\nfs=SelectKBest(score_func=f_regression,k=5)\nX_new=fs.fit_transform(X_train,y_train)\nprint X_train.shape\nprint X_new.shape\nprint zip(fs.get_support(),data.feature_names) \n", "intent": "We can select the most important features with sklearn\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscalerX = StandardScaler().fit(X_train)\nscalery = StandardScaler().fit(y_train)\nprint np.max(y_train), np.min(y_train), np.mean(y_train), \nX_train = scalerX.transform(X_train)\ny_train = scalery.transform(y_train)\nX_test = scalerX.transform(X_test)\ny_test = scalery.transform(y_test)\nprint np.max(y_train), np.min(y_train), np.mean(y_train)\n", "intent": "Data normalization:\n"}
{"snippet": "from sklearn.feature_selection import *\nfs=SelectKBest(score_func=f_regression,k=5)\nX_new=fs.fit_transform(X_train,y_train)\nprint X_train.shape\nprint X_new.shape\nprint zip(fs.get_support(),data.feature_names)\n", "intent": "We can select the most important features with sklearn\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscalerX = StandardScaler().fit(X_train)\nscalery = StandardScaler().fit(y_train)\nX_train = scalerX.transform(X_train)\ny_train = scalery.transform(y_train)\nX_test = scalerX.transform(X_test)\ny_test = scalery.transform(y_test)\nprint np.max(X_train), np.min(X_train), np.mean(X_train), np.max(y_train), np.min(y_train), np.mean(y_train)\n", "intent": "Data normalization:\n"}
{"snippet": "tf_idf = text.TfidfVectorizer()\nX = tf_idf.fit_transform(X_raw)\n", "intent": "[TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF)\n"}
{"snippet": "txt = [\"Hello there\", \"Anybody there\"]\ntemp = text.TfidfVectorizer()\nX_temp = temp.fit_transform(txt)\ntemp.get_feature_names()\n", "intent": "Just getting to know here\n"}
{"snippet": "ss = RobustScaler()\ndata_ss = pd.DataFrame(ss.fit_transform(data[cols]), index=data.index, columns=cols)\n", "intent": "After some testing, I found that the RobustScaler standardization technique performs better for this dataset due to its robust handling of outliers.\n"}
{"snippet": "data = pd.read_csv('carInsurance_train.csv',index_col='Id')\n", "intent": "Let's look at our dataset. You can download it here: https://www.kaggle.com/kondla/carinsurance\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA()\npca.fit(x_cols)\nprint(pca.explained_variance_ratio_.cumsum())\n", "intent": "---\nFirst let's look at all the PCA components to see how much variance is explained\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimp = Imputer()\ndata[['avg_rating_by_driver', 'avg_rating_of_driver']] = \\\n    imp.fit_transform(data[['avg_rating_by_driver', \n                            'avg_rating_of_driver']])\nprint('Missing data by column: \\n{}'.format(data.isnull().sum()))\n", "intent": "Next I'll fill in the numerical values of *avg_rating_by_driver* and *avg_rating_of_driver* by simply using the mean.\n"}
{"snippet": "lr = LinearRegression()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nlr.fit(X_train, y_train)\nlr.score(X_test, y_test)\n", "intent": "**Generalization is Key**\nSklearn provides some great tools to help with this.\n`train_test_split()`\n`cross_val_score()`\n"}
{"snippet": "from sklearn import model_selection,datasets\nfrom sklearn import linear_model\nboston = datasets.load_boston()\nprint(boston.DESCR)\nboston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\nboston_df\n", "intent": "Models are cheap. Lets make lots of them!\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(tfidf_vecs, \n                                                    ng.target, \n                                                    test_size=0.33)\nnb = MultinomialNB()\nnb.fit(X_train, y_train)\nnb.score(X_test, y_test)\n", "intent": "Let's try simple Naive Bayes classification on TFIDF vectors from above in `sklearn`:\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nimport sklearn.metrics.pairwise as smp\nX_train, X_test, y_train, y_test = train_test_split(ng_lsi, ng.target, \n                                                    test_size=0.33)\nknn = KNeighborsClassifier(n_neighbors=3, metric=smp.cosine_distances)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)\n", "intent": "- Try some simple classification on the result LSI vectors for the 20 NG set:\n"}
{"snippet": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ntext = ['That is should come to this!', 'This above all: to thine own self be true.', 'Something is rotten in the state of Denmark.']\nvectorizer = CountVectorizer(ngram_range=(1,2))\nvectorizer.fit(text)\nx = vectorizer.transform(text)\nx_back = x.toarray()\npd.DataFrame(x_back, columns=vectorizer.get_feature_names())\n", "intent": "We have been working with a number of techniques and tools that help us navigate the world of NLP.       \n**For example, we have our Vectorizer:**\n"}
{"snippet": "categories = ['comp.graphics', 'rec.sport.baseball', 'rec.motorcycles', 'sci.space', 'alt.atheism']\nng_train = datasets.fetch_20newsgroups(subset='train', categories=categories, \n                                      remove=('headers', 'footers', 'quotes'))\n", "intent": "Let's retain only a subset of the 20 categories in the original 20 Newsgroups Dataset.\n"}
{"snippet": "ratings = pd.read_table('~/data/ml-20m/ratings.csv', sep=',')\n", "intent": "Load the ratings.dat data into a `ratings` variable with the same separator, and the column names UserID, MovieID, Rating, Timestamp.\n"}
{"snippet": "le = LabelEncoder()\nX_df.name = le.fit_transform(X_df['name'])\nX_df.animal_type = le.fit_transform(X_df['animal_type'])\nX_df.color = le.fit_transform(X_df['color'])\nX_df.breed = le.fit_transform(X_df['breed'])\n", "intent": "Let's try to apply LaberEncoder to features.\nFor faster calculations, reduce the sample size (no need to do that if you have enough power)\n"}
{"snippet": "pd.DataFrame(VT.T)\n", "intent": "If I transpose this, the rows are items, and the columns are the items in the \"hidden\" vector space created by the truncated SVD.\n"}
{"snippet": "pd.DataFrame(U)\n", "intent": "**U** is a matrix where each row is a user and each column shows the location in the hidden vector space created by the SVD.\n"}
{"snippet": "pd.DataFrame(Sigma)\n", "intent": "**Sigma** is just the singular values of the decomposition. In this case, we're not particularly interested in **Sigma**.\n"}
{"snippet": "max_features = 2000\n(X_train, y_train), (X_test, y_test) = reuters.load_data(\n    num_words=max_features)\nmaxlen = 10\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nnb_epoch = 20\n", "intent": "- Let's try an RNN for the same Reuters classification task:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=4444)\n", "intent": "Split the data into a test and training set. \n"}
{"snippet": "for column in house_votes_df.iloc[:,1:]:\n    col_mode = house_votes_df[column].mode()[0]\n    house_votes_df[column] = house_votes_df[column].fillna(col_mode)\n", "intent": "Convert each ? to the mode of the column (if a senator has not voted, make their vote 1 if most others voted 1, make it 0 if most others voted 0).\n"}
{"snippet": "df=pd.read_csv('house-votes-84.csv')\n", "intent": "<h1>Challenge 1</h1>\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:], df['republican'], test_size=.3, random_state=4444)\n", "intent": "<h1>Challenge 2</h1>\n"}
{"snippet": "df=pd.read_csv('house-votes-84.csv')\ndf.replace('y',1, inplace=True)\ndf.replace('n',0, inplace=True)\ncols=list(df.columns)\ncols.pop(0)\nfor i in cols:\n    df[i].replace('?',int(df[i].mode()), inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:-1], df['y.8'], test_size=.3, random_state=4444)\n", "intent": "<h1>Challenge 10</h1>\n"}
{"snippet": "X_df = df.drop('outcome_type', axis=1)[:10000]\nX_df = pd.get_dummies(X_df, columns=['animal_type','color','breed'])\nX_df.name = le.fit_transform(X_df['name'])\nX_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.3)\n", "intent": "Let's try to encode the data using pd.get_dummies, let's see how the quality of the models will change\n"}
{"snippet": "df= pd.read_csv('haberman.csv')\nprint(\"the average age of patients: {}\".format(df['30'].mean()))\nprint(\"the stdev age of patients: {}\".format(df['30'].std()))\nprint(\"the average age of patients: {}\".format(df.loc[df['1.1']==1]['30'].mean()))\nprint(\"the average age of patients: {}\".format(df.loc[df['1.1']==1]['30'].std()))\nprint(\"the average age of patients: {}\".format(df.loc[df['1.1']==2]['30'].mean()))\nprint(\"the average age of patients: {}\".format(df.loc[df['1.1']==2]['30'].std()))\n", "intent": "<h1>Challenge 12</h1>\n"}
{"snippet": "X = df2.loc[:, 'handicapped-infants':'export-administration-act-south-africa']\ny = df2['Class Name']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n", "intent": "Split the data into a test and training set. Use this function:\n"}
{"snippet": "movies_df = pd.read_csv('2013_movies.csv', header=0)\n", "intent": "For your movie classifiers, calculate the precision and recall for each class.\n"}
{"snippet": "haberman_df = pd.read_csv(\"haberman.data\", header=None)\nhaberman_df.rename(columns={0:'age_operation',1:'year_operation',2:'num_nodes',3:'survival'}, inplace=True)\n", "intent": "Draw the ROC curve (and calculate AUC) for the logistic regression classifier from challenge 12.\n"}
{"snippet": "movies = pd.read_csv('/home/kalgi/ds/metis/sf17_ds8/challenges/challenges_data/2013_movies.csv')\n", "intent": "For your movie classifiers, calculate the precision and recall for each class.\n"}
{"snippet": "column_names = ['Age of patient at time of operation', 'Patients year of operation','Number of positive axillary nodes detected',\n'Survival status']\nhaberman = pd.read_csv('/home/kalgi/Downloads/data.txt', names=column_names, header=None)\n", "intent": "Draw the ROC curve (and calculate AUC) for the logistic regression classifier from challenge 12.\n"}
{"snippet": "df=pd.read_csv('Dataframe519.csv', sep='|')\n", "intent": "<h1>Data Cleaning</h1>\n"}
{"snippet": "dfExpon=dfFinal.copy()\ndfExpon['EYear']=dfExpon['Year']**2\nX_trainB, X_testB, y_trainB, y_testB = train_test_split(dfExpon.dropna().iloc[:,1:], dfExpon.dropna().loc[:,'Price'], test_size=0.1,random_state=42)\n", "intent": "<h1>Create exponential features and complete linear regression analysis</h1>\n"}
{"snippet": "X_trainC, X_testC, y_trainC, y_testC = train_test_split(dfCoeff.dropna().iloc[:,1:], dfCoeff.dropna().loc[:,'Price'], test_size=0.1,random_state=42)\n", "intent": "<h1>RandomForest with coeff replaced dummies</h1>\n"}
{"snippet": "full_df.pivot_table(['ConvertedSalary'], ['CareerSatisfaction'], aggfunc='mean')\n", "intent": "Let's see average salaries by career satisfactions ranks\n"}
{"snippet": "files=os.listdir(\"/home/sufyan/Metis/Week4/Project3/newdata/\")\nall_dfs=[]\nfor file in files:\n    all_dfs.append(pd.read_csv(\"/home/sufyan/Metis/Week4/Project3/newdata/\"+file))\nfor i in range(len(all_dfs)):\n    all_dfs[i].set_index('Country or Area', inplace=True)\nCountryCodes=pd.read_csv(\"country names and ID.csv\")\n", "intent": "<h1>Build Working Panel</h1>\n"}
{"snippet": "mean_theatre = df3['Theaters'].mean()\nmean_runtime = df3['Runtime'].mean()\ndf3['Theaters'] = df3['Theaters'].fillna(value=mean_theatre)\ndf3['Runtime'] = df3['Runtime'].fillna(value=mean_runtime)\n", "intent": "**Multiple Regression 2**\n"}
{"snippet": "from keras.datasets import mnist\nimport numpy as np\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n", "intent": "Loading the MNIST dataset\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nx_train=x_train[0:20000]\n", "intent": "We will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784.\n"}
{"snippet": "prices_df=pd.DataFrame()\nprices_df[\"Price_Predictions\"]=m.fittedvalues.values\nprices_df[\"Real_Prices\"]=bos.PRICE.values\nsns.regplot(y=\"Price_Predictions\", x=\"Real_Prices\", data=prices_df, fit_reg = True)\n", "intent": "**Your turn:** Create a scatterpot between the predicted prices, available in `m.fittedvalues` and the original prices. How does the plot look?\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndv = DictVectorizer()\ntransformed_data = dv.fit_transform(census_data).toarray()\ntransformed_data\n", "intent": "Now, let's encode those features and instances.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntransformed_labels = le.fit_transform(new_df_labels)\ntransformed_labels\n", "intent": "Now that we've done that, let's encode the labels.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ncensus_train, census_test, labels_train, labels_test = train_test_split(transformed_data, transformed_labels)\n", "intent": "Now that we've done that, can you separate the `transformed_data` and `transformed_labels` into training and test sets?\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits_data = digits.data\ndigits_labels = digits.target\nd_train, d_test, dl_train, dl_test = train_test_split(digits_data, digits_labels)\n", "intent": "This model is a much better fit for our dataset, and is much more accurate than k-nearest neighbors.\n"}
{"snippet": "os_df = full_df[['CareerSatisfaction', 'OperatingSystem']]\nos_df['OperatingSystem'] = os_df['OperatingSystem'].map(\n    lambda os: category_encoders['OperatingSystem'].inverse_transform(os)\n)\nos_dummies_df = pd.get_dummies(os_df, columns=['OperatingSystem'], prefix=\"OS\")\n", "intent": "And also <b>OperatingSystem</b>.\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nboston_data = boston.data\nboston_target = boston.target\nb_train, b_test, bl_train, bl_test = train_test_split(boston_data, boston_target)\n", "intent": "So our score is rather low.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits_data = digits.data\ndigits_labels = digits.target\n", "intent": "This model is a much better fit for our dataset, and is much more accurate than k-nearest neighbors.\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nboston_data = boston.data\nboston_target = boston.target\n", "intent": "So our score is rather low.\n"}
{"snippet": "from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\ncal_house = fetch_california_housing()    \nX = pd.DataFrame(data=cal_house['data'], \n                 columns=cal_house['feature_names'])\\\n             .iloc[:,:-2]\ny = cal_house['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=1)\nprint(X_train.head(3))\n", "intent": "> **Ex. 12.2.0:** Load the california housing data with scikit-learn using the code below. Inspect the data set. \n"}
{"snippet": "degree_index = pd.Index(degrees,name='Polynomial degree ~ model complexity')\nax = pd.DataFrame({'Train set':train_mse, 'Test set':test_mse})\\\n    .set_index(degree_index)\\\n    .plot(figsize=(10,4))\nax.set_ylabel('Mean squared error')\n", "intent": "*So what happens to the model performance in- and out-of-sample?*\n"}
{"snippet": "order_idx = pd.Index(range(n_degrees+1),name='Polynomial order')\nax = pd.DataFrame(parameters,index=order_idx)\\\n.abs().mean(1)\\\n.plot(logy=True)\nax.set_ylabel('Mean parameter size')\n", "intent": "*What do you mean coefficient size increase?*\n"}
{"snippet": "print('\\n'.join(load_boston()['DESCR'].split('\\n')[13:28]))\n", "intent": "*Let's some load Boston house price data*\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_boston\nX = load_boston().data\ny = load_boston().target\nprint(load_boston().feature_names)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\npipe_preproc.fit(X_train) \nX_train_prep = pipe_preproc.transform(X_train) \nX_test_prep = pipe_preproc.transform(X_test) \n", "intent": "*And how do I apply the pipe on the data?*\n"}
{"snippet": "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=1/3, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=1/2, random_state=1)\n", "intent": "*What would this look like in Python?*\n"}
{"snippet": "preprocessed_df = preprocessed_df.fillna(0)\n", "intent": "Fill other NANs with zeros as the simple strategy. (To be honest I've tried other ones, but they didn't show significantly better results)\n"}
{"snippet": "df = pd.DataFrame({'Criminal':[1]*5+[0]*10,\n                   'From Jutland':np.random.randint(0,2,15),                   \n                   'Parents together':[0]*4+[1]*10+[0],\n                   'Parents unemployed':[1]*7+[0]*8})\nprint(df.sample(n=5))\n", "intent": "*Suppose we have data like below, we are interested in predicting criminal*\n"}
{"snippet": "my_split = df\\\n.groupby(['Parents together', \n          'Parents unemployed'])\\\n.Criminal\\\n.value_counts()\\\n.unstack(level=-1)\\\n.fillna(0)\nprint(my_split)\n", "intent": "*What if we also split by 'Parents unemployed'? And From Jutland?*\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv') \n", "intent": "Lets see how it does on a real dataset.\n"}
{"snippet": "from sklearn import datasets\nimport numpy as np\nfrom datetime import datetime as dt\niris = datasets.load_iris().data\nstart = dt.now()\npairwise_distances = []\nfor i in range(iris.shape[0]):\n    for j in range(i+1, iris.shape[0]):\n        pairwise_distances.append(np.sqrt(sum((iris[i] - iris[j])**2)))\nprint(\"Time:\", (dt.now() - start).total_seconds())\n", "intent": ">**Ex. 19.1.3**: Take the following bit of code and parallelize it. Report the speedup (if any)\n"}
{"snippet": "T = 1000\ndata = {v:np.cumsum(np.random.randn(T)) for v in ['A', 'B']}\ndata['time'] = pd.date_range(start='20150101', freq='D', periods=T)\nts_df = pd.DataFrame(data)\nts_df.set_index('time').plot(figsize=(10,5))\n", "intent": "We can easily make and plot time series.\n"}
{"snippet": "nan_data = [[1,np.nan,3],\n            [4,5,None],\n            [7,8,9]]\nnan_df = pd.DataFrame(nan_data, columns=['A','B','C'])\nprint(nan_df.isnull().sum())\n", "intent": "*What does a DataFrame with missing data look like?*\n"}
{"snippet": "print(nan_df.fillna(2))\nselection = nan_df.B.isnull()\nnan_df.loc[selection, 'B'] = -99\nprint(nan_df)\n", "intent": "*How do we fill observations with a constant?*\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndf = pd.read_csv(\n    'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv',\n     header=None,\n     usecols=[0,1,2]\n    )\ndf.columns=['Class label', 'Alcohol', 'Malic acid']\ndf.head()\n", "intent": "** Loading the wine data set **\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler, StandardScaler\nminmax_scale = MinMaxScaler().fit(df[['Alcohol', 'Malic acid']])\nmin_max_x = minmax_scale.transform(df[['Alcohol', 'Malic acid']])\n", "intent": "Mahematical formula is \n$x' = \\dfrac{(x-min)}{(max-min)}$\nAfter rescaling all values are in the range between 0 and 1\n"}
{"snippet": "test_df = pd.read_csv('test.csv', index_col='Respondent')\n", "intent": "Relatively high model improvement, as expected.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\ndf = pd.read_csv('adult.csv', na_values=['\n", "intent": "Task: Given attributes about a person, predict whether their income is <=50K or >50K\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('hw2data.csv')\ndf.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "X = df_final[['balance', 'income', 'student']]\ny = df_final['default']\nrandom_state = [10,20,40,80,160]\nfor s in random_state:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=s)\n    fpr,tpr,roc_auc, thresholds = generate_auc(X_train,y_train,LogisticRegression,C=100, penalty='l1')\n    generate_ROCplot(fpr,tpr,'LR',roc_auc)\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(documents)\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "import pandas as pd\nvehicles = pd.read_csv('used_vehicles.csv')\nvehicles.head(2)\n", "intent": "In this example we are going to predict price of vehicles\n"}
{"snippet": "train_features,test_features,train_labels,test_labels = train_test_split(main_features,labels)\n", "intent": "Let us split our dataset into training and test sets using the cross_validation module in SKLEARN\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3 , random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "data = pd.read_csv('yelp.csv')\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "import keras\nfrom keras.datasets import cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint(y_train[:10])\n", "intent": "---\nIn this notebook, we train a CNN on augmented images from the CIFAR-10 database.\n"}
{"snippet": "test_df = pd.read_csv('test.csv', index_col='Respondent')\n", "intent": "Anyway, the model performs better.\n"}
{"snippet": "man = train[train.Sex == 'male']['Survived'].value_counts()\nwoman = train[train.Sex == 'female']['Survived'].value_counts()\ndf = pd.DataFrame([man,woman])\ndf.index = ['Man','Woman']\ndf.plot(kind='bar',stacked=True, figsize=(8,4))\n", "intent": "<b> 1. Proportion of survived men and women </b>\n"}
{"snippet": "survived_class = train[train['Survived']==1]['Pclass'].value_counts()\ndead_class = train[train['Survived']==0]['Pclass'].value_counts()\ndf = pd.DataFrame([survived_class,dead_class])\ndf.index = ['Survived','Dead']\ndf.plot(kind='bar',stacked=True, figsize=(8,4))\n", "intent": "<b> 2. Proportion of survived and dead in PClass </b>\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('hw2data.csv')\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "size = len(df)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "with open('../sentiment-network/reviews.txt', 'r') as f:\n    reviews = f.read()\nwith open('../sentiment-network/labels.txt', 'r') as f:\n    labels = f.read()\n", "intent": "if you are looking for sentiment-network, here's the link: https://github.com/udacity/deep-learning/tree/master/sentiment-network\n"}
{"snippet": "fare_means = df.pivot_table('Fare', index='Pclass', aggfunc='mean')\nfare_means\n", "intent": "For the column Fare, however, it makes sense to fill in the NaN values with the mean by the column Pclass, or Passenger class.\n"}
{"snippet": "contentImage = io.BytesIO(urlopen(\"https://raw.githubusercontent.com/Baidaly/FORK_deep-learning-v2-pytorch/master/style-transfer/images/octopus.jpg\").read())\ncontent = load_image(contentImage).to(device)\nstyleImage = io.BytesIO(urlopen(\"https://raw.githubusercontent.com/Baidaly/FORK_deep-learning-v2-pytorch/master/style-transfer/images/hockney.jpg\").read())\nstyle = load_image(styleImage, shape=content.shape[-2:]).to(device)\n", "intent": "Next, I'm loading in images by file name and forcing the style image to be the same size as the content image.\n"}
{"snippet": "url = 'https://raw.githubusercontent.com/Baidaly/IPython-Notebooks/master/pytorch-udacity-challenge/Lesson%207/anna.txt'\nresponse = urllib.request.urlopen(url)\ndata = response.read()      \ntext = data.decode('utf-8') \n", "intent": "Then, we'll load the Anna Karenina text file and convert it into integers for our network to use. \n"}
{"snippet": "source = 'https://fb55.cartodb.com/api/v2/sql'\ndef queryCartoDB_py3(source, query, format='CSV'):\n    p = urllib.parse.urlencode({'q':query,\n                                'format':format\n                                })\n    with urllib.request.urlopen(source + '?' + p) as response:\n        the_page = response.read().decode('utf8')\n    return the_page\n", "intent": "Query to access Census 2000 and 2010 data from fb55's CartoDB acct.\n"}
{"snippet": "test_df = pd.read_csv('test.csv', index_col='Respondent')\n", "intent": "Load hold-out set from the disk\n"}
{"snippet": "data = pd.read_csv('D:/PjtData/4_PD/edX_DSE200x/wk7_ML/minute_weather.csv')\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nCreating a Pandas DataFrame from a CSV file<br><br></p>\n"}
{"snippet": "df = pd.read_csv('hw2data.csv')\ndf.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "regr.set_params(alpha=best_alpha)\ndf_coeffs = pd.DataFrame({'coeffs':regr.coef_, 'feature number':X.columns.values})\nsorted_coeffs=df_coeffs.sort_values('coeffs', ascending = False)\nsorted_coeffs[:100].plot(x='feature number',y='coeffs',kind='bar', figsize =(20,10))\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "df = pd.read_csv('https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv')\ndf.head()\n", "intent": "1) Load in the dataset `https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv` into a pandas dataframe\n"}
{"snippet": "PAULG_PATH = 'paulg/'\nPAULG_FILENAME = 'paulg/paulg.txt'\nwith open(PAULG_FILENAME) as f:\n    lines = f.read().split('\\n')\nlines[0]\n", "intent": "http://suriyadeepan.github.io/2017-02-13-unfolding-rnn-2/\n"}
{"snippet": "train = pd.read_csv( 'labeledTrainData.tsv', delimiter = '\\t' )\ntest  = pd.read_csv( 'testData.tsv', delimiter = '\\t' )\ntrain.head()\n", "intent": "Kaggle knowledge competition: movie review sentiment analysis. [homepage](https://www.kaggle.com/c/word2vec-nlp-tutorial)\n"}
{"snippet": "vect1 = CountVectorizer( stop_words = 'english' )\nX_train_dtm = vect1.fit_transform(X_train)\nX_test_dtm  = vect1.transform(X_test)\n", "intent": "Use the most simplest approach: remove english stopwords, bag of words + default logistic regression and naive bayes.\n"}
{"snippet": "tf_vect1 = TfidfVectorizer( \n    stop_words = 'english', \n    tokenizer = tokenizer_porter, \n    ngram_range = ( 1, 2 ),\n    min_df = 2\n)\nX_train_dtm = tf_vect1.fit_transform(X_train)\nX_test_dtm  = tf_vect1.transform(X_test)\n", "intent": "Use tf-idf instead of bag of words.\n"}
{"snippet": "token_text = [token.orth_ for token in parsed_review]\ntoken_pos = [token.pos_ for token in parsed_review]\npost = pd.DataFrame( list( zip(token_text, token_pos) ),\n                     columns = ['token_text', 'part_of_speech'] )\npost.head()\n", "intent": "What about part of speech tagging (POST)?\n"}
{"snippet": "pca = PCA(n_components=pca_comp.n_components).fit(X_train_scaled)\npca_features_train = pca.transform(X_train_scaled)\npca_features_test = pca.transform(X_test_scaled)\n", "intent": "PCA needs only 73 components to explain the variance. \nLets fit and transform train and test data with these components\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split( x, y, test_size = 0.3, random_state = 0 )\n", "intent": "`train_test_split` Split the dataset into 70 percent training and 30 percent testing \n"}
{"snippet": "sc = StandardScaler()\nsc.fit(x_train)\nx_train_sd = sc.transform(x_train)\nx_test_sd  = sc.transform(x_test)\n", "intent": "`StandardScaler` use .fit and .transform the perform the feature scaling\n"}
{"snippet": "class_le = LabelEncoder()\ndf[\"classlabel\"] = class_le.fit_transform( df[\"classlabel\"] )\ndf\n", "intent": "Same functionality using sklearn. Note that `fit_transform` is a shortcut for calling fit and transform separately.\n"}
{"snippet": "df = pd.read_csv(\n    filepath_or_buffer = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n    header = None,\n    sep = ','\n)\ndf.columns = ['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\ndf.dropna( how = \"all\", inplace = True ) \ndf.tail()\n", "intent": "Warming up using the iris dataset as an example. [Link](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html) to the warm up.\n"}
{"snippet": "x = df.iloc[ :, 2: ].values\ny = df.iloc[ :, 1 ].values\nle = LabelEncoder()\ny = le.fit_transform(y)\n", "intent": "Extract the input and output features and label encode the output feature's class.\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split( x, y, test_size = 0.20, random_state = 1 )\n", "intent": "Split the full dataset into 80 training / 20 testing.\n"}
{"snippet": "iris = datasets.load_iris()\nX, y = iris.data[ 50:, [1, 2] ], iris.target[50:]\nle = LabelEncoder()\ny  = le.fit_transform(y)\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.5, random_state = 1 )\n", "intent": "Load iris data. Use only two of the features and work on classifying two classes.\n"}
{"snippet": "from sklearn.base import clone\nlablenc_ = LabelEncoder()\nlablenc_.fit(y)\nclasses_ = lablenc_.classes_ \nclassifiers_ = []\nfor clf in classifiers:\n    fitted_clf = clone(clf).fit( X, lablenc_.transform(y) )\n    classifiers_.append(fitted_clf)\n", "intent": "Loop through all the classfiers and fit all the models.\n"}
{"snippet": "file_dir = 'lastfm-dataset-360K'\nfile_path = os.path.join(file_dir, 'usersha1-artmbid-artname-plays.tsv')\nif not os.path.isdir(file_dir):\n    subprocess.call(['curl', '-O', 'http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-360K.tar.gz'])\n    subprocess.call(['tar', '-xvzf', 'lastfm-dataset-360K.tar.gz'])\ncol_names = ['user', 'artist', 'plays']\ndata = pd.read_csv(file_path, sep = '\\t', usecols = [0, 2, 3], names = col_names)\ndata = data.dropna(axis = 0)\nprint(data.shape)\ndata.head()\n", "intent": "http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-360K.html\n"}
{"snippet": "def make_comparison_dataframe(historical, forecast):\n    return forecast.set_index('ds')[['yhat', 'yhat_lower', 'yhat_upper']].join(historical.set_index('ds'))\n", "intent": "Lets combine Historic and Forecast data together\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint('X_train shape:', X_train.shape)\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0] , 'test samples')\n", "intent": "Now that we have a basic understanding of the definition of a ResNet, we will build one a train it on the MNIST dataset.\n"}
{"snippet": "target_size = 224, 224\nimg = image.load_img(img_path, target_size=target_size)\nimg = image.img_to_array(img)\nimg = np.expand_dims(img, axis=0)\nprint(img[0, 0, 0])\nimg = img[..., ::-1]\nprint(img[0, 0, 0])\n", "intent": "In case your curious, the following two code cells shows what's happening underneath the hood when we call resnet50's `preprocess_input` function.\n"}
{"snippet": "train = pd.read_csv('Dataset/Train.csv')\ntest  = pd.read_csv('Dataset/Test.csv')\ntrain['source'] = 'train' \ntest['source']  = 'test'\ndata = pd.concat( [train, test], ignore_index = True )\ndata.shape\n", "intent": "Load the training and testing data and combine them together for preprocessing.\n"}
{"snippet": "digits = load_digits()\ndigits.data.shape\n", "intent": "Now we load the classic handwritten digits [datasets](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\n"}
{"snippet": "print(\"total variance:{}\".format(np.sum(np.var(X,0))))\npca = PCA(2) \npca.fit(X) \nprint(\"variance explained via the first and second components:{}, {}\\n\".format(pca.explained_variance_[0],pca.explained_variance_[1]))\nprint(\"Transformation matrix:\\n{}\".format(pca.components_)) \n", "intent": "(in this particular example ~97.6% of the variance is preserved if we project down to the leading principal component)\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nspacing = np.linspace(0, max(df['gross']), 100)\nlabels = []\nlabels = [\"low\", \"low-mid\", \"high-mid\", \"high\"]\ndf['gross_group'] = pd.qcut(df['gross'], 4, labels=labels)\nrating_group = df['gross_group'].values\nrating_encoder = LabelEncoder()\nrating_df = pd.DataFrame(rating_encoder.fit_transform(rating_group), columns=['encoded_gross']).astype(str)\ndf = pd.concat([df, rating_df], axis=1)\n", "intent": "- add another label if I want to! And talk about it!\n"}
{"snippet": "labels = ['age', 'cholesterol', 'sex', 'death']\ninfarkty = pd.read_csv('exponea-MLworkshop/lecture3/infarkty.txt', sep=\"\\t\\t\", names=labels).drop(0)\ninfarkty.index -=1\ninfarkty.head(10)\n", "intent": "- age: hodnoty voci veku 50 rokov\n- cholesterol: hodnoty oproti 5 mmol/liter (ideal pre dospelaka)\n- sex: 0 muzi, 1 zeny\n- death: True/False\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\n", "intent": "http://scikit-learn.org/stable/modules/preprocessing.html\n"}
{"snippet": "college = pd.read_csv('College_Data', index_col = 0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n", "intent": "Separate our data into the main and test sets in the ratio 70/30.\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features,columns=KNN_data.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101 )\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "coeff_ecom = pd.DataFrame(lm.coef_, X.columns, columns = ['Coef'])\ncoeff_ecom\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "df = pd.read_csv('kyphosis.csv')\n", "intent": "Read in the kyphosis csv and check out the head of the dataframe\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head()\n", "intent": "**Create the new dataframe with scaled features**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(scaled_features,df['TARGET CLASS'],\n                                                    test_size=0.30)\n", "intent": "**Notice that we can tie the TARGET CLASS back into our train_test_split by passing it as an additional argument to our function below**\n"}
{"snippet": "h = pd.read_csv('USA_housing.csv')\n", "intent": "Create a new dataframe, h, which will store the USA_housing.csv file\n"}
{"snippet": "coeff_df = pd.DataFrame(lm.coef_, X.columns, columns = ['Coefficients'])\ncoeff_df\n", "intent": "**Create a dataframe of coefficients**\n"}
{"snippet": "train = pd.read_csv('titanic_train.csv')\n", "intent": "Let's start by reading in the titanic_train.csv file into a pandas dataframe\n"}
{"snippet": "df = pd.read_json(PATH_TO_DATA + '/Dataset for Detection of Cyber-Trolls.json', lines= True)\ndf.head()\n", "intent": "**Read the data and have a look at it**\n"}
{"snippet": "data = datasets.load_boston()\nprint data.DESCR\n", "intent": "**Load the boston housing data with the `datasets.load_boston()` function.**\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\npredictors=['CRIM','AGE','TAX']\nY=boston.target\nX_train,X_test,Y_train,Y_test=train_test_split(X[predictors],Y,train_size=0.9)\nlr=LinearRegression()\nlr.fit(X_train,Y_train)\nlr.fit(X_test,Y_test)\n", "intent": "- Score and plot.  \n- How do your metrics change?\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X[predictors], y, train_size=0.7, random_state=8)\nfrom sklearn.linear_model import LinearRegression\nlr2 = LinearRegression()\nlr2.fit(X_train,y_train)\nlr2.score(X_test, y_test)\n", "intent": "- Score and plot.  \n- How do your metrics change?\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50)\nknn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(X_train, y_train)\n", "intent": "**Split the data into 66% training and 33% testing. Fit a KNN classifier with `n_neighbors=25` on the training data.**\n"}
{"snippet": "hsq = pd.read_csv('/Users/Mahendra/Desktop/GA/hw/5.5.2_evaluation-classifiers_confusion_matrix_roc-lab/datasets/hsq_data.csv')\nhsq.head()\n", "intent": "It is worth reading over the description of the data columns above for this.\n"}
{"snippet": "df = pd.read_csv('/Users/Mahendra/desktop/GA/hw/6.1.2_optimization-feature_selection-lab/datasets/titanic_train.csv')\n", "intent": "We'll be working with the titanic datasets - go ahead and import it from the dataset folder (or query for it as described above). \n"}
{"snippet": "ss = StandardScaler()\nXs = ss.fit_transform(X)\n", "intent": "**10.C Standardize the predictor matrix.**\n"}
{"snippet": "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.5,random_state=12)\n", "intent": "**Split data into training and testing with 50% in testing.**\n"}
{"snippet": "sd = pd.read_csv('/Users/Indraja/Documents/Dsi/7.4.2_pca-intro-lab/datasets/speed_dating.csv')\nsd.head()\n", "intent": "---\n- Remove columns with over 200 missing values.\n- Then, remove rows with missing values.\n- Verify no rows contain NaNs.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size = 0.3, stratify = target, random_state = 31)\n", "intent": "**Split dataset on train and test**\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "First we'll load the famous *iris* dataset, dealing with plant classification:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n                                                    train_size=0.75, test_size=0.25)\n", "intent": "Now we split the data into training and testing:\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "For demonstration, we will use a boston housing dataset, which comes with scikit-learn:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n                                                    train_size=0.75, test_size=0.25)\n", "intent": "Now that we're familiar with the input data, we need to split it up for training and testing:\n"}
{"snippet": "boston = load_boston()\nX, y = shuffle(boston.data, boston.target, random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "Let's load the data and split into training and testing portions\n"}
{"snippet": "X, y = shuffle(new_boston[\"data\"], new_boston[\"target\"], random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "Now we can reassign the new data to training and testing\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"booksummaries.txt\", sep=\"\\t\")\n", "intent": "First we'll read in the .tsv file into a pandas dataframe:\n"}
{"snippet": "import pandas as pd\nvocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\nprint (vocab_frame.shape[0])\nprint (vocab_frame)\n", "intent": "Our data frame will map tokenized words to stemmed words, recalling our work with pandas in Day 3 of the introductory series:\n"}
{"snippet": "from sklearn.manifold import MDS\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state = 10)\npos = mds.fit_transform(dist)  \nxs, ys = pos[:, 0], pos[:, 1] \n", "intent": "Two dimensional scaling must be applied for plotting:\n"}
{"snippet": "num_na = df_person.columns[df_person.isna().any()].tolist()\ndf_person = df_person.fillna(df_person.median())\nna_count = df_person.isna().sum()\nna_count[na_count > 0]\n", "intent": "As for the rest of columns with NAs, those are numeric, and we will fill NAs there with medians.\n"}
{"snippet": "sstrain = StandardScaler()\nsstest = StandardScaler()\nlr_sep = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\nXtrain_n = sstrain.fit_transform(Xtrain)\nXtest_n = sstest.fit_transform(Xtest)\n", "intent": "**For the sake of example, standardize the Xtrain and Xtest separately and show that their normalization parameters differ.**\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=10)\nskl_pca = pca.fit_transform(X_2)\nskl_pca\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "X = StandardScaler().fit_transform(x)\n", "intent": "Standardize the data and compare at least one of the scatterplots for the scaled data to unscaled above.\n"}
{"snippet": "cvt      =  CountVectorizer(strip_accents='unicode', ngram_range=(1,1))\nX_all    =  cvt.fit_transform(insults_df['Comment'])\ncolumns  =  np.array(cvt.get_feature_names())          \nX_all\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "cvt = CountVectorizer(stop_words=\"english\", ngram_range=(2,4))\nX_all = cvt.fit_transform(insults_df['Comment'])\ncolumns  =  np.array(cvt.get_feature_names())\nfreq_words = get_freq_words(X_all, columns)\nfreq_words\n", "intent": "Display the top 75 ngrams with frequencies.\n"}
{"snippet": "cvt      =  CountVectorizer(strip_accents='unicode')\nX_all    =  cvt.fit_transform(insults_df['Comment'])\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "df = pd.read_csv('./datasets/breast_cancer_wisconsin/breast_cancer.csv', na_values='?')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "bin_features_list = [\"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\", \"CODE_GENDER\"]\nlabel_encoder = LabelEncoder()\nfor feature in bin_features_list:\n    label_encoder.fit(train_data[feature])\n    train_data[feature] = label_encoder.transform(train_data[feature])\n    test_data[feature] = label_encoder.transform(test_data[feature])\n", "intent": "Some of the categorical features have only two values so it seems reasonable to transform them into binary format instead of one-hot encoding.\n"}
{"snippet": "for feature in np.concatenate((house_related_features, enquiry_related_features, social_surrounding_features)):\n    print(\"Filling missing data for feature {}\".format(feature))\n    train_data_encoded[feature].fillna(train_data_encoded[feature].mean(), inplace=True)\n    test_data_encoded[feature].fillna(test_data_encoded[feature].mean(), inplace=True)\n", "intent": "It was decided to fill missing values of house-related, enquiry-related and social surrounding-related features with mean value of each feature.\n"}
{"snippet": "df = pd.read_csv('../../data/telecom_churn.csv')\n", "intent": "In the first article, we looked at the data on customer churn for a telecom operator. We will reload the same dataset into a `DataFrame`:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2, whiten=True).fit(X)\nX_pca = pca.transform(X)\n", "intent": "To simplify the analysis, and aid visualization, we will again perform a PCA to isolate the majority of the variation into two principal components.\n"}
{"snippet": "from sklearn.decomposition import PCA\nfrom sklearn import datasets\niris = datasets.load_iris()\npca = PCA(n_components=2, whiten=True).fit(iris.data)\nX_pca = pca.transform(iris.data)\ny = iris.target\n", "intent": "To demonstrate, we will implement a DP to cluster the iris dataset.\n"}
{"snippet": "normalizer = preprocessing.Normalizer().fit(X)\n", "intent": "As with scaling, there is also a `Normalizer` class that can be used to establish normalization with respect to a training set.\n"}
{"snippet": "lb = preprocessing.LabelBinarizer()\nlb.fit([1, 2, 6, 4, 2])\n", "intent": "`LabelBinarizer` is a utility class to help create a label indicator matrix from a list of multi-class labels:\n"}
{"snippet": "lb = preprocessing.MultiLabelBinarizer()\nlb.fit_transform([(1, 2), (3,)])\n", "intent": "For multiple labels per instance, use MultiLabelBinarizer:\n"}
{"snippet": "le = preprocessing.LabelEncoder()\nle.fit([1,2,2,6])\n", "intent": "`LabelEncoder` is a utility class to help normalize labels such that they contain only consecutive values between 0 and `n_classes-1`.\n"}
{"snippet": "mode_imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n", "intent": "In our educational outcomes dataset, we are probably better served using mode imputation:\n"}
{"snippet": "y = impute_subset.pop('mother_hs').values\nX = preprocessing.StandardScaler().fit_transform(impute_subset.astype(float))\n", "intent": "Next, we scale the predictor variables to range from 0 to 1, to improve the performance of the regression model.\n"}
{"snippet": "from sklearn import model_selection\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n        wine.values, grape.values, test_size=0.4, random_state=0)\n", "intent": "Implementing cross-validation on our wine SVC is straightforward:\n"}
{"snippet": "tsne = TSNE(random_state=17)\ntsne_repr = tsne.fit_transform(X_scaled)\n", "intent": "Now, let's build a t-SNE representation:\n"}
{"snippet": "test_X, test_y = datasets.make_moons(50, noise=0.20)\ntest_X = test_X.astype(np.float32)\ntest_y = test_y.astype(np.int32)\n", "intent": "Since we used the scikit-learn interface, its easy to take advantage of the `metrics` module to evaluate the MLP's performance.\n"}
{"snippet": "train = pd.read_csv(\"train_data.csv\")\nvalid = pd.read_csv(\"validation_data.csv\")\ntrain_x = train[\"x\"].tolist()\ntrain_y = train[\"y\"].tolist()\nvalid_x = valid[\"x\"].tolist()\nvalid_y = valid[\"y\"].tolist()\n", "intent": "Next, we evaluate each of these models on our data.  Fill in the parts of the function marked \"TODO\" below.\n"}
{"snippet": "import pandas as pd\ntrain = pd.read_csv(\"train_data.csv\")\nvalid = pd.read_csv(\"validation_data.csv\")\ntrain_x = train[\"x\"].tolist()\ntrain_y = train[\"y\"].tolist()\nvalid_x = valid[\"x\"].tolist()\nvalid_y = valid[\"y\"].tolist()\n", "intent": "Now we evaluate each of these techniques on our data.  Fill in the parts of the function marked \"TODO\" below\n"}
{"snippet": "cwd = os.getcwd()\ndatadir = '/'.join(cwd.split('/')[0:-1]) + '/data/'\nf = datadir + 'ads_dataset_cut.txt'\ndata = pd.read_csv(f, sep = '\\t')\ndata.columns, data.shape\n", "intent": "First we'll load the dataset and take a quick peak at its columns and size\n"}
{"snippet": "df_auc = pd.DataFrame(pd.Series(feature_auc_dict), columns = ['auc'])\ndf_mi = pd.DataFrame(pd.Series(feature_mi_dict), columns = ['mi'])\nfeat_imp_df = df_auc.merge(df_mi, left_index = True, right_index = True)\nfeat_imp_df\n", "intent": "Next we want to add both of the dictionaries created above into a data frame.\n"}
{"snippet": "import numpy as np\nres = pd.DataFrame(xval_dict)\nres['low'] = res['mu'] - 1.96*res['sig']/np.sqrt(10)\nres['up'] = res['mu'] + 1.96*res['sig']/np.sqrt(10)\n", "intent": "<p>We can now load the results from above into a dataframe and begin to analyze\n</p>\n"}
{"snippet": "X = data['text']\nY = data['spam']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=.75)\n", "intent": "Since we are going to do some modeling, we should split our data into a training and test set.\n"}
{"snippet": "df = pd.read_csv('moviebuzz.csv')\n", "intent": "Let us go ahead and take a look at the dataset\n"}
{"snippet": "pca_orig = PCA(n_components = 2).fit(original_imgs)\n", "intent": "Run PCA with 2 components on the original images\n"}
{"snippet": "data2 = pd.DataFrame({'Age':  [17, 64, 18, 20, 38, 49, 55, 25, 29, 31, 33], \n                      'Salary': [25, 80, 22, 36, 37, 59, 74, 70, 33, 102, 88], \n             'Loan Default': [1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1]})\ndata2\n", "intent": "Let's consider a more complex example by adding the \"Salary\" variable (in the thousands of dollars per year).\n"}
{"snippet": "pca_orig = PCA\npca_orig = PCA(n_components = 2).fit(original_imgs)\n", "intent": "Run PCA with 2 components on the original images\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXs_new = ss.fit_transform(X_new)\nXs_new1 = ss.fit_transform(X_new1)\n", "intent": "Note : I have taken top 30 features of logistic and obseved the accuracy levels.They don't seem considerably good when compared with top 20 features\n"}
{"snippet": "pd.DataFrame([pca_results[\"Explained Variance\"], pca_results[\"Explained Variance\"].cumsum().rename(\"Cum. sum\")])                                                                                       \n", "intent": "To answer the question I'll print variances and their cummulated sum below:\n"}
{"snippet": "df_sample = pd.DataFrame(features).join(income_raw).sample(n=2500, random_state=0)\nincome_sampe = df_sample['income']\nfeatures_sample = df_sample.drop('income', axis = 1)\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, random_state=0, method='exact')\nX_2D_porjection = tsne.fit_transform(features_sample) \n", "intent": "As we can see we have significat correlation between several features. Let's also build 2D projection of data sample:\n"}
{"snippet": "df3 = pd.read_csv('/home/cneiderer/Metis/Neiderer_Metis/Challenges/challenges_data/2013_movies.csv',\n                 header='infer')\n", "intent": "For your movie classifiers, calculate the precision and recall for each class.\n"}
{"snippet": "cols = ['Age', 'Year', 'NumNodes', 'Survived']\ndf4 = pd.read_csv('/home/cneiderer/Metis/Neiderer_Metis/Challenges/challenges_data/haberman.data', \n                  header=0, names=cols)\n", "intent": "Draw the ROC curve (and calculate AUC) for the logistic regression classifier from challenge 12.\n"}
{"snippet": "trans_fitted = scaler.fit_transform(df.drop('Class', axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "df_fitted = pd.DataFrame(trans_fitted, columns=df.columns[:-1])\ndf_fitted.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "** Use SciKit Learn to create training and testing sets of the data as we've done in previous lectures:**\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nknn_pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\nknn_params = {'knn__n_neighbors': range(1, 10)}\nknn_grid = GridSearchCV(knn_pipe, knn_params,\n                        cv=5, n_jobs=-1, verbose=True)\nknn_grid.fit(X_train, y_train)\nknn_grid.best_params_, knn_grid.best_score_\n", "intent": "Now, let's tune the number of neighbors $k$ for k-NN:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nX = cv.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))\n    print(df)\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "pivoted2=pd.pivot_table(mpg, values=['cty', 'hwy'], index='year')\npivoted2\n", "intent": "3c. Has the average city mileage improved from 1999 to 2008?   Has the average highway mileage improved from 1999 to 2008?\n"}
{"snippet": "pd.pivot_table(iris, values=['petal_width'], index='species')\n", "intent": "4c. Compute the average petal width for each of the \"species\"-categories.\n"}
{"snippet": "sc = MinMaxScaler(feature_range=(0,1))\ntraining_set_scaled = sc.fit_transform(training_set)\n", "intent": "fit -> get min and max of training set, such that x_norm = (x - min(x))/(max(x) - min(x))\n"}
{"snippet": "residuals = pd.DataFrame(model_fit.resid)\nresiduals.plot();\n", "intent": "First, we get a line plot of the residual errors, suggesting that there may still be some trend information not captured by the model.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nprint '\\n The object \"iris\" here is of type:', type(iris)\nprint '\\n Keys of the bunch:', iris.keys()\n", "intent": "---\n* **Packaged Data:** small datasets packaged with `scikit-learn`, can be downloaded using \n    * ``sklearn.datasets.load_*``\n"}
{"snippet": "cv = CountVectorizer()\ncv.fit(text_train)\nlen(cv.vocabulary_)\n", "intent": "**First, we will create a dictionary of all the words using CountVectorizer**\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\nprint digits.keys()\nprint digits.images.shape\nprint digits.data.shape\n", "intent": "<img src='http://myselph.de/mnistExamples.png'>\n"}
{"snippet": "pd.concat([pd.DataFrame(df_1['parameters'].tolist()),\n           df_1['mean_validation_score']], axis=1).sort_values('mean_validation_score', ascending=False).iloc[:10, :]\n", "intent": "- Tradeoff Complexity vs. Generalizability\n"}
{"snippet": "from sklearn.datasets import make_classification\nX, y = make_classification(1000, n_features=20, n_informative=2, \n                           n_redundant=2, n_classes=2, random_state=0)\nfrom pandas import DataFrame\ndf = DataFrame(np.hstack((X, y[:, None])), \n               columns = range(20) + [\"class\"])\n", "intent": "We will generate some simple toy data using [sklearn](http://scikit-learn.org)'s `make_classification` function:\n"}
{"snippet": "from sklearn.datasets import make_circles\nX, y = make_circles(n_samples=1000, random_state=2)\n", "intent": "We generate another dataset for binary classification and apply a `LinearSVC` again.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits(n_class=6)\nX = digits.data\ny = digits.target\nn_samples, n_features = X.shape\nprint \"Dataset consist of %d samples with %d features each\" % (n_samples, n_features)\n", "intent": "Now for one of the classic datasets used in machine learning, which deals with optical digit recognition: \n"}
{"snippet": "from sklearn import (manifold, decomposition, random_projection)\nrp = random_projection.SparseRandomProjection(n_components=2, random_state=42)\nstime = time.time()\nX_projected = rp.fit_transform(X)\nplot_embedding(X_projected, \"Random Projection of the digits (time: %.3fs)\" % (time.time() - stime))\n", "intent": "Already a random projection of the data to two dimensions gives a not too bad impression:\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42, n_samples=1000, centers=3, cluster_std=1.5)\nX.shape\n", "intent": "Let's start of with a very simple and obvious example:\n"}
{"snippet": "sent = [\"The grass is green, and sometimes it feels nice to run barefoot.\",\n       \"The sky is blue but sometimes the fog doesn't let you see it.\"]\nvect = CountVectorizer()\nvect.fit(sent)\n", "intent": "[CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape\n", "intent": "- `**lowercase:**` boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "data = pd.read_csv('../../data/telecom_churn.csv').drop('State', axis=1)\ndata['International plan'] = data['International plan'].map({'Yes': 1, 'No': 0})\ndata['Voice mail plan'] = data['Voice mail plan'].map({'Yes': 1, 'No': 0})\ny = data['Churn'].astype('int').values\nX = data.drop('Churn', axis=1).values\n", "intent": "We will work our data on customer churn of telecom operator.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(newsgroup.data, \n                                                    newsgroup.target, \n                                                    train_size=0.8, \n                                                    random_state=42)\n", "intent": "In scikit-learn, a Train-Test split is simply done with the function train_test_split from the cross_validation package.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nlabel_dict = dict()\nfor col in disc_cols:\n    le = LabelEncoder()\n    df_data[col] = le.fit_transform(df_data[col])\n    label_dict[col] = dict()\n    for cls, label in zip(le.classes_, le.transform(le.classes_)):\n        label_dict[col][label] = cls\n", "intent": "(sklearn classifiers do not take string values as input)\n"}
{"snippet": "cat_encoder = OneHotEncoder(sparse=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot\n", "intent": "Alternatively, you can set `sparse=False` when creating the `OneHotEncoder`:\n"}
{"snippet": "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n                                        index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.most_frequent_)\n", "intent": "We will also need an imputer for the string categorical columns (the regular `Imputer` does not work on those):\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\nX_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\nX_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\ny_train = y_train.astype(np.int32)\ny_test = y_test.astype(np.int32)\nX_valid, X_train = X_train[:5000], X_train[5000:]\ny_valid, y_train = y_train[:5000], y_train[5000:]\n", "intent": "**Warning**: `tf.examples.tutorials.mnist` is deprecated. We will use `tf.keras.datasets.mnist` instead.\n"}
{"snippet": "california_housing_dataframe = pd.read_csv(\"https://storage.googleapis.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\n", "intent": "Next, we'll load our data set.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'])\n", "intent": "Here we're splitting the data into two sets, test and train. We'll create the model with train and then see how well it works with the test model.\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n", "intent": "Fashion MNIST data is being built into ``from keras.datasets`` module.\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n", "intent": "Fashion MNIST data is being built into ``from keras.datasets`` module.\n"}
{"snippet": "boston = load_boston()\nX, y = boston['data'], boston['target']\n", "intent": "**We will work with Boston house prices data (UCI repository).**\n**Download the data.**\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n", "intent": "MNIST data is being built into ``from keras.datasets`` module.\n"}
{"snippet": "df_combined[\"Cabin\"].fillna(\"U\", inplace=True)\nprint(\"Number of Nan values in Cabin column: {}\".format(df_combined[\"Cabin\"].isnull().sum()))\n", "intent": "We will leave first letter of Cabin as type and repleace all Nan values with \"U\" which means unknown.\n"}
{"snippet": "df_combined[\"Embarked\"].fillna(\"S\", inplace=True)\nprint(\"Number of Nan values in Embarked column: {}\".format(df_combined[\"Embarked\"].isnull().sum()))\n", "intent": "Fill embarked values by most recent appearing embarkment\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    X_data, y_data, test_size=0.2, random_state=RANDOM_SEED)\n", "intent": "Podzielmy dane na zbiory: treningowy oraz testowy.\n"}
{"snippet": "train_content = pd.read_table(os.path.join(PATH_TO_DATA,'train_content.txt'), sep='\\n', header=None)\ntest_content = pd.read_table(os.path.join(PATH_TO_DATA,'test_content.txt'), sep='\\n', header=None)\n", "intent": "***Tf-Idf with article content ***\n"}
{"snippet": "train_title = pd.read_table(os.path.join(PATH_TO_DATA,'train_title.txt'), sep='\\n', header=None)\ntest_title = pd.read_table(os.path.join(PATH_TO_DATA,'test_title.txt'), sep='\\n', header=None)\n", "intent": "***Tf-Idf with article titles***\n"}
{"snippet": "train_author = pd.read_table(os.path.join(PATH_TO_DATA,'train_author.txt'), sep='\\n', header=None).\\\n                applymap(lambda x: eval(x))\ntest_author = pd.read_table(os.path.join(PATH_TO_DATA,'test_author.txt'), sep='\\n', header=None).\\\n                applymap(lambda x: eval(x))\n", "intent": "***Bag of authors (i.e. One-Hot-Encoded author names)***\n"}
{"snippet": "train_published = pd.read_table(os.path.join(PATH_TO_DATA,'train_published.txt'), sep='\\n', header=None).\\\n                applymap(lambda x: eval(x))\ntest_published = pd.read_table(os.path.join(PATH_TO_DATA,'test_published.txt'), sep='\\n', header=None).\\\n                applymap(lambda x: eval(x))\n", "intent": "*** Time features***\n"}
{"snippet": "train_target = pd.read_csv(os.path.join(PATH_TO_DATA,'train_log1p_recommends.csv'), \n                           index_col='id')\ny_train = train_target['log_recommends'].values\ny_train = y_train[idx]\n", "intent": "**Read train target and split data for validation.**\n"}
{"snippet": "data = np.array([1, 1, 0, -1, 2, 1, 2, 3, -2, 4, 100]).reshape(-1, 1).astype(np.float64)\nStandardScaler().fit_transform(data)\n", "intent": "But, to some extent, it protects against outliers:\n"}
{"snippet": "train_df = pd.read_csv('../../data/train_sessions.csv',\n                       index_col='session_id')\ntest_df = pd.read_csv('../../data/test_sessions.csv',\n                      index_col='session_id')\ntimes = ['time%s' % i for i in range(1, 11)]\ntrain_df[times] = train_df[times].apply(pd.to_datetime).fillna(method='ffill', axis=1)\ntest_df[times] = test_df[times].apply(pd.to_datetime).fillna(method='ffill', axis=1)\ntrain_df = train_df.sort_values(by='time1')\ntrain_df.head()\n", "intent": "Reading original data\n"}
{"snippet": "new_feat_train = pd.DataFrame(index=train_df.index)\nnew_feat_test = pd.DataFrame(index=test_df.index) \nnew_feat_train['year_month'] = train_df['time1'].apply(lambda ts: ts.year * 100 + ts.month)\nnew_feat_test['year_month'] = test_df['time1'].apply(lambda ts: ts.year * 100 + ts.month)\n", "intent": "Add features based on the session start time: hour, whether it's morning, day or night and so on.\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimr = Imputer(missing_values='NaN', strategy='mean', axis=0)\nimr = imr.fit(x)\nx = imr.transform(x.values)\nx.shape\nx.mean()\n", "intent": "Read the page 102 of the book \"Python Machine Learning Unlock deeper insights ...\" (Raschka2015).\n"}
{"snippet": "import scipy\nfrom sklearn.preprocessing import scale\nx_s = scale(x,with_mean=True,with_std=True,axis=0)\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nx_s2 = mms.fit_transform(x)\nfrom sklearn.preprocessing import StandardScaler\nstdsc = StandardScaler()\nx_s3 = stdsc.fit_transform(x)\nprint(np.max(x_s2), np.max(x_s3), np.max(x_s2 - x_s3))\n", "intent": "Transforming all features to the same scale.\n"}
{"snippet": "seed_X = preprocessing.StandardScaler().fit_transform(seeds.iloc[:,:-1])\nseed_y = seeds.variety\nseed_pca = decomposition.PCA(n_components=2)\nseed_res = seed_pca.fit_transform(seed_X)\nseed_pca_rf = ensemble.RandomForestClassifier(random_state=42)\nsp_X_train, sp_X_test, sp_y_train, sp_y_test = model_selection.\\\n    train_test_split(seed_res, seed_y, test_size=.3, random_state=42)\nseed_pca_rf.fit(sp_X_train, sp_y_train)\nseed_pca_rf.score(sp_X_test, sp_y_test)\n", "intent": "* Run a classification on PCA'd data\n* How does it perform versus the raw data?\n"}
{"snippet": "data_scaled = pd.DataFrame(data=X, columns=data.drop('Class', axis=1).columns)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df_scaled = pd.DataFrame(data=X, columns=df.drop(['TARGET CLASS'], axis=1).columns)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "X = count_vectorizer.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "from sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nx_data_kbest = SelectKBest(f_classif, k=5).fit_transform(x_data_generated, y_data_generated)\nx_data_varth = VarianceThreshold(.9).fit_transform(x_data_generated)\n", "intent": "There are other ways that are also [based on classical statistics](http://scikit-learn.org/stable/modules/feature_selection.html\n"}
{"snippet": "in_words = \"alice's\"\nfor _ in range(maxlen):\n    in_sequence = sequence.pad_sequences(tokenizer.texts_to_sequences([in_words]), maxlen=maxlen)\n    wordid = model.predict_classes(in_sequence, verbose=0)[0]\n    for k, v in tokenizer.word_index.items():\n        if v == wordid:\n            in_words += \" \" + k\n            break\nprint(in_words)\n", "intent": "Text generation based on the maximum likelihood of the connected words.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"DIMSIM.csv\")\npd.options.display.max_columns = 999\n", "intent": "+ Duct Tape, or...\n+ WD-40\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = datasets.load_iris()\n", "intent": "+ Graph Decision Tree for Iris Dataset\n"}
{"snippet": "print(\"Extracting tf features for LDA...\")\ntf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n                                max_features=10000,\n                                stop_words='english')\nt0 = time()\ntf_vectorizer.fit(sentencelist)\nX = tf_vectorizer.transform(sentencelist)\nprint(\"done in %0.3fs.\" % (time() - t0))\n", "intent": "+ Here is the count vectorizer\n"}
{"snippet": "of_df = pd.read_csv(\"old-faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "import os\nimport pandas as pd\nimport matplotlib as mpl\nimport numpy as np\ndf = pd.read_csv('/Users/jamiew/GA-DataScience/GA-MyDSRepo/lesson-18/chipotle.tsv', sep='\\t')\n", "intent": "+ hint - examine how the values are separated \n+ What's the difference between a tsv and csv?\n"}
{"snippet": "import re\ndf['choice_description'] = df['choice_description'].fillna(\"[none]\").apply(lambda x: re.findall(r\"[\\w']+\",x))\n", "intent": "**Reid's Solution**\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"chipotle.tsv\", sep=\"\\t\")\n", "intent": "+ hint - examine how the values are separated \n+ What's the difference between a tsv and csv?\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"/Users/jamiew/GA-DataScience/GA-MyDSRepo/lesson-7/2008.csv\").fillna(0)\n", "intent": "http://stat-computing.org/dataexpo/2009/the-data.html\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nlfw_people = datasets.fetch_lfw_people(min_faces_per_person=50, \n                resize=0.4, data_home='../../data/faces')\nprint('%d objects, %d features, %d classes' % (lfw_people.data.shape[0],\n      lfw_people.data.shape[1], len(lfw_people.target_names)))\nprint('\\nPersons:')\nfor name in lfw_people.target_names:\n    print(name)\n", "intent": "PCA often serves as a preprocessing technique. Let's take a look at a face recognition task \n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nonehot = OneHotEncoder(categorical_features = \"all\", sparse=False) \nX =onehot.fit(df2[['UniqueCarrier', 'TailNum']])\nX =onehot.transform(df2[['UniqueCarrier', 'TailNum']])\n", "intent": "+ BUT YOU NEED TO SPECIFY WHICH VARIABLES ARE CATEGORICAL\n+ Could we use \"integer\" type as a proxy for that?\n"}
{"snippet": "from sklearn.preprocessing import  LabelEncoder\nle = LabelEncoder()\nle.fit(df['UniqueCarrier'].head(10))\nle.transform(df['UniqueCarrier'].head(10))\n", "intent": "+ THESE ARE NOT DUMMY VARIABLES\n+ Note the fit vs fit_transform function\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nonehot = OneHotEncoder(categorical_features = \"????\", sparse=True)\nX =onehot.fit_transform(df)\n", "intent": "+ BUT YOU NEED TO SPECIFY WHICH VARIABLES ARE CATEGORICAL\n+ Could we use \"integer\" type as a proxy for that?\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"/Users/jamiew/GA-DataScience/GA-MyDSRepo/lesson-7/2008.csv\").fillna(\"unk\")\n", "intent": "http://stat-computing.org/dataexpo/2009/the-data.html\n"}
{"snippet": "from sklearn.preprocessing import  LabelEncoder\nle = LabelEncoder()\nle.fit(df2['TailNum'])\nle.transform(df2['TailNum'])\n", "intent": "+ THESE ARE NOT DUMMY VARIABLES\n+ Note the fit vs fit_transform function\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nonehot = OneHotEncoder(categorical_features = is_cat_list, sparse=False)\nonehot.fit(df2)\nX =onehot.transform(df2)\n", "intent": "+ BUT YOU NEED TO SPECIFY WHICH VARIABLES ARE CATEGORICAL\n+ Could we use \"integer\" type as a proxy for that?\n"}
{"snippet": "from sklearn import datasets, neighbors, metrics\nimport pandas as pd\nimport seaborn as sns\niris = datasets.load_iris()\nirisdf = pd.DataFrame(iris.data, columns=iris.feature_names)\nirisdf['target'] = iris.target\nirisdf.head()\n", "intent": "+ Today we'll be working with the Iris dataset\n+ First we'll explore\n+ Then we'll build a terrible classifer\n+ Then we'll build a KNN classifer!\n"}
{"snippet": "data_100 = PCA(n_components=100).fit_transform(data.astype(float32) / 255)\nembeddings = TSNE(init=\"pca\", random_state=777, verbose=2).fit_transform(data_100)\nembeddings -= embeddings.min(axis=0)\nembeddings /= embeddings.max(axis=0)\n", "intent": "PCA them to 100 dimensions and run t-SNE. Normalize the results to [0, 1].\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\nX_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\ny_train = y_train.astype(np.int32)\n", "intent": "Let's try it on MNIST:\n"}
{"snippet": "print(\"Extracting features from the training dataset using a sparse vectorizer\")\nvectorizer = TfidfVectorizer(max_df=0.5, max_features=1000,\n                             min_df=2, stop_words='english')\nX = vectorizer.fit_transform(dataset.data)\nprint(\"n_samples: %d, n_features: %d\" % X.shape)\n", "intent": "**Build Tf-Idf features for texts**\n"}
{"snippet": "tracks.to_csv('../data/fma_tracks_with_genre.csv')\n", "intent": "To avoid having to collect the data everytime you restart the IPython kernel, save the DataFrame as a CSV file.\n"}
{"snippet": "tracks = pd.read_csv('../data/fma_tracks_with_genre.csv', index_col=0)\n", "intent": "You can now load it back with the following call instead of running the code in sections 1.1 to 1.3.\n"}
{"snippet": "str_dataName = './data/bdb_cleaned2.csv'\nbeers.to_csv(str_dataName)\n", "intent": "After replacing the NaN and discard the beers where the NaN can not be replaced, the data is saved in one csv file.\n"}
{"snippet": "NEIGHBORS = 80 \nRawCatweights = GetWeights(CatDistances,NEIGHBORS)\nfeatures = pd.DataFrame()\nfor df in newFrames:\n    features[df.columns] = df[df.columns]\n", "intent": "Here we use kNN method to sparsify the distance matrix. The our algortihm is extremly sensitive to the vlaue of this parameter\n"}
{"snippet": "all_movies_new =     pd.read_csv('350000-movies/AllMoviesDetailsCleaned.csv', sep=';', encoding='utf-8', low_memory=False,\n                                 error_bad_lines=False)\nall_movies_casting = pd.read_csv('350000-movies/AllMoviesCastingRaw.csv', sep=';', encoding='utf-8', low_memory=False,\n                                 error_bad_lines=False)\n", "intent": "Merge the information from two separate datasets\n"}
{"snippet": "metacritic_ratings = pd.read_csv('Saved_Datasets/metacritic_ratings.csv')\nlen(metacritic_ratings)\n", "intent": "Merge with csv created in Get_Metacritic_ratings\n"}
{"snippet": "df = pd.read_csv('Saved_Datasets/CleanDataset.csv')\n", "intent": "The final, merged dataset is then displayed below:\n"}
{"snippet": "df_ten = pd.read_csv('Saved_Datasets/NewFeaturesDataset.csv')\n", "intent": "These tenures were then added to the main dataset and can be seen in the column tenure of the following dataset:\n"}
{"snippet": "labels = preprocessing.LabelEncoder().fit_transform(df['success'])\nG_budg.set_coordinates(G_budg.U[:,1:3])\n", "intent": "From the graph of the eigenvalues, it can be seen that the first eigenvector explain 90% of the data.\n"}
{"snippet": "digits = datasets.load_digits()\nX = digits.data\ny = digits.target\n", "intent": "Let's look at the handwritten numbers dataset that we used before in the [3rd lesson](https://habrahabr.ru/company/ods/blog/322534/\n"}
{"snippet": "labels = preprocessing.LabelEncoder().fit_transform(df['success'])\nG_act_ten_spars.set_coordinates(G_act_ten_spars.U[:,1:3])\nG_act_ten_spars.plot_signal(labels, vertex_size=20)\n", "intent": "By looking at the eigenvalues, it can be seen than when the weight matrix is sparsed the first eigenvalue doesn't describe the data well. \n"}
{"snippet": "labels_reg = preprocessing.LabelEncoder().fit_transform(df['ROI']/2.64)\nG.plot_signal(labels_reg, vertex_size=20)\n", "intent": "In this second case, the graph is embedded on the first two eigenvectors with the their signals being the ROI.\n"}
{"snippet": "Genres = pd.DataFrame(genreArray, columns=Diffgenres)\nGenres.head(10)\n", "intent": "Observe the result in the dataframe\n"}
{"snippet": "NbGenre = pd.DataFrame(freqGenre, columns=Diffgenres)\nNbGenre\n", "intent": "Display of the number of times a genre appears in the dataframe\n"}
{"snippet": "ranking = np.linspace(1, len(Diffgenres)-1, num=len(Diffgenres)-1, endpoint=True, retstep=False, dtype=int)\nRankdf = pd.DataFrame(assosRank, index=ranking)\nRankdf\n", "intent": "Display of the ranking of the other genres with which each genre is most often associated\n"}
{"snippet": "street_names= {\"name\":df.name.unique()}\ndfp_features = pd.DataFrame(street_names)\ndfp_features.index.name='Progressive_index'\n", "intent": "To store all the features, we firstly initialize a \"classical\" Pandas Dataframe with the Street Names\n"}
{"snippet": "candidate_pca = decomposition.PCA().fit(can_answers)\ncandidate_features_pca = candidate_pca.transform(can_answers)\ncan_party_label = preprocessing.LabelEncoder().fit_transform(can_labels_true)\n", "intent": "Subsequently the actual PCA is performed on the candidate dataset\n"}
{"snippet": "exploitation = pd.read_csv('attacks.csv', index_col=0)\nexploitation.head()\n", "intent": "We will drop features that might not give insightful information for the next analysis.\n"}
{"snippet": "exploitation = pd.read_csv('attacks.csv', index_col=0)\n", "intent": "Again, we will drop features that might not give insightful information for the next analysis. \n"}
{"snippet": "df = pd.read_csv(os.path.join(PATH_TO_ALL_DATA, 'bank_train.csv'))\nlabels = pd.read_csv(os.path.join(PATH_TO_ALL_DATA,\n                                  'bank_train_target.csv'), \n                     header=None)\ndf.head()\n", "intent": "Let's explore the [UCI bank marketing dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing) where most of  features are categorial.\n"}
{"snippet": "boston = load_boston()\n", "intent": "Boston dataset is extremely common in machine learning experiments thus it is embedded in sklearn.\n"}
{"snippet": "X = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = boston.target\n", "intent": "Create pandas dataframe with objects in rows and features in columns\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "Let's split our data to train and test set in fraction of $\\frac{4}{1}$\n"}
{"snippet": "scaler = MinMaxScaler()\n", "intent": "Let's also do normalization to the range of $(0; 1)$ to make our data insensitive to the scale of features\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "Split into train and test set (with the same $\\text{random_state}$ which means we can compare results)\n"}
{"snippet": "boston = load_boston()\n", "intent": "Load Boston house prices dataset\n"}
{"snippet": "(X_train, y_train), (X_valid, y_valid) = mnist.load_data()\nprint(len(X_train), 'train samples')\nprint(len(X_valid), 'validation samples')\n", "intent": "Let's download MNIST dataset. There is a special function in Keras for that purpose (because MNIST is extremely popular)\n"}
{"snippet": "iris = load_iris()\n", "intent": "Iris dataset is extremely common in machine learning experiments thus it is embedded in sklearn.\n"}
{"snippet": "X = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n", "intent": "Create pandas dataframe with objects in rows and features in columns\n"}
{"snippet": "categorical_columns = df.columns[df.dtypes \n                                 == 'object'].union(['education'])\nfor column in categorical_columns:\n    df[column] = label_encoder.fit_transform(df[column])\ndf.head()\n", "intent": "Let's apply the transformation to other columns of type `object`.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X.values, label_binarize(y, classes=[0, 1, 2]), \n                                                    test_size=0.2, random_state=42)\n", "intent": "Split into train and test set\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint(len(X_train), \"train samples\")\nprint(len(X_test), \"test samples\")\n", "intent": "Let\"s download MNIST dataset. There is a special function in Keras for that purpose (because MNIST is extremely popular)\n"}
{"snippet": "import os\nprint(os.path.abspath(\".\"))\nmodel = model_from_json(open(\"mnist_cnn.json\").read())\n", "intent": "Load model architecture\n"}
{"snippet": "tsne = TSNE(perplexity=30, n_components=2, init=\"pca\", n_iter=3000)\nemb = tsne.fit_transform(reps)\n", "intent": "Fit our tSNE mapping to the `reps` data vectors\n"}
{"snippet": "tsne = TSNE(perplexity=30, n_components=2, init=\"pca\", n_iter=3000)\nemb = tsne.fit_transform(reps)\n", "intent": "Create a tSNE plot with a deep fully-connected layer. Fit our tSNE mapping to the `reps` data vectors\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(pima.iloc[:, :8], pima.iloc[:, 8], test_size=0.30, random_state=42)\n", "intent": "Create Training and testing split\n"}
{"snippet": "results = pd.DataFrame(columns=[\"Model\", \"Masking\", \"Embedding\", \"Accuracy train, %\", \"Accuracy test, %\", \"Train Time, s\"])\nresults\n", "intent": "Table of results for Reuters expetiments\n"}
{"snippet": "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nprint(len(X_train), 'train sequences')\nprint(len(X_valid), 'test sequences')\n", "intent": "Train/Test split in *stratified* (preserves the distribution across classes) manner in fraction of $4/1$.\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint(len(X_train), 'train samples')\nprint(len(X_test), 'test samples')\n", "intent": "Let's download MNIST dataset. There is a special function in Keras for that purpose (because MNIST is extremely popular)\n"}
{"snippet": "train_documents, test_documents, train_labels_mult, test_labels_mult = \\\n    train_test_split(all_documents, all_targets_mult, random_state=7)\nwith open(os.path.join(PATH_TO_ALL_DATA, \n                       '20news_train_mult.vw'), 'w') as vw_train_data:\n    for text, target in zip(train_documents, train_labels_mult):\n        vw_train_data.write(to_vw_format(text, target))\nwith open(os.path.join(PATH_TO_ALL_DATA, \n                       '20news_test_mult.vw'), 'w') as vw_test_data:\n    for text in test_documents:\n        vw_test_data.write(to_vw_format(text))\n", "intent": "**The data is the same, but we have changed the labels, train_labels_mult and test_labels_mult, into label vectors from 1 to 20.**\n"}
{"snippet": "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=21)\n", "intent": "Let's split our data into train and validation in fraction of $\\frac{1}{4}$.\n"}
{"snippet": "hist_full = pd.DataFrame(np.array([hist.history[\"loss\"], \n                                   hist.history[\"val_loss\"], \n                                   cb.ed_hist, \n                                   cb.norm_ed_hist]).T, \n                         columns=[\"Train CTC Loss\", \n                                  \"Test CTC Loss\", \n                                  \"Test Edit Distance\", \n                                  \"Test Normalized Edit Distance\"], \n                         index=np.arange(1, setting.epochs + 1))\n", "intent": "Let's also save the history of the training (both CTC loss and Edit Distance loss) for further analysis\n"}
{"snippet": "results = pd.DataFrame(columns=[\"RMSE\", \"MAE\", \"MAPE\"])\n", "intent": "We're going to use three types of models:\n* Recurrent (LSTM)\n* Multi-Layer Perceptron (Dense)\n* Convolutional\n"}
{"snippet": "data_train, data_test, ans_train, ans_test = train_test_split(data, ans, test_size=0.2)\n", "intent": "Splitting data into train and test in the fraction of $\\frac{4}{1}$\n"}
{"snippet": "boston = load_boston()\nX = boston.data\ny = boston.target\nX.shape, y.shape\n", "intent": "We will use the Boston house prices, available with scikit-learn, as an example.\n"}
{"snippet": "series = pd.read_csv('internet-traffic-data-in-bits-fr.csv', header = None, index_col=0, squeeze = True, \n                     parse_dates = True, dtype=\"float64\")\nprint(len(series))\nprint(series.head())\n", "intent": "We will analyze time series data downloaded from https://datamarket.com/data/set/232j. \nThese are internet traffic data collected on an hourly basis.\n"}
{"snippet": "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna('C')\n", "intent": "We can see that for 1st class median line is coming around fare $80 for embarked value 'C'. So we can replace NA values in Embarked column with 'C'\n"}
{"snippet": "cumul_sales = cumul_sales.set_index(\n    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(\n        level=-1).fillna(0)\ncumul_sales.columns = cumul_sales.columns.get_level_values(1)\ncumul_sales.shape\n", "intent": "Ah... they're creating a multi-task learning problem\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\ndf['Weight_mms'] = mms.fit_transform(df[['Weight']])\ndf['Height_mms'] = mms.fit_transform(df[['Height']])\ndf.describe().round(2)\n", "intent": "After scaling min = 0, max = 1\n$$x_{mms}(i) = \\frac{x(i) - min(X)}{max(X) - min(X)}$$\n"}
{"snippet": "ads = pd.read_csv('../../data/ads.csv', index_col=['Time'], parse_dates=['Time'])\ncurrency = pd.read_csv('../../data/currency.csv', index_col=['Time'], parse_dates=['Time'])\n", "intent": "As an example, let's look at real mobile game data. Specifically, we will look into ads watched per hour and in-game currency spend per day:\n"}
{"snippet": "train = pd.read_csv('r8-train-all-terms.txt', header=None, sep='\\t')\ntest = pd.read_csv('r8-test-all-terms.txt', header=None, sep='\\t')\ntrain.columns = ['label', 'content']\ntest.columns = ['label', 'content']\n", "intent": "https://www.cs.umb.edu/~smimarog/textmining/datasets/\n"}
{"snippet": "data = pd.read_csv('pima-indians-diabetes.csv')\ndisplay(data.info())\ndisplay(data.head())\ndisplay(data.describe())\ndisplay(data['Class'].value_counts())\n", "intent": "https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes\n"}
{"snippet": "from keras.preprocessing.sequence import pad_sequences\nmaxlen = 20\nX_train = pad_sequences(sentences_trained, padding='post', maxlen=maxlen)\ndisplay(X_train.shape)\ndisplay(sentences_trained[0])\ndisplay(X_train[0])\n", "intent": "text sequence has in most cases different length of words\n    -> pads the sequence of words with zeros\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=23)\nprint(\"Validation Set: {} samples\".format(len(X_validation)))\n", "intent": "validation_file = 'valid.p'\nwith open(validation_file, mode='rb') as f:\n    valid = pickle.load(f)\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(faces.data,\n        faces.target, random_state=0)\nprint(X_train.shape, X_test.shape)\n", "intent": "We'll perform a Support Vector classification of the images.  We'll\ndo a typical train-test split on the images to make this happen:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y_cls, random_state=0)\n", "intent": "<img src=\"./images/02-knns.png\" alt=\"Drawing\" style=\"width: 1500px;\"/>\n"}
{"snippet": "from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\nprint(\"Size of training set: {} size of test set: {}\".format(X_train.shape[0], X_test.shape[0]))\n", "intent": "Using *for* loops over the parameters of a model.\n"}
{"snippet": "import pandas as pd\nfrom IPython.display import display\nresults = pd.DataFrame(grid_search.cv_results_)\ndisplay(results.head())\n", "intent": "We can inspect the results of the cross-validated grid search.\n"}
{"snippet": "from sklearn.pipeline import Pipeline\npipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\npipe.fit(X_train, y_train)\nprint(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))\n", "intent": "You can create a *workflow* for training and classification after preprocessing.\n"}
{"snippet": "df = pd.read_csv('../../data/medium_posts.csv.zip', sep='\\t')\n", "intent": "We will predict the daily number of posts published on [Medium](https://medium.com/).\nFirst, we load our dataset.\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\nlabelencoder = LabelEncoder()\nX[:,3]\n", "intent": "hadndle categorical data: apply labelencoder and then onehotencoder\n"}
{"snippet": "cancer = datasets.load_breast_cancer()\nX = cancer.data\ny = cancer.target\nnp.random.seed(seed=133) \nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\n", "intent": "\"Model the probability that some class occurs\"\n"}
{"snippet": "pca = PCA(2)  \nXproj = pca.fit_transform(X)\nprint(X.shape)\nprint(Xproj.shape)\n", "intent": "Now, we can apply PCA to reduce the 64-dimensional data (8px by 8px) to a simple 2-dimensional form, and plot it visually:\n"}
{"snippet": "train_df = pd.read_csv('train.csv')\ntest_df = pd.read_csv('test.csv')\n", "intent": "Now we can read our data into pandas dataframes\n"}
{"snippet": "X_train, X_test, y_train, y_test = load_binarized_newsgroups_data(TfidfVectorizer(), \n                                                                  TruncatedSVD(n_components=100))\n", "intent": "__TfidfVectorizer + TruncatedSVD__\n"}
{"snippet": "X_train, X_test, y_train, y_test = load_binarized_newsgroups_data(TfidfVectorizer(),\n                                                                  SparseRandomProjection(n_components=100))\n", "intent": "__TfidfVectorizer + SparseRandomProjection__\n"}
{"snippet": "X_train, X_test, y_train, y_test = load_binarized_newsgroups_data(HashingVectorizer(n_features=100))\n", "intent": "__TfidfVectorizer + FeatureHasher__\n"}
{"snippet": "dv = DictVectorizer(dtype=np.uint8)\nrecords = df_train[['loc1', 'loc2', 'loc12', 'para1', 'dow']].to_dict(orient=\"records\")\ncategorical_train = dv.fit_transform(records)\ncategorical_train\n", "intent": "For categorical data, let's use One-Hot-Encoding\n"}
{"snippet": "scale = StandardScaler()\nnum_train = df_train[['para2', 'para3', 'para2', 'para4']].values\nnum_train = scale.fit_transform(num_train)\n", "intent": "We will also standardize the numerical variables to help the model converge and for easier coefficient interpretation\n"}
{"snippet": "cmp_df = make_comparison_dataframe(df, forecast)\ncmp_df.tail(n=3)\n", "intent": "Let's apply this function to our last forecast:\n"}
{"snippet": "categories = df_all.category_id.astype('str')\ncat_vectorizer = CountVectorizer(binary=1, dtype='uint8', min_df=10)\ncat_matrix = cat_vectorizer.fit_transform(categories)\n", "intent": "Also we can try using only category\n"}
{"snippet": "titles = df_all.title\ntitle_vectorizer = CountVectorizer(binary=1, dtype='uint8', min_df=10)\ntitle_matrix = title_vectorizer.fit_transform(titles)\n", "intent": "Using only this already gives 70% AUC.\nLet's try to use titles alone and then descriptions alone\n"}
{"snippet": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, Normalizer\ndf = pd.read_csv(\"https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv\")\ndf.head()\n", "intent": "1) Load in the dataset `https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv` into a pandas dataframe\n"}
{"snippet": "coef_df = pd.DataFrame({'coeffs':coef_optim, 'name':X.columns.values})\ncoef_df=coef_df.sort(['coeffs'])\ncoef_df.plot(x='name',y='coeffs',kind='bar')\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "rand_state = np.random.randint(0, 100)\nX_train, X_test, y_train, y_test = train_test_split(\n    scaled_X, y, test_size=0.2, random_state=rand_state)\nprint('Using:',orient,'orientations',pix_per_cell,\n    'pixels per cell and', cell_per_block,'cells per block')\nprint('Feature vector length:', len(X_train[0]))\n", "intent": "Then the nomalized data was randomly splitted into training and test sets as below.\n"}
{"snippet": "scaled_df = pd.DataFrame(scaled_features, columns=data.columns[:-1])\nscaled_df.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "fs = ExtraTreesClassifier() \nfs = fs.fit(train,target)\npd.DataFrame(train.columns, fs.feature_importances_).sort(ascending=False)\n", "intent": "test_new = fit.transform(test)\n"}
{"snippet": "x_train, x_test,y_train,y_test=train_test_split(\nx,y,test_size=0.2,random_state=43)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "cmp_df2 = make_comparison_dataframe(df, forecast2)\nfor err_name, err_value in calculate_forecast_errors(cmp_df2, prediction_size).items():\n    print(err_name, err_value)\n", "intent": "Here we will reuse our tools for making the comparison dataframe and calculating the errors:\n"}
{"snippet": "clean.to_csv('DATA/house-prices/train_cleaned.csv')\n", "intent": "To save the dataframe as csv, run this line of code.\n"}
{"snippet": "data1=pd.read_csv(\"https://serv.cusp.nyu.edu/~lw1474/ADS_Data/session07/data1.csv\")\ndata1.head()\n", "intent": "This is an artificial data set. It has five features and let's explore clustering models on this data set.\n"}
{"snippet": "print(\"total variance:{}\".format(np.sum(np.var(X,0))))\npca = PCA(2)\npca.fit(X)\nprint(\"variance explained via the first and second components:{}\\n\".format(pca.explained_variance_))\nprint(\"principal components:\\n{}\".format(pca.components_))\npca.explained_variance_ratio_\nnp.var(X,0)\n", "intent": "(in this particular example ~97.6% of the variance is preserved if we project down to the leading principal component)\n"}
{"snippet": "data = pd.read_csv('data/pokemon.csv')\ndata= data.set_index(\"\ndata.head()\n", "intent": "* Indexing using square brackets\n* Using column attribute and row label\n* Using loc accessor\n* Selecting only some columns\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"D:/Machine_Learning/MachineLearning-Python3/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "X_train, X_test, y_train, y_test = model_selection.train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\nclf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)   \n", "intent": "A single train/test split is made easy with the train_test_split function in the cross_validation library:\n"}
{"snippet": "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\ntfidf_train_x = tf_idf_vect.fit_transform(train_review_x['CleanedText'])\ntfidf_test_x = tf_idf_vect.transform(test_review_x['CleanedText'])\n", "intent": "<b>Naive Bayes with TF-IDF</b>\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = pd.read_csv(\"../data/hw2data.csv\")\ndata.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "from sklearn import tree\nimport graphviz \nX_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size = 0.2,random_state = 10)\nclf = tree.DecisionTreeClassifier(max_depth=5)\nclf.fit(X_train, y_train)\n", "intent": "7) Train a Decision Tree classifier with maximum depth 5 and plot the decision tree. How does performance compare?\n"}
{"snippet": "df_mean=pd.DataFrame(data=fancyimpute.SimpleFill().fit_transform(df.values), columns=df.columns, index=df.index)\ndf_iterative=pd.DataFrame(data=fancyimpute.IterativeImputer().fit_transform(df.values), columns=df.columns, index=df.index)\ndf_soft=pd.DataFrame(data=fancyimpute.SoftImpute().fit_transform(df.values), columns=df.columns, index=df.index)\n", "intent": "We will first construct the dataframe for the bottom three because for KNN we need to find the optimum value of the hyperparameter. \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nx_train_dataset1, x_test_dataset1, y_train_dataset1, y_test_dataset1 = train_test_split(x_1, y, test_size=0.3, random_state=42)\n", "intent": "Divide the training set and test set:\n"}
{"snippet": "from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nselect_feature = SelectKBest(chi2, k=5).fit(x_train_dataset1, y_train_dataset1)\n", "intent": "Next, the five best features are selected by Chi-squared stats to get the final training data and test data.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nraw_train_df, valid_df = train_test_split(mammo_df, \n                                   test_size = 0.25, \n                                   random_state = 2018,\n                                   stratify = mammo_df[['CLASS_ID']])\nprint('train', raw_train_df.shape[0], 'validation', valid_df.shape[0])\ntrain_df = raw_train_df.groupby(['CLASS']).apply(lambda x: x.sample(100, replace = True)\n                                                      ).reset_index(drop = True)\nprint('New Data Size:', train_df.shape[0], 'Old Size:', raw_train_df.shape[0])\ntrain_df[['CLASS_ID']].hist(figsize = (10, 5))\n", "intent": "We upsampling to balance the training data category.\n"}
{"snippet": "exog = pd.DataFrame(Exog)\nexog.columns = [\"f1\", \"f2\", \"f3\"]\nexog[\"ds\"] = pd.date_range(start='1/1/2018', periods=len(exog), freq='M')\n", "intent": "**Prepare dataframe of exogenous regressors with the same dates**\n"}
{"snippet": "with open('anna.txt', 'r') as f:\n    text=f.read()\nvocab = set(text)\nenum=enumerate(vocab)\nvocab_to_int = {c: i for i, c in enumerate(vocab)}\nint_to_vocab = dict(enumerate(vocab))\nchars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\nprint (enum)\n", "intent": "First we'll load the text file and convert it into integers for our network to use.\n"}
{"snippet": "data = df.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_data = scaler.fit_transform(data)\n", "intent": "Let's normalize our data for the machine learning algorithms to function better\nWe will use MinMaxScaler\n"}
{"snippet": "df = pd.read_csv('DJIA_table.csv', parse_dates=['Date'], index_col=['Date'] )\ndf = df.sort_index(ascending=True)\nprint(df.info())\n", "intent": "The downloaded data contain the daily price data between '2008-08-08' and '2016-07-01'\nfor ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n"}
{"snippet": "X, y = make_blobs(n_samples=10000, \n                  n_features=100, centers=100, random_state=0)\n", "intent": "X, y = make_blobs...\nfig = plt.figure(figsize=(10,8))\nax = fig.add_subplot(111)\nax.scatter...\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=30)\n", "intent": "- Create test/train splits of the original data set\nx_train, x_test, y_train, y_test = train_test_split(...)\n"}
{"snippet": "df = pd.read_csv('../../data/ads_hour.csv',index_col=['Date'], parse_dates=['Date'])\n", "intent": "scaler = MinMaxScaler(feature_range=(-1, 1))\nscaled = scaler.fit_transform(df.values)\n"}
{"snippet": "le = preprocessing.LabelEncoder()\nle.fit(authors)\nauthorship['Author_num'] = le.transform(authorship['Author']) \nprint(authorship['Author_num'])\n", "intent": "Use the LabelEncoder to encode Authors to integers...\n1. What does the LabelEncoder do for us? \n"}
{"snippet": "train_sc_df = pd.DataFrame(train_sc, columns=['Scaled'], index=train.index)\ntest_sc_df = pd.DataFrame(test_sc, columns=['Scaled'], index=test.index)\ntrain_sc_df.head()\n", "intent": "train_sc and test_sc are the scaled training and test dataset\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.85, random_state=1234)\n", "intent": "First of all, lets split our train data to train and validation sets:\n"}
{"snippet": "station_data = pd.read_csv(filepath_or_buffer='data/station_data.csv',\n                           sep=',', header=0,\n                           names=['Station', 'Name', 'Lat', 'Long', 'Dock_Count', 'City'])\nprint('dimensions of station data:',station_data.shape)\nprint('check if any data is missing:', \n      station_data.isnull().values.any())\nstation_data.head(5)\n", "intent": "The data set is ingested and feature names are relabeled to ease data manipulation.\n"}
{"snippet": "trip_data_agg['net_rate_previous_hour'] = \\\n        trip_data_agg.groupby(['Station', 'Date'])['net_rate']\\\n                .shift(1).fillna(0)\ntrip_data_agg['net_customers_previous_hour'] = \\\n        trip_data_agg.groupby(['Station', 'Date'])['net_customers']\\\n                .shift(1).fillna(0)\ntrip_data_agg['net_subscribers_previous_hour'] = \\\n        trip_data_agg.groupby(['Station', 'Date'])['net_subscribers']\\\n                .shift(1).fillna(0)\ntrip_data_agg.drop(['net_customers',  'net_subscribers'], inplace=True, axis=1)\n", "intent": "We now engineer time lagged features (i.e. by one hour).\n"}
{"snippet": "import reverse_geocoder as rg\nmta_data = pd.read_csv(filepath_or_buffer='data/MTA.pedvolumemodel_data.csv',\n                       sep=',')\nmta_data[['the_geom', 'MODEL6_VOL']].head(5)\n", "intent": "Having more time I would have matched these intersections with the bike stations.\n"}
{"snippet": "(trainX, trainy), (testX, testy) = mnist.load_data()\n", "intent": "Pixel scaling method is to calculate the mean pixel value across the entire training dataset, then subtract it from each image.\n"}
{"snippet": "ratings = pd.read_csv(PATH/'ratings.csv')\nratings.head()\n", "intent": "Table user/movie -> rating\n"}
{"snippet": "movies = pd.read_csv(PATH/'movies.csv')\nmovies.head()\n", "intent": "Table to get the titles of the movies.\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\n", "intent": "In this article we will use toy sklearn dataset **\"breast_cancer\"** (binary classification task). Lets load the dataset and take a look at the data.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ngender = le.fit_transform(dataset['Gender'])\ngeo = pd.get_dummies(dataset['Geography'],drop_first=True)\nX = X.drop(['Gender','Geography'],axis=1)\nX['Gender'] = gender\nX = pd.concat([X,geo],axis=1)\n", "intent": "Transformamos los features que son strings\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n", "intent": "Dividimos el dataset para entrenar y probar\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n", "intent": "Standarizamos los dataframes con los features\n"}
{"snippet": "data = pd.read_csv('College_Data.csv',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df = pd.read_csv('knn_project_data.csv')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "loan = pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nfor label in ['embarked','sex']:\n    titanic[label] = LabelEncoder().fit_transform(titanic[label])\n", "intent": "Three binomials, two categoricals, and four numerical features.\n"}
{"snippet": "pca = PCA(2) \npca.fit(X) \n", "intent": "Visualize the data using PCA (two dimensions). Color the points by the day of the week\n"}
{"snippet": "scaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df)\n", "intent": "We will use LogisticRegression as our base algorithm. Firstly, scaling the data. \n"}
{"snippet": "scaler_model = prep.MinMaxScaler()\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "import pandas as pd\ndata_frame = pd.read_csv('data/driving_log.csv', usecols=[0,1,2,3])\ndata_frame.describe(include='all')\n", "intent": "Now lets explore the Udacity's diriving_log.csv\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_feats,columns=df.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "coeff_df = pd.DataFrame(lm.coef_,X.columns,columns=['Coefficient'])\ncoeff_df\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": " X_train, X_test, y_train, y_test = train_test_split( yelp_class['text'], yelp_class['stars'], test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "PATH_TO_DATA = ('/Users/y.kashnitsky/Documents/Machine_learning/org_mlcourse_open/private/competitions/kaggle_alice/')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "train = train_df[['site%d' % i for i in range(1, 11)]].fillna(0).astype('int')\ntest = test_df[['site%d' % i for i in range(1, 11)]].fillna(0).astype('int')\ntrain_text = [' '.join([str(i) for i in j]) for j in train.values]\ntext_text = [' '.join([str(i) for i in j]) for j in test.values]\n", "intent": "Build Tf-Idf features based on sites. You can use `ngram_range`=(1, 3) and `max_features`=100000 or more\n"}
{"snippet": "scaler = StandardScaler()\ntrain_features_scaled = scaler.fit_transform(train_df[['year_month', 'morning', 'day', 'week_hour']])\ntest_features_scaled = scaler.transform(test_df[['year_month', 'morning', 'day', 'week_hour']])\n", "intent": "Scale this features and combine then with Tf-Idf based on sites (you'll need `scipy.sparse.hstack`)\n"}
{"snippet": "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), \n                                   max_features=100000)\n", "intent": "**Tf-Idf with article content.**\n"}
{"snippet": "X_sbs = sbs.fit_transform(df_scaled, y, custom_feature_names=columns)\n", "intent": "There is information about CV scoring on each iteration in log. The best quality we have with subset with 15 and from 17 to 24 features.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n", "intent": "- Now repeat the above after scaling your input data.\n"}
{"snippet": "X,Y = one_hot_data.iloc[:,1:].values, one_hot_data.admit.values\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\nprint(\"Number of training samples is\", len(X_train))\nprint(\"Number of testing samples is\", len(X_test))\nprint(X_train[:10])\nprint(\"\\n\")\nprint(X_test[:10])\n", "intent": "In order to test our algorithm, we'll split the data into a Training and a Testing set. The size of the testing set will be 10% of the total data.\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, Y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['over_200k', 'under_200k'],\n                        columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "std_scale = preprocessing.StandardScaler().fit(df[['SQFT', 'BDRMS', 'AGE']])\ndf_std = std_scale.transform(df[['SQFT', 'BDRMS', 'AGE']])\ndf_std\n", "intent": "[StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n"}
{"snippet": "df = pd.read_csv('march-machine-learning-mania-2016-v2/TourneySeeds.csv')\ndf\n", "intent": "Get season and seeds data\n"}
{"snippet": "df2 = pd.read_csv('march-machine-learning-mania-2016-v2/RegularSeasonDetailedResults.csv')\ndf2\n", "intent": "Get detailed season results (2003 onwards)\n"}
{"snippet": "seasons = pd.read_csv('march-machine-learning-mania-2016-v2/Seasons.csv')\nseasons = seasons[seasons['Season'] > 2002]\n", "intent": "Import season data (2003 and above)\n"}
{"snippet": "teams = pd.read_csv('march-machine-learning-mania-2016-v2/Teams.csv')\nteams.head()\n", "intent": "Import Teams data and replace team code's with team names\n"}
{"snippet": "games.to_csv('all_games.csv')\n", "intent": "Save games df to csv incase we lose the kernel\n"}
{"snippet": "pd.DataFrame(data = [pd.Series(dict_rfe),pd.Series(dict_pca), pd.Series(sbs_dict)], index = ['RFE', 'PCA', 'SBS']).T\n", "intent": "Comparing CV scores of all algorithms\n"}
{"snippet": "bin_NUMBER=3\ntemp2=pd.DataFrame(df[\"Salary\"].copy(),columns=[\"Salary\"])\nbins2,replacements2=create_bins(min(temp2[\"Salary\"]),max(temp2[\"Salary\"]),bin_NUMBER)\ntemp=pd.DataFrame(df[\"Salary\"].copy(),columns=[\"Salary\"])\nbins,replacements=create_bins(min(temp[\"Salary\"]),max(temp[\"Salary\"]),bin_NUMBER)\ntemp[\"Normalized Salary\"]=temp[\"Salary\"].apply(lambda x: salary_bin(bins,replacements,x))\ndf1=temp[\"Normalized Salary\"]\n", "intent": "The code below creates bin names for the salary data and classifies the correct output accordingly. This is used for the logistic regression.\n"}
{"snippet": "train_data = pd.read_csv(f'{PATH}train.csv')\ntest_data = pd.read_csv(f'{PATH}test.csv')\n", "intent": "For this one I am gonna use the title of the name; :) feature engineering.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"../DataScience-Python3/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\niris = load_iris()\nmodel = RandomForestClassifier(n_estimators=10)\nmodel = model.fit(iris['data'], iris['target'])\nmodel\n", "intent": "Let's train a simple random forest model and deploy it in the Predictive Service.\n<img src=\"images/left.png\"></img>\n"}
{"snippet": "X_train_pca=pd.DataFrame(pca.transform(X_train_stand),index=X_train.index)\n", "intent": "Transformation of data\n"}
{"snippet": "pd.DataFrame(\n    np.array([\n        sizes, \n        pca_rf_scores('scatt', model='rf'), \n        pca_rf_scores('resnet', model='rf'), \n        pca_rf_scores('vgg19', model='rf')\n    ]).transpose(),columns=['size of training data', 'scatt', 'resnet', 'vgg19'])\n", "intent": "The above table (1) shows the resulting test accuracies of different feature sets with the best number of components achieved above.\n"}
{"snippet": "pd.DataFrame(scores,\n             columns=['predicted day %s' % i for i in range(1,6)],\n             index=['with features of previous %s days' % i for i in range(6)])\n", "intent": "The AUC scores on each predicted day are as follows.\n"}
{"snippet": "pd.DataFrame(scores_chained,\n             columns=['predicted day %s' % i for i in range(1,6)],\n             index=['with features of previous %s days' % i for i in range(6)])\n", "intent": "The AUC scores on each predicted day are as follows.\n"}
{"snippet": "Enc = dfMCats.apply(lambda x: d[x.name].fit_transform(x))\nEnc.head()\n", "intent": "Label Encoder is a pretty quick solution\n"}
{"snippet": "dataset = load_iris()\nprint('Feature names:', dataset.feature_names)\nprint('Iris names:', dataset.target_names)\nprint('Number of instances:', dataset.data.shape[0])\n", "intent": "Now load the dataset and take a look at its properties:\n"}
{"snippet": "coef = model.coef_\nCoeff = pd.DataFrame(coef)\nCoeff\n", "intent": "See what is going on with the Coefficients\n"}
{"snippet": "A1 = b.loc[\"accepted\"]\nA0 = b.loc[\"rejected\"]\nO1 = A1/A0\nOdds['Odds'] = pd.DataFrame(O1)\nOdds\n", "intent": "Lets compare the rate of acceptance between prestige of 4 and of 1 using odds ratio. <br/>\nOdds Ratio is a simple statistic that tells a big story\n"}
{"snippet": "modelCVb = LogisticRegressionCV(cv=20)\nresults20 = modelCVb.fit(X ,y)\nCoefCVb = results20.coef_\nCoefCV20 = pd.DataFrame(CoefCVb)\nCoefCV20\n", "intent": "Cross Validation k-folds = 5 (above) <br/> <br/>\nCross Validatoin k-folds = 20 (below)\n"}
{"snippet": "boston_data = load_boston()\nx = boston_data['data']\ny = boston_data['target']\nmodel = LinearRegression()\nmodel.fit(x, y)\n", "intent": "Here we are importing the linear model from scikit-learn. We are also \n"}
{"snippet": "dataframe = pd.read_csv('housing_prices.csv') \ndataframe = dataframe.drop(['index', 'price', 'sq_price'], axis=1)\ndataframe = dataframe[0:10]\ndataframe\n", "intent": "Pandas to work with data as tables\nnumpy to use number matrices\nmatplotlib for creating and diplaying graphs\ntensorflow for machine learning\n"}
{"snippet": "import numpy as np\nfrom sklearn import preprocessing\nlabels = np.array([1,5,3,2,1,4,2,1,3])\nlb = preprocessing.LabelBinarizer()\nlb.fit(labels)\nlb.transform(labels)\n", "intent": "Transforming your labels into one-hot encoded vectors is pretty simple with scikit-learn using LabelBinarizer. Check it out below!\n"}
{"snippet": "rfe = RFE(LogisticRegression(), 2)\nrfe = rfe.fit(sensorReadings, lable)\n", "intent": "Create a classifier used to evaluate a subset of attributes. Also create the RFE model and select 2 attributes.\n"}
{"snippet": "def prepare_images(path, factor):\n    for file in os.listdir(path):\n        img = cv2.imread(path + '/' + file)\n        h, w, c = img.shape\n        new_height = h / factor\n        new_width = w / factor\n        img = cv2.resize(img, (new_width, new_height), interpolation = cv2.INTER_LINEAR)\n        img = cv2.resize(img, (w, h), interpolation = cv2.INTER_LINEAR)\n        print('Saving {}'.format(file))\n        cv2.imwrite('images/{}'.format(file), img)\n", "intent": "http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ntrain = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', subset='train', shuffle=True, random_state=23)\ntest = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', subset='test', shuffle=True, random_state=23)\n", "intent": "Before we begin, we must repopulate this Notebook with both the necessary\ndata and supporting functions.\n-----\n"}
{"snippet": "target = np.array([dataset.target_names[x] for x in dataset.target]).reshape(-1, 1)\ndf_full = pd.DataFrame(\n    np.concatenate([dataset.data, target], axis=1),\n    columns=feature_names + ['target'])\ndf_full[feature_names] = df_full[feature_names].astype(float)\ndf_full.sample(5, random_state=RANDOM_SEED)\n", "intent": "Load the dataset into DataFrame:\n"}
{"snippet": "X_dv, Y_dv = preproc.load_data(DEV_FILE)\n", "intent": "- Loading Dev data for english:\n"}
{"snippet": "reload(preproc);\nX_tr_nr, Y_tr_nr = preproc.load_data(NR_TRAIN_FILE)\n", "intent": "- Loading training data for norwegian:\n"}
{"snippet": "X_dv_nr, Y_dv_nr = preproc.load_data(NR_DEV_FILE)\n", "intent": "- Loading dev data for norwegian:\n"}
{"snippet": "X_te_nr, Y_te_nr = preproc.load_data(NR_TEST_FILE_HIDDEN)\n", "intent": "- Loading test data for norwegian:\n"}
{"snippet": "df_train = pd.read_csv('lyrics-train.csv')\n", "intent": "Read the data into a dataframe\n"}
{"snippet": "y = data['class']\nx = data.drop('class', axis=1)\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n", "intent": "Now I want to set my data set to train and test so that I can use some classification algorithms and test how good/bad I am doing.\n"}
{"snippet": "trainSet['ApplicantIncome'].fillna(trainSet['ApplicantIncome'].mean(), inplace = True)\ntrainSet['CoapplicantIncome'].fillna(trainSet['CoapplicantIncome'].mean(), inplace = True)\ntrainSet['LoanAmount'].fillna(trainSet['LoanAmount'].mean(), inplace = True)\ntrainSet['Loan_Amount_Term'].fillna(trainSet['Loan_Amount_Term'].mean(), inplace = True)\n", "intent": "Now I can replace the null values with the mean values.\n"}
{"snippet": "y = trainSet['Loan_Status']\ny=y.astype('int') \nX = trainSet.drop('Loan_Status', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n", "intent": "Now it is time to try the machine learning algorithm to train the model.\n"}
{"snippet": "y = trainSet['Loan_Status']\ny=y.astype('int') \nX = trainSet.drop('Loan_Status', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n", "intent": "OK finally! It sort of look ok now. Even though I am not happy with negative values but let's see what happens.\n"}
{"snippet": "df_train, df_test, y_train, y_test = train_test_split(df_full.drop('target', axis=1), df_full.target, test_size=0.4)\ndf_train.shape, y_train.shape, df_test.shape, y_test.shape\n", "intent": "Now, let's split our data to train and test, having the holdout set of 40%:\n"}
{"snippet": "products = pd.read_csv('data/amazon_baby.csv')\n", "intent": "We will use a dataset consisting of baby product reviews on Amazon.com.\n"}
{"snippet": "products['review'] = products['review'].fillna('')\n", "intent": "Replace nan with empty strings\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nmodel = LogisticRegression(penalty = 'l2', C=0.1)\nfrom patsy import dmatrix\nX = dmatrix(\"city + is_senior + is_manager + 0\", data=salary_data) \nX_scaled = scaler.fit_transform(X)\ny = salary_data['HighSalary']\nmodel.fit(X_scaled, y)\n", "intent": "- You can either create the dummy features manually or use the `dmatrix` function from `patsy`\n- Remember to scale the feature variables as well!\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\ndata.head()\n", "intent": "We are interested to find out whether there are linear relationships between advertising dollars spent and the sales of a product\n"}
{"snippet": "data = pd.read_csv('../../assets/datasets/train.tsv', sep='\\t', na_values='?')\ndata['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\ndata['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n", "intent": "- These are websites that always relevant like recipies or reviews (as opposed to current events)\n- Look at some examples\n"}
{"snippet": "sac = pd.read_csv('../../data/Sacramentorealestatetransactions.csv')\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "import numpy as np\nfrom sklearn import svm, grid_search, datasets\nfrom sklearn.neighbors import KNeighborsClassifier\niris_data = datasets.load_iris()\n", "intent": "While GridSearch can work with most sklearn models, we will try it out on KNN to start with iris dataset.\n"}
{"snippet": "union = make_union(age_pipe,\n                   one_hot_pipe,\n                   gender_pipe,\n                   fare_pipe)\nunion.fit_transform(df.head())\n", "intent": "Use the `make_union` function from the `sklearn.pipeline` modeule to combine all the pipes you have created.\n"}
{"snippet": "house = datasets.load_boston()\nbcancer = datasets.load_breast_cancer()\n", "intent": "We're going to be working with the Boston and breast cancer data sets, so let's go ahead and load those from sklearn's datasets library\n"}
{"snippet": "pd.DataFrame([{\n    'sepal_length': 4.0,\n    'sepal_width': 5.0,\n    'petal_length': 1.0,\n    'petal_width': 0.5\n}])\n", "intent": "Hmmm, it's not _setosa_, so what happened? Let's check how the DataFrame is constructed:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "Construct, fit, and predict with a linear model on both datasets\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(analytic_df, y)\nx_train.shape, x_test.shape\n", "intent": "Looks like there might be overfitting! Let's redo this but *first* make a training and a test set!\n"}
{"snippet": "data = pd.read_csv('~/Desktop/DSI-SF-2/datasets/401_k_abadie/401ksubs.csv') \ndata.head(2)\n", "intent": "1. Read the 401ksubs.csv data into Pandas.\n2. Explore the data by sorting, plotting, group_by, and any other ideas/techniques you have been using.\n"}
{"snippet": "wine_df = pd.read_json(api_response.text)\nwine_df.head(2)\n", "intent": "This sometimes works, but the data may need adjusting\n"}
{"snippet": "wine = pd.read_csv('../../assets/datasets/wine_v.csv')\n", "intent": "And read in our data:\n"}
{"snippet": "x_standard = StandardScaler().fit_transform(x)\n", "intent": "Since we don't know what units our data was measured in, let's standardize \"x\"\n"}
{"snippet": "df_new = pd.DataFrame(data=df, columns=['workclass_num', 'education-num', 'hours-per-week', 'income_num'])\ndf_new2 = pd.DataFrame(data=df, columns=['workclass_num', 'education-num'])\nprint df_new.head(n=5)\nprint len(df_new)\n", "intent": "Create a New Dataframe with just numerical data\n"}
{"snippet": "df = pd.DataFrame(data=adult, columns=['workclass', 'education-num', 'hours-per-week', 'income'])\ndf.head(n=5)\n", "intent": "Convert the data to a Pandas dataframe to work with the data:\n"}
{"snippet": "df_new = pd.DataFrame(data=df, columns=['workclass_num', 'education-num', 'hours-per-week', 'income_num'])\ndf_new2 = pd.DataFrame(data=df, columns=['workclass_num', 'education-num'])\ndf_new.head(n=5)\n", "intent": "Create a New Dataframe with just numerical data for the analysis\n"}
{"snippet": "scaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_train, X_val,y_train,y_val = train_test_split(X,y, test_size=0.2)\n", "intent": "Linear models like scaled input\n"}
{"snippet": "PCA_set = PCA(n_components=5)\nY = PCA_set.fit_transform(Xvote)\n", "intent": "Now, conduct a PCA using scikit learn\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "df = pd.read_csv(\"assets/datasets/iris.csv\")\ndf.head(5)\n", "intent": "Let's do some clustering with the iris dataset.\n"}
{"snippet": "votes = pd.read_csv('votes.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "PCA_set = PCA(n_components=5)\nY = PCA.fit_transform(xStand)\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "airports_pca = PCA(n_components=2)\nY = pcask2.fit_transform(xStand)\n", "intent": "Finally, conduct the PCA - use the results about to guide your selection of \"n\" componants\n"}
{"snippet": "Ydf = pd.DataFrame(Y)\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = mnist.load_data()\nn_train, height, width = X_train.shape\nn_test, _, _ = X_test.shape\n", "intent": "First, we can download the data with Keras. \n"}
{"snippet": "def make_image(xs):\n    a = np.fromstring(xs, sep=' ', dtype=np.float).reshape(96, 96)\n    return a / 256.0\ndf = pd.read_csv('../data/training.csv', converters={'Image': make_image})\ndf['Image'].head()\n", "intent": "Load data from csv files.\nImage is provided as space separated values in 'Image' column.\nTODO:\n  * Scala output labels into (-1, 1) range)\n"}
{"snippet": "def make_image(xs):\n    a = np.fromstring(xs, sep=' ', dtype=np.float).reshape(96, 96)\n    return a / 256.0\ndf = pd.read_csv('../data/training.csv', converters={'Image': make_image})\ndf['Image'].head()\n", "intent": "Load data from csv files.\nImage is provided as space separated values in 'Image' column.\n"}
{"snippet": "word_tsne = TSNE().fit_transform(word_vectors )\n", "intent": "Draw a graph as usual\n"}
{"snippet": "data = load_breast_cancer()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = pd.Series(data.target)\nX = X.drop(['worst concave points', 'mean concave points', 'worst perimeter', 'worst radius', 'worst area', 'mean concavity'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=.33,\n                                                    random_state=0)\n", "intent": "Let's investigate the accuracy of a random forests compared with a single decision tree using the boston dataset. \n"}
{"snippet": "insects = pd.read_csv('./data/insects.csv', sep='\\t')\ninsects.head()\n", "intent": "The data set we will use to demonstrate linear regression contains measurements on a single species of insect captured on two continents.\n"}
{"snippet": "sample_train = pd.read_csv(path + \"train_sample.csv\")\ndisplay(sample_train.head(n=5))\n", "intent": "----\nExplore the sample training data , which are 100,000 randomly-selected rows of training data.\n"}
{"snippet": "def make_some_noise(seed):\n    np.random.seed(seed)\n    return pd.DataFrame(np.random.randint(0, 7, size=(3, 1)))\nX_noise = make_some_noise(1)\nX_train_noisy = X_train + X_noise\nX_test_noisy = X_test + X_noise\n_ = lin_reg(X_train_noisy, y_train, X_test_noisy, y_test)\n", "intent": "Let's try another example that adds a little noise to our input values so we can see how this affects the model.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\niris_train, iris_test = train_test_split(iris)\nX_train = iris_train.drop(['sepal_length'], axis=1)\nX_test = iris_test.drop(['sepal_length'], axis=1)\ny_train = iris_train[['sepal_length']]\ny_test = iris_test[['sepal_length']]\ntry:\n    regr = lin_reg(X_train, y_train, X_test, y_test, graph=False, normalize=True)\nexcept ValueError as e:\n    print(\"ValueError :\", e)\n", "intent": "If we try to use linear regression to predict the the sepal length with our current features, we will get an error.\n"}
{"snippet": "ufo = pd.read_table('http://bit.ly/uforeports', sep=',')\n", "intent": "Let's read some data into a table and manipulate that data\n"}
{"snippet": "from pandas import DataFrame\ndata = DataFrame([[1., 6.5, 3.], [1., NA, NA], [NA, NA, NA], [NA, 6.5, 3.]])\ncleaned = data.dropna()\ndata\n", "intent": "In the next examples we will se the same application of filtering in DataFrame objects. By default _dropna_ erases all the rows that have NaNs.\n"}
{"snippet": "import numpy as np\ndf = DataFrame(np.random.rand(7,3))\ndf\n", "intent": "If we only want to keep a certain number of observations, remember that we can select them with the method iloc from the object DataFrame\n"}
{"snippet": "df.fillna({1:0.5, 2:0}) \n", "intent": "In the same way we can use a dictionary in order to define more specifically the data to replace in the NaNs\n"}
{"snippet": "from sklearn.datasets import load_diabetes\ndiabetes = load_diabetes()\nn = diabetes.data.shape[0]\ndata = diabetes.data\ntargets = diabetes.target\n", "intent": "We will demonstrate and compare different algorithms on diabetes dataset from sklearn.datasets. Let's load it.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = pd.DataFrame(np.random.randn(1000, 4))\ndata.describe() \n", "intent": "The next method allows us to understand how the data in a DataFrame is distributed.\n"}
{"snippet": "X = [[1., -1., 2.],\n    [2., 0., 0.],\n    [0., 1., -1.]]\nnormalizer = preprocessing.Normalizer().fit(X)\nnormalizer\n", "intent": "Remember the idea is to make a normalizer in order to use in other data that we didn't use during the fit.\n"}
{"snippet": "DataFrame(data, columns=['year','pop', 'state'])\n", "intent": "You can sort the during the creation process of the DataFrame\n"}
{"snippet": "data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],\n       'year': [2000, 2001, 2002, 2001, 2002, 2003],\n       'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\nframe2 =  DataFrame(data, columns=['year','state', 'pop',  'debt'],\n                   index=['one', 'two', 'three', 'four', 'five', 'six'])\nframe2\n", "intent": "If we didn't declare the values in a column we will have NaN registers.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n                 header=None, names=[\"sepal_length\",\"sepal_width\", \"petal_length\",\"petal_width\",\"class\"])\ndf_park = pd.read_csv('../datasets/parks.csv', index_col=['Park Code'], encoding='utf-8')\n", "intent": "The next code lines will download a dataset from the repository of the University of California and save it in a Dataframe\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n                 header=None, names=['sepal_length',\n                                     'sepal_width', \n                                     'petal_length',\n                                     'petal_width',\n                                    'class'])\ndf.tail()\n", "intent": "Pandas is a library to load the Iris dataset from a public repository into a DataFrame Object \n"}
{"snippet": "def build_q_table(n_states, actions):\n    table = pd.DataFrame(\n    np.zeros((n_states, len(actions))), \n    columns = np.arange(0,9), \n    )\n    return table\nbuild_q_table(N_STATES, ACTIONS)\n", "intent": "Second, we are going to define the Q table to this problem\n"}
{"snippet": "pd.DataFrame(zip(features.columns, np.transpose(model_lr.coef_)))\n", "intent": "**2) Which features are predictive for this logistic regression? Explain your thinking. Do not simply cite model statistics.**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncont_data = data.drop(['Channel','Region'], axis=1)\nX = scaler.fit_transform(cont_data)\nkm = KMeans(3)\nkm.fit(X)\n", "intent": "**3) Using ONLY the continuous features in the dataset, apply the K-means algorithm to find clusters in the data.**\n"}
{"snippet": "gs_results_df=pd.DataFrame(np.transpose([-gs.cv_results_['mean_test_score'],\n                                         gs.cv_results_['param_learning_rate'].data,\n                                         gs.cv_results_['param_max_depth'].data,\n                                         gs.cv_results_['param_n_estimators'].data]),\n                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\ngs_results_df.plot(subplots=True,figsize=(10, 10))\n", "intent": "We have managed to improve the results. But spent a lot of time on it. Let's look how our parameters have been changing from iteration to iteration:\n"}
{"snippet": "fitter = StandardScaler()\nfit_train = fitter.fit_transform(X_train)\n", "intent": "The data is rather normalized already but to review the process:\n"}
{"snippet": "masked_epi = masker.inverse_transform(samples)\nmax_zscores = image.math_img(\"np.abs(img).max(axis=3)\", img=masked_epi)\nplotting.plot_stat_map(max_zscores, bg_img=anat, dim=-.5)\n", "intent": "To recover the original data shape (giving us a masked and z-scored BOLD series), we simply use the masker's inverse transform:\n"}
{"snippet": "coef_vol = masker.inverse_transform(svc.coef_)\nplotting.plot_stat_map(coef_vol, bg_img=anat, dim=-.5)\n", "intent": "Since we have a value for each voxel, we can simply map this back to the volume using our `masker`, and visualize the weights.\n"}
{"snippet": "departments_df = pd.DataFrame(artworks['Department'].value_counts())\ndepartments_df.head(10)\n", "intent": "Let's see how art works split by department.\n"}
{"snippet": "titles = data['Title'].fillna('')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(max_features = 250000, ngram_range=(1,2), stop_words='english')\nvectorizer.fit(titles)\nX = vectorizer.transform(titles).toarray()\nX\n", "intent": "Not so great.  Let's try it instead with TfIDfVectorizer.  \n"}
{"snippet": "data.diameter_bin = data.diameter_bin.fillna(0)\ndata.height_bin = data.height_bin.fillna(0)\ndata.length_bin = data.length_bin.fillna(0)\ndata.width_bin = data.width_bin.fillna(0)\ndata.depth_bin = data.depth_bin.fillna(0)\ndata.circumference_bin = data.circumference_bin.fillna(0)\n", "intent": "Now we fill in the Nan values with 0 to indicate that they are distinct from the other data.\n"}
{"snippet": "pd.DataFrame(zip(X.columns, np.transpose(model.coef_)))\n", "intent": "Okay about 70 per cent.  Can we obtain more insights from examining the coefficients? Let's put them in a dataframe and look further.\n"}
{"snippet": "of_df = pd.read_csv(\"faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    contains_org = False\n    contains_person = False\n    for word in parsed:\n        contains_org |= word.ent_type_ == 'ORG'\n        contains_person |= word.ent_type_ == 'PERSON'\n    return contains_org, contains_person\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title']].head()\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "tpe_results=np.array([[x['result']['loss'],\n                      x['misc']['vals']['learning_rate'][0],\n                      x['misc']['vals']['max_depth'][0],\n                      x['misc']['vals']['n_estimators'][0]] for x in trials.trials])\ntpe_results_df=pd.DataFrame(tpe_results,\n                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\ntpe_results_df.plot(subplots=True,figsize=(10, 10))\n", "intent": "We have managed to find even better solution comparing to the RandomizedSearch.\nLet's look at the visualization of the process\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    contains_org = \n    contains_person = \n    return \ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title']].head()\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    contains_org |= any([word.ent_type_ == 'ORG' for word in parsed])\n    contains_person |= any([word.ent_type_ == 'PERSON' for word in parsed])\n    return contains_org and contains_person\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title']].head()\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "yelp_best_worst = yelp[(yelp['stars']==5)| (yelp['stars']==1)]\nX = yelp_best_worst.text\ny = yelp_best_worst.stars\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n", "intent": "- Select only 5-star and 1-star reviews.\n- The text will be the features, the stars will be the target.\n- Create a train-test split.\n"}
{"snippet": "X = rt.quote.values\ny = rt['is_fresh']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n", "intent": "---\nIt is up to you what ngram range you want to select. **Make sure that `binary=True`**\n"}
{"snippet": "s_df = pd.DataFrame(sanders)\ns_df.head()\n", "intent": "> *Hint: this is as easy as passing it to the DataFrame constructor!*\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',\n                     header=None)\ncolumns = ['Class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity',  'Magnesium', 'Total phenols', \n        'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n        'OD280/OD315 of diluted wines', 'Proline']\ndataset.columns = columns\nprint('Shape: ', dataset.shape, end='\\n\\n')\nprint(dataset.head(10))\n", "intent": "Abstract: Using chemical analysis determine the origin of wines   \nSource: https://archive.ics.uci.edu/ml/datasets/Wine\n"}
{"snippet": "class Preprocess:\n    def __init__(self, df, dim):\n        self.__df = df\n        self.__norm = Normalizer(self.__df.values)\n        self.__red = PCA(self.__norm(self.__df.values), dim)\n    def __call__(self, iid):\n        return self.__red(self.__norm(self.__df.loc[iid, :].values))\n", "intent": "Preprocess, PCA and Normalizer instances containts values that are related to the model, they have to be saved for future use in production !\n"}
{"snippet": "scaler = MinMaxScaler((-1,1))\nscaler.fit(x)\nscaled_x = scaler.transform(x)\nprint(scaled_x[0:10])\n", "intent": "Let's assume we want to rescale the values of this numpy array to the range from -1 to 1.\n"}
{"snippet": "import numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nvalues = ['Warm', 'Cold', 'Warm', 'Hot', 'Hot', 'Cold']\nlabenc = LabelEncoder()\nint_encoded = labenc.fit_transform(values)\nprint(int_encoded)\n", "intent": "LabelEncoder will take an input of labels and **encode these as sequential integers**. It uses the same syntax as the scalers.\n"}
{"snippet": "scores_df=pd.DataFrame(index=range(n_iter))\nscores_df['Grid Search']=gs_results_df['score'].cummin()\nscores_df['Random Search']=rs_results_df['score'].cummin()\nscores_df['TPE']=tpe_results_df['score'].cummin()\nscores_df['Annealing']=sa_results_df['score'].cummin()\nax = scores_df.plot()\nax.set_xlabel(\"number_of_iterations\")\nax.set_ylabel(\"best_cumulative_score\")\n", "intent": "Let's plot best_cumulative_score vs. number_of_iterations for all approaches:\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "We load the dataset from the datasets module in sklearn.\n"}
{"snippet": "boston = datasets.load_boston()\n", "intent": "We load the dataset from the datasets module in sklearn.\n"}
{"snippet": "ads = pd.read_csv('data/ads.csv', index_col = 0)\n", "intent": "- Review Mutlivariate Linear Regression\n- Coding Qualitative Variables\n- Polynomial Regression\n"}
{"snippet": "std_scaled = StandardScaler()\ncol = X.columns\nX_scaled = std_scaled.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled,columns=cols)\n", "intent": "Now let's use a `StandardScaler` to scale the features.\n"}
{"snippet": "X_train,X_test,y_train,y_test = train_test_split(X,y)\n", "intent": "Oddly enough it seems to have made it worse. Let's revert back to our original non-scaled variables.\n"}
{"snippet": "vect = CountVectorizer(ngram_range=(2,2))\n", "intent": "handles phrases, not just words\n"}
{"snippet": "X = ks_sample[['duration','backers','usd_goal_real']].join(cat_var)\ny = ks_sample.state_simple\nX_train,X_test,y_train,y_test = train_test_split(X,y)\n", "intent": "Let's redefine the variables.\n"}
{"snippet": "data_path_test = os.path.join(os.getcwd(),'datasets','spambase_test.csv')\nspambase_test = pd.read_csv(data_path_test, delimiter=',')\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure\n"}
{"snippet": "file_A_path = os.path.join(os.getcwd(),'datasets','train_20news_partA.csv')\nfile_B_path = os.path.join(os.getcwd(),'datasets','train_20news_partB.csv')\nnews_A = pd.read_csv(file_A_path)\nnews_B = pd.read_csv(file_B_path)\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "def generate_leaderboards(values):\n    df = pd.DataFrame(values, columns=['target'])\n    public_lb, private_pb = train_test_split(df, test_size=0.3, shuffle=False)\n    return public_lb, private_pb\n", "intent": "Now, we are going to create some functions, which will help us in testing exploits.\n"}
{"snippet": "X,y = make_blobs(n_samples=200,centers=3,cluster_std=0.60,random_state=0)\n", "intent": "Let us generate and plot the dataset\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimputer = Imputer()\nX = imputer.fit_transform(X)\n", "intent": "We need to handle missing values.\n"}
{"snippet": "admission = pd.read_csv('https://serv.cusp.nyu.edu/~cq299/ADS2016/Data/admission.csv')  \nadmission.head()\n", "intent": "Example - learning the probability of school admission based on two exams\n"}
{"snippet": "data1=pd.read_csv(\"https://serv.cusp.nyu.edu/~lw1474/ADS_Data/session07/data1.csv\")\nprint(data1.shape)\ndata1.head()\n", "intent": "This is an artificial data set. It has five features and let's explore clustering models on this data set.\n"}
{"snippet": "ads = pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "messages = pandas.read_csv('./data/SMSSpamCollection', sep='\\t', quoting=csv.QUOTE_NONE,\n                           names=[\"label\", \"message\"])\nprint(messages)\n", "intent": "Instead of parsing TSV (or CSV, or Excel...) files by hand, we can use Python's `pandas` library to do the work for us:\n"}
{"snippet": "bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])\nprint len(bow_transformer.vocabulary_)\n", "intent": "Each vector has as many dimensions as there are unique words in the SMS corpus:\n"}
{"snippet": "loc = os.path.join(os.getcwd(),'datasets','spambase_test.csv')\nspambase_test = pd.read_csv(loc)\nspambase_test\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure\n"}
{"snippet": "locA = os.path.join(os.getcwd(),'datasets','train_20news_partA.csv')\nlocB = os.path.join(os.getcwd(),'datasets','train_20news_partB.csv')\nnews_A = pd.read_csv(locA)\nnews_B = pd.read_csv(locB)\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "train_target = pd.read_csv(os.path.join(PATH_TO_RAW_DATA, 'train_log1p_recommends.csv'), \n                           index_col='id')\n", "intent": "Read targets from file.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train[:, 3:5] = sc_X.fit_transform(X_train[:, 3:5])\nX_test[:, 3:5] = sc_X.transform(X_test[:, 3:5])\nprint('X_train: ')\nprint(X_train)\nprint()\nprint('X_test: ')\nprint(X_test)\n", "intent": "Normalization : $$x' = \\frac{x - mean(x)}{max(x) - min(x)}$$\n"}
{"snippet": "X = pd.DataFrame(scaled_features,columns=df.columns[:-1])\nX.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "dataset = pd.read_csv('cal_housing_clean.csv')\n", "intent": "** Import the cal_housing_clean.csv file with pandas. Separate it into a training (70%) and testing set(30%).**\n"}
{"snippet": "X, _ = datasets.make_circles(n_samples=200, factor=.1, noise=.05)\nscatter(X[:, 0], X[:, 1]);\n", "intent": "Test the different clustering approaches with a \"circles\" dataset.\n"}
{"snippet": "from sklearn import lda\nlda = decomposition.LatentDirichletAllocation(n_topics=6)\nlda.fit(tfidf)\nW = lda.transform(tfidf)\nH = lda.components_\n", "intent": "Compare the results of Nonnegative Matrix Factorization (NMF) with Latent Dirichlet Allocation (LDA).\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n", "intent": "Prepare the data with Features and transformations.\n"}
{"snippet": "(U, E, V) = principal_component_analysis(XX, 2)\npca = PCA(n_components=2)\npca.fit(XX)\nif np.transpose(V).shape == pca.components_.shape:\n    print(\"Try transposing your eigenvector output\")\nassert(np.allclose([1, 1], np.abs(np.diagonal(np.inner(V, pca.components_)))))\nprint(\"Passed\")\n", "intent": "Check that your function outputs a solution that agrees with the canonical python implementation of PCA\n"}
{"snippet": "text_clf = Pipeline([('vect', CountVectorizer()),\n                      ('tfidf', TfidfTransformer()),\n                     ('clf', MultinomialNB()),\n])\ntext_clf.fit(train.summary, train.overall)\n", "intent": "choosing summary, not reviewText as input may improve the model.\n"}
{"snippet": "df['v2'] = df['v2'] + df['Unnamed: 2'].fillna('') + df['Unnamed: 3'].fillna('') + df['Unnamed: 4'].fillna('')\n", "intent": "* spam - 1\n* ham - 0\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import classification_report\ndata=pd.read_csv(\"user_to_1st_time_investor.csv\")\n", "intent": "<h1>Prepare data</h1>\n"}
{"snippet": "from sklearn.ensemble import ExtraTreesClassifier\netree = ExtraTreesClassifier()\netree.fit(X, y)\nimportant_feature_df = pd.DataFrame()\nimportant_feature_df['score'] = etree.feature_importances_\nimportant_feature_df['indexes'] = X.columns\nimportant_feature_df.sort_values(by ='score',ascending=False).head(12)\n", "intent": "<h1> Feature Importance Seclection </h1>\n"}
{"snippet": "finalDF = pd.DataFrame({'vals':data.iloc[:,2]}) \nfinalDF =finalDF.join(weekday)\nfinalDF = finalDF.join(hour)\nfinalDF = finalDF.join(month)\nfinalDF.head()\n", "intent": "Then we reconstruct the dataframe\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LinearRegression\ny_data = finalDF['vals']\nx_data = finalDF.drop('vals', 1)\nxtrain, xtest, ytrain, ytest = train_test_split(x_data, y_data)\n", "intent": "Our first model is a simple linear regression. This is seen to be a poor choice, with the reasons why being explored below.\n"}
{"snippet": "x_ETtrain, x_ETtest, y_ETtrain, y_ETtest = train_test_split(pca_X, y_data)\nET.fit(x_ETtrain, y_ETtrain)\n", "intent": "Let's train another ExtraTressRegressor on PCA data.\n"}
{"snippet": "df = pd.read_csv('../shared-resources/heights_weights_genders.csv')\nmask = df.Gender == 'Female'\ndf[mask]\ndf.describe()\n", "intent": "The scatter plot shows a linear relationship between height and weight.\n"}
{"snippet": "new_sentence = [\"The quick brown fox jumps over the lazy dog.\"]\ntfm_new = vectorizer.transform(new_sentence)\ntfidf_new = transformer.transform(tfm_new)\nnew_sentence = pd.DataFrame({\"A. tfm\":tfm_new.toarray()[0],\"B. tfidf\":tfidf_new.toarray()[0]}, index = vectorizer.get_feature_names())\nnew_sentence[new_sentence.iloc[:,1] != 0]\n", "intent": "\"The quick brown fox jumps over the lazy dog.\"\n"}
{"snippet": "def make_X(data):\n    vectorizer = CountVectorizer()\n    term_doc_matrix = vectorizer.fit_transform(data)\n    term_doc_matrix = term_doc_matrix.todense()\n    term_doc_matrix = pd.DataFrame(term_doc_matrix)\n    transformer = TfidfTransformer()\n    tfidf = transformer.fit_transform(term_doc_matrix)\n    tfidf = pd.DataFrame(tfidf.toarray())\n    return tfidf, vectorizer, transformer\n", "intent": "that gives you:\n- New X Frame\n- Vectorizer to get TFM\n- Transformer that gives you TFIDF\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test,y_train, y_test = train_test_split(X2,y, test_size=0.15, random_state=100)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n", "intent": "Train Test Split\n- Set random seed to 100 for reproducibility\n- Split data to 85% train, 15% test.\n- Other methods include k-folds crossvalidation\n"}
{"snippet": "scaler = MinMaxScaler()\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "g = sns.clustermap(cdystonia.pivot_table(index=['patient'], columns=\"week\", values=\"twstrs\").dropna(), cmap=\"summer\")\n", "intent": "We can discover structures through clustering these values in a *clustermap*:\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree = 4, include_bias = False)\nx_new = poly.fit_transform(X)\n", "intent": "We can use sklearn to examine higher polynomial features.\n"}
{"snippet": "train = pd.read_csv('train.csv')\n", "intent": "In this subsection shows an analysis of the given data\n"}
{"snippet": "def process_embarked():\n    global data_all\n    data_all.Embarked.fillna('S',inplace=True)\n", "intent": "The categorical features (Dummys)  are created.\n"}
{"snippet": "features = pd.DataFrame()\nfeatures['feature'] = train.columns\nfeatures['importance'] = model.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n", "intent": "The importance of each feature for the trained model\n"}
{"snippet": "def data_look(car_list, notcar_list):\n    data_dict = {}\n    data_dict[\"n_cars\"] = len(car_list)\n    data_dict[\"n_notcars\"] = len(notcar_list)\n    example_img = mpimg.imread(car_list[0])\n    data_dict[\"image_shape\"] = example_img.shape\n    data_dict[\"data_type\"] = example_img.dtype\n    return data_dict\n", "intent": "Get some basic information of the data set such as the number of images, and the images size\n"}
{"snippet": "img1 = cv2.imread('test_images/test1.jpg')\nimg2 = cv2.imread('test_images/test2.jpg')\nimg3 = cv2.imread('test_images/test3.jpg')\nimg4 = cv2.imread('test_images/test4.jpg')\nimg5 = cv2.imread('test_images/test5.jpg')\nimg6 = cv2.imread('test_images/test6.jpg')\n", "intent": "Using the classifer on sliding windows to detect whether an image contain cars\n"}
{"snippet": "le = LabelEncoder()\n", "intent": "le = LabelEncoder()\nfor c in X.columns:\n    X[c] = le.fit_transform(X[c])\nenc = OneHotEncoder()\nX = enc.fit_transform(X)\nonehotlabels = X\n"}
{"snippet": "((trainX, trainY), (testX, testY)) = mnist.load_data()\nprint('trainX shape:', trainX.shape)\nprint('Number of training examples:', trainX.shape[0])\nprint('Number of test examples:', testX.shape[0])\n", "intent": "We start off by loading the dataset into memory. Fortunately, Keras has an in-built API to download and load the dataset we are going to use (MNIST).\n"}
{"snippet": "milk = pd.read_csv('monthly-milk-production.csv',index_col='Month')\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "((train_X, train_Y), (test_X, test_Y)) = cifar10.load_data()\nprint('train_X shape:', train_X.shape)\nprint('Number of training examples:', train_X.shape[0])\nprint('Number of test examples:', test_X.shape[0])\n", "intent": "We start off by loading the dataset into memory. Fortunately, Keras has an in-built API to automatically download and load the CIFAR-10 dataset.\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nR_train=X_train/255.0\nR_test=X_test/255.0\n", "intent": "* Load the MNIST data set using keras and plot a few numbers\n"}
{"snippet": "y = LabelEncoder().fit_transform(y)\n", "intent": "The ```StackingClassifier``` needs numeric class labels, so we encode the original ```str``` class names. \n"}
{"snippet": "iris_dataset = datasets.load_iris()\n", "intent": "Loading the iris dataset \n"}
{"snippet": "import tensorflow as tf\niris = datasets.load_iris()\nX = iris.data[:, :2]  \nY = iris.target\nprint(\"X:\",X)\nprint(\"Y:\",Y)\n", "intent": "Logistic Regression for classification. Simplest case for 2 classes. \nLogistic Neuron\n$y = \\frac{1}{1+exp(-z)}$\n$\\frac{dy}{dz}=y(1-y)$\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.keys()\n", "intent": "Get some data to play with\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(digits.data,\n                                                    digits.target)\n", "intent": "Split the data to get going\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "1) Instantiate the model\n"}
{"snippet": "pca = PCA(n_components=2)\n", "intent": "1) Instantiate the model\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n", "intent": "With Neural Network models, its important to scale the data, again we can do this easily with SciKit Learn (I promise we'll get to TensorFlow soon!)\n"}
{"snippet": "x = np.arange(100)\ny = np.ones(100)\nsynth_data = pd.DataFrame({\n    'x': x,\n    'y': y\n})\nregr = lm.LinearRegression()\nregr.fit(synth_data.x.values.reshape(100,1), synth_data.y)\nseaborn.regplot(x=\"x\", y=\"y\", data=synth_data)\nprint(\"Pearson correlation: %.3f, Regresion coefficient: %.3f\" % (synth_data.x.corr(synth_data.y), regr.coef_[0]))\n", "intent": "Sklon regresnej krivky je uplne iny ako velkost korelacie. Len znamienko indikujuce smer je rovnake.\n"}
{"snippet": "x = np.arange(100)\ny = x + stats.norm(0,1).rvs(100)\nsynth_data = pd.DataFrame({\n    'x': x,\n    'y': y\n})\nregr = lm.LinearRegression()\nregr.fit(synth_data.x.values.reshape(100,1), synth_data.y)\nseaborn.regplot(x=\"x\", y=\"y\", data=synth_data)\nprint(\"Pearson correlation: %.3f, Regresion coefficient: %.3f\" % (synth_data.x.corr(synth_data.y), regr.coef_[0]))\n", "intent": "Tu je perfektne linearny vztah, kde je jasne vidiet, ze ten sklon je uplne iny.\n"}
{"snippet": "x = np.arange(100)\ny = x + stats.norm(0,30).rvs(100)\nsynth_data = pd.DataFrame({\n    'x': x,\n    'y': y\n})\nregr = lm.LinearRegression()\nregr.fit(synth_data.x.values.reshape(100,1), synth_data.y)\nseaborn.regplot(x=\"x\", y=\"y\", data=synth_data)\nprint(\"Pearson correlation: %.3f, Regresion coefficient: %.3f\" % (synth_data.x.corr(synth_data.y), regr.coef_[0]))\n", "intent": "Aj ked pridame trochu sumu, tak je to velmi podobne\n"}
{"snippet": "titanic = pd.read_csv('data/titanic/train.csv')\ntitanic.head()\n", "intent": "* Two-way table\n* Heatmap\n* Stacked bar plot\n* Chi-kvadrat testy\n"}
{"snippet": "import pandas as pd\nyelp = pd.read_csv('data/yelp.csv')\nyelp.head()\n", "intent": "Read **`yelp.csv`** into a pandas DataFrame and examine it.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n", "intent": "Use CountVectorizer to create **document-term matrices** from X_train and X_test.\n"}
{"snippet": "final=pd.read_csv('test.csv',usecols=['PassengerId'])\nfinal['Survived']=results\nfinal=final.set_index('PassengerId')\nfinal.to_csv('final.csv') \n", "intent": "- then we set the PassengerId as the index (check the file gender_submission.csv (from the 3 files that kaggle provides) to see the format)\n"}
{"snippet": "import numpy as np\nimport pandas as pd\ntrain = pd.read_csv('labeledTrainData.tsv', \n                    header=0,\n                    delimiter=\"\\t\", \n                    quoting=3 )\n", "intent": "To import the tsv file, it is recommended to use the pandas package. The provided file can be imported as follows\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer = 'word',   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 5000) \ntrain_data_features = vectorizer.fit_transform(clean_train_reviews)\ntrain_data_features = train_data_features.toarray()\n", "intent": "For generating a bag of words model, we will use the scikit-learn package. Use the following code\n"}
{"snippet": "scaled_x_train = scaler.fit_transform(X_train)\nscaled_x_test = scaler.transform(X_test)\n", "intent": "Keep in mind we only fit the scaler to the training data, we don't want to assume we'll have knowledge of future test data. \n"}
{"snippet": "scaler = StandardScaler()\nscaler.fit(bank.drop('Class',axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n", "intent": "** Use SciKit Learn to create training and testing sets of the data as we've done in previous lectures:**\n"}
{"snippet": "data=pd.read_csv('College_Data.csv',index_col=0)\ndata.head()\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "dataframe=pd.read_csv('KNN_Project_Data.csv')\ndataframe.head()\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaled_features=scaler.transform(dataframe.drop('TARGET CLASS',axis=1))\ndf_feat=pd.DataFrame(scaled_features,columns=dataframe.columns[:-1])\ndf_feat.head()\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "yelp=pd.read_csv('yelp.csv')\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "X=CountVectorizer().fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "X=yelp_class['text']\ny=yelp_class['stars']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "from sklearn.cross_validation import train_test_split \nX=iris.drop('species',axis=1)\ny=iris['species']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=101)\n", "intent": "** Split your data into a training set and a testing set.**\n"}
{"snippet": "bos = pd.DataFrame(boston.data)\nbos.head()\n", "intent": "Now let's explore the data set itself. \n"}
{"snippet": "from sklearn import linear_model, metrics, model_selection\nadmissions_path = Path('.', 'data', 'admissions.csv')\nadmissions = pd.read_csv(admissions_path).dropna()\nadmissions.head()\n", "intent": "The true positive and false positive rates gives us a much clearer picture of where predictions begin to fall apart.\n"}
{"snippet": "feature_columns = [\"gre\"]\nX = admissions.loc[:, feature_columns]\ny = admissions.loc[:, \"admit\"]\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=46)\nlogit_simple = linear_model.LogisticRegression().fit(X_train, y_train)\n", "intent": "**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**\n"}
{"snippet": "from sklearn import linear_model, metrics, model_selection\nadmissions_path = Path('..', 'data', 'admissions.csv')\nadmissions = pd.read_csv(admissions_path).dropna()\nadmissions.head()\n", "intent": "The true positive and false positive rates gives us a much clearer picture of where predictions begin to fall apart.\n"}
{"snippet": "X = admissions.loc[:, ['gre']]\ny = admissions['admit']\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=46)\nlogit_simple = linear_model.LogisticRegression().fit(X_train, y_train)\n", "intent": "**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = pd.DataFrame(boston.target, columns=['MEDV'])\nboston = pd.concat([X, y], axis=1)\nboston.head()\n", "intent": "Use the Boston housing data for the exercises below.\n"}
{"snippet": "mammals = (\n    pd.read_csv(mammals_path, sep='\\t', names=cols, header=0)\n    .dropna()\n    .sort_values('body')\n    .reset_index(drop=True)\n)\nmammals = mammals.loc[mammals.loc[:, 'body'] < 200, :]\nmammals.hist();\nmammals.plot(kind='scatter', x='body', y='brain');\n", "intent": "When your data is very skewed, try a log transformation.\n"}
{"snippet": "path = Path('.', 'data', 'vehicles_test.csv')\ntest = pd.read_csv(path)\n", "intent": "<a id=\"testing-preds\"></a>\n"}
{"snippet": "path = Path('..', 'data', 'vehicles_train.csv')\ntrain = pd.read_csv(path)\n", "intent": "<a id=\"cutpoint-demo\"></a>\n"}
{"snippet": "path = Path('..', 'data', 'vehicles_test.csv')\ntest = pd.read_csv(path)\n", "intent": "<a id=\"testing-preds\"></a>\n"}
{"snippet": "from sklearn.model_selection import train_test_split\n_, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itest] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"/home/gautam/datascience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "The first step with any scikit-learn (sklearn) is to **instantiate** an object. Let's **instantiate** a StandardScaler object. \n"}
{"snippet": "one_hot = OneHotEncoder(sparse=False)\n", "intent": "Just like before, we need to **instantiate** our transformer, in this case a `OneHotEncoder` object. Let's do that now:\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ncustomer_one_hot_df = pd.DataFrame(ohe.fit_transform(categorical_customer_df), columns=['region_1', 'region_2', 'region_3'])\n", "intent": "1. save the result as a new dataframe\n1. Make sure to give the columns name\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\nohe.fit(categorical_customer_df)\ncustomer_one_hot_df = pd.DataFrame(ohe.transform(categorical_customer_df),\n                                   columns=['region_1', 'region_2', 'region_3'])\n", "intent": "1. save the result as a new dataframe\n1. Make sure to give the columns name\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ncategorical_ohe_df = pd.DataFrame(ohe.fit_transform(categorical_customer_df), columns = ['region_1', 'region_2', 'region_3'])\n", "intent": "1. save the result as a new dataframe\n1. Make sure to give the columns name\n"}
{"snippet": "pca_9 = PCA(n_components=9)\nfeature_pca_np = pca_9.fit_transform(feature_df)\nfeature_pca_df = pd.DataFrame(feature_pca_np, columns = feature_df.columns)\n", "intent": "It should create 9 components. \n1. fit the PCA model using `feature_df`\n1. tranform the dataframe and assign the output to `feature_pca_np`\n"}
{"snippet": "pca_3 = PCA()\n", "intent": "It should have 6 components.\n"}
{"snippet": "pca = PCA(n_components=9)\npca = pca.fit(feature_df)\nfeature_pca_np = pca.transform(feature_df)\n", "intent": "It should create 9 components. \n1. fit the PCA model using `feature_df`\n1. tranform the dataframe and assign the output to `feature_pca_np`\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nitrain, itest = train_test_split(xrange(critics.shape[0]), train_size=0.7)\nmask=np.ones(critics.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\n", "intent": "Lets set up the train and test masks first:\n"}
{"snippet": "pca_reg_3 = PCA(n_components=6)\npca_reg_3.fit(feature_region_3_df)\nfeature_pca_reg_3_df = pca_reg_3.transform(feature_region_3_df)\npca_variance(pca_reg_3, feature_region_3_df)\n", "intent": "It should have 6 components.\n"}
{"snippet": "pca_reg_3_reduced = PCA(n_components=4)\npca_reg_3_reduced.fit(feature_region_3_df)\nfeature_pca_reg_3_reduced_np = pca_reg_3_reduced.transform(feature_region_3_df)\nfeature_pca_reg_3_reduced_df = pd.DataFrame(feature_pca_reg_3_reduced_np,\n                                            columns=['Dimension 1', 'Dimension 2',\n                                                     'Dimension 3', 'Dimension 4'])\n", "intent": "1. It should have 2 components.\n2. Assign the result of the transform to `feature_pca_reg_3_reduced_np`\n3. Convert this to a dataframe.\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ncategorical_ohe_df = pd.DataFrame(ohe.fit_transform(categorical_customer_df))\n", "intent": "1. save the result as a new dataframe\n1. Make sure to give the columns name\n"}
{"snippet": "pca_9 = PCA(n_components=9)\nfeature_pca_np = pca_9.fit_transform(feature_df)\nfeature_pca_df = pd.DataFrame(feature_pca_np, columns=feature_df.columns)\n", "intent": "It should create 9 components. \n1. fit the PCA model using `feature_df`\n1. tranform the dataframe and assign the output to `feature_pca_np`\n"}
{"snippet": "boston_df_log = pd.DataFrame(np.log(1+boston_df), columns=boston_df.columns)\n", "intent": "From looking at the charts, we can see that we want to \nDESKEW: CRIM, ZN, NOX, KN, AGE, DIS, PTRATIO, B, LSTAT\nBIMODAL: INDUS, TAX?\nBOOLEAN: CHAS\n"}
{"snippet": "iris = load_iris()\niris_features_df = pd.DataFrame(iris['data'], columns=iris['feature_names'])\n", "intent": "Import and scale the data\n"}
{"snippet": "pca = PCA(n_components=2)\n", "intent": "Now, let's visualize our data to pick a good value of k. Perform PCA and create a scatter plot of PC1 and PC2\n"}
{"snippet": "pca = PCA()\niris_features_scaled_pca = pca.fit_transform(iris_features_df_scaled)\n", "intent": "Now, let's visualize our data to pick a good value of k.\n"}
{"snippet": "boston_scaler = StandardScaler()\nX_train_scaled = boston_scaler.fit_transform(X_train)\n", "intent": "Now, let's instantiate a scaler and fit it to the training data, then transform our training data.\n"}
{"snippet": "scaled_features = scaler.fit_transform(bank.drop('Class',axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "scaler = StandardScaler()\nscaler.fit(X_t)\nX_ts = scaler.transform(X_t)\n", "intent": "Now, let's instantiate a scaler and fit it to the training data, then transform our training data.\n"}
{"snippet": "pca = PCA()\npca.fit(X_ts)\nX_tsp = pca.transform(X_ts)\n", "intent": "Just so we can visualize our data, let's do a PCA and plot our components 1 and 2, and make the color the target for our train set. \n"}
{"snippet": "boston_scaler = StandardScaler()\n", "intent": "Now, let's instantiate a scaler and fit it to the training data, then transform our training data.\n"}
{"snippet": "pca = PCA()\nboston_train_pca = pca.fit_transform(X_train_scaled)\nboston_test_pca = pca.transform(X_test_scaled)\n", "intent": "Just so we can visualize our data, let's do a PCA and plot our components 1 and 2, and make the color the target for our train set. \n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ntarget_encoded = encoder.fit_transform(target)\ntarget_encoded[:5]\n", "intent": "Label encode your target so it's numeric\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(student_data_dummies, target, test_size = .3, random_state = 42)\n", "intent": "For reproducability, use:\n* test_size = .3\n* random_state = 42\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n", "intent": "Make sure to `fit` your scaler object on your train set and use it to transform your train and test sets.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ny = digits.target == 9\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, y, random_state=0)\n", "intent": "- Type-I Error - False Positive\n- Type-II Error - False Negative\n"}
{"snippet": "X_indices = np.arange(X.shape[-1])\nselector = SelectPercentile(f_classif, percentile=10)\nselector = SelectKBest(f_classif, k=3)\nselector.fit(X, y)\n", "intent": "    SelectKBest(f_classif)\n    SelectPercentile(f_classif)\n    SelectKBest(f_regression)\n    SelectPercentile(f_regression)\n"}
{"snippet": "college = pd.read_csv('College_Data',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "SVD = TruncatedSVD(20)\nlatent_semantic_analysis = SVD.fit_transform(document_term_matrix)\n", "intent": "Take your search query, encode it the way they encoded theyir text, and take top results\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\n", "intent": "1. Create a new `LabelEncoder`. \n1. Use it to label encode the `sex` column.\n1. Save the label encoded vector back to the `titanic_df`.\n"}
{"snippet": "this_label_encoder = LabelEncoder()\nthis_label_encoder.fit(titanic_df['sex'])\ntitanic_df['sex'] = this_label_encoder.transform(titanic_df['sex'])\n", "intent": "1. Create a new `LabelEncoder`. \n1. Use it to label encode the `sex` column.\n1. Save the label encoded vector back to the `titanic_df`.\n"}
{"snippet": "this_label_encoder = LabelEncoder()\nthis_label_encoder.fit(titanic_df['embarked'])\nlabel_encoded_embarked = this_label_encoder.transform(titanic_df['embarked'])\n", "intent": "1. Create a new `LabelEncoder`. \n1. Use it to label encode the `embarked` column.\n1. Save the label encoded vector back to the `titanic_df`.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA()\nfeat_pca = pca.fit_transform(feat_scaled)\n", "intent": "* Use your **target** as your color parameter.\n"}
{"snippet": "iris = load_iris()\nX = iris.data[:,:2]\ny = iris.target\ny = np.where( y==2 , 0,y)\ny\n", "intent": "we use the poly or linear kernel to clasify the last point correctly , poly seems more right\n"}
{"snippet": "datscienc = pd.read_csv(\"data_scientist.csv\")\ndatscienc\n", "intent": "The decision boundaries are not so right , they do not tend to seperate the groups correctly\n"}
{"snippet": "def annihilate_data(X,y,num=10):\n    y_0 = len(X[y == 0])\n    y_1 = len(X[y == 1])\n    smaller = 0 if y_0 < y_1 else 1\n    idx = np.random.choice(np.where(y == smaller)[0],size = num)\n    full_idx = np.append(np.where(y != smaller)[0],idx)\n    return X[full_idx],y[full_idx]\niris = load_iris()\nX = iris.data[:,:2]\ny = iris.target\n", "intent": "The svc has two distinct groups compared to logistic regression\n"}
{"snippet": "from pandas.tools.plotting import scatter_matrix\ndf = datasets.load_iris().data[:]\ndf = pd.DataFrame(df)\nscatter_matrix(df, alpha=0.2, figsize=(6, 6), diagonal='kde');\n", "intent": "Change to the full iris data set.  datasets.load_iris().data\n"}
{"snippet": "data = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaled_df = pd.DataFrame(scaled_features, columns=df.columns[:-1])\nscaled_df.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "coeff_df = pd.DataFrame(lm.coef_, X.columns, columns=['Coefficient'])\ncoeff_df\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "from sklearn.preprocessing import  LabelEncoder\nle = LabelEncoder()  \nle.fit(df['UniqueCarrier'].head(10))\nle.transform(df['UniqueCarrier'].head(10))\n", "intent": "+ THESE ARE NOT DUMMY VARIABLES\n+ Note the fit vs fit_transform function\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"../../DS-SF-32/2008.csv\").fillna(0)\n", "intent": "http://stat-computing.org/dataexpo/2009/the-data.html\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"/Users/Reid/OneDrive/GADataScience/sandbox/2008.csv\").fillna(\"unk\")\n", "intent": "http://stat-computing.org/dataexpo/2009/the-data.html\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"../../DS-SF-32/lessons/lesson-10/DIMSIM.csv\")\npd.options.display.max_columns = 999\n", "intent": "+ Duct Tape, or...\n+ WD-40\n"}
{"snippet": "of_df = pd.read_csv(\"../../DS-SF-32/lessons/lesson-14/old-faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "path = '../../DS-SF-32/lessons/lesson-18/chipotle.tsv'\ndf = pd.read_csv(path,sep='\\t')\n", "intent": "+ hint - examine how the values are separated \n+ What's the difference between a tsv and csv?\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(ngram_range=(1,2))\nd3 = df[df[cd].notnull()].copy()\nchoices = d3[cd].fillna('')\ncv.fit(choices)\nX = cv.transform(choices)\nprint cv.vocabulary_\nX\n", "intent": "+ Use a vectorizer of your choice!\n+ Consider a dimension reduction technique! \n    + PCA? SVD? LDA?\n"}
{"snippet": "(input_train, input_test, labels_train, labels_test, dec_input_train, dec_input_test\n    ) = train_test_split(input_, labels_, dec_input_, test_size=0.1)\n", "intent": "Sklearn's <tt>train_test_split</tt> is an easy way to split data into training and testing sets.\n"}
{"snippet": "raw_data = {'first_name': ['Jason', np.nan, 'Tina', 'Jake', 'Amy'], \n        'last_name': ['Miller', np.nan, 'Ali', 'Milner', 'Cooze'], \n        'age': [42, np.nan, 24, 24, np.nan], \n        'sex': ['m', np.nan, 'f', np.nan, 'f'], \n        'preTestScore': [4, np.nan, np.nan, 2, 3],\n        'postTestScore': [25, np.nan, np.nan, 62, 70]}\ndf = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'sex', 'preTestScore', 'postTestScore'])\ndf\n", "intent": "- Numerical features\n- Categorical features\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\niris = datasets.load_iris()\nX = iris.data\nY = iris.target\n", "intent": "                                            *** Using Iris Dataset ***\n"}
{"snippet": "import pandas as pd\ndf_Iris_Full = pd.read_csv(\".//Iris Dataset//Iris.csv\")\ndf_Iris_Batch1 = pd.read_csv(\".//Iris Dataset//Iris_Batch1.csv\")\ndf_Iris_Batch2 = pd.read_csv(\".//Iris Dataset//Iris_Batch2.csv\")\n", "intent": "*** Load Iris Data and Try **\n"}
{"snippet": "boston = load_boston()\nboston_df = pd.DataFrame(boston.data, columns= boston.feature_names)\nboston_df[\"target\"] = boston.target\n", "intent": "- load the Boston dataset using scikit-learn.\n"}
{"snippet": "auto_df = pd.read_csv(\"../data/Auto.csv\", na_values=\"?\")\nauto_df = auto_df.dropna() \nprint(len(auto_df.mpg),'rows')\nauto_df.head()\n", "intent": "<img src=\"../images/3.8.jpg\">\n"}
{"snippet": "carseats = pd.read_csv('../data/carseats.csv')\ncarseats = carseats.drop(\"Unnamed: 0\", axis=1)\ncarseats.head()\n", "intent": "<img src=\"../images/3.10.jpg\">\n"}
{"snippet": "df = pd.read_csv(\"../data/Auto.csv\", na_values=\"?\").dropna()\ndf.head(2)\n", "intent": "Dataset available on http://www-bcf.usc.edu/~gareth/ISL/data.html\n"}
{"snippet": "train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n", "intent": "In Kaggle, data that can be accessed by a Kernel is saved under ``../inputs/``\nFrom there we can load it with pandas:\n"}
{"snippet": "testfile = pd.read_csv('../input/test.csv')\n", "intent": "To create the ids required for the submission we need the original test file one more time\n"}
{"snippet": "features = scaler.fit_transform(df.drop(\"Class\", axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n", "intent": "Now, load the train and test data.\n"}
{"snippet": "medians = pd.DataFrame({'Median Sales' :df.iloc[:282451].groupby(by=['Type','Dept','Store','Month'])['Weekly_Sales'].median()}).reset_index()\n", "intent": "> \nWe will take the store median in the available data as one of its properties\n"}
{"snippet": "testfile['prediction']=final_y_prediction\ntestfile['DateType'] = [datetime.strptime(date, '%Y-%m-%d').date() for date in testfile['Date'].astype(str).values.tolist()]\ntestfile['Month'] = [date.month for date in testfile['DateType']]\ntestfile['Month'] = 'Month_' + testfile['Month'].map(str)\ntestfile=testfile.merge(medians, how = 'outer', on = ['Type','Dept','Store','Month'])\ntestfile['prediction'].fillna(testfile['prediction'].median(), inplace=True) \ntestfile['Median Sales'].fillna(testfile['Median Sales'].median(), inplace=True) \ntestfile['prediction']+=testfile['Median Sales']\ntestfile.describe()\n", "intent": "Let's add the means to our testfile and then subtract the expected difference.\n"}
{"snippet": "import numpy as np\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn import datasets\nfrom sklearn import svm\niris = datasets.load_iris()\n", "intent": "Let's revisit the Iris data set:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\nclf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)\n", "intent": "A single train/test split is made easy with the train_test_split function in the cross_validation library:\n"}
{"snippet": "data = pd.read_csv(\"data/imdb.csv\", quotechar='\"', escapechar='\\\\')\ndata[\"Text\"][1]\n", "intent": "1\\. Read in the data located in `data/imdb.csv`. Don't forget to set the `quotechar` to `\"` and `escapechar` to `\\`. **`[`*`5 points`*`]`**\n"}
{"snippet": "data = pd.read_csv(\"data/imdb.csv\", ...)\n", "intent": "1\\. Read in the data located in `data/imdb.csv`. Don't forget to set the `quotechar` to `\"` and `escapechar` to `\\`. **`[`*`5 points`*`]`**\n"}
{"snippet": "import pandas as pd\nusers = pd.read_csv(\"data/users.csv\")\n", "intent": "For structured data like we have here, we will use `pandas`.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"data/mailing.csv\")\nX = data.drop(['class'], 1)\nY = data['class']\n", "intent": "First read the data in and put the target variable in `Y` and the features in `X`.\n"}
{"snippet": "features_mat = pd.DataFrame(features, columns=df.columns[:-1])\nfeatures_mat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data = pd.read_csv(\"data/categorical.csv\")\n", "intent": "We have examined two ways of dealing with categorical data: binarizing/dummy variables and numerical scaling. We will practice these here.\n"}
{"snippet": "bike= pd.read_csv(('data/Bike-Sharing-Dataset/day.csv'))\nbike['dteday'] = pd.to_datetime(bike['dteday']).dt.strftime('%j')\nbike = bike.drop(0)\nbike.head(4)\n", "intent": "We'll be using the same bike sharing data as last week!\n"}
{"snippet": "bike = pd.read_csv(('data/Bike-Sharing-Dataset/day.csv'))\nbike['dteday'] = pd.to_datetime(bike['dteday']).dt.strftime('%j')\nbike = bike.drop(0)\nbike.head(4)\n", "intent": "We'll be using the same bike sharing data as last week!\n"}
{"snippet": "bike=Table().read_table(('data/Bike-Sharing-Dataset/day.csv'))\nbike['dteday'] = pd.to_datetime(bike['dteday']).strftime('%j')\nbike = bike.drop(0)\nbike.show(4)\n", "intent": "We'll be using the same bike sharing data as last week!\n"}
{"snippet": "components = pd.DataFrame({'component 1': Y_pca[:, 0],\n                          'component 2': Y_pca[:, 1],\n                          'Candidate': y})\ncolors = candidate_colors.values()\nsns.lmplot(x = 'component 1', y = 'component 2', data = components, hue='Candidate', legend=True, fit_reg=False, \n            hue_order=candidate_colors.keys(),palette=colors, scatter_kws={'s':5}, size=7);\n", "intent": "Finally, plot the data by running the cell below.\n"}
{"snippet": "lsa_components = pd.DataFrame({'component1': Y_lsa[:, 0],\n                          'component2': Y_lsa[:, 1],\n                          'Type': y})\nsns.lmplot(x = 'component1', y = 'component2', data = lsa_components, hue='Type', legend=True, fit_reg=False, \n           hue_order=['press release', 'speech', 'statement'], palette=colors, scatter_kws={'s':5}, size=7);\n", "intent": "Now, run the next cell to plot the transformed data.\n"}
{"snippet": "lsa_components = pd.DataFrame({'component1': Y_lsa[:, 0],\n                          'component2': Y_lsa[:, 1],\n                          'Type': campaign.Type})\nsns.lmplot(x = 'component1', y = 'component2', data = lsa_components, hue='Type', legend=True, fit_reg=False, \n            scatter_kws={'s':8}, size=12);\n", "intent": "And, take a look at the exact same data, but with the document type marked in colors rather than the candidates.\n"}
{"snippet": "lsa_components = pd.DataFrame({'component1': Y_lsa[:, 0],\n                          'component2': Y_lsa[:, 1],\n                          'Candidate': y})\nsns.lmplot(x = 'component1', y = 'component2', data = lsa_components, hue='Candidate', legend=True, fit_reg=False, \n           hue_order=candidate_colors.keys(), palette=candidate_colors.values(), scatter_kws={'s':8}, size=12);\n", "intent": "Now, run the next cell to plot the transformed data.\n"}
{"snippet": "old_bailey = pd.read_csv('data/obc_1822_1832.csv', index_col='trial_id')\nold_bailey = old_bailey.loc[:, ['year', 'transcript']]\nold_bailey.head()\n", "intent": "----\nLet's get working with the data.\n"}
{"snippet": "data = pd.read_csv(\"College_Data\", index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "from sklearn.datasets import fetch_olivetti_faces, fetch_lfw_people\nfrom ipywidgets import interact\nimage_shape = (64, 64)\ndataset = fetch_olivetti_faces('./')\nfaces = dataset.data\n", "intent": "Next, we will take a look at what happens if we project some dataset consisting of human faces onto some basis we call\nthe \"eigenfaces\".\n"}
{"snippet": "eth = pd.read_csv('./data/ethereum_dataset.csv',)\nprint(eth.shape)\neth.head()\n", "intent": "Load the data `./data/ethereum_dataset` and have a look. \n"}
{"snippet": "pipeline = Pipeline([\n    ('scaling', StandardScaler()),\n])\nX = pipeline.fit_transform(eth.values)\nprint(X.shape)\n", "intent": "You can clearly see the drop.\nApply a standard scaling to the data\n"}
{"snippet": "ccfd = pd.read_csv('./data/creditcard.csv')\nccfd.head()\n", "intent": "Load the `creditcard.csv` data into a dataframe called `ccfd`\n"}
{"snippet": "pipeline = Pipeline([\n    ('scaling', StandardScaler()),\n])\npreprocessor = pipeline.fit(XTrain)\nXTrain_s = preprocessor.transform(XTrain)\nXTest_s = preprocessor.transform(XTest)\n", "intent": "Apply the usual scaling preprocessing (on both the training and test set)\n"}
{"snippet": "new_gen_feature_arr = gen_ohe.transform(df_poke_new[['Gen_Label']]).toarray()\nnew_gen_features = pd.DataFrame(new_gen_feature_arr, columns=gen_feature_labels)\nnew_leg_feature_arr = leg_ohe.transform(df_poke_new[['Lgnd_Label']]).toarray()\nnew_leg_features = pd.DataFrame(new_leg_feature_arr, columns=leg_feature_labels)\nnew_poke_ohe = pd.concat([df_poke_new, new_gen_features, new_leg_features], axis=1)\ncolumns = sum([['Name', 'Generation', 'Gen_Label'], gen_feature_labels, ['Legendary', 'Lgnd_Label'], \n               leg_feature_labels], [])\nnew_poke_ohe[columns]\n", "intent": "We can now use our previously built LabelEncoder objects and perform one hot encoding on these new data observations using the following code.\n"}
{"snippet": "from sklearn.feature_selection import VarianceThreshold\nvt = VarianceThreshold(threshold=0.15)\nvt.fit(poke_gen)\n", "intent": "Next, we want to remove features from the one hot encoded features where the variance is less than 0.15.\n"}
{"snippet": "from dateutil.parser import parse\nimport os\nimport pandas as pd\ndf = pd.read_csv('meat.csv')\ndf.date = df.date.apply(parse)\ndf = df.sort(['date'])\ndf.index = df.date\ndf = df.fillna(0)\ndf.boxplot();\n", "intent": "**Note:** these two cells can't be executed because the dataset referenced isn't available anymore, and I don't know where it comes from.\n"}
{"snippet": "train_data, test_data = train_test_split(df, test_size=0.2, random_state=0)\nX_train=train_data.sqft_living\nX_train=X_train.reshape(-1, 1)\ny_train=train_data.price\ny_train=y_train.reshape(-1, 1)\nregr = linear_model.LinearRegression()\nregr.fit(X=X_train, y=y_train)\n", "intent": "Split data into training and testing.  \nWe use random_state=0 so that everyone running this notebook gets the same results.  \n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "data_path = 'c:/Users/E411208/Documents/Python Scripts/data/Bluetooth/'\ndata = pd.DataFrame()\nfor id,file in enumerate(os.listdir(data_path)):\n    if file.endswith(\".csv\"):\n        print('Loading file ... ' + file)\n        measurement = pd.read_csv(data_path + file)\n        measurement['measurement']=id \n        data = data.append(measurement,ignore_index = True)\n", "intent": "Here we load all CSV files, one CSV file equaels one measurement\n"}
{"snippet": "data_path = 'c:/Users/E411208/Documents/Python Scripts/data/Bluetooth/'\ndata = pd.DataFrame()\nfor id,file in enumerate(os.listdir(data_path)):\n    if file.endswith(\".csv\"):\n        print('Loading file ... ' + file)\n        measurement = pd.read_csv(data_path + file)\n        measurement['measurement']=id \n        data = data.append(measurement,ignore_index = True)\n", "intent": "Here we load all CSV files, merge them and normalize the data\n"}
{"snippet": "teens_labels = pd.DataFrame(kmeans.labels_ , columns = list(['labels']))\nteens_labels['labels'] = teens_labels['labels'].astype('category')\n", "intent": "kmeans.labels_\npd.DataFrame(kmeans.labels_, columns = \"label\")\n"}
{"snippet": "lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train.author.values)\n", "intent": "We use the LabelEncoder from scikit-learn to convert text labels to integers, 0, 1 2\n"}
{"snippet": "train = pd.read_csv(os.path.join(os.curdir, \"train.csv\"))\ntest    = pd.read_csv(os.path.join(os.curdir, \"test.csv\"))\nfull = train.append( test , ignore_index = True )\ntitanic = full[ :891 ]\ndel train , test\nprint ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)\n", "intent": "Now that our packages are loaded, let's read in and take a peek at the data.\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "Z = allmyfeatures.fit_transform(train)\nprint(\"Z has type \", type(Z))\nprint(\"Z has shape \", Z.shape)\n", "intent": "Now we'll call our mega-transformer on the original data, and hopefully get a sparse matrix out that encapsulates all of the features. \n"}
{"snippet": "degree, alpha = 9, 0\nstand_combo = [(\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n               (\"stand\", StandardScaler()),\n               (\"ridge\", Ridge(alpha=alpha))]\ndeg9regpipe = Pipeline(stand_combo)\ndeg9regpipe.fit(X_train, y_train)\n", "intent": "**Part C**: Next we'll fit a high-degree polynomial model with no regularization, which is very likely to overfit. \n"}
{"snippet": "pd.DataFrame({\"feature\": dfHitters.columns[:-1], \"weight\": lasso.coef_})\n", "intent": "We need to look at the resulting estimated regression parameters from the Lasso.  \n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_valid, y_valid) = mnist.load_data()\n", "intent": "**Part A**: First, we'll load the MNIST data directly from Keras. \n"}
{"snippet": "X = CountVectorizer().fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "with open('anna.txt', 'r') as f:\n    text=f.read()\nprint(text[100:200])\n", "intent": "Read the file as text.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(.95)\npca.fit(train_img)\ncomponents = pca.transform(train_img)\napproximation = pca.inverse_transform(components)\nprint('PCA used %s components' %pca.n_components_)\n", "intent": "Hint: For computing the Cumulative Explained Variance over PCA use:\n```\npca.explained_variance_ratio_.cumsum()\n```\n"}
{"snippet": "from sklearn.cluster import KMeans\nfrom sklearn.datasets.samples_generator import make_blobs\nX, y_true = make_blobs(n_samples=6000, n_features=60, centers=6,\n                       random_state=0)\n", "intent": "Hint: Use the sklearn.datasets.make_blobs to generate the data\n"}
{"snippet": "iris = pd.read_csv('iris.csv', encoding=\"latin1\")\niris.head()\n", "intent": "Read the iris CSV file\n"}
{"snippet": "iris_features_train, iris_features_test, iris_target_train, iris_target_test = \\\n    train_test_split(features,target,test_size=0.20, random_state=0)\n", "intent": "Split the iris data into training set and test set\n"}
{"snippet": "titanic_test = pd.read_csv('test.csv', encoding='latin1')\ntitanic_test.head()\n", "intent": "Create a DataFrame with the test.csv data\n"}
{"snippet": "titanic_test.Sex.replace(to_replace=['male','female'], value=[True,False], inplace=True)\navg_age = titanic_test.Age.mean()\ntitanic_test.Age = titanic_test.Age.fillna(avg_age)\ntitanic_test = titanic_test[pd.notnull(titanic_test['Fare'])]\nprint(titanic_test.info())\n", "intent": "Clean text and missing values\n"}
{"snippet": "titanic_test.Sex.replace(['male','female'],[True,False], inplace=True)\navg_age = titanic_test.Age.mean()\ntitanic_test.Age = titanic_test.Age.fillna(avg_age)\nprint(titanic_test.info())\n", "intent": "Clean text and missing values\n"}
{"snippet": "X_reduced = PCA(n_components=3).fit_transform(plotly_iris.data)\n", "intent": "<hr style=\"border: 1px solid\n"}
{"snippet": "X = yelp_class[\"text\"]\ny = yelp_class[\"stars\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = tf.keras. \\\n                            datasets.mnist.load_data()\n", "intent": "https://en.wikipedia.org/wiki/MNIST_database\n"}
{"snippet": "unique_list = []\nfor comedian in data.columns:\n    uniques = data[comedian].nonzero()[0].size\n    unique_list.append(uniques)\ndata_words = pd.DataFrame(list(zip(full_names, unique_list)), columns=['comedian', 'unique_words'])\ndata_unique_sort = data_words.sort_values(by='unique_words')\ndata_unique_sort\n", "intent": "Tools: *Numpy, and previous ones from above*\n"}
{"snippet": "from sklearn import decomposition\npca = decomposition.PCA(n_components=20)\npca.fit(imagesWithPixelsScaled)\nimage_pca_Representation = pca.transform(imagesWithPixelsScaled)\nprint(image_pca_Representation.shape)\n", "intent": "So now we can use this, with arbirtrary numbers of dimensions, to reduce the number of features we need to do our logistic regression\n"}
{"snippet": "import pandas as pd\nresults = pd.DataFrame(estimator.cv_results_)\nresults.head()\n", "intent": "The scores from all the different sets of parameters was recorded in the estimator object and can be converted to a dataframe. \n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X)\n", "intent": "But we can do better - let's do PCA on the data `X`\n"}
{"snippet": "data = loadmat('data/ex8_movies.mat')\ndf_r = pd.DataFrame(data['R'])\ndf_y = pd.DataFrame(data['Y'])\n", "intent": "**Recommender System - Collaborative Filtering**\n"}
{"snippet": "path = os.getcwd() + '/data/ex1data1.txt'\ndata = pd.read_csv(path, header=None, names=['Population', 'Profit'])\ndata.head()\n", "intent": "**2 Linear regression with one variable**\n"}
{"snippet": "path = os.getcwd() + '/data/ex1data2.txt'\ndata2 = pd.read_csv(path, header=None, names=['Size', 'Bedrooms', 'Price'])\ndata2.head()\n", "intent": "**3 Linear regression with multiple variables**\n"}
{"snippet": "path = os.getcwd() + '/data/ex2data2.txt'\ndata2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])\norig_data2 = data2.copy()\ndata2.head()\n", "intent": "Logistic Regression with Regularization\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "[https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)\n"}
{"snippet": "pca = sklearn.decomposition.PCA(n_components=1)\npca.fit(X_norm)\n", "intent": "**Sklearn Solution**\n"}
{"snippet": "faces = loadmat('data/ex7faces.mat')\nX = pd.DataFrame(faces['X'])\nX.shape\n", "intent": "**PCA and Eigenfaces**\n"}
{"snippet": "import statsmodels.api as sm\nfrom statsmodels.formula.api import logit, glm, ols\ndat = pd.DataFrame(data, columns = ['Temperature', 'Failure'])\nlogit_model = logit('Failure ~ Temperature',dat).fit()\nprint (logit_model.summary())\n", "intent": "Lets plot this data\n"}
{"snippet": "dfhw=pd.read_csv(\"https://dl.dropboxusercontent.com/u/75194/stats/data/01_heights_weights_genders.csv\")\nprint (dfhw.shape)\ndfhw.head()\n", "intent": "(I encountered this dataset in Conway, Drew, and John White. Machine learning for hackers. \" O'Reilly Media, Inc.\", 2012.)\n"}
{"snippet": "itrain, itest = train_test_split(range(df.shape[0]), train_size=0.6)\nmask=np.ones(df.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\nmask[:10]\n", "intent": "We split the data into training and test sets...\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500)\nX = cv.fit_transform(corpus).toarray()\n", "intent": "Through tokenization\n"}
{"snippet": "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", \n                 header=None)\ndf.head()\n", "intent": "Read in Wisconsin Breast Cancer Dataset\n"}
{"snippet": "df = pd.DataFrame(data=mtcars)\ndf.head()\n", "intent": "Convert to a Pandas Dataframe for our analysis\n"}
{"snippet": "df = pd.DataFrame(adult)\n", "intent": "Convert the data to a Pandas dataframe to work with the data:\n"}
{"snippet": "import pandas as pd\npd.DataFrame(X, columns=iris.feature_names).head()\n", "intent": "**\"Observations\"** are also known as samples, instances, or records.\n"}
{"snippet": "iris = pd.read_csv(\"../../assets/datasets/iris.csv\")\n", "intent": "And read in our data:\n"}
{"snippet": "iris = pd.read_csv(\"../../assets/datasets/iris.csv\")\niris.head()\n", "intent": "First, let's import our data. We'll be using *Iris* again for this exercise. \n"}
{"snippet": "iris = pd.read_csv(\"..\\..\\assets\\datasets\\iris.csv\")\n", "intent": "First, let's import our data. We'll be using *Iris* again for this exercise. \n"}
{"snippet": "db = pd.read_csv(\"../../assets/datasets/iris.csv\")\ndb.head()\n", "intent": "We're going to load the iris data from the scikit \"datasets\" package\n"}
{"snippet": "db = pd.read_csv(\"..\\assets\\datasets\\\")\n", "intent": "We're going to load the iris data from the scikit \"datasets\" package\n"}
{"snippet": "df3 = pd.DataFrame(sales2016[\"Store_Number\"])\ndf3[\"2016Model\"] = predictions2016\ndf3[\"2016Model1\"] = predictions22016\ndf3[\"DifferenceRate\"] = (df3[\"2016Model\"] - df3[\"2016Model1\"])/df3[\"2016Model\"]\ndf3.head()\n", "intent": "df3 obsevers the difference in sales values for 20156 between our two models (with cross validation and without).\n"}
{"snippet": "df3 = pd.DataFrame(sales2016[\"Store_Number\"])\ndf3[\"2016Model\"] = predictions2016\ndf3[\"2016Model1\"] = predictions22016\ndf3[\"Difference\"] = df3[\"2016Model\"] - df3[\"2016Model1\"]\ndf3.head()\n", "intent": "df3 obsevers the difference in sales values for 20156 between our two models (with cross validation and without).\n"}
{"snippet": "import pandas as pd\nX_train, Y_train = X[:-nb_train//10], Y_train[:-nb_train//10]\nX_val, y_val = X[-nb_train//10:], y[-nb_train//10:]\nhistory = model.fit(X_train, Y_train, callbacks=[ThreeAccuracyCallback(X_val, y_val)])\npd.DataFrame(history.history).plot();\n", "intent": "The following code fits a multi-label logistic regression model to the multi-digit MNIST images and uses an `AccuracyCallback` for evaluation.\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nvae.fit(x_train, x_train,\n        shuffle=True,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_data=(x_test, x_test))\n", "intent": "That was easy. Now we'll train the thing on the usual dataset\n"}
{"snippet": "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()\n", "intent": "We'll load up our raw data set exactly as before:\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df['Sex'])\ndf['Sex_label_encoded'] = le.transform(df['Sex'])\n", "intent": "[sklearn.preprocessing.LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n"}
{"snippet": "df = pd.read_csv('../datasets/raw-material-properties.csv')\n", "intent": "Quelle: [https://openmv.net/info/raw-material-properties](https://openmv.net/info/raw-material-properties)\n"}
{"snippet": "from sklearn.preprocessing import Imputer\ndf_tmp = df.drop('Sample', axis=1)\nimp = Imputer(missing_values='NaN', strategy='mean', axis=0)\nimp.fit(df_tmp.values)\nimputed_data = imp.transform(df_tmp.values)\ndf_tmp = pd.DataFrame(imputed_data, columns=df_tmp.columns)\nprint('Fehlende Werte enthalten: {}'.format(df_tmp.isnull().values.any()))\nprint()\nprint('Anzahl Zeilen: {}'.format(df_tmp.shape[0]))\nprint('Anzahl Spalten: {}'.format(df_tmp.shape[1]))\n", "intent": "[sklearn.preprocessing.Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.33, random_state=42)\n", "intent": "[sklearn.model_selection.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n"}
{"snippet": "df = pd.read_csv('../../datasets/pima-indians-diabetes.csv')\n", "intent": "Quelle: [https://www.kaggle.com/uciml/pima-indians-diabetes-database](https://www.kaggle.com/uciml/pima-indians-diabetes-database)\n"}
{"snippet": "df=pd.read_csv(\"./datasets/spambase_test.csv\")\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure\n"}
{"snippet": "data_path_train = os.path.join(os.getcwd(), 'datasets', 'splice_train.csv')\nsplice_train = pd.read_csv(data_path_train, delimiter = ',')\nprint('Number of instances: {}, number of attributes: {}'.format(splice_train.shape[0], splice_train.shape[1]))\nsplice_train.head(5)\n", "intent": "Load the `splice_train.csv` and `splice_test.csv` into two separate dataframes. Display the shape and first 10 instances for each dataframe.\n"}
{"snippet": "ss = StandardScaler()\n", "intent": "Standardize features by removing the mean and scaling to unit variance\n"}
{"snippet": "word_freq_df = pd.DataFrame(frequency_array.toarray(),\n                            columns = vectorizer.get_feature_names())\n", "intent": "As before, this array can be turned into a dataframe.\n"}
{"snippet": "import pandas as pd\nmasses_data = pd.read_csv('mammographic_masses.data.txt')\nmasses_data.head()\n", "intent": "Start by importing the mammographic_masses.data.txt file into a Pandas dataframe (hint: use read_csv) and take a look at it.\n"}
{"snippet": "stem_vectorizer = CountVectorizer(tokenizer = tokenize_and_stem)\n", "intent": "The `tokenize` function can be added as an option for a count vectorizer.\n"}
{"snippet": "bg_df = pd.read_csv('data/boardgames.csv')\n", "intent": "Let's do it again, but with a different data set\n"}
{"snippet": "for dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\nprint (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())\n", "intent": "the embarked feature has some missing value. and we try to fill those with the most occurred value ( 'S' ).\n"}
{"snippet": "for dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\nprint (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())\n", "intent": "Fare also has some missing value and we will replace it with the median. then we categorize it into 4 ranges.\n"}
{"snippet": "path = sorted(tf.gfile.Glob(os.path.join(data_path, '*.ndjson')))[0]\nprint(tf.gfile.Open(path).read()[:1000] + '...')\n", "intent": "Let's further explore what the `NDJSON` file format is.\n"}
{"snippet": "feature_cols = ['CG_SPEND','LEADS','CAR2','CAR6','GBR2','GBR6','JPC6','JTR2','JTR6','N652','N656','SA60','SD60','ST12','ST60','TKM2','TKM6','WMN2','WMN6','Cable_Air_Type','Local Cable_Air_Type','Network_Air_Type','Regional Cable_Air_Type','Satellite_Air_Type','Spot_Air_Type','Syndication_Air_Type']\nX = tv_dummies.loc[:,feature_cols]\ny = tv_dummies.loc[:, 'CPS']\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=1)\n", "intent": "Based on the testing of appropriate feature columns above we will use CREATIVE and AIR TYPE as our feature columns\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=99)\n", "intent": "Notice that we create the train/test split first. This is because we will reveal information about our testing data if we standardize right away.\n"}
{"snippet": "feature_cols = ['gre']\nX = admissions.loc[:, feature_cols]\ny = admissions.loc[:, 'admit']\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=46)\nlogit_simple = linear_model.LogisticRegression().fit(X_train, y_train)\n", "intent": "**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**\n"}
{"snippet": "path = Path('.', 'data', 'vehicles_train.csv')\ntrain = pd.read_csv(path)\n", "intent": "<a id=\"cutpoint-demo\"></a>\n"}
{"snippet": "from sklearn import preprocessing\nscaler = preprocessing.StandardScaler()\nall_features_scaled = scaler.fit_transform(all_features)\nall_features_scaled\n", "intent": "Some of our models require the input data to be normalized, so go ahead and normalize the attribute data. Hint: use preprocessing.StandardScaler().\n"}
{"snippet": "vect = CountVectorizer()\nvect.fit(X_train)\nX_train_dtm = vect.transform(X_train)\n", "intent": "<a id='countvectorizer-model'></a>\n"}
{"snippet": "vect = CountVectorizer()\nvect.fit(X_train)\nX_train_dtm = vect.transform(X_train)\nX_test_dtm = vect.transform(X_test)\n", "intent": "greater, because now all the words like Pizza and pizza are separate\n"}
{"snippet": "vect = TfidfVectorizer(stop_words='english')\n", "intent": "<a id='yelp_tfidf'></a>\n"}
{"snippet": "vect = CountVectorizer()\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\n", "intent": "<a id='countvectorizer-model'></a>\n"}
{"snippet": "path = Path('.', 'data', 'vehicles_train.csv')\ntrain = pd.read_csv(path)\ntrain\n", "intent": "<a id=\"manual-bagged\"></a>\n"}
{"snippet": "path = Path('.', 'data', 'titanic.csv')\ntitanic = pd.read_csv(path)\n", "intent": "- Use a random forest classifier to predict who survives on the Titanic. Use five-fold cross-validation to evaluate its accuracy.\n"}
{"snippet": "path = Path('..', 'data', 'vehicles_train.csv')\ntrain = pd.read_csv(path)\n", "intent": "<a id=\"manual-bagged\"></a>\n"}
{"snippet": "path = Path('..', 'data', 'titanic.csv')\ntitanic = pd.read_csv(path)\n", "intent": "- Use a random forest classifier to predict who survives on the Titanic. Use five-fold cross-validation to evaluate its accuracy.\n"}
{"snippet": "df1 = pd.read_csv('train.csv') \ndf2 = pd.read_csv('test.csv')\n", "intent": "Reading the testing and training data set.\n"}
{"snippet": "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())\n", "intent": "Keras returns *classes* as a single column, so we convert to one hot encoding\n"}
{"snippet": "norm=preprocessing.Normalizer().fit(x)\n", "intent": "each row value / norm(row)\n"}
{"snippet": "pd.DataFrame({'cnt':data_train.tags.value_counts(),'%':data_train.tags.value_counts()/data_train.tags.count()})\n", "intent": "The distribution of tag in the training set is not highly biased.\n"}
{"snippet": "PATH_TO_TRAINING = os.path.join(os.getcwd(), \"data\", \"train.csv\")\nassert os.path.isfile(PATH_TO_TRAINING), \"file does not exist\"\noriginal_df = pd.read_csv(PATH_TO_TRAINING)\n", "intent": "Get the data and do some initial cleaning\n"}
{"snippet": "df = pd.read_csv('data.csv')\nprint('Shape of dataframe: ', df.shape)\ndf.head(50)\n", "intent": "The first step is always to clean the data and prepare it for its further analysis. \n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\nprint(x_train.shape, x_test.shape)\n", "intent": "First we have to create our train and test set. \n"}
{"snippet": "d = { 'name' : ['PSY_GANGNAM', 'KATY_ROAR', 'LMFAO_PARTY', 'EMINEM_LOVELIE', 'SHAKIRA_WAKA'], 'views' : [2951311450,2269849627,1348417123,1393162453,1542731300],'release' : ['2012-07-15','2013-09-05','2011-03-08','2010-08-05','2010-06-04']}\nd = pd.DataFrame(d)\nd\n", "intent": "The data set comprises of comments from years 2013 to 2016, while the actual release dates of these videos is around 2010-2013.\n"}
{"snippet": "Comparison = pd.DataFrame([CountVectorizer_scores.CountVectorizer, TfidfVectorizer_scores.TfidfVectorizer]) \n", "intent": "The following function compares the performance of the the two models and the above classifiers\n"}
{"snippet": "def get_stock_data(stock_name, normalized=0):\n    url = 'http://chart.finance.yahoo.com/table.csv?s=%s&a=11&b=15&c=2011&d=29&e=10&f=2016&g=d&ignore=.csv' % stock_name\n    col_names = ['Date','Open','High','Low','Close','Volume','Adj Close']\n    stocks = pd.read_csv(url, header=0, names=col_names) \n    df = pd.DataFrame(stocks)\n    date_split = df['Date'].str.split('-').str\n    df['Year'], df['Month'], df['Day'] = date_split\n    df[\"Volume\"] = df[\"Volume\"] / 10000\n    df.drop(df.columns[[0,3,5,6, 7,8,9]], axis=1, inplace=True) \n    return df\n", "intent": "Read data and store it in a dataframe. Then, use df.head() to show the first rows of the dataset. \n"}
{"snippet": "pca = PCA(n_components=2)\nprint pca.fit_transform(X)[0:5,:]\n", "intent": "__fit_transform__ is also available (and is sometimes faster).\n"}
{"snippet": "ratings = pd.read_csv(path+'ratings.csv')\nratings.head()\n", "intent": "Let us know explore the data and see .\n"}
{"snippet": "num_tops = 22\nmodel = decomposition.NMF(init=\"nndsvd\", \n                          n_components=num_tops, \n                          max_iter=400, \n                          tol=0.0001,\n                          )\nW = model.fit_transform(A)\nH = model.components_\nprint (\"Generated factor W of size %s and factor H of size %s\" % ( str(W.shape), str(H.shape) ) )\n", "intent": "Do the factorization and produce the factors\n"}
{"snippet": "df_interactions = pd.DataFrame(interactions)\ndf_interactions.head(10)\n", "intent": "Now we will start to use the dataset of user interactions dataset. Again, we can easily convert our list to a DataFrame.\n"}
{"snippet": "iso = manifold.Isomap(n_neighbors=25, n_components=2)\nX_iso = iso.fit_transform(X)\nfig = bk.figure(title='Isomap - S-Curve', x_axis_label='c1', y_axis_label='c2',\n                plot_width=750, plot_height=400)\nfig.scatter(X_iso[:, 0], X_iso[:, 1], size=8, alpha=0.8, line_color='black',\n            fill_color=pal.linear_map(y, seqcolors))\nbk.show(fig)\n", "intent": "Manifold learning algorithms, however, available in the `sklearn.manifold` submodule, are able to recover the underlying 2-dimensional manifold:\n"}
{"snippet": "X_train, X_test, y_train, y_test = cross_validation.train_test_split(np.array(X[X.columns[2:3]]),\\\n                                                                     np.array(y),\\\n                                                                     test_size=0.25, random_state=0)\n", "intent": "Keep just the attributes in the column number 2 (3rd column) and use 25% of the data as a testing set:\n"}
{"snippet": "X_train, X_test, y_train, y_test = cross_validation.train_test_split(np.array(X),\\\n                                                                     np.array(y),\\\n                                                                     test_size=0.25, \n                                                                     random_state=0)\n", "intent": "Keep all the 10 attributes and split training and testing sets:\n"}
{"snippet": "idx_train, idx_valid = cross_validation.train_test_split(boston.index, test_size=0.20)\nboston_train, boston_valid = boston.ix[idx_train], boston.ix[idx_valid]\nbostony_train, bostony_valid = data.target[idx_train], data.target[idx_valid]\n", "intent": "Split the data in **Training and Validation Set**:\n"}
{"snippet": "hidden_size = 10\nactivation_function = 'tanh'\nstart_time = time.time()\nerrors = pd.DataFrame([a for a in multitest([activation_function], [hidden_size],\n                                            slider_source.data['batch'], \n                                            slider_source.data['rate'], \n                                            X_train2, y_train2, X_test2, y_test2)])\nerrors.columns = ['activation', 'hidden', 'batch', 'rate', 'error']\nprint(\"Execution time: %s seconds\" % (time.time() - start_time))\n", "intent": "Next we perform the test with all the combination of parameters selected. It can take some time (up to 25 minutes in cpu!), please be patient.\n"}
{"snippet": "X_train, X_test, y_train, y_test = cross_validation.train_test_split(np.array(X),\\\n                                                                     np.array(y),\\\n                                                                     test_size=0.25, random_state=0)\n", "intent": "Keep all the 10 attributes and split training and testing sets:\n"}
{"snippet": "sdf = pd.DataFrame(sd, columns = df.columns.drop('TARGET CLASS'))\nsdf.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "ratings = pd.read_csv(path+'ratings.csv')\nratings.head()\n", "intent": "We're working with the movielens data, which contains one rating per row, like this:\n"}
{"snippet": "data = pd.read_csv('aquastat.csv.gzip', compression='gzip')\ndata.region = data.region.apply(lambda x: simple_regions[x])\ndata = data.loc[~data.variable.str.contains('exploitable'),:]\ndata = data.loc[~(data.variable=='national_rainfall_index')]\n", "intent": "http://www.fao.org/nr/water/aquastat/data/query/index.html\n"}
{"snippet": "print(\"Computing Isomap embedding\")\niso = manifold.Isomap(n_neighbors, n_components).fit_transform(cats_and_dogs)\nprint(\"Done.\")\nplotting(iso[:n_cats], iso[n_cats:])\n", "intent": "Let's first try the  **Isomap embedding**\n"}
{"snippet": "print(\"Computing Spectral embedding\")\nemb = manifold.LocallyLinearEmbedding(n_neighbors, n_components, eigen_solver='auto', \n                                        method = 'standard').fit_transform(cats_and_dogs)\nprint(\"Done.\")\nplotting(emb[:n_cats], emb[n_cats:])\n", "intent": "Now let's do **Spectral embedding**. There are different method for this routine. However this 'standard' is the one giving less problems!\n"}
{"snippet": "iris = load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\nprint('Class labels:', np.unique(y))\n", "intent": "We reload the Iris data set:\n"}
{"snippet": "import os\npath = os.getcwd() + '\\data\\ex2data2.txt'\ndata2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])\ndata2.head()\n", "intent": "Similar to part 1, let's start by visualizing the data.\n"}
{"snippet": "ratings = pd.read_csv(path + 'ratings.csv')\nratings.head()\n", "intent": "We're working with the movielens data, which contains one rating per row, like this:\n"}
{"snippet": "movies = pd.read_csv(path + 'movies.csv')\nmovies.head()\n", "intent": "Just for display purposes, let's read in the movie names too.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\npipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\n                     LogisticRegression())\nparam_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(text_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n", "intent": "\\begin{equation*}\n\\text{tfidf}(w, d) = \\text{tf} \\log\\big(\\frac{N + 1}{N_w + 1}\\big) + 1\n\\end{equation*}\n"}
{"snippet": "import pandas as pd\nimport os\ndataset = pd.read_csv('data/kc_house_data.csv')\nprint(dataset.columns)\nprint()\nprint(dataset.head())\n", "intent": "The File contains 19 house features plus the price and the id columns, along with 21613 observations.\n"}
{"snippet": "movie_names = pd.read_csv(path+'movies.csv').set_index('movieId')['title'].to_dict()\n", "intent": "Just for display purposes, let's read in the movie names too.\n"}
{"snippet": "df = pd.read_csv('../data/kaggle/train.csv')\n", "intent": "Agora vamos carregar o dataset e inspecionar algumas coisas:\n"}
{"snippet": "test = pd.read_csv('data/house_prices_test.csv')\n", "intent": "__MAKING A SUBMISSION__\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_df=0.8, max_features=15000, sublinear_tf=True, use_idf=True)\n", "intent": "Mas, primeiro, precisamos criar a matriz de atributos. Para isso, vamos utilizar o TF-IDF.\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\ny_train_wide = to_categorical(y_train, num_classes)\ny_test_wide = to_categorical(y_test, num_classes)\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n", "intent": "Load in the data again as we need to keep it in image shape\n"}
{"snippet": "from urllib2 import urlopen\npath = 'faithful.txt'\nremote = urlopen('https://raw.githubusercontent.com/aidiary/PRML/master/ch9/faithful.txt')\nwith open('faithful.txt', 'w') as f:\n    f.write(remote.read())\n", "intent": "**NOTE:** In order to help you out, I will get you started by downloading the data and plotting it\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"tips.csv\")\nlen(df)\n", "intent": "As a second example, we will examine a dataset of 244 meals, with details of total meal bill and tip amount.\n"}
{"snippet": "df = pd.read_csv(\"advertising.csv\", index_col=0)\nx = df.drop(\"Sales\",axis=1)\nx.head()\n", "intent": "We will use the previous advertising dataset, which had 3 independent features: TV, Radio, Newspaper.\n"}
{"snippet": "vectorizer = CountVectorizer(stop_words=\"english\")\nX = vectorizer.fit_transform(raw_documents)\n\"and\" in vectorizer.vocabulary_\n", "intent": "We can use the built-in list of stop-words for a given language by just specifying the name of the language (lower-case):\n"}
{"snippet": "custom_stop_words = [ \"and\", \"the\", \"game\" ] \nvectorizer = CountVectorizer(stop_words=custom_stop_words)\nX = vectorizer.fit_transform(raw_documents)\n\"game\" in vectorizer.vocabulary_\n", "intent": "Or we could use our own custom stop-word list, which might be more appropriate for specific applications:\n"}
{"snippet": "seq_len = 500\ntrn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\ntest = sequence.pad_sequences(test, maxlen=seq_len, value=0)\n", "intent": "Pad (with zero) or truncate each sentence to make consistent length.\n"}
{"snippet": "import nltk\ndef stem_tokenizer(text):\n    standard_tokenizer = CountVectorizer().build_tokenizer()\n    tokens = standard_tokenizer(text)\n    stemmer = PorterStemmer()\n    stems = []\n    for token in tokens:\n        stems.append( stemmer.stem(token) )\n    return stems\n", "intent": "To use NLTK stemming with Scikit-learn, we need to create a custom tokenisation function:\n"}
{"snippet": "vectorizer = CountVectorizer(tokenizer=stem_tokenizer)\nX = vectorizer.fit_transform(raw_documents)\nterms = vectorizer.get_feature_names()\nprint(terms[200:220])\n", "intent": "Now we can use our custom tokenizer with the standard CountVectorizer approach:\n"}
{"snippet": "def lemma_tokenizer(text):\n    standard_tokenizer = CountVectorizer().build_tokenizer()\n    tokens = standard_tokenizer(text)\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    lemma_tokens = []\n    for token in tokens:\n        lemma_tokens.append( lemmatizer.lemmatize(token) )\n    return lemma_tokens\n", "intent": "We can perform lemmatisation in the same way, using NLTK with Sckit-learn:\n"}
{"snippet": "vectorizer = CountVectorizer(stop_words=\"english\",min_df = 3,tokenizer=lemma_tokenizer)\nX = vectorizer.fit_transform(raw_documents)\nprint(X.shape)\n", "intent": "Let's put all of these steps together:\n"}
{"snippet": "final_res_train = pd.DataFrame()\nfinal_res_train[\"count\"]= train_cnt.iloc[:][0]\nfinal_res_test = pd.DataFrame()\nfinal_res_test[\"count\"]= test_cnt.iloc[:][0]\n", "intent": "Resultant final dataframe to be merged to train dataset\n"}
{"snippet": "final_train=pd.concat([train_wNA, final_res_train], axis=1)\nfinal_train = final_train[(final_train['count']<80)]\nfinal_train = final_train[final_train.columns.difference([\"count\"])]\nfinal_train.to_csv('row_filter_train.csv')\nfinal_test=pd.concat([train_wNA, final_res_test], axis=1)\nfinal_test = final_test[(final_test['count']<80)]\nfinal_test = final_test[final_test.columns.difference([\"count\"])]\nfinal_test.to_csv('row_filter_test.csv')\n", "intent": "Merging the frequency column to training and testing dataset for further filtering\n"}
{"snippet": "final_res_train = pd.DataFrame()\nfinal_res_train[\"count\"]= train_cnt.iloc[:][0]\nfinal_res_train.head()\nfinal_res_test = pd.DataFrame()\nfinal_res_test[\"count\"]= test_cnt.iloc[:][0]\nfinal_res_test.head()\n", "intent": "Resultant final dataframe to be merged to train dataset\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.30)\n", "intent": "Split the data into a **training set** and a **test set**\n"}
{"snippet": "def lemma_tokenizer(text):\n    standard_tokenizer = TfidfVectorizer().build_tokenizer()\n    tokens = standard_tokenizer(text)\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    lemma_tokens = []\n    for token in tokens:\n        lemma_tokens.append( lemmatizer.lemmatize(token) )\n    return lemma_tokens\n", "intent": "As we are using the TFIDF so we are crating the tokenizer from TfidfVectorizer instead of CountVectorizer\n"}
{"snippet": "electoral_votes = pd.read_csv(\"data/electoral_votes.csv\").set_index('State')\nelectoral_votes.index.name = None\nelectoral_votes.head()\n", "intent": "*As a matter of convention, we will index all our dataframes by the state name*\n"}
{"snippet": "sac = pd.read_csv('../../assets/datasets/Sacramentorealestatetransactions.csv')\nsac.describe()\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "conmat = np.array(confusion_matrix(y_test, y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['over_200k', 'under_200k'],\n                         columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ncategoricals = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n                'native-country', 'income']\nfor x in categoricals:\n    df[x] = (pd.Series(le.fit_transform(df[x])))\ndf.head()\n", "intent": "Convert the categorical Data to numeric for our analysis. **HINT:** Refer to lesson 1.1 for writing a function of this sort\n"}
{"snippet": "X_scaled = StandardScaler().fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled, columns=X.columns)\nX_scaled\n", "intent": "First, let's standardize the data\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder()\n", "intent": " - n_values\n - n_features\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nfile = \"datatraining.txt\"\ndataframe = pd.read_csv(file)\nsubframe = dataframe.values[:,1:7]\n", "intent": "- Reference the dataset and turn it into a dataframe with relevant column identities\n- Select the columns valid for Rescaling in a subframe\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nfile = \"datatraining.txt\"\ndataframe = pd.read_csv(file)\nsubframe = dataframe.values[:,1:7]\n", "intent": "- Reference the dataset and turn it into a dataframe with relevant column identities\n- Select the columns valid for Standardization in a subframe\n"}
{"snippet": "X = subframe\nscaler = StandardScaler().fit(X)\nrescaled = scaler.transform(X)\nnp.set_printoptions(precision=3)\nprint(rescaled[0:5,:])\n", "intent": "- Use sklearn to Standardize the subframe based on a mean of 0 and stdev of 1 (experiment)\n- Display the changes in a summary\n"}
{"snippet": "from sklearn.preprocessing import Normalizer\nfile = \"datatraining.txt\"\ndataframe = pd.read_csv(file)\nsubframe = dataframe.values[:,1:7]\n", "intent": "- Reference the dataset and turn it into a dataframe with relevant column identities\n- Select the columns valid for Normalization in a subframe\n"}
{"snippet": "polls04=pd.read_csv(\"data/p04.csv\")\npolls04.State=polls04.State.replace(states_abbrev)\npolls04.set_index(\"State\", inplace=True);\npolls04.head()\n", "intent": "Let us also load in data about the 2004 elections from `p04.csv` which gets the results in the above form for the 2004 election for each state.\n"}
{"snippet": "from sklearn.preprocessing import Binarizer\nfile = \"datatraining.txt\"\ndataframe = pd.read_csv(file)\nsubframe = dataframe.values[:,1:7]\n", "intent": "- Reference the dataset and turn it into a dataframe with relevant column identities\n- Select the columns valid for Binarization in a subframe\n"}
{"snippet": "le = preprocessing.LabelEncoder()\n", "intent": "<hr />\nFirst thing we need to do is initialize the label encoder.\nThen we need to use *fit_transform()* to assign each features data to a new label.\n"}
{"snippet": "features = pd.DataFrame()\nfeatures['feature'] = train.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(['importance'], ascending=False).head(12)\n", "intent": "<hr />\n>This step is enabled by the built in '**feature\\_importances\\_**' attribute in the *ExtraTreesClassifier()* model we initialized earlier\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = np.expand_dims(x_train / np.max(x_train), -1)\nx_test = np.expand_dims(x_test / np.max(x_test), -1)\ny_train = keras.utils.to_categorical(y_train, 10)\ny_test = keras.utils.to_categorical(y_test, 10)\n", "intent": "And now -- knowing about Convolution and Pooling, let's use these layers and do some machine learning.\nFirst -- the MNIST digits.\n"}
{"snippet": "pay_features = ['pay_'+str(i) for i in range(2,7)]\npay_features_pca = PCA().fit(default[pay_features])\npay_features_pca.explained_variance_ratio_\n", "intent": "Since we know that pay_1 is important we will keep it as is.\n"}
{"snippet": "img = tf_keras.preprocessing.image.load_img('../images/monkey.jpg')\nx = tf_keras.preprocessing.image.img_to_array(img) \nx = x.reshape((1,) + x.shape)\ni = 0\nfor batch in datagen.flow(x, batch_size=1,\n                          save_to_dir='../images',\n                          save_prefix='monkey', save_format='jpeg'):\n    i += 1\n    if i > 16:\n        break      \n", "intent": "<img src=\"../../images/monkey.jpg\" width=\"400\">\n"}
{"snippet": "from urllib2 import urlopen\npath = 'faithful.txt'\nremote = urlopen('https://raw.githubusercontent.com/aidiary/PRML/master/ch9/faithful.txt')\nwith open('faithful.txt', 'w') as f:\n    f.write(remote.read())\n", "intent": "Let's start with the old faithful data you used in your homework:\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntvec = TfidfVectorizer(stop_words=None)\ntvec.fit([uber_text])\ndf  = pd.DataFrame(tvec.transform([uber_text]).todense(),\n                   columns=tvec.get_feature_names(),\n                   index=['uber_text'])\ndf.transpose().sort_values('uber_text', ascending=False).head(10).transpose()\n", "intent": "<span style=\"font-size:1.2em; color:orange\">Do the same for TFIDF</span>\n"}
{"snippet": "def references_organization(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == 'ORG' for word in parsed])\ndata['references_organization'] = data['title'].fillna(u'').map(references_organization)\ndata[data.references_organization][['title']].head()\n", "intent": "Let's see if we can find organizations in our page titles.\n"}
{"snippet": "results2012 = pd.read_csv(\"data/2012results.csv\")\nresults2012.set_index(\"State\", inplace=True)\nresults2012 = results2012.sort_index()\nresults2012.head()\n", "intent": "We load in the actual 2012 results so that we can compare our results to the predictions.\n"}
{"snippet": "classfive = DataFrame({\n    'x' : random.random(50) * 50 + 100,\n    'y' : random.random(50) * 50 + 100,\n    'label' : ['orange' for i in range(50)]\n})\n", "intent": "What happens when we introduce a new length of cluster?\n"}
{"snippet": "pca_transformer = PCA(29).fit(x_train)\nx_train_2d = pca_transformer.transform(x_train)\nx_test_2d =  pca_transformer.transform(x_test)\nfitted_lr = LogisticRegression(C=100000).fit(x_train_2d, y_train)\nprint(\"Train set score: {0:4.4}%\".format(fitted_lr.score(x_train_2d, y_train)*100))\nprint(\"Test set score: {0:4.4}%\".format(fitted_lr.score(x_test_2d, y_test)*100))\n", "intent": "<HR>\nWe need 29 principal components to capture at least 90% of the variance. They capture 91.53% of the variance.\n<HR>\n"}
{"snippet": "np.random.seed(9001)\ndf = pd.read_csv('data/dataset_hw5_2.csv')\nmsk = np.random.rand(len(df)) < 0.5\ndata_train = df[msk]\ndata_test = df[~msk]\n", "intent": "**7.0:** First task: split the data using the code provided below. \n"}
{"snippet": "data = datasets.load_iris()\n", "intent": "First, we load the famous IRIS dataset\n"}
{"snippet": "def load_dataset(split):\n    dataset = datasets.load_iris()\n    X, y = dataset['data'], dataset['target']\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=123)\n    return X_train, X_test, y_train, y_test\n", "intent": "The iris data set (https://en.wikipedia.org/wiki/Iris_flower_data_set) is loaded and split into train and test parts by the function `load_dataset`.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\nprint(data)\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"data/cell2cell_data_80_percent.csv\")\n", "intent": "1\\. Load the data into a pandas `DataFrame()`.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"data/imdb.csv\")\n", "intent": "1\\. Load the data into a pandas `DataFrame()`.\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=.75)\n", "intent": "Let's start by dividing our data into training and test sets.\n"}
{"snippet": "electoral_votes = pd.read_csv(\"data/electoral_votes.csv\").set_index('State')\nelectoral_votes.head()\n", "intent": "*As a matter of convention, we will index all our dataframes by the state name*\n"}
{"snippet": "def whiskey_distance(name, distance_measures, n):\n    distances = pd.DataFrame()\n    whiskey_location = np.where(data.index == name)[0][0]\n    for distance_measure in distance_measures:\n        current_distances = distance.squareform(distance.pdist(data, distance_measure))\n        most_similar = np.argsort(current_distances[:, whiskey_location])[0:n]\n        distances[distance_measure] = zip(data.index[most_similar], current_distances[most_similar, whiskey_location])\n    return distances\n", "intent": "What other entries do we have that are similar?\n"}
{"snippet": "vect = CountVectorizer()\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\n", "intent": "Note: Whatever you train it on it only knows those words.\n"}
{"snippet": "vect = CountVectorizer(ngram_range=(1, 2))\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape\n", "intent": "number of word phrases\n"}
{"snippet": "import pandas as pd\nyelp = pd.read_csv('/Users/jennawhite/Documents/DS-SEA-4/data/yelp.csv')\nyelp.head(1)\n", "intent": "Read `yelp.csv` into a DataFrame.\n"}
{"snippet": "tfidf_transformer = TfidfTransformer().fit(bow)\n", "intent": "Fit the transformed bag of words into the tf-idf transformer\n"}
{"snippet": "coeffecients = pd.DataFrame(lm.coef_,X.columns)\ncoeffecients.columns = ['Coeffecient']\ncoeffecients\n", "intent": "<a id='coefficients'></a>\n"}
{"snippet": "scaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(ks[[\"usd_goal_real\"]])\nscaled_data\n", "intent": "Let scale the amount of money they asked for (usd_goal_real)\n"}
{"snippet": "norm_features = preprocessing.scale(features)\npd.DataFrame(norm_features).head()\n", "intent": "|label|1|2|3|4|5|6|7|8|9|10|\n|-|-|\n|genre|'Pop_Rock'|'Electronic'|'Rap'|'Jazz'|'Latin'|'RnB'|'International'|'Country'|'Reggae'|'Blues'|\n"}
{"snippet": "scaled_features = preprocessing.scale(train_data)\npd.DataFrame(scaled_features).head()\n", "intent": "Center to the mean and component wise scale to unit variance.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target)\nprint(Xtrain.shape, Xtest.shape)\n", "intent": "Let's try a classification task on the digits. The first thing we'll want to do is split the digits into a training and testing sample:\n"}
{"snippet": "pca = PCA(n_components=.9, whiten=True)\npca.fit(X_train)\n", "intent": "**90% variance is explained with 5 features**\n"}
{"snippet": "X, y = pima_predict_df.drop(['Class variable'], 1), pima_predict_df['Class variable']\nX_scale = scale(X)\nX_train, X_test, y_train, y_test = train_test_split(X_scale, y, \n                                                    test_size=0.2, random_state=2)\ncv = ShuffleSplit(X_train.shape[0], n_iter=5, test_size=0.2, random_state=6)\n", "intent": "Use kNN to get baseline accuracy\n==\n"}
{"snippet": "X, y = pima_predict_df.drop(['Class variable'], 1), pima_predict_df['Class variable']\nX_scale = scale(X)\nX_train, X_test, y_train, y_test = train_test_split(X_scale, y, \n                                                    test_size=0.2, random_state=2)\ncv = ShuffleSplit(X_train.shape[0], n_iter=5, test_size=0.2, random_state=6)\n", "intent": "Use GaussianNb to get baseline accuracy\n==\n"}
{"snippet": "X, y = pima_predict_df.drop(['Class variable'], 1), pima_predict_df['Class variable']\nX_scale = scale(X)\nX_train, X_test, y_train, y_test = train_test_split(X_scale, y, \n                                                    test_size=0.2, random_state=2)\ncv = ShuffleSplit(X_train.shape[0], n_iter=5, test_size=0.2, random_state=6)\npca = PCA(n_components=.9).fit(X_train)\nX_train = pca.transform(X_train)\nX_test = pca.transform(X_test)\n", "intent": "**Use GaussianNb**\n==\n"}
{"snippet": "from sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\njob_Trans = mlb.fit_transform([{str(val)} for val in Marketing['job'].values])\neducation_Trans = mlb.fit_transform([{str(val)} for val in Marketing['education'].values])\nmonth_Trans = mlb.fit_transform([{str(val)} for val in Marketing['month'].values])\nday_of_week_Trans = mlb.fit_transform([{str(val)} for val in Marketing['day_of_week'].values])\n", "intent": "For other categorical variables, we encode the levels as digits using Scikit-learn's MultiLabelBinarizer and treat them as new features.\n"}
{"snippet": "df_train = pd.read_csv('./data/' + label_csv_name + '.csv', header=None,nrows =19999)\ndf_train.columns = ['id','imageId', 'url', 'labelId']\ndf_train.head()\n", "intent": "image_demo = cv2.imread('./data/base/Images/coat_length_labels/fff3f9da02b33c0d2619a1dde0914737.jpg')\nimage_demo.shape\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n         X, y, test_size=0.3, random_state=0)\n", "intent": "Splitting data into 70% training and 30% test data:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\nX_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=0.3, random_state=0)\n", "intent": "Splitting the data into 70% training and 30% test subsets.\n"}
{"snippet": "import pandas as pd\ndf_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)\ndf_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash', \n'Alcalinity of ash', 'Magnesium', 'Total phenols', \n'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', \n'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\ndf_wine.head()\n", "intent": "Loading the *Wine* dataset from Chapter 4.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nsample_submission = pd.read_csv(\"sample_submission.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "X = boston.data\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X)\nX_reduced = pca.transform(X)\nprint(\"Reduced dataset shape:\", X_reduced.shape)\n", "intent": "So we have 506 examples and 13 points.\n"}
{"snippet": "from sklearn import linear_model\ny = boston.target\nX_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(X, y, random_state=0)\nX_train_reduced, X_test_reduced, y_train_reduced, y_test_reduced = sklearn.cross_validation.train_test_split(X_reduced, y, random_state=0)\nnp.testing.assert_array_equal(y_train_reduced, y_train)\n", "intent": "Now let's run our regression, both *in the reduced space* and in the original\n"}
{"snippet": "from sklearn.datasets import fetch_lfw_people\nfaces = fetch_lfw_people(min_faces_per_person = 60)\nprint(faces.target_names)\nprint(faces.images.shape)\n", "intent": "we can use cross validation to decide on the optimal C value, and it generally depends on the kind of results wished for ...\nSVMs in Action!!\n"}
{"snippet": "digits_new = pca.inverse_transform(data_new)\nplot_digits(digits_new)\n", "intent": "Finally, we can use the inverse transform of the PCA object to construct the new digits\n"}
{"snippet": "import numpy as np \nfrom sklearn import datasets\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n", "intent": "try to do the same thing with the Iris data.\nhttps://archive.ics.uci.edu/ml/datasets/Iris\n. you can also download it using the code below:\n"}
{"snippet": "window_size = 7\nX,y = window_transform_series(series = dataset,window_size = window_size)\ntest_window_transform(dataset,window_size)\n", "intent": "With this function in place apply it to the series in the Python cell below.  We use a window_size = 7 for these experiments.\n"}
{"snippet": "img1 = mpimg.imread(img_list_1[80])\nimg2 = mpimg.imread(img_list_2[80])\nplot_2_images(img1, img2, title1='Vehicle Image', title2='Non Vehicle Image')\n", "intent": "Let's have a look at one image from each class:\n"}
{"snippet": "vect2 = CountVectorizer(stop_words='english')\nX_vect2 = vect2.fit_transform(bar['COMMENTS']) \nlda = LatentDirichletAllocation(n_topics=12, learning_method=\"batch\")\nX_lda = lda.fit_transform(X_vect2)\nlda.fit(X_vect2)\n", "intent": "Much of the old categories are distinguishable from the topics as well as the clusters. \n"}
{"snippet": "best_feature_selector = SelectKBest(f_regression, k=10)\nX_train_best = best_feature_selector.fit_transform(X_train, y_train)\nX_train_best.shape\n", "intent": "Best features determined by looking at the correlation between each feature and the output.\n"}
{"snippet": "import pandas as pd\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nsample_submission = pd.read_csv(\"sample_submission.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "pipeline = Pipeline([\n  ('feature_selection', SelectFromModel(Lasso(alpha=100.0))),\n  ('regression', LinearRegression())\n])\npipeline.fit(X_train, y_train)\n", "intent": "Best features determined using the lasso (a linear model with L1 regularisation).\n"}
{"snippet": "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\ntwenty_train = fetch_20newsgroups(subset='train',\n                                  categories=categories,\n                                  shuffle=True,\n                                  random_state=0)\ntwenty_test = fetch_20newsgroups(subset='test',\n                                 categories=categories,\n                                 shuffle=True,\n                                 random_state=0)\ntwenty_train.target_names\n", "intent": "The following code will download training and test sets containing the documents. It might take a little bit of time to fetch the data!\n"}
{"snippet": "X_new = pd.DataFrame({'TV': [50000]})\nX_new.head()\n", "intent": "Thus, we would predict Sales of **2,383 widgets** in that market.\nOf course, we can also use Statsmodels to make the prediction:\n"}
{"snippet": "collage=pd.read_csv('College_Data',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df_coeff=pd.DataFrame(data=lm.coef_,index=X_train.columns,columns=['coeff'])\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "advdata = pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nselected = titanic.copy()\nselected = selected.drop(['embarked'], axis=1)\nselected['sex'] = selected['sex'] == 'female'\nselected_data_train, selected_data_test, selected_labels_train, selected_labels_test = \\\n  train_test_split(selected, labels, test_size=0.25, random_state=42)\nrf, score = rf_eval(selected_data_train, selected_labels_train, selected_data_test, selected_labels_test)\nscore\n", "intent": "Take these with a hint of salt.  Classification without \"age\" performs just as well!\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size = 0.3,random_state=123)\n", "intent": "Score and plot. How do your metrics change? What does this tell us about the size of training/testing splits?\n"}
{"snippet": "columns = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole Weight\",\n           \"Shucked weight\", \"Viscera weight\", \"Shell weight\", \"Rings\" ]\ndf = pd.read_csv(\"abalone.data\", names=columns)\ndf.head()\n", "intent": "We'll deal with Titanic dataset again, this time with XGBClassifier\n"}
{"snippet": "pd.read_csv(\"data/sample_submission.csv\").head()\n", "intent": "What answer should look like.\n"}
{"snippet": "data_dets['versaoDocumento'] = data_dets['versaoDocumento'].fillna(0)\ndata_dets['versaoDocumento'].value_counts()\n", "intent": "We can see that versaoDocumento only contains the value 1.0 and Nan. We can replace Nan for 0\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\ndet_prod_uCom_encoder = LabelEncoder()\ndet_prod_xProd_encoder = LabelEncoder()\ninfAdic_infCpl_encoder = LabelEncoder()\ndata_dets['det.prod.uCom_encoded'] = det_prod_uCom_encoder.fit_transform(data_dets['det.prod.uCom'])\ndata_dets['det.prod.xProd_encoded'] = det_prod_xProd_encoder.fit_transform(data_dets['det.prod.xProd'])\ndata_dets['infAdic.infCpl_encoded'] = infAdic_infCpl_encoder.fit_transform(data_dets['infAdic.infCpl'])\ndata_dets.loc[:3,['det.prod.uCom','det.prod.uCom_encoded','det.prod.xProd','det.prod.xProd_encoded', 'infAdic.infCpl', 'infAdic.infCpl_encoded']]\n", "intent": "Handeling categorical columns like det.prod.uCom, det.prod.xProd. We will add an encoded column for each one\n"}
{"snippet": "future_dates = my_model.make_future_dataframe(periods=7*24, freq='h')\nfuture_dates.tail(3)\n", "intent": "Let's create the dataframe with the future dates we want to forcast. This means 7 days * 24 hours extra datepoints.\n"}
{"snippet": "future_dates = my_model.make_future_dataframe(periods=7*24, freq='h')\nfuture_dates.tail(7*24)\n", "intent": "Let's create the dataframe with the future dates we want to forcast. This means 7 days * 24 hours extra datepoints.\n"}
{"snippet": "def factor_betas(pca, factor_beta_indices, factor_beta_columns):\n    assert len(factor_beta_indices.shape) == 1\n    assert len(factor_beta_columns.shape) == 1\n    fbetas = pd.DataFrame(pca.components_.T, factor_beta_indices, factor_beta_columns)\n    return fbetas\nproject_tests.test_factor_betas(factor_betas)\n", "intent": "Implement `factor_betas` to get the factor betas from the PCA model.\n"}
{"snippet": "def factor_returns(pca, returns, factor_return_indices, factor_return_columns):\n    assert len(factor_return_indices.shape) == 1\n    assert len(factor_return_columns.shape) == 1\n    freturns = pd.DataFrame(pca.transform(returns), factor_return_indices, factor_return_columns)\n    return freturns\nproject_tests.test_factor_returns(factor_returns)\n", "intent": "Implement `factor_returns` to get the factor returns from the PCA model using the returns data.\n"}
{"snippet": "def idiosyncratic_var_matrix(returns, factor_returns, factor_betas, ann_factor):\n    print(factor_betas)\n    common_returns_ = pd.DataFrame(np.dot(factor_returns, factor_betas.T), returns.index, returns.columns)\n    residuals_ = (returns - common_returns_)\n    return pd.DataFrame(np.diag(np.var(residuals_))*ann_factor, returns.columns, returns.columns)\nproject_tests.test_idiosyncratic_var_matrix(idiosyncratic_var_matrix)\n", "intent": "Implement `idiosyncratic_var_matrix` to get the idiosyncratic variance matrix.\n"}
{"snippet": "def idiosyncratic_var_vector(returns, idiosyncratic_var_matrix):\n    x = pd.DataFrame(np.diag(idiosyncratic_var_matrix),  returns.columns)\n    print(x)\n    return x\nproject_tests.test_idiosyncratic_var_vector(idiosyncratic_var_vector)\n", "intent": "Implement `idiosyncratic_var_vector` to get the idiosyncratic variance Vector.\n"}
{"snippet": "def sharpe_ratio(factor_returns, annualization_factor):\n    df_sharpe = pd.DataFrame(data=annualization_factor*factor_returns.mean()/factor_returns.std(),columns=['Sharpe Ratio'])\n    print(df_sharpe['Sharpe Ratio'])\n    return df_sharpe['Sharpe Ratio']\nproject_tests.test_sharpe_ratio(sharpe_ratio)\n", "intent": "The last analysis we'll do on the factors will be sharpe ratio. Implement `sharpe_ratio` to calculate the sharpe ratio of factor returns.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"adult.data.csv\", header=None, names=COLUMNS)\n", "intent": "We load the data into pandas because it is small enough to manage in memory, and look at some properties.\n"}
{"snippet": "car_sales = pd.read_csv('car_data.csv')\n", "intent": "* pandas to_datetime\n"}
{"snippet": "train = pd.read_csv('resources/Machine Learning Sections/Logistic-Regression/titanic_train.csv')\n", "intent": "Linear vs logistic slope\n"}
{"snippet": "recipes = pd.read_csv('recipes_muffins_cupcakes.csv')\nrecipes\n", "intent": "__Step 2:__ Import Data\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(centers=2, random_state=0, cluster_std=0.7)\nprint('X ~ n_samples x n_features:', X.shape)\nprint('y ~ n_samples:', y.shape)\nprint('\\nFirst 5 samples:\\n', X[:5, :])\nprint('\\nFirst 5 labels:', y[:5])\n", "intent": "First, we will look at a two class classification problem in two dimensions. We use the synthetic data generated by the ``make_blobs`` function.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nseed=8\nval_size=0.3\nD1_train, D1_val,= train_test_split(D1, test_size=val_size, random_state=seed)\nD2_train, D2_val,= train_test_split(D2, test_size=val_size, random_state=seed)\n", "intent": "* Generate a training and test partition (70%-30%)\n"}
{"snippet": "seed=8\nval_size=0.3\nnewdata_train, newdata_val,= train_test_split(newdata, test_size=val_size, random_state=seed)\n", "intent": "Separamos la data en training (70%) y test (30%)\n"}
{"snippet": "df = pd.read_csv(\"College_Data\",index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head(5)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.3, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"adult.test.csv\", header=None, names=COLUMNS)\n", "intent": "We load the data into pandas because it is small enough to manage in memory, and look at some properties.\n"}
{"snippet": "X= ad_data[[ 'Age', 'Area Income', 'Daily Time Spent on Site',\n       'Daily Internet Usage', 'Male']]\ny=ad_data[[\"Clicked on Ad\"]]\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.3, random_state=101)\n", "intent": "** Split the data into training set and testing set using train_test_split**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()  \n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "X = cv.fit_transform(X)   \n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "X = yelp_class['text']\ny= yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(\n  X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "f = 'alice.txt'\nwith open(f, 'r') as fp:\n    txt = fp.read()\n", "intent": "- setting up the data\n"}
{"snippet": "rgrs = Lasso(alpha=alpha_optim)\nrgrs.fit(x_tr, y_tr)\nCFS = pd.DataFrame({'coefficients':range(len(rgrs.coef_)), 'VAL':rgrs.coef_})\nCFS = CFS.sort_values(['VAL'])[::-1]\nCFS[0:50].plot(x='coefficients',y='VAL',kind='bar',figsize=(12,6), cmap='Greys')\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "from sklearn import preprocessing\ntotdata = float(len(x))\nscaler = preprocessing.StandardScaler()\nx = scaler.fit_transform(x)\nx_tr = x[0:int(totdata*0.8)]\nx_ts = x[int(totdata*0.8):]\ny_tr = y[0:int(totdata*0.8)]\ny_ts = y[int(totdata*0.8):]\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "df = pd.read_csv('data/hw2data.csv')\ndf.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "coeff = pd.DataFrame({'Coefficient Number': range(len(our_lasso.coef_)), 'Value': our_lasso.coef_})\ntop_sorted = coeff.sort_values('Value', ascending=False)[:100]\ntop_sorted.plot(x='Coefficient Number', y='Value', kind='bar', figsize=(20,10))\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntraining_inputs, testing_inputs, training_classes, testing_classes = train_test_split(all_inputs, all_classes, train_size=0.75, random_state=1)\n", "intent": "Now split the data.\n"}
{"snippet": "filename = './japanese_credit.data'\ndf = pd.read_csv(filename, index_col=None, header=None)\n", "intent": "Dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/\n"}
{"snippet": "labelencoder = LabelEncoder()\ntarget_label = labelencoder.fit_transform(data.LotShape) \ntarget_label[0:30]\n", "intent": "One-hot and label encoding\n"}
{"snippet": "train, test = train_test_split(data, test_size = 0.4)\n", "intent": "Splitting dataset into train and test\n"}
{"snippet": "df=pd.read_csv(\"train.csv\")\ndf.head()\n", "intent": "Dataset link: https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/\n"}
{"snippet": "data = pd.read_csv('train.csv')\n", "intent": "Load the \"train.csv\" dataset.\n"}
{"snippet": "le = LabelEncoder()\nX['Sex'] = le.fit_transform(X['Sex'])\n", "intent": "Convert Sex column into 1/0 using Label Encoding.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n", "intent": "Split dataset into train and test with test size as 20% of total dataset.\n"}
{"snippet": "df = pd.read_csv('creditcard.csv')\n", "intent": "Let's load the data in order to do some analysis\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=0)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "df = pd.read_csv('../../data/ads_hour.csv',index_col=['Date'], parse_dates=['Date'])\n", "intent": "We will take real time-series data of total ads watched by hour in one of our games.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = np.asmatrix(X).astype(np.float)\nX = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=0)\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "df = pd.read_csv('./wine/winequality-white.csv', header=0, sep=';')\n", "intent": "Data: https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n"}
{"snippet": "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2)\nX += np.random.random(X.shape)\ndatasets = [make_moons(noise=0.1), make_circles(noise=0.1, factor=0.5), (X, y)]\n", "intent": "<h1 align=\"center\">2D datasets</h1> \n"}
{"snippet": "import shutil\nshutil.rmtree('data/sines', ignore_errors=True)\nos.makedirs('data/sines/')\nnp.random.seed(1) \nfor i in xrange(0,10):\n  to_csv('data/sines/train-{}.csv'.format(i), 1000)  \n  to_csv('data/sines/valid-{}.csv'.format(i), 250)\n", "intent": "<h3> Cloud ML Engine </h3>\nNow to train on Cloud ML Engine with more data.\n"}
{"snippet": "df = pd.read_csv('weights.csv', sep=';', index_col=0)\ndf.head()\n", "intent": "Data -  [link](https://www.dropbox.com/s/8srfeh34lnj2cb3/weights.csv?dl=0)\n"}
{"snippet": "from sklearn.decomposition import PCA  \nmodel = PCA(n_components=2)            \nmodel.fit(X_iris)                      \nX_2D = model.transform(X_iris)         \n", "intent": "Following the sequence of steps outlined earlier, we have:\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nrnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\nrnd_clf.fit(iris[\"data\"], iris[\"target\"])\nfor name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n    print(name, score)\n", "intent": "Now, lets try using the Random Forest Classifier to tell us which of features from the Iris Setosa data set are the most useful.\n"}
{"snippet": "import numpy as np \nimport pandas as pd \nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris[\"data\"]\ny = iris[\"target\"]\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nprint('The accuracy of the Random Forest classifier on training data is {:.2f}'.format(random_forest.score(X_train, y_train)))\nprint('The accuracy of the Random Forest classifier on test data is {:.2f}'.format(random_forest.score(X_test, y_test)))\n", "intent": "Apply Gradient Boost, Random Forest and Ada Boost to the iris data set.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder = LabelEncoder()\nstate = dataset.State  \ndataset['State_Encoded'] = labelencoder.fit_transform(state.values)\n", "intent": "The `State` column contains categorical features. This needs to be converted into Dummy Variables\n"}
{"snippet": "ss = StandardScaler()\nX_train_scaled=ss.fit_transform(X_train)\nX_test_scaled= ss.transform(X_test)\n", "intent": "Now train a simple linear regression on scaled data:\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"higgs.csv\")\nX = data.drop(\"Label\",1)\ny = data.Label.values\n", "intent": "__Before you begin__ - extract (un-zip) __`./higgs.csv.zip`__ contents to this location. There should be just one file (`higgs.csv`).\n"}
{"snippet": "import pandas as pd\nfrom IPython.display import FileLink\ndef save_results(filename, y_ans):\n    answer_dataframe = pd.DataFrame(columns=[\"ID\", \"ans\"])\n    answer_dataframe['ID'] = range(0,len(y_ans))\n    answer_dataframe['ans'] = y_ans\n    answer_dataframe.to_csv('{}'.format(filename), index=False)\n    return FileLink('{}'.format(filename))\n", "intent": "Saving you results to file.\n"}
{"snippet": "parksInfo_sub = parksInfo_dummied[['ACRES','SUM_TOTPOPSVCA','total_amenities',]]\ny = np.array(parksInfo_dummied.tweet_class)\nX = parksInfo_sub\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42)\nparam_grid = {'n_estimators': range(1,50)}\nrf = RandomForestClassifier()\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid)\ngrid_search.fit(train_X, train_y)\ngrid_search.best_params_\n", "intent": "**This hyperparameterized-RF classification model with a reduced \nLet's look at the accuracy with the 3 initial variables of interest...\n"}
{"snippet": "url = \"https://www.dropbox.com/s/2qe1mai9eyd4mqa/abstracts1.zip?dl=1\"  \nimport urllib.request\nu = urllib.request.urlopen(url)\ndata = u.read()\nu.close()\nwith open(\"abstracts1.zip\", \"wb\") as f :\n    f.write(data)\n", "intent": "We downloaded ~50k abstracts on various Physics topics from arxiv.\nThe code below helps you to donload zipped abstracts from dropbox and unzip them.\n"}
{"snippet": "def import_data(filename):\n    data = pd.read_csv(filename)\n    return data\n", "intent": "Import training data:\n"}
{"snippet": "def preprocess_names(data):\n    data['Title'] = data.Name.apply(lambda x: re.search(\", (.*?)\\.\", x).group(1))\n    data['NameLength'] = data.Name.apply(len)\n    data = data.drop('Name', axis=1)\n    le = LabelEncoder()\n    data.Title = le.fit_transform(data.Title)\n    return data\n", "intent": "`Name` values include titles. Otherwise, they are unique, so let's drop `Name` and describe it with the title and the length of the name.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(scaled_X, scaled_y, test_size=0.2)\n", "intent": "... and repeat the training\n"}
{"snippet": "diabetes_4top = pd.DataFrame(data=feat, columns=['Glucose', 'Insulin', 'BMI', 'Age'])\n", "intent": "We can see the top 4 performing features are **Glucose**, **Insulin**, **BMI** and **Age**\n"}
{"snippet": "from keras.datasets import cifar10\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n", "intent": "- The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes\n<img src ='imgs/cifar.png'/>\n"}
{"snippet": "df = pd.read_csv('../../data/telecom_churn.csv')\n", "intent": "In the first article, we looked at the data on customer churn for a telecom operator. We will load again that dataset into a `DataFrame`:\n"}
{"snippet": "wine = pd.read_csv('./datasets/winequality_merged.csv')\nwine.head(2)\n", "intent": "**Load the wine dataset and pull out red vs. white as the true clusters.**\n"}
{"snippet": "boston = learn.datasets.load_dataset('boston')\nx, y = boston.data, boston.target\ny.resize( y.size, 1 ) \ntrain_x, test_x, train_y, test_y = cross_validation.train_test_split(\n                                    x, y, test_size=0.2, random_state=42)\nprint( \"Dimension of Boston test_x = \", test_x.shape )\nprint( \"Dimension of test_y = \", test_y.shape )\nprint( \"Dimension of Boston train_x = \", train_x.shape )\nprint( \"Dimension of train_y = \", train_y.shape )\n", "intent": "<h2>Import the Boston Data</h2>\n<br />\nWe don't worry about adding column names to the data.\n"}
{"snippet": "scaler = preprocessing.StandardScaler( )\ntrain_x = scaler.fit_transform( train_x )\ntest_x  = scaler.fit_transform( test_x )\n", "intent": "We scale the inputs to have mean 0 and standard variation 1.\n"}
{"snippet": "bostonDF = pd.DataFrame( boston.data )\nbostonDF.head()\n", "intent": "<h2>Convert the boston data into a panda data-frame</h2>\n"}
{"snippet": " X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n      X, y, test_size=0.2, random_state=42)\n", "intent": "Split the data into training and test data.\n"}
{"snippet": "scaler = preprocessing.StandardScaler( )\nX_train = scaler.fit_transform( X_train )\nX_train\n", "intent": "Scale the X data to 0 mean and unit standard deviation\n"}
{"snippet": "scaler = MinMaxScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "We use Sklearn's MinMaxScaler to normalize our data between 0 and 1\n"}
{"snippet": "y_all = LabelEncoder().fit_transform(y_all)\ny_all = np_utils.to_categorical(y_all)\nX_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, \n                                                    test_size=0.2, random_state=23, \n                                                    stratify=y_all)\n", "intent": "One-Hot_encode the labels, then create a stratified train/validattion split.\n"}
{"snippet": "from sklearn.datasets import load_iris, load_breast_cancer, load_wine\niris = load_iris()\ncancer = load_breast_cancer()\nwine = load_wine()\nprint(type(iris))\n", "intent": "load iris, breast cancer and wine standard datasets and assign them to iris, cancer and wine.\n"}
{"snippet": "df = pd.read_csv('../../data/medium_posts.csv', sep='\\t')\n", "intent": "We will predict the daily number of posts published on [Medium](https://medium.com/).\nFirst, we load our dataset:\n"}
{"snippet": "x = StandardScaler().fit_transform(df)\nX = pd.DataFrame(x, columns = wine.feature_names )\nX.head()\n", "intent": "* __Standardize the dataset__\n"}
{"snippet": "y = pd.DataFrame(wine.target, columns =['class'])\n", "intent": "* Perform using Singular Value Decomposition (**SVD**)\n* For simplicity chose two components\n* Implement first for wine dataset\n"}
{"snippet": "x = StandardScaler().fit_transform(df)\nX = pd.DataFrame(x, columns =wine.feature_names )\nX.head()\n", "intent": "* __Standardize the dataset__\n"}
{"snippet": "from sklearn import datasets, linear_model\ndiabetes = datasets.load_diabetes()\nprint(diabetes.DESCR)\n", "intent": "Let's now visit regression, which is a linear least squares problem, using the diabetes dataset that is part of Scikit-learn.  \n"}
{"snippet": "from sklearn.decomposition import PCA\nX = np.array( [ [1.0,0], [0,1.0], [1,1]])\npca = PCA(n_components=2) \npca.fit(X)\nprint(\"singular values\")\nprint(pca.singular_values_)  \nprint(\"explained variance ratio\")\nprint(pca.explained_variance_ratio_) \n", "intent": "Lets use sci-kit learn to find the PCA decomposition first\n"}
{"snippet": "m_cols = ['movie_id', 'title', 'release_date']\nmovie_info = pd.read_csv('data/ml-100k/movie-info.dat', sep='|', names=m_cols, usecols=range(3))\nmovie_info.head()\n", "intent": "Let's see which are the most highly rated movies that we can recommend.  First, let's load the movie information into another dataframe\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(min_df=1)\nposts = [post1, post2, post3, post4, post5]\nX_train = vectorizer.fit_transform(posts)\n", "intent": "Now we will train our vectorizer\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components = 2) \nX2D = pca.fit_transform(X)\nprint(pca.components_[0]) \nprint(pca.components_.T[:,0]) \n", "intent": "* Uses SVD decomposition as before.\n* You can access each PC using *components_* variable. (\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_mldata\nmnist = fetch_mldata('MNIST original')\nX, y = mnist[\"data\"], mnist[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nX = X_train\npca = PCA()\npca.fit(X)\nd = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1\nd\n", "intent": "* Example applying PCA to MNIST dataset with 95% preservation = results in ~150 features (original = 28x28 = 784)\n"}
{"snippet": "max_users = 70\nmax_movies = 50\nclustered = pd.concat([most_rated_movies_1k.reset_index(), pd.DataFrame({'group':predictions})], axis=1)\nhelper.draw_movie_clusters(clustered, max_users, max_movies)\n", "intent": "To visualize some of these clusters, we'll plot each cluster as a heat map:\n"}
{"snippet": "rnd_pca = PCA(\n    n_components=154, \n    random_state=42, \n    svd_solver=\"randomized\")\nX_reduced = rnd_pca.fit_transform(X_mnist)\n", "intent": "* Stochastic algorithm, quickly finds approximation of 1st d components. Dramatically faster.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfTransformer\ntf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\nX_train_tf = tf_transformer.transform(X_train_counts)\nX_train_tf.shape\n", "intent": "From occurrences to frequencies\n"}
{"snippet": "tfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nX_train_tfidf.shape\n", "intent": "Or we can call fit and transform using fit_transform\n"}
{"snippet": "scaler = StandardScaler()\nsvm_clf1 = LinearSVC(C=1, loss=\"hinge\")\nscaled_svm_clf1 = Pipeline([\n(\"scaler\", scaler),\n    (\"linear_svc\", svm_clf1)\n])\nscaled_svm_clf1.fit(X,y)\n", "intent": "Step 2: Do feature scaling of the features using StandardScaler() and model the SVM Linear classifier\n"}
{"snippet": "doc = nlp('I get a discount on newspapers')\ntags = {}\nfor word in doc:\n    tags[word.orth_] = {'lemma': word.lemma_, 'pos (coarse)': word.pos_, 'pos (fine)':word.tag_}\npd.DataFrame(tags).T\n", "intent": "<a name=\"section2\"></a>\nLets see how we can access parts of speech with spacy:\n"}
{"snippet": "pd.DataFrame(np.transpose([X.columns, lm.coef_]), columns=[\"Variables\", \"Coefficients\"])\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "backorder_test=pd.read_csv(\"Kaggle_Test_Dataset_v2.csv\")\ntest = (backorder_test\n              .replace(['Yes', 'No'], [1, 0]))               \n", "intent": "Apply ML model to the test set\n"}
{"snippet": "df_d= pd.get_dummies(df[['age','job','education','day_of_week','y']], drop_first = True)\nLR=LogisticRegression()\nX1 = df_d.drop('y', axis =1)\ny1 = df_d['y']\nx_train1, x_test1, y_train1, y_test1 = train_test_split(X1,y1, random_state =42)\nLR.fit(x_train1, y_train1)\n", "intent": "**Build a Model**  \n"}
{"snippet": "data_raw = pd.read_csv(\"datasets/titanic_train.csv\", index_col='PassengerId')\ndata_validate = pd.read_csv(\"datasets/titanic_test.csv\", index_col='PassengerId')\ndata_raw.sample(10)\n", "intent": "Let's take a view into the dataset itself.\n"}
{"snippet": "cluster.fillna('').head()\n", "intent": "And the actual ratings in the cluster look like this:\n"}
{"snippet": "features_train, features_test, labels_train, labels_test = train_test_split(data_features, data_labels,\n                                                                            test_size=0.2, random_state=42)\n", "intent": "Splitting up the labels and features into training and testing sets.\n"}
{"snippet": "import pandas as pd\nseeds_df = pd.read_csv('../datasets/seeds.csv')\nseeds_df\n", "intent": "**Step 1:** Load the dataset _(written for you)_.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../datasets/fish.csv')\ndf.head()\n", "intent": "**Step 1:** Load the dataset _(this bit is written for you)_.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../datasets/fish.csv')\ndf.head()\n", "intent": "**Step 1:** Load the dataset, extracting the species of the fish as a list `species` _(done for you)_\n"}
{"snippet": "data = pd.read_csv(\"datasets/titanic_train.csv\", index_col='PassengerId')\ndata_validate = pd.read_csv(\"datasets/titanic_test.csv\", index_col='PassengerId')\n", "intent": "Let's take a view into the dataset itself.\n"}
{"snippet": "data.loc[:,'Embarked']=data.Embarked.map({'S':0,'C':1,'Q':2})\ndata.loc[:,'Embarked']=data.Embarked.fillna(0)\ndata.loc[:,'Embarked']=data.Embarked.astype('int')\n", "intent": "There's a couple missing values for Embarked. We'll use the most common value (S/0). \n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nimport sklearn.metrics.pairwise as smp\nimport numpy as np\nX_train, X_test, y_train, y_test = train_test_split(X, ng_train.target, test_size=0.3)\n", "intent": "Let's try some simple classification on the result LSI vectors for the 20 NG set and see how we do:\n"}
{"snippet": "for dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')    \nprint (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())\n", "intent": "the embarked feature has some missing value. and we try to fill those with the most occurred value ( 'S' ).\n"}
{"snippet": "for dataset in full_data: \n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())    \ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)    \nprint (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())\n", "intent": "Fare also has some missing value and we will replace it with the median. then we categorize it into 4 ranges.\n"}
{"snippet": "import pandas as pd\nurl = '../data/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer\n", "intent": "1. K-means clustering\n2. Clustering evaluation\n3. DBSCAN clustering\n"}
{"snippet": "coefs = pd.DataFrame(coef_CV_lambda,columns=range(1,10),index=np.arange(.0001,.01,.0001))\nprint(coefs.loc[lambda_opt])\n", "intent": "Parameters for optimal lambda\n"}
{"snippet": "token_text = [token.orth_ for token in parsed_review]\ntoken_pos = [token.pos_ for token in parsed_review]\npd.DataFrame(zip(token_text, token_pos),\n             columns=['token_text', 'part_of_speech'])\n", "intent": "What about part of speech tagging?\n"}
{"snippet": "token_lemma = [token.lemma_ for token in parsed_review]\ntoken_shape = [token.shape_ for token in parsed_review]\npd.DataFrame(zip(token_text, token_lemma, token_shape),\n             columns=['token_text', 'token_lemma', 'token_shape'])\n", "intent": "What about text normalization, like stemming/lemmatization and shape analysis?\n"}
{"snippet": "token_entity_type = [token.ent_type_ for token in parsed_review]\ntoken_entity_iob = [token.ent_iob_ for token in parsed_review]\npd.DataFrame(zip(token_text, token_entity_type, token_entity_iob),\n             columns=['token_text', 'entity_type', 'inside_outside_begin'])\n", "intent": "What about token-level entity analysis?\n"}
{"snippet": "token_text = [token.orth_ for token in parsed_review]\ntoken_pos = [token.pos_ for token in parsed_review]\npd.DataFrame({'token_text':token_text,'part_of_speech':token_pos})\n", "intent": "What about part of speech tagging?\n"}
{"snippet": "token_lemma = [token.lemma_ for token in parsed_review]\ntoken_shape = [token.shape_ for token in parsed_review]\npd.DataFrame({'token_text':token_text,'token_lemma':token_lemma,'token_shape':token_shape})\n", "intent": "What about text normalization, like stemming/lemmatization and shape analysis?\n"}
{"snippet": "token_entity_type = [token.ent_type_ for token in parsed_review]\ntoken_entity_iob = [token.ent_iob_ for token in parsed_review]\npd.DataFrame({'token_text':token_text,'entity_type':token_entity_type,'inside_outside_begin':token_entity_iob})\n", "intent": "What about token-level entity analysis?\n"}
{"snippet": "partA_loc = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\npartA = pd.read_csv(partA_loc)\npartA.head() \n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "pca = PCA(n_components=50)\nX_train_pca = pca.fit_transform(X_train_robust, y = y_train) \nX_val_pca = pca.transform(X_val_robust)\nsvc_radial_basis = SVC(kernel='rbf', random_state = 0)\nsvc_radial_basis.fit(X_train_pca, y_train)\nprint('RBF accuracy - validation set: {:.5f}'.format(svc_radial_basis.score(X_val_pca, y_val)))\n", "intent": "<h2>SVC attempt with PCA Dimensionality Reduction</h2>\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape\n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "labels.pivot_table(\n    index = 'cluster',\n    aggfunc = 'count'\n)\n", "intent": "Let's see how many stations in each cluster\n"}
{"snippet": "token_list=word_tokenize(book.read())\n", "intent": "and create the list of tokens\n"}
{"snippet": "corpus = \" \".join([book.read() for book in books])\n", "intent": "and we put them together in a large corpus\n"}
{"snippet": "train_sessions = train_dataset[site_cols].astype(str).apply(lambda s: ' '.join(s), axis=1)\ntest_sessions = test_dataset[site_cols].astype(str).apply(lambda s: ' '.join(s), axis=1)\nvec = TfidfVectorizer(ngram_range=(1, 6), max_features=200000, stop_words=['0'])\nvec = vec.fit(test_sessions.append(train_sessions))\ntrain_v = vec.transform(train_sessions)\ntest_v = vec.transform(test_sessions)\n", "intent": "Build Tf-Idf features based on sites. You can use `ngram_range`=(1, 3) and `max_features`=100000 or more\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\nlabel = LabelEncoder()\nX[:,0] = label.fit_transform(X[:,0])\nX\n", "intent": "<img src=\"images/ed.PNG\"/>\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ndummy_features = ohe.fit_transform(np.r_[y_train, y_test].reshape(-1, 1))\ny_train, y_test = dummy_features[:len(y_train)], dummy_features[len(y_train):]\n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "dat = pd.read_csv(\"datasets/regionalhappy.csv\")\n", "intent": "Load Data\n=========\nNext, let's load the data.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    customer_data.drop(['BikeBuyer'], axis=1),\n    customer_data['BikeBuyer'],\n    test_size=0.3)\n", "intent": "The first step is to split the data into training and testing divisions.\n"}
{"snippet": "nr.seed(1115)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 300)\nX_train = Features[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nX_test = Features[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])\n", "intent": "Now, execute the code in the cell below to create training and testing splits of the dataset. \n"}
{"snippet": "vect = TfidfVectorizer(stop_words='english')\ndtm = vect.fit_transform(yelp.text)\nfeatures = vect.get_feature_names()\ndtm.shape\n", "intent": "**More details:** [TF-IDF is about what matters](http://planspace.org/20150524-tfidf_is_about_what_matters/)\n"}
{"snippet": "nr.seed(1115)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 300)\nx_train = Features[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nx_test = Features[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])\n", "intent": "Now, execute the code in the cell below to creat training and testing splits of the dataset. \n"}
{"snippet": "pca_mod = skde.PCA()\npca_comps = pca_mod.fit(x_train)\npca_comps\n", "intent": "The code in the cell below computes the principle components for the training feature subset. Execute this code:\n"}
{"snippet": "pd.DataFrame(lm.coef_, X.columns, columns=['Coefficients'])\n", "intent": "That was almost too easy. Let's take a look at the coefficents of the model.\n"}
{"snippet": "pd.DataFrame(lm.coef_, X.columns, columns=['Coefficients'])\n", "intent": "The question was whether company resources should be biased towards the website or the app. Recall this table of the coefficients:\n"}
{"snippet": "X = data.drop(['Clicked on Ad', 'Timestamp'], axis=1)\ny = data['Clicked on Ad']\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n", "intent": "Begin by splitting the data into training and testing chunks.\n"}
{"snippet": "anon = pd.read_csv('classified.csv')\nanon.head()\n", "intent": "The file `classified.csv` contains the anonymized data that we will use. Import it as a pandas `DataFrame` and look at the head\n"}
{"snippet": "anon = pd.read_csv('classified.csv', index_col=0)\nanon.head()\n", "intent": "It looks like the first column is supposed to be the index. Let's try importing it again while setting that first column as the index.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(scaled_anon, anon['TARGET CLASS'], test_size=0.3)\n", "intent": "Developing a KNN model requires a train-test split, just like any other.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df.drop('Kyphosis', axis=1)\ny = df['Kyphosis']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "In this toy dataset we will not be doing any fancy resampling methods.\n"}
{"snippet": "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/titanic.csv'\ntitanic = pd.read_csv(url)\ntitanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\ntitanic.Age.fillna(titanic.Age.median(), inplace=True)\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\nembarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\ntitanic = pd.concat([titanic, embarked_dummies], axis=1)\ntitanic.head()\n", "intent": "We'll build a classification tree using the Titanic data:\n"}
{"snippet": "cancer = load_breast_cancer()\n", "intent": "The breast cancer that is included with Scikit-learn is ideal for this demonstration.\n"}
{"snippet": "df = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n", "intent": "We can work with the data more easily if we turn it into a pandas `DataFrame` object.\n"}
{"snippet": "pca = PCA(n_components=2)\npca.fit(scaled_data)\npc = pca.transform(scaled_data)\n", "intent": "PCA in Scikit-learn functions very similarly to the standard scaler and other machine learning models.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nsms_train, sms_test, label_train, label_test = \\\n    train_test_split(sms['message'], sms['label'], test_size=0.2)\nlengths = (len(x) for x in (sms_train, sms_test, label_train, label_test))\nprint(tuple(lengths))\n", "intent": "OK, you know that we should have done this step earlier, but whatever.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nbag_transformer = CountVectorizer(stop_words='english')\n", "intent": "In order to make use of the text data, we must vectorize it. Import a `CountVectorizer` and use it to transform `X` into a bag of words.\n"}
{"snippet": "import pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\ndf = pd.read_csv('iris.csv')\n", "intent": "We return the iris dataset for this exercise.\n"}
{"snippet": "def bar_chart(feature): \n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "df = pd.read_csv('ISL_Fig_2_9_data.csv', index_col=0)\n", "intent": "But now the concept is *classification*, not *regression*.\n"}
{"snippet": "df = pd.read_csv('RWA_DHS6_2010_2011_HH_ASSETS.CSV', index_col=0)\n__________________\n", "intent": "Load, clean, and prepare DHS asset ownership data:\n"}
{"snippet": "def probability_to_rank(prediction, scaler=1):\n    pred_df=pd.DataFrame(columns=['probability'])\n    pred_df['probability']=prediction\n    pred_df['rank']=pred_df['probability'].rank()/len(prediction)*scaler\n    return pred_df['rank'].values\n", "intent": "**5.1.2. Xgboost K-fold & OOF function**\nWork in progress\n"}
{"snippet": "df = pd.read_csv('IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv')\n", "intent": "Load and clean PHMRC VA data:\n"}
{"snippet": "df['Cause'] = df.gs_text34.map({'Stroke':'Stroke', 'Diabetes':'Diabetes'}).fillna('Other')\ny = np.array(df.Cause)\n", "intent": "For class purposes, we will simplify the prediction task: was the death due to stroke, diabetes, or something else?\n"}
{"snippet": "df = pd.read_csv('IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv', low_memory=False)\n", "intent": "What do you think we should do about that warning?\n"}
{"snippet": "df = pd.read_csv('../Week_4/IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv', low_memory=False)\nX = np.array(df.filter(like='word_'))\ndf['Cause'] = df.gs_text34.map({'Stroke':'Stroke', 'Diabetes':'Diabetes'}).fillna('Other')\ny = np.array(df.Cause)\nimport sklearn.tree\nweights = 1000. / df.Cause.value_counts()\nsample_weight = np.array(weights[y])\n", "intent": "We used gini; let's check if entropy is any different!\n"}
{"snippet": "def my_transform(X, phases=[0,.5], freqs=[1., 2.]):\n", "intent": "How about if you know that this is some sort of periodic function?\n"}
{"snippet": "df = pd.read_csv('weather-numeric.csv')\ndf\n", "intent": "Or, since H: drive is preventing me from loading that into Sage Cloud, let's look at the good, old weather data from Week 1 of class:\n"}
{"snippet": "pca = sklearn.decomposition.PCA()\nXt = pca.fit_transform(X)\n", "intent": "How long does this take?\n"}
{"snippet": "X = np.array(df.filter(like='X_'))\npca = sklearn.decomposition.PCA()\nXt = pca.fit_transform(X)\n", "intent": "What will happen now?\n"}
{"snippet": "pca = sklearn.decomposition.PCA()\nXt = pca.fit_transform(X)\n", "intent": "Have a look at plain, old PCA on this data:\n"}
{"snippet": "train = pd.read_csv(\"../input/train.csv\")\ntest    = pd.read_csv(\"../input/test.csv\")\nfull = train.append( test , ignore_index = True )\ntitanic = full[ :891 ]\ndel train , test\nprint ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)\n", "intent": "Now that our packages are loaded, let's read in and take a peek at the data.\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "rte = sklearn.ensemble.RandomTreesEmbedding()\nXt = rte.fit_transform(X)\nXt\n", "intent": "Read more, if interested: http://scikit-learn.org/stable/modules/ensemble.html\n"}
{"snippet": "pca = sklearn.decomposition.PCA()\nXt = pca.fit_transform(X)\n", "intent": "We can use PCA to look at it:\n"}
{"snippet": "df = pd.read_csv('../Week_4/IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv', low_memory=False)\n", "intent": "An example from Verbal Autopsy: symptom duration\n"}
{"snippet": "df = pd.read_csv('../Week_4/IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv', low_memory=False)\n", "intent": "A sketch of the ways with the VA data:\n"}
{"snippet": "df = pd.read_csv('/homes/abie/ML4HM/Week_4/IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv', low_memory=False)\n", "intent": "An example from Verbal Autopsy: symptom duration\n"}
{"snippet": "df = pd.read_csv('/homes/abie/ML4HM/Week_4/IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv', low_memory=False)\n", "intent": "A sketch of the ways with the VA data:\n"}
{"snippet": "(x_train, _), (x_test, _) = mnist.load_data()\nprint(\"The shape of x_train dataset is\", x_train.shape)\n", "intent": "Downloading the MNIST data\n"}
{"snippet": "from sklearn import datasets\nX, _ = datasets.make_circles(n_samples=150, factor=.5, noise=.05)\nplot(X[:,0], X[:,1], 'bx')\n", "intent": "Test the different clustering approaches with a \"circles\" dataset.\n"}
{"snippet": "from sklearn.feature_extraction import text\nvectorizer = text.CountVectorizer(max_df=0.8, max_features=10000, stop_words=text.ENGLISH_STOP_WORDS, ngram_range=(1,2))\ncounts = vectorizer.fit_transform(dataset.data)\ntfidf = text.TfidfTransformer().fit_transform(counts)\n", "intent": "Se diamo il numero di ngrams tra 1 e 2 abbiamo anche le coppie di parole\n"}
{"snippet": "cabin = pd.DataFrame()\ncabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\ncabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\ncabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\ncabin.head()\n", "intent": "*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntrain,test=train_test_split(df,test_size=0.2)\nX_train=train[['TV','radio','newspaper']]\ny_train=train['sales']\nX_test=test[['TV','radio','newspaper']]\ny_test=test['sales']\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "import pandas as pd\ndata=pd.read_csv(\"hw2data.csv\")\ndata.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "opt_regr=linear_model.Lasso(alpha=0.001)\nopt_regr.fit(X_train,y_train)\n[round(x,2) for x in range(len(opt_regr.coef_))]\ndata_coeffs=pd.DataFrame({'coeffs':opt_regr.coef_,'name':X_train.columns.values})\ndata_coeffs=data_coeffs.sort_values(['coeffs'])\ndata_coeffs[::-1][0:50].plot(x='name',y='coeffs',kind='bar',figsize=(15,15),color='gray')\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "X_predict = data_predict.drop(columns=['age', 'gender', 'income', 'per_id', 'gender_F',\\\n                                       'gender_M', 'gender_None', 'gender_O', 'gender_enc'])\nX_predict_scaled = pd.DataFrame(ss.transform(X_predict))\nX_predict_scaled.columns = X_predict.columns\nX_predict_scaled.index = X_predict.index\nX_predict_pca = pd.DataFrame(pca.transform(X_predict_scaled))\nX_predict_pca.columns = [\"pca_comp_\" + str(i) for i in range(n_components)]\nX_predict_pca.index = X_predict_scaled.index\n", "intent": "Apply scaling and dimensionality reduction to data with missing values\n"}
{"snippet": "docs_meta = pd.read_csv(\"../download/documents_meta.csv\", usecols=['document_id', 'source_id', 'publisher_id']) \ndocs_meta.count()\n", "intent": "Sources and publishers - Yes!\n"}
{"snippet": "docs_ent = pd.read_csv(\"../download/documents_entities.csv\", usecols=['document_id', 'entity_id'])\ndocs_ent.count()\n", "intent": "Entities - 1 out of 15389 is missing\n"}
{"snippet": "events_CTR_train = pd.read_csv(\"../generated/final/events_CTR_train.csv\")\nevents_CTR_train.count()\n", "intent": "Can be run after feature_base_3\n"}
{"snippet": "page_addoc_dist = pd.read_csv(\"../generated/final/page_addoc_topics_no_w_dist.csv\")\npage_addoc_dist.head()\n", "intent": "Can be run after feature_base_4_topics\n"}
{"snippet": "page_addoc_dist = pd.read_csv(\"../generated/final/page_addoc_cats_dist.csv\")\npage_addoc_dist.head()\n", "intent": "Can be run after feature_base_4_cats\n"}
{"snippet": "family = pd.DataFrame()\nfamily[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\nfamily[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\nfamily[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\nfamily[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\nfamily.head()\n", "intent": "The two variables *Parch* and *SibSp* are used to create the famiy size variable\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "page_addoc_dist = pd.read_csv(\"../generated/final/page_addoc_topics_no_w_dist_test.csv\")\npage_addoc_dist.head()\n", "intent": "Can be run after feature_base_4_topics\n"}
{"snippet": "page_addoc_dist = pd.read_csv(\"../generated/final/page_addoc_cats_dist_test.csv\")\npage_addoc_dist.head()\n", "intent": "Can be run after feature_base_4_cats\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = bikes[\"temp\"]\ny = bikes[\"total_rentals\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(len(X_train), len(X_test))\nprint(len(y_train), len(y_test))\n", "intent": "Step 2 - split your data into training and test sets\n"}
{"snippet": "import pandas as pd\nimport seaborn as sns\nbikes = pd.read_csv(\"assets/data/bikeshare.csv\")\nbikes.rename(columns={\"count\": \"total_rentals\"}, inplace=True)\nbikes.head()\n", "intent": "This is just the code snippets from the slides extracted into an easier-to-read format.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df[predictors]\ny = df[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(len(X_train), len(X_test))\nprint(len(y_train), len(y_test))\n", "intent": "Make sure to:\n- do a train/test split\n- fit your model\n- evaluate your model (with an appropriate metric)\n"}
{"snippet": "from sklearn.model_selection import KFold, train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nseven_fold_cv = KFold(n_splits=7, shuffle=True, random_state=42)\n", "intent": "We'll be using the training set for cross-validation\n"}
{"snippet": "import pandas as pd\nbikes = pd.read_csv(\"../assets/data/bikeshare.csv\")\nbikes.rename(columns={\"count\": \"total_rentals\"}, inplace=True)\n", "intent": "For this example, we will build on the bikes dataset we used last time, this time with more \"best practice\".\nRead in the bikes dataset.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n", "intent": "We'll be using the training set for cross-validation\n"}
{"snippet": "df = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf[\"target\"] = iris.target\ndf[\"target\"] = df[\"target\"].map({idx:name for idx, name in enumerate(iris.target_names)})\ndf.head()\n", "intent": "Making iris a DataFrame\n"}
{"snippet": "train_valid_X = full_X[ 0:891 ]\ntrain_valid_y = titanic.Survived\ntest_X = full_X[ 891: ]\ntrain_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\nprint (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)\n", "intent": "Below we will seperate the data into training and test datasets.\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nvalues = np.array([0, 4.7, 7, 9, 24.4, 58, 100]).reshape(-1, 1)\nmm = MinMaxScaler()\nvalues_normed = mm.fit_transform(values)\nvalues_normed\n", "intent": "- normalise to between 0 and 1 (how?)\n"}
{"snippet": "bikes = pd.read_csv(\"assets/data/bikeshare.csv\")\n", "intent": "- must remember to change interpretation\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.3,\n                                                    random_state=99)\nscaler = StandardScaler()\nX_train_transformed = scaler.fit_transform(X_train)\nX_test_transformed = scaler.transform(X_test)\n", "intent": "- to convert back to original units, you can do it with `StandardScaler`\n"}
{"snippet": "df2 = df_with_dummies\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(df2.drop(\"status_group\", axis=1),\n                                                            df2[\"status_group\"],\n                                                            test_size=0.3,\n                                                            random_state=1,\n                                                            stratify=y)\ndisplay(y_train_2.value_counts() / len(y_train_2))\ndisplay(y_test_2.value_counts() / len(y_test_2))\n", "intent": "Try:\n- standardisation: to make KNN consider features equally\n- stratification: to improve on the \"representativeness\" of your samples\n"}
{"snippet": "from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.3,\n                                                    random_state=44)\n", "intent": "Do the train-test split:\n"}
{"snippet": "df_iris_2 = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf_iris_2[\"target\"] = iris.target\ndf_iris_2[\"target\"] = df_iris_2[\"target\"].map({idx:name for idx, name in enumerate(iris.target_names)})\ndf_iris_2 = df_iris_2[df_iris_2[\"target\"] != \"setosa\"]\n", "intent": "Retrain logistic regression for two harder iris classes\n"}
{"snippet": "X = pd.get_dummies(bank[[\"job\", \"cons.conf.idx\", \"euribor3m\"]], columns=[\"job\"], drop_first=False)\nX.drop([\"job_admin.\"], axis=1, inplace=True)\ny = bank[\"y\"]\nX_train, X_test_2, y_train, y_test_2 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\ngrid = GridSearchCV(LogisticRegression(),\n                    param_grid={\"penalty\": [\"l1\", \"l2\"],\n                                \"C\": np.logspace(-4, 2, 7)},\n                    scoring=\"roc_auc\",\n                    cv=StratifiedKFold(10))\ngrid.fit(X_train, y_train);\n", "intent": "Try job, consumer confidence index, and euribor3m\n"}
{"snippet": "baseball = pd.read_csv(\"assets/data/hitters.csv\")\nbaseball = baseball[baseball[\"Salary\"].isnull() == False]\nbaseball.head()\n", "intent": "This avoids the same tree being constructed each time (which would happen if one feature is \"good\" by our measure of importance)\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df.loc[df[\"stars\"].isin([1, 5]), \"text_stemmed\"]\ny = df.loc[df[\"stars\"].isin([1, 5]), \"stars\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\ny.value_counts()\n", "intent": "Let's start with \"1 star\" vs. \"5 star\" reviews as a binary classification\n"}
{"snippet": "all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n", "intent": "- **MiscFeature** : data description says NA means \"no misc feature\"\n"}
{"snippet": "try_new_vectoriser(CountVectorizer(binary=False,\n                                   stop_words='english',\n                                   min_df=2,\n                                   max_features=1000\n                                  ),\n                   X_train,\n                   y_train)\n", "intent": "Limit to top 1000 most frequent words\n"}
{"snippet": "vectorizer_1000 = CountVectorizer(binary=False,\n                                   stop_words='english',\n                                   min_df=2,\n                                   max_features=1000)\nX_train_text = vectorizer_1000.fit_transform(X_train)\nrf = RandomForestClassifier()\nrf.fit(X_train_text, y_train);\n", "intent": "Fit another Random Forest on the latest model\n"}
{"snippet": "try_new_vectoriser(CountVectorizer(binary=False,\n                                   stop_words='english',\n                                   min_df=2,\n                                   ngram_range=(1, 2)\n                                  ),\n                   X_train,\n                   y_train)\n", "intent": "Let's try mixing **both** words and n-grams\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vec = TfidfVectorizer(stop_words=\"english\",\n                            min_df=2,\n                            max_features=1000)\ntry_new_vectoriser(tfidf_vec,\n                   X_train,\n                   y_train)\n", "intent": "- It's best to think of TF-IDF as a value that measure the importance of a word in a document\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df.loc[:, \"text\"]\ny = df.loc[:, \"airline_sentiment\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\ny.value_counts()\n", "intent": "Do a train-test split so we can test our best algorithm at the end\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df[\"text_cleaned\"],\n                                                    df[\"airline_sentiment\"],\n                                                    test_size=0.3,\n                                                    random_state=42,\n                                                    stratify=df[\"airline_sentiment\"])\n", "intent": "Do a train-test split so we can test our best algorithm at the end\n"}
{"snippet": "import pandas as pd\ncars = pd.read_csv(\"assets/data/cars.csv\")\n", "intent": "Car data (source: [Kaggle](https://www.kaggle.com/abineshkumark/carsdata/version/1))\n"}
{"snippet": "import pandas as pd\nloans = pd.read_csv(\"/Users/tyrone/programming/ga/datascience/18_group_project_2/data/loans.csv.gz\")\nloans.rename(columns={'title':'Loan Title', 'loan_amnt':'Loan Amount'}, inplace=True)\nloans.head(3)\n", "intent": "<b> 1.0 Load and examine the data\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df[['emahist', 'bblower_dist', 'macdhist', 'Volume_BTC_ma6', 'rsi_ma6']]\ny = df[\"target\"]\ny_binary = y.map({\"buy\": 1, \"sell\": 0})\nX_train, X_test, y_train, y_test = train_test_split(X, y_binary, stratify=y, test_size=0.3, random_state=42)\nprint(len(X_train), len(X_test))\n", "intent": "<B> 1.4.1 KNeighborsClassifier\n"}
{"snippet": "all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n", "intent": "- **Alley** : data description says NA means \"no alley access\"\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df[[\"alcohol\",\"density\", \"chlorides\"]]\ny = df[\"quality\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(len(X_train), len(X_test))\nprint(len(y_train), len(y_test))\n", "intent": "Remember: we want to avoid testing on the final test set until the end.\n"}
{"snippet": "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nX = df_white.drop(\"quality\", axis=1)\ny = df_white[\"quality\"]\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.3,\n                                                    random_state=42)\nprint(len(X_train), len(X_test))\n", "intent": "Remember: we want to avoid testing on the final test set until the end.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nX = df[[\"fixed acidity\", \"chlorides\", \"total sulfur dioxide\"]]\ny = df[\"colour\"]\ny_binary = y.map({\"white\": 0, \"red\": 1})\nX_train, X_test, y_train, y_test = train_test_split(X, y_binary, stratify=y, test_size=0.3, random_state=42)\n", "intent": "Using the make-up of classes investigated above as an indication - should you or shouldn't you stratify your samples in the train-test split?\n"}
{"snippet": "import cv2\nimg = cv2.imread('data/python-cv/empire.jpg')\ndetector = cv2.FeatureDetector_create('SIFT')\nextractor = cv2.DescriptorExtractor_create('SIFT')\nkeypoints = detector.detect(img)\ndescriptions = extractor.compute(img, keypoints)\n", "intent": "***SIFT Descriptor***\n"}
{"snippet": "import pandas as pd\nimport pandas.tools.rplot as rplot\ntips = pd.read_csv('data/tips.csv')\nprint tips.head()\nprint \"any missing values:\", any(pd.isnull(tips))\n", "intent": "- RPlot is a flexible API for producing Trellis plots. These plots allow you to arrange data in a rectangular grid by values of certain attributes.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nn_samples, n_features = X.shape\ntrain_index, test_index = train_test_split(range(n_samples), test_size = 0.2)\ndata_records = []\ndata_names = []\n", "intent": "***FEATURE and DATA ENGINEERING***\n"}
{"snippet": "sf = SparseFilter(n_features=50, n_iterations=1000)\nsf_X = sf.fit_transform(X)\npca = PCA(n_components=15)\npca_X = pca.fit_transform(X)\nsf_pca_X = np.c_[sf_X, pca_X]\nss = StandardScaler()\nnorm_sf_pca_X = ss.fit_transform(sf_pca_X)\nprint norm_sf_pca_X.shape\n", "intent": "- Use X, y, train_index, test_index\n"}
{"snippet": "X_harafull_train, X_harafull_test, y_train, y_test = train_test_split(X_hara_full, y, \n                                                                      random_state = 0)\nprint X_harafull_train.shape, X_harafull_test.shape\nprint y_train.shape, y_test.shape\n", "intent": "hara full performance\n"}
{"snippet": "ss = StandardScaler()\nX_harasurf_norm = ss.fit_transform(X_harasurf_full)\n", "intent": "normalization (not sparse anymore) + sgd\nIt runs much faster than SVC and achieves comparable result - good features go a long way\n"}
{"snippet": "all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n", "intent": "- **Fence** : data description says NA means \"no fence\"\n"}
{"snippet": "import pandas as pd\npath = 'material/yelp.csv'\nyelp = pd.read_csv(path)\n", "intent": "First, we read **`yelp.csv`** into a pandas DataFrame and examine it.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(150)\npca.fit(faces.data)\n", "intent": "Similarly to the lecture we will apply a PCA dimensionality reduction to 150 dimensions: \n"}
{"snippet": "vect = CountVectorizer(ngram_range=(1, 2))\n", "intent": "n-grams concatenate n words to form a token. The following accounts for 1- and 2-grams\n"}
{"snippet": "tfidf = TfidfTransformer(use_idf=True, smooth_idf=False, norm='l2')\ntfidf.fit_transform(tf).toarray()[-1][:3]\n", "intent": "For example, we would normalize our 3rd document `'The sun is shining and the weather is sweet'` as follows:\n"}
{"snippet": "tfidf = TfidfTransformer(use_idf=True, smooth_idf=True, norm='l2')\ntfidf.fit_transform(tf).toarray()[-1][:3]\n", "intent": "$$\\text{idf}(t) = log{\\frac{1 + n_d}{1+\\text{df}(d,t)}} + 1$$ \n"}
{"snippet": "token_text = [token.orth_ for token in parsed_review]\ntoken_pos = [token.pos_ for token in parsed_review]\npd.DataFrame(list(zip(token_text, token_pos)), columns=['token_text', 'part_of_speech'])\n", "intent": "What about part of speech tagging?\n"}
{"snippet": "ordered_vocab = [(term, voc.index, voc.count)\n                 for term, voc in food2vec.wv.vocab.items()]\nordered_vocab = sorted(ordered_vocab, key=lambda x: -x[2])\nordered_terms, term_indices, term_counts = zip(*ordered_vocab)\nword_vectors = pd.DataFrame(food2vec.wv.syn0norm[term_indices, :],\n                            index=ordered_terms)\nword_vectors\n", "intent": "Let's take a look at the word vectors our model has learned.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "Let's demonstrate the naive approach to validation using the Iris data, which we saw previously:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=42)\n", "intent": " We split the data into a training and testing set:\n"}
{"snippet": "all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n", "intent": "- **FireplaceQu** : data description says NA means \"no fireplace\"\n"}
{"snippet": "user_interaction_df = pd.read_csv(user_interaction_path)\nuser_interaction_results_df = pd.read_csv(user_interaction_results_path)\n", "intent": "Load data into dataframes:\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n", "intent": "* Cross Validation \n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\ndef one_hot_dataframe(data, cols, replace=False):\n    enc = OneHotEncoder()\n    e = enc.fit_transform(data[cols].values)\n    vecData = pd.DataFrame(e.toarray())\n    if replace is True:\n        data = data.drop(cols, axis=1)\n        data = data.join(vecData)\n    return (data, vecData)\n", "intent": "Now we can start playing with the features...\n"}
{"snippet": "df = pd.read_csv(\"yield.csv\",sep=\"\\t\")\ndf.head()\n", "intent": "data from - https://onlinecourses.science.psu.edu/stat501/node/325\n"}
{"snippet": "X_test = pd.DataFrame([[3, 5, 4, 2], [5, 4, 3, 2]], columns=iris.feature_names)\nX_test.head()\n", "intent": "**step3:** Test the Classifier on new data\n"}
{"snippet": "data=data[['Adj. Close','High_Low_Change','Change_Perc','Adj. Volume']]\ndata.fillna(data.mean(),inplace=True)\n", "intent": "** Now, let's make our new dataframe: **\n"}
{"snippet": "from sklearn import cross_validation\nX_train,X_test,y_train,y_test=cross_validation.train_test_split(X,y,test_size=0.3)\n", "intent": "**splitting our data as usual+cross-validation: **\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(features, target,train_size=0.6,random_state=0)\n", "intent": "**Having encoded our categorical values, we are now ready to build our model**\n"}
{"snippet": "nino = pd.read_csv('../data/tao-all2.dat.gz', sep=' ', names=names, na_values='.', \n                   parse_dates=[[1,2,3]])\nnino.columns = [x.replace('.', '_').replace(' ', '_') for x in nino.columns]\nnino['air_temp_F'] = nino.air_temp_ * 9/5 + 32\nwind_cols = [x for x in nino.columns if x.endswith('winds')]\nfor c in wind_cols:\n    nino['{}_mph'.format(c)] = nino[c] * 2.237\npd.to_datetime(nino.date, format='%y%m%d')\nnino = nino.drop('obs', axis=1)\nnino['year'] = nino.year_month_day.dt.year\n", "intent": "* Using the nino dataset, see if you can predict what the temperature (``air_temp_F``) will be for the next day \n"}
{"snippet": "for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n", "intent": "- **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\nfrom pprint import pprint\ncats = ['rec.sport.baseball', 'sci.electronics', 'misc.forsale']\ntrain_data = fetch_20newsgroups(subset='train', categories=cats)\n", "intent": "In this lab, we consider the 20 newsgroups text dataset from [scikit-learn](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html).\n"}
{"snippet": "year = '2015'\ncols = ['Country Name', year]\ngdp = pandas.read_csv('../datasets/API_NY.GDP.MKTP.CD_DS2_en_csv_v2.csv',\n                      index_col=0, skiprows=4, usecols=cols)\ngdp.rename(columns={year : 'GDP'}, inplace=True);\ngdp.head()\n", "intent": "Let us consider the following prediction problem :\n* $y$ : Gross national product (in dollars)\n* $x_1$ : Population size\n* $x_2$ : Literacy rate\n"}
{"snippet": "smarket = pd.read_csv('../datasets/Smarket.csv', index_col=0, parse_dates=True)\nsmarket.head()\n", "intent": "We consider the *Standard & Poor's 500* (S&P) stock index over a 5 year period :\n"}
{"snippet": "df3 = pd.read_csv('https://github.com/ApoorvP02121996/Sentiment-Analysis---Movie-Reviews/raw/master/Naive%20Bayes/training_set.csv')\ndf3.columns = ['target', 'text']\nprint (df3.shape)\ndf3.head()\n", "intent": "Dataset source: https://github.com/ApoorvP02121996/Sentiment-Analysis---Movie-Reviews/blob/master/Naive%20Bayes/training_set.csv\n"}
{"snippet": "X.fillna(0, inplace = True)\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n", "intent": "First, I will try to apply default random forest regressor to the training and testing dataframes.\n"}
{"snippet": "def imp_df(column_names, importances):\n    df = pd.DataFrame({'feature': column_names,\n                       'feature_importance': importances}) \\\n           .sort_values('feature_importance', ascending = False) \\\n           .reset_index(drop = True)\n    return df\ndef var_imp_plot(imp_df, title):\n    imp_df.columns = ['feature', 'feature_importance']\n    sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df, orient = 'h', color = 'royalblue') \\\n       .set_title(title, fontsize = 20)\n", "intent": "I define a few helper functions to make analysis more convenient and presentable.\n"}
{"snippet": "weights=[3, 1, 1, 3]\nprofessional_id = 'd7f9afe721af42b1a03a993909e0568c'\nrecommendation, score = recommend_questions_for_professional(professional_id, weights=weights)\npd.DataFrame(recommendation)\n", "intent": "Professional with already 60 answered questions.\n"}
{"snippet": "weights=[0, 5, 1/5, 3]\nprofessional_id = 'ea75c5fce38348e0a151c3c346929e6a'\nrecommendation, score = recommend_questions_for_professional(professional_id, weights=weights)\npd.DataFrame(recommendation)\n", "intent": "A new Professional without any answered questions. The recommendation is based on the hashtags.\n"}
{"snippet": "stats = tuner.analytics().dataframe()              \nstats = stats.nsmallest(2, 'FinalObjectiveValue')   \nstats.head()\n", "intent": "First, let's figure out what the top 2 jobs are.\n"}
{"snippet": "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n", "intent": "- **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n"}
{"snippet": "scaler = StandardScaler()\nX_tr_ = scaler.fit_transform(X_tr)\nX_va_ = scaler.transform(X_va)\neval_clf(X_tr_, y_tr, X_va_)\n", "intent": "Scaling improves significantly all model but Decision Tree has lower F1 score.\n"}
{"snippet": "X_ba, y_ba = balance(X_tr, y_tr)\nscaler = StandardScaler()\nX_ba_ = scaler.fit_transform(X_ba)\nX_va_ = scaler.transform(X_va)\neval_clf(X_ba_, y_ba, X_va_)\n", "intent": "Both preprocessing procedures (balancing and scaling) are good from Decision Tree, 3NN, Neural Net but not for Naive Bayes model.\n"}
{"snippet": "data = pandas.read_csv(data_file, index_col='Id')\ndata = data.drop(['Unnamed: 0'], axis=1)\ndata.info()\ndata.head()\n", "intent": "Drop 'Unnamed: 0' columns and set 'Id' as index.\n"}
{"snippet": "X = df_tr.drop(['SalePrice'], axis=1)\nX_te = df_te.drop(['SalePrice'], axis=1)\npca = PCA()\nX = pca.fit_transform(X)\nX_te = pca.transform(X_te)\n", "intent": "Use PCA to transform features. Hereafter the feature obtained from PCA will be used.\n"}
{"snippet": "df_tr, df_te = train_test_split(data, test_size=0.25, random_state=17)\nX = df_tr.drop(['SalePrice'], axis=1)\nX_te = df_te.drop(['SalePrice'], axis=1)\npca = PCA()\nX = pca.fit_transform(X)\nX_te = pca.transform(X_te)\n", "intent": "select as less as possible features\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = pd.read_csv(\"housing_price_univariate.csv\")\ndata.head()\n", "intent": "***\n- Now, let's have a look at the data \n - Every row displays the Price and Area of each house\n"}
{"snippet": "data = pd.read_csv('https://raw.githubusercontent.com/madmashup/targeted-marketing-predictive-engine/master/banking.csv', header=0)\ndata = data.dropna()\nprint(data.shape)\nprint(list(data.columns))\n", "intent": "This dataset provides the customer information. It includes 41188 records and 21 fields.\n"}
{"snippet": "my_data = pd.read_csv(\"drug200.csv\", delimiter=\",\")\nmy_data[0:5]\n", "intent": "now, read data using pandas dataframe:\n"}
{"snippet": "df = pd.read_csv('../Datasets/loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv**\n"}
{"snippet": "for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n", "intent": "- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\n", "intent": "Okay, we split our dataset into train and test set:\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target\n", "intent": "Example 1 - Simple Stacked Classification\n"}
{"snippet": "questions = pd.read_csv(\"socialmedia_relevant_cols_clean.csv\")\nquestions.columns=['text', 'choose_one', 'class_label']\nquestions.head()\n", "intent": "It looks solid, but we don't really need urls, and we would like to have our words all lowercase (Hello and HELLO are pretty similar for our task)\n"}
{"snippet": "data = pd.read_csv('../data/training.csv')\n", "intent": "Download the training & test data from the Practice Problem approach. We'll do a bit of quick investigation on the dataset:\n"}
{"snippet": "X_train['Dependents'] = X_train['Dependents'].fillna('0')\nX_train['Self_Employed'] = X_train['Self_Employed'].fillna('No')\nX_train['Loan_Amount_Term'] = X_train['Loan_Amount_Term'].fillna(X_train['Loan_Amount_Term'].mean())\n", "intent": "We'll compile a list of `pre-processing` steps that we do on to create a custom `estimator`.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(data[pred_var], data['Loan_Status'], \\\n                                                    test_size=0.25, random_state=42)\n", "intent": "To make sure that this works, let's do a test run for it:\n"}
{"snippet": "pred_var = ['Gender','Married','Dependents','Education','Self_Employed','ApplicantIncome','CoapplicantIncome',\\\n            'LoanAmount','Loan_Amount_Term','Credit_History','Property_Area']\nX_train, X_test, y_train, y_test = train_test_split(data[pred_var], data['Loan_Status'], \\\n                                                    test_size=0.25, random_state=42)\n", "intent": "- Next step is creating `training` and `testing` datasets:\n"}
{"snippet": "import pandas as pd\ntest_df = pd.read_csv('../data/test.csv', encoding=\"utf-8-sig\")\ntest_df = test_df.head()\n", "intent": "- Load the test set:\n"}
{"snippet": "credit_df = pd.read_csv('creditcard.csv')\ncredit_df.head()\n", "intent": "The dataset we use is publicly available at https://www.kaggle.com/mlg-ulb/creditcardfraud\n"}
{"snippet": "all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n", "intent": "- **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. \n"}
{"snippet": "df = pd.DataFrame({'coefficient-name':range(len(regr.coef_)), 'value':regr.coef_})\nsrt = df.sort_values(['value'])[: : -1]\nsrt[0:100].plot(x='coefficient-name',y='value',kind='bar',figsize=(15,8))\n", "intent": "The lasso regression decays quickly to reach zero. Once it reaches it, it tries to maintain the value 0 with a few overshoots\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "pd.DataFrame(zip(train_cols.columns, np.transpose(result.coef_)))\n", "intent": "hint 1: np.exp(X)\nhint 2: conf['OR'] = params\n           conf.columns = ['2.5%', '97.5%', 'OR']\n"}
{"snippet": "f = open('../../lectures/data/temperature.csv')\ntemperature = pd.read_csv(f,sep=',', header='infer', parse_dates=[0],index_col=0)\nf.close()\ntemperature.index\ndata = data.set_index(['Time'])\n", "intent": "Create a new DataFrame with the temperature data, and set the index to be the Timestamp.\n"}
{"snippet": "from sklearn.datasets import load_boston\nprint(load_boston().DESCR)\ndf = pd.DataFrame(\n        data=np.column_stack((load_boston().data,load_boston().target)),\n        columns=np.append(load_boston().feature_names,['Median_Value'])\n    )\ndf.describe()\n", "intent": "Now let's try a regression task. To begin, let's use an existing dataset.\n"}
{"snippet": "column_names = pd.read_excel('langevincodebook.xlsx',sheetname = 'Sheet2')\ndata_names = column_names['Description'].values\ndata = pd.read_csv('LANGEVIN_DATA.txt',sep=' ',names = data_names,index_col =False)\n", "intent": "Importing the data and the column names from the codebook:\n"}
{"snippet": "X1_train, X1_test, Y_train, Y_test = train_test_split(X1, Y, test_size=0.3)\nreg = tree.DecisionTreeRegressor()\nreg = reg.fit(X1_train,Y_train)\nr2_score_avg1 = np.average([reg.score(X1_test,Y_test) for i in range(5000)])\nprint('R^2 value: ', r2_score_avg1)\n", "intent": "The first regression tree will test our environmental and time variables.\n"}
{"snippet": "dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\nCSVdata=pd.read_csv('Building Electrical.csv', parse_dates=[0], date_parser=dateparse)\n", "intent": "First, read the csv data files, and convert the index 'Timestamp' to datetimeindex.\n"}
{"snippet": "f = open('C:/F16-12-752-master/projects/thongyi_weijian1/data/CBECS.csv')\ndata = pd.read_csv(f,sep=',', header='infer', parse_dates=[1])\ndata = data.set_index('PUBID')\ndata.tail()\n", "intent": "Please download the dataset and change the file path.\n"}
{"snippet": "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n", "intent": "- **MSZoning (The general zoning classification)** :  'RL' is by far  the most common value.  So we can fill in missing values with 'RL'\n"}
{"snippet": "print(\"Extracting tf features for LDA...\")\nn_features = 1000\nn_samples = 2000\ntf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n                                max_features=n_features,\n                                stop_words='english')\nt0 = time()\ntf = tf_vectorizer.fit_transform(data_samples)\nprint(\"done in %0.3fs.\" % (time() - t0))\nprint tf[0][0][0]\n", "intent": "Now we are ready to compute the token counts.\n"}
{"snippet": "t0 = time()\ncorpus_lda = lda.fit_transform(tf)\nprint corpus_lda[10]/np.sum(corpus_lda[10])\nprint(\"done in %0.3fs.\" % (time() - t0))\nprint corpus_titles[10]\n", "intent": "**Task**: Fit model `lda` with the token frequencies computed by `tf_vectorizer`.\n"}
{"snippet": "name = \"birds.jpg\"\nname = \"Seeds.jpg\"\nbirds = imread(\"Images/\" + name)\nbirdsG = np.sum(birds, axis=2)\n", "intent": "Select and visualize image `birds.jpg` from file and plot it in grayscale\n"}
{"snippet": "of_df = pd.read_csv(\"./datasets/old-faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "colleges = pd.read_csv('College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "wb = pd.read_csv('world_bank/API_19_DS2_en_csv_v2.csv', sep=',', header=0, skiprows=3) \nwb = wb.drop(wb.columns[[-1]], 1)\nprint(\"Dataframe shape:\", wb.shape)\nwb.head()\n", "intent": "Load the World Bank data as a dataframe.\n"}
{"snippet": "wb_meta_country = pd.read_csv('world_bank/Metadata_Country_API_19_DS2_en_csv_v2.csv', sep=',', header=0) \nwb_meta_country = wb_meta_country.drop(wb_meta_country.columns[[-1]], 1)\nwb_meta_country.head()\n", "intent": "Load the additional csv file which contains the metadata of the countries on the main World Bank data.\n"}
{"snippet": "wb_meta_indi = pd.read_csv('world_bank/Metadata_Indicator_API_19_DS2_en_csv_v2.csv', sep=',', header=0) \nwb_meta_indi = wb_meta_indi.drop(wb_meta_indi.columns[[-1]], 1)\nwb_meta_indi.head()\n", "intent": "We also have to include the another csv file containing the metadata of the indicator on the main World Bank data. \n"}
{"snippet": "wb_inter = pd.DataFrame()\nwb_inter = wb[wb.columns.values]\nwb_inter[list(wb.columns.values[range(column_start_of_year,column_start_of_year+interval_duration)])] = wb_interpolated[list(range(0,len(years_column)))]\n", "intent": "A new dataframe is created to accomodate the new interpolated values. \n"}
{"snippet": "all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n", "intent": "- **Functional** : data description says NA means typical\n"}
{"snippet": "df_ccpi_clean = pd.DataFrame()\narray_flag = np.ones((len(df_ccpi)), dtype=bool)\nseries_flag = pd.Series(data = array_flag, index=range(len(df_ccpi)))\nfor indi in sorted(set(list_deleted_indi)):\n    series_flag = (series_flag & (df_ccpi['Indicator Name'] != indi))\ndf_ccpi_clean_reset = df_ccpi[series_flag]\nprint('Number of row with missing value:', df_ccpi_clean_reset.isnull().any().sum())\n", "intent": "We will have to remove 24 indicators from the data. It leaves us with the remaining 56 indicators.\n"}
{"snippet": "df_ccpi_feature_processed = pd.DataFrame()\nfor indi in set_indi_average:\n    df_indi = df_ccpi_feature_label_removed[(df_ccpi_feature_label_removed['Indicator Name'] == indi)]\n    df_indi_ave = (df_indi.sum(axis = 0)) \n    df_ccpi_feature_processed[indi] = df_indi_ave[4:-1]/len(ccpi_country)\nprint('Dataframe shape:',df_ccpi_feature_processed.shape)\n", "intent": "New engineered dataframe is generated. First the dataframe with the averaged features is appended. \n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\ntarget_int = (dataset_tempe['annual mean']*100).astype(int)\nRFEmodel = LogisticRegression()\nrfe = RFE(RFEmodel, 7)\nrfe = rfe.fit(X_tempe_train, target_int)\n", "intent": "In order to know which indicators has big impact on global temperature, Recrusive Feature Elimination is implemented.\n"}
{"snippet": "wb_inter = pd.DataFrame()\nwb_inter = wb[wb.columns.values]\nwb_inter[list(wb.columns.values[range(4,61)])] = wb_interpolated[list(range(0,57))]\nwb_inter.head(8)\n", "intent": "A new dataframe is created to accomodate the new interpolated values. \n"}
{"snippet": "df_ccpi = pd.DataFrame()\nfor country in country_class_y['Country Name']:\n    df1 = wb_inter[wb_inter['Country Name'] == country]\n    df_ccpi = pd.concat( [df_ccpi,df1], ignore_index=True, axis=0)\ndf_ccpi[:8]\n", "intent": "New dataframe which contains only the data from countries being mentioned in CCPI document is created.\n"}
{"snippet": "df_ccpi_feature_processed = pd.DataFrame()\nfor indi in set_indi_average:\n    df_indi = df_ccpi_feature_label_removed[(df_ccpi_feature_label_removed['Feature Interaction'] == 'average') & (df_ccpi_feature_label_removed['Indicator Name'] == indi)]\n    df_indi_ave = (df_indi.sum(axis = 0)) \n    df_ccpi_feature_processed[indi] = df_indi_ave[4:-1]/len(country_class_y)\nprint('Dataframe shape:',df_ccpi_feature_processed.shape)    \ndf_ccpi_feature_processed.head()\n", "intent": "Generate new engineered dataframe. First we append the dataframe with the averaged features. \n"}
{"snippet": "df_ccpi = pd.DataFrame()\nfor country in ccpi_country['Country Name']:\n    df1 = wb_inter[wb_inter['Country Name'] == country]\n    df_ccpi = pd.concat( [df_ccpi,df1], ignore_index=True, axis=0)\ndf_ccpi[:8]\n", "intent": "New dataframe which contains only the data from countries being mentioned in CCPI document is created.\n"}
{"snippet": "df_ccpi_feature_processed = pd.DataFrame()\nfor indi in set_indi_average:\n    df_indi = df_ccpi_feature_label_removed[(df_ccpi_feature_label_removed['Feature Interaction'] == 'average') & (df_ccpi_feature_label_removed['Indicator Name'] == indi)]\n    df_indi_ave = (df_indi.sum(axis = 0)) \n    df_ccpi_feature_processed[indi] = df_indi_ave[4:-1]/len(ccpi_country)\nprint('Dataframe shape:',df_ccpi_feature_processed.shape)    \ndf_ccpi_feature_processed.head()\n", "intent": "Generate new engineered dataframe. First we append the dataframe with the averaged features. \n"}
{"snippet": "wb_inter = pd.DataFrame()\nwb_inter = wb[wb.columns.values]\nwb_inter[list(wb.columns.values[range(4,61)])] = wb_interpolated[list(range(0,57))]\n", "intent": "A new dataframe is created to accomodate the new interpolated values. \n"}
{"snippet": "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n", "intent": "- **Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\ntarget_int = (dataset_tempe['annual mean']*100).astype(int)\nRFEmodel = LogisticRegression()\nrfe = RFE(RFEmodel, 7)\nrfe = rfe.fit(X_tempe_train, target_int) \n", "intent": "In order to know which indicators has big impact on global temperature, Recrusive Feature Elimination is implemented.\n"}
{"snippet": "df_test_clean = pd.DataFrame()\nfor indi in set_saved_indi_filter:\n    df_temp = df_test[df_test['Indicator Name'] == indi]\n    df_test_clean = pd.concat([df_test_clean, df_temp], ignore_index = True, axis = 0)\nprint(df_test_clean.shape)\ndf_test_clean.head()\n", "intent": "We will be using only 49 features from the indicator, because the others 31 is not available (for all category on at least one class (bad:good))\n"}
{"snippet": "wb = pd.read_csv('world_bank/API_19_DS2_en_csv_v2.csv', sep=',', header=0, skiprows=3) \nwb = wb.drop(wb.columns[[-1]], 1)\nwb.head()\n", "intent": "Load the world bank data as a dataframe.\n"}
{"snippet": "wb_meta_country = pd.read_csv('world_bank/Metadata_Country_API_19_DS2_en_csv_v2.csv', sep=',', header=0) \nwb_meta_country = wb_meta_country.drop(wb_meta_country.columns[[-1]], 1)\nwb_meta_country.head()\n", "intent": "Read the additional csv file which contains the metadata of the counries on the main world bank data.\n"}
{"snippet": "wb_meta_indi = pd.read_csv('world_bank/Metadata_Indicator_API_19_DS2_en_csv_v2.csv', sep=',', header=0) \nwb_meta_indi = wb_meta_indi.drop(wb_meta_indi.columns[[-1]], 1)\nwb_meta_indi.head()\n", "intent": "We also have to include the another csv file containing the metadata of the indicator on the main world bank data. \n"}
{"snippet": "df_ccpi = pd.DataFrame()\nfor country in country_class_y['Country Name']:\n    df1 = wb_inter[wb_inter['Country Name'] == country]\n    df_ccpi = pd.concat( [df_ccpi,df1], ignore_index=True, axis=0)\ndf_ccpi[:8]\n", "intent": "New dataframe which contains only the data from countries mention in CCPI document.\n"}
{"snippet": "import operator\nlist_deleted_indi = []\nlist_tuple_country_del_indi = []\nfor c, i in enumerate(range(0,4480,80)):\n    list_tuple_country_del_indi.append((df_ccpi['Country Name'][i], df_ccpi[i:i+80].isnull().any(axis=1).sum())) \n    for i in df_ccpi[i:i+80]['Indicator Name'][df_ccpi[i:i+80].isnull().any(axis=1)]:\n        list_deleted_indi.append(i)    \ndf_tuple_cdi = pd.DataFrame(list_tuple_country_del_indi, columns = ['Country Name','Number of Missing Indicator'])\ndf_tuple_cdi = df_tuple_cdi.set_index('Country Name')\n", "intent": "Check the data for each country which the entire years of a indicator are missing.\n"}
{"snippet": "dict_df_temp = {}\nrange_tempe = range(1960, 2017)\nfor year in range_tempe:\n    year = str(year)\n    dict_df_temp[year] = pd.DataFrame()\n    dict_df_temp[year] = wb[['Country Name', year]]\n    dict_df_temp[year] = dict_df_temp[year].fillna(0)\ndict_df_temp[year].head()\n", "intent": "We could also use the entire countries instead too make our features matrix bigger (not necessarily better).\n"}
{"snippet": "df_ccpi = pd.DataFrame()\nfor country in country_class_y['Country Name']:\n    df1 = wb[wb['Country Name'] == country]\n    df1 = wb_inter[wb_inter['Country Name'] == country]\n    df_ccpi = pd.concat( [df_ccpi,df1], ignore_index=True, axis=0)\ndf_ccpi[:8]\n", "intent": "New dataframe which contains only the data from countries mention in CCPI document is created.\n"}
{"snippet": "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n", "intent": "- **KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent)  for the missing value in KitchenQual.\n"}
{"snippet": "dict_df_temp = {}\nrange_tempe = range(1960, 2017)\nfor year in range_tempe:\n    year = str(year)\n    dict_df_temp[year] = pd.DataFrame()\n    dict_df_temp[year] = wb[['Country Name', year]]\n    dict_df_temp[year] = dict_df_temp[year].fillna(0)\ndict_df_temp[year].head()\n", "intent": "We could also use the entire countries to make our features matrix bigger (not necessarily better).\n"}
{"snippet": "print(pd.DataFrame({'effect': params_tempe*10**20, 'error': err[0]*10**20}))\n", "intent": "List above shows which countries and their indicator contribute to decrease global temperature.\n"}
{"snippet": "wb_meta_country = pd.read_csv('world_bank/Metadata_Country_API_19_DS2_en_csv_v2.csv', sep=',', header=0) \nwb_meta_country = wb_meta_country.drop(wb_meta_country.columns[[-1]], 1)\nwb_meta_country.head()\n", "intent": "Read the additional csv file which contains the metadata of the countries on the main world bank data.\n"}
{"snippet": "df_ccpi = pd.DataFrame()\nfor country in country_class_y['Country Name']:\n    df1 = wb_inter[wb_inter['Country Name'] == country]\n    df_ccpi = pd.concat( [df_ccpi,df1], ignore_index=True, axis=0)\ndf_ccpi[:8]\n", "intent": "New dataframe which contains only the data from countries mention in CCPI document is created.\n"}
{"snippet": "df_ccpi = pd.DataFrame()\nfor country in country_class_y['Country Name']:\n    df1 = wb[wb['Country Name'] == country]\n    df1 = wb_inter[wb_inter['Country Name'] == country]\n    df_ccpi = pd.concat( [df_ccpi,df1], ignore_index=True, axis=0)\ndf_ccpi[:8]\n", "intent": "New dataframe which contains only the data from countries mention in CCPI document.\n"}
{"snippet": "df_ccpi_clean = pd.DataFrame()\narray_flag = np.ones((len(df_ccpi)), dtype=bool)\nseries_flag = pd.Series(data = array_flag, index=range(len(df_ccpi)))\nfor indi in sorted(set(list_indi)):\n    series_flag = (series_flag & (df_ccpi['Indicator Name'] != indi))\ndf_ccpi_clean = df_ccpi[series_flag]\ndf_ccpi_clean_reset = df_ccpi_clean.reset_index(drop=True)\ndf_ccpi_clean_reset = df_ccpi_clean_reset.fillna(0)\ndf_ccpi_clean_reset.head(8)\n", "intent": "Filter the data frame so it only consists of the indicators which are complete for the entire year (1960-2016).\n"}
{"snippet": "display(pd.DataFrame(np.round(pca_samples, 4), columns = pca_results.index.values))\n", "intent": "The code below will help us see how the log-transformed sample data has changed after having a PCA transformation applied to it in six dimensions.\n"}
{"snippet": "df = pd.read_csv('College_Data', index_col='Unnamed: 0')\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "scaler = StandardScaler()\nX = df.drop('TARGET CLASS', axis=1)\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n", "intent": "- **Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n"}
{"snippet": "df_err = pd.DataFrame(data=error_rate, columns=['Error'])\ndf_err.plot()\n", "intent": "**Now create the following plot using the information from your for loop.**\n"}
{"snippet": "df = pd.read_csv(\"Classified Data\", index_col=0)\n", "intent": "Set index_col=0 to use the first column as the index.\n"}
{"snippet": "df = pd.DataFrame({\n    \"name\": [\"Goldy\", \"Scooby\", \"Brian\", \"Francine\", \"Goldy\"],\n    \"kind\": [\"Fish\", \"Dog\", \"Dog\", \"Cat\", \"Dog\"],\n    \"age\": [0.5, 7., 3., 10., 1.]\n}, columns = [\"name\", \"kind\", \"age\"])\ndf\n", "intent": "Here we create a toy DataFrame of pets including their name and kind:\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\nvec_enc = DictVectorizer()\nvec_enc.fit(df.to_dict(orient='records'))\n", "intent": "Scikit-Learn also has several library for constructing one-hot-encodings.\n"}
{"snippet": "qsos = pd.read_csv(\"qso10000.csv\",index_col=0,\n                                  usecols=usecols)\nqsos = qsos[(qsos[\"dered_r\"] > -9999) & (qsos[\"g_r_color\"] > -10) & (qsos[\"g_r_color\"] < 10)]\nqso_features = copy.copy(qsos)\nqso_redshifts = qsos[\"spec_z\"]\ndel qso_features[\"spec_z\"]\n", "intent": "Looks like there are some missing values in the catalog which are set at -9999. Let's zoink those from the dataset for now.\n"}
{"snippet": "qsos.to_csv(\"qsos.clean.csv\")\n", "intent": "Ok. This looks pretty clean. Let's save this for future use.\n"}
{"snippet": "df = pd.read_csv(\"../data/coal_prod_cleaned.csv\")\n", "intent": "Using cleaned data from [Data Cleaning](Data%20Cleaning.ipynb) Notebook. See Notebook for details.\n"}
{"snippet": "app_train = pd.read_csv(path + \"application_train.csv\")\napp_train.head()\n", "intent": "Application data consists of static data for all applications and every row represents one loan.\n"}
{"snippet": "train_df = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\ntest_df = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\nmacro_df = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])\ntrain_df = pd.merge(train_df, macro_df, how='left', on='timestamp')\ntest_df = pd.merge(test_df, macro_df, how='left', on='timestamp')\nprint(train_df.shape, test_df.shape)\nulimit = np.percentile(train_df.price_doc.values, 99)\nllimit = np.percentile(train_df.price_doc.values, 1)\ntrain_df['price_doc'].ix[train_df['price_doc']>ulimit] = ulimit\ntrain_df['price_doc'].ix[train_df['price_doc']<llimit] = llimit\n", "intent": "Let us read the train, test and macro files and combine macro information with train and test.\n"}
{"snippet": "all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n", "intent": "- **SaleType** : Fill in again with most frequent which is \"WD\"\n"}
{"snippet": "train_df = pd.read_csv(\"../input/train.csv\")\ntrain_df.shape\n", "intent": "First let us import the train file and get some idea about the data.\n"}
{"snippet": "df = pd.read_csv(\"./dataset_Facebook.csv\", delimiter = \";\")\nfeatures = [\"Category\",\n            \"Page total likes\",\n            \"Type\",\n            \"Post Month\",\n            \"Post Hour\",\n            \"Post Weekday\",\n            \"Paid\"]\ndf[features].head()\n", "intent": "* We have loaded the necessary libraries above\n* Now let's load the data\n"}
{"snippet": "df = pd.read_csv(\"./dataset_Facebook.csv\", delimiter = \";\")\n", "intent": "* We have loaded the necessary libraries above\n* Now let's load the data\n"}
{"snippet": "df['age_medianimpute'] = df['age'].fillna(df['age'].median())\ndf['age_modeimpute'] = df['age'].fillna(df['age'].mode())\n", "intent": "   1) median (middle value) for that column\n   2) mode (most frequent value) for that column\n"}
{"snippet": "df['age'].fillna(df['age'].median(), inplace=True)\ndf['age'].fillna(df['age'].mode(), inplace=True)\n", "intent": "   1) median (middle value) for that column\n   2) mode (most frequent value) for that column\n"}
{"snippet": "path_train = os.path.join(os.getcwd(), 'datasets', 'landsat', 'landsat_train.csv')\nlandsat_train = pd.read_csv(path_train, delimiter = ',')\nprint(\"There are {} entries and {} columns in the landsat_train DataFrame\"\\\n      .format(landsat_train.shape[0], landsat_train.shape[1]))\n", "intent": "Load the `landsat_train.csv` dataset into a `pandas` DataFrame called  `landsat_train` and display the shape of the DataFrame.\n"}
{"snippet": "X = sales2015\ny = sales2015[\"Sale_Dollars_sum_2015\"]\nX_train, X_test, y_train, y_test = train_test_split(sales2015, y, test_size=0.4)\nprint X_train.shape, y_train.shape\nprint X_test.shape, y_test.shape\n", "intent": "Sales Decrease in 2016\n"}
{"snippet": "header = [\"Sample_Code_Num\",\"Clump_T\",\"Uni_Cell_Size\",\"Uni_Cell_Shape\",\"Marg_Adh\",\"SECS\",\"Bare_Nuc\",\"Bland_Chro\",\"Norm_Nuc\",\"Mitoses\",\"Class\"]    \ndfCancer = pd.read_csv(\"../../assets/datasets/breast-cancer-wisconsin.csv\", names = header)\ndfCancer.head()\n", "intent": "The column names are taken from the dataset info file. Create an array\nwith the column names and assign them as the header when loading the\ncsv.\n"}
{"snippet": "df = pd.DataFrame(\"../assets/datasets/breast-cancee)\n", "intent": "The column names are taken from the dataset info file. Create an array\nwith the column names and assign them as the header when loading the\ncsv.\n"}
{"snippet": "all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n", "intent": "- **MSSubClass** : Na most likely means No building class. We can replace missing values with None\n"}
{"snippet": "tfidfvectorizer = TfidfVectorizer(min_df=2, stop_words='english')\nX, y = make_xy(critics, tfidfvectorizer)\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.3, train_size = 0.7, random_state = 2018)\nclf = MultinomialNB(alpha = best_alpha)\nclf.fit(X_train, Y_train)\ntraining_accuracy = clf.score(X_train, Y_train)\ntest_accuracy = clf.score(X_test, Y_test)\nprint(\"Accuracy on training data: {:2f}\".format(training_accuracy))\nprint(\"Accuracy on test data:     {:2f}\".format(test_accuracy))\n", "intent": "5) Use TF-IDF weighting instead of word counts\n"}
{"snippet": "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\nfor f in categorical:\n        if train_df[f].dtype == 'object':\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))\n            features_to_use.append(f)\n", "intent": "Encode categorical values into numerical values between 0 and n_classes - 1 (different from get_dummies\n"}
{"snippet": "X_train = pd.read_json(\"train.json\")\nX_test = pd.read_json(\"test.json\")\n", "intent": "Here both \"listing_id\" and 'created' represents the order when the post was created.\n"}
{"snippet": "train = pd.read_csv('cleaned_train.csv', header = 0)\n", "intent": "This training data is exported data from previous statistical analysis.\n"}
{"snippet": "for i in range(len(train_stack_list)):\n    stat = pd.DataFrame(train_stack_list[i], columns = column_name_list[i])\n    stat['row_id'] = range(stat.shape[0])\n    train_df = pd.merge(train_df, stat)\nfor i in range(len(test_stack_list)):\n    stat = pd.DataFrame(test_stack_list[i], columns = column_name_list[i])\n    stat['row_id'] = range(stat.shape[0])\n    test_df = pd.merge(test_df, stat)\n", "intent": "Both train_stack_list and test_stack_list are of dimension (10, 49352, 6).\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nimport itertools\nvectorizer = CountVectorizer(min_df=1, stop_words = 'english')\nx = vectorizer.fit_transform(itertools.chain.from_iterable(train['features']))\nnames = vectorizer.get_feature_names()\ncounts = np.sum(x.toarray(), axis=0)\nn2c = list(zip(names, counts))\nn2c.sort(key=lambda x: x[1], reverse=True)\nprint(n2c)\n", "intent": "Let's first extract important features from the training dataset.\n"}
{"snippet": "a = pd.read_csv(\"FE6_rf.csv\", header = 0)\nb = pd.read_csv(\"FE6_xgb.csv\", header = 0)\nc = pd.read_csv(\"FE6_withnan_lgbm.csv\", header = 0)\nd = pd.read_csv(\"FE6_lgbm.csv\", header = 0)\navg = pd.DataFrame()\navg['listing_id'] = a['listing_id']\navg['high'] = (a['high'] + b['high'] + c['high'] + d['high'])/4.0\navg['medium'] = (a['medium'] + b['medium'] + c['medium'] + d['medium'])/4.0\navg['low'] = (a['low'] + b['low'] + c['low'] + d['low'])/4.0\navg.to_csv('FE6_4avg.csv', index = False)\n", "intent": "Decrease the prediction logloss slightly.\n"}
{"snippet": "validation_size = 0.30\nseed = 2018\nx = train_df.values\nX_train, X_validation, Y_train, Y_validation = train_test_split(x, y, test_size = validation_size, random_state = seed)\n", "intent": "Split the training data into train/validation data.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nimport itertools\nvectorizer = CountVectorizer(min_df=1)\nx = vectorizer.fit_transform(itertools.chain.from_iterable(train['features']))\nnames = vectorizer.get_feature_names()\ncounts = np.sum(x.toarray(), axis=0)\nn2c = list(zip(names, counts))\nn2c.sort(key=lambda x: x[1], reverse=True)\nprint(n2c[:30])\n", "intent": "Let's first extract important features from the training dataset.\n"}
{"snippet": "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()\n", "intent": "Is there any remaining missing value ? \n"}
{"snippet": "cls['homepage'] = cls['homepage'].fillna('').apply(lambda x: 0 if x == '' else 1)\ng = sns.PairGrid(data=cls, x_vars=['homepage'], y_vars='return', size=5)\ng.map(sns.pointplot, color=sns.xkcd_rgb[\"plum\"])\ng.set(ylim=(0, 1))\n", "intent": "It seems that movies that belong to a franchise have a higher probability of being a success. \n"}
{"snippet": "train_X, test_X, train_y, test_y = train_test_split(X, y, \n                                                    train_size=0.5, \n                                                    random_state=123,\n                                                    stratify=y)\nprint('All:', np.bincount(y) / float(len(y)) * 100.0)\nprint('Training:', np.bincount(train_y) / float(len(train_y)) * 100.0)\nprint('Test:', np.bincount(test_y) / float(len(test_y)) * 100.0)\n", "intent": "So, in order to stratify the split, we can pass the label array as an additional option to the `train_test_split` function:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n", "intent": "Again, we start by splitting our dataset into a training (75%) and a test set (25%):\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntext_train, text_test, y_train, y_test = train_test_split(text, y, \n                                                          random_state=42,\n                                                          test_size=0.25,\n                                                          stratify=y)\n", "intent": "Next, we split our dataset into 2 parts, the test and training dataset:\n"}
{"snippet": "print('CountVectorizer defaults')\nCountVectorizer()\n", "intent": "Now, we use the CountVectorizer to parse the text data into a bag-of-words model.\n"}
{"snippet": "from sklearn.datasets import load_digits\nfrom sklearn.neighbors import KNeighborsClassifier\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, random_state=0)\nparam_grid = {'n_neighbors': [1, 3, 5, 10, 50]}\ngs = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=5, verbose=3)\ngs.fit(X_train, y_train)\nprint(\"Score on test set: %f\" % gs.score(X_test, y_test))\nprint(\"Best parameters: %s\" % gs.best_params_)\n", "intent": "Apply grid-search to find the best setting for the number of neighbors in ``KNeighborsClassifier``, and apply it to the digits dataset.\n"}
{"snippet": "from sklearn.manifold import TSNE\ntsne = TSNE(random_state=42)\ndigits_tsne = tsne.fit_transform(digits.data)\n", "intent": "Using a more powerful, nonlinear techinque can provide much better visualizations, though.\nHere, we are using the TSNE manifold learning method:\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nh_pipeline = Pipeline((\n    ('vec', HashingVectorizer(encoding='latin-1')),\n    ('clf', LogisticRegression(random_state=1)),\n))\nh_pipeline.fit(docs_train, y_train)\n", "intent": "Finally, let us train a LogisticRegression classifier on the IMDb training subset:\n"}
{"snippet": "df['one'].fillna(df['one'].mean())\n", "intent": "- Another common way to impute is by the mean of the column.\n"}
{"snippet": "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n", "intent": "**Skewed features**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nitrain, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itest] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "A, b = load_svmlight_file(\"../data/ionosphere.txt\")\n", "intent": "or use a real dataset in LibSVM format\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\n", "intent": "For this exercise, we'll use Fisher's Iris Data Set:\n"}
{"snippet": "df = pd.read_csv('./datasets/breast_cancer_wisconsin/wdbc.data', \n                 header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "mle = MultiColumnLabelEncoder()\nle = LabelEncoder()\nohe = OneHotEncoder()\n", "intent": "https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn\n"}
{"snippet": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten, \\\n                                            Convolution2D, MaxPooling2D,Conv2D\nfrom keras.utils import np_utils\nfrom keras import backend\nimport tensorflow\nfrom keras.datasets import mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n", "intent": "https://elitedatascience.com/keras-tutorial-deep-learning-in-python\n"}
{"snippet": "users = pd.read_csv('takehome_users.csv', encoding = \"ISO-8859-1\", parse_dates=['creation_time'])\nengagement = pd.read_csv('takehome_user_engagement.csv', encoding = \"ISO-8859-1\", parse_dates=['time_stamp'])\n", "intent": "Read CSV files and explore the dataset properties\n"}
{"snippet": "logins = pd.read_json('logins.json')\nlogins.info()\nlogins.head()\n", "intent": "**Import login data into pandas dataframe**\n"}
{"snippet": "riders_data = pd.get_dummies(riders, columns=['city'], drop_first=True)\nriders_data = pd.get_dummies(riders_data, columns=['phone'], dummy_na=True, drop_first=True)\nriders_data.ultimate_black_user = riders_data.ultimate_black_user.astype(np.int)\nriders_data.retained = riders_data.retained.astype(np.int)\nriders_data[['avg_rating_of_driver','avg_rating_by_driver']] = riders_data[['avg_rating_of_driver', 'avg_rating_by_driver']].apply(lambda x: x.fillna(x.mean()))\n", "intent": "**Prepare dataframe for modeling**\n"}
{"snippet": "columns=['rf','et','logit','nb','xgb','lgb']\ntrain_pred_df_list=[rf_train_pred_df, et_train_pred_df, logit_train_pred_df, nb_train_pred_df,\n                    xgb_train_pred_df, lgb_train_pred_df]\ntest_pred_df_list=[rf_test_pred_df, et_test_pred_df, logit_test_pred_df, nb_test_pred_df,\n                    xgb_test_pred_df, lgb_test_pred_df]\nlv1_train_df=pd.DataFrame(columns=columns)\nlv1_test_df=pd.DataFrame(columns=columns)\nfor i in range(0,len(columns)):\n    lv1_train_df[columns[i]]=train_pred_df_list[i]['prediction_probability']\n    lv1_test_df[columns[i]]=test_pred_df_list[i]['prediction_probability']\n", "intent": "Let's group ouf level 1 OOF predictions output together to genenerate the input for level 2 stacking\n"}
{"snippet": "predict_whole = predict_start + ['avg_rating_of_driver', 'avg_rating_by_driver', 'avg_surge', 'surge_pct', 'weekday_pct']\nX_whole = riders_data[predict_whole].values\nXw_train, Xw_test, yw_train, yw_test = train_test_split(X_whole, y, train_size=0.8)\n", "intent": "* **Logistic regression** grid search using all features (even those extending beyond the first 30 days)\n"}
{"snippet": "scaler = StandardScaler()\nto_scale = popular_prescribers[['LISINOPRIL', 'presc_drug_count']]\nscaler.fit(to_scale.values)\nscaled = scaler.transform(to_scale.values)\nlogd = np.log1p(scaled)\npopular_prescribers.loc[:,'LISINOPRIL.transf'] = logd[:,0].reshape(-1,1)\npopular_prescribers.loc[:,'presc_drug_count_transf'] = logd[:,1].reshape(-1,1)\n", "intent": "We will standard scale the variables, and also try seeing if taking a log might help. \n"}
{"snippet": "scaler = StandardScaler()\ntop_19_drugs = [drug for drug in top_20_drugs if drug != 'LISINOPRIL']\nto_scale = popular_prescribers[top_19_drugs]\nscaler.fit(to_scale.values)\nscaled = scaler.transform(to_scale.values)\nlogd = np.log1p(scaled)\ntransf_drugs_df = pd.DataFrame(logd, columns = ['{}.transf'.format(drug) for drug in top_19_drugs])\npopular_prescribers_transf = pd.concat([popular_prescribers, transf_drugs_df], axis=1 ).fillna(0)\npopular_prescribers_transf.loc[:, dummy_columns] = popular_prescribers_transf[dummy_columns].astype(np.int64)\n", "intent": "Now we'll see how this simple exponential transformation to the target variable affects performance for other drugs. \n"}
{"snippet": "tweets_with_originals = pd.read_csv('tweets_with_originals.csv')\n", "intent": "There are tweets without the original text.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words = stopwords, strip_accents = 'unicode', min_df = 10)\ntweet_input = tweets_with_originals.text_clean\ntweet_input=tweet_input.str.replace(r\" (\\d|\\W)+\",\"\") \nT = tfidf_vectorizer.fit_transform(tweet_input) \n", "intent": "Using TF-IDF, we'll look for additional stopwords \n"}
{"snippet": "from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\ny = tweets_with_originals.airline_sentiment.map({'negative': 0, 'neutral':1, 'positive':2})\ntfidf_learning_inputs = []\nfor M in tfidf_input_mats:\n    T_train, T_test, y_train, y_test = train_test_split(T, y, test_size=0.3)\n    sm = SMOTE() \n    T_train_sm, y_train_sm = sm.fit_sample(T_train, y_train.ravel())\n    tfidf_learning_inputs.append([T_train_sm, T_test, y_train_sm, y_test])\n", "intent": "There are a lot of strategies we could try to rebalance the dataset. The easiest would be oversampling the neutral/positive sentiment tweets. \n"}
{"snippet": "boston_df = pd.read_csv('Boston.csv')\nboston_df.info()\n", "intent": "see http://www-bcf.usc.edu/~gareth/ISL/\n"}
{"snippet": "PCA_set = PCA(n_components=4)\nY = PCA_set.fit_transform(Xvote)\n", "intent": "Now, conduct a PCA using scikit learn\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "X_scaled = pd.DataFrame(data=X_scaled_np, columns=data.drop(columns=['Class']).columns)\nX_scaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "lv2_columns=['rf_lf2', 'logit_lv2', 'xgb_lv2','lgb_lv2']\ntrain_lv2_pred_list=[rf_lv2_train_pred, logit_lv2_train_pred, xgb_lv2_train_pred, lgb_lv2_train_pred]\ntest_lv2_pred_list=[rf_lv2_test_pred, logit_lv2_test_pred, xgb_lv2_test_pred, lgb_lv2_test_pred]\nlv2_train=pd.DataFrame(columns=lv2_columns)\nlv2_test=pd.DataFrame(columns=lv2_columns)\nfor i in range(0,len(lv2_columns)):\n    lv2_train[lv2_columns[i]]=train_lv2_pred_list[i]\n    lv2_test[lv2_columns[i]]=test_lv2_pred_list[i]\n", "intent": "On level 3, we follow simlar workflow as level 2. First we put the OOF output from level 2 together, and then send them to our chosen algorithms.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(df_feat_scaled, df['TARGET CLASS'], test_size=0.3)\n", "intent": "**Use train_test_split to split your data into a training set and a testing set.**\n"}
{"snippet": "X_cv = count_vectorizer.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "X = yelp_class.text\ny = yelp_class.stars\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "train_data_10users.to_csv(os.path.join(PATH_TO_DATA, \n                                       'train_data_10users.csv'), \n                        index_label='session_id', float_format='%d')\ntrain_data_150users.to_csv(os.path.join(PATH_TO_DATA, \n                                        'train_data_150users.csv'), \n                         index_label='session_id', float_format='%d')\n", "intent": "**Write dataframes to csv files for further analysis.**\n"}
{"snippet": "img_path = '../data/convnet_vis/cat.jpg'\nfrom keras.preprocessing import image\nimport numpy as np\nimg = image.load_img(img_path, target_size=(150, 150))\nimg_tensor = image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\nimg_tensor /= 255.\nprint(img_tensor.shape)\n", "intent": "This will be the input image we will use -- a picture of a cat, not part of images that the network was trained on:\n"}
{"snippet": "import cv2\nimg = cv2.imread(img_path)\nheatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\nheatmap = np.uint8(255 * heatmap)\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\nsuperimposed_img = heatmap * 0.4 + img\n", "intent": "Finally, we will use OpenCV to generate an image that superimposes the original image with the heatmap we just obtained:\n"}
{"snippet": "X1 = PolynomialFeatures(1).fit_transform(df.age.reshape(-1,1))\nX2 = PolynomialFeatures(2).fit_transform(df.age.reshape(-1,1))\nX3 = PolynomialFeatures(3).fit_transform(df.age.reshape(-1,1))\nX4 = PolynomialFeatures(4).fit_transform(df.age.reshape(-1,1))\nX5 = PolynomialFeatures(5).fit_transform(df.age.reshape(-1,1))\ny = (df.wage > 250).map({False:0, True:1}).as_matrix()\nprint('X4:\\n', X4[:5])\nprint('y:\\n', y[:5])\n", "intent": "Create polynomials for 'age'. These correspond to those in R, when using raw=TRUE in poly() function.\n"}
{"snippet": "allData = pd.read_csv('data/data.csv')\ndata = allData[allData['shot_made_flag'].notnull()].reset_index()\ndata['game_date_DT'] = pd.to_datetime(data['game_date'])\ndata['dayOfWeek']    = data['game_date_DT'].dt.dayofweek\ndata['dayOfYear']    = data['game_date_DT'].dt.dayofyear\ndata['secondsFromPeriodEnd']   = 60*data['minutes_remaining']+data['seconds_remaining']\ndata['secondsFromPeriodStart'] = 60*(11-data['minutes_remaining'])+(60-data['seconds_remaining'])\ndata['secondsFromGameStart']   = (data['period'] <= 4).astype(int)*(data['period']-1)*12*60 + (data['period'] > 4).astype(int)*((data['period']-4)*5*60 + 3*12*60) + data['secondsFromPeriodStart']\ndata.loc[:10,['period','minutes_remaining','seconds_remaining','secondsFromGameStart']]\n", "intent": "show the newly created fields as a sanity check\n"}
{"snippet": "featureInds = mainLearner.feature_importances_.argsort()[::-1]\nfeatureImportance = pd.DataFrame(np.concatenate((featuresDB.columns[featureInds,None], mainLearner.feature_importances_[featureInds,None]), axis=1),\n                                  columns=['featureName', 'importanceET'])\nfeatureImportance.iloc[:30,:]\n", "intent": "look at the feature importances according to ET Classifier\n"}
{"snippet": "submission=sample_submission.copy()\nsubmission['target']=logit_lv3_test_pred*0.5+ xgb_lv3_test_pred*0.5\nfilename='stacking_demonstration.csv.gz'\nsubmission.to_csv(filename,compression='gzip', index=False)\n", "intent": "Well, for training score, we manage to arravie at 0.28443.\nWe can now try to apply the same weight distribution to generate our submission.\n"}
{"snippet": "from io import StringIO\nfrom sklearn.feature_extraction.text import CountVectorizer\nsent1 = \"The quick brown fox jumps over the lazy brown dog.\"\nsent2 = \"Mr brown jumps over the lazy fox.\"\nwith StringIO('\\n'.join([sent1, sent2])) as fin:\n    count_vect = CountVectorizer(analyzer=preprocess_text)\n    count_vect.fit_transform(fin)\ncount_vect.vocabulary_ \n", "intent": "Or just **override the analyzer** totally with our preprocess text:\n"}
{"snippet": "count_vect = CountVectorizer(analyzer=preprocess_text)\nfull_train_set = count_vect.fit_transform(df_train['request_text_edit_aware'])\nfull_tags = df_train['requester_received_pizza']\ntest_set = count_vect.transform(df_test['request_text_edit_aware'])\nclf = MultinomialNB() \nclf.fit(full_train_set, full_tags) \n", "intent": "More data == better model (in most cases)\n"}
{"snippet": "grailPOS = pd.Series(grail.count_by(spacy.attrs.POS))/len(grail)\npridePOS = pd.Series(pride.count_by(spacy.attrs.POS))/len(pride)\nrcParams['figure.figsize'] = 16, 8\ndf = pd.DataFrame([grailPOS, pridePOS], index=['Grail', 'Pride'])\ndf.columns = [tagDict[column] for column in df.columns]\ndf.T.plot(kind='bar')\n", "intent": "It's fun to compare the distribution of parts of speech in each text: \n"}
{"snippet": "def verbsToMatrix(verbCounts): \n    return pd.Series({t[0]: t[1] for t in verbCounts})\nverbsDF = pd.DataFrame({'Elizabeth': verbsToMatrix(elizabethVerbs), \n                        'Darcy': verbsToMatrix(darcyVerbs), \n                        'Jane': verbsToMatrix(janeVerbs)}).fillna(0)\nverbsDF.plot(kind='bar', figsize=(14,4))\n", "intent": "We can now merge these counts into a single table, and then we can visualize it with Pandas. \n"}
{"snippet": "inaugural = [nlp(open(doc, errors='ignore').read()) for doc in inauguralFilenames]\n", "intent": "Let's load the Inaugural Address documents into SpaCy to analyze things like average sentence length. SpaCy makes this really easy. \n"}
{"snippet": "df_fn = pd.DataFrame(fn_sorted, columns=['right_prob', 'test_idx', 'dataset_idx'])\ndf_fp = pd.DataFrame(fp_sorted, columns=['right_prob', 'test_idx', 'dataset_idx'])\n", "intent": "Each of fp_sorted and fn_sorted has list of tuples (probability of their right class, instance's index in the dataset)\n"}
{"snippet": "X.to_csv('data/training_features.csv', index = False)\nX_test.to_csv('data/testing_features.csv', index = False)\ny.to_csv('data/training_labels.csv', index = False)\ny_test.to_csv('data/testing_labels.csv', index = False)\n", "intent": "The naive method of guessing the median training value provides us a low baseline for our models to beat! \n"}
{"snippet": "df_fn = pd.DataFrame(fn_sorted, columns=['right_prob', 'test_idx'])\ndf_fp = pd.DataFrame(fp_sorted, columns=['right_prob', 'test_idx'])\n", "intent": "Each of fp_sorted and fn_sorted has list of tuples (probability of their right class, instance's index in the dataset)\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import ShuffleSplit\nimport visuals as vs\ndata = pd.read_csv('housing.csv')\nprices = data['MEDV']\nfeatures = data.drop('MEDV', axis = 1)\nprint \"Boston housing dataset has {} data points with {} variables each.\".format(*data.shape)\n", "intent": "[Dataset download link](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)\n"}
{"snippet": "train = pd.read_csv(\"../testdata/train.csv\")\ntest    = pd.read_csv(\"../testdata/test.csv\")\nfull = train.append( test , ignore_index = True )\ntitanic = full[ :891 ]\ndel train , test\nprint ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)\n", "intent": "Now that our packages are loaded, let's read in and take a peek at the data.\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "zip_pop_df = pd.read_csv('zip-pop',delimiter='\\t')\nzip_inc_df = pd.read_csv('zip-income',delimiter='\\t')\nzip_inc_df.drop(['rank','location','city','population'], axis=1, inplace = True)\nzip_inc_df.columns = ['zip_code','ave_income']\nzip_pop_df.columns = ['\nzip_pop_df.drop('\nzip_data_df = pd.merge(zip_pop_df,zip_inc_df,on='zip_code')\nzip_data_df['city'] = zip_data_df['city'].str[:-6]\n", "intent": "Add data from Zipatlas.com.  Median Income and population per zip code in Iowa.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(stop_words='english')\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "polyLonModel = svm.SVR()\nX_train, X_test, y_train, y_test = train_test_split(polyFeatures[['lat','lon','latlon']], dataFt['height'], test_size=0.2)\npolyLonModel.fit(X_train, y_train)\n", "intent": "Ok! let's try with our new feature!\n"}
{"snippet": "def load_data(filename1, filename2):\n    X_complete = pd.read_csv(filename1, usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n    Y_complete = pd.read_csv(filename1, usecols=[20])\n    X_holdout = pd.read_csv(filename2)\n    return X_complete, Y_complete,X_holdout\nX_complete,Y_complete, X_holdout = load_data('data.csv','holdout.csv')\n", "intent": "We get X_complete, Y_complete from data.csv, and get holdout_data from holdout.csv.  \n"}
{"snippet": "df = pd.DataFrame()\nentries = {}\nfor dfStock in dfs:\n    entries[dfStock['Ticker'][0]] = sum(dfStock.isnull().values.ravel())\ndf = df.append(entries,ignore_index=True)\nprint \"Missing value count for each stock\"\ndisplay(df)\ndf.plot(kind='bar',title=\"Missing Value Cells\",figsize=(18,9)).set(xlabel='Ticker', ylabel='Count')\n", "intent": "Let's try to find out whether there are missing values\n"}
{"snippet": "dates = dfs[0]['Date'].values\ndates_list = [dt.datetime.strptime(date, \"%Y-%m-%d\") for date in dates]\ndf = pd.DataFrame(index=dates_list)\ndfNormalized = pd.DataFrame(index=dates_list)\nfor dfStock in dfsProcessed:\n    dfTemp = Util().FilterDataFrameByDate(dfStock.copy(),dates_list[0],dates_list[-1])\n    df[dfStock['Ticker'][0]] = dfTemp['Adjusted_Close'].values\n    dfNormalized[dfStock['Ticker'][0]] = dfTemp['Norm_Adjusted_Close'].values\ndf.plot(title=\"Ajusted Close Prices\",figsize=(18,9)).set(xlabel='Date', ylabel='Price')\ndfNormalized.plot(title=\"Normalized Ajusted Close Prices\",figsize=(18,9)).set(xlabel='Date', ylabel='Price Variation')\n", "intent": "The below figure shows the prices behaviour after pre-processing data:\n"}
{"snippet": "table = pd.DataFrame({'probability':[0.25, 1/3, 2/3, .95]})\ntable['odds'] = table.probability / (1 - table.probability)\ntable\n", "intent": "**Exercise.**\nConvert the following probabilities to odds:\n1. .25\n1. 1/3\n1. 2/3\n1. .95\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfeature_cols = ['gre']\nX = admissions.loc[:, feature_cols]\ny = admissions.loc[:, 'admit']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 46)\nlogit_simple = LogisticRegression()\nlogit_simple.fit(X_train, y_train)\n", "intent": "**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**\n"}
{"snippet": "train = pd.read_csv(dataf,index_col=0)\nfeatures_names = list(train)[1:]\n", "intent": "Next, we give this dataset the `train` name.\n"}
{"snippet": "localtrain, localval=train_test_split(train, test_size=0.25, random_state=2017)\ndrop_cols=['id','target']\ny_localtrain=localtrain['target']\nx_localtrain=localtrain.drop(drop_cols, axis=1)\ny_localval=localval['target']\nx_localval=localval.drop(drop_cols, axis=1)\n", "intent": "Next step, let's create the local train and validation data for this little exercise\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\ndata.head(5)\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "num_trees = [100,500,1000,5000,10000]\noob_score_RF = []\nfor i in num_trees:\n    rf = RandomForestRegressor(n_estimators=i,max_features=6,min_samples_leaf=5,oob_score=True)\n    rf.fit(X,y)\n    oob_score_RF.append(rf.oob_score_)\noob_score_df = pd.DataFrame({'num_trees':num_trees,'oob_scores':oob_score_RF})\nprint oob_score_df\noob_score_df.plot(x='num_trees',y='oob_scores')\n", "intent": "Six features to use at random per node performs best\n"}
{"snippet": "num_samples = range(1,11)\noob_score_RF = []\nfor i in num_samples:\n    rf = RandomForestRegressor(n_estimators=1000,max_features=6,min_samples_leaf=i,oob_score=True)\n    rf.fit(X,y)\n    oob_score_RF.append(rf.oob_score_)\noob_score_df = pd.DataFrame({'num_samples':num_samples,'oob_scores':oob_score_RF})\nprint oob_score_df\noob_score_df.plot(x='num_samples',y='oob_scores')\n", "intent": "500 or 1000 estimators performs best - Varies based on run\n"}
{"snippet": "clf = PCA(4)\nX_trans = clf.fit_transform(demean(X))\nX_trans.shape\n", "intent": "Two principle components capture over 99% of variance\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import mean_squared_error\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)\ntreereg = DecisionTreeRegressor(max_depth=4)\ntreereg.fit(X_train,y_train)\n", "intent": "Depth of 4 appears to be ideal\n"}
{"snippet": "df = pd.read_csv('./AVGO.csv')\ndf1 = pd.read_csv('./AAPL.csv')\ndf2 = pd.read_csv('./SMH.csv')\n", "intent": "The spreadhseets were downloaded from Yahoo Finance. They are also part of the capstone project repository.\n"}
{"snippet": "df_scaled = pd.DataFrame(scaled, columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df = pd.read_csv('College_Data')\ndf.rename(columns={\"Unnamed: 0\":\"College Name\"}, inplace=True)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "X = df_feat\ny = df['TARGET CLASS']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Use train_test_split to split your data into a training set and a testing set.**\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nX,y = datasets.load_diabetes(return_X_y=True)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nreg = xgb.XGBRegressor(objective=squared_log_error,n_estimators=200)\nreg.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=10,eval_metric=rmsle_metric)\n", "intent": "Trying to fit diabetes dataset. Here we use our custom objective function log_error\n"}
{"snippet": "data = pd.read_csv('data/bank.csv', header=0, delimiter=';')\ndata = data.dropna()\nprint(data.shape)\nprint(list(data.columns))\n", "intent": "This dataset provides the customer information. It includes 41188 records and 21 fields.\n"}
{"snippet": "data = pd.read_csv('data/daily_weather.csv')\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nCreating a Pandas DataFrame from a CSV file<br></p>\n"}
{"snippet": "import pandas as pd\ntrain = pd.read_csv(\"../data/train.csv\")\n", "intent": "1. Drop missing values\n2. Fill missing values with test statistic\n3. Predict missing values with a machine learning algorithm\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values = 'NaN', strategy ='mean', axis = 0)\nimp.fit(train)\ntrain = imp.transform(train)\n", "intent": "Alternative way of filling missing value with test statistic is by using our Imputer method found in sklearn.preprocessing.\n"}
{"snippet": "data['Age']= data.fillna(data.Age.median())\ndata['Fare'] = data.fillna(data.Fare.median())\ndata['Embarked'] = data['Embarked'].fillna('S')\ndata.info()\n", "intent": "* Impute missing values:\n"}
{"snippet": "model_results = []\nfor _ in range(100):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=50)\n    vect = CountVectorizer(min_df=2)\n    X_trans = vect.fit_transform(X_train.text)\n    model = MultinomialNB().fit(X_trans, y_train)\n    acc = model.score(vect.transform(X_test.text), y_test)\n    model_results.append(acc)\nsum(model_results)/len(model_results)\n", "intent": "Because the model is perfect, the prediction will completely line up with what we had before.\n"}
{"snippet": "key_pts_frame = pd.read_csv('/data/training_frames_keypoints.csv')\nn = 0\nimage_name = key_pts_frame.iloc[n, 0]\nkey_pts = key_pts_frame.iloc[n, 1:].as_matrix()\nkey_pts = key_pts.astype('float').reshape(-1, 2)\nprint('Image name: ', image_name)\nprint('Landmarks shape: ', key_pts.shape)\nprint('First 4 key pts: {}'.format(key_pts[:4]))\n", "intent": "Then, let's load in our training data and display some stats about that dat ato make sure it's been loaded in correctly!\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'splice_train.csv')\nsplice_train = pd.read_csv(data_path, delimiter = ',')\nprint(\"No. of instances: \", splice_train.shape[0], \"No. of attributes: \", splice_train.shape[1])\nsplice_train.head(10)\n", "intent": "Load the `splice_train.csv` and `splice_test.csv` into two separate dataframes. Display the shape and first 10 instances for each dataframe.\n"}
{"snippet": "pd.DataFrame(np.exp(logr.coef_).T, index = X.columns.values)\n", "intent": "Look at the result of our logistic regression, taking into account how the odds change when compared to each of our control dummy variables:\n"}
{"snippet": "X,y = datasets.load_boston(return_X_y=True)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nreg = xgb.XGBRegressor(objective=squared_log_error,n_estimators=200)\nreg.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=10,eval_metric=rmsle_metric)\n", "intent": "The same for boston dataset boston. Using custom objective here\n"}
{"snippet": "from sklearn.feature_selection import RFE\nrfe = RFE(logr, 10)\nrfe = rfe.fit(X,y)\n", "intent": "Use Recursive Feature Elimination to see if we can hone in on some attributes to rebuild a logr model\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_boston\nboston = load_boston()\nX = pd.DataFrame(boston.data)\ny = pd.DataFrame(boston.target)\n", "intent": "Load the Boston housing data.  Fix any problems, if applicable.\n"}
{"snippet": "import patsy\nX = patsy.dmatrix('~ C(PdDistrict) + C(Resolution) + C(Weekday)', df)\nX = pd.DataFrame(X, columns=X.design_info.column_names)\n", "intent": "If you're having trouble with patsy formulas, read more at http://patsy.readthedocs.io/en/latest/formulas.html\n"}
{"snippet": "df_1k_H = pd.read_csv('../assets/_CSVs/df_1k_H.csv', index_col=0)\ndf_1k_W = pd.read_csv('../assets/_CSVs/df_1k_W.csv', index_col=0)\ndf_1k_S = pd.read_csv('../assets/_CSVs/df_1k_S.csv', index_col=0)\n", "intent": "1000-, 500-, and 100-record samples for each original writer - Twain, Wilde, Lincoln, and Modern\n"}
{"snippet": "df_1k_H = pd.read_csv('_CSVs/df_1k_H.csv', index_col=0)\ndf_1k_W = pd.read_csv('_CSVs/df_1k_W.csv', index_col=0)\ndf_1k_S = pd.read_csv('_CSVs/df_1k_S.csv', index_col=0)\ndf_500_H = pd.read_csv('_CSVs/df_500_H.csv', index_col=0)\ndf_500_W = pd.read_csv('_CSVs/df_500_W.csv', index_col=0)\ndf_500_S = pd.read_csv('_CSVs/df_500_S.csv', index_col=0)\ndf_100_H = pd.read_csv('_CSVs/df_100_H.csv', index_col=0)\ndf_100_W = pd.read_csv('_CSVs/df_100_W.csv', index_col=0)\ndf_100_S = pd.read_csv('_CSVs/df_100_S.csv', index_col=0)\n", "intent": "1000-, 500-, and 100-record samples for each original writer - Twain, Wilde, Lincoln, and Modern\n"}
{"snippet": "results_df = pd.DataFrame(columns=['pair', 'vec', 'features', 'model', 'm_acc', 'm_prec'])\n", "intent": "I would like to be able so store my results of the various tests.\n"}
{"snippet": "categories = [\n    'alt.atheism',\n    'talk.religion.misc',\n]\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\ndata = fetch_20newsgroups(subset='train', categories=categories)\nprint(\"%d documents\" % len(data.filenames))\nprint(\"%d categories\" % len(data.target_names))\nprint()\n", "intent": "Load some categories from the training set\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfeatures = [c for c in df.columns if c != 'acceptability'] \nfor c in df.columns:\n    df[c] = le.fit_transform(df[c]) \nX = df[features] \ny = df['acceptability'] \n", "intent": "Since most of the features are categorical text we will need to encode them as numbers using the LabelEncoder.\n"}
{"snippet": "import graphviz\nexport_graphviz(dt, out_file=\"mytree.dot\")\nwith open(\"mytree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)\n", "intent": "Next, let's visualize the tree:\n"}
{"snippet": "X,y = datasets.load_boston(return_X_y=True)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nreg = xgb.XGBRegressor(objective='reg:linear',n_estimators=200)\nreg.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=10,eval_metric=rmsle_metric)\n", "intent": "Now switch to objective='reg:linear'. The results are better. Why?\n"}
{"snippet": "X = StandardScaler().fit_transform(X)\n", "intent": "Who does a DBSCAN on unscaled data?! Savages. That's who.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=10)\nskl_pca = pca.fit_transform(X_2)\nskl_pca\n", "intent": "Now, repeat the process with sklearn.\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "air = pd.read_csv('datasets/airport.csv')\n", "intent": "In this case, we want to look at this dataset in an unsupervised manner. \n"}
{"snippet": "import numpy as np\nimport pandas as pd\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ntargets = pd.DataFrame(data.target, columns=['MEDV'])\ndata.feature_names\ndata.target\ntargets.head()\n", "intent": "Let's take a minute to see what the data looks like.\n"}
{"snippet": "X_norm = pd.DataFrame(X_norm,columns = features)\n", "intent": "Conduct additional exploratory data analysis. These must include both univariate and bivariate analyses.\n"}
{"snippet": "URL = 'https://raw.githubusercontent.com/josephofiowa/GA-DSI/master/NHL_Data_GA.csv'\nnhl = pd.read_table(URL, names=['Team','PTS','Rank','TOI','GF','GA','GF60','GA60','GF%','SF','SA','SF60','SA60','SF%','FF','FA','FF60','FA60','FF%','CF','CA','CF60','CA60','CF%','Sh%','Sv%','PDO','PIM'],sep=',')\nnhl = nhl[1:]\nnhl.head()\n", "intent": "Feel free to also do basic EDA. At least check the head()!\n"}
{"snippet": "votes = pd.read_csv(votes_file)\nairport = pd.read_csv(airport_file)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas and handle any preprocessing that it may need. \n"}
{"snippet": "cvt = CountVectorizer(max_df=50)\nX_all = cvt.fit_transform(insults_df[\"Comment\"])\nwords = cvt.vocabulary_\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "X = insults_df['Comment']\ny = insults_df['Insult']\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size=.33)\n", "intent": "Try 70/30 to start.\n"}
{"snippet": "from sklearn import preprocessing    \ndf = pd.DataFrame(['A', 'B', 'B', 'C'], columns=['Col'])    \ndf['Fact'], indexer = pd.factorize(df['Col'])\n", "intent": "[Ref](https://stackoverflow.com/questions/40336502/want-to-know-the-diff-among-pd-factorize-pd-get-dummies-sklearn-preprocessing)\n"}
{"snippet": "import tensorflow as tf\nimport numpy as np\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\n", "intent": "Let's try out using `tf.keras` and Cloud TPUs to train a model on the fashion MNIST dataset.\nFirst, let's grab our dataset using `tf.keras.datasets`.\n"}
{"snippet": "coeffs = pd.DataFrame(model.coef_, columns = iris.feature_names, index =iris.target_names)\ncoeffs\n", "intent": "some cause neg changes other positive changes\n"}
{"snippet": "from sklearn.feature_selection  import SelectKBest\nkbest=SelectKBest(k=5)\n", "intent": "other libraries  google feature selection\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer(\n    binary=True,  \n    stop_words='english', \n    max_features=50, \n)\nX = v.fit_transform(data.title).todense()     \nX = pd.DataFrame(X, columns=v.get_feature_names())\nX.head()\n", "intent": "- `CountVectorizer` builds a feature per word automatically as we did manually for `recipe`, `electronic` above.\n"}
{"snippet": "from sklearn import datasets \ndata = datasets.load_boston()\n", "intent": "from sklearn import datasets\ndata = dataset.load_boston()\n"}
{"snippet": "df = pd.read_csv(\"../../assets/datasets/iris.csv\")\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "df = pd.read_csv(\"../assets/datasets/votes.csv\", index_col=0)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "napolean = StandardScaler().fit_transform(x)\nprint napolean.shape\nprint napolean[0:5]\n", "intent": "Next, create the covariance matrix from the standardized x-values and decompose these values to find the eigenvalues and eigenvectors\n"}
{"snippet": "df = pd.read_csv(\"../../assets/datasets/airport_operations.csv\", index_col=0)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "WORK_DIRECTORY = '/tmp/pytorch-example/cifar-10-data'\ndata_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)\n", "intent": "We will use the tools provided by the SageMaker Python SDK to upload the data to a default bucket.\n"}
{"snippet": "path = Path('.', 'data', 'titanic.csv')\ntitanic = pd.read_csv(path)\ntitanic.head()\n", "intent": "We'll build a classification tree using the Titanic survival data set:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXs_train = ss.fit_transform(X_train)\nXs_test= ss.fit_transform(X_test)\n", "intent": "Make sure to...\n* instantiate a `StandardScaler` object\n* `fit` the scaler on your training data\n* `transform` both your training and test data.\n"}
{"snippet": "pd.DataFrame(example_results)\n", "intent": "Then we pass the results list to pass to a DataFrame.\n"}
{"snippet": "pd.DataFrame(results)\n", "intent": "Use a DataFrame to display your results.\n"}
{"snippet": "def plot_coef(model, top_n = 10):\n    cols = X_train.columns\n    coef = model.coef_\n    zipped = list(zip(cols, coef))\n    zipped.sort(key=lambda x: x[1], reverse = True)\n    top_10 = pd.DataFrame(zipped).head(top_n)\n    bottom_10 = pd.DataFrame(zipped).tail(top_n)\n    return pd.concat([top_10, bottom_10], axis=0).plot.barh(x = 0, y = 1)\n", "intent": "For your best model, \n* plot relevant coefficients using the `plot_coef` functoin.\n"}
{"snippet": "def plot_coef(model, top_n = 10):\n    cols = X_train.columns\n    coef = model.coef_\n    zipped = list(zip(cols, coef))\n    zipped.sort(key=lambda x: x[1], reverse = True)\n    top_10 = pd.DataFrame(zipped).head(top_n)\n    bottom_10 = pd.DataFrame(zipped).tail(top_n)\n    return pd.concat([top_10, bottom_10], axis=0).plot.barh(x = 0, y = 1)\n", "intent": "For your best model, \n* plot relevant coefficients using the `plot_coef` functoin.\n* Interpret coefficients in terms of \\$ value\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=42)\n", "intent": "But in reality our model has to form predictions on *unseen* data. Let's model this situation.\n"}
{"snippet": "from sklearn.datasets import load_iris\ndata = load_iris()\nX = data.data\ny = data.target\nprint(type(X))\nprint(X.shape)\nprint(\"First three rows of data\\n %s\" % X[:3])\nprint(\"First three labels: %s\" % (y[:3]))\n", "intent": "The data set is distributed with sci-kit learn, the only thing we have to do is to important a function and call it.\n"}
{"snippet": "from sklearn.datasets import load_iris\ndata = load_iris()\nX = data.data\ny = data.target\nprint(type(X))\nprint(X.shape)\nprint(\"First three rows of data\\n {}\".format(X[:3]))\nprint(\"First three labels: {}\".format(y[:3]))\n", "intent": "The data set is distributed with sci-kit learn, the only thing we have to do is to important a function and call it.\n"}
{"snippet": "WORK_DIRECTORY = '/tmp/cifar-10-data'\ndata_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)\n", "intent": "We will use the tools provided by the SageMaker Python SDK to upload the data to a default bucket.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"/home/ubuntu/workspace/src/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "data = load_digits()\nX = data.data\nprint(X.shape)\n", "intent": "* Laad de digits dataset.\n* Sla de data samples op onder de variabele 'X'.\n* Hoeveel samples zijn er? En hoeveel features heeft elk sample?\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n", "intent": "* Verdeel de data in een train set (80%) en test set (20%).\n"}
{"snippet": "X = load_digits()\nsamples,col=X.data.shape\nprint(samples)\nprint(col)\n", "intent": "* Laad de digits dataset.\n* Sla de data samples op onder de variabele 'X'.\n* Hoeveel samples zijn er? En hoeveel features heeft elk sample?\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X.data, y, test_size=0.2)\nprint(len(X_train))\nprint(len(X_test))\n", "intent": "* Verdeel de data in een train set (80%) en test set (20%).\n"}
{"snippet": "df_trace=pm.trace_to_dataframe(traces_13_2_2, varnames=['alpha_ed'])\npd.plotting.scatter_matrix(df_trace.iloc[-1000:,0:6], diagonal='kde')\ndf_trace.iloc[burnin::,0:6].corr()\n", "intent": "Below we will look at the parameter correlation plots of the alpha values and see that there are a lot of them!\n"}
{"snippet": "sac = pd.read_csv('Sacramentorealestatetransactions.csv')\nsac.head()\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, Y_pred))\nconfusion = pd.DataFrame(conmat, index=['under_200k', 'over_200k'],\n                         columns=['predicted_under_200k','predicted_over_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "import pandas\nurl = 'http://www.jdawiseman.com/papers/trivia/monopoly-rents.html'\ndfs = pandas.read_html(url, header=1, index_col=0)\n", "intent": "The set of properties is on an HTML table here:\nhttp://www.jdawiseman.com/papers/trivia/monopoly-rents.html\n"}
{"snippet": "response = s3_client.get_object(Bucket=data_bucket_name, Key=file_data)\nresponse_body = response[\"Body\"].read()\ncounties = pd.read_csv(io.BytesIO(response_body), header=0, delimiter=\",\", low_memory=False) \n", "intent": "Grab the data from the CSV file in the bucket.\n"}
{"snippet": "import pandas\nwheel = pandas.read_csv('wheel.csv')\nwheel\n", "intent": "The data is in wheel.csv.\nWe would like to understand the relationship between _seconds_ and _signal_\n"}
{"snippet": "with open('pg1679.txt') as f:\n    one_book = f.read().decode('utf-8',errors='ignore')\n", "intent": "total word count \n[121684, 596920, 18834, 7322, 34693, 92436, 6002, 3991, 38592, 7961, 67483, 69952, 13895, 41845, 136545, 24523, 10014, 88327]\n"}
{"snippet": "pd.read_csv(submission_file_name, index_col='id')\n", "intent": "You can download this file and submit on the Kaggle website or use the Kaggle command line tool's \"submit\" method.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\nimport pandas as pd\nfile = open(\"B1.txt\", 'rt')\ntext = file.read()\ntext=nltk.word_tokenize(text)\nprint(text[0:10])\n", "intent": "Text files for books was obtained from http://www.glozman.com/textpages.html.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\nimport pandas as pd\nfile = open(\"B1.txt\", 'rt')\ntext = file.read()\ntext=nltk.word_tokenize(text)\nprint(text[0:15])\n", "intent": "Text for books was obtained from http://www.glozman.com/textpages.html.\n"}
{"snippet": "feature_cols = ['gre']\nX = admissions.loc[:, feature_cols]\ny = admissions.loc[:, 'admit']\nX_train, X_test, y_train, y_test = (\n    model_selection.train_test_split(X, y, random_state=46)\n)\nlogit_simple = linear_model.LogisticRegression().fit(X_train, y_train)\n", "intent": "**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**\n"}
{"snippet": "vect = CountVectorizer()\nvect = vect.fit(X_train)\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\n", "intent": "<a id='countvectorizer-model'></a>\n"}
{"snippet": "sanders_df = pd.DataFrame(sanders)\nsanders_df.head()\n", "intent": "> *Hint: this is as easy as passing it to the DataFrame constructor!*\n"}
{"snippet": "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", header = None)\ndata_test = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", header = None, skiprows=1)\ndata.columns = ['age', 'workclass','fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', \n'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'IncomeGroup']\ndata_test.columns = ['age', 'workclass','fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', \n'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'IncomeGroup']\n", "intent": "Now lets read this into a Pandas data frame and take a look.\n"}
{"snippet": "scaler = StandardScaler()\nscaled_data = scaler.fit_transform(X)\n", "intent": " --------------------------------------------------------------------------------------\n"}
{"snippet": "scaled_data_df = pd.DataFrame(scaled_data)\nfor col in scaled_data_df.columns:\n    scaled_data_df[col].hist()\n", "intent": "** Distribution for all columns:**\n"}
{"snippet": "pca_1 = PCA(n_components = 5, random_state=0)\ndata_pca_1 = pca_1.fit_transform(scaled_data)\npca_1\n", "intent": "***The dataset appears to have a normal distribution but some skew to the right.***\n"}
{"snippet": "pd.read_csv('file_name.csv').to_sql('table_name',con=conn,if_exists='replace',index=False)\n", "intent": "Load our csv files into tables\n"}
{"snippet": "airport_df = pd.read_csv('airports.csv')\n", "intent": "Join airport_cancellations.csv and airports.csv into one table (try to do it with both pandas and SQLite)\n"}
{"snippet": "pca = PCA(n_components = 5, random_state=0)\ndata_pca = pca.fit_transform(scaled_data)\npca\n", "intent": "***The dataset appears to have a normal distribution but some skew to the right.***\n"}
{"snippet": "with open('anna.txt', 'r') as f:\n    text = f.read()\nvocab = set(text)\nvocab2int = {c: i for i, c in enumerate(vocab)}\nint2vocab = dict(enumerate(vocab))\nchars = np.array([vocab2int[c] for c in text], dtype=np.int32)\n", "intent": "First we load the text file and convert it into integers for network to use\n"}
{"snippet": "data_train['num_room'].fillna(data_train[\"num_room\"].mean(), inplace=True)\ndata_test['num_room'].fillna(data_train[\"num_room\"].mean(), inplace = True)\ndata_train.head()\ndata_test.head()\n", "intent": "Let's just build a model using the mean num of rooms for the missing values\n"}
{"snippet": "ids_valid = val_ids.iloc[:,0].values\ntemp_df = pd.DataFrame()\ntemp_df[\"images\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(ids_valid)]\ntemp_df[\"masks\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(ids_valid)]\nx_valid = np.array(temp_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\ny_valid = np.array(temp_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\ndel temp_df\ngc.collect()\n", "intent": "I have fixed the validation set for all my experiments. This allows me to easily compare different model performance.\n"}
{"snippet": "train_list = np.random.rand(len(data_bin)) < 0.8\ndata_train = data_bin[train_list]\ndata_val = data_bin[~train_list]\ndata_train.to_csv(\"formatted_train.csv\", sep=',', header=False, index=False) \ndata_val.to_csv(\"formatted_val.csv\", sep=',', header=False, index=False) \ndata_test_bin.to_csv(\"formatted_test.csv\", sep=',', header=False,  index=False) \n", "intent": "Split the data into 80% training and 20% validation and save it before calling XGboost\n"}
{"snippet": "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)\n", "intent": "Using the salt coverage as a stratification criterion. Also show an image to check for correct upsampling.\n"}
{"snippet": "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\ntags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n                     dtype=\"long\", truncating=\"post\")\n", "intent": "Next, we cut and pad the token and label sequences to our desired length.\n"}
{"snippet": "train = pd.read_csv(TRAIN_DATA_FILE)\ntest = pd.read_csv(TEST_DATA_FILE)\nlist_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values\n", "intent": "Read in our data and replace missing values:\n"}
{"snippet": "tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n", "intent": "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed).\n"}
{"snippet": "COMMENT = 'comment_text'\ntrain[COMMENT].fillna(\"unknown\", inplace=True)\ntest[COMMENT].fillna(\"unknown\", inplace=True)\n", "intent": "There are a few empty comments that we need to get rid of, otherwise sklearn will complain.\n"}
{"snippet": "submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\nsubmission.to_csv('submission.csv', index=False)\n", "intent": "And finally, create the submission file.\n"}
{"snippet": "df_train = pd.read_csv('titanic/train_preprocessed.csv')\n", "intent": "Read our preprocessed data from our last lesson, in the `titanic` data folder.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "We can use the `iris` dataset which is built into sklearn.  You can import the dataset directly as follows.\n"}
{"snippet": "pd.DataFrame({'lags':range(30), 'pvalue':sm.stats.diagnostic.acorr_ljungbox(arima_mod.resid.values, lags=30)[1], \n              'critial':np.array([0.05]*30)}).set_index('lags').plot(figsize=(15,5))\n", "intent": "Autocorrelation test\n"}
{"snippet": "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_header, strip_newsgroup_quoting, strip_newsgroup_footer\ndata = []\nfor f in file_list:\n    with open(f, 'rb') as fin:\n        content = fin.read().decode('latin1')        \n        content = strip_newsgroup_header(content)\n        content = strip_newsgroup_quoting(content)\n        content = strip_newsgroup_footer(content)        \n        data.append(content)\n", "intent": "Here we read in the content of all the files and remove the header, footer and quotes (of earlier messages in each email).\n"}
{"snippet": "count_vect_train_transformer = CountVectorizer()\ncount_vect_train = count_vect_train_transformer.fit_transform(train_data)\n", "intent": "*Hint : Use Count vectorizer*\n- .fit()\n- .transform()\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "X = pd.DataFrame(iris.data, columns=iris.feature_names)\nY = iris.target\n", "intent": "Define the \"x\" and \"y\" variables. *Remember*, y is our classifer, and x is our attributes.\n"}
{"snippet": "vote_df = pd.read_csv(votes_file)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "location_data = pd.read_csv('../data/ia_zip_city_county_sqm.csv')\n", "intent": "To ensure accuracy, we are importing a list of Iowa zip codes, cities, counties and county numbers.\n"}
{"snippet": "location_data = pd.read_csv('/Users/stel/joce/data_science/project-3-datasets/ia_zip_city_county_sqm.csv')\n", "intent": "To ensure accuracy, we are importing a list of Iowa zip codes, cities, counties and county numbers.\n"}
{"snippet": "from sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nboston = datasets.load_boston()\nX = boston.data\ny = boston.target\n", "intent": "Use the following code to import the Boston House Prices dataset and linear models in python.\n"}
{"snippet": "from sklearn import datasets\ndiabetes = datasets.load_diabetes()\nX = diabetes.data\ny = diabetes.target\n", "intent": "Use the following codes to import the diabetes dataset.\n"}
{"snippet": "from sklearn import datasets\ndiabetes = datasets.load_diabetes()\nX = diabetes.data\ny = diabetes.target\nfrom sklearn.preprocessing import PolynomialFeatures\nX = PolynomialFeatures(2, include_bias=False).fit_transform(X)\n", "intent": "Import the diabetes dataset as in Q1 and add the interaction variables.\nYou should have 65 variables and one target variable.\n"}
{"snippet": "import tarfile\nfrom sagemaker.session import Session\nwith tarfile.open('onnx_model.tar.gz', mode='w:gz') as archive:\n    archive.add('resnet152v1.onnx')\nmodel_data = Session().upload_data(path='onnx_model.tar.gz', key_prefix='model')\n", "intent": "Now that we have the model data locally, we will need to compress it and upload the tarball to S3 for the SageMaker Python SDK to create a Model\n"}
{"snippet": "images_new = pca.inverse_transform(images_pca)\nprint(\"transformed shape:\", images_new.shape)\n", "intent": "The transformed data has been reduced to a single dimension. Then we reproject the data back using the inverse transform\n"}
{"snippet": "examples = [] \nfor file in frames: \n    with fs.open(file, 'rb') as f: \n        examples.append(imresize(imread(f), .125)) \n", "intent": "Read in the images into an array, downsizing by an eigth\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(examples, target,\n                                                random_state=42, test_size=0.3)\n", "intent": "Now we'll use the utility function ```train_test_split``` provided by scikit-learn, to create a random split of test and train images\n"}
{"snippet": "pca  =  PCA(n_components =10).fit(Xtrain)\nimages_pca = pca.transform(Xtrain)\ntree = DecisionTreeClassifier().fit(images_pca, ytrain)\ntree.score(pca.transform(Xtest), ytest)\n", "intent": "About 10 components explains ~90% of the variance. Let's use those\n"}
{"snippet": "names = [f for f in fs.ls(root+'/imrecog_data/NWPU-RESISC45/train/airplane') if f.endswith('.jpg')]\npositive_patches = []\nfor name in names:\n    with fs.open(name, 'rb') as f:\n        positive_patches.append(color.rgb2gray(imread(f, 'jpg')))\n", "intent": "We need to build libraries of both positive (airplane) and negative (not airplane) examples\n"}
{"snippet": "infile = infile.replace('.segments.pkl', '.jpg')\ntraining_rgb = imread(infile)\n", "intent": "Let's display that matrix ontop of the image\n"}
{"snippet": "infile = 'data/1367407802.Wed.May.01_11_30_02.GMT.2013.jvspeijk.c2.snap.jpg' \ntest_rgb = imread(infile)\ntest_rgb = test_rgb[:nrows_mask,:,:]\ntest_rgb = preprocessing.scale(test_rgb[:,:,0]) \nM_tst, N_tst = test_rgb.shape\n", "intent": "Let's test at an image from the same place but at a different time\n"}
{"snippet": "from scipy.misc import imresize\ninfile = 'data/1367407802.Wed.May.01_11_30_02.GMT.2013.jvspeijk.c2.snap.jpg' \ntest_rgb = imread(infile)\ninfile = 'data/1369558802.Sun.May.26_09_00_02.GMT.2013.jvspeijk.c2.snap.jpg'\ntraining_rgb = imread(infile)\ntraining_rgb = imresize(training_rgb, .125)\ntest_rgb = imresize(test_rgb, .125)\n", "intent": "We're going to use downscaled versions of images to speed up the process\n"}
{"snippet": "with fs.open(root+'/imrecog_data/UCMerced_LandUse/Images/train/beach/beach99.jpg', 'rb') as fim:\n    image = imread(fim) \n", "intent": "Load in a test image and show it\n"}
{"snippet": "WORK_DIRECTORY = 'data'\ntrain_input = sagemaker_session.upload_data(WORK_DIRECTORY, key_prefix=\"{}/{}\".format(prefix, WORK_DIRECTORY) )\n", "intent": "Once we have the data locally, we can use use the tools provided by the SageMaker Python SDK to upload the data to a default bucket. \n"}
{"snippet": "imgr = imresize(img, fct)\nLcr = np.round(imresize(Lc, fct, interp='nearest')/255 * np.max(Lc))\n", "intent": "Step 7: We're going to resize to speed things up a little in the next step (CRF)\n"}
{"snippet": "print('CRF ... ')\nres = getCRF(imgr, Lcr.astype('int'), theta, n_iter, labels, compat_spat, compat_col, scale, prob)\ndel imgr\nresr = np.round(imresize(res, 1/fct, interp='nearest')/255 * np.max(res))\ncode1 = np.unique(res)\ncode2 = np.unique(resr)   \nresrr = np.zeros(np.shape(resr), dtype='int8')\nfor kk in range(len(code1)):\n   resrr[resr==code2[kk]] = code1[kk]   \ndel res, resr\n", "intent": "Step 8: Conditional Random Field post-processing. This takes a couple of minutes\n"}
{"snippet": "from scipy.misc import imresize\nfrom imageio import imread\nimages = []\nfor file in image_files:\n    with fs.open(file, 'rb') as f:\n        images.append(imresize(imread(f), imsize))\n", "intent": "Below we're reading and resizing each image one by one\n"}
{"snippet": "from scipy.io import loadmat\nclasses = []\nfor file in class_files:\n    with fs.open(file, 'rb') as f:\n        tmp = (loadmat(f)['class']!=2).astype('uint8')\n        classes.append(imresize(tmp, imsize).astype('int'))\n", "intent": "The classes are stored in .mat format. Water is class '2'\nWe load in each file, binarize it (water=0, not water=1), and resize it\n"}
{"snippet": "reviews = pd.read_csv('reviews.txt', header=None)\nlabels = pd.read_csv('labels.txt', header=None)\n", "intent": "- store multiple reviews in DataFrame dim(n,1)\n"}
{"snippet": "bank_data = pd.read_csv('./data/bank_campaign_small.csv')\nbank_data.head()\n", "intent": "We will be using the dataset available from [UCI data repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n"}
{"snippet": "import pandas as pd\nfrom io import StringIO\ncsv_data = unicode(csv_data)\ndf = pd.read_csv(StringIO(csv_data))\ndf\n", "intent": "Raschka, Sebastian. Python Machine Learning. Packt Publishing. Kindle Edition. \n"}
{"snippet": "import pandas as pd\nfrom sklearn import model_selection, metrics\nimport numpy as np\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nsample_submission = pd.read_csv(\"sample_submission.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "print(predict, y_val)\nprint(my_scaler.inverse_transform(predict))\nprint(my_scaler.inverse_transform(y_val))\n", "intent": "Predicting all values\n"}
{"snippet": "from sklearn import preprocessing\ninterval_data = df.as_matrix(columns=interval_attributes)\ninterval_imputer = preprocessing.Imputer(strategy='mean')\nimputed_interval_data = interval_imputer.fit_transform(interval_data)\nnominal_data = df.as_matrix(columns=nominal_attributes)\nbinary_data  = df.as_matrix(columns=binary_attributes)\ncat_imputer = preprocessing.Imputer(strategy='most_frequent')\nimputed_nominal_data = cat_imputer.fit_transform(nominal_data)\nimputed_binary_data  = cat_imputer.fit_transform(binary_data)\n", "intent": "- Finding the missing values.\n- Imputing the missing values.\n"}
{"snippet": "def make_xy(critics, vectorizer=None):\n    if vectorizer is None:\n        vectorizer = CountVectorizer(ngram_range=(1, 5))\n    X = vectorizer.fit_transform(critics.quote)\n    X = X.tocsc()  \n    y = (critics.fresh == 'fresh').values.astype(np.int)\n    return X, y\nX, y = make_xy(critics)\n", "intent": "For task 1, we can try 2 to 5 grams. and use the same to build a niave bayes model with the same alpha as before.\n"}
{"snippet": "vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\nX, y = make_xy(critics, vectorizer)\ntrain_X, test_X, train_y, test_y =  train_test_split(X,y, train_size = 0.7)\nclf = MultinomialNB(alpha=1).fit(xtrain, ytrain)\ntraining_accuracy = clf.score(xtrain, ytrain)\ntest_accuracy = clf.score(xtest, ytest)\nprint(\"Accuracy on training data: {:2f}\".format(training_accuracy))\nprint(\"Accuracy on test data:     {:2f}\".format(test_accuracy))\n", "intent": "For task 5, we will use tfidfvectorizer as the example above.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidfvectorizer = TfidfVectorizer(min_df=1, stop_words='english')\nXtfidf=tfidfvectorizer.fit_transform(critics.quote)\nX = Xtfidf.tocsc()  \ny = (critics.fresh == 'fresh').values.astype(np.int)\n", "intent": "For task 5, we will use tfidfvectorizer as the example above.\n"}
{"snippet": "milk = pd.read_csv('./monthly-milk-production.csv', index_col='Month')\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "electoral_votes = pd.read_csv(\"hw2_data/electoral_votes.csv\").set_index('State')\nelectoral_votes.head()\n", "intent": "*As a matter of convention, we will index all our dataframes by the state name*\n"}
{"snippet": "import statsmodels.api as sm\nfrom statsmodels.formula.api import logit, glm, ols\ndat = pd.DataFrame(data, columns = ['Temperature', 'Failure'])\nlogit_model = logit('Failure ~ Temperature',dat).fit()\n", "intent": "Lets plot this data\n"}
{"snippet": "excel = pd.ExcelFile('breatcancer_expr.xlsx').parse(sheet_name=0, dtype=object, engine='xlrd', verbose=True)\nexcel.to_csv(path_or_buf='expr.csv', sep=',', header=True, index=False, mode='w', encoding='CP949')\nexpr = pd.read_csv('expr.csv', engine='c', sep=',', encoding='CP949')\nexcel = pd.ExcelFile('breatcancer_clinical.xlsx').parse(sheet_name=0, dtype=object, engine='xlrd', verbose=True)\nexcel.to_csv(path_or_buf='clinic.csv', sep=',', header=True, index=False, mode='w', encoding='CP949')\nclinical = pd.read_csv('clinic.csv', engine='c', sep=',', encoding='CP949')\n", "intent": "- Load excel files by using Pandas\n- Change excel files to csv files\n- Csv files are faster than excel files when we read data\n"}
{"snippet": "sc = StandardScaler()\nsc.fit(x_train)\nx_train_std = sc.transform(x_train)\nx_test_std = sc.transform(x_test)\n", "intent": "- Use StandardScaler\n- Standardize train data & test data\n"}
{"snippet": "df = MinMaxScaler().fit_transform(df)\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "loan = pd.read_csv(\"loan_data.csv\")\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "kp = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaled_dp = pd.DataFrame(scaled_feature,columns= kp.columns[ :-1])\nscaled_dp.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data = pd.read_csv('https://github.com/Thinkful-Ed/data-201-resources/raw/master/hotel-reviews.csv')\n", "intent": "This notebook will guide you through the creation of a simple bag of words model for text matching.\n"}
{"snippet": "iris_dataframe = pd.DataFrame(X_train, columns = iris_dataset.feature_names)\npd.plotting.scatter_matrix(\n    iris_dataframe,\n    c = y_train,\n    figsize = (15, 15),\n    marker = 'o',\n    hist_kwds = {'bins':20},\n    s = 60,\n    alpha = .8\n)\n", "intent": "<h3>Create Pair Plots</h3>\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    iris_dataset['data'], iris_dataset['target'], random_state = 0\n)\nknn = KNeighborsClassifier(n_neighbors = 1)\nknn.fit(X_train, y_train)\nprint(\"Test set score : {:2f}\".format(knn.score(X_test, y_test)))\n", "intent": "<h3>Training and Evaluation in one line</h3>\n"}
{"snippet": "d = load_iris()\n", "intent": "https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1469-1809.1936.tb02137.x\n"}
{"snippet": "import spacy\nfile = open(\"trump.txt\", \"r\",encoding='utf-8') \ntrump = file.read() \nnlp = spacy.load(\"en\")\ndoc = nlp(trump)\nfor span in doc.sents:\n    print(\"> \", span)\n", "intent": "Let's take one of President Trump's speech and divide into words.\n"}
{"snippet": "from sklearn.manifold import TSNE\nimport pandas as pd\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X)\ndf = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\ndf\n", "intent": "To train we just use the TSNE to reduce the dimensionality:\n"}
{"snippet": "from rasa_nlu.training_data import load_data\nfrom rasa_nlu.model import Trainer, Interpreter\nfrom rasa_nlu.components import ComponentBuilder\nimport rasa_nlu.config\ncfg = 'config.json'\ntraining_data = load_data('anna.json')\ntrainer = Trainer(rasa_nlu.config.load(cfg))\ntrainer.train(training_data)\nmodel_directory = trainer.persist('.')\n", "intent": "The training is straight forward.\n"}
{"snippet": "df = pd.read_csv('College_Data',index_col=0)\n", "intent": "** Import the libraries you usually use for data analysis.**\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint(\"The MNIST database has a training set of %d examples.\" \n      % len(X_train))\nprint(\"The MNIST database has a test set of %d examples.\" % len(X_test))\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the MNIST database.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"/Users/user/Desktop/DataScience-Python3/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "scaler = StandardScaler()\ncustomer_sc = scaler.fit_transform(customer_features)\ncustomer_sc_df = pd.DataFrame(customer_sc, columns=customer_features.columns)\nsc_stats = customer_sc_df.describe().T\nsc_stats['skew'] = st.skew(customer_features)\nsc_stats['kurt'] = st.kurtosis(customer_features)\ndisplay(stats)\ndisplay(sc_stats)\n", "intent": "$$Z = \\frac{X-\\mu}{\\sigma}$$\n"}
{"snippet": "train, test = train_test_split(boston_df, test_size = 0.3)\ntrain.head(5)\n", "intent": "b) Divide the filtered data randomly into a train set (70% of the data) and test set (30% of the data).\n"}
{"snippet": "boston_df = pd.read_csv('Boston1.csv')\nboston_train, boston_test = train_test_split(boston_df, test_size = 0.3)\nboston_df.head(4)\n", "intent": "1) Create train and test sets from the Boston data set.\n"}
{"snippet": "train = pd.read_csv(\"train.csv\")\n", "intent": "- We need to open the file as Pandas DataFrame .. \n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, binarized_labels, test_size=0.3)\n", "intent": "- http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html\n"}
{"snippet": "fa = FactorAnalysis()\nprint 'components'\nprint fa.components_ \n", "intent": "For homoscedastic case, both fa and pca work well\n"}
{"snippet": "data.to_csv(os.path.join(output_dir,'cluster_data_w_label.csv'), index = False, sep = '|')\n", "intent": "** Save the data matrix with the cluster lables **\n"}
{"snippet": "adv = pd.read_csv(\"advertising.csv\")\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "scale = StandardScaler()\nXs = scale.fit_transform(X)\n", "intent": "We know that we need to scale our data for the KNN algorithm right?\n"}
{"snippet": "ss = StandardScaler()\ncities_ss = ss.fit_transform(cities)\ncities_ss = pd.DataFrame(cities_ss, index=cities.index, columns=cities.columns)\ncities_ss.head()\n", "intent": "This data definitely needs scaling\n"}
{"snippet": "data = make_classification(n_samples=400, n_features=2,n_informative=2,\n                           n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, random_state=1)[0]\n", "intent": "Clustering is usually never this easy. Let's look at some more difficult data.\n"}
{"snippet": "ss = StandardScaler()\nXs = ss.fit_transform(X)\nkm3_s = KMeans(n_clusters=3, random_state=10)\nkm3_s.fit(Xs)\nlabs3_s = km3_s.labels_\npd.value_counts(labs3_s)\n", "intent": "Let's go ahead and scale the data and refit a clustering algorithm\n"}
{"snippet": "country_labels = pd.DataFrame(index=df.index, data=labs3_s, columns=[\"cluster\"])\ncountry_labels.head()\n", "intent": "Let's go ahead and look at some of the countries in each cluster.\nWe're going randomly pick 10 countries from each cluster (0, 1, 2)\n"}
{"snippet": "X, y = make_moons(n_samples=3000,\n                    noise=0.12,\n                    random_state=0)\n", "intent": "We're going to use the keras library to the fake moons dataset from sklearn\n"}
{"snippet": "Xr, yr = make_regression(n_samples=2000, n_features=1, noise=2, random_state=4,bias = 0.9)\n", "intent": "Now let's train a neural net on a regression dataset\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.25, random_state=42)\n", "intent": "Try it out on a testing set\n"}
{"snippet": "sample = {\"income\":[30000, 55000, 36000], \n          \"white_pop\":[50, 85, 95], \n          \"college_deg\":[15, 40, 50], \n          \"class\":[\"A\",\"B\", \"X\"]}\nsample= pd.DataFrame(sample)\nsample\n", "intent": "Let's take a look at this sample data set.\n"}
{"snippet": "teams = pd.read_csv(zf.open(tablenames[tablenames.index('Teams.csv')]))\nplayers = pd.read_csv(zf.open(tablenames[tablenames.index('Batting.csv')]))\nsalaries = pd.read_csv(zf.open(tablenames[tablenames.index('Salaries.csv')]))\nfielding = pd.read_csv(zf.open(tablenames[tablenames.index('Fielding.csv')]))\nmaster = pd.read_csv(zf.open(tablenames[tablenames.index('Master.csv')]))\n", "intent": "Create pandas DataFrames for each of the five data sets. \n"}
{"snippet": "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\ndata.head()\nX = data.drop(\"sales\", axis = 1)\ny = data.sales\n", "intent": "Let's use train/test split with RMSE to decide whether Newspaper should be kept in the model:\n"}
{"snippet": "X = titanic.drop(\"survived\", axis =1)\ny = titanic.survived\nX_train, X_test, y_train, y_test = train_test_split(X ,y, test_size = .25, random_state = 4)\n", "intent": "We are going to split our titanic dataset into two sets: training and testing.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X ,y, \n                                                    test_size = .25,\n                                                   random_state = 42)\nmodel = DecisionTreeClassifier(max_depth = 6, random_state= 42)\nmodel.fit(X_train, y_train)\ntestscore = model.score(X_test, y_test)\nprint (\"The test score is {:.3f} percent\".format(testscore*100))\n", "intent": "Train a model with the best depth value and evaluate it on a test set\n"}
{"snippet": "from sklearn.tree import export_graphviz\nimport graphviz\nexport_graphviz(dt, out_file='titanic.dot', \n                    feature_names=X.columns, \n                    class_names=[\"dead\", \"alive\"])\nwith open(\"titanic.dot\") as f: \n        dot_graph = f.read()\ngraphviz.Source(dot_graph)\n", "intent": "Visualize the tree!\n"}
{"snippet": "from sklearn.tree import export_graphviz\nimport graphviz\nexport_graphviz(iris_model, out_file='iris.dot', \n                    feature_names=X.columns, \n                    class_names=y.unique())\nwith open(\"iris.dot\") as f: \n        dot_graph = f.read()\ngraphviz.Source(dot_graph)\n", "intent": "Visualize the decision trees using graphviz\n"}
{"snippet": "le = LabelEncoder()\nemb_encoded = le.fit_transform(X_train.Embarked)\nemb_encoded[:20]\n", "intent": "Now let's try this on the Embarked column\n"}
{"snippet": "onehot = OneHotEncoder()\nemb_onehot = onehot.fit_transform(emb_encoded.reshape(-1, 1))\nemb_onehot.toarray()\n", "intent": "How to use the OneHotEncoder object\n"}
{"snippet": "lb = LabelBinarizer()\nbinarized_data = lb.fit_transform(X_train.Embarked.fillna(\"unknown\"))\nbinarized_data[:30]\n", "intent": "We can also use the LabelBinarizer to do this as well\n"}
{"snippet": "cm_digits = pd.DataFrame(confusion_matrix(y_test, preds))\ncm_digits\n", "intent": "Confusion matrix time\n"}
{"snippet": "import sklearn\ndigits = sklearn.datasets.load_digits()\n", "intent": "Character recognition in images is a classic problem solved with neural networks.\n"}
{"snippet": "path = \"../../data/NLP_data/yelp.csv\"\nyelp = pd.read_csv(path, encoding='unicode-escape')\nyelp.head()\n", "intent": "Let's analyze the sentiment of yelp reviews\n"}
{"snippet": "vect = CountVectorizer()\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\n", "intent": "Use CountVectorizer to create document-term matrices from X_train and X_test\n"}
{"snippet": "pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names()).head()\n", "intent": "Let's put it in a dataframe\n"}
{"snippet": "vect = CountVectorizer(binary=True)\ndf = vect.fit_transform(simple_train).toarray().sum(axis=0)\npd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())\n", "intent": "Binary = True assigns a 1 if a word is present irregardless of count, and 0 for absent words.\n"}
{"snippet": "vect = TfidfVectorizer()\npd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n", "intent": "Let's check out the sklearn version\n"}
{"snippet": "vect = CountVectorizer(stop_words=\"english\")\nX_dtm = vect.fit_transform(X)\n", "intent": "Let's make our first model\n"}
{"snippet": "vect = TfidfVectorizer(stop_words=\"english\")\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\nnb = MultinomialNB()\nnb.fit(X_train_dtm, y_train)\nprint nb.score(X_train_dtm, y_train)\nprint nb.score(X_test_dtm, y_test)\n", "intent": "Let's do this again with the tfidf vectorizer\n"}
{"snippet": "df = pd.read_table(\"../../data/NLP_data/sms.tsv\",encoding=\"utf-8\", names= [\"label\", \"message\"])\ndf.head()\n", "intent": "This is a really helpful technique to find the words most associated with either class.\n"}
{"snippet": "path = \"../../data/NLP_data/yelp.csv\"\nyelp = pd.read_csv(path, encoding='unicode-escape')\nyelp.head()\n", "intent": "To wrap our text classification section, we're going to learn how to incorporate stemming and lemmatization in our vectorizers. \n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "This data set is available in the [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\n"}
{"snippet": "poly = PolynomialFeatures(2)\nXp = poly.fit_transform(X)\n", "intent": "Let's try this exercise again but with the polynomial transformed features.\n"}
{"snippet": "kc = pd.read_csv(\"../../data/kc_house_data.csv\")\n", "intent": "Same exercise that we did in the previous class but now try polynomial and regularized models. on the King County housing dataset.\n"}
{"snippet": "pca = PCA(2)  \ndata_pca = pca.fit_transform(data)\n", "intent": "Let's PCA the digits data using two components\n"}
{"snippet": "pca = PCA(n_components=.5).fit(data)\npca.n_components_\n", "intent": "2 components gets us about 28.5 of the way there, let's see how many components it takes to get 50, 70, and 90%.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=.25, \n                                                    random_state=42)\n", "intent": "Try it out on a testing set\n"}
{"snippet": "X = \ny = \nX_train, X_test, y_train, y_test = train_test_split()\n", "intent": "We are going to split our titanic dataset into two sets: training and testing.\n"}
{"snippet": "from sklearn.datasets import make_classification\ndata = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, \n                    class_sep=.20, random_state = 34)\n", "intent": "Let's bring back to the model plotting function for the purpose of visualizing an overfit model against a test set.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X ,y, \n                                                    test_size = .25,\n                                                   random_state = 42)\ntestscore = \nprint (\"The test score is {:.3f} percent\".format(testscore*100))\n", "intent": "Train a model with the best depth value and evaluate it on a test set\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X ,y, test_size = .25,\n                                                    random_state = 38)\ntrain_errors = []\ntest_errors = []\ndepths = range(1,21)\n", "intent": "Graph 1: Plot validation curve of model complexity versus error rates for training and test sets\n"}
{"snippet": "theurl = \"http://en.wikipedia.org/wiki/Election_Day_(United_States)\"\nwikitable = pd.read_html(theurl, match=\"Midterm\", header=0)[0]\nprint wikitable.head()\n", "intent": "When was the election?\n"}
{"snippet": "vect = \nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape \n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "vect = \ndf = vect.fit_transform(simple_train).toarray().sum(axis=0)\npd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())\n", "intent": "Binary = True assigns a 1 if a word is present irregardless of count, and 0 for absent words.\n"}
{"snippet": "vect = \npd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n", "intent": "Let's check out the sklearn version\n"}
{"snippet": "f=pd.DataFrame([[0.068966, 0.137931, 0.068966], [0.344828, 0.241379, 0.137931]], columns=['T=Hot', 'T=Mild', 'T=Cold'], index=['W=Sunny', \"W=Cloudy\"])\nprint(f)\n", "intent": "Look at the following table:\n"}
{"snippet": "vectorizer=CountVectorizer(stop_words='english', max_df=1.0, min_df=0.025) \n", "intent": "We need to create our Bag-of-Words representation (BoW). Here's how\n"}
{"snippet": "descriptions_bow=vectorizer.fit_transform(corpus)   \ndescription_vocabulary = vectorizer.get_feature_names()  \n", "intent": "As many other objects in Sklearn, CountVectorizer is applied with the function fit_transform\n"}
{"snippet": "lda=LatentDirichletAllocation(n_components=10, learning_method='batch')\nx=lda.fit_transform(descriptions_bow)\n", "intent": "It is finally time to run our LDA! We will try with sklearn first...\n"}
{"snippet": "index_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = []\nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n", "intent": "Since the test data is generated with combination of shops and items, we have to restructure train data to match the test data generation. \n"}
{"snippet": "for feat in sales_m.columns:\n    if 'item_cnt' in feat:\n        sales_m[feat]=sales_m[feat].fillna(0)\n    elif 'item_price' in feat:\n        sales_m[feat]=sales_m[feat].fillna(sales_m[feat].median())\n", "intent": "Fill missing values\n"}
{"snippet": "iris = sklearn.datasets.load_iris()\nX = iris.data\nY = iris.target\n", "intent": "Iris example\n====\nWe take all four features now and apply SVD:\n"}
{"snippet": "import pandas\nstations = pandas.read_csv(\"Divvy_Stations_2016_Q3.csv\")\nbikes = pandas.concat([pandas.read_csv(\"Divvy_Trips_2016_Q3.csv\"),\n                       pandas.read_csv(\"Divvy_Trips_2016_Q4.csv\")])\n", "intent": "We know the stations.\n"}
{"snippet": "features = df.pivot_table(index=[\"to_station_id\", \"to_station_name\", \"stopweekday\"],\n                          columns=\"stoptime10\", values=\"dist\").reset_index()\nfeatures.head()\n", "intent": "Let's build the features.\n"}
{"snippet": "piv = features.pivot_table(index=[\"to_station_id\",\"to_station_name\"], columns=\"stopweekday\", values=\"cluster\")\npiv.head()\n", "intent": "We first need to get 7 clusters for each stations, one per day.\n"}
{"snippet": "features = df.pivot_table(index=[\"station_id\", \"station_name\", \"weekday\"],\n                          columns=\"time10\", values=[\"startdist\", \"stopdist\"]).reset_index()\nfeatures.head()\n", "intent": "Let's build the features.\n"}
{"snippet": "piv = features.pivot_table(index=[\"station_id\",\"station_name\"], \n                           columns=\"weekday\", values=\"cluster\")\npiv.head()\n", "intent": "We first need to get 7 clusters for each stations, one per day.\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\nimport pandas\ndf = pandas.DataFrame(data.data, columns=data.feature_names)\ndf.to_csv(\"cancer.txt\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n", "intent": "Let's save some data first.\n"}
{"snippet": "import pyensae\npyensae.download_data(\"OnlineNewsPopularity.zip\", url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00332/\")\n", "intent": "[Online News Popularity Data Set](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity)\n"}
{"snippet": "df = pd.read_csv('songdata.csv')\ndf['text'] = 'trats ' + df['text'] + ' dne' \ndata = df['text'].str.cat(sep=' ').lower() \ndata = ' '.join(word.strip(string.punctuation) for word in data.split()) \n", "intent": "The next cell will import the data and do some parsing for you. In the end you will have a single string with all the data\n"}
{"snippet": "import pandas as pd\ndf = pd.DataFrame({'predictions':y_pred, 'actual label':y_test})\novo_acc = round((df[df.predictions == df['actual label']] .shape[0]/df.shape[0])*100, 2)\novo_acc\n", "intent": "<h1><a name=\"acc ovo\">One Vs One Accuracy</a></h1>\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom time import time\nfrom IPython.display import display \ndata = pd.read_csv(\"data/creditcard.csv\")\ndisplay(data.head(n=1))\n", "intent": "Load data and explore them\n"}
{"snippet": "survived = train[train['Survived']==1]['Sex'].value_counts()\ndead = train[train['Survived']==0]['Sex'].value_counts()\ndf = pd.DataFrame([survived,dead])\ndf.index = ['Survived','Dead']\ndf.head()\ndf.plot(kind='bar',stacked=True, figsize=(10,5))\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "content_image = scipy.misc.imread(\"images/louvre_small.jpg\")\nimshow(content_image)\ncontent_image = reshape_and_normalize_image(content_image)\n", "intent": "Let's load, reshape, and normalize our \"content\" image (the Louvre museum picture):\n"}
{"snippet": "style_image = scipy.misc.imread(\"images/monet.jpg\")\nimshow(style_image)\nstyle_image = reshape_and_normalize_image(style_image)\n", "intent": "Let's load, reshape and normalize our \"style\" image (Claude Monet's painting):\n"}
{"snippet": "party = house_data[\"party\"]\nvotes = house_data.loc[:, 1:]\ntrain_input, test_input, train_outcome, test_outcome = train_test_split(votes, party, test_size = 0.33, random_state = 42)\n", "intent": "Split the data into a test and training set. But this time, use this function:\n```python\nfrom sklearn.cross_validation import train_test_split\n```\n"}
{"snippet": "data.drop(['Item_Type', 'Outlet_Establishment_Year'], axis=1, inplace=True)\ntrain = data.loc[data['source']==\"train\"]\ntest = data.loc[data['source']==\"test\"]\ntest.drop(['Item_Outlet_Sales', 'source'], axis=1, inplace=True)\ntrain.drop(['source'], axis=1, inplace=True)\ntrain.to_csv(\"train_modified.csv\", index=False)\ntest.to_csv(\"test_modified.csv\", index=False)\n", "intent": "- Convert data back into train and test data sets.\n"}
{"snippet": "img = (io.imread('Images/Q2/colorful1.jpg',dtype='float64')/255.0)\ncompileResults(img,5)\n", "intent": "Results for Image 1: Colorful.jpg\n"}
{"snippet": "img2 = (io.imread('Images/Q2/colorful2.jpg',dtype='float64')/255.0)\n", "intent": "Results for Image 2: Colorful2.jpg\n"}
{"snippet": "img3 = (io.imread('Images/Q2/colorful3.jpg',dtype='float64')/255.0)\n", "intent": "Results for Image 3: Colorful3.jpg\n"}
{"snippet": "df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf = df.copy(deep=True)\ndf.head()\n", "intent": "    create copy of data to maintain original file integrity\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_, X_validation = train_test_split(features, random_state = 42, test_size = 0.1)\n", "intent": "Split 10% data for models final validation\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('train.csv')\ndf.head(10) \n", "intent": "<span style=\"font-size:2em\">View the data with Pandas</span>\n"}
{"snippet": "dataV2 = pd.read_csv('lalonde.csv', index_col=0)\nlogistic = linear_model.LogisticRegression()\nZ = dataV2['treat']\nX = dataV2.drop('treat', axis=1)\nlogistic.fit(X,Z)\n", "intent": "Let us train our data to estimate the propensity score for each datapoint, i.e the probability of being treated (or not).\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(ad_data.drop(['Clicked on Ad', 'Ad Topic Line', 'Country', 'City', 'Timestamp'],axis=1), \n                                                    ad_data['Clicked on Ad'], test_size=0.30, \n                                                    random_state=101)\n", "intent": "** Split the data into training set and testing set using train_test_split**\n"}
{"snippet": "df_scaled = pd.DataFrame(scaled_features, columns=df.columns[:-1])\ndf_scaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "import pandas as pd\ndf_train = pd.read_csv('data/titanic/train.csv')\ndf_test = pd.read_csv('data/titanic/test.csv')\n", "intent": "training data set and testing data set are given by Kaggle\nyou can download from kaggle directly [kaggle](https://www.kaggle.com/c/titanic/data)  \n"}
{"snippet": "def bar_chart(feature):\n    survived = df_train[df_train['Survived']==1][feature].value_counts()\n    dead = df_train[df_train['Survived']==0][feature].value_counts()\n    df_survived_dead = pd.DataFrame([survived,dead])\n    df_survived_dead.index = ['Survived','Dead']\n    df_survived_dead.plot(kind='bar',stacked=True, figsize=(15,5))\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "def bar_chart(feature):\n    survived = df_train[df_train['Survived']==1][feature].value_counts()\n    dead = df_train[df_train['Survived']==0][feature].value_counts()\n    df_survived_dead = pd.DataFrame([survived,dead])\n    df_survived_dead.index = ['Survived','Dead']\n    df_survived_dead.plot(kind='bar',stacked=True, figsize=(10,5))\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "def load_data():\n", "intent": "Load the data and generate a train and a test set.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)\n", "intent": "<h3>Standerdize the data</h3>\n"}
{"snippet": "X_train, X_test = train_test_split(X_, random_state = 42, test_size = 0.2)\n", "intent": "Split into train and test set\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nsplit_frac = 0.8\nval_x,train_x,val_y,train_y = train_test_split(features,labels,test_size=split_frac)\n", "intent": "With our data in nice shape, we'll split it into training, validation, and test sets.\n"}
{"snippet": "train = pd.read_csv('mnist_train_data.csv')\ntrain_lab = pd.read_csv('mnist_train_labels.csv')\ntest = pd.read_csv('mnist_test_data.csv')\ntest_lab = pd.read_csv('mnist_test_labels.csv')\n", "intent": "load data into python and check data structure\n"}
{"snippet": "wine_ori = pd.read_csv('wine_original.csv')\nlabels = wine_ori['class']\ndel wine_ori['class']\n", "intent": "Load the wine dataset\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\ndigits = datasets.load_digits()\n", "intent": "Loading a sample dataset\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = iris.data[:, :2]\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=4)\n", "intent": "Splitting data into validation, testing and training samples\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vec=CountVectorizer(tokenizer=tokenize)\nX_train=count_vec.fit_transform(X_train) \nX_test=count_vec.transform(X_test) \n", "intent": "* Construct a Document Term Matrix and Bag of Words Model for the input data.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nimport string\ndef tokenize(sentence):\n    sentence=sentence.translate(str.maketrans('','',string.punctuation))\n    tokens=nltk.word_tokenize(sentence)\n    return tokens\ncount_vec=CountVectorizer(tokenizer=tokenize)\nX_train=count_vec.fit_transform(X_train) \nX_test=count_vec.transform(X_test) \n", "intent": "> * Construct a Document Term Matrix\n> * Construct Bag of Words\n"}
{"snippet": "Trump2018 = open('C:/Users/ushai/Dropbox/Data Science/Datasets/trump_state_union_2018.txt')\nTrump2019 = open('C:/Users/ushai/Dropbox/Data Science/Datasets/trump_state_union_2019.txt')\nTrump2018_raw = Trump2018.read()\nTrump2019_raw = Trump2019.read()\nTrump_raw = Trump2018_raw + Trump2019_raw\n", "intent": "Donald trump (R) State of the Union 2018, 2019\n"}
{"snippet": "ypred_Dem = pd.DataFrame()\n", "intent": "- Here we divide the data in to four different samples and check consistency of clusters across samples for both Democratic and Republican documents.\n"}
{"snippet": "X_copy = X.copy()  \nX_scaled = scaler.transform(X_copy)\npd.DataFrame({\n    \"Mean\": X_scaled.mean(axis=0), \n    \"Std\": X_scaled.std(axis=0)\n})\n", "intent": "**Exercise**: It looks like some of the features are on a larger numerical scale.  Let's put all the data on the same scale.\n"}
{"snippet": "messages = pandas.read_csv(DATA_FILENAME, sep='\\t', quoting=csv.QUOTE_NONE,\n                           names=[\"label\", \"message\"])\nprint(messages)\n", "intent": "Instead of parsing TSV (or CSV, or Excel...) files by hand, we can use Python's `pandas` library to do the work for us:\n"}
{"snippet": "bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])\nprint(len(bow_transformer.vocabulary_))\n", "intent": "Each vector has as many dimensions as there are unique words in the SMS corpus:\n"}
{"snippet": "sub = pd.DataFrame()\nsub['ImageId'] = new_test_ids\nsub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\nsub.to_csv('sub-dsbowl2018-3rd.csv', index=False)\n", "intent": "... and then finally create our submission!\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nitrain, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itrain] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "audio_file_path = '../../Datasets'\nfilename = '/5e1b34a6_nohash_0.wav'\nsample_rate, samples = wavfile.read(str(audio_file_path) + filename)\n", "intent": "Let us read a sample audio file from this dataset: \n"}
{"snippet": "from keras.datasets import cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n", "intent": "CIFAR-10 is a dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.\n"}
{"snippet": "from keras.datasets import fashion_mnist\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n", "intent": "**TASK 1: Run the code below to download the dataset using Keras.**\n"}
{"snippet": "from bs4 import BeautifulSoup\nfname = 'time.html'\nfname = os.path.join(DATA_DIR, fname)\nwith open(fname) as f:\n    html = f.read()\n    soup = BeautifulSoup(html)\n", "intent": "The best way to read in `.html` files in Python is with the `BeautifulSoup` package.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncountvec = CountVectorizer()\nsparse_dtm = countvec.fit_transform(no_digits)\n", "intent": "Our next step is to turn the text into a document term matrix using the scikit-learn function called `CountVectorizer`.\n"}
{"snippet": "dataset = pd.read_csv('mushrooms.csv')\n", "intent": "To load the data from the file, we use the [Pandas](https://pandas.pydata.org/) library.\n"}
{"snippet": "from sklearn.datasets import make_regression\nX, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4, bias=100.0)\n", "intent": "What is linear data?\n"}
{"snippet": "try:\n    churn_data = pd.read_csv('telecom_churn_data.csv')\nexcept:\n    churn_data = pd.read_csv('data/telecom_churn_data.csv')\ntotal_records = churn_data.shape[0]\nchurn_data.shape\n", "intent": "Load the churn data for the telecom provider\n"}
{"snippet": "text = None\nwith open('anna.txt', 'r') as f:\n    text = f.read()\nch_set = set(text)\nch_size = len(ch_set)\nprint('our input has %d unique characters' % ch_size)\nch_to_int = {c:i for i,c in enumerate(ch_set)}\nint_to_ch = dict(enumerate(ch_set))\ndatas = np.array([ch_to_int[c] for c in text], dtype = np.int32)\n", "intent": "We will train our char-RNN on Anna Karenina (anna.txt), we need to load our data into memory to get the character-sizes ($D$ and $C$)\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values='NaN', strategy='mean', axis=1)\nX = imputer.fit_transform(df.iloc[:, :8].values)\ny = df.iloc[:, 8].values\nfit_and_score_rlr(X, y)\n", "intent": "Next, replace missing features through mean imputation. Run  a regression and measure the performance of the model.\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(categorical_features=range(5))\nX_encoded = enc.fit_transform(X)\nfit_and_score_rlr(X_encoded, y, normalize=False)\n", "intent": "Now, encode the categorical variables with a one-hot encoder. Again, run a classification and measure performance.\n"}
{"snippet": "class SBS(object):\n    def __init__(self):\n        pass\n    def fit(self):\n        pass\n    def transform(self):\n        pass\n    def fit_transform(self):\n        pass\n", "intent": "- Implement SBS below. Then, run the tests.\n"}
{"snippet": "df_raw = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "X_mat = scaler.transform(df_raw.drop('TARGET CLASS',axis=1))\nX_scale = pd.DataFrame(X_mat, columns=df_raw.columns[:-1])\nX_scale.head()\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "df_scale = pd.DataFrame(df_scale)\ndf_scale.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "y = encoder.fit_transform(dataset.iloc[:,0])\n", "intent": "Let's start by encoding the binary classification output.\n"}
{"snippet": "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n                                                  test_size=10000,\n                                                  random_state=42)\n", "intent": "To measure our Neural Networks performance we will need some validation data. The `train_test_split` helper from scikit-learn does this for us.\n"}
{"snippet": "url = './titanic.csv'\ntitanic = pd.read_csv(url)\ntitanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\ntitanic.Age.fillna(titanic.Age.median(), inplace=True)\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\nembarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\ntitanic = pd.concat([titanic, embarked_dummies], axis=1)\ntitanic.head()\n", "intent": "We'll build a classification tree using the Titanic data:\n"}
{"snippet": "train_df = pd.read_csv('data/train.csv', index_col=0, infer_datetime_format=True)\ntrain_df.index.name=None \ntrain_df.index = pd.to_datetime(train_df.index) \ntest_df = pd.read_csv('data/test.csv', index_col=0, infer_datetime_format=True)\ntest_df.index.name=None \ntest_df.index = pd.to_datetime(test_df.index) \n", "intent": "- What does each 'unit' (e.g. row) of data represent?\n"}
{"snippet": "pd.DataFrame(contributions, columns=predictors)\n", "intent": "prediction = bias + sum(contributions)\n"}
{"snippet": "X_train_new = pd.DataFrame(X_train_new, columns=X_train.columns)\nmeans = []\nstdevs = []\nfor k in X_train_new.columns:\n    means.append(X_train_new[k].mean())\n    stdevs.append(X_train_new[k].std())\n", "intent": "Extract Mean and Standard Dev from original train + validation dataset. \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n", "intent": "Splitting data into validation, testing and training samples\n"}
{"snippet": "import numpy as np\niris = datasets.load_iris()\nX = iris.data[:, :2]  \ny = iris.target\nh = .02  \n", "intent": "Plotting Decision Boundaries\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\ntheta,residuals,rank,s = numpy.linalg.lstsq(X_train, y_train)\n", "intent": "Least Square Regression\n"}
{"snippet": "for i in pd.DataFrame(zip(X.columns, lm.coef_), columns=['features','estimatedCoefficients']):\n    print i\n", "intent": "Outputting list of features that are deemed to be statistically significant by this process\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8)  \n", "intent": "- To avoid this problem, we split up the dataset in a **training set** for fitting the model, and a **test set** for testing the model.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled)\n", "intent": "Now actually running the regression on the reduced dataset\n"}
{"snippet": "from yellowbrick.features.manifold import Manifold\nvisualizer = Manifold(manifold='lle', target='continuous')\nvisualizer.fit_transform(X_scaled[random_features],y_scaled)\nvisualizer.poof()\n", "intent": "Now passing Locally Linear Embedding - does this help us for linaer regression?\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_normed,y_normed, test_size=0.3, random_state=42)\n", "intent": "Step1: Simple Model with Linear Regression\n"}
{"snippet": "poly = PolynomialFeatures(interaction_only=True)\nX_normed_wint = poly.fit_transform(X_normed)\n", "intent": "The above did not return any interactions! That means the interactions are probably not going to make large changes to our models above\n"}
{"snippet": "selector = RFECV(lin_reg, step=1, cv=5, scoring='neg_mean_squared_error')\nselector.fit(X_ratios, y_normed)\n", "intent": "Feature Selection - We use this promising model above to pick only the best features\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_new, y_normed, test_size=0.3, random_state=42)\n", "intent": "Now let's rerun our best model so far and evaluate changes to model metrics resulting from removing unneeded features\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_normed, y_normed, test_size=0.3, random_state=42)\n", "intent": "Finally, let's see how the models performs against our Test Set\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_ratios, y_normed, test_size=0.3, random_state=42)\n", "intent": "Best model After Adding Ratios and Doing Feature Selection\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_ratios, y_normed, test_size=0.3, random_state=42)\n", "intent": "Let's introduce some degree of regularization to see if we can decrease validation RMSE even further\n"}
{"snippet": "poll_data = pd.read_csv(path_to_repo + 'data/538/2012_poll_data_states.csv', sep='\\t')\ncensus_data = pd.read_csv(path_to_repo + 'data/538/census_demographics.csv')\nstates = pd.read_csv(path_to_repo + 'data/538/states.csv')\n", "intent": "Let's look at the 538 polling data and see if we can use the census data to predict who was ahead in the polls, using kNN.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_normed, y_normed)\n", "intent": "Now actually running the regression on the reduced dataset\n"}
{"snippet": "X_train, X_val, y_train, y_val = train_test_split(X_new,y_normed, test_size=0.3, random_state=42)\n", "intent": "Now let's try Linear Regression with CV again and let's see if we have any improvements with the transformed version of X\n"}
{"snippet": "estimator = linear_model.LinearRegression(fit_intercept=True, normalize=False)\nselector = RFECV(estimator, step=1, cv=5, scoring='neg_mean_squared_error')\nselector.fit(X_train_int, y_train)\nprint(\"Optimal number of features : %d\" % selector.n_features_)\n", "intent": "Let's pick only interaction features that minimize CV RMSE by feeding the new data into RCEFV\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_normed,y_normed, test_size=0.3, random_state=42)\n", "intent": "Let's introduce some degree of regularization to see if we can decrease validation RMSE even further\n"}
{"snippet": "from sklearn.feature_selection import RFECV\nestimator2 = linear_model.LinearRegression()\nselector2 = RFECV(estimator2, step=3, cv=5)\nselector2 = selector2.fit(data_scaled, y)\n", "intent": "Trying to select important features by doing a RFECV instead of looking at univariate linear regressions first\n"}
{"snippet": "cal = pd.read_csv('Datasources/inside_airbnb/calendar.csv')\n", "intent": "Preparing the dataset first: taking the avg of each listing price time series\n"}
{"snippet": "linear_regression_wke = linear_model.LinearRegression(normalize=False, fit_intercept=True)\nstandardization = StandardScaler()\nStand_coef_linear_reg_wke = make_pipeline(standardization, linear_regression_wke)\nlinear_regression_wke.fit(X_wke,target_wke)\nfor coef, var in sorted(zip(map(abs, linear_regression_wke.coef_), X_wke.columns[:-1]),reverse=True):\n    print (\"%6.3f %s\" % (coef,var))\n", "intent": "Feature Importance for Weekends\n"}
{"snippet": "linear_regression_wkd = linear_model.LinearRegression(normalize=False, fit_intercept=True)\nstandardization = StandardScaler()\nStand_coef_linear_reg_wkd = make_pipeline(standardization, linear_regression_hol)\nlinear_regression_wkd.fit(X_wkd,target_wkd)\nfor coef, var in sorted(zip(map(abs, linear_regression_wkd.coef_), X_wkd.columns[:-1]),reverse=True):\n    print (\"%6.3f %s\" % (coef,var))\n", "intent": "Feature Importance for Weekdays\n"}
{"snippet": "label_bin = LabelBinarizer()\n", "intent": "Binarizing Variables\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8)\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n", "intent": "- Now use a training and test set for fit and test your model.\n- How is its performance different?\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X[features_one], np.log(np.log(target)), test_size=.30, random_state=1)\nX_train_two, X_val, y_train_two, y_val = train_test_split(X_train, y_train, test_size=.30, random_state=1)\n", "intent": "In this iteration, we try predicting the log of the log of the target\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_ratios[best_features], y_normed, test_size=0.3, random_state=42)\n", "intent": "Best model After Adding Ratios and Doing Feature Selection\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_ratios, y_normed, test_size=0.3, random_state=42)\n", "intent": "Starting with Lasso\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n", "intent": "Now actually running the regression on the reduced dataset\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(data, y)\n", "intent": "Now actually running the regression on the reduced dataset\n"}
{"snippet": "from yellowbrick.features.manifold import Manifold\nvisualizer = Manifold(manifold='lle', target='continuous')\nvisualizer.fit_transform(data[random_features],y)\nvisualizer.poof()\n", "intent": "Now passing Locally Linear Embedding - does this help us for linaer regression?\n"}
{"snippet": "bitcoin = pd.read_csv('data/bitcoin_historical_prices.csv')\n", "intent": "Let's load the dataset as a pandas `DataFrame`. This will make it easy to compute basic properties from the dataset and to clean any irregularities. \n"}
{"snippet": "train = pd.read_csv('data/train_dataset.csv')\n", "intent": "Neural networks typically work with vectors and tensors, both mathematical objects that organize data in a number of dimensions. \n"}
{"snippet": "train = pd.read_csv('data/train_dataset.csv')\n", "intent": "We will load our same train and testing set from previous activitites. \n"}
{"snippet": "import numpy as np\nscores = []\nfor i in xrange(15):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8)\n    scores.append(model.fit(X_train, y_train).score(X_test, y_test))\nfor score in scores:\n    print np.round(score, 2),\nprint \"\\navg:\", np.mean(scores)\n", "intent": "Note that `train_test_split` makes random splits, so your accuracy can be different than the one above, as you can see below:\n"}
{"snippet": "scaler = MinMaxScaler()\n", "intent": "Scale features before creating Ridge model. Unscaled features would otherwise get penalized differently by the regularization term\n"}
{"snippet": "sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n", "intent": "Scaling Training and Test Sets:\n"}
{"snippet": "labels_path = os.path.join(os.getcwd(), 'datasets', 'landsat', 'landsat_classes.csv')\nlandsat_labels = pd.read_csv(labels_path, delimiter=',', index_col=0)\nlandsat_labels\nlandsat_labels_dict = landsat_labels.to_dict()[\"Class\"]\ntrain_path = os.path.join(os.getcwd(), 'datasets', 'landsat', 'landsat_train.csv')\ntest_path = os.path.join(os.getcwd(), 'datasets', 'landsat', 'landsat_test.csv')\nlandsat_train = pd.read_csv(train_path, delimiter=',')\nlandsat_test = pd.read_csv(test_path, delimiter=',')\n", "intent": "Load the `landsat_train.csv` dataset into a `pandas` DataFrame called  `landsat_train` and display the shape of the DataFrame.\n"}
{"snippet": "wages = pd.DataFrame(dict(\n        monthly_income_amount = data[u'monthly_income_amount'],\n        monthly_rent_amount = data[u'monthly_rent_amount'],\n        Credit_Line_approved = data[u'Credit_Line_approved'],\n        age=data[u'age']) )\nwages = pd.concat([wages, data['flgGood']], axis = 1)\ng = sns.pairplot(wages.sort('flgGood'), hue=\"flgGood\", palette=myPalette)       \n", "intent": "Is there any relations between wages and age? And total spend with default?\n"}
{"snippet": "cols = [u'raw_serasa_score', u'raw_unit4_score',\n        u'raw_lexisnexis_score',u'raw_TU_score', u'raw_FICO_money_score']\nraw_scores_bin = pd.DataFrame(dict(\n        raw_serasa_score = mu.binarize(data[u'raw_serasa_score']),\n        raw_unit4_score = mu.binarize(data[u'raw_unit4_score']),\n        raw_lexisnexis_score = mu.binarize(data[u'raw_lexisnexis_score']),\n        raw_TU_score = mu.binarize(data[u'raw_TU_score']),\n        raw_FICO_money_score = mu.binarize(data[u'raw_FICO_money_score'])))\nraw_scores = pd.concat([raw_scores_bin, data['flgGood']], axis = 1)\n", "intent": "Are the scores given by other entities good? Do they guarantee good payment?\n"}
{"snippet": "def rmsle(Y_test, Y_pred):\n    Y_pred[Y_pred <0] = 0\n    return np.sqrt( np.sum( (np.log(Y_pred+1)-np.log(Y_test+1))**2 )/len(Y_test))\nfrom datetime import datetime\ndef prepare_submission(X_test, Y_pred, file_name):\n    d = X_test.datetime.apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n    Y_pred[Y_pred<0]=0\n    ret = pd.DataFrame( { 'datetime':d , 'count':Y_pred})\n    ret.to_csv(file_name,sep=',', index=False)\n    return ret\n", "intent": "As stated by Kaggle, this competition will be analyzed using the Root Mean Squared Logarithmic Error (RMSLE), so lets get it from scikit\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfrom sklearn import linear_model\nfrom sklearn import ensemble\na_train, a_test, b_train, b_test = train_test_split(X_train, Y_train)\n", "intent": "Now that the vector are ready, its time to apply the machine learning algorithms\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\na_train, a_test, b_train, b_test = train_test_split(X_train, Y_train)\n", "intent": "Now that the vector are ready, its time to apply the machine learning algorithms\n"}
{"snippet": "features = pd.concat([data.get(['Fare', 'Age']),\n                      pd.get_dummies(data.Sex, prefix='Sex'),\n                      pd.get_dummies(data.Pclass, prefix='Pclass'),\n                      pd.get_dummies(data.Embarked, prefix='Embarked')],\n                     axis=1)\nfeatures = features.drop('Sex_male', 1)\nfeatures = features.fillna(features.dropna().median())\nfeatures.head(5)\n", "intent": "Let us now rebuild a new version of the data by treating the class as categorical variable:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfor k in xrange(10):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8)\n    print model.fit(X_train, y_train).score(X_test, y_test).round(3), \n", "intent": "Supervised problems, like linear regression, always require a training and a testset, to avoid having overfitted your model.\n"}
{"snippet": "from keras.datasets import mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n", "intent": "The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays.\nDocumentation : https://keras.io/datasets/\n"}
{"snippet": "from keras.datasets import mnist\nfrom keras.utils import to_categorical\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\ntrain_images = train_images.reshape((60000, 28, 28, 1))\ntrain_images = train_images.astype('float32') / 255\ntest_images = test_images.reshape((10000, 28, 28, 1))\ntest_images = test_images.astype('float32') / 255\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\n", "intent": "We will reuse a lot of the code we have already covered in the MNIST example from episode 1.\n"}
{"snippet": "img_path = '../../data/cats_and_dogs_small/test/cats/cat.1700.jpg'\nfrom keras.preprocessing import image\nimport numpy as np\nimg = image.load_img(img_path, target_size=(150, 150))\nimg_tensor = image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\nimg_tensor /= 255.\nprint(img_tensor.shape)\n", "intent": "This will be the input image we will use -- a picture of a cat, not part of images that the network was trained on:\n"}
{"snippet": "daily_user_views_diff = pd.DataFrame(df['views']\\\n                                     .resample('D')\\\n                                     .sum()\\\n                                     .diff(periods=1)\\\n                                     .astype(float))['2015-07-02':] \n", "intent": "I'll run the augmented dickey-fuller test on the differenced data to show that the differencing should make the data set more stationary:\n"}
{"snippet": "of_df = pd.read_csv(\"../../assets/dataset/old-faithful.csv\")\nof_df.head()\n", "intent": "Not so great on this dataset. Now let's try some real data.\n"}
{"snippet": "of_df = pd.read_csv(\"old-faithful.csv\")\nof_df.head()\n", "intent": "Not so great on this dataset. Now let's try some real data.\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    contains_org = any([word.ent_type_ == 'ORG' for word in parsed])\n    contains_person = any([word.ent_type_ == 'PERSON' for word in parsed])\n    return contains_org and contains_person\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title']].head()\n", "intent": "Lets write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n"}
{"snippet": "def references_organization(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == 'ORG' for word in parsed])\ndata['references_organization'] = data['title'].fillna(u'').map(references_organization)\ndata[data['references_organization']][['title','references_organization']].head() \n", "intent": "Let's see if we can find organizations in our page titles.\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    contains_org = any([word.ent_type_ == 'ORG' for word in parsed])\n    contains_person = any([word.ent_type_ == 'PERSON' for word in parsed])\n    return contains_org and contains_person\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title','references_org_person']].head()\n", "intent": "Lets write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nX, y = boston.data, boston.target\nfeatures = boston.feature_names\n", "intent": "In this exercise, we'll use one of sklearn's standard datasets to analyze Boston house prices.\n"}
{"snippet": "df = pd.read_csv('iris.csv',\n                 names=['SepalLength','SepalWidth','PetalLength','PetalWidth','Name'],\n                 header=None)\n", "intent": "Let's create a Pandas DataFrame with the data.\n"}
{"snippet": "df_additional_full = pd.read_csv('bank-additional-full.csv', sep = ';')\n", "intent": "Extend the analysis and cross-validation to bank-additional-full.csv. How does the performance change?\n"}
{"snippet": "dists = pd.DataFrame(dists, columns = df_wide.index, index=df_wide.index)\ndists.ix[0:10, 0:10]\n", "intent": "Convert dists to a Pandas DataFrame, use the index as column index as well (distances are a square matrix).\n"}
{"snippet": "movies = pd.read_table('movielens/movies.dat', sep='::', \n                       names= ['ITEMID', 'Title', 'Genres'], index_col= 'ITEMID')\n", "intent": "Import 'movies.dat' to a 'movies' pandas dataframe. Make sure you name the columns, use the correct separator and define the index.\n"}
{"snippet": "df = pd.read_csv(\"heart_disease.csv\",header=None)\ndf.columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']\ndf.head()\n", "intent": "Note: You'll have to manually add column labels\n"}
{"snippet": "labels=[\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \n        \"ca\", \"thal\", \"num\"]\ntarget=[\"num\"]\ndf = pd.read_csv(\"./heart_disease.csv\", names=labels)\ndf.head()\n", "intent": "Note: You'll have to manually add column labels\n"}
{"snippet": "data = pd.read_csv('houses.csv')\n", "intent": "Load a subset of the housing data\n"}
{"snippet": "Xpd = pd.DataFrame(digits.data) \nXpd.head()\n", "intent": "Here's a small routine to visually plot the first 400 rows (i.e. digits) in the dataset:\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\nX = iris['data']\nNames = iris['feature_names']\ntarget_names = iris['target_names']\ny = iris['target']\nprint target_names\nprint Names\nprint X[:1]\n", "intent": "Import the data from the iris dataset in sklearn, generate X and y arrays\n"}
{"snippet": "test = pd.read_csv('/Users/ruben/Downloads/test.csv')\n", "intent": "We're gonna concatenate the training and test set, so we're sure our feature matrices are aligned.\n"}
{"snippet": "ratdata = pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/ML_2017/Manhattan 311 Rodent Complaint Locations 2016.csv\").values\nratdata\n", "intent": "Find out which neighborhoods(see map below) that have more serious rat problem.\n<img src=\"Manhattan neighborhood map.png\" width=600 height=400>\n"}
{"snippet": "s = pd.Series([1,3,5,np.nan,6,8])\ndates = pd.date_range('20130101', periods=6)\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))\n", "intent": "10 Minutes to pandas: http://pandas.pydata.org/pandas-docs/stable/10min.html\n"}
{"snippet": "path = 'https://serv.cusp.nyu.edu/~cq299/ADS2016/Data/Bayesian/'\ndata=pd.read_csv(path+\"example3.csv\")\nX=np.matrix(data.iloc[:,:-1])\ny=np.asarray(data.Y)\n", "intent": "http://scikit-learn.org/stable/modules/grid_search.html\n"}
{"snippet": "data=pd.read_csv(\"session_3_stop.csv\")\n", "intent": "Please download the data set here or on NYU-Classes.\nhttps://serv.cusp.nyu.edu/classes/ML_2016_Spring/ML_2017/\nfile: \"session_3_stop.csv\"\n"}
{"snippet": "import pandas as pd\ndata = pd.DataFrame(data={'fruit': [\"banana\", \"apple\", \"banana\", \"apple\", \"banana\",\"apple\", \"banana\", \n                                    \"apple\", \"apple\", \"apple\", \"banana\", \"banana\", \"apple\", \"banana\",], \n                          'tasty': [\"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \n                                    \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\"], \n                          'size': [\"large\", \"large\", \"large\", \"small\", \"large\", \"large\", \"large\",\n                                    \"small\", \"large\", \"large\", \"large\", \"large\", \"small\", \"small\"]})\nprint(data)\n", "intent": "(1) Parameters Learning\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values='NaN', strategy='median', axis=0)\n", "intent": "Impute missing values with median\n"}
{"snippet": "data = datasets.load_iris()\nX = data.data[:100, :2]\ny = data.target[:100]\nX_full = data.data[:100, :]\n", "intent": "- Load the Iris dataset\n"}
{"snippet": "from keras.datasets import cifar10\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\ny_train = y_train.reshape(-1)\ny_test = y_test.reshape(-1)\n", "intent": "Dataset of 50,000 32x32 color training images, \nlabeled over 10 categories, \nand 10,000 test images.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\n_, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[_] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"/Users/ruben/Downloads/train.csv\")\n", "intent": "Let's do some data exploration first\n"}
{"snippet": "coeffdf = pd.DataFrame(lm.coef_,index = X_train.columns,columns=['coeff'])\ncoeffdf\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "yelp_class = pd.DataFrame(data = yelp[(yelp['stars'] == 1) |(yelp['stars'] ==5)] )\nyelp_class.head()\n", "intent": "**Dataframe - yelp_class that contains the columns of yelp dataframe but for only the 1 or 5 star reviews.**\n"}
{"snippet": "import pandas as pd\nurl = 'https://raw.githubusercontent.com/JamesByers/GA-SEA-DAT2/master/data/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer\n", "intent": "1. K-means clustering\n2. Clustering evaluation\n3. DBSCAN clustering\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    train_size=800, \n                                                    random_state=42)\n", "intent": "We will build a decision tree on the TRAIN data. (800 rows)\nWe will TEST our PREDICTIONS on the TEST data set (remaining rows)\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df2, test_size=0.2, \n                                     random_state = 12)\n", "intent": "Use the ready-made function from sklearn.\nWe can cut into two ourselves.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nfor label in ['embarked','sex']:\n    titanic[label] = LabelEncoder().fit_transform(titanic[label])\nprint(titanic.head())\n", "intent": "Three binomials, two categoricals, and four numerical features.\n"}
{"snippet": "with hana_query(all_transcripts_sql) as cursor:\n    rows = cursor.fetchall()\nt_df = pd.DataFrame(rows, columns=COLUMNS['TRANSCRIPT']+[\"HeightF\", \"WeightF\", \"BMIF\", \"TemperatureF\"])\nt_df.describe()\n", "intent": "We want to evaluate the quality of the data - in particular with regards to null values - and determine boundaries to cut off outliers.\n"}
{"snippet": "missing_df = pd.read_csv('./dataset/dataset_1_missing.txt')\nfull_df = pd.read_csv('./dataset/dataset_1_full.txt')\nno_y_ind = missing_df[missing_df['y'].isnull()].index\nwith_y_ind = missing_df[missing_df['y'].notnull()].index\nk=3\npredicted_knn, r_knn = fill_knn(missing_df, full_df, no_y_ind, with_y_ind, k)\nprint 'R^2 value of KNN fit, for k=', k, ': ', r_knn\npredicted_lin, r_lin = fill_lin_reg(missing_df, full_df, no_y_ind, with_y_ind)\nprint 'R^2 value of linear regression fit:', r_lin\n", "intent": "**Solution:**\nEvaluate predicted values using linear regression vs KNN.\n"}
{"snippet": "data = pd.read_csv('green_tripdata_2015-01.csv', usecols=range(0, 21), index_col=False)\ndata.head(n=3)\n", "intent": "Let's now read the csv file correctly and extract the time stamp values.\n"}
{"snippet": "test = pd.read_csv(\"/Users/ruben/Downloads/test.csv\")\n", "intent": "Let's make our predictions on the test set.\n"}
{"snippet": "hd_data = pd.read_csv('datasets/dataset_3_train.txt')\ndisease = hd_data.iloc[:, -1:]\nindicators = hd_data.iloc[:, :-1]\ntest1_data = pd.read_csv('datasets/dataset_3_test_1.txt')\ndisease_t1 = test1_data.iloc[:, -1:]\nindicators_t1 = test1_data.iloc[:, :-1]\ntest2_data = pd.read_csv('datasets/dataset_3_test_2.txt')\ndisease_t2 = test2_data.iloc[:, -1:]\nindicators_t2 = test2_data.iloc[:, :-1]\n", "intent": "- Fit a logistic regression model to the training set, and report its accuracy on both the test sets. \n"}
{"snippet": "page = urllib.urlopen('https://cs109alabs.github.io/lab_files/').read()\nsoup = BeautifulSoup(page, \"lxml\")\nprint soup.prettify()[:1000]\n", "intent": "In this part we use the solution to the Challenge problem in Homework \n"}
{"snippet": "ensemble = pd.DataFrame({}) \ncost_weighting = []\npreds_alter = preds - 1    \ntypes = preds.value_counts()\nprint types\nprint cost(y, preds_alter)\ncost_weighting.append(cost(y, preds_alter))\nensemble['logreg'] = preds_alter\n", "intent": "We have our predictions... how does our model do.\n"}
{"snippet": "ensemble = pd.DataFrame({}) \ncost_weighting = []\npreds_alter = preds - 1\nprint cost(y, preds_alter)\ncost_weighting.append(cost(y, preds_alter))\nensemble['logreg'] = preds_alter\n", "intent": "We have our predictions... how does our model do.\n"}
{"snippet": "loans = pd.read_csv('loan_data.csv')\n", "intent": "**Nutze Pandas, um die Datei \"loan_data.csv\" als DataFrame namens \"loans\" zu laden.**\n"}
{"snippet": "df = pd.read_csv('College_Data',index_col=0)\n", "intent": "** Lies die CSV-Datei \"College_Data\" mit `read_csv` ein. Finde heraus, wie du die erste Spalte als Index definierst.**\n"}
{"snippet": "df = pd.read_csv('KNN_Project_Data')\n", "intent": "**Lese die CSV-Datei \"KNN_Project_Data\" in einen DataFrame ein.**\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "**Erstelle ein StandardScaler() Objekt namens \"scaler\".**\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head()\n", "intent": "**Konvertiere die skalierten Eigenschaften in einen DataFrame und sieh dir dessen Head an.**\n"}
{"snippet": "from patsy import dmatrix\nall_data = pd.concat([data, test])\nfeatures = \"SalaryNormalized ~ ContractType + Category + County + City + Town + Hood + \" + \" + \".join(top_keywords)\nY, X = dmatrices(features, data=all_data.fillna(0), return_type='dataframe')\ndescription_features = cv.fit_transform(all_data.FullDescription).todense()\nX = np.hstack((X, description_features))\n", "intent": "Design model on entire dataset, so that columns and coefficients will be aligned.\n"}
{"snippet": "ad_data = pd.read_csv('advertising.csv')\n", "intent": "**Lese die advertising.csv Datei ein und erstelle einen DataFrame namens ad_data.**\n"}
{"snippet": "train = pd.read_csv('titanic_train.csv')\n", "intent": "Beginnen wir damit die Datei \"titanic_train.csv\" in einen Pandas DataFrame zu laden.\n"}
{"snippet": "df_store = pd.read_csv('sc_data/Store_Transactions.csv')\ndf_store.shape\ndf_store.head()\n", "intent": "**1:** Utilize the Apriori Algorithm to uncover frequent itemsets\n"}
{"snippet": "df_movie_pivot = pd.pivot_table(df_ratings, index='movieId', columns='userId',values='rating').fillna(0)\ndf_movie_pivot.head()\n", "intent": "**3:** Pick 2 movies and find movies that are similar to the movies you have picked\n"}
{"snippet": "projects  = pd.read_csv('./data/projects.csv')\nstr_to_bool(projects)\nprint(projects.shape[0])\nprint(projects.dtypes)\noutcomes  = pd.read_csv('./data/outcomes.csv')\nstr_to_bool(outcomes)\nprint(outcomes.shape[0])\nprint(outcomes.dtypes)\n", "intent": "First, check the size of the datasets:\n"}
{"snippet": "from sklearn.datasets import make_regression\nX, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4)\n", "intent": "What is linear data?\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ndata = X.copy()\nlabel_encoder.fit(data[\"gender\"])\nlabel_encoder.classes_\n", "intent": "Label Encoding simply encodes each category as an integer value. Sklearn provides a preprocessing library to assist with this. \n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\nv = DictVectorizer(sparse = False)\nD = [{'A':1, 'B':2}, {'B':3, 'C':1}]\nX = v.fit_transform(D)\nX\n", "intent": "- https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=50, centers=2, cluster_std=0.5, random_state=4)\ny = 2 * y - 1\n", "intent": "- https://datascienceschool.net/view-notebook/6c6d450cb2ee49558856fd924b326e00/\n"}
{"snippet": "import pandas as pd\nwith open(\"/Users/ruben/repo/personal/ga/DAT-23-NYC/data/amazon/small-movies.txt\") as f:\n    data = pd.DataFrame([{line[:line.find(':')]: line[line.find(':')+2:] \n                          for line in review.split('\\nreview/')}\n                         for review in ('\\n' + f.read()).split('\\nproduct/') if len(review) > 1])\n    data['score'] = data.score.astype(float).astype(int)\n    data['helpfulness'] = data.helpfulness.str.split('/').map(\n        lambda frac: float(frac[0])/float(frac[1]) if frac[1] != '0' else None)\n", "intent": "This was an exercise in lesson \n"}
{"snippet": "data = pd.read_csv(\"data/mailing.csv\")\nX = data.drop(['class'], 1)\nY = data['class']\n", "intent": "Let's read our data in and put the target variable in `Y` and all the other features in `X`.\n"}
{"snippet": "data = pd.read_csv(\"data/imdb.csv\", quotechar=\"\\\"\", escapechar=\"\\\\\")\n", "intent": "1\\. Read in the data located in `data/imdb.csv`. Don't forget to set the `quotechar` to `\"` and `escapechar` to `\\`. **`[`*`5 points`*`]`**\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ntext_from_corpus = fetch_20newsgroups(subset='train')\n", "intent": "**Step \n*Reference*: http://scikit-learn.org/stable/datasets/index.html\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nimport sys, os\nimport datetime as dt\nfrom os import linesep as endl\npd.set_option('display.width', 170)\ntab = pd.read_csv('retailer.csv', sep = '|')\nprint tab.sample(10)\n", "intent": "The dataset contains numbers of items sold per week and store for each SKU.  \nSKUs belong to BRANDS, SEGMENTS, TYPES and COLLECTIONS\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, Y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['over_200k', 'under_200k'],\n                         columns=['predicted_over_200k','predicted_under_200k'])\nconfusion\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "df['SQFT_SC_test'] = MinMaxScaler().fit_transform(df['SQFT'])\ndf['BDRMS_SC_test'] = MinMaxScaler().fit_transform(df['BDRMS'])\ndf['AGE_SC_test'] = MinMaxScaler().fit_transform(df['AGE'])\ndf['PRICE_SC_test'] = MinMaxScaler().fit_transform(df['PRICE'])\ndf.head(3)\n", "intent": "Refer to this if you are confused as to the formula: [Standardization](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n"}
{"snippet": "df_SC['SQFT_SC_test'] = StandardScaler().fit_transform(df['SQFT'])\ndf_SC['BDRMS_SC_test'] = StandardScaler().fit_transform(df['BDRMS'])\ndf_SC['AGE_SC_test'] = StandardScaler().fit_transform(df['AGE'])\ndf_SC['PRICE_SC_test'] = StandardScaler().fit_transform(df['PRICE'])\n", "intent": "[StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n"}
{"snippet": "iris = load_iris()\nprint iris.keys()\nprint ''\niris_sublist = ['target_names', 'feature_names']\nfor i in iris_sublist:\n    for x in iris[i]:\n        print i + \": \" + x\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "pd.DataFrame(metrics.confusion_matrix(y, labels))\n", "intent": "Compute the Confusion Matrix to test the performance of the clustering analysis\n"}
{"snippet": "path_to_repo = '/Users/ruben/repo/personal/ga/DAT-23-NYC/'\ncolumn_headers = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\ndata = pd.read_csv(path_to_repo + 'data/iris/iris.csv', header=None, names=column_headers)\n", "intent": "Our model will predict if a flower is a versicolor or not, based only on the petal's width and length.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nx_train,x_test, y_train,  y_test =train_test_split(wine_df,y, test_size=0.25)\n", "intent": "Let's create a test that simulates fresh data that model might be predicting on when it is put into production.\n"}
{"snippet": "path = \"../../data/titanic.csv\"\ntitanic = pd.read_csv(path)\ntitanic.head()\n", "intent": "Before we go over how to make validation and learning curve plots in sklearn, let's import the titanic dataset and clean it.\n"}
{"snippet": "scale = StandardScaler()\nXs = scale.fit_transform(X)\nXs\n", "intent": "We know that we need to scale our data for the KNN algorithm right?\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\ndf = pd.DataFrame(boston[\"data\"])\ndf.columns = boston[\"feature_names\"]\ndf[\"MEDV\"] = boston[\"target\"]\ndf.head()\n", "intent": "Ridge is better for dealing with multicollinearity and Lasso is better for high number of features.\n"}
{"snippet": "pca = PCA()\nX_transformed = pd.DataFrame(pca.fit_transform(X_train))\nX_transformed.head()\n", "intent": "Now lets transform the data into principal component space.\n"}
{"snippet": "pca_dim_reducer = PCA(n_components = 20)\nX_transformed_reduced = pd.DataFrame(pca_dim_reducer.fit_transform(X_train))\nreduced_dim_covar = X_transformed_reduced.cov()\nnp.trace(reduced_dim_covar)\n", "intent": "As a general rule, want to retain 90% of the total original variance\n"}
{"snippet": "pca = PCA()\nX_transformed = pca.fit_transform(X_train)\nX_transformed_reduced = X_transformed[:,:20]\n", "intent": "Transform the data using the original PCA, but keep only the first num_comp columns\n"}
{"snippet": "train_df, test_df = train_test_split(df, random_state = 0)\n", "intent": "Redo the train/test split\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df[predictors], df[target], random_state=2)\n", "intent": "This time, let's separate X from y\n"}
{"snippet": "categories = ['alt.atheism', 'sci.space', 'talk.religion.misc', 'comp.graphics']\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\nprint newsgroups_train.target_names\nprint newsgroups_train.filenames.shape, newsgroups_train.target.shape\n", "intent": "It is possible to load only a sub-selection of the categories:\n"}
{"snippet": "from sklearn.feature_extraction import FeatureHasher\nfh = FeatureHasher(n_features=5)\nfeature_dict = X_train[categorical_predictors].to_dict(orient='records')\nfh.fit(feature_dict)\nout = pd.DataFrame(fh.transform(feature_dict).toarray())\n", "intent": "1: Feature Hashing\nCollapses levels of the variable to a specified number of features (hashing trick?)\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndv = DictVectorizer(sparse=False)\nfeature_dict = X_train[categorical_predictors].to_dict(orient='records')\ndv.fit(feature_dict)\nout = pd.DataFrame(\n    dv.transform(feature_dict),\n    columns = dv.feature_names_\n)\n", "intent": "2: One-hot encoding  \nCreates a dummy variable for each level of the categorical variable\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndv = DictVectorizer(sparse=False)\nfeature_dict = X_train[categorical_predictors].to_dict(orient='records')\ndv.fit(feature_dict)\nout = pd.DataFrame(\n    dv.transform(feature_dict),\n    columns = dv.feature_names_\n)\nfeature_dict\n", "intent": "2: One-hot encoding  \nCreates a dummy variable for each level of the categorical variable\n"}
{"snippet": "from sklearn.pipeline import FeatureUnion\nfu = FeatureUnion([\n        ('numerical_pipe', pipe),\n        ('categorical_pipe', categorical_pipe)\n    ] \n)\n", "intent": "**Exercise:** Use sklearn's FeatureUnion to combine both of your pipelines (one continuous and one categorical) into a single step.\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndv = DictVectorizer(sparse=False)\nfeature_dict = X_train[categorical_predictors].to_dict(orient='records')\ndv.fit(feature_dict)\nout = pd.DataFrame(\n    dv.transform(feature_dict),\n    columns = dv.feature_names_\n)\n", "intent": "2: One-hot encoding\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nclass MyImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, cols):\n        self.cols = cols\n    def fit(self, X, y=None):\n        self.imp = Imputer(strategy='mean')\n        self.imp.fit(X[self.cols])\n        return self\n    def transform(self, X):\n        return self.imp.transform(X[self.cols])\n", "intent": "**Exercise:** Write your own class, MyImputer that takes as an argument the columns you would like to impute missing values for.\n"}
{"snippet": "from sklearn.pipeline import FeatureUnion\nfu = FeatureUnion( \n)\n", "intent": "**Exercise:** Use sklearn's FeatureUnion to combine both of your pipelines (one continuous and one categorical) into a single step.\n"}
{"snippet": "pca = PCA()\nX_transformed = pca.fit_transform(X_train)\n", "intent": "Now lets transform the data into principal component space.\n"}
{"snippet": "new_covar = pd.DataFrame(X_transformed).cov()\n", "intent": "What does the covariance look like now?\n"}
{"snippet": "iris = datasets.load_iris()\nX = iris.data[:, :2]  \ny = iris.target\n", "intent": "Import some data to play with.\n"}
{"snippet": "X_transformed = PCA(n_components=20).fit_transform(X_train)\n", "intent": "Create a new PCA object that is explicity a dimension reducer\n"}
{"snippet": "new_covar = np.round(pd.DataFrame(X_transformed).cov(), 10)\n", "intent": "What does the covariance look like now?\n"}
{"snippet": "X_reduced = PCA(n_components=num_comp).fit_transform(X_train)\n", "intent": "Create a new PCA object that is explicity a dimension reducer\n"}
{"snippet": "train_df, test_df = train_test_split(df, random_state=0)\n", "intent": "Redo the train/test split\n"}
{"snippet": "from sklearn.pipeline import FeatureUnion\nfu = FeatureUnion([('categorical', categorical_pipe), ('numeric', pipe)])\n", "intent": "**Exercise:** Use sklearn's FeatureUnion to combine both of your pipelines (one continuous and one categorical) into a single step.\n"}
{"snippet": "for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    comb_data[col] = comb_data[col].fillna('None')\n", "intent": "All this columns Show or Describe same parameter that is condition of Garage. So we can replace their missing value in single loop\n"}
{"snippet": "for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    comb_data[col] = comb_data[col].fillna(0)\n", "intent": "This columns show or Describe only parameter that is Basement. Since the house with no Basement we can replace the missing value as '0'.\n"}
{"snippet": "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    comb_data[col] = comb_data[col].fillna('None')\n", "intent": "As this columns explain only same parameter about Basement.\n"}
{"snippet": "comb_data[\"Functional\"] = comb_data[\"Functional\"].fillna(\"Typ\")\n", "intent": "This feature explain the Home Functionality rating, and its column contain most high contribution as 'Typ'\nSo we will replace the Nan value but Typ\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ncategories = ['alt.atheism', 'sci.space', 'talk.religion.misc', 'comp.graphics']\nnewsgroups = fetch_20newsgroups(subset='train', categories=categories)\nprint newsgroups.target_names \nprint len(newsgroups.data), 'newsgroups articles in', len(newsgroups.target_names), 'groups'\n", "intent": "Let's import the dataset. We use the keyword argument `categories=[...]` to limit our dataset. Omit this argument to load all twenty newsgroups.\n"}
{"snippet": "comb_data['Exterior1st'] = comb_data['Exterior1st'].fillna(comb_data['Exterior1st'].mode()[0])\ncomb_data['Exterior2nd'] = comb_data['Exterior2nd'].fillna(comb_data['Exterior2nd'].mode()[0])\n", "intent": "Again like above column this column also contain only 1 missing value. We can replace missing value using mode method.\n"}
{"snippet": "comb_data['SaleType'] = comb_data['SaleType'].fillna(comb_data['SaleType'].mode()[0])\n", "intent": "Fill the missing value using Mode method since only one category is maximum.\n"}
{"snippet": "comb_data['MSSubClass'] = comb_data['MSSubClass'].fillna(\"None\")\n", "intent": "This columns gives description as building class. In this Na means that house has not given class, so we will replace the nan value as None\n"}
{"snippet": "comb_data[\"GarageCars\"].fillna(0, inplace=True)\n", "intent": "In this columns we will replace null value as 0.\n"}
{"snippet": "sessions = pd.read_csv('./data/sessions.csv')\ninput_train = pd.read_csv('./data/train_users.csv')\ndf_train, train_labels = input_train.iloc[:,:-1], input_train.iloc[:,-1:]\n", "intent": "Here, we will try to infer additional features from the sessions data and use it in conjunction with the user train data.\n"}
{"snippet": "sessions['action_type'].fillna('-unknown-', inplace=True)\nsessions['action_detail'].fillna('-unknown-', inplace=True)\n", "intent": "We will fill NaN with the -unknown- tag.\n"}
{"snippet": "sessions['secs_elapsed'].fillna(0, inplace=True)\n", "intent": "Let's impute the NaN values with the minimum. i.e. secs_elapsed = 0.\n"}
{"snippet": "scaled_df = pd.DataFrame(data=tranformed_scaler, columns=bank_data.drop('Class', axis=1).columns)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "college = pd.read_csv('College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "X, y = load_iris().data, load_iris().target\n", "intent": "Load the iris dataset.\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_feat, columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train1, X_test1, y_train1, y_test1 = train_test_split(yelp_class['text'], y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "df = dd.read_csv(taxi_fp, \n                 parse_dates=['tpep_pickup_datetime', \n                              'tpep_dropoff_datetime'])\n", "intent": "*Dask Profiler* will now be available at [`http://localhost:8787/status/`](http://localhost:8787/status/)\n"}
{"snippet": "train_data = pd.read_csv(\"E:\\\\INSOFE\\\\AI and Deep Learning\\\\Cute5\\\\train.csv\")\ntest_data = pd.read_csv(\"E:\\\\INSOFE\\\\AI and Deep Learning\\\\Cute5\\\\test.csv\")\n", "intent": "Read train and test data\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import cross_validation\nn_trees = np.arange(30, 70, 10)\nfor n in n_trees:\n    clf = RandomForestClassifier(n_estimators=n)\n    X_train, X_test, y_train, y_test = cross_validation.train_test_split(df_data, df_target, test_size=0.4)\n    clf.fit(X_train, y_train)    \n    print(n, clf.score(X_test, y_test))\n", "intent": "Selected values: learning_rate = 0.05, n_estimators = 80\n"}
{"snippet": "clf.feature_importances_\nfeat_imp = pd.DataFrame()\nfeat_imp['feature'] = X.columns\nfeat_imp['imp'] = clf.feature_importances_\nsns.barplot(x='imp', y='feature', data=feat_imp.sort_values('imp'), orient='h', palette='Blues')\n", "intent": "Select two features with highest importance just to visualize decision surface. CV score for the model with only two features selected is quite low.\n"}
{"snippet": "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()\nTrain_tfidf = tfidf.fit_transform(newsgroups.data)\n", "intent": "$${\\displaystyle \\mathrm {idf} (t,D)=\\log {\\frac {|D|}{|\\{\\,d_{i}\\in D\\mid t\\in d_{i}\\,\\}|}}} ,$$\n"}
{"snippet": "titanic_df = pd.read_csv(\"../data/titanic.csv\")\n", "intent": "Pandas can import and export variet of data files like:\n* csv, text\n* SQL databse\n* Excel\n* json\n* others (eg. HDF5, pickle,etc)\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimputer = Imputer(strategy=\"median\")\nimputer.fit(trainset_num)\nimputer.transform(trainset_num)\n", "intent": "Imputer transformation can be used to fill in blanks with e.g. median\n"}
{"snippet": "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, min_df=2, max_df=0.5)\nX = vectorizer.fit_transform(dataset.data)\nprint \"Dimensions feature matrix:\", X.shape\n", "intent": "Extract features from the training dataset using a sparse vectorizer.\n"}
{"snippet": "X = df3.iloc[:,:3]\ny = df3.iloc[:,3]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=4444)\n", "intent": "Use logistic regression to predict survival after 5 years. How well does your model do?\n"}
{"snippet": "X = df.iloc[:,1:]\ny = df[0]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4444)\n", "intent": "Split the data into a test and training set. Use this function:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(bcs_df.iloc[:,:3], bcs_df.iloc[:,3], test_size=0.3)\n", "intent": "The earliest year of surgery is 1958. The most recent year is 1969.\n"}
{"snippet": "df = pd.read_csv(\"../house_votes_84.csv\",header=None)\n", "intent": "For the house representatives data set, calculate the accuracy, precision, recall and f1 scores of each classifier you built (on the test set).\n"}
{"snippet": "df = pd.read_csv('house-votes-84.data', names=['Party']+['X'+str(i) for i in xrange(1,17)])\ndf.replace('y', 1, inplace=True)\ndf.replace('n', 0, inplace=True)\ndf.replace('?', np.nan, inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.head()\n", "intent": "For the house representatives data set, calculate the accuracy, precision, recall and f1 scores of each classifier you built (on the test set).\n"}
{"snippet": "dfCon = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data',header=None)\n", "intent": "For the house representatives data set, calculate the accuracy, precision, recall and f1 scores of each classifier you built (on the test set).\n"}
{"snippet": "pca = PCA(n_components=2).fit(X_train)\nX_train = pca.transform(X_train)\nX_test = pca.transform(X_test)\npca_std = PCA(n_components=2).fit(X_train_std)\nX_train_std = pca_std.transform(X_train_std)\nX_test_std = pca_std.transform(X_test_std)\n", "intent": "Now, we perform a PCA on the standardized and the non-standardized datasets to transform the dataset onto a 2-dimensional feature subspace.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nimport sklearn.metrics.pairwise as smp\nX_train, X_test, y_train, y_test = train_test_split(ng_lsi, ng.target, test_size=0.33)\nknn = KNeighborsClassifier(n_neighbors=3, metric=smp.cosine_distances)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)\n", "intent": "- Try some simple classification on the result LSI vectors for the 20 NG set:\n"}
{"snippet": "ratings = pd.read_table('./movielens/ratings.dat', sep='::', names= ['UserID','MovieID','Rating','Timestamp'])\n", "intent": "Load the ratings.dat data into a `ratings` variable with the same separator, and the column names UserID, MovieID, Rating, Timestamp.\n"}
{"snippet": "n, r = 100, .1\nX = pd.DataFrame()\ncentroids = [[.2, .2], [.8, .2], [.5, .8]]\nfor centroid in centroids:\n    X = X.append(pd.DataFrame(dict(\n        x1=centroid[0] + r * np.random.randn(n),\n        x2=centroid[1] + r * np.random.randn(n))), ignore_index=True)\nX = pd.DataFrame(X)\n", "intent": "Create some test data.\n"}
{"snippet": "df = pd.read_csv('movie_data.csv')\ndf\n", "intent": "**2000 top grossing films based on domestic adjusted gross\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "Split the dataset, then set aside the test set for later...\n"}
{"snippet": "df_all = pd.concat([pd.to_numeric(df_num.CA),df_rsp,df_att],axis=1)\ndf_all = df_all[pd.notnull(df_all['CA'])]\ndf_all.fillna(value=0,inplace=True)\n", "intent": "Make a giant-ass matrix with y as column1 and features as rest of the columns\n"}
{"snippet": "import pandas as pd\ncombats = pd.read_csv('train.csv')\ncombats.head(3)\n", "intent": "http://rautaku.hatenablog.com/entry/2017/12/10/000000\n"}
{"snippet": "iris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "- Multi-class classification\n- Using accuracy as evaluation metric\n- Using cross-validation to select tuning parameters (aka \"hyperparameters\")\n"}
{"snippet": "data = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT5/master/data/default.csv')\nX = data[['balance']]\ny = data.default\n", "intent": "- Binary classification\n- Using AUC as evaluation metric\n- Using cross-validation to select between models\n"}
{"snippet": "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\n", "intent": "- Regression\n- Using RMSE as evaluation metric\n- Using cross-validation to select features\n"}
{"snippet": "from sklearn.datasets import fetch_olivetti_faces\noliv=fetch_olivetti_faces()\nprint oliv.keys()\nprint oliv.data.shape \n", "intent": "This notebook is based on Shankar Muthuswamy's example at https://shankarmsy.github.io/posts/pca-sklearn.html\n"}
{"snippet": "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \nsentences_train = vect.fit_transform(paragraphs_text)\nmodel = lda.LDA(n_topics=10, n_iter=500)\nmodel.fit(sentences_train) \nn_top_words = 10\ntopic_word = model.topic_word_\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n    print('Topic {}: {}'.format(i+1, ', '.join(topic_words)))\n", "intent": "We can try running LDA using the paragraphs as documents.\n"}
{"snippet": "votes = pd.read_csv('http://gadatascience.com/datasets/congress/congressional_votes.csv')\ncongress = votes[['display_name', 'id', 'party', 'state']].drop_duplicates().sort('display_name').reset_index(drop=True)\nprint \"We have %d votes on %d bills from %d members in congress.\" % \\\n    (len(votes), votes.question.nunique(), len(congress))\n", "intent": "Download voting data.\n"}
{"snippet": "titanic = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT5/master/data/titanic_train.csv')\ntitanic.head(10)\n", "intent": "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle.\n"}
{"snippet": "data, features, feature_names, target, target_names, labels = load_data()\nfeatures = features[~is_setosa]\nlabels = labels[~is_setosa]\nvirginica = (labels == \"virginica\")\n", "intent": "So, if the petal length is less than 2 the Iris is a setosa\n"}
{"snippet": "import os\nFILE_PATH = './BuildingMachineLearningSystemsWithPython/ch03/data/toy/'\nposts = [open(os.path.join(FILE_PATH, f)).read() for f in os.listdir(FILE_PATH)]\nvecorizer = CountVectorizer(min_df=1)\nX_train = vecorizer.fit_transform(posts)\nnum_samples, num_features = X_train.shape\nprint('\n", "intent": "So the first sentence contains all but one of the words in the feature set, the second all but three\n"}
{"snippet": "gbc_tuned_df = pd.DataFrame(data=gbc.feature_importances_[:15], index=X_train.columns.values[:15])\ngbc_tuned_plot = gbc_tuned_df.sort_values(by=0).plot.barh(figsize=(15, 10), fontsize=14)\ngbc_tuned_plot.axes.legend().set_visible(False)\ngbc_tuned_plot.set_title('Feature Importance Ranked', fontsize=14)\ngbc_tuned_plot.axes.set_xlabel('Relative ranking score that sums to 1.0', fontsize=14)\ngbc_tuned_plot.axes.set_ylabel('Features', fontsize=14)\n", "intent": "<!--\n-->\nHere is a horizontal barchart of the sorted feature importances from the tuned **`GradientBoostingClassifier`**:\n"}
{"snippet": "data_A = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\ndataframeA= pd.read_csv(data_A, delimiter=',' )\ndata_B = os.path.join(os.getcwd(), 'datasets',  'train_20news_partB.csv')\ndataframeB= pd.read_csv(data_B, delimiter= ',' )\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "data_A = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\ndataframeA= pd.read_csv(data_A, delimiter=',' )\ndata_B = os.path.join(os.getcwd(), 'datasets',  'train_20news_partB.csv')\ndataframeB= pd.read_csv(data_B, delimiter= ',' )\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n<span style=\"color:red\">OK</span>\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'spambase_test.csv')\nspambase_test = pd.read_csv(data_path, delimiter = ',')\nspambase_test\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure\n"}
{"snippet": "data_path_A = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\nnews_A = pd.read_csv(data_path_A, delimiter = ',')\ndata_path_B = os.path.join(os.getcwd(), 'datasets', 'train_20news_partB.csv')\nnews_B = pd.read_csv(data_path_B, delimiter = ',')\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'train_images_partB.csv')\nmc_train = pd.read_csv(data_path, delimiter = ',')\ndata_path = os.path.join(os.getcwd(), 'datasets', 'valid_images_partB.csv')\nmc_val = pd.read_csv(data_path, delimiter = ',')\ndata_path = os.path.join(os.getcwd(), 'datasets', 'test_images_partB.csv')\nmc_test = pd.read_csv(data_path, delimiter = ',')\n", "intent": "*Your answer goes here (max. 600 words)*\n"}
{"snippet": "vectorizer = CountVectorizer(stop_words='english', max_features=10000, min_df=2, max_df=0.5)\nX = vectorizer.fit_transform(dataset.data)\nn_samples, n_features = X.shape\nprint \"Dimensions feature matrix:\", X.shape\n", "intent": "Extract features from the training dataset using a sparse vectorizer.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nprint('train/test split: {}/{}={:.2f}'.format(len(X_train), len(X_valid), len(X_valid)/len(X_train)))\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=0)\nprint('train/test split: {}/{}={:.2f}'.format(len(X_train), len(X_valid), len(X_valid)/len(X_train)))\n", "intent": "The training (and test) dataset is more balanced now.\n"}
{"snippet": "pseudo_inputs = sparse\nnew_test_inputs = training_inputs[0:1000,:]\ntest_cov, normal_msense, pseudo_msense, normal_peroutput_msense, pseudo_peroutput_msense, normal_mu, pseudo_mu, K_normal, K_pseudo = dp4gp.get_noise_scale(ys,new_test_inputs,training_inputs,pseudo_inputs,lengthscales,sigma,calc_normal=use_normal)  \n", "intent": "Sample at each training point (not we don't leave one out, we are leaving in the training data)\n"}
{"snippet": "pseudo_inputs = sparse\ntest_inputs = training_inputs[0:1000,:]\ncalc_normal = False\ntest_cov, normal_msense, pseudo_msense, normal_peroutput_msense, pseudo_peroutput_msense, normal_mu, pseudo_mu, K_normal, K_pseudo = dp4gp.get_noise_scale(ys,test_inputs,training_inputs,pseudo_inputs,lengthscales,sigma,calc_normal=calc_normal)\n", "intent": "Sample at each training point (not we don't leave one out, we are leaving in the training data)\n"}
{"snippet": "for j in range(5):\n    print(\"zipcodes in cluster\", j)\n    print(cleannyczips[km.labels_ == j])\n    print(\"\\n\\n\\n\\n\")\nclustersdf = gp.GeoDataFrame()\nclustersdf['ZIPCODE'] = cleannyczips\nclustersdf['cluster'] = km.labels_\n", "intent": "the cluster centers for 5 k-means clusters of business patterns (number of businesses) at the zipcode level for NYC zipcodes\n"}
{"snippet": "for j in range(5):\n    print(\"zipcodes in cluster\", j)\n    print(cleannyczips[labelsag == j])\n    print(\"\\n\\n\\n\\n\")\nclustersdf = gp.GeoDataFrame()\nclustersdf['ZIPCODE'] = cleannyczips\nclustersdf['cluster'] = labelsag\n", "intent": "as Figure 2 but for hierarchical agglomerative clustering\n"}
{"snippet": "for j in range(nc):\n    print(\"zipcodes in cluster\", j)\n    print(cleannyczips[labelsag == j])\n    print(\"\\n\\n\\n\\n\")\nclustersdf = gp.GeoDataFrame()\nclustersdf['ZIPCODE'] = cleannyczips\nclustersdf['cluster'] = labelsag\n", "intent": "As figure 2 and 4: time series of business counts in NYC by zipcode, with hierarchical clustering in 7 clusters. \n"}
{"snippet": "for j in range(nc):\n    print(\"zipcodes in cluster\", j)\n    print(cleannyczips[labelsag == j])\n    print(\"\\n\\n\\n\\n\")\nclustersdf = gp.GeoDataFrame()\nclustersdf['ZIPCODE'] = cleannyczips\nclustersdf['cluster'] = labelsag\n", "intent": "As Figure  2, 4, 6 but for smoothed time series \n"}
{"snippet": "save_dir = \"/home/ucsd-train02/projects/single_cell_intestine/results/\"\ndf_tsne_pca.to_csv(save_dir+\"tsne_phenograph_k30_results.csv\")\n", "intent": "Save the cluster assignments to a file that we will use later for differential gene expression.\n"}
{"snippet": "dataset = pd.read_csv('data.csv')\ndataset['Timestamp'] = pd.to_datetime(dataset['Unnamed: 0'])\nlabel = dataset['Measured & upscaled [MW]']\ndataset = dataset.drop(columns=['Unnamed: 0', 'Measured & upscaled [MW]', 'Monitored Capacity [MW]'])\ndataset = dataset.set_index('Timestamp')\n", "intent": "Here I'm making some preprocessing to use data in machine learning algorithms.\n"}
{"snippet": "model = PCA(n_components=2)\nPCs = model.fit_transform(X)\n", "intent": "Let's find the principal components in this data. Note that we can't have more components than n_features, so we're looking for all two components.\n"}
{"snippet": "from sklearn import decomposition\nlda1 = decomposition.LatentDirichletAllocation(n_topics=6)\nlda1.fit(tfidf)\nW = lda1.transform(tfidf)\nH = lda1.components_\n", "intent": "Compare the results of Nonnegative Matrix Factorization (NMF) with Latent Dirichlet Allocation (LDA).\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n                                                    train_size=0.75, test_size=0.25)\n", "intent": "Here we'll use 75% of the data for training, and test on the remaining 25%.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n                                                    train_size=0.75, test_size=0.25)\n", "intent": "Now we can use the train_test_split feature:\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "First we'll load the Boston dataset.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n                                                    train_size=0.75, test_size=0.25)\n", "intent": "Then split our data into train and test sets.\n"}
{"snippet": "Loadings =  pd.DataFrame((pc_final.components_.T * np.sqrt(pc_final.explained_variance_)).T,columns=credit_new.columns).T\n", "intent": "Loadings=Eigenvectors * sqrt(Eigenvalues)\nloadings are the covariances/correlations between the original variables and the unit-scaled components.\n"}
{"snippet": "from sklearn import datasets,svm\ndigits = datasets.load_digits()\nx_digits= digits.data\ny_digits = digits.target\nsvc = svm.SVC(C=1,kernel = 'linear')\nsvc.fit(x_digits[:-100],y_digits[:-100]).score(x_digits[-100:],y_digits[-100:])\n", "intent": "**Model selection: choosing estimators and their parameters**\n"}
{"snippet": "from sklearn import cluster,datasets\niris= datasets.load_iris()\nx_iris = iris.data\ny_iris = iris.target\nk_means = cluster.KMeans(n_clusters =3)\nk_means.fit(x_iris)\n", "intent": "**Unsupervised learning: seeking representations of the data**\n"}
{"snippet": "x1= np.random.normal(size=100)\nx2= np.random.normal(size=100)\nx3=x1+x2\nX=np.c_[x1,x2,x3]\nfrom sklearn import decomposition\npca = decomposition.PCA()\npca.fit(X)\n", "intent": "**Principal component analysis: PCA**\n"}
{"snippet": "max_features = 5000\ncv = CountVectorizer(max_features=max_features)\nX_style = cv.fit_transform(data.beer_style)\n", "intent": "Let's use each word in the beer style as a feature as well (e.g., \"IPA\")\n"}
{"snippet": "from sklearn.svm import LinearSVC\nfrom sklearn.datasets import make_classification\nX,y = make_classification(n_features=4,random_state=0)\nclf = LinearSVC(random_state=0)\nclf.fit(X,y)\n", "intent": "[kernel functions](http://scikit-learn.org/stable/modules/svm.html\n"}
{"snippet": "v = VarianceThreshold(0.5)\nv.fit_transform(modelData.fillna(-999)).shape\n", "intent": "http://bluewhale.cc/2016-11-25/use-scikit-learn-for-feature-selection.html\n"}
{"snippet": "df = pd.read_csv(\"/Users/Tenorio/vermonster/exercises/DS_BOS_07/Data/Spam Classification/sms.csv\")\ndf.head()\n", "intent": "Now read in the data with `pandas` `read_csv()` and check it out with `head()`:\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer(decode_error = 'ignore')\nvect.fit(X_train)\nprint(vect.get_feature_names())\n", "intent": "Use `sklearn.feature_extraction.text.CountVectorizer` on the training set to create a vectorizer called `vect`.\n"}
{"snippet": "titanic = pd.read_csv('/Users/Tenorio/vermonster/exercises/DS_BOS_07/Data/Titanic/titanic.csv')\ntitanic.head()\n", "intent": "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle.\n"}
{"snippet": "iris = datasets.load_iris()\nprint iris.feature_names\nX = iris.data\nprint X[:, 1]\nprint iris.target_names\n", "intent": "Display the features names\n"}
{"snippet": "df = pd.read_csv(\"example data/Classified Data\",index_col=0)\n", "intent": "Set index_col=0 to use the first column as the index.\n"}
{"snippet": "from sklearn.datasets import load_iris\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier()\niris = load_iris()\nclf = clf.fit(iris.data, iris.target)\ntree.export_graphviz(clf, out_file='tree2.dot')  \n", "intent": "** note: need to install graphviz and run notebook from Anaconda launcher - otherwise may complain about path\n"}
{"snippet": "pred = pd.DataFrame(index=y_test.index)\npred[\"Actual\"] = y_test\n", "intent": "Create the prediction DataFrame:\n"}
{"snippet": "from sklearn import datasets\nboston = datasets.load_boston() \nX = boston['data']   \nY = boston['target'] \n", "intent": "Let's take a look at the famous Boston Housing data\n"}
{"snippet": "import pandas as pd\ndf = pd.read_table('../smsspamcollection/SMSSpamCollection',\n                    sep='\\t',\n                    header = None,\n                    names=['label','sms_message'])\ndf.head()\n", "intent": "https://github.com/udacity/machine-learning/blob/master/projects/practice_projects/naive_bayes_tutorial/Naive_Bayes_tutorial.ipynb\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=4)\nprint X_train[0]\nprint X_test[0]\nprint y_train[0]\nprint y_test[0]\n", "intent": "* refaire le meme avec random_state = 4\n"}
{"snippet": "import pandas\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nimport pickle\ndf = pandas.read_csv(\"train_loan.csv\")\n", "intent": "* Chargement du fichier sur les prets bancaires\n"}
{"snippet": "val = {}\nmodule = 'Adult'\nval[module] = pd.read_csv('../3-data/phmrc_cleaned.csv')\n", "intent": "An approach to really do the cross-validation *out of sample*:\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidfvec = TfidfVectorizer()\ndtm_tfidf_df = pandas.DataFrame(tfidfvec.fit_transform(df.body).toarray(), columns=tfidfvec.get_feature_names(), index = df.index)\ndtm_tfidf_df\n", "intent": "To do so, we simply do the same thing we did above with CountVectorizer, but instead we use the function TfidfVectorizer.\n"}
{"snippet": "import os\nreview_path = '../../A-Data/poems/reviewed/'\nrandom_path = '../../A-Data/poems/random/'\nreview_files = os.listdir(review_path)\nrandom_files = os.listdir(random_path)\nreview_texts = [open(review_path+file_name).read() for file_name in review_files]\nrandom_texts = [open(random_path+file_name).read() for file_name in random_files]\nreview_texts[0] \n", "intent": "First we will read the texts and turn them into lists.  \n"}
{"snippet": "df1 = pandas.DataFrame(review_texts, columns = ['body'])\ndf1['label'] = \"review\"\ndf2 = pandas.DataFrame(random_texts, columns = ['body'])\ndf2['label'] = \"random\"\ndf = pandas.concat([df1,df2])\ndf\n", "intent": "Notice the strange output here? These poems are saved in a bag of words format.\nNext we'll create a Pandas dataframe.\n"}
{"snippet": "import os\nreview_path = '../A-Data/poems/reviewed/'\nrandom_path = '../A-Data/poems/random/'\nreview_files = os.listdir(review_path)\nrandom_files = os.listdir(random_path)\nreview_texts = [open(review_path+file_name).read() for file_name in review_files]\nrandom_texts = [open(random_path+file_name).read() for file_name in random_files]\nreview_texts[0] \n", "intent": "First we will read the texts and turn them into lists.  \n"}
{"snippet": "population_index.to_csv(\"test_df_to_csv.csv\")\n", "intent": "> The data can be saved to new csv files with teh following command or to html using df.to_html()\n"}
{"snippet": "from sklearn import svm\nfrom sklearn import datasets\nclf = svm.SVC()\niris = datasets.load_iris()\nX, y = iris.data, iris.target\nclf.fit(X, y)\n", "intent": "we might sometimes build a model that take a long time to construct. We can easily save the model (on disk) for future use.\n"}
{"snippet": "rand = np.random.mtrand.RandomState(8675309)  \ncats = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\ntraindata = fetch_20newsgroups(subset='train',                  \n                          categories=cats,                      \n                          shuffle=True,                         \n                          remove=('headers', 'footers', 'quotes'), \n                          random_state=rand)\n", "intent": "_20newsgroups_ dataset\nSee http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n"}
{"snippet": "X = encode[:50]\ntsne_data = manifold.TSNE(n_components=2).fit_transform(X)\n", "intent": "We will take first 50 values to visualize the data.\n"}
{"snippet": "tsne_data = manifold.TSNE(n_components=2).fit_transform(X)\nplot_clustering(tsne_data[indices], X[indices], y_num, title='t-SNE')\n", "intent": "**Excerise 5:** Apply T-SNE on the data\n"}
{"snippet": "from sklearn import decomposition\nimport numpy as np\npca = decomposition.PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nplot_clustering(X_pca, X, y, title='PCA')\n", "intent": "**Excerise 1:** Apply PCA on the data\n"}
{"snippet": "tsne_data = manifold.TSNE(n_components=2).fit_transform(X)\nplot_clustering(tsne_data, X, y, title='t-SNE')\n", "intent": "**Excerise 4:** Apply t-SNE on the data\n"}
{"snippet": "from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split\nknr = KNeighborsRegressor(n_neighbors=3)\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=0)\nknr.fit(X_train, y_train)\nknr.score(X_test, y_test)\n", "intent": "Split the boston housing data into training and test set, apply the ``KNeighborsRegressor`` and compute the test set $R^2$.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression, Lasso, Ridge\nfrom sklearn.cross_validation import train_test_split\nimport patsy\nformula = 'label ~ spelling_errors_ratio + linkwordscore + non_markup_alphanum_characters + frameTagRatio -1'\ny_mat, x_mat = patsy.dmatrices(formula, data = su)\nx_train, x_test, y_train, y_test = train_test_split(x_mat,y_mat, test_size=0.3)\n", "intent": "And print out the results as shown in the example above.\n---\n"}
{"snippet": "formula = 'label ~ C(alchemy_category) + spelling_errors_ratio + linkwordscore + non_markup_alphanum_characters + frameTagRatio -1'\ny_mat, x_mat = patsy.dmatrices(formula, data = norm_su)\nx_train, x_test, y_train, y_test = train_test_split(x_mat,y_mat, test_size=0.3)\n", "intent": "And print out the results as shown in the example.\n---\n"}
{"snippet": "formula = 'label ~ C(alchemy_category) + spelling_errors_ratio + linkwordscore + non_markup_alphanum_characters + frameTagRatio -1'\ny_mat, x_mat = patsy.dmatrices(formula, data = norm_su)\nx_train, x_test, y_train, y_test = train_test_split(x_mat,y_mat, test_size=0.3)\n", "intent": "Normalize the numeric and categorical columns of the predictor matrix.\n---\n"}
{"snippet": "import pandas as pd\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/titanic.csv'\ntitanic = pd.read_csv(url, index_col='PassengerId')\ntitanic.shape\n", "intent": "scikit-learn models expect that all values are **numeric** and **hold meaning**. Thus, missing values are not allowed by scikit-learn.\n"}
{"snippet": "dfv = pd.read_csv(votes_file)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "Xs = StandardScaler().fit_transform(X)\n", "intent": "Next, create the covariance matrix from the standardized x-values and decompose these values to find the eigenvalues and eigenvectors\n"}
{"snippet": "PCA_set = PCA(n_components=5)\nX = PCA_set.fit_transform(Xs)\n", "intent": "Now, conduct a PCA using scikit learn\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "df = pd.read_csv('./assets/datasets/airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "Xs = StandardScaler().fit_transform(X)\n", "intent": "Then, standardize the features for analysis\n"}
{"snippet": "pc = pd.DataFrame(Xpc, columns = ['PC1', 'PC2'])\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "y = boston.target\nboston = pd.DataFrame(boston.data)\n", "intent": "Next, separate the data into the features and target using the following code:\n`y = boston.target`\n`boston = pd.DataFrame(boston.data)`\n"}
{"snippet": "churn_file = pd.read_csv(r'C:\\Users\\LinRi001\\Desktop\\churn project\\train.csv')\ntransactions = pd.read_csv(r'C:\\Users\\LinRi001\\Desktop\\churn project\\transactions.csv')\nmembers = pd.read_csv(r'C:\\Users\\LinRi001\\Desktop\\churn project\\members_v3.csv') \n", "intent": "This is a Kaggle competition data\nhttps://www.kaggle.com/c/kkbox-churn-prediction-challenge/data\n"}
{"snippet": "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\npcaSimple = PCA(n_components=2)\npcaSimple.fit(X)\nprint(pcaSimple.explained_variance_ratio_) \n", "intent": "Run a simple example\n"}
{"snippet": "of_df = pd.read_csv(\"../assets/datasets/old-faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\n", "intent": "The datasets are loaded into a dictionary.\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\nprint(digits.keys())\n", "intent": "The datasets are loaded into a dictionary\n"}
{"snippet": "project_data = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaled_df = pd.DataFrame(scaled_features, columns=project_data.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = scaled_df\ny = project_data['TARGET CLASS']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n", "intent": "**Use train_test_split to split your data into a training set and a testing set.**\n"}
{"snippet": "from sklearn import datasets            \ndiabetes = datasets.load_diabetes()     \nx_train = diabetes.data[:-20]           \ny_train = diabetes.target[:-20]          \nx_test = diabetes.data[-20:]            \ny_test = diabetes.target[-20:]           \n", "intent": "Start by breaking the 442 patients into a training set (composed of the first 422 patients) and a test set (the last 20 patients).\n"}
{"snippet": "import pandas as pd\npd.set_option('display.max_columns', 500)\nimport zipfile\nwith zipfile.ZipFile('KaggleCredit2.csv.zip', 'r') as z:\n    f = z.open('KaggleCredit2.csv')\n    data = pd.read_csv(f, index_col=0)\ndata.head()\n", "intent": "Read the data into Pandas \n"}
{"snippet": "s_data = scaler.fit_transform(csv.drop('Class',axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "df_feat = pd.DataFrame(data=s_data, columns=csv.columns.drop('Class'))\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "def references_organization(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == 'ORG' for word in parsed])\ndata['references_organization'] = data['title'].fillna(u'').map(references_organization)\ndata[data['references_organization']][['title']].head()\n", "intent": "Let's see if we can find organizations in our page titles.\n"}
{"snippet": "seed = 8 \nX_train, X_test, y_train, y_test = train_test_split(X, y1, test_size=0.2,\n                                                    random_state=seed)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n", "intent": "Split data into training and test datasets\n"}
{"snippet": "def create_ND_clusters(num_clusters, num_features):\n    X, y = sklearn.datasets.make_blobs(n_samples=50000, n_features=num_features, centers=num_clusters, \n                                       cluster_std=1.0, center_box=(0.0, 10.0), shuffle=True, \n                                       random_state=2)\n    return X, y\nnum_clusters = 5\nnum_dimensions = 100\nX_Nd, y_Nd = create_ND_clusters(num_clusters, num_dimensions)\n", "intent": "This section creates a generic dataset to use in an N-dimensional unit test using a single core.\n"}
{"snippet": "token_text = [token.orth_ for token in parsed_review]\ntoken_pos = [token.pos_ for token in parsed_review]\npd.DataFrame(data=list(zip(token_text, token_pos)),\n             columns=['token_text', 'part_of_speech'])\n", "intent": "<h2>Part of Speech Tagging</h2>\n"}
{"snippet": "token_lemma = [token.lemma_ for token in parsed_review]\ntoken_shape = [token.shape_ for token in parsed_review]\npd.DataFrame(list(zip(token_text, token_lemma, token_shape)),\n             columns=['token_text', 'token_lemma', 'token_shape'])\n", "intent": "<h2>Text normalization, like stemming/lemmatization and shape analysis?</h2>\n"}
{"snippet": "token_entity_type = [token.ent_type_ for token in parsed_review]\ntoken_entity_iob = [token.ent_iob_ for token in parsed_review]\npd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)),\n             columns=['token_text', 'entity_type', 'inside_outside_begin'])\n", "intent": "<h2>Token-level entity analysis</h2>\n"}
{"snippet": "warPeacePOS = pd.Series(war_peace.count_by(spacy.attrs.POS))/len(war_peace)\nrcParams['figure.figsize'] = 16, 8\ndf = pd.DataFrame([warPeacePOS], index=['war_peace'])\ndf.columns = [tagDict[column] for column in df.columns]\ndf.T.plot(kind='bar')\n", "intent": "<h2>It's fun to compare the distribution of parts of speech in each text:</h2>\n"}
{"snippet": "def verbsToMatrix(verbCounts): \n    return pd.Series({t[0]: t[1] for t in verbCounts})\nverbsDF = pd.DataFrame({'Napoleon': verbsToMatrix(NapoleonVerbs), \n                        'Pierre': verbsToMatrix(PierreVerbs), \n                        'Nicholas': verbsToMatrix(verbsForCharacters(war_peace, 'Nicholas'))}).fillna(0)\nverbsDF.plot(kind='bar', figsize=(14,4));\n", "intent": "<h2>Now merge these counts into a single table, and then visualize it with Pandas.</h2>\n"}
{"snippet": "grailPOS = pd.Series(grail.count_by(spacy.attrs.POS))/len(grail)\npridePOS = pd.Series(pride.count_by(spacy.attrs.POS))/len(pride)\nrcParams['figure.figsize'] = 16, 8\ndf = pd.DataFrame([grailPOS, pridePOS], index=['Grail', 'Pride'])\ndf.columns = [tagDict[column] for column in df.columns]\ndf.T.plot(kind='bar')\n", "intent": "<h2>It's fun to compare the distribution of parts of speech in each text:</h2>\n"}
{"snippet": "def verbsToMatrix(verbCounts): \n    return pd.Series({t[0]: t[1] for t in verbCounts})\nverbsDF = pd.DataFrame({'Elizabeth': verbsToMatrix(elizabethVerbs), \n                        'Darcy': verbsToMatrix(darcyVerbs), \n                        'Jane': verbsToMatrix(janeVerbs)}).fillna(0)\nverbsDF.plot(kind='bar', figsize=(14,4))\n", "intent": "<h2>Now merge these counts into a single table, and then visualize it with Pandas.</h2>\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    contains_org = any([word.ent_type_ == 'ORG' for word in parsed])\n    contains_person = any([word.ent_type_ == 'PERSON' for word in parsed])\n    return contains_org and contains_person\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title']].head()\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "N = 200\nnp.random.seed(0)\nX, y = sklearn.datasets.make_circles(N, noise = 0.05, factor = 0.5)\n", "intent": "Create data for a simple binary classification problem.\n"}
{"snippet": "data = pd.read_csv(\"task/task_data.csv\").drop([\"sample index\"], axis=1)\ndata.head()\n", "intent": "Read data to `pandas` DataFrame object, discarding first column as it doesn't contain useful information.\n"}
{"snippet": "result_1.to_csv(\"result_1.csv\", header=[\"score\"], index_label=\"sensor\")\n", "intent": "Write it to the file `result_1.csv`:\n"}
{"snippet": "result_3.to_csv(\"result_3.csv\", index=False)\n", "intent": "Note: in this table, score means how much accuracy we lose permuting respective feature.\nSave the results in `result_3.csv` file:\n"}
{"snippet": "df = pd.read_csv('data/train.csv')\nX = df.loc[:, df.columns != 'label'].as_matrix().astype('uint8')\nY = df.label.as_matrix().tolist()\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.25)\n", "intent": "load and split kaggle training data\n"}
{"snippet": "clf2 = joblib.load('mnist.randomforestmodel.sav')\ndf = pd.read_csv('data/test.csv')\nX_sample = df.values\n", "intent": "load the trained model and the kaggle sample set then visualize the first entry\n"}
{"snippet": "index = [i for i in range(1, len(Y_sample)+1)]\nindex\ndf = pd.DataFrame(Y_sample, index=index, columns=[\"Label\"])\ndf.index.name = \"ImageId\"\ndf.to_csv('data/result.csv')\n", "intent": "structure kaggle submission according to requirements\n"}
{"snippet": "data2 = pd.read_csv(\"LogR2.csv\",header = None)\ndata2.columns = [\"Microchip1\",\"Microchip2\",\"Pass\"]\ndata2.head()\n", "intent": "Now imagine this situation, where you have two microchip voltage values. \n"}
{"snippet": "esquerda = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n                     'A': ['A0', 'A1', 'A2', 'A3'],\n                     'B': ['B0', 'B1', 'B2', 'B3']})\ndireita = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n                          'C': ['C0', 'C1', 'C2', 'C3'],\n                          'D': ['D0', 'D1', 'D2', 'D3']})    \n", "intent": "Parecido com o `JOIN` em SQL:\n"}
{"snippet": "import pandas as pd\nyelp = pd.read_csv('../data/yelp.csv')\nyelp.head(1)\n", "intent": "Read `yelp.csv` into a DataFrame.\n"}
{"snippet": "df= pd.DataFrame()\ndf['residuals'] = bos.PRICE - pred\ndf['predicted_vals'] = pred\n", "intent": "The mean is 22.53 and the standard deviation is 7.91\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('http://www.ats.ucla.edu/stat/data/binary.csv')\ndata\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\nrfModel = rf.fit(trainingData)\nrfFeatureImportance = pd.DataFrame([(name, rfModel.featureImportances[idx]) for idx, name in attrs],columns=['feature_name','feature_importance'])\nprint(rfFeatureImportance.sort_values(by=['feature_importance'],ascending =False))\n", "intent": "1. Build and train a RandomForestClassifier and print out a table of feature importances from it.\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.svm import LinearSVC\niris = datasets.load_iris()\niris = datasets.load_iris()\nX2 = iris.data[:, :2]  \ny2 = iris.target\nsvc = SVC(kernel='linear').fit(X2, y2)\nrbf_svc = SVC(kernel='rbf', gamma=0.7).fit(X2, y2)\npoly_svc = SVC(kernel='poly', degree=3).fit(X2, y2)\nlin_svc = LinearSVC().fit(X2, y2)\n", "intent": "Credits: Forked from http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html\n"}
{"snippet": "df_original=pd.read_csv(\"Fraud_Data.csv\",header=0)\n", "intent": "Reading csv file and storing as dataframe\n"}
{"snippet": "Pipe_Rf_roc=Pipeline([(\"Scaler\",StandardScaler()),(\"Rf\",RandomForestClassifier())])\nParam_grid_Rf_roc={\"Scaler__with_std\":[True],'Rf__n_estimators':[30],'Rf__min_samples_split':[650],\\\n                  'Rf__max_features':['auto',1,2],'Rf__class_weight':[{0:1,1:1},'balanced',{0:1,1:2}]}\nbest_Rf_roc=GridSearchCV(Pipe_Rf_roc,Param_grid_Rf_roc,cv=4,verbose=1,n_jobs=1,scoring='roc_auc')\nbest_Rf_roc.fit(Train_X,Train_Y)\nprint best_Rf_roc.best_params_,best_Rf_roc.best_score_,\n", "intent": "I also Tried to optimize for roc-auc score for random forests.  The results were not much different. \n"}
{"snippet": "to_keep_ml3=[\"device use frequency\",\"hours_to_purchase\",\"Country_mask\",\"purchase_value\",\"age\"]\nx_array3=df_ml[to_keep_ml3].values\ny_array3=df_ml[\"class\"]\nprint x_array3.shape,y_array3.shape\nTrain_X3,Test_X3,Train_Y3,Test_y3=train_test_split(x_array3,y_array3,test_size=0.25,stratify=y_array2,random_state=42)\n", "intent": "Now I tried adding some other values such as purchase value and age to our feature set.\n"}
{"snippet": "Scaler_vis=StandardScaler()\nx_scaled=Scaler_vis.fit_transform(x_array)\n", "intent": "Lets try to visualize how the fraud and not fraud classes look for the two most important variables after they have been standard scaled.\n"}
{"snippet": "Pipe_Rf=Pipeline([(\"Scaler\",StandardScaler()),(\"Rf\",RandomForestClassifier())])\nParam_grid_Rf={\"Scaler__with_std\":[True],'Rf__n_estimators':[20],'Rf__min_samples_split':[650,800],\\\n                  'Rf__max_features':['auto'],'Rf__class_weight':['balanced',{0:1,1:1}]}\nbest_Rf_recall=GridSearchCV(Pipe_Rf,Param_grid_Rf,cv=4,verbose=1,n_jobs=1,scoring='recall')\nbest_Rf_recall.fit(Train_X,Train_Y)\nprint best_Rf_recall.best_params_,best_Rf_recall.best_score_,\n", "intent": "We do the same thing for Random forests and spot very similar trends. \n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\n", "intent": "Use CountVectorizer to create document-term matrices from X_train and X_test.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nCount=CountVectorizer()\nX_train_dtm=Count.fit_transform(X_train)\nX_test_dtm=Count.transform(X_test)\n", "intent": "Use CountVectorizer to create **document-term matrices** from X_train and X_test.\n"}
{"snippet": "def references_organization(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == 'ORG' for word in parsed])\ndata['references_organization'] = data['title'].fillna(u'').map(references_organization)\ndata[data['references_organization']][['title']].head().values\n", "intent": "Let's see if we can find organizations in our page titles.\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == 'ORG' for word in parsed]) and any([word.ent_type_ == \"PERSON\" for word in parsed])\ndata['references_org_person'] = data['title'].fillna(u'').map(references_organization)\ndata[data['references_org_person']][['title']].head().values\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "labels_ADMSSN = data_valid['ADMSSN']\ndata_modeling = data_valid[['APPLCN','SATNUM','ACTNUM','YEAR', \n                            'ACTCM25_NORM', 'ACTCM75_NORM', 'SATCM25_NORM', 'SATCM75_NORM']]\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data_modeling, \n                                                    labels_ADMSSN, \n                                                    test_size = 0.20, \n                                                    random_state = 11)\n", "intent": "Enough of pre-processing, let's put our data into a LinearRegression model and see how it performs.\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.svm import LinearSVC\niris = datasets.load_iris()\niris = datasets.load_iris()\n", "intent": "Credits: Forked from http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html\n"}
{"snippet": "daily_rets = prices_df.pivot_table(values=['daily_ret'],\n                                   index='date', \n                                   columns='ticker')\nvol_horizons = [x*21 for x in [1, 3, 6, 12]]\nfor period in vol_horizons:\n    factor_data[str(period)+'d_vol'] = (daily_rets['daily_ret'].rolling(period, \n                                                                  min_periods=(int(period*.8)))\\\n                                      .std() * np.sqrt(252)).stack()\n", "intent": "Define stock price volatility over 1, 3, 6, and 12 months\n"}
{"snippet": "tech_factor_dictionaries = [bop, mfi, dx, stoch_osc, ATR]\ntech_factor_names = ['bop', 'mfi', 'dx', 'stoch_osc', 'atr']\ntech = []\nfor factor, col in zip(tech_factor_dictionaries, tech_factor_names):\n    df = pd.DataFrame(pd.concat(factor), columns=[col])\n    df = df.swaplevel()\n    df.sort_index(level=0, inplace=True)\n    tech.append(df)\ntech_factors = pd.concat(tech, axis=1)\ntech_factors.index.names = ['date', 'ticker']\n", "intent": "Combine dictionaries for technical factors\n"}
{"snippet": "all_factors.to_csv('data//all_factors.csv')\n", "intent": "Write final factor data to csv\n"}
{"snippet": "fwd_ret_horizons = [1, 5, 21, 42, 63]\nfor ret in fwd_ret_horizons:\n    all_factors['fwdret_' + str(ret) + 'd'] = all_factors.pivot_table(values='adj_close',\n                                                                      index='date', \n                                                                      columns='ticker').pct_change(ret).shift(-ret).stack()\n", "intent": "The 63 day future return horizon will be used for this analysis.\n"}
{"snippet": "vect = TfidfVectorizer(stop_words='english')\ndtm = vect.fit_transform(yelp.text)\nfeatures = vect.get_feature_names()\ndtm.shape\n", "intent": "Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF!\n"}
{"snippet": "from keras.datasets import imdb\n(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n                                                      num_words=None,\n                                                      skip_top=0,\n                                                      maxlen=None,\n                                                      seed=113,\n                                                      start_char=1,\n                                                      oov_char=2,\n                                                      index_from=3)\n", "intent": "Load data from keras\n"}
{"snippet": "indir = 'C:\\\\Users\\\\useradmin\\\\Desktop\\\\Learning\\\\Python-Data-Science-and-Machine-Learning-Bootcamp\\\\Machine Learning Sections\\\\Decision-Trees-and-Random-Forests\\\\'\nloans = pd.read_csv(indir +'loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "df = pd.read_csv('../data/housing-data.csv')\n", "intent": "This dataset contains multiple columns:\n- sqft\n- bdrms\n- age\n- price\n"}
{"snippet": "df = pd.read_csv('churn.csv')\n", "intent": "Load the csv file into memory using Pandas\n"}
{"snippet": "bnd_new = pd.DataFrame(bnd_trans,columns=bnd.columns[:-1])\nbnd_new.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "knn_data = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "knn_data_scl = pd.DataFrame(scaled_f,columns=knn_data.columns[:-1])\nknn_data_scl.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nbow_trans = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "X = bow_trans.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "titles = data['title'].fillna('')\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features = 1000, \n                             ngram_range=(1, 2), \n                             stop_words='english',\n                             binary=True)\nvectorizer.fit(titles)\nX = vectorizer.transform(titles)\n", "intent": " We previously used the Count Vectorizer to extract text features for this classification task\n"}
{"snippet": "sequences = []\ny = []\nfor i in xrange(5000):\n    y0 = gen_curve()\n    y1 = gen_curve() + gen_anomaly()\n    sequences.append(y0)\n    sequences.append(y1)\n    y.extend([False, True])\nsequences = pd.DataFrame(sequences)\ny = np.array(y)\n", "intent": "- 10000 curves\n- all with the same length\n- 50% with anomaly\n- 50% without anomaly\n"}
{"snippet": "from scipy.signal import welch\nfff, Pxx = welch(sequences_scaled, nperseg=64, axis=1)\nX = pd.DataFrame(Pxx)\nX.head()\n", "intent": "In this example, the anomaly has a higher frequency than the base signal. Could we isolate it based on the frequency spectrum?\n"}
{"snippet": "features_missing_values = ['Total_Images','Total_Links']\ndata_missing_values = data4[features_missing_values]\nimputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\ndata_missing_values_array = imputer.fit_transform(data_missing_values)\ndata_missing_values = pd.DataFrame(data_missing_values_array, index=data_missing_values.index, columns=data_missing_values.columns)\ndata_missing_values\n", "intent": "- 'Total_Images'\n- 'Total_Links'\n"}
{"snippet": "features_missing_values = ['Total_Images','Total_Links', 'Total_Past_Communications']\ndata_missing_values = data3[features_missing_values]\nimputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\ndata_missing_values_array = imputer.fit_transform(data_missing_values)\ndata_missing_values = pd.DataFrame(data_missing_values_array, index=data_missing_values.index, columns=data_missing_values.columns)\ndata_missing_values\n", "intent": "- 'Total_Images'\n- 'Total_Links'\n- 'Total_Past_Communications'\n"}
{"snippet": "sales = pd.read_csv('data/kc_housing_sales_data.csv')\nsales.head()\n", "intent": "Dataset is from house sales in King County, the region where the city of Seattle, WA is located.\nRead data from csv file and put in dataframe.\n"}
{"snippet": "train_data,test_data = train_test_split(sales, test_size = 0.2)\n", "intent": "Split the data in train data and test data (80:20) ratio\n"}
{"snippet": "train_data,test_data = train_test_split(sales, test_size = 0.2)\n", "intent": "First let's split the data into training and test data.\n"}
{"snippet": "train_data,test_data = train_test_split(sales, test_size = 0.2, train_size = 0.8, random_state = 1)\n", "intent": "Let us split the dataset into training set and test set. Make sure to use `seed=0`:\n"}
{"snippet": "train_data,test_data = cross_validation.train_test_split(sales, test_size=0.2, train_size=0.8, random_state=1)\n", "intent": "Let us split the sales dataset into training and test sets.\n"}
{"snippet": "X_train, X_test = train_test_split(dataframe, test_size=1, random_state=5) \ny_train =X_train[\"y_auto_cura\"]\ny_test = X_test[\"y_auto_cura\"]\n", "intent": "Lo primero es dividir los datos de entrada en entrenamiento y test.\n"}
{"snippet": "count_month = df.groupby('month')['count'].sum()\nnew_index = ['Jan', 'Feb', 'Mar', 'Apr']\ncount_month = count_month.reindex(new_index)\ncount_month = pd.DataFrame(count_month).reset_index()\n", "intent": "**Weekends are when most of the rides are happening**\n"}
{"snippet": "df = pd.read_csv('../input/train.csv')\ndf\n", "intent": "<a id=\"24\"></a> <br>\n"}
{"snippet": "df = pd.read_csv('../input/train.csv')\ndf\n", "intent": "<a id=\"26\"></a> <br>\n"}
{"snippet": "df = pd.read_csv('../input/train.csv')\ndf = df[df['Age']==50]\ndf\n", "intent": "<a id=\"27\"></a> <br>\n"}
{"snippet": "df = pd.DataFrame(['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D'],\n                  index=['excellent', 'excellent', 'excellent', 'good', 'good', 'good', 'ok', 'ok', 'ok', 'poor', 'poor'])\ndf.rename(columns={0: 'Grades'}, inplace=True)\ndf\n", "intent": "<a id=\"28\"></a> <br>\n"}
{"snippet": "normal_sample = np.random.normal(loc=0.0, scale=1.0, size=10000)\nrandom_sample = np.random.random(size=10000)\ngamma_sample = np.random.gamma(2, size=10000)\ndf = pd.DataFrame({'normal': normal_sample, \n                   'random': random_sample, \n                   'gamma': gamma_sample})\n", "intent": "<a id=\"38\"></a> <br>\n"}
{"snippet": "np.random.seed(123)\ndf = pd.DataFrame({'A': np.random.randn(365).cumsum(0), \n                   'B': np.random.randn(365).cumsum(0) + 20,\n                   'C': np.random.randn(365).cumsum(0) - 20}, \n                  index=pd.date_range('1/1/2017', periods=365))\ndf.head()\n", "intent": "<a id=\"42\"></a> <br>\n"}
{"snippet": "concrete_data = pd.read_csv('https://ibm.box.com/shared/static/svl8tu7cmod6tizo6rk0ke4sbuhtpdfx.csv')\nconcrete_data.head()\n", "intent": "Let's download the data and read it into a <em>pandas</em> dataframe.\n"}
{"snippet": "df = pd.DataFrame({'A':[11, 33, 22],'B':[3, 3, 2]})\n", "intent": "Try to convert the following Pandas Dataframe  to a tensor\n"}
{"snippet": "df = pd.read_csv('iris.csv', skipinitialspace=True)\ndf.head()\n", "intent": "**Prepping the Model**\n"}
{"snippet": "data = pd.read_csv('data/heart.csv')\ndata.head()\n", "intent": "Read in the heart disease dataset using `pandas`.\n"}
{"snippet": "dummy_e = DummyEncoder()\ncp_dummy = dummy_e.fit_transform(cp_imp)\ncp_dummy.shape\n", "intent": "Now we can use this estimator just as the `OneHotEncoder` estimator:\n"}
{"snippet": "pipeline_cont = Pipeline([('impute', SimpleImputer(missing_values=np.nan, \n                                     strategy='mean', \n                                     copy=True)), \n                          ('norm', StandardScaler())])\noldpeak_out = pipeline_cont.fit_transform(data[['oldpeak']])\noldpeak_out.mean(), oldpeak_out.std()\n", "intent": "Now let's create a second pipeline for the continuous variables using the imputation and normalization estimators.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X, y, test_size=0.25)\nprint('XTrain shape:', X_train_raw.shape, 'YTrain shape:', y_train_raw.shape, '\\n')\nprint('XTest shape:', X_test_raw.shape, 'YTest shape:', y_test_raw.shape)\n", "intent": "Now we can use the train_test_split function to split the entire dataset into 75% `train` data and 25% `test` data:\n"}
{"snippet": "age_scaler = StandardScaler()\n", "intent": "We'll also need to create a `StandardScaler` to scale the y (age) back and forth:\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\n", "intent": "Import the MNIST handwritten digits dataset from sklearn.datasets and perform a multinomial logistic regression classifier like the one above.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris=load_iris()\ndf=pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n                     columns=iris['feature_names'] + ['target'])\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "X=pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\ny= iris['target']\n", "intent": "Define the \"x\" and \"y\" variables. *Remember*, y is our classifer, and x is our attributes.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris=load_iris()\ndf-pd.DataFrame(iris)\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "df = pd.read_csv('census_data.csv')\ndf.head()\n", "intent": "**Prepping the Data/Model**\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['over_200k', 'under_200k'],columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "df.pivot_table(index = \"Group\", aggfunc = (lambda x: len([i for i in x if i > 100])))\n", "intent": "1. Reference a column which has a certain value, then change the value of the corresponding row in a separate column\n"}
{"snippet": "af = pd.DataFrame(features)\n", "intent": "CONVERT & MERGE TO ONE DATAFRAME\n"}
{"snippet": "X = pd.DataFrame(iris.data)\ny = pd.DataFrame(iris.target)\nX.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\ny.columns = ['Targets']\n", "intent": "Define the \"x\" and \"y\" variables. *Remember*, y is our classifer, and x is our attributes.\n"}
{"snippet": "votes = pd.read_csv(votes_file, index_col=0)\nvotes.head()\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "pca1 = PCA(n_components=5)\npca1.fit(X_standard)\npca1.components_\n", "intent": "Now, conduct a PCA using scikit learn\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "x_standard = StandardScaler().fit_transform(X)\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "pca = PCA(n_components=5)\npca.fit(x_standard)\nprint pca.components_\n", "intent": "Finally, conduct the PCA - use the results above to guide your selection of n components\n"}
{"snippet": "votes = pd.read_csv(votes_file)\nvotes.head()\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "from sklearn.datasets import make_regression\nimport numpy as np\nnp.random.seed(99)\nX, y, coef = make_regression(30,10, 10, bias=1, noise=2, coef=True)\ncoef\n", "intent": "We'll create a synthetic data set using the code below. (Read the documentation for `make_regression` to see what is going on).\n"}
{"snippet": "onehot_enc = OneHotEncoder()\nX_1hot = onehot_enc.fit_transform(X_le).toarray()\nX_1hot.shape\n", "intent": "**OneHotEncoder() accepts multidimensional array, but it returns sparse matrix.  Use .toarray() to obtain just the array**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(stop_words=updated_stop_words)\n", "intent": "With updated stop words list, pass the new list to the CountVectorizer constructor:\n"}
{"snippet": "encoder = LabelBinarizer(sparse_output=True)\n", "intent": "This returns a regular/dense matrix.  To return a sparse matrix, just pass ```sparse_output=True``` to the constructor:\n"}
{"snippet": "df['minmax'] = preproc.minmax_scale(df['n_tokens_content'])\n", "intent": "Min-Max scaling squeezes (or stretches) values to be within a range of values between 0 and 1\n"}
{"snippet": "X_partno_labelencoded = enc_label.fit_transform(df_balanced.Fail_Short_PartNo.values)\n", "intent": "Label encode it first:\n"}
{"snippet": "X_partno_onehot = enc_onehot.fit_transform(X_partno_labelencoded.reshape(-1,1))\n", "intent": "Then onehot encode it:\n"}
{"snippet": "X_complaint_counts = count_vect.fit_transform(df_balanced.Customer_Contention_Text.values)\nX_complaint_counts\n", "intent": "First, CountVectorize() it:\n"}
{"snippet": "X_complaint_tfidf = tfidf_transformer.fit_transform(X_complaint_counts)\nX_complaint_tfidf.shape\n", "intent": "Then, tfidf tranform it:\n"}
{"snippet": "X_partno_labelencoded = enc_label.fit_transform(data['Part5'])\n", "intent": "Label encode it first:\n"}
{"snippet": "fig = sns.jointplot(x=0, y=1, data=pd.DataFrame(g.samples2), marker='.')\nfig.ax_joint.collections[0].set_alpha(0.1);\nfig.set_axis_labels('$X_1$', '$X_2$');\n", "intent": "As we can see, we have managed to get 10000 non-autocorrelated samples.\nWe can check the distribution of samples as follows.\n"}
{"snippet": "X_partno_onehot_categorical = enc_labelbinarizer.fit_transform(part5)\n", "intent": "Alternatively, you can use LabelBinarizer to label encode and one hot encode all in one step.\n"}
{"snippet": "X_complaint_counts = count_vect.fit_transform(data['Customer_Complaint'])\nX_complaint_counts\n", "intent": "First, CountVectorize() it:\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\n", "intent": "We will now apply the IsolationForest algorithm to spot digits written in an unconventional way.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(X)\nX = sc.transform(X)\n", "intent": "Normalize data: the unit of measurement might differ so lets normalize the data before building the model\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n", "intent": "Split data into train and test. When ever we are using radom function its advised to use a seed to ensure the reproducibility of the results.\n"}
{"snippet": "import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import datasets\nimport numpy as np\niris = datasets.load_iris()\n", "intent": "The Iris dataset comes as part of the scikit-learn dataset package which contains some of the populare datasets of machine learning literature.\n"}
{"snippet": "df = pd.read_csv('Data/Grade_Set_2.csv')\nprint df\ndf.plot(kind='scatter', x='Hours_Studied', y='Test_Grade', title='Grade vs Hours Studied')\nprint(\"Correlation Matrix: \")\ndf.corr()\n", "intent": "It is a form higher order linear regression modeled between dependent and independent variables as an nth degree polynomial.\n"}
{"snippet": "from sklearn.datasets import make_classification\nX, y = make_classification(100, 2, 2, 0, weights=[.5, .5], random_state=0) \nclf = SVC(kernel='linear', random_state=0)\nclf.fit(X, y)\n", "intent": "Let's consider a two class example to keep things simple\n"}
{"snippet": "df = pd.read_csv('Data/TS.csv')\nts = pd.Series(list(df['Sales']), index=pd.to_datetime(df['Month'],format='%Y-%m'))\n", "intent": "Let's predict sales data using ARIMA\n"}
{"snippet": "survey = pd.read_csv(\"survey.csv\")\nages = np.array(survey['age'])\nknow_lgbtq_rate = survey['knowlgbtq'] / survey['numr']\nknow_lgbtq_rate = np.array(know_lgbtq_rate)\nresp = survey['numr']\n", "intent": "**Solution**\nWe'll start by importing our count data into pandas and doing some basic EDA.\n"}
{"snippet": "def Find_Optimal_Cutoff(target, predicted):\n    fpr, tpr, threshold = metrics.roc_curve(target, predicted)\n    i = np.arange(len(tpr)) \n    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n    roc_t = roc.ix[(roc.tf-0).abs().argsort()[:1]]\n    return list(roc_t['threshold']) \n", "intent": "To simplify finding optimal probability threshold and bring in re-usability, I have made a function to find the optimal probability cutoff point.\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=3, include_bias=False)\nX2 = poly.fit_transform(X)\nX2 \n", "intent": "Let's add some synthetic features that are higher-order powers of X:\n"}
{"snippet": "hs_inv = ohe.inverse_transform(hs_train_transformed)\nhs_inv\n", "intent": "We can verify all values by inverting the entire transformed array.\n"}
{"snippet": "hs_train_transformed = ohe.fit_transform(hs_train_imputed)\nhs_train_transformed\n", "intent": "From here we can then encode as we did previously.\n"}
{"snippet": "si_step = ('si', SimpleImputer(strategy='constant', fill_value='MISSING'))\nohe_step = ('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))\nsteps = [si_step, ohe_step]\npipe = Pipeline(steps)\nhs_train = train[['HouseStyle']].copy()\nhs_train.iloc[0, 0] = np.nan\nhs_transformed = pipe.fit_transform(hs_train)\nhs_transformed.shape\n", "intent": "Each step is a two-item tuple consisting of a string that labels the step and the instantiated estimator.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nnum_si_step = ('si', SimpleImputer(strategy='median'))\nnum_ss_step = ('ss', StandardScaler())\nnum_steps = [num_si_step, num_ss_step]\nnum_pipe = Pipeline(num_steps)\nnum_transformers = [('num', num_pipe, num_cols)]\nct = ColumnTransformer(transformers=num_transformers)\nX_num_transformed = ct.fit_transform(train)\nX_num_transformed.shape\n", "intent": "Once we have our numeric column names, we can use the `ColumnTransformer` again.\n"}
{"snippet": "cols = ['density', 'residual sugar', 'total sulfur dioxide', 'fixed acidity']\nsubset_df = wines[cols]\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nscaled_df = ss.fit_transform(subset_df)\nscaled_df = pd.DataFrame(scaled_df, columns=cols)\nfinal_df = pd.concat([scaled_df, wines['wine_type']], axis=1)\nfinal_df.head()\n", "intent": "Another way of visualizing multivariate data for multiple attributes together is to use ***parallel coordinates***.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('data/bikeshare.csv')\n", "intent": "- from Kaggle ([Description](https://www.kaggle.com/c/bike-sharing-demand/data))\n"}
{"snippet": "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\n", "intent": "- Advertising dataset from **\"An Introduction to Statistical Learning\"**\n- http://www-bcf.usc.edu/~gareth/ISL/\n"}
{"snippet": "df = pd.read_csv(os.path.join('..', '..', 'dataset', 'dataset-ucla-admissions.csv'))\ndf.dropna(inplace = True)\ndef question_3(df):\n    print '%.1f (%.1f)' % (df['gpa'].mean(), df['gpa'].std())\n    print '%.1f (%.1f)' % (df['gre'].mean(), df['gre'].std())\n    for i in range(1, 5):\n        print '%s' % (df[df.prestige == i].shape[0])\nquestion_3(df[df.admit == 0])\nprint\nquestion_3(df[df.admit == 1])\n", "intent": "> \n>\n> Provide a table that explains the data by admission status.\n"}
{"snippet": "import pandas as pd\nurl = 'data/titanic.csv'\ntitanic = pd.read_csv(url)\ntitanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\ntitanic.Age.fillna(titanic.Age.median(), inplace=True)\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\nembarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\ntitanic = pd.concat([titanic, embarked_dummies], axis=1)\ntitanic.head()\n", "intent": "- Kaggle knowledge competition: https://www.kaggle.com/c/titanic\n"}
{"snippet": "from keras.datasets import mnist\nimport h5py as h5py\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint(\"The MNIST database has a training set of %d examples.\" % len(X_train))\nprint(\"The MNIST database has a test set of %d examples.\" % len(X_test))\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the MNIST database.\n"}
{"snippet": "df.to_csv('processed_bank.csv')\n", "intent": "Our data looks good, so we will now save our processed data so we have it ready for building our model in the next chapter.\n"}
{"snippet": "from sklearn import preprocessing \nle = preprocessing.LabelEncoder()\n", "intent": "Data Label Encoding\n"}
{"snippet": "file_name='https://ibm.box.com/shared/static/worxgoibhvr53yccwmwx71iuly491h6u.csv'\ndf=pd.read_csv(file_name)\ndf.head()\n", "intent": " Let's load the dataset using Pandas:\n"}
{"snippet": "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=0.33,random_state=0)\n", "intent": "We split the data into a training and testing set: \n"}
{"snippet": "Z_test=pr.fit_transform(X_test)\nZ_train=pr.fit_transform(X_train)\n", "intent": "We transform the data:\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"data.csv\")\n", "intent": "duplicate in dataset\n"}
{"snippet": "test_img = mpimg.imread('./test_images/test6.jpg')\nprint (test_img.shape[1])\n", "intent": "Applying classifer to detect cars in image\n"}
{"snippet": "columns_X = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\nX = df[columns_X]\nscaler = preprocessing.MinMaxScaler().fit(X)\nX = scaler.transform(X)\n", "intent": "- (http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n"}
{"snippet": "filtered_train_data = train_df[importance_df1.feature]\nfiltered_train_data = train_df[importance_df1.feature]\nX_train, X_dev, y_train, y_dev = train_test_split(train_df, target, test_size=0.30, random_state=42)\nX_train_filtered, X_dev_filtered, y_train_filtered, y_dev_filtered = train_test_split(filtered_train_data, target, test_size=0.30, random_state=42)\n", "intent": "Looking at the decision tree, we also do not see any of the variables that Lasso and Ridge eliminated in the important variables.\n"}
{"snippet": "new_covar = pd.DataFrame(X_transformed).cov()\nnew_covar.head().round(3)\n", "intent": "What does the covariance look like now?\n"}
{"snippet": "pca = PCA().fit(X_train)\n", "intent": "Fit PCA to the training data\n"}
{"snippet": "X_reduced = PCA(n_components=20).fit_transform(X_train)\n", "intent": "Create a new PCA object that is explicity a dimension reducer\n"}
{"snippet": "X_reduced = PCA().fit_transform(X_train)[:, :20]\n", "intent": "Transform the data using the original PCA, but keep only the first num_comp columns\n"}
{"snippet": "X = pd.DataFrame(np.random.rand(100, 3))\n", "intent": "Generate a 100x3 data frame of random numbers\n"}
{"snippet": "X = pd.DataFrame(np.random.rand(100,3))\n", "intent": "Generate a 100x3 data frame of random numbers\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(df[predictors], df[target], random_state=2)\n", "intent": "This time, let's separate X from y\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndv = DictVectorizer(sparse=False) \nX_train = pd.DataFrame(\n    dv.fit_transform(X_train.to_dict(orient='records')),\n    columns = dv.feature_names_\n    )\nX_test = pd.DataFrame(\n    dv.transform(X_test.to_dict(orient='records')),\n    columns = dv.feature_names_\n)\n", "intent": "fetaure transformation \n"}
{"snippet": "x_df = pd.DataFrame(index = df.index)\nnp.random.seed(seed = 0)\nfor i in range(100):\n    x = 'X{}'.format(i)\n    x_df[x] = np.random.random(df.shape[0])\nformula = 'SalePrice ~ 0 + IsAStudio + Beds + Baths + Size + LotSize + BuiltInYear + '\nformula += ' + '.join(x_df.columns.values)\n", "intent": "> Let's now add some artificial noise:\n"}
{"snippet": "of_df = pd.read_csv(\"../datasets/old-faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "bike = pd.read_csv(('data/Bike-Sharing-Dataset/day.csv'))\nbike['dteday'] = pd.to_datetime(bike['dteday'])\nbike = bike.drop(0)\nbike.head(4)\n", "intent": "We'll be using the same bike sharing data as last week!\n"}
{"snippet": "un_cv = CountVectorizer(max_df=.95, min_df=2, stop_words='english')\n", "intent": "**Question 3.3:** Now, implement a model using `scikit-learn`. Again, follow similar steps from the [second section](\n"}
{"snippet": "tfidf_vectorizer_1832 = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\ntfidf_1832 = tfidf_vectorizer_1832.fit_transform(transcripts_1832)\ntfidf_feature_names_1832 = tfidf_vectorizer_1832.get_feature_names()\n", "intent": "**EXERCISE:** Create the TfidfVectorizer, fit_transform the data, and get the feature names for 1832.\n"}
{"snippet": "nmf_1832 = NMF(n_components=num_topics, random_state=1, init='nndsvd').fit(tfidf_1832)\n", "intent": "**EXERCISE:** Run NMF using `num_topics` for the number of components on the data from 1832.\n"}
{"snippet": "import numpy as np\nnp.random.seed(23) \nimport keras\nfrom keras.datasets import cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n", "intent": "---\nIn this notebook, we train a CNN to classify images from the CIFAR-10 database.\n"}
{"snippet": "target_sleep = 8 * 60 \nlabels = pd.read_csv(\"output.csv\") \nlabels = labels.drop([\"Date\", \"Minutes Awake\", \"Number of Awakenings\", \"Time in Bed\"], axis=1) \nlabels[\"WELL\"] = (labels['Minutes Asleep'] >= target_sleep).astype(int)\nlabels.loc[:, (\"BAD\")] = labels[\"WELL\"] == 0           \nlabels.loc[:, (\"BAD\")] = labels[\"BAD\"].astype(int)    \nlabels = labels.drop([\"Minutes Asleep\"], axis=1) \nlabels \n", "intent": "The dataframe now only has the features. Let's introduce the labels.\n"}
{"snippet": "data = pd.read_csv('monthly-milk-production.csv',index_col = 'Month')\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "data = pd.read_csv('titanic_train.csv')\n", "intent": "Pour charger le fichier csv dans un DataFrame pandas:\n"}
{"snippet": "np.random.seed(0)\ndf = pd.DataFrame(index = range(100))\ndf['x'] = np.random.uniform(0, 1, size = df.shape[0])\ndf['Noise'] = np.random.normal(size = df.shape[0])\ndf['y'] = df.x.apply(f) * (1 + .5 * df.Noise)\n", "intent": "- (http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n"}
{"snippet": "with open('enronemail_1h.txt','rb') as f:\n    data=pd.read_csv(f, sep='\\t', header=None, na_filter=True)\ndf_columns = [\"id\", \"spamflag\", \"subject\", \"body\"]\ndata.columns = df_columns\ndata['subject_body'] = data[\"subject\"] + data[\"body\"]\ndataClean = data.dropna()\ndataClean\n", "intent": "There are some lines with \"NA\" as the body and these should be removed to properly train the model\n"}
{"snippet": "data_feat = pd.DataFrame(scaled_features, columns=data.columns[0:-1])\ndata_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "x = data_feat\ny = data['TARGET CLASS']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n", "intent": "**Use train_test_split to split your data into a training set and a testing set.**\n"}
{"snippet": "diabetes = datasets.load_diabetes()\nX = diabetes.data\ny = diabetes.target.reshape(-1,1)\n", "intent": "We will be using the well known **diabetes** dataset taken from Sklearn's built in datasets\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n", "intent": "- Encode text into frequencies of vocabulary terms\n"}
{"snippet": "style_image = scipy.misc.imread(\"images/vango.jpg\")\nimshow(style_image)\n", "intent": "For our running example, we will use the following style image: \n"}
{"snippet": "content_image = scipy.misc.imread(\"images/chunge.jpg\")\ncontent_image = reshape_and_normalize_image(content_image)\n", "intent": "Let's load, reshape, and normalize our \"content\" image (the Louvre museum picture):\n"}
{"snippet": "style_image = scipy.misc.imread(\"images/vango.jpg\")\nstyle_image = reshape_and_normalize_image(style_image)\n", "intent": "Let's load, reshape and normalize our \"style\" image (Claude Monet's painting):\n"}
{"snippet": "data = pd.read_csv('./monthly-milk-production.csv').set_index('Month')\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape \n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "df_loan = read_csv('data/loan.csv')\n", "intent": "Let's start by loading the raw data.\n"}
{"snippet": "df_loan.describe(include='all').T.join(DataFrame(df_loan.dtypes, columns=['dtype']))\n", "intent": "Now let's profile the dataset. \n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\ndf['sub_grade_enc'] = enc.fit_transform(df['sub_grade'])\nplot_scatter(df_loan, 'int_rate', 'sub_grade_enc', sample=df_loan.shape[0] / 50)\n", "intent": "Now let's encode **sub_grade** and look at its correlation with **int_rate**.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\ndf = pd.read_csv(data_path, delimiter = ',')\ndata_B = os.path.join(os.getcwd(), 'datasets', 'train_20news_partB.csv')\ndf_2 = pd.read_csv(data_B, delimiter = ',')\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "np.random.seed(186)\nn = 20\nx1 = np.random.uniform(0,10,n)\nx2 = np.random.uniform(0,10,n)\ny = 7 + 1.3 * x1 + 2.5 * x2 + np.random.normal(0,4,n)\ndata = pd.DataFrame({'x1':x1,'x2':x2, 'y':y})\ndata.head()\n", "intent": "We will now generate a dataset to illustrate how multiple regression works. \n"}
{"snippet": "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.2, random_state=1)\nX_val, X_test, y_val, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=1)\nprint(y_train.shape)\nprint(y_val.shape)\nprint(y_test.shape)\n", "intent": "We split the dataset into subsets for using in training, validation, and testing. We will use an 80/10/10 split.\n"}
{"snippet": "sX_train, sX_holdout, sy_train, sy_holdout = train_test_split(X_scaled, y, test_size=0.2, random_state=1)\nsX_val, sX_test, sy_val, sy_test = train_test_split(sX_holdout, sy_holdout, test_size=0.5, random_state=1)\nprint(sy_train.shape)\nprint(sy_val.shape)\nprint(sy_test.shape)\n", "intent": "We need to create training, validation, and testing sets for our new scaled dataset. \n"}
{"snippet": "tfidf = TfidfVectorizer()\ndtm = tfidf.fit_transform(yelp.text)\nfeatures = tfidf.get_feature_names()\n", "intent": "Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF!\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\nprint(y_train.shape)\nprint(y_test.shape)\n", "intent": "You will be asked to create a train/test split. The size of the test set will be provided. \n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'spambase_test.csv')\nspambase_test = pd.read_csv(data_path, delimiter = ',')\nspambase_test.head()\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure called spambase_test\n"}
{"snippet": "def loadDataSet(name):\n    data_path = os.path.join(os.getcwd(), 'datasets', name + '.csv')\n    return pd.read_csv(data_path, delimiter = ',')\npartA = loadDataSet('train_20news_partA')\npartB = loadDataSet('train_20news_partB')\nprint(partA.head())\npartB.head()\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "def labelEncodeColumn(name):\n    le = LabelEncoder()\n    le.fit(auto_full[name]) \n    return le.transform(auto_full_edit[name])\n", "intent": "http://scikit-learn.org/0.17/modules/generated/sklearn.preprocessing.LabelEncoder.html\n"}
{"snippet": "print \"shape of auto_full_edit before one hot encoding\"\nprint auto_full_edit.shape\nencoder = OneHotEncoder(categorical_features=booleanMask)\ntransformation = encoder.fit_transform(auto_full_edit)\nprint encoder.n_values_  \nprint encoder.feature_indices_ \nprint encoder.active_features_ \nautoFullPreprocessed = transformation.toarray()\nautoFullPreprocessed.shape\n", "intent": "http://scikit-learn.org/0.17/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n"}
{"snippet": "def loadDataSet(name):\n    data_path = os.path.join(os.getcwd(), 'datasets', name + '.csv')\n    return pd.read_csv(data_path, delimiter = ',')\npartA = loadDataSet('train_20news_partA')\npartB = loadDataSet('train_20news_partB')\nprint(partA.head())\npartB.head()\n", "intent": "<span style=\"color:red\">OK\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size = 0.8, test_size=0.2, random_state=0\n)\nmodel = LinearRegression(normalize=True)\nmodel.fit(X_train, y_train)\n", "intent": "<font color=\"red\"> Good comparion</font>\n"}
{"snippet": "import pandas as pd\nfrom copy import deepcopy\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nsample_submission = pd.read_csv(\"sample_submission.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "encoder = OneHotEncoder()\n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "vect = CountVectorizer()\n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "df = pd.read_csv(\".data/2007.csv\")\ndf\n", "intent": "First, let's read the raw data for 2007 into a Pandas dataframe.\n"}
{"snippet": "ss = StandardScaler()\n", "intent": "**Create a StandardScaler() object called scaler.**\n"}
{"snippet": "X = pd.DataFrame(X, columns = bank_data.drop('Class', axis = 1).columns)\nX.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data = pd.read_csv('College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "scaled_df = pd.DataFrame(scaler.transform(df[features]), columns=features)\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "train_x, test_x, train_y, test_y = train_test_split(yelp_class['text'], yelp_class['stars'],\n                                                   test_size = 0.3, random_state = 101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "Ad_data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\nAd_data.head()\n", "intent": "An Introduction to Statistical Learning:  \nhttp://www-bcf.usc.edu/~gareth/ISL\n"}
{"snippet": "iris = load_iris()\nX= iris.data\ny=iris.target\nX_train, X_test,y_train,y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n", "intent": "b- Split\t the\t dataset\t into\t testing\t and\t training\t sets\t with\t the\t following\t parameters:\t\ntest_size=0.4,\trandom_state=1\t\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\n", "intent": "c- Split\t the\t dataset\t into\t testing\t and\t training\t sets\t with\t the\t following\t parameters:\t\ntest_size=0.2,\trandom_state=3.\t\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n", "intent": "[Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) of Decision Tree Regressor\n"}
{"snippet": "weather_df = pd.read_csv('weather.csv')\ndel weather_df[\"Unnamed: 0\"]\nweather_df.head()\n", "intent": "What weather conditions are associated with flight delays leaving NYC? Use graphics to explore.\n"}
{"snippet": "from keras.datasets import mnist\n((X_train, y_train), (X_test, y_test)) = mnist.load_data()\n", "intent": "  * see [this link](http://yann.lecun.com/exdb/mnist/)\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "  * see [this link](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "  * see [this link](https://archive.ics.uci.edu/ml/datasets/iris)\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "- Boston Housing Dataset\n"}
{"snippet": "scaled_feature = scaler.fit_transform(data.drop('Class',axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "heart_data = pd.read_csv(\"heart.csv\",sep=\" \")\n", "intent": "Load the data set \"heart.csv\" [1] in a variable called `heart_data`.\n"}
{"snippet": "data_features_int_encoding = pd.DataFrame(data_features_int_encoding.tolist())\ndata_features_int_encoding.head()\n", "intent": "Next we put these feature vectors back in a Pandas DataFrame as follows:\n"}
{"snippet": "import numpy as np\nfrom pandas import read_csv\nurl = 'https://raw.githubusercontent.com/szorowi1/qmss2017/master/module3/random/random.csv'\ndata = read_csv(url)\ndata.head(3)\n", "intent": "Pandas can read tabulated data directly from URLs. \n"}
{"snippet": "diabetes_df = pd.DataFrame(diabetes.data)\n", "intent": "Sometimes it is convenient to convert a Numpy array to a Pandas DataFrame:\n"}
{"snippet": "data_norm_poly_norm = pd.DataFrame(MinMaxScaler().fit_transform(data_norm_poly),columns=data_norm_poly.columns.values)\n", "intent": "Perform the appropriate action!\n"}
{"snippet": "cols = data_features_bin.columns.values\ntmp = pd.DataFrame()\ntmp['feature'] = cols\ntmp['coefficients'] = model.coef_[0]\n", "intent": "Can you plot the model paramters of the fitted logisti regression model?\n"}
{"snippet": "X_pca_stand = pca.fit_transform(X_stand)\n", "intent": "Let's now run another PCA on our standardized data this time:\n"}
{"snippet": "Y_mds = mds.fit_transform(X)\n", "intent": "Now we run the MDS: this is a big dataset, it will take some time...\n"}
{"snippet": "yeast = pd.read_table('yeast.csv', sep = ' ', header=0)\n", "intent": "Let's see if the MDS does a better job on Yeast gene expression data.\n"}
{"snippet": "cols = data_b.columns.values\nscaler = StandardScaler()\nscaler.fit(data_b)\ndata_b_norm = pd.DataFrame(scaler.transform(data_b),columns=cols)\n", "intent": "- Standardize the data set and re-run the code.\n"}
{"snippet": "iris = datasets.load_iris()\nirisdf = pd.DataFrame(iris.data, columns=iris.feature_names)\nirisdf['target'] = iris.target\n", "intent": "* What are the features?\n* What is the target variable?\n* How many labels does the target variable have? Is it binary?\n"}
{"snippet": "pd.DataFrame(irisdf.corr()).sort_values('target', ascending=False)\n", "intent": "*  Which features have the highest correlation with the target variable? Should you start with using the highest correlated feature as a predictor? \n"}
{"snippet": "import numpy as np\nfrom pandas import read_csv\nurl = 'https://raw.githubusercontent.com/szorowi1/qmss2017/master/module3/random/random.csv'\ndata = read_csv(url)\ndata = data.set_index('y')\ndata.describe().round(3)\n", "intent": "Data coding describes processes related to preprocessing of the data, such as rescaling and recoding data for use in fitting models.\n"}
{"snippet": "json.loads('{\"title\":\"IBM Sees Holographic Calls Air Breath\"}').get('title', '')\ndata =  pd.read_csv(\"../../assets/dataset/stumbleupon.tsv\", sep=\"\\t\",encoding=\"utf-8\")\ndata['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\ndata['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\ndata.head()\n", "intent": "* Located at `\"../../assets/dataset/stumbleupon.tsv\"`\n* Encodinge is `\"utf-8\"`\n"}
{"snippet": "titles = data['title'].fillna('')\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features = 1000, \n                             ngram_range=(1, 2), \n                             stop_words='english',\n                             binary=True)\nvectorizer.fit(titles)\nX = vectorizer.transform(titles)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MaxAbsScaler\npipeline = Pipeline([\n        ('features', vectorizer),\n        ('scaling', MaxAbsScaler()),\n        ('model', model)   \n    ])\npipeline.fit(X_train, y_train)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=15)\nparameters = {'C': [0.001, 0.1, 1, 10, 100]}\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg, param_grid=parameters)\nlogreg_cv.fit(X_train, y_train)\nprint(logreg_cv.best_params_)\n", "intent": "Lasso minimized all of the coefficents down to zero except for last_trip_date making this an important predictive factor for retained users.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",header = None)\ndf.head(10)\n", "intent": "** Remarks: Collecting data phase **\n"}
{"snippet": "data_path = \"~/dsi-sf-7-materials/datasets/churn.csv\"\ndf = pd.read_csv(data_path)\n", "intent": "Perform the same cleaning and scaling in preperation for modeling buidling.\n"}
{"snippet": "x_ss = StandardScaler().fit_transform(xTrain)\nx_mas = MaxAbsScaler().fit_transform(x_ss)\n", "intent": "Bengio paper states that the data's mean should be centered aroud zero and scaled between [-1,1]\n"}
{"snippet": "pca = PCA(n_components=30)\n", "intent": "We are going to take the first 30 principal component vectors\n"}
{"snippet": "count_vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\ntermFreq = count_vectorizer.fit_transform(US_corpus)\n", "intent": "Due to the way in which LDA works, we have to vectorize our corpus into a [Bag-of-Words model](https://en.wikipedia.org/wiki/Bag-of-words_model).\n"}
{"snippet": "fwe_cols = SelectFwe(alpha=0.05).fit(data,data.index).get_support()\nfwe_cols\n", "intent": "Scikit-Learn attributes can be **chained.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nX = df.as_matrix(columns=[\"gre\", \"gpa\", \"rank\"])\ny = df.as_matrix(columns=[\"admit\"])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n", "intent": "<h1>Build the model</h1>\n<p>Create train test splits for your data.  Use the training data to .</p>\n"}
{"snippet": "datafile = 'Rpolar'\ndt = pd.read_csv('working_data{}{}.csv'.format(os.sep,datafile))\ndt = dt.iloc[:,1:] \ny = dt.iloc[:,0]\nX = dt.iloc[:,1:]\nprint(X[0:5])\nprint(y[0:5])\n", "intent": "We use pandas to read the data. Don't worry, the use of pandas here is very basic, which you can easily grasp.\n"}
{"snippet": "os.chdir(os.pardir)\ndatafile = 'Rpolar'\ndt = pd.read_csv('working_data{}{}.csv'.format(os.sep,datafile))\ndt = dt.iloc[:,1:] \ny = dt.iloc[:,0]\nX = dt.iloc[:,1:]\nprint(X[0:5])\nprint(y[0:5])\n", "intent": "We use pandas to read the data. Don't worry, the use of pandas here is very basic, which you can easily grasp.\n"}
{"snippet": "def references_organization(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == 'ORG' for word in parsed])\ndata['references_organization'] = data['title'].fillna(u'').map(references_organization)\n", "intent": "Let's see if we can find organizations in our page titles.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=25)\n", "intent": "Out of curiosity, I decided to run a train_test_split model and see the results.  They were, not surprisingly, immensely overfit and unreliable.\n"}
{"snippet": "df3 = joined_tables.dropna()\ndf3.to_csv('df3.csv')\n", "intent": "Query the database for our intial data\n"}
{"snippet": "xStand = preprocessing.StandardScaler().fit_transform(x)\n", "intent": "I then want to standardize my x variable (I chose to do this using StandardScalar):\n"}
{"snippet": "PCA_A = PCA(n_components=5)\nY = PCA_A.fit_transform(xStand)\n", "intent": "Now we can conduct our PCA analysis:\n"}
{"snippet": "Ydf = pd.DataFrame(Y, columns=[\"PC1\", \"PC2\", \"PCA3\", \"PCA4\", \"PCA5\"])\n", "intent": "Creating a dataframe from the PCA analysis will allow us to visualize our results:\n"}
{"snippet": "import os\nfrom pandas import read_table, concat\nred = read_table(os.path.join('wine','winequality-red.csv'), sep=';')\nred['color'] = 'red'\nwhite = read_table(os.path.join('wine','winequality-white.csv'), sep=';')\nwhite['color'] = 'white'\nwines = concat([red,white])\nwines.head(5)\n", "intent": "**Hint:** Remember to add a new column denoating the red and white wines. Also, the datasets are semi-colon-separated.\n"}
{"snippet": "QuasarCandidates = pd.read_csv('QuasarCandidatesData.csv')\n", "intent": "We will merge these probabilities into the QuasarCandidate.csv file.\n"}
{"snippet": "map_characters = {0: 'Interphase', 1: 'Mitosis'}\ndict_characters=map_characters\ndf = pd.DataFrame()\ndf[\"labels\"]=y_train2\nlab = df['labels']\ndist = lab.value_counts()\nsns.countplot(lab)\nprint(dict_characters)\n", "intent": "Note class size imbalance\n"}
{"snippet": "map_characters = {0: 'G1/G2/S', 1: 'Anaphase', 2: 'Metaphase',3:'Prophase',4:'Telophase'}\ndict_characters=map_characters\ndf = pd.DataFrame()\ndf[\"labels\"]=y_train\nlab = df['labels']\ndist = lab.value_counts()\nsns.countplot(lab)\nprint(dict_characters)\n", "intent": "Note class size imbalance\n"}
{"snippet": "dataset = read_csv('../input/train.csv')\ndataset=dataset[['Id','Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age','Outcome']]\ndataset.head(10)\n", "intent": "*Step 3: Inspect and Clean Data*\n"}
{"snippet": "model = DecisionTreeClassifier(max_depth=2,min_samples_leaf=2)\nX_train, X_test, y_train, y_test = train_test_split(trainingFeatures2, trainingLabels, test_size=0.2, random_state=1)\nmodel.fit(X_train, y_train)\ncolumns = trainingFeatures2.columns\nfeature_names = trainingFeatures2.columns.values\ncoefficients = model.feature_importances_.reshape(trainingFeatures2.columns.shape[0], 1)\nabsCoefficients = abs(coefficients)\nfullList = pd.concat((pd.DataFrame(columns, columns = ['Feature']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\nprint('\\nFeature Importance:\\n\\n',fullList,'\\n')\nplot_decision_tree(model,feature_names)\n", "intent": "*Step 5: Evaluate Model*\n"}
{"snippet": "list_1=[]\nfor i in range(28000):\n    i=i+1\n    list_1.append(i)\nlist_2 = [7]*28000\nkerasmnist = os.path.join('.', 'working/kerasmnist')\nos.makedirs(kerasmnist, exist_ok = True)\ndf = pd.DataFrame(data={\"ImageId\": list_1, \"Label\": list_2})\ndf = df.to_csv(\"./working/kerasmnist/mnist_dummy_submission.csv\", sep=',',index=False)\n", "intent": "Submit to a competition on Kaggle\n"}
{"snippet": "from keras.datasets import mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nkerasmnist = os.path.join('.', 'working/kerasmnist')\nos.makedirs(kerasmnist, exist_ok = True)\nnp.savez(\"working/kerasmnist/MNIST_X_train\", x_train)\nnp.savez(\"working/kerasmnist/MNIST_Y_train\", y_train)\nnp.savez(\"working/kerasmnist/MNIST_X_test\", x_test)\nnp.savez(\"working/kerasmnist/MNIST_Y_test\", y_test)\n", "intent": "Add a new file to the dataset and create a new dataset version\n"}
{"snippet": "import seaborn as sns\ndf = pd.DataFrame()\ndf[\"labels\"]=y_train\nlab = df['labels']\ndist = lab.value_counts()\nsns.countplot(lab)\nprint(dict_characters)\n", "intent": "*Step Five: Describe Augmented Dataset*\n"}
{"snippet": "train_df = pd.read_json('../input/train.json')\ntest_df = pd.read_json('../input/test.json')\ntrain=train_df\ntrain.head(15)\n", "intent": "*Step 2: Exploratory Data Analysis*\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncvec = CountVectorizer(lowercase=False)\ncfit = cvec.fit(documents)\nprint(cfit.get_feature_names())\n", "intent": "We will use **CountVectorizer** to convert these documents into their numeric representation.\n"}
{"snippet": "cols = ['Label','Latin Name', 'Common Name','Train Images', 'Validation Images']\nlabels = pd.read_csv(\"../input/10-monkey-species/monkey_labels.txt\", names=cols, skiprows=1)\nlabels\n", "intent": "*Step 2: Load Data*\n"}
{"snippet": "data = pd.read_csv('../input/train.csv')\ntestingData = pd.read_csv('../input/test.csv')\nX = data.drop(\"label\",axis=1).values\ny = data.label.values\ndef describeDataset(features,labels):\n    print(\"\\n'X' shape: %s.\"%(features.shape,))\n    print(\"\\n'y' shape: %s.\"%(labels.shape,))\n    print(\"\\nUnique elements in y: %s\"%(np.unique(y)))\ndescribeDataset(X,y)\n", "intent": "*Step 1.2: Load Data*\n"}
{"snippet": "artist_file = '../input/notorious-big.txt'\nwith open(artist_file) as f: \n    print (f.read(1000))\n", "intent": "Here are the first 1000 characters from the collection of poems by Notorious B.I.G.\n"}
{"snippet": "artist_file = '../input/Lil_Wayne.txt'\nwith open(artist_file) as f: \n    print (f.read(1000))\n", "intent": "Here are the first 1000 characters from the collection of poems by Lil Wayne\n"}
{"snippet": "def markov(text_file):\n\tread = open(text_file, \"r\", encoding='utf-8').read()\n\ttext_model = markovify.NewlineText(read)\n\treturn text_model\n", "intent": "Markov Chain (https://github.com/jsvine/markovify)\n"}
{"snippet": "def split_lyrics_file(text_file):\n\ttext = open(text_file, encoding='utf-8').read()\n\ttext = text.split(\"\\n\")\n\twhile \"\" in text:\n\t\ttext.remove(\"\")\n\treturn text\n", "intent": "Separate each line of the input txt\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\ndata = np.c_[cancer.data, cancer.target]\ncolumns = np.append(cancer.feature_names, [\"target\"])\nsizeMeasurements = pd.DataFrame(data, columns=columns)\nX = sizeMeasurements[sizeMeasurements.columns[:-1]]\ny = sizeMeasurements.target\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)\nprint('\\n Feature Names: \\n\\n', X.columns.values, \"\\n\")\n", "intent": "*Step 2: Load and Describe Data*\n"}
{"snippet": "X_trainScaled, X_testScaled, Y_trainScaled, Y_testScaled = train_test_split(xValuesScaled, yValues, test_size=0.2)\nX_trainScaledPCA, X_testScaledPCA, Y_trainScaledPCA, Y_testScaledPCA = train_test_split(xValuesScaledPCA, yValues, test_size=0.2)\n", "intent": "*Step 10: Evaluate Additional Regressors*\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\ndata = np.c_[cancer.data, cancer.target]\ncolumns = np.append(cancer.feature_names, [\"target\"])\nsizeMeasurements = pd.DataFrame(data, columns=columns)\nX = sizeMeasurements[sizeMeasurements.columns[:-1]]\ny = sizeMeasurements.target\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)\nprint('\\n Column Values: \\n\\n', sizeMeasurements.columns.values, \"\\n\")\n", "intent": "*Step 2: Load Data*\n"}
{"snippet": "from pandas import read_csv\nmetadata = read_csv(os.path.join('nsf','abstracts_metadata.csv'))\nmetadata.Directorate.value_counts()\n", "intent": "Next we will load in the metadata associated with the abstracts.\n"}
{"snippet": "hall_of_fame = pd.read_csv(\"core/HallOfFame.csv\")\nhall_of_fame = hall_of_fame[hall_of_fame.category == \"Player\"]\nprint(hall_of_fame.info())\nhall_of_fame.head()\n", "intent": "From the data, we can see that the create of the hall of fame is from **1939** year.  \n"}
{"snippet": "salary = pd.read_csv(\"./core/Salaries.csv\")\nplayer = player.join(salary.groupby([\"playerID\"])[[\"playerID\",\"salary\"]].mean(),on=\"playerID\")\nprint(salary.info())\nsalary.head()\n", "intent": "---  \nFrom the input data, we see only less than 1/4 that the salary is not null to the player.\n"}
{"snippet": "from keras.datasets import mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n", "intent": "When Determinant is less than zero we get a reflection about y axis but the deformation is same (except the reflection)\n"}
{"snippet": "import numpy as np\ndataPort = pd.read_csv('../../data/Portfolio.csv', index_col = 0)\n", "intent": "First we will read in the `Portfolio` data and import the `numpy` package as we will need some of its functions below.\n"}
{"snippet": "import numpy as np\ndataPort = pd.read_csv('../data/Portfolio.csv', index_col = 0)\n", "intent": "First we will read in the `Portfolio` data and import the `numpy` package as we will need some of its functions below.\n"}
{"snippet": "dataCarseats = pd.read_csv('../../data/Carseats.csv', index_col = 0)\ndataCarseats.dtypes\n", "intent": "First import the data and take a look at the data types.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\ndataCarseats.ShelveLoc = enc.fit_transform(dataCarseats.ShelveLoc)\ndataCarseats.Urban = enc.fit_transform(dataCarseats.Urban)\ndataCarseats.US = enc.fit_transform(dataCarseats.US)\ntrain, test = train_test_split(dataCarseats, test_size = 0.5, random_state = 1)\ntrain_x = train.drop('Sales', axis = 1)\ntest_x = test.drop('Sales', axis = 1)\ntrain_y = train['Sales']\ntest_y = test['Sales']\n", "intent": "The `object` types will need to be converted to dummy variables before splitting into test and training sets.\n"}
{"snippet": "pd.DataFrame({'feature': train_x.columns,\n              'score': rf_carseats.feature_importances_\n             }).sort_values('score', ascending = False)\n", "intent": "In this case, with $m = \\sqrt{p}$, we have a Test MSE of 3.18\n"}
{"snippet": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndataOJ = pd.read_csv('../../data/OJ.csv', index_col = 0)\ndataOJ.dtypes\n", "intent": "a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.\n"}
{"snippet": "X = tfidf_mat\ny = metadata.Directorate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\nmnb = MultinomialNB(alpha=1)\nmnb_fit = mnb.fit(X_train, y_train)\nprint('MultinomialNB train: score = %0.3f' %mnb_fit.score(X_train,y_train))\nprint('MultinomialNB test: score = %0.3f' %mnb_fit.score(X_test,y_test))\n", "intent": "Let's show the most representative tokens now.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(dataHitters, test_size = 63, random_state = 1)\ntrain_x = train.drop('Salary', axis = 1)\ntest_x = test.drop('Salary', axis = 1)\ntrain_y = train['Salary']\ntest_y = test['Salary']\n", "intent": "b) Create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations.\n"}
{"snippet": "gbr_hitters = GradientBoostingRegressor(n_estimators = 1000,\n                                        learning_rate = 0.32,\n                                        random_state = 1)\ngbr_hitters.fit(train_x, train_y)\nhitters_imp = pd.DataFrame({'feature':  train_x.columns\n                            , 'score': gbr_hitters.feature_importances_\n                            }).sort_values('score')\nhitters_imp.sort_values('score', ascending = False)\n", "intent": "f) Which variables appear to be the most important predictors in the boosted model?\n"}
{"snippet": "for col in house.columns[house.isnull().sum() > 0]:\n    if house[col].dtype == object:\n        print '--------------Before Fill--------------'\n        print house[col].value_counts()\n        house[col] = house[col].fillna('None')\n        print '--------------After Fill--------------'\n        print house[col].value_counts()\n", "intent": "Filling the missing value with 'None'\n"}
{"snippet": "house.LotFrontage = house.LotFrontage.fillna(np.mean(house.LotFrontage))\n", "intent": "With 'LotFrontage' we can fill with either the mean or the median value. \n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor i in str_col:\n    fixed[i] = le.fit_transform(fixed[i])\n", "intent": "Using LabelEncoder to convert categorical to numerical\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXs_train = ss.fit_transform(Xtr)\nXs_test = ss.fit_transform(Xte)\ny_train = house_pre2010.SalePrice\ny_test = house_2010.SalePrice\n", "intent": "Standardising with StandardScaler\n"}
{"snippet": "for i in str_cols:\n    renovate[i] = le.fit_transform(renovate[i])\nrenovate.dtypes\n", "intent": "Using LabelEncoder to convert categorical to numerical\n"}
{"snippet": "sal_X = df.iloc[:,0:len(df.columns)-1]\nsal_y = pd.DataFrame(df[\"above_med_sal\"])\n", "intent": "Creating the train test split with the salary data.\n"}
{"snippet": "from sklearn.datasets import load_iris, load_breast_cancer\nml.plots.plot_tree(load_breast_cancer(), class_names=[\"malignant\", \"benign\"])\n", "intent": "Analyzing Decision Trees manually\n- Visualize and find the path that most data takes\n"}
{"snippet": "from pandas import DataFrame\ncolnames = ['Topic%0.2d' %n for n in range(n_topics)]\nloadings = DataFrame(document_loadings, columns=colnames)\ninfo = metadata.merge(loadings, left_index=True, right_index=True)\ninfo = info.melt(id_vars=['Year','Funds'], value_vars=colnames, \n                 var_name='Topic', value_name='Loading')\ngb = info.groupby(['Year','Topic'])\nwa = gb.apply(lambda x: np.average(x.Funds, weights=x.Loading))\nwa = wa.unstack().reset_index().melt(id_vars='Year', value_name='WA')\nwa.head(5)\n", "intent": "If desired, we can use **groupby** to further separate this weighted average for each year between 2006 and 2016.\n"}
{"snippet": "d = {'Sent': all_sents, 'Contains_claims': labels}\nsentDF = pd.DataFrame(d)\n", "intent": "Major problem: when there suppose to be 2294 claims, I'm only able to find 1702 sentences that contain claims using this method.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npivot['x'] = pca.fit_transform(pivot[x_cols])[:,0]\npivot['y'] = pca.fit_transform(pivot[x_cols])[:,1]\npca_df = pivot[['customer_name','cluster','x','y']]\npca_df.head(3)\n", "intent": "There is seperation between clusters. It can be better. Each cluster has its own grouping.\nThe best value for K seems to be 3.\n"}
{"snippet": "users_df = pd.read_csv('takehome_users.csv')\nprint(users_df.shape)\nusers_df.head()\n", "intent": "**Note:** 'takehome_users.csv' was re-encoded to utf-8 to allow proper opening.\n"}
{"snippet": "df = pd.read_csv('adult.csv')\n", "intent": "Let's start by reading in the adult.csv file into a pandas dataframe.\n"}
{"snippet": "df = pd.read_csv('breast-cancer-wisconsin.data.txt', \n                 names=['id','clump_thickness','uni_cellsize','uni_cellshape',\n                        'adhesion','epi_cellsize','nuclei','chromatin','nucleoli','mitoses',\n                       'class'])\n", "intent": "Let's start by reading in the adult.csv file into a pandas dataframe.\n"}
{"snippet": "data = pd.DataFrame(scaled_feat,columns= df.columns[:-1] )\n", "intent": "Converting the 'scaled_feat' to a dataframe\n"}
{"snippet": "df = pd.read_csv('iris-dataset.csv', names=['col1','col2','col3','col4','type'])\n", "intent": "Let's start by reading in the adult.csv file into a pandas dataframe.\n"}
{"snippet": "df = pd.read_csv('mammographic_masses.data', names=['bi-rads', 'age', 'shape', 'margin', 'density', 'severity'])\n", "intent": "Let's start by reading in the adult.csv file into a pandas dataframe.\n"}
{"snippet": "df = pd.read_csv('Prostate_Cancer.csv')\n", "intent": "Let's start by reading in the adult.csv file into a pandas dataframe. Cleaning the data, if required.\n"}
{"snippet": "from sklearn import preprocessing as prep\ngre_norm=pd.DataFrame(prep.normalize(dat_trans[\"gre\"]))\ngpa_norm=pd.DataFrame(prep.normalize(dat_trans[\"gpa\"]))\n", "intent": "But that doesn't look much better. Maybe that's because I'm doing something wrong. But he, let's try something from the SciKit!\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_glass, df['Type'], test_size=0.33, random_state=101)\n", "intent": "Creating the Training & Test Data from the Data Set\n"}
{"snippet": "coefficients = pd.DataFrame(lm.coef_, index=df.columns[:-1], columns=['Coefficients'])\n", "intent": "** Coefficients of PE **\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(df.drop(['Yearly Amount Spent','Email','Address','Avatar'], axis=1), df['Yearly Amount Spent'], \n                                                   test_size=0.3, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "df2012_13 = pd.read_csv(os.getenv('FDS')+'LoanStats_2012_to_2013.csv',low_memory=False,skiprows=1)\ndf2014 = pd.read_csv(os.getenv('FDS')+'LoanStats_2014.csv',low_memory=False,skiprows=1)\n", "intent": "<p>\n<span style=\"color:blue\">\n> Importing data of Lending club for the years 2012-14\n</span>\n</p>\n"}
{"snippet": "y_pred = pd.DataFrame(predictions)\n", "intent": "** Each item in your list will look like this: **\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"/Users/sofia/Projects/workspace/courses/udemy/data-science-and-machine-learning-with-python/data/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "columns_nunique = pd.DataFrame(raw_data_allyears_noEPA.nunique(),columns = ['n'])\n", "intent": "* **Finding the number of unique values in every columns:**\n"}
{"snippet": "columns_nunique_n = pd.DataFrame(raw_data_allyears_noEPA.nunique(),columns = ['n'])\n", "intent": "* ** Removing constant valued columns (no. of unique values = 1):**\n"}
{"snippet": "df_sparsity = pd.DataFrame(raw_data_allyears_noEPA.isnull().sum()/raw_data_allyears_noEPA.shape[0]\n                           ,columns=['sparsity'])\n", "intent": "* **Checking feature data sparsity:**\n"}
{"snippet": "admitraw = pd.read_csv(\"assets/admissions.csv\")\naddata1 = admitraw.dropna() \nprint addata1.head()\n", "intent": "<em>but this time I'm going to include the intercept in my cartesian graph</em></font>\n"}
{"snippet": "param_grid_1 = {'logisticregression__C': np.logspace(-3, 3, 10),\n                'logisticregression__class_weight': ['balanced',None]}\ngrid_logit_1 = GridSearchCV(make_pipeline(CountVectorizer(),\n                                          LogisticRegression(),\n                                          memory = \"cache_folder\"),\n                          param_grid= param_grid_1,\n                            scoring=ROC_AUC_score, cv=5)\ngrid_logit_1.fit(X_train_title,y_train_1)\n", "intent": "**Using Gridsearch**:\n"}
{"snippet": "feature_names_12 = review_vect.get_feature_names()\ncoef_12 = logit_12.coef_[0]\ntop_20_feat = (np.absolute(coef_12).argsort())[-20:]\ntop20_feat_names = np.array(feature_names_12)[top_20_feat]\ntop20_coef = coef_12[top_20_feat]\ntop20_feat_df = pd.DataFrame(list(zip(top20_feat_names,top20_coef)),\n                             columns = ['Feature','Coefficient'])\ntop20_feat_df = top20_feat_df.sort_values(by='Coefficient',\n                                          ascending = False)\nprint(top20_feat_df)\n", "intent": "**Visualizing coefficients**\n"}
{"snippet": "param_grid_12 = {'logisticregression__C': np.logspace(-3, 3, 10),\n                 'logisticregression__class_weight': ['balanced',None]}\ngrid_logit_12 = GridSearchCV(make_pipeline(CountVectorizer(),\n                                           LogisticRegression(),\n                                           memory = \"cache_folder\"),\n                          param_grid= param_grid_12,\n                             scoring=ROC_AUC_score, cv=5)\ngrid_logit_12.fit(non_review_train['Review'].values,y_train_12)\n", "intent": "**Tuning the parameters**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ndef get_bag_of_words(docs):\n    vectorizer = CountVectorizer()\n    bag_of_words = vectorizer.fit_transform(docs)\n    return bag_of_words.toarray()\nproject_tests.test_get_bag_of_words(get_bag_of_words)\n", "intent": "Let's extract some features from the text data using bag of words. Implement the `get_bag_of_words` using sklearn's `CountVectorizer` function.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ndef word_freq(docs):\n    return TfidfVectorizer().fit_transform(docs).toarray()\nword_freq_ten_ks = {}\nfor ticker, ten_ks in ten_ks_by_ticker.items():\n    word_freq_ten_ks[ticker] = {\n        'item1a': word_freq([' '.join(ten_k['lemma_item1a']) for ten_k in ten_ks]),\n        'item7': word_freq([' '.join(ten_k['lemma_item7']) for ten_k in ten_ks]),\n        'item7a': word_freq([' '.join(ten_k['lemma_item7a']) for ten_k in ten_ks])}\nproject_helper.print_ten_k_data([word_freq_ten_ks[example_ticker]], ['item1a', 'item7', 'item7a'])\n", "intent": "Just like we did with the bag of words, lets create TFIDF values.\n"}
{"snippet": "import pandas as pd\nteam_df = pd.read_csv(\"../Data/teaminfo.csv\")\nteam_df.tail(10)\n", "intent": "<u>Load the team data.</u>\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n", "intent": "- Produce training and testing sets. The framework comes with a simple utility: ```train_test_split.```\n"}
{"snippet": "errs = pd.DataFrame()\nerrs['e']=cmp['w']-cmp['pred_w']\nerrs['e']=errs['e'].abs()\navg=errs['e'].mean()\nstd=errs['e'].std()\nprint(\"Avg error = \", avg)\nprint(\"Std Deviation = \", std)\n", "intent": "- That is rather odd looking. How accurate were we?\n"}
{"snippet": "X, y, attribute_names = dataset.get_data(\n    target=dataset.default_target_attribute, \n    return_attribute_names=True)\neeg = pd.DataFrame(X, columns=attribute_names)\neeg['class'] = y\nprint(eeg[:10])\n", "intent": "Get the actual data.  \nReturned as numpy array, with meta-info (e.g. target feature, feature names,...)\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33)\n", "intent": "[sklearn train_test_split documentation](http://scikit-learn.org/stable/modules/cross_validation.html\n"}
{"snippet": "from sklearn.datasets import load_iris\niris_dataset = load_iris()\nprint(\"Keys of iris_dataset: {}\".format(iris_dataset.keys()))\nprint(iris_dataset['DESCR'][:193] + \"\\n...\")\n", "intent": "Iris is included in scikitlearn, we can just load it.  \nThis will return a `Bunch` object (similar to a `dict`)\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    iris_dataset['data'], iris_dataset['target'], \n    random_state=0)\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"y_train shape: {}\".format(y_train.shape))\nprint(\"X_test shape: {}\".format(X_test.shape))\nprint(\"y_test shape: {}\".format(y_test.shape))\n", "intent": "To evaluate our classifier, we need to test it on unseen data.  \n`train_test_split`: splits data randomly in 75% training and 25% test data.\n"}
{"snippet": "iris_df = pd.DataFrame(X_train, \n                       columns=iris_dataset.feature_names)\nsm = pd.scatter_matrix(iris_df, c=y_train, figsize=(10, 10), \n                  marker='o', hist_kwds={'bins': 20}, s=60, \n                  alpha=.8, cmap=mglearn.cm3)\n", "intent": "We can use a library called `pandas` to easily visualize our data. Note how several features allow to cleanly split the classes.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nX, y = mglearn.datasets.make_forge()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nclf = KNeighborsClassifier(n_neighbors=3)\nclf.fit(X_train, y_train)\n", "intent": "Let's build a kNN model for this dataset (called 'Forge')\n"}
{"snippet": "from sklearn.neighbors import KNeighborsRegressor\nX, y = mglearn.datasets.make_wave(n_samples=40)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nreg = KNeighborsRegressor(n_neighbors=3)\nreg.fit(X_train, y_train)\n", "intent": "To do regression, simply use `KNeighborsRegressor` instead\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nX, y = mglearn.datasets.load_extended_boston()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nlr = Ridge().fit(X_train, y_train)\n", "intent": "Linear regression can be found in `sklearn.linear_model`. We'll evaluate it on the Boston Housing dataset.\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\nlogreg = LogisticRegression().fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n", "intent": "Model selection: Logistic regression\n"}
{"snippet": "import pandas as pd\nresults = pd.DataFrame(grid_search.cv_results_)\ndisplay(results.head())\n", "intent": "We can retrieve and visualize the cross-validation resulst to better understand the impact of hyperparameters\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n", "intent": "Effect of pre-pruning: default tree overfits, setting `max_depth=4` is better\n"}
{"snippet": "sac = pd.read_csv('../assets/datasets/Sacramentorealestatetransactions.csv')\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n                                                    random_state=1)\n", "intent": "- Lets apply a data transformation _manually_, then use it to train a learning algorithm\n- First, split the data in training and test set\n"}
{"snippet": "from sklearn.svm import SVC\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n                                                    random_state=0)\nsvm = SVC()\nsvm.fit(X_train, y_train)\nprint(\"Test set accuracy: {:.2f}\".format(svm.score(X_test, y_test)))\n", "intent": "* First, we train the SVM without scaling\n"}
{"snippet": "scaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nsvm.fit(X_train_scaled, y_train)\nprint(\"Scaled test set accuracy: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\n", "intent": "* With scaling, we get a much better model\n"}
{"snippet": "from sklearn.feature_selection import SelectPercentile, f_regression\nselect = SelectPercentile(score_func=f_regression, percentile=5).fit(X, y)\nX_selected = select.transform(X)\nprint(\"X_selected.shape: {}\".format(X_selected.shape))\n", "intent": "* First, we select the 5% most informative features with `SelectPercentile`, and then evaluate a `Ridge` regressor\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best params:\\n{}\\n\".format(grid.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\n", "intent": "The Scaling+SVM pipeline wins!\n"}
{"snippet": "import os\ndata = pd.read_csv(\n    os.path.join(mglearn.datasets.DATA_FOLDER, \"adult.data\"), header=None, index_col=False,\n    names=['age', 'workclass', 'fnlwgt', 'education',  'education-num',\n           'marital-status', 'occupation', 'relationship', 'race', 'gender',\n           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n           'income'])\ndata = data[['age', 'workclass', 'education', 'gender', 'hours-per-week',\n             'occupation', 'income']]\ndisplay(data.head())\n", "intent": "Convert a feature with c categories to $c$ dummy variables:  \n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nprint(\"Test score: {:.2f}\".format(logreg.score(X_test, y_test)))\n", "intent": "Now the data is represented in a way that scikit-learn can work with, and we can\nproceed as usual:\n"}
{"snippet": "from pandas import read_csv\nfrom datetime import datetime\ndataset = read_csv('PRSA_data_2010.1.1-2014.12.31.csv')\ndataset\n", "intent": "The data is not ready to use. We must prepare it first.\nBelow are the first few rows of the raw dataset.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nvar_mod = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']\nle = LabelEncoder()\nfor i in var_mod:\n    df[i] = le.fit_transform(df[i])\ndf.dtypes \n", "intent": "In python, use Skicit-Learn to create a predictive model \nFirst, we have to convert all categorical models into numeric variables\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, Y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['over_200k', 'under_200k'],\n                         columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "with urllib.request.urlopen(\"http://goo.gl/j0Rvxq\") as url:\n    s = url.read()\n", "intent": "Object that get data from an URL\n"}
{"snippet": "dataset = datasets.load_iris()\n", "intent": "We'll import Iris dataset and put inside the object called dataset \n"}
{"snippet": "credit = pd.read_csv('https://raw.githubusercontent.com/fclesio/learning-space/master/Datasets/02%20-%20Classification/default_credit_card.csv')\n", "intent": "Now we'll import a structured dataset that all columns are numeric.\n"}
{"snippet": "algorithms = {'Algorithm': ['Random Forests', 'Gradient Boosting', 'Extra Trees', 'Ada Boosting', 'SVM', 'KNN', 'Decision Trees', 'Perceptron', 'Logistic Regression'],\n        'MSE': [round(mse_rfc,4), round(mse_gbc,4), round(mse_etc,4), round(mse_abc,4), round(mse_svc,4), round(mse_knc,4), round(mse_dtc,4), round(mse_ptc,4), round(mse_lrc,4)]}\nalgos = pd.DataFrame(algorithms)\nalgos.sort_values(by='MSE', ascending=1)\n", "intent": "Ok, let's ranking our algorithms to see the best one to start our analysis. \n"}
{"snippet": "from sklearn import cross_validation\nX_1, X_2, Y_1, Y_2 = cross_validation.train_test_split(\n    target_features, outcome_feature, test_size=0.5, random_state=0)\n", "intent": "Generate training and test set\n"}
{"snippet": "data = pd.read_csv('./data/MNIST/train.csv')\ndata = np.array(data)\n", "intent": "* http://www.fast.ai/, Fast.ai ML, Fast.ai DL\n* http://ruder.io/optimizing-gradient-descent/\n* http://cs231n.github.io/neural-networks-3/\n"}
{"snippet": "df_submission = pd.DataFrame({'id': id_test, 'price_doc': y_pred.astype('int64')})\n", "intent": "You need to create your submission when you want to test your model using the test data. These next cells creates a file with the submission format.\n"}
{"snippet": "from linear_models import load_data\ndataframe = load_data()\nprint dataframe.iloc[0]\n", "intent": "The dataset is about ridership of NYC subways. Here is a typical record in the dataset.\n"}
{"snippet": "train = pd.read_csv(\"train.csv\")\ntest  = pd.read_csv(\"test.csv\")\nfull = train.append( test , ignore_index = True )\ntitanic = full[ :891 ]\ndel train , test\nprint ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)\n", "intent": "Now that our packages are loaded, let's read in and take a peek at the data.\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, Y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['has_cancer', 'is_healthy'],\n                         columns=['predicted_cancer','predicted_healthy'])\nprint(confusion)\n", "intent": "Let's say again that we are predicting cancer based on some kind of detection measure, as before.\n"}
{"snippet": "X = imputer.transform(housing_num)\ntype(X)\nhousing_tr = pd.DataFrame(X,columns=housing_num.columns)\nhousing_tr.head()\n", "intent": "Transforming Training Set\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\nlabelEnc=LabelEncoder()\ncat_vars=['Embarked','Sex',\"Title\",\"FsizeD\",\"NlengthD\",'Deck']\nfor col in cat_vars:\n    titanic[col]=labelEnc.fit_transform(titanic[col])\n    titanic_test[col]=labelEnc.fit_transform(titanic_test[col])\n", "intent": "    Using Label Encoder one at a time as Categorical Encoder is not available\n"}
{"snippet": "from sklearn import preprocessing\nstd_scale = preprocessing.StandardScaler().fit(titanic[['Age', 'Fare']])\ntitanic[['Age', 'Fare']] = std_scale.transform(titanic[['Age', 'Fare']])\nstd_scale = preprocessing.StandardScaler().fit(titanic_test[['Age', 'Fare']])\ntitanic_test[['Age', 'Fare']] = std_scale.transform(titanic_test[['Age', 'Fare']])\n", "intent": "    Scaling Age & Fare\n"}
{"snippet": "dataset =  load_data(path=TRAINING_DATA)\ntest_set = load_data(path=TEST_DATA)\n", "intent": "Since we already have different Training & Test sets, we don't neeed to create any splits. Hence, directly loading the data\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\ndata = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  \nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n", "intent": "You have already seen the code to load the soccer/football data:\n"}
{"snippet": "X_train, y_train, X_test, y_test = load_data('../input/sp500.csv', 50, True)\n", "intent": "The code cell below loads your training and test data. Do a quick overview of the data to make sure you understand it.\n"}
{"snippet": "movies_path = os.path.join(input_dir, 'movie.csv')\nmovies_df = pd.read_csv(movies_path, index_col=0)\nmovies_df.head()\n", "intent": "What movie is this the embedding of? Let's load up our dataframe of movie metadata.\n"}
{"snippet": "import pandas as pd\nmelbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'\nmelbourne_data = pd.read_csv(melbourne_file_path) \nfiltered_melbourne_data = melbourne_data.dropna(axis=0)\ny = filtered_melbourne_data.Price\nmelbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n                        'YearBuilt', 'Lattitude', 'Longtitude']\nX = filtered_melbourne_data[melbourne_features]\nfrom sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\n", "intent": "The data is loaded into **train_X**, **val_X**, **train_y** and **val_y** using the code you've already seen (and which you've already written).\n"}
{"snippet": "X_new = pd.DataFrame({'TV': [50]})\nX_new.head()\nX_new2 = pd.DataFrame({'Radio': [50]})\nX_new2.head()\n", "intent": "Thus, we would predict Sales of **9,409 widgets** in that market.\nOf course, we can also use Statsmodels to make the prediction:\n"}
{"snippet": "conmat_10 = np.array(confusion_matrix(Y_test, Y_pp.pred_class_thresh10.values, labels=[1,0]))\nconfusion_10 = pd.DataFrame(conmat_10, index=['has_cancer', 'is_healthy'],\n                            columns=['predicted_cancer','predicted_healthy'])\nprint(confusion_10)\n", "intent": "This will reduce our false negative rate to 0, but at the expense of our false positive rate.\n"}
{"snippet": "train_whole = pd.read_csv(\"train_clean_csv.csv\", index_col=0)\ntarget_whole = train_whole['SalePrice']\ntrain = train_whole.iloc[:1000,:]\ntest = train_whole.iloc[1000:,:]\ntrain_target = train[\"SalePrice\"]\ntest_target = test[\"SalePrice\"]\n", "intent": "Load the data and split it into a train and test set.\n"}
{"snippet": "features = cont_feat + ord_feat + dum_feat\nfeature_importances = R_2.feature_importances_\nfeatures_df = pd.DataFrame({'Features': features, 'Importance Score': feature_importances})\nfeatures_df.sort('Importance Score', inplace=True, ascending=False)\nfeatures_df.head(), features_df.tail()\n", "intent": "A random forest with all features performs well.\n"}
{"snippet": "Final = pd.DataFrame()\nfor f in os.listdir(datapath):\n    filepath = os.path.join(datapath,f)\n    if filepath.endswith('.csv'):\n        Res = make_inputs(filepath)\n        Final = Final.append(Res)\n", "intent": "I'll iterate over each file, run the above and concat to a final df. Then we'll pivot\n"}
{"snippet": "result_last.to_csv('12-10-3.csv',index=False)\n", "intent": "df2 = pd.read_csv('test_modified_12-10.csv')\naa=solo(X, y,df2)\nresult_last = pd.DataFrame({'Id':df2['Id'],'SalePrice':aa})\n"}
{"snippet": "from keras.models import model_from_json\nwith open(\"./models/c13/simple_nn.json\", 'r') as json_file:\n    loaded_model_json = json_file.read()\nloaded_model = model_from_json(loaded_model_json)\nloaded_model.load_weights(\"./models/c13/simple_nn.h5\")\nloaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n", "intent": "Load model structure from simple_nn.json, and load weights from simple_nn.h5\n"}
{"snippet": "from keras.preprocessing import sequence\nmax_words = 500\nX_train = sequence.pad_sequences(X_train, maxlen=max_words)\nX_val = sequence.pad_sequences(X_val, maxlen=max_words)\nprint len(X_train[0])\n", "intent": "Bound the length of word sequence to 500, truncating longer reviews and zero-padding shorter reviews.\n"}
{"snippet": "movie_names = pd.read_csv(path+'movies.csv').set_index('movieId')['title'].to_dict\n", "intent": "Just for display purposes, let's read in the movie names too.\n"}
{"snippet": "train = pd.DataFrame(utils2.load_array(data_path+'train/train_features.bc'),columns=['TRIP_ID', 'CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID',\n       'TIMESTAMP', 'DAY_TYPE', 'MISSING_DATA', 'POLYLINE', 'LATITUDE', 'LONGITUDE', 'TARGET', 'COORD_FEATURES', 'DAY_OF_WEEK',\n                            'QUARTER_HOUR', 'WEEK_OF_YEAR'])\n", "intent": "Meanshift clustering as performed in the paper\n"}
{"snippet": "train = pd.DataFrame(utils2.load_array(data_path+'train/train_features.bc'),columns=['TRIP_ID', 'CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID',\n       'TIMESTAMP', 'DAY_TYPE', 'MISSING_DATA', 'POLYLINE', 'LATITUDE', 'LONGITUDE', 'TARGET',\n                            'COORD_FEATURES', \"DAY_OF_WEEK\", \"QUARTER_HOUR\", \"WEEK_OF_YEAR\"])\n", "intent": "Load training data and cluster centers\n"}
{"snippet": "metrics_pct = np.array(bcw.metrics_pct.values)\nmetrics_pct = metrics_pct[:, np.newaxis]\nX_train, X_test, Y_train, Y_test = train_test_split(metrics_pct, bcw[['class']].values, \n                                                    test_size=0.33, stratify=bcw[['class']].values,\n                                                    random_state=77)  \n", "intent": "Split into 66% training set and 33% testing set\n>```\n>X = metrics_pct (predictor)\n>Y = class (non-cancer:0 vs cancer:1)\n>```\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"bank_note_data.csv\")\n", "intent": "** Use pandas to read in the bank_note_data.csv file **\n"}
{"snippet": "scaledDf = pd.DataFrame(X_train, columns=df.columns[:-1])\nscaledDf.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "** Use SciKit Learn to create training and testing sets of the data as we've done in previous lectures:**\n"}
{"snippet": "import cufflinks as cf\ndv = pd.DataFrame(error_rate, index=range(1, 100), columns=['Error rate'])\ndv.head()\ndv.iplot()\n", "intent": "**Now create the following plot using the information from your for loop.**\n"}
{"snippet": "X = CountVectorizer().fit_transform(X)\nprint(X.shape)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = features\ny = new_df.Danger\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=0.2,random_state=42)\n", "intent": "Created different models to test for performance and started with a Logistic Regression model.\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\none_hot_y = enc.fit_transform(y.reshape(-1, 1)).todense()\n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "with open(\"classes.txt\", \"r\") as f:\n    classes = f.read().splitlines()\nprint(classes[9])\n", "intent": "Now we should put the weights into their places:\n"}
{"snippet": "from Bio import Phylo\ntree = Phylo.read('../data/tree.nwk', 'newick')\ndistances = []\nfor node in tree.get_terminals():\n    distances.append(tree.distance(tree.root, node))\nsum(distances)/float(len(distances))\n", "intent": "Can you compute the average root-to-tip distance in the `data/tree.nwk` file?\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconmat = np.array(confusion_matrix(Y_test, Y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['has_cancer', 'is_healthy'],\n                         columns=['predicted_cancer','predicted_healthy'])\nprint(confusion)\n", "intent": "Look at the confusion matrix\n"}
{"snippet": "df2 = pd.DataFrame(scaled_features, columns=df.columns[:-1])\ndf2.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "college = pd.read_csv(\"College_Data\",index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "yelp = pd.read_csv(\"yelp.csv\")\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nx = np.array([1, 2, 3, 4])\npoly = PolynomialFeatures(3, include_bias=False)\npoly.fit_transform(x[:, None])\n", "intent": "**Polynomial basis functions**\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "**Model validation the wrong way**\n"}
{"snippet": "X = iris.data\npca = PCA( n_components = 2, whiten = True ).fit( X )\nX_pca = pca.transform( X )\nX_pca\n", "intent": "Our data has 3 different species, 150 samples and 4 features(dimenesions)\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(lowercase = True, token_pattern ='(?u)\\\\b\\\\w\\\\w+\\\\b')\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "df[\"avg_rating_by_driver\"]=df[\"avg_rating_by_driver\"].fillna(-1) \ndf[\"avg_rating_of_driver\"]=df[\"avg_rating_of_driver\"].fillna(-1) \nbins = [-2, 0, 1.1, 2.1,  3.1, 4.1 ,5.1]\ngroup_names = ['None', '0-1', '1-2', '2-3', '3-4', '4-5']\ndf['avg_rating_by_driver_grouped'] = pd.cut(df[\"avg_rating_by_driver\"], bins, labels=group_names)\ndf['avg_rating_of_driver_grouped'] = pd.cut(df[\"avg_rating_of_driver\"], bins, labels=group_names)\n", "intent": "For both the ratings we will bin them into categories None ,0-1, 1-2, 2-3, 3-4, 4-5, where 0 would be the category with missing ratings.\n"}
{"snippet": "df['phone']=df['phone'].fillna('Missing')\n", "intent": "Create a category \"Missing\" for those users with missing entries for \"phone\".\n"}
{"snippet": "lr_cm = confusion_matrix(y_test, lr_ypred, labels=lr.classes_)\nlr_cm = pd.DataFrame(lr_cm, columns=lr.classes_, index=lr.classes_)\nlr_cm\n", "intent": "[Confusion Matrix](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nencoder.fit(y.reshape(len(y),-1))\ny_encoded = encoder.transform(y.reshape(len(y),-1))\nX_mnist = X\ny_mnist = y_encoded\n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "sc = StandardScaler()\npc = PCA(n_components=2)\nclf = CLF\n", "intent": "There is no deskew class in `sklearn` so we can not put this in a `Pipeline`.\n"}
{"snippet": "c = list(Counter([tuple(set(x)) for x in battles.dropna(subset = [\"attacker_king\", \"defender_king\"])[[\"attacker_king\", \"defender_king\"]].values if len(set(x)) > 1]).items())\np = pd.DataFrame(c).plot.barh(figsize = (10, 6))\n_ = p.set(yticklabels = [\"%s vs. %s\" % (x[0], x[1]) for x in list(zip(*c))[0]], xlabel = \"No. of Battles\"), p.legend(\"\")\n", "intent": "Which pairs fought the most battles?\n"}
{"snippet": "pca = PCA(2)  \nprojected = pca.fit_transform(digits.data)\nprint(digits.data.shape)\nprint(projected.shape)\n", "intent": "Reduce the dimensionality.\n"}
{"snippet": "pca_1 = PCA(n_components=3)\n", "intent": "Basically almost all of our data is captured by the first principal component. \n"}
{"snippet": "pca_1 = PCA(n_components=1)\n", "intent": "Basically almost all of our data is captured by the first principal component. \n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "Instantiate the scaler.\n"}
{"snippet": "customer_sc_df = pd.DataFrame(customer_sc, columns=customer_features.columns)\n", "intent": "Load it into a dataframe.\n"}
{"snippet": "stats = pd.DataFrame()\nstats['mean'] = categorical_encoded_df[ms_sub_class_encoded_cols].mean()\nstats['std'] = categorical_encoded_df[ms_sub_class_encoded_cols].std()\nstats['var'] = categorical_encoded_df[ms_sub_class_encoded_cols].var()\nstats.sort_values('std', ascending=False)\n", "intent": "Next, let's look at mean and standard deviation of the filtered dataframe.\n"}
{"snippet": "data = pd.read_csv('../../assets/datasets/train.tsv', sep='\\t', na_values='?')\ndata['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', '')).fillna('')\ndata['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', '')).fillna('')\n", "intent": "- These are websites that always relevant like recipies or reviews (as opposed to current events)\n- Look at some examples\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "- Iris: Classify flower species based on measurements\n- Handwritten Digits: Is the digit a 0, 1, 2, 3, ... ?\n"}
{"snippet": "import pandas as pd\nsongs = pd.read_csv('data/songs.csv')\n", "intent": "Use kmeans to cluster the following song data.  Discuss the meaning of the clusters. \n"}
{"snippet": "titanic_train = pd.read_csv('data/titanic_train.csv', index_col=0)\n", "intent": "- Describe Titanic Task\n"}
{"snippet": "nhl = pd.read_csv('data/NHL_Data_GA.csv')\n", "intent": "**Load the Data**\nLocated in data folder, named as follows:\n```\ndata/NHL_Data_GA.csv\n```\n"}
{"snippet": "PATH_TO_DATA = ('../../data')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "tfidf_merged_obj = TfidfVectorizer(max_features=80000, ngram_range=(1,5), sublinear_tf=True)\ntfidf_merged_obj.fit(full_sites_merged['sites_visited'])\n", "intent": "Build Tf-Idf features based on sites. You can use `ngram_range`=(1, 3) and `max_features`=100000 or more\n"}
{"snippet": "full_new_feat = pd.DataFrame(index=full_df.index)\nfull_new_feat['start_month'] = full_df['time1'].apply(lambda ts: 100 * ts.year + ts.month)\n", "intent": "Add features based on the session start time: hour, whether it's morning, day or night and so on.\n"}
{"snippet": "strt_mnth_sc = StandardScaler().fit_transform(full_new_feat[['start_month']])\nX_train_strt_mnth_s = csr_matrix(hstack([X_train, strt_mnth_sc[:idx_split,:]]))\nprint(get_auc_lr_valid(X_train_strt_mnth_s, y_train))\n", "intent": "Scale these features and combine them with Tf-Idf based on sites (you'll need `scipy.sparse.hstack`)\n"}
{"snippet": "strt_mnth_hr_isday_s = StandardScaler().fit_transform(full_new_feat[['isdaytime']])\nX_train_strt_hour_isday_morn_s = csr_matrix(hstack([X_train, strt_mnth_hr1[:idx_split,:],strt_mnth_hr_isday_s[:idx_split,:], strt_mnth_mrng[:idx_split,:]]))\nprint(get_auc_lr_valid(X_train_strt_hour_isday_morn_s, y_train))\n", "intent": "Perform cross-validation with logistic regression.\n"}
{"snippet": "def examine_coefficients(model, df):\n    df = pd.DataFrame(\n        { 'Coefficient' : model.coef_[0] , 'Feature' : df.columns}\n    ).sort_values(by='Coefficient')\n    return df[df.Coefficient !=0 ]\n", "intent": "- Examine the coefficients using the `examine_coefficients` function provided\n"}
{"snippet": "scale_obj = StandardScaler()\nX_scaled = scale_obj.fit_transform(X)\n", "intent": "- 30 degrees\n- 45 degrees\n- 60 degrees\n- 75 degrees\n"}
{"snippet": "from PIL import Image\nlfw_people = datasets.fetch_lfw_people(min_faces_per_person=50, \n                resize=0.4, data_home='../../data/faces')\nprint('%d objects, %d features, %d classes' % (lfw_people.data.shape[0],\n      lfw_people.data.shape[1], len(lfw_people.target_names)))\nprint('\\nPersons:')\nfor name in lfw_people.target_names:\n    print(name)\n", "intent": "Let's load a dataset of peoples' faces and output their names. (This step requires stable, fast internet connection.)\n"}
{"snippet": "liqdata = pd.read_csv('/Users/sebozek/GA/Iowa_Liquor_Sales_reduced.csv')\n", "intent": "Load your data from project 3\n"}
{"snippet": "import numpy as np\nfrom sklearn import svm, grid_search, datasets\nfrom sklearn.neighbors import KNeighborsClassifier\niris = datasets.load_iris()\n", "intent": "While GridSearch can work with most sklearn models, we will try it out on KNN to start with iris dataset.\n"}
{"snippet": "pca = PCA(n_components=3)\nX_r = pca.fit(Xn).transform(Xn)\n", "intent": "Now, conduct a PCA using scikit learn\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "pca = PCA(n_components=2)\nX_r = pca.fit(Xn).transform(Xn)\n", "intent": "Finally, conduct the PCA - use the results about to guide your selection of \"n\" componants\n"}
{"snippet": "cvt = CountVectorizer(stop_words='english', \n                      ngram_range = (2, 4))\ncvt.fit_transform(insults_df['Comment'])\nsummaries = \"\".join(insults_df[\"Comment\"])\nngrams_summaries = cvt.build_analyzer()(summaries)\nngrams_2_4 = Counter(ngrams_summaries).most_common(75)\npd.DataFrame(ngrams_2_4, columns = ['ngram', 'Counts'])\n", "intent": "Display the top 75 ngrams with frequencies.\n"}
{"snippet": "pipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('cls', BernoulliNB())\n]) \npipeline.fit(X_train, Y_train)\npipeline.score(x_test, y_test)\n", "intent": "How do they compare?\n"}
{"snippet": "pipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('rf', RandomForestClassifier())\n]) \npipeline.fit(X_train, Y_train)\npipeline.score(x_test, y_test)\n", "intent": "How do they compare?\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer(\n    binary=True,  \n    stop_words='english', \n    max_features=50, \n)\nX = v.fit_transform(data.title).todense()\nX = pd.DataFrame(X, columns=v.get_feature_names())\nX.head()\n", "intent": "- `CountVectorizer` builds a feature per word automatically as we did manually for `recipe`, `electronic` above.\n"}
{"snippet": "def bar_chart(feature):\n    survived = train[train[\"Survived\"] == 1][feature].value_counts()\n    dead = train[train[\"Survived\"] == 0][feature].value_counts()\n    df = pd.DataFrame([survived, dead])\n    df.index = ['Survived', 'Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits() \nprint(digits.keys())\nprint(\"\\nuse data-X target-y to train and test\")\n", "intent": "Classfication of digit with hand written image (gray scale data matrix)\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nboston.keys()\n", "intent": "(1) import data and explore\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target)\n", "intent": "(2) split into train/test with boston dataset:\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nboston.keys()\n", "intent": "Unsupervised transformations for preprocessing\n--------------------------------------------------\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits(n_class=5)\nX, y = digits.data, digits.target\nprint(X.shape)\n", "intent": "PCA for dimensionality Reduction\n---------------------------------\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=16)\nX.shape\n", "intent": "Clustering (KMeans)\n=============\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nX, y = digits.data, digits.target\n", "intent": "A less trivial example (bad adjusted_rand_score)\n-------------------------\n"}
{"snippet": "from sklearn.datasets import load_iris\nfrom sklearn.neighbors import KNeighborsClassifier\niris = load_iris()\nX, y = iris.data, iris.target\nn_samples = X.shape[0]\nprint(X.shape)\nprint(y.shape)\nnp.unique(y)\n", "intent": "Cross-Validation (more robost compared with  training / test split)\n=====================================\n"}
{"snippet": "from sklearn import preprocessing\nstd_scale = preprocessing.StandardScaler().fit(df[['SQFT', 'BDRMS', 'AGE']])\ndf_std = std_scale.transform(df[['SQFT', 'BDRMS', 'AGE']])\n", "intent": "Refer to this if you are confused as to the formula: [Standardization](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n", "intent": "Avoid Overfitting with Hyper-Parameters\n----------------------------------------\n"}
{"snippet": "from sklearn.datasets import make_regression\nfrom sklearn.cross_validation import train_test_split\nX, y, true_coefficient = make_regression(n_samples=80, n_features=30, n_informative=10, noise=100, coef=True, random_state=5)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)\nprint(X_train.shape)\nprint(X_test.shape)\ntrue_coefficient\n", "intent": "```\ny_pred = x_test[0] * coef_[0] + ... + x_test[n_features-1] * coef_[n_features-1] + intercept_\n```\n"}
{"snippet": "d = {'col1': \"ts1\", 'col2': \"ts2\"}\ndf =pd.DataFrame(data=d,index = [0,1])\ndf\n", "intent": "How many rows are in the dataset?\n"}
{"snippet": "sac = pd.read_csv('../../assets/datasets/Sacramentorealestatetransactions.csv')\nsac.head(10)\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "subset = ['ASSAULT','VANDALISM']\nsf_crime_sub = sf_crime[sf_crime['Category'].str.contains('|'.join(subset))]\nX = patsy.dmatrix('~ C(hour) + C(DayOfWeek) + C(PdDistrict) + X + Y', sf_crime_sub)  \nY = sf_crime_sub.Category.values\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, stratify=Y, random_state=5)\n", "intent": "setting threshhold for binary class\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nstudent = pd.read_csv('data/student-mat.csv')\nstudent.head()\n", "intent": "Pandas allows for easy reading and provides a dataframe of the dataset.\n"}
{"snippet": "try:\n    from sklearn.model_selection import train_test_split    \nexcept:\n    from sklearn.cross_validation import train_test_split   \nXTrain, XTest, yTrain, yTest = train_test_split(preproData,target_classes,test_size = 0.25) \n", "intent": "<div class=\"exo\"> <b>Question:</b> Use Scikit-Learn to split the dataset into learning and training sets with <tt>train_test_split</tt>.\n</div>\n"}
{"snippet": "try:\n    from sklearn.model_selection import train_test_split    \nexcept:\n    from sklearn.cross_validation import train_test_split   \nXTrain, XTest, yTrain, yTest = train_test_split(num_features,dfy.values.ravel(),test_size = 0.25) \n", "intent": "<h1> Testing samples </h1>\n"}
{"snippet": "df = df_raw\ndf.pivot_table(\n    index = 'label',\n    aggfunc=[np.min, np.max],\n    values = 'prob').apply(\n    lambda x: np.round(x, decimals = 8))\n", "intent": "Because we have our data set labeled, we can identify an epsilon (threshold, aka \"outlier fraction\") using descriptive stats.\n"}
{"snippet": "from sklearn import preprocessing\nstd_scale = preprocessing.StandardScaler().fit(df[['SQFT', 'BDRMS', 'AGE']])\ndf_std = std_scale.transform(df[['SQFT', 'BDRMS', 'AGE']])\n", "intent": "[StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n"}
{"snippet": "with_zip=pd.DataFrame()\nwith_zip['zip']=zips\n", "intent": "Further the clustering of 5 will be used.\n"}
{"snippet": "conmat = np.array(confusion_matrix(y_test, pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['over_200k', 'under_200k'],\n                         columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "df_cities = pd.read_csv('ds_cities.csv')\ndf_cities.head()\n", "intent": "Source\n[Best_cities_for_data_scientists]('https://infogr.am/30f00c18-997e-4a9d-b903-191899a53890')\n"}
{"snippet": "adult = pd.DataFrame(adult)\nadult.head()\n", "intent": "Convert the data to a Pandas dataframe to work with the data:\n"}
{"snippet": "vote = pd.read_csv(votes_file)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "pc = PCA(n_components=5)\npc.fit(X_s)\n", "intent": "Now, conduct a PCA using scikit learn\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "ip = pd.read_csv('IP_address.txt', sep = '\\t')\nip.head()\n", "intent": "source: http://proxylist.hidemyass.com/\n"}
{"snippet": "df.to_csv('iowa_clean_10.csv')\n", "intent": "Perform some exploratory statistical analysis and make some plots, such as histograms of transaction totals, bottles sold, etc.\n"}
{"snippet": "adult = pd.read_csv(\n    './data/adult.data', \n    names=[\n        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n        \"Hours per week\", \"Country\", \"Target\"], \n    header=None, na_values=\"?\")\nadult = pd.get_dummies(adult)\nadult[\"Target\"] = adult[\"Target_ >50K\"]\n", "intent": "<h1 align=\"center\">Adult test</h1> \n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('https://www.dropbox.com/s/1k9cgsd7bzce0yk/housing-data.csv?dl=1')\n", "intent": "https://www.dropbox.com/s/1k9cgsd7bzce0yk/housing-data.csv?dl=1\n"}
{"snippet": "coefficients = pd.DataFrame({\"Feature\":X_train.columns,\"Coefficients\":np.transpose(rcv.coef_)})\n", "intent": "RidgeCV fit the best of all the Regularized Cross Validated Fits in terms of r squared.\n"}
{"snippet": "train = []\nfor category_id, category in enumerate(CATEGORIES):\n    for file in os.listdir(os.path.join(train_dir, category)):\n        train.append(['train/{}/{}'.format(category, file), category_id, category])\ntrain = pd.DataFrame(train, columns=['file', 'category_id', 'category'])\nprint('train df shape = ', train.shape)\ntrain.head(2)\n", "intent": "There are at least 221 images for each species of seedling in the dataset.\n"}
{"snippet": "messages = pandas.read_csv('data/smsspamcollection/SMSSpamCollection', sep='\\t',\n                           names=[\"label\", \"message\"])\nmessages.head()\n", "intent": "We'll use **read_csv** and make note of the **sep** argument, we can also specify the desired column names by passing in a list of *names*.\n"}
{"snippet": "training_file = 'train.p'\ntesting_file = 'test.p'\nwith open(training_file, mode='rb') as f_train:\n    train = pickle.load(f_train)\nwith open(testing_file, mode='rb') as f_test:\n    test = pickle.load(f_test)\nX_train, X_val, y_train, y_val = train_test_split(train[\"features\"], train[\"labels\"], random_state=0, test_size=0.33)\nX_test, y_test = test[\"features\"], test[\"labels\"]\n", "intent": "Start by importing the data from the pickle file.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.15, random_state = 0)\n", "intent": "Ok..we are ready to start building the model now , let's start by splitting our data into training and test sets\n"}
{"snippet": "titanic_test.fillna(-999,inplace = True)\n", "intent": "and finally replace the null values of the dataframe with -999\n"}
{"snippet": "Submission = pd.DataFrame({\"PassengerID\":titanic_submission[:,0].astype(int),\"Survived\":submission})\n", "intent": "Let's go ahead and create the submission file , (My first ever! )\n"}
{"snippet": "Submission.to_csv(\"Submit_file_TPOT.csv\")\n", "intent": "Save the result as a csv format in my drive!\n"}
{"snippet": "df = pd.read_csv(\"train.csv\")\ndf.isnull().sum()\n", "intent": "Reading the files and investigating the null values\n"}
{"snippet": "pd.DataFrame({'feature':feature_cols,\n              'importance':treeclf.feature_importances_}).sort_values('importance',\n                                                                      ascending=False).head()\n", "intent": "Notice the split in the bottom right, which was made only to increase node purity.\n"}
{"snippet": "submission.to_csv(\"Titanic_CatBoost.csv\")\n", "intent": "And finally saving the file in a csv format in my drive.\n"}
{"snippet": "def pd_centers(featuresUsed, centers):\n    colNames = list(featuresUsed)\n\tcolNames.append('prediction')\n\tZ = [np.append(A, index) for index, A in enumerate(centers)]\n\tP = pd.DataFrame(Z, columns=colNames)\n\tP['prediction'] = P['prediction'].astype(int)\n\treturn P\n", "intent": "Let us first create some utility functions which will help us in plotting graphs:\n"}
{"snippet": "df_predict = pd.read_csv(\"./data/pima-data-trunc.csv\")\nprint(df_predict.shape)\n", "intent": "Once the model is loaded we can use it to predict on some data.  In this case the data file contains a few rows from the original Pima CSV file.\n"}
{"snippet": "fill_0 = Imputer(missing_values=0, strategy=\"mean\", axis=0)\nX_predict = fill_0.fit_transform(X_predict)\n", "intent": "Data has 0 in places it should not.  \nJust like test or test datasets we will use imputation to fix this.\n"}
{"snippet": "cleaned_stock_data = pd.read_csv('output/stocks_merged_ml_data-03-19-18.csv')\ncleaned_stock_data.head()\n", "intent": "**Read in the advertising.csv file and set it to a data frame called cleaned_stock_data.**\n"}
{"snippet": "import pandas as pd\ndf = pd.read_table('C:\\Users\\MLUSER\\Documents\\GitHub\\Udacity\\Naive Bayes Tutorial/SMSSpamCollection',\n                  sep='\\t',\n                  header=None,\n                  names=['label','sms_message'])\ndf.head()\n", "intent": "Step 1.1: Understanding our dataset\n--\n- Parse the data\n- Dataset from - https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n"}
{"snippet": "df = pd.read_csv(\"College.csv\")\ndata = None\ndata1 = data[:,2:19]\nX = None\nY = None\n", "intent": "Import the data from the file College.csv\n"}
{"snippet": "train_df = pd.read_csv(\"../data/train.csv\")\n", "intent": "** Read in the Customers Bike Rental csv train and test files as a Pandas DataFrame.**\n"}
{"snippet": "train_no_dummy_year_piv = all_no_dummy_interpolate[all_no_dummy_interpolate['train'] == 1].pivot_table(values='count',index='month',columns='year').rename(index=monthDict)\ntrain_no_dummy_year_piv\n", "intent": "** Let's have a close look on ditribution across different years. **\n"}
{"snippet": "wine_df = pd.read_json(api_response.text)\nwine_df.tail(2)\n", "intent": "This sometimes works, but the data may need adjusting\n"}
{"snippet": "train_no_dummy_year_temp_piv = all_no_dummy_interpolate[all_no_dummy_interpolate['train'] == 1].round().pivot_table(values='count',index='temp',columns=['workingday']).dropna()\n", "intent": "** Let's cluster temp and workingday impact on rentals. **\n"}
{"snippet": "spark_train_dummy = sqlContext.createDataFrame(trainingData_dummy)\nspark_test_dummy = sqlContext.createDataFrame(testData_dummy)\nspark_train_no_dummy = sqlContext.createDataFrame(trainingData_no_dummy)\nspark_test_no_dummy = sqlContext.createDataFrame(testData_no_dummy)\n", "intent": "** Create Apache Spark Data Frames ** \n"}
{"snippet": "spark_Kaggle_test_dummy = sqlContext.createDataFrame(all_dummy_interpolate[all_dummy_interpolate['train'] != 1].reset_index())\nspark_Kaggle_test_no_dummy = sqlContext.createDataFrame(all_no_dummy_interpolate[all_no_dummy_interpolate['train'] != 1].reset_index())\n", "intent": "** Prepare Kaggle test data. **\n"}
{"snippet": "DUMMY = True\ntrainingData_dummy = sqlContext.createDataFrame(spark_train_dummy.rdd.map(transformToLabeledPoint), [\"hour_cat\", \"label_count\", \"label_registered\", \"label_casual\", \"features_to_filter\"])\ntestData_dummy = sqlContext.createDataFrame(spark_test_dummy.rdd.map(transformToLabeledPoint), [\"hour_cat\", \"label_count\", \"label_registered\", \"label_casual\", \"features_to_filter\"])\ntestKaggle_dummy = sqlContext.createDataFrame(spark_Kaggle_test_dummy.rdd.map(transformToLabeledPointKaggle), [\"hour_cat\", \"label_count\", \"label_registered\", \"label_casual\", \"features_to_filter\"])\nchiSq_dummy = sqlContext.createDataFrame(spark_train_dummy.unionAll(spark_test_dummy)\\\n                                         .rdd.map(transformToLabeledPointChiSqSelector), [\"hour_cat\", \"label_count_cat\", \"label_registered_cat\", \"label_casual_cat\", \"features_to_filter\"])\n", "intent": "** Preparation Apache Spark data frame for linear regression **\n"}
{"snippet": "DUMMY = False\ntrainingData_no_dummy = sqlContext.createDataFrame(spark_train_no_dummy.rdd.map(transformToLabeledPoint), [\"hour_cat\", \"label_count\", \"label_registered\", \"label_casual\", \"features_to_filter\"])\ntestData_no_dummy = sqlContext.createDataFrame(spark_test_no_dummy.rdd.map(transformToLabeledPoint), [\"hour_cat\", \"label_count\", \"label_registered\", \"label_casual\", \"features_to_filter\"])\ntestKaggle_no_dummy = sqlContext.createDataFrame(spark_Kaggle_test_no_dummy.rdd.map(transformToLabeledPointKaggle), [\"hour_cat\", \"label_count\", \"label_registered\", \"label_casual\",\"features_to_filter\"])\nchiSq_no_dummy = sqlContext.createDataFrame(spark_train_no_dummy.unionAll(spark_test_no_dummy)\\\n                                         .rdd.map(transformToLabeledPointChiSqSelector), [\"hour_cat\", \"label_count_cat\", \"label_registered_cat\", \"label_casual_cat\", \"features_to_filter\"])\n", "intent": "** Preparation Apache Spark data frame for random forest and gradient boosted regression. No need for dummy variables. **\n"}
{"snippet": "kaggleSubmission_sum.to_csv('../data/kaggleSubmission_sum.csv', index=False)\nkaggleSubmission_count.to_csv('../data/kaggleSubmission_count.csv', index=False)\n", "intent": "** Kaggle result: 0.42860 **\n"}
{"snippet": "c = pd.DataFrame({'VarA':['aa','bb'], 'VarB':[22.2,33.3]}, \\\n                 index = ['Case1','Case2'])\nc\n", "intent": "DataFrames are designed to store heterogeneous multivariate data.\n"}
{"snippet": "import pandas as pd\nimport seaborn as sn\nd = pd.read_json('http://api.citybik.es/bicing.json')   \nd.info()\n", "intent": "We can read online data from public API's such as http://www.citybik.es/\n"}
{"snippet": "kids = pd.read_csv(\"http://www.mosaic-web.org/go/datasets/kidsfeet.csv\")\nkids.shape\n", "intent": "The <code>ix[]</code> can also be used to get a subset of the dataframe:\n"}
{"snippet": "df = pd.read_csv(\"assets/datasets/iris.csv\")\nprint df.Name.value_counts()\ndf.head(5)\n", "intent": "Let's do some clustering with the iris dataset.\n"}
{"snippet": "c = pd.DataFrame({'VarA':['aa', np.nan, 'cc'], 'VarB':[20,30,55], 'VarC':[1234, 3456, 6789]}, \\\n                 index = ['Case1','Case2','Case3'])\nc = c.fillna(\"\")\nc\n", "intent": "Another option od to use the ``fillna`` function to fillthem with empty strings:\n"}
{"snippet": "import pandas as pd\nunames = ['user_id', 'gender', 'age', 'occupation', 'zip']\nusers = pd.read_table('files/ml-1m/users.dat', sep='::', header=None, names=unames, engine='python')\nrnames = ['user_id', 'movie_id', 'rating', 'timestamp']\nratings = pd.read_table('files/ml-1m/ratings.dat', sep='::', header=None, names=rnames, engine='python')\nmnames = ['movie_id', 'title', 'genres']\nmovies = pd.read_table('files/ml-1m/movies.dat', sep='::', header=None, names=mnames, engine='python')\n", "intent": "We can read the database:\n"}
{"snippet": "data = pd.read_csv('files/AirPassengers.csv')\nprint data.head()\nprint '\\n Data Types:'\nprint data.dtypes\n", "intent": "Now, we can load the data set and look at some initial rows and data types of the columns:\n"}
{"snippet": "df2 = pd.read_csv('data/BattingAverage.csv', usecols=[0,1,2,3], dtype={'PriPos':'category'})\ndf2['BatAv'] = df2.Hits.divide(df2.AtBats)\ndf2.info()\n", "intent": "See also section 9.5.1\n"}
{"snippet": "ftransform = lambda x: 1./ (x + 0.001)\nfor var in range(13):\n    X_train1, X_test1, y_train1, y_test1 = train_test_split(bostonhp[:,[var]], target, test_size=0.2, random_state=42)\n    X_train2, X_test2, y_train2, y_test2 = train_test_split(ftransform(bostonhp[:,[var]]), target, test_size=0.2, random_state=42)\n    lin_mod1 = LinearRegression(fit_intercept=True)\n    lin_mod1.fit(X_train1,y_train1)\n    lin_mod2 = LinearRegression(fit_intercept=True)\n    lin_mod2.fit(X_train2,y_train2)\n    print(var,\"R^2 = {0}\".format(lin_mod1.score(X_test1,y_test1)), \" transf: R^2 = {0}\".format(lin_mod2.score(X_test2,y_test2)))\n", "intent": "We will now see that applying a simple transformation to some variables, their correlation with the target (measured with $R^2$) increases.\n"}
{"snippet": "ftransform = lambda x: 1./ (x + 0.001)\nvariable_list = [bostonhp[:,5], bostonhp[:,12], ftransform(bostonhp[:,12]), bostonhp[:,10]]\nmulti_variable = np.zeros((bostonhp.shape[0], len(variable_list)))\nfor i in range(len(variable_list)):\n    multi_variable[:,i] = variable_list[i] \n    X_train, X_test, y_train, y_test = train_test_split(multi_variable, target, test_size=0.2, random_state=42)\n    multi_lin_mod = LinearRegression(fit_intercept=True)\n    multi_lin_mod.fit(X_train, y_train)\n    print \"num_var %d, my score %f\" %(0 + i ,multi_lin_mod.score(X_test, y_test))\n", "intent": "In this first multiple varible regression we select 4 variable which are considered the most correlated with our target\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(reb_text, reb_target, test_size=0.2, random_state=42)\n", "intent": "Now we slpit our balanced dataset in Training and Test parts.\n"}
{"snippet": "df = pd.DataFrame()\ndf['ship_type'] = np.random.choice([\"romulan\", \"human\", \"klingon\", \"borg\",'red_shirt','ovid'], size=500)\ndf['baths'] = np.random.choice(np.arange(1, 4, 0.5), size=500)\ndf['ship_value'] = np.random.randint(200000, 10000000, size=500)\ndf['speed'] = np.random.randint(10,60, size=500)\n", "intent": "With 2-3 continious variables, and one categorical.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncolumns = ['ship_value','speed']\ndf[columns] = scaler.fit_transform(df[columns])\nformulas = 'baths ~ ship_value + speed ' \ny, x = patsy.dmatrices(formula, data=df, return_type='dataframe')\nlm = LinearRegression()\nmodel = lm.fit(x,y)\nscore = model.score(x,y)\nprint 'R^2', score\n", "intent": "More code will be written...\n"}
{"snippet": "X_scaled = preprocessing.MinMaxScaler().fit_transform(df[cols])\n", "intent": "Next, since each of our features have different units and ranges, let's do some preprocessing:\n"}
{"snippet": "scaler = StandardScaler()\nsvm_clf1 = LinearSVC(C=1, loss=\"hinge\")\nscaled_svm_clf1 = Pipeline((\n(\"scaler\", scaler),\n    (\"linear_svc\", svm_clf1)\n))\nscaled_svm_clf1.fit(X,y)\n", "intent": "Step 2: Do feature scaling of the features using StandardScaler() and model the SVM Linear classifier\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidVectorizer = TfidfVectorizer()\ntext_vectorized = tfidVectorizer.fit_transform(df.text)\ntext_vectorized_array = text_vectorized.toarray()\n", "intent": "see http://www.gadatascience.com/modeling/text_processing.html\n"}
{"snippet": "feature_importances = pd.DataFrame(dt.feature_importances_,\n                                   index = X.columns,\n                                    columns=['importance'])\nfeature_importances.sort_values(by='importance', ascending=False).head()\n", "intent": "We can calculate this in sklearn\n"}
{"snippet": "dt = DecisionTreeClassifier()\ndt.fit(X, y)\nimportances = pd.DataFrame(zip(dt.feature_importances_,rf.feature_importances_,),\n                           index=X.columns, columns=['dt_importance','rf_importance']).sort_values('rf_importance',ascending=False)               \nimportances.head()\n", "intent": "Let's compare the 2 models (re-init Decision Tree with no max depth constraint)\n"}
{"snippet": "black_holes = parser.parse(\"../data/ScienceDirect/black_holes.dat\")\nall_abstracts = abstracts + black_holes\ndata = pd.DataFrame()\ndata['abstracts'] = all_abstracts\n", "intent": "The TfdifVectorizer didn't seem to change anything. I am going to download more data and see what happens with different abstracts involved \n"}
{"snippet": "conf = pd.DataFrame()\nconf = res.conf_int()\nconf['OR'] = res.params\nprint(conf)\nprint(np.exp(conf))\n", "intent": "hint 1: np.exp(X)\nhint 2: conf['OR'] = params\n           conf.columns = ['2.5%', '97.5%', 'OR']\n"}
{"snippet": "from sklearn.datasets import load_iris\ndata = load_iris()\nX = data.data\ny = data.target\nprint(type(X))\nprint(X.shape)\nprint(\"First three rows of data\\n {}\".format(X[:3]))\nprint(\"First three labels: {}\".format(y[:3]))\nprint(data.DESCR)\n", "intent": "The data set is distributed with sci-kit learn, the only thing we have to do is to important a function and call it.\n"}
{"snippet": "DATA_ROOT = r\"~/Dropbox/DATA/Pawal Model Pipeline Data/data\"\ndf = pd.read_csv(os.path.join(DATA_ROOT,\"ebaytitles.csv\"))\ndf.head()\n", "intent": "Read csv file into a dataframe\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nsmall_X_tr = X_tr[1:10]\nX_train_counts = count_vect.fit_transform(small_X_tr)\nprint(X_train_counts.shape)\n", "intent": "Now fit it to a Vectorizer\n"}
{"snippet": "data = {'f1': [0.1, 2.1, 0.1], 'f2': [1.5, 5.7, 2.1], 'f3': [6, 33, 41], 'f4': ['red', 'blue', 'green']}\ndf = pd.DataFrame(data)\ndf\n", "intent": "```python\npd.get_dummies()\n```\n"}
{"snippet": "gbt_sort = pd.DataFrame({'pred':train_scores_gbt[:,1], 'y':y_train}).reset_index(drop=1)\ngbt_sort = gbt_sort.sort_values(by='pred')\ngbt_sort.head()\n", "intent": "Create sorted pred and y df\n"}
{"snippet": "from sklearn.datasets import make_hastie_10_2\nfrom sklearn.ensemble import GradientBoostingClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nclf_1 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n    max_depth=1, random_state=0).fit(X_train, y_train)\n", "intent": "Gradient Boosting Classifier\n"}
{"snippet": "df_loandata.to_csv('LoanData_Cleansed.csv')\n", "intent": "- includes target and feature variables\n"}
{"snippet": "df = pd.read_csv('knn-project-data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ntrain_Y = label_encoder.fit_transform([y for _,y in train_filtered])\ndev_Y = label_encoder.transform([y for _,y in dev_filtered])\n", "intent": "Next we convert the labels using the `LabelEncoder`.\n"}
{"snippet": "import pandas as pd\npd.DataFrame(data, columns = [\"x\",\"y\"])\n", "intent": "Consider loss on this data:\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\nvectorizer = DictVectorizer()\ntrain_X = vectorizer.fit_transform([feats(x) for x,_ in train_filtered])\ndev_X = vectorizer.transform([feats(x) for x,_ in dev_filtered])\ndev_X[0]\n", "intent": "Apply to training and test instances:\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ntrain_Y = label_encoder.fit_transform([y for _,y in train_filtered])\ndev_Y = label_encoder.transform([y for _,y in dev_filtered])\ndev_Y[:10]\n", "intent": "scikit-learn wants prefers numbers as classes:\n* $\\text{positive}\\rightarrow 0$\n* $\\text{negative}\\rightarrow 1$\n"}
{"snippet": "boston = load_boston()\nX = boston.data \ny = boston.target \ncolNames = boston.feature_names \n", "intent": "Let's try it on the Boston data set\n"}
{"snippet": "from sklearn.datasets import load_iris\ndata = load_iris()\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "ridge_models = []\nfor alpha in ridge_alphas:\n    scaler = XyScaler()\n    scaler.fit(X_train.values, y_train.values)\n    X_train_std, y_train_std = scaler.transform(X_train.values, y_train.values)\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(X_train_std, y_train_std)\n    ridge_models.append(ridge)\n", "intent": "First, let's build up an array of fit models.\n"}
{"snippet": "X, y = make_classification(\n    n_samples=100,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_classes=2,\n    random_state=0)\n", "intent": "**Q:** Generate a dataset using sklearn's make_classification module.\n"}
{"snippet": "data = np.genfromtxt('data/part3_nonseparable.csv', delimiter=',')\nX3 = data[:,1:3]\ny3 = data[:,3]\nX3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.3, random_state=300)\nplot_data_basic(X3,y3)\n", "intent": "1\\. Load the file `data/part3_nonseparable.csv` into a dataframe.\n"}
{"snippet": "npts = 10000\ny = scs.bernoulli(0.2).rvs(npts)\nX = pd.DataFrame({'a':scs.norm(0, 1).rvs(npts) + 0.3*y,\n                  'b':scs.poisson(2*y + 2).rvs(npts)})\n", "intent": "Let's start with some (fake) data, with two features (one continuous, one integer) and a binary label.\n"}
{"snippet": "Ybdf = pd.DataFrame(Yb)\n", "intent": "Let's compare SVM to logistic regression.\nFirst make the data \"shittier\" since its doing so well.\n"}
{"snippet": "df113 = pd.read_csv(path % 113)\ndf114 = pd.read_csv(path % 114)\ndf115 = pd.read_csv(path % 115)\n", "intent": "Once the above batches have been run and fitted, we can further test it by computing the AUC score for the remaining batches: 113, 114, and 115.\n"}
{"snippet": "testdf.fillna(0, inplace=True)\nY = testdf.player_won.values\nX = testdf[[c for c in testdf.columns if not c == 'player_won']].values\n", "intent": "Here we simply fill the concatenated dataframe's `NaN` values with 0 and instantiate the target variable, Y, and the predictor variable, X. \n"}
{"snippet": "import json\nsu['title'] = su.boilerplate.map(lambda x: json.loads(x).get('title', ''))\nsu['title'].fillna('', inplace=True)\n", "intent": "You will need to parse the json from the boilerplate field.\n---\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features = 5000,\n                             ngram_range=(1, 2),\n                             stop_words='english',\n                             binary=True)\n", "intent": "It is up to you what range of ngrams and features, and whether or not you want the columns binary or counts.\n---\n"}
{"snippet": "votes = pd.read_csv('../assets/datasets/votes.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "su['title'] = su.boilerplate.map(lambda x: json.loads(x).get('title', ''))\nsu['body'] = su.boilerplate.map(lambda x: json.loads(x).get('body', ''))\ntitles = su['title'].fillna('')\nbody = su['body'].fillna('')\ntitles[0:5]\n", "intent": "You will need to parse the json from the boilerplate field.\n---\n"}
{"snippet": "title_vectorizer = CountVectorizer(max_features = 1000, ngram_range = (1, 2), stop_words = 'english', binary = True)\ntitle_vectorizer.fit(titles)\n", "intent": "It is up to you what range of ngrams and features, and whether or not you want the columns binary or counts.\n---\n"}
{"snippet": "movies = pd.DataFrame(imdb.top_250(), columns = ['num_votes', 'rating', 'tconst', 'title', 'year'])\n", "intent": "Keep the fields:\n    num_votes\n    rating\n    tconst\n    title\n    year\nAnd discard the rest\n"}
{"snippet": "from sklearn.feature_extraction.text import HashingVectorizer\nhvec = HashingVectorizer()\nhvec.fit([spam])\n", "intent": "> \n>\n    from sklearn.feature_extraction.text import HashingVectorizer\n    hvec = HashingVectorizer()\n    hvec.fit([spam])\n"}
{"snippet": "df = pd.DataFrame(data=mtcars)\ncar = pd.DataFrame(data=mtcars, columns=['Car'])\n", "intent": "Convert to a Pandas Dataframe\n"}
{"snippet": "pca = pd.DataFrame(pca)\npca.head()\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_all, all_tweets['text'], test_size = 0.3)\n", "intent": "Double check that you are getting random data before moving forward.  What would happen if you over sample Trump more than Sanders?\n"}
{"snippet": "cvt = CountVectorizer()\nX_all = cvt.fit_transform(insults_df[\"Comment\"]).toarray()\nsummaries = \"\".join(insults_df['Comment'])\ncount_insults = cvt = cvt.build_analyzer()(summaries)\ncomment_count = []\nfor i in Counter(count_insults).most_common():\n     if i[1] > 50:\n        comment_count.append(i)\ncomment_count\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "cvt = CountVectorizer(ngram_range = (2, 4), stop_words=\"english\")\nX_all = cvt.fit_transform(insults_df[\"Comment\"])\nsummaries = \"\".join(insults_df['Comment'])\ncount_insults = cvt = cvt.build_analyzer()(summaries)\ncomment_count = []\nfor i in Counter(count_insults).most_common():\n     if i[1] > 50:\n        comment_count.append(i)\ncomment_count\n", "intent": "Display the top 75 ngrams with frequencies.\n"}
{"snippet": "PCA_set = PCA(n_components=5)\nX = PCA_set.fit_transform(xStand)\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "degree = 300\nsimple_model = make_pipeline(MinMaxScaler((0, 0.5)), PolynomialFeatures(degree), LinearRegression())\nsimple_model.fit(X_train.reshape((X_train.shape[0], 1)), Y_train)\n", "intent": "Let's fit a degree 300 polynomial to our data, as you can already imagine, the high degree is over-kill and will lead to overfitting.\n"}
{"snippet": "df_mms = p.MinMaxScaler().fit_transform(df)\n", "intent": "`3.` Now it's your turn.  Try fitting the **MinMaxScaler** transformation to this dataset. You should be able to use the previous example to assist.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n", "intent": "Now let's do the train/test split.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"C:/Users/kohleggermichael/OneDrive/Teaching/[WCIS] CRM und Information Mining II/Block-1/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "path = Path('..', 'data', 'vehicles_train.csv')\ntrain = pd.read_csv(path)\ntrain['vtype'] = train.vtype.map({'car':0, 'truck':1})\ntrain\n", "intent": "<a id=\"manual-bagged\"></a>\n"}
{"snippet": "df = pd.read_csv('titanic-train.csv')\n", "intent": "Load the csv file into memory using Pandas\n"}
{"snippet": "df['Age'].fillna(median_age, inplace = True)\ndf.info()\n", "intent": "impute the missing values for Age using the median Age\n"}
{"snippet": "survival_by_gender = df[['Sex','Survived']].pivot_table(columns =\n                        ['Survived'], index = ['Sex'], aggfunc=len)\nsurvival_by_gender\n", "intent": "Check the influence of Sex on Survival\n"}
{"snippet": "survival_by_Pclass = df[['Pclass','Survived']].pivot_table(columns =\n                        ['Survived'], index = ['Pclass'], aggfunc=len)\nsurvival_by_Pclass\n", "intent": "Check the influence of Pclass on Survival\n"}
{"snippet": "xStand = StandardScaler().fit_transform(x2)\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "average_age = data.Age.mean()\ndata.Age = data.Age.fillna(average_age)\nprint data.info()\n", "intent": "Fill missing values for age with average\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nf_train, f_test, t_train, t_test = train_test_split(features, target, test_size = .2, random_state = 0)\n", "intent": "Split the data into training and test sets\n"}
{"snippet": "test_data = pd.read_csv('test.csv')\nprint test_data.head()\nprint test_data.info()\nprint test_data.describe()\n", "intent": "Create a DataFrame with the test.csv data\n"}
{"snippet": "test_data.Sex = test_data.Sex.replace(['male','female'],[True,False])\ntest_data.Age = test_data.Age.fillna(average_age)\ntest_data.info()\n", "intent": "Clean the test data\n"}
{"snippet": "kaggle = test_data[['PassengerId','Survived']]\nkaggle.to_csv('kaggle_titanic_submission.csv', index=False)\n", "intent": "Save predictions in Kaggle submissions format - PassengerId and Survived (Hint - remember to set index=False in the to_csv function)\n"}
{"snippet": "seed = 7\nnp.random.seed(seed)\ntrain_path = \"C:/deep_learning/kaggle/amazon/train/\"\ntest_path = \"C:/deep_learning/kaggle/amazon/test/\"\nfeat_output_path = \"\"\ntrain = pd.read_csv(\"C:/deep_learning/kaggle/amazon/train_v2.csv\")\ntest =  pd.read_csv(\"C:/deep_learning/kaggle/amazon/sample_submission_v2.csv\")\ntrain.head()\n", "intent": "Set path variables:\n"}
{"snippet": "seed = 7\nnp.random.seed(seed)\ntrain_path = \"C:/deep_learning/kaggle/amazon/train/\"\ntest_path = \"C:/deep_learning/kaggle/amazon/test/\"\nfeat_output_path = \"C:/deep_learning/kaggle/amazon/feat/\"\ntrain = pd.read_csv(\"C:/deep_learning/kaggle/amazon/train_v2.csv\")\ntest =  pd.read_csv(\"C:/deep_learning/kaggle/amazon/sample_submission_v2.csv\")\ntrain.head()\n", "intent": "Set path variables:\n"}
{"snippet": "random_seed = 0\nrandom.seed(random_seed)\nnp.random.seed(random_seed)\ntrain_path = \"/media/alex/B44254FE4254C730/Users/Alex/Downloads/Kaggle Data/train-jpg/train-jpg/\"\ntest_path = \"/media/alex/B44254FE4254C730/Users/Alex/Downloads/Kaggle Data/test-jpg/\"\ntrain = pd.read_csv(\"/home/alex/Desktop/Rainforest/Data/train_v2.csv\")\ntest =  pd.read_csv(\"/home/alex/Desktop/Rainforest/Data/sample_submission_v2.csv\")\nobj_save_path = \"/home/alex/Desktop/Rainforest/Models/XGB/Objects/\"\nsubm_output_path = \"/home/alex/Desktop/Rainforest/Submissions/\"\n", "intent": "Set path variables:\n"}
{"snippet": "bunchobject = datasets.load_iris()\nfeature_index = [0,2]\nmy_data = bunchobject.data[:,feature_index]\n", "intent": "I chose the Iris dataset to illustrate this because the groupings are really obvious.\n"}
{"snippet": "airports_pca = PCA(n_components=3)\nairports_pca.fit(xStand)\nX = airports_pca.transform(xStand)\nX\n", "intent": "Finally, conduct the PCA - use the results above to guide your selection of n components\n"}
{"snippet": "def bar_chart(feature):\n  survived = train[train['Survived']==1][feature].value_counts()\n  dead = train[train['Survived']==0][feature].value_counts()\n  df = pd.DataFrame([survived, dead])\n  df.index = ['Survuved','Dead']\n  df.plot(kind='bar', stacked=True, figsize=(10,5))\n", "intent": "- Pclass\n- Sex\n- SibSp (\n- Parch (\n- Embarked\n- Cabin\n"}
{"snippet": "for dataset in train_test_data:\n  dataset['Embarked'] = dataset['Embarked'].fillna('S')\n", "intent": "more than 50% of 1st class are from S embark\nmore than 50% of 2nd class are from S embark\nmore than 50% of 3rd class are from S embark\n"}
{"snippet": "X_temp, X_test, y_temp, y_test = train_test_split(ingredients_weighted, data.cuisine, test_size=0.2, random_state=42)\nX_train, X_CV, y_train, y_CV = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\nprint('Number of training entries: {0} -> {1:.0f}% of data'.format(X_train.shape[0], 100*X_train.shape[0]/data.shape[0]))\nprint('Number of CV entries: {0} -> {1:.0f}% of data'.format(X_CV.shape[0], 100*X_CV.shape[0]/data.shape[0]))\nprint('Number of test entries: {0} -> {1:.0f}% of data'.format(X_test.shape[0], 100*X_test.shape[0]/data.shape[0]))\n", "intent": "Now that the sparse matrix is created, we will select splits for training, CV and test sets for our models.\n"}
{"snippet": "print(train.shape)\ncate = [f for f in train.columns if train.dtypes[f] =='O']\nfor c in cate:\n    train[c] = train[c].astype('category')\n    if train[c].isnull().any():\n        train[c] = train[c].cat.add_categories(['MISSING'])\n        train[c] = train[c].fillna('MISSING')\n", "intent": "look at the distribution of all categorical features\n"}
{"snippet": "features_missing = missing_data.index[missing_data['Total']>0].tolist()\nfor col in features_missing:\n    if full_dataset[col].dtypes=='O':\n        full_dataset[col] = full_dataset[col].fillna(full_dataset[col].mode()[0])\n    else:\n        full_dataset[col] = full_dataset[col].fillna(full_dataset[col].mean())\n", "intent": "We use MODE to fill catigorical features and MEAN to fill numerical features.\n"}
{"snippet": "train = full_dataset.loc['train']\ntest = full_dataset.loc['test']\ndummy_full = pd.get_dummies(full_dataset)\ntrain_dum = dummy_full.loc['train']\ntest_dum = dummy_full.loc['test']\nscaler = StandardScaler().fit(train_dum) \ntrain_scaled = scaler.transform(train_dum)\ntest_scaled = scaler.transform(test_dum)\nprint('The shape of dummy full dataset', dummy_full.shape)\n", "intent": "1) feature engineering\n    Transforming numerical features:\n    1. get dummy variables\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nfor c in catigorical_features:\n    lbl = LabelEncoder() \n    lbl.fit(list(full_data[c].values)) \n    full_data[c] = lbl.transform(list(full_data[c].values))\nprint('Shape all_data: {}'.format(full_data.shape))\n", "intent": "    4.lable encoder: Label Encoding some categorical variables that may contain information in their ordering set\n"}
{"snippet": "def pca_choose_feature(N_components, train, test):\n    Data_pca = PCA(n_components=N_components).fit(train)\n    train_pca = Data_pca.transform(train) \n    test_pca = Data_pca.transform(test)\n    return train_pca, test_pca\ntrain_pca_20, test_pca_20 = pca_choose_feature(20, train_scaled, test_scaled)\ntrain_pca_40, test_pca_40 = pca_choose_feature(40, train_scaled, test_scaled)\ntrain_pca_60, test_pca_60 = pca_choose_feature(60, train_scaled, test_scaled)\ntrain_pca_80, test_pca_80 = pca_choose_feature(80, train_scaled, test_scaled)\ntrain_pca_100, test_pca_100 = pca_choose_feature(100, train_scaled, test_scaled)\n", "intent": "2) feature selection: <BR>\n    1. PCA: select 20, 40, 60, 80, 100 features using Principle Component Analysis.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom patsy import dmatrix\nfeatures_df = dfrttf[['CA','MDW','NE','PNW','SE','SW','TX','Junior','Senior']]\ntarget_df = dfrttf['high_salary']\nX_train, X_test, Y_train, Y_test = train_test_split(features_df, target_df, \n                                                    test_size=0.33, random_state=5)\nscaler = StandardScaler() \nX_train_s = scaler.fit_transform(X_train)\nX_test_s = scaler.fit_transform(X_test)\n", "intent": "- You can either create the dummy features manually or use the `dmatrix` function from `patsy`\n- Remember to scale the feature variables as well!\n"}
{"snippet": "dfair = pd.read_csv('/Users/ajbentley/GA-DSI/curriculum/week-07/3.2-pca-lab-1/assets/datasets/Airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "def dummyEncode(df):\n        names_of_columns_to_transform = list(df.select_dtypes(include=['category','object']))\n        le = LabelEncoder()\n        for feature in names_of_columns_to_transform:\n            try:\n                df[feature] = le.fit_transform(df[feature])\n            except:\n                print('Error encoding '+feature)\n        return df\n", "intent": "source: http://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python\n"}
{"snippet": "bike = pd.read_csv('bike_rentals.csv')\nbike.head()\n", "intent": "We can also use categorical variables as predictors in logistic regressions, we just have to turn them into dummy features.\n"}
{"snippet": "bike = pd.read_csv('bike_rentals.csv')\nbike.head()\n", "intent": "Let's recap using a straight line to predict rentals using multiple input variables\n"}
{"snippet": "import pandas as pd\nhr = pd.read_csv('HR.csv')\nhr.head()\n", "intent": "Use either SVM or Naive Bayes to correctly classify at risk employees.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "Now, let's split the data into train and test.\n"}
{"snippet": "df_usa_today = pd.read_csv(\"data/Trump_AND_China_6742.csv\",sep=\"\\t\",header=None,parse_dates=[0]).sample(150)\ndf_usa_today[1] = \"USA Today\" \ndf_ny_times = pd.read_csv(\"data/Trump_AND_China_8213.csv\",sep=\"\\t\",header=None,parse_dates=[0]).sample(150)\ndf_ny_times[1] = \"New York Times\"\ndf_ny_post = pd.read_csv(\"data/Trump_AND_China_164207.csv\",sep=\"\\t\",header=None,parse_dates=[0]).sample(150)\ndf_ny_post[1] = \"New York Post\"\ndf_ny_post.head()\n", "intent": "**Read the data we just downloaded**\n"}
{"snippet": "test_size = 0.30 \nseed = 2  \nxTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=test_size, random_state=seed)\n", "intent": "We would get a different result if we split the dataset randomly in ratio 70:30.\n"}
{"snippet": "advertising = pd.read_csv('../data/Advertising.csv', usecols=[1, 2, 3, 4])\nadvertising.info()\n", "intent": "Datasets available on http://www-bcf.usc.edu/~gareth/ISL/data.html\n"}
{"snippet": "PATH_TO_DATA = ('')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "airports_pca = PCA(n_components=3)\nairports_pca.fit(xStand)\nX = airports_pca.transform(xStand)\n", "intent": "Finally, conduct the PCA - use the results above to guide your selection of n components\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "Comparing machine learning models in scikit-learn\n"}
{"snippet": "col_names = (\"Sample code number\", \"Clump Thickness\", \"Uniformity of Cell Size\",\"Uniformity of Cell Shape\",\n\"Marginal Adhesion\",\n\"Single Epithelial Cell Size\",\n\"Bare Nuclei\",\n\"Bland Chromatin\",\n\"Normal Nucleoli\",\n\"Mitoses\",\n\"Class\")\ndf=pd.read_csv(\"../../assets/datasets/breast-cancer-wisconsin.csv\",names=col_names)\ndf\n", "intent": "The column names are taken from the dataset info file. Create an array\nwith the column names and assign them as the header when loading the\ncsv.\n"}
{"snippet": "sac = pd.read_csv('../../assets/datasets/Sacramentorealestatetransactions.csv')\nsac.head()\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "X_scaled=StandardScaler().fit_transform(X)\nX_scaled\n", "intent": "First, let's standardize the data\n"}
{"snippet": "coeff_df = pd.DataFrame(lm.coef_,X.columns,columns=['Coeffiecients'])\ncoeff_df['Coeffiecients'].values.tolist()\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "nychouses = pd.read_csv(\"allzips.csv\")\n", "intent": "read in zillow data\n"}
{"snippet": "from sklearn import metrics, cross_validation\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(titanic_short.values, \n                                                            titanic[\"Survived\"], test_size=0.33, random_state=42)\n", "intent": "cross validation will give me a better sense of the score. if the score on test and train is very different im overfitting\n"}
{"snippet": "path = 'https://serv.cusp.nyu.edu/~cq299/ADS2016/Data/Bayesian/'\ndata=pd.read_csv(path+\"example3.csv\")\ndata\nX=np.matrix(data.iloc[:,:-1])\ny=np.asarray(data.Y)\n", "intent": "http://scikit-learn.org/stable/modules/grid_search.html\n"}
{"snippet": "from sklearn.decomposition import PCA, NMF\nnmf = NMF(n_components=10)\nX2 = nmf.fit(x_scaled).transform(x_scaled)\nX2 = nmf.inverse_transform(X2)\n", "intent": "We can use non-negative matrix factorization for unsupervised extraction of consistent patterns in the data.\n"}
{"snippet": "iris = pd.read_csv('/Users/ajbentley/GA-DSI/curriculum/week-07/2.4-dimensionality-reduction/assets/datasets/iris.csv')\niris.head()\n", "intent": "Use an SVM, GridSearchCV and the iris dataset to find an optimal model.\nMake sure to search for optimal paramaters for kernel, C, \n"}
{"snippet": "scaler_model = MinMaxScaler()\n", "intent": "scale_model is an instance of MinMaxScaler()\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "train : test = 7 : 3, random_state is just random seed\n"}
{"snippet": "scale_model = MinMaxScaler()\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "adjacencies = pd.read_csv('/Users/bramvandesande/Projects/lcb/resources/rscenic/1.5_GENIE3_linkList.txt',\n                          usecols=['TF', 'Target', 'weight'])\nadjacencies.columns = [\"TF\", \"target\", \"importance\"]\n", "intent": "------\nStart from the GENIE3 output and check if derived co-expression modules are similar.\n"}
{"snippet": "r_df = pd.read_csv(os.path.join(RESOURCES_FOLDER, \"rscenic/2.4_motifEnrichment_selfMotifs.txt\"))\nlen(r_df)\n", "intent": "Load R table of enriched motifs and convert it to a comparable format.\n"}
{"snippet": "py_df = pd.read_csv(os.path.join(RESOURCES_FOLDER, \"regulomes_zeisel_2015.csv\"),\n                 index_col=[0,1], header=[0,1], skipinitialspace=True)\nlen(py_df)\n", "intent": "Load python table of enriched motifs and convert it to a comparable format.\n"}
{"snippet": "r_incidences = pd.read_csv(os.path.join(RESOURCES_FOLDER, \"rscenic/2.6_regulons_asIncidMat.txt\"), index_col=0)\n", "intent": "Create regulomes for R pipeline.\n"}
{"snippet": "df = pd.read_csv('KNN_Project_Data', index_col=0)\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "df.pivot_table(index=['country_name'], values=['status_num']).sort_values(by=['status_num'], ascending=False)\n", "intent": "<h3> This tells me to expect a significant difference in funding rates across sectors </h3>\n"}
{"snippet": "dfn_r = pd.DataFrame(dfn)\ndfn_r.isnull().sum()\n", "intent": "Now the data is all set and we're ready to model.\nThere's definitely going to be a ton of colinearity among these features.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\ndef label_encode_col(col):\n    le = LabelEncoder()\n    return le.fit_transform(train_[col])    \n", "intent": "* normalize labels using `LabelEncoder`\n"}
{"snippet": "from sklearn.cluster import KMeans\nkmeans_model = KMeans(n_clusters=2, random_state=1)\nsenator_distances = kmeans_model.fit_transform(votes.iloc[:, 3:])\nsenator_distances[:5, :]\n", "intent": "Use the [fit_transform()](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n* Assign the result to senator_distances.\n"}
{"snippet": "scaled_features = scaler.fit_transform(df.drop('TARGET CLASS',axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\nprint('Train size: {}'.format(len(X_train)))\nprint('Test size: {}'.format(len(X_test)))\n", "intent": "Shuffle the data and split it to train and test parts.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(tokenizer=stem_tokenizer)\nvectorizer.fit(X_train)\nprint (vectorizer.transform([sms]))\n", "intent": "Fit a vectorizer which converts texts to count vectors.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit(vectorizer.transform(X_train))\nprint(tfidf_transformer.transform(vectorizer.transform([sms])))\n", "intent": "Convert count vectors to TFIDF\n"}
{"snippet": "import pandas as pd\nreviews = pd.read_csv('../data/en_reviews.csv', sep='\\t', header=None, names =['rating', 'text'])\nreviews[35:45]\n", "intent": "Implement and evaluate a classifier of user reviews using methods described in the NLP tutorial.\n"}
{"snippet": "import pandas as pd\nreviews = pd.read_csv('../data/en_reviews.csv', sep='\\t', header=None, names =['rating', 'text'])\nreviews[35:45]\n", "intent": "Implement and evaluate a classifier of user reviews using Support Vector Machines with RBF kernel. Use the word2vec vectors as features.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(vectors, target, test_size=0.2)\nprint('Train size: {}'.format(len(X_train)))\nprint('Test size: {}'.format(len(X_test)))\n", "intent": "Shuffle the data and split it to train and test parts.\n"}
{"snippet": "dfn_r = pd.DataFrame(dfn)\n", "intent": "Now the data is all set and we're ready to model.\nThere's definitely going to be a ton of colinearity among these features.\n"}
{"snippet": "array = dataset.values\nX=array[:,0:4]\nY=array[:,4]\nvalidation_size=0.20\nseed=7\nX_train,X_validation,Y_train,Y_validation=train_test_split(X,Y,test_size=validation_size,random_state=seed)\nprint (X_train[0:5])\nprint (Y_train[0:5])\n", "intent": "Note the diagonal grouping of some pairs of attributes. This suggests a high correlation and\na predictable relationship.\n"}
{"snippet": "PATH_TO_DATA = ('../../data')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'websites_train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'websites_test_sessions.csv'), index_col='session_id')\nwith open(r\"../../data/site_dic.pkl\", \"rb\") as input_file:\n    site_dict = pickle.load(input_file)\n", "intent": "Reading original data\n"}
{"snippet": "df_scaled = pd.DataFrame(bn_scaled, columns=bn.columns[1:])\ndf_scaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(yelp_class['text'], yelp_class['stars'])\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "sms = pd.read_table('data/sms.tsv', header=None, names=['label', 'message'])\n", "intent": "In this example we are going to train two models to classify SMS as \"Spam\" or \"Ham\".\n"}
{"snippet": "trdf = pd.read_csv('/data/arpit.goel/18_InventoryPrediction/02.ExtractedData/train.csv',header=0)\ntrdf_stores = trdf.merge(vf.drop_duplicates(subset=['Cliente_ID']), how=\"left\")\n", "intent": "Finally, we can apply these new tags on the actual Training and Test data sets that have been provided!\n"}
{"snippet": "trdf.to_csv('/data/arpit.goel/18_InventoryPrediction/04.FeatureEngineering/train_with_cnames.csv')\ntsdf.to_csv('/data/arpit.goel/18_InventoryPrediction/04.FeatureEngineering/test_with_cnames.csv')\n", "intent": "Write the data to file to save it for a new session....\n"}
{"snippet": "from sklearn import model_selection\nX_train, X_test, y_train, y_test = \\\n    model_selection.train_test_split(X, y, test_size=0.3, random_state=17)\n", "intent": "We randomly split the observations into a training set (70% of all data) and a test set (30%).\n"}
{"snippet": "df = pd.read_csv('data/phenotypes.pheno', sep=' ')\ndf_2W = df['2W']\ndf_4W = df['4W']\nsamples_with_phenotype_temp = np.where(pd.isnull(df_2W)!=True)[0].tolist() + np.where(pd.isnull(df_4W)!=True)[0].tolist()\nsamples_with_phenotype = []\nfor s in samples_with_phenotype_temp:\n    if samples_with_phenotype_temp.count(s) > 1 and s not in samples_with_phenotype:\n        samples_with_phenotype.append(s)\n", "intent": "**Note to authors:** we may let a empty cells for the student who will reach this point \n"}
{"snippet": "cancel = pd.read_csv(\"../../projects-weekly/project-07_faa/assets/airport_cancellations.csv\")\nops = pd.read_csv(\"../../projects-weekly/project-07_faa/assets/Airport_operations.csv\")\nap = pd.read_csv(\"../../projects-weekly/project-07_faa/assets/airports.csv\")\n", "intent": "PostgreSQL created in Navicat.\n"}
{"snippet": "pad_sents = keras.preprocessing.sequence.pad_sequences(sents, maxlen=max_len)\nlabel = np.asarray(label, dtype='float32')\n", "intent": "Using max length for each sentense is 200\n"}
{"snippet": "path = '../../datasets/climate'\nfilename = os.path.join(path, 'jena_climate_2009_2016.csv')\nwith open(filename) as fh:\n    lines = fh.read().split('\\n')\n    header = lines[:1][0].split(\",\")\n    lines = lines[1:]\n", "intent": "**Reading the Data**\n"}
{"snippet": "plot_points = np.hstack((X[:, :2], y.reshape(-1, 1)))\nplot_points = pd.DataFrame(plot_points)\n", "intent": "    Adding one more column in numpy array and converting it to pandas dataframe\n"}
{"snippet": "df = pd.read_csv('./data/all_stocks_5yr.csv.zip', compression='zip')\ndf.head()\n", "intent": "The data was downloaded from https://www.kaggle.com/camnugent/sandp500\n"}
{"snippet": "def answer_zero():\n    columns = np.append(cancer.feature_names, 'target');   \n    print(\"Features Column Size: \" + str(np.size(columns)))\n    index = pd.RangeIndex(start=0, stop=569, step=1);\n    data = np.column_stack((cancer.data, cancer.target))\n    print(\"Data Column Size: \" + str(np.size(data) / 569))\n    df = pd.DataFrame(data=data, index=index, columns=columns)\n    return len(cancer['feature_names'])\nanswer_zero() \n", "intent": "How many features does the breast cancer dataset have?\n*This function should return an integer.*\n"}
{"snippet": "sns.regplot('Latency', 'Throughput',\n           data=pd.DataFrame(X, columns=['Latency', 'Throughput']), \n           fit_reg=False,\n           scatter_kws={\"s\":20,\n                        \"alpha\":0.5})\n", "intent": "Visualize training data\n"}
{"snippet": "import pandas as pd\nhouses = pd.read_csv('cal_housing_clean.csv')\n", "intent": "** Import the cal_housing_clean.csv file with pandas. Separate it into a training (70%) and testing set(30%).**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_housing_data = scaler.fit_transform(housing.data)\nscaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n", "intent": "First we will use the standardscaler of sklearn to get a normalized dataset for the gradient descent. \n"}
{"snippet": "housing_prepared = full_pipeline.fit_transform(housing)\nhousing_prepared\n", "intent": "And you can run the whole pipeline simply:\n"}
{"snippet": "PCAdf3 = pd.DataFrame(X3, columns=['PC1','PC2','PC3'])\nj = PCAdf3.PC1\nk = PCAdf3.PC2\nl = PCAdf3.PC3\nPCAdf3.head()\n", "intent": "So it's mostly noise with a little signal in there somewhere. Maybe we need to look at the data a little differently...\n"}
{"snippet": "tf_idf_vectorizer = TfidfVectorizer(vocabulary = corpus_set)\nbow_tfidf = tf_idf_vectorizer.fit_transform(df.pureTextTweet)\nbow_tfidf.shape\n", "intent": "Bag-of-words where tf-idf count:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(train_files,train_targets ,test_size=0.3,random_state=42)\n", "intent": "Split Data into train and test\n"}
{"snippet": "def create_submission(predictions, test_id, loss):\n    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n    now = datetime.datetime.now()\n    if not os.path.isdir('subm'):\n        os.mkdir('subm')\n    suffix = str(round(loss, 6)) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n    sub_file = os.path.join('subm', 'submission_' + suffix + '.csv')\n    result1.to_csv(sub_file, index=False)\n", "intent": "Create submission format\n"}
{"snippet": "from sklearn.datasets import load_boston\nX,Y = load_boston(return_X_y = True)\n", "intent": "To make your life easier you can use the following to load your dataset (you need internet)\n"}
{"snippet": "path='./pycon-2016-tutorial/data/sms.tsv'\nsms = pd.read_table(path, header=None, names=['label', 'message'])\n", "intent": "* vect.fit(train)\n* vect.transform(train)\n* vect.transform(test)\n"}
{"snippet": "with open('anna.txt', 'r') as f:\n    text = f.read()\nvocab = set(text)\n", "intent": "Parse the text into a integer dictionary\n"}
{"snippet": "dfTrain = pd.read_csv(\"train.csv\") \ndfTest = pd.read_csv(\"test.csv\") \n", "intent": "Let's import train and test datasets:\n"}
{"snippet": "dfFull['CabinCat'] = dfFull.Cabin.str[0].fillna('Z')\ndfFull.loc[dfFull.CabinCat=='G','CabinCat']= 'Z'\ndfFull.loc[dfFull.CabinCat=='T','CabinCat']= 'Z'\ndfFull['CabinCat'] = dfFull['CabinCat'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'Z': 6}).astype(int)\ndfTrain = dfFull.loc[1:891,:]\ndfTest = dfFull.loc[892:,:]\ndfTrain.groupby('CabinCat').Survived.mean().plot(kind='bar')\n", "intent": "Now it is better. We have to move last two items to N/A and make 'Z' category:\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndocs = [{\"Mayur\": 1, \"is\": 1, \"awesome\": 2}, {\"No\": 1, \"I\": 1, \"dont\": 2, \"wanna\": 3, \"fall\": 1, \"in\": 2, \"love\": 3}]\ndv = DictVectorizer()\nX = dv.fit_transform(docs)\nprint(X.todense())\n", "intent": "DictVectorizer will convert mappings to vectors. \n"}
{"snippet": "cancel = pd.read_csv(\"../../projects-weekly/project-07/assets/airport_cancellations.csv\")\nops = pd.read_csv(\"../../projects-weekly/project-07/assets/Airport_operations.csv\")\nap = pd.read_csv(\"../../projects-weekly/project-07/assets/airports.csv\")\n", "intent": "PostgreSQL created in Navicat.\n"}
{"snippet": "bureau   = pd.read_csv(DATA_PATH + \"bureau.csv\")\nprint(\"Shape of bureau:\",    bureau.shape)\n", "intent": "---\n<a id=\"bureau\"></a>\n"}
{"snippet": "bureau_balance = pd.read_csv(DATA_PATH + \"bureau_balance.csv\")\nprint(\"Shape of bureau_balance:\",  bureau_balance.shape)\nprint(\"\\nColumns of bureau_balance:\")\nprint(\" --- \".join(bureau_balance.columns.values))\n", "intent": "<a id=\"bureau_bal\"></a>\n"}
{"snippet": "prev_app = pd.read_csv(DATA_PATH + \"previous_application.csv\")\nprint(\"Shape of prev_app:\",  prev_app.shape)\n", "intent": "---\n<a id=\"prev_app\"></a>\n"}
{"snippet": "pcb = pd.read_csv(DATA_PATH + \"POS_CASH_balance.csv\")\nprint(\"Shape of pcb:\",  pcb.shape)\nprint(\"\\nColumns of pcb:\")\nprint(\" --- \".join(pcb.columns.values))\n", "intent": "---\n<a id=\"pos_cash\"></a>\n"}
{"snippet": "for col in (\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\"):\n    pcb[col] = pcb[col].transform(lambda x: x.fillna(x.median()))\n", "intent": "<a id=\"pos_nan\"></a>\n"}
{"snippet": "install_pay = pd.read_csv(DATA_PATH + \"installments_payments.csv\")\nprint(\"Shape of install_pay:\",  install_pay.shape)\nprint(\"\\nColumns of install_pay:\")\nprint(\" --- \".join(install_pay.columns.values))\n", "intent": "---\n<a id=\"install_pay\"></a>\n"}
{"snippet": "for col in (\"DAYS_ENTRY_PAYMENT\", \"AMT_PAYMENT\"):\n    install_pay[col + \"_nan\"] = install_pay[col].map(lambda x: 1 if np.isnan(x) else 0)\n    install_pay[col] = install_pay[col].fillna(0)\n", "intent": "<a id=\"install_nan\"></a>\n"}
{"snippet": "credit_card = pd.read_csv(DATA_PATH + \"credit_card_balance.csv\")\nprint(\"Shape of credit_card:\",  credit_card.shape)\nprint(\"\\nColumns of credit_card:\")\nprint(\" --- \".join(credit_card.columns.values))\n", "intent": "---\n<a id=\"credit\"></a>\n"}
{"snippet": "train = pd.read_csv(DATA_PATH + \"train.csv\")\ntest  = pd.read_csv(DATA_PATH + \"test.csv\")\nprint(\"Shape of train:\", train.shape)\nprint(\"Shape of test:\",  test.shape)\n", "intent": "---\n<a id=\"final_merge\"></a>\n"}
{"snippet": "dfc_corr.to_csv('../../projects-weekly/project-07/assets/dfc_corr.csv')\n", "intent": "With the condensing of airports there are far more highly correlated pairs. I'm going to take this out into Excel to decide where to start pruning.\n"}
{"snippet": "cols = [\"DAYS_CREDIT_ENDDATE\", \"DAYS_ENDDATE_FACT\", \"AMT_CREDIT_MAX_OVERDUE\",\n        \"AMT_CREDIT_SUM\", \"AMT_CREDIT_SUM_DEBT\", \"AMT_CREDIT_SUM_LIMIT\",\n        \"AMT_ANNUITY\"]\nfor col in tqdm(cols):\n    bureau[col + \"_nan\"] = bureau[col].map(lambda x: 1 if np.isnan(x) else 0)\n    mode                 = bureau[bureau[col].notnull()][col].mode().iloc[0]\n    bureau[col]          = bureau[col].fillna(mode)\nsum(bureau.isnull().sum())\n", "intent": "<a id=\"bureau_nan\"></a>\n"}
{"snippet": "pd.DataFrame({\n    \"SK_ID_CURR\": test_id,\n    \"TARGET\": predictions\n}).to_csv(\"../submissions/old_method_mapped_max_days.csv\", index=False)\n", "intent": "---\n<a id=\"save\"></a>\n"}
{"snippet": "DATA_PATH = \"../../data/home_default/\"\nbureau   = pd.read_csv(DATA_PATH + \"bureau.csv\")\nprev_app = pd.read_csv(DATA_PATH + \"previous_application.csv\")\nprint(\"Shape of bureau:\",    bureau.shape)\nprint(\"Shape of prev_app:\",  prev_app.shape)\n", "intent": "---\n<a id=\"load\"></a>\n"}
{"snippet": "bureau   = pd.read_csv(DATA_PATH + \"bureau.csv\")\nprint(\"Shape of bureau:\", bureau.shape)\nprint(\"\\nColumns of bureau:\")\nprint(\" --- \".join(bureau.columns.values))\n", "intent": "---\n<a id=\"bureau\"></a>\n"}
{"snippet": "prev_app = pd.read_csv(DATA_PATH + \"previous_application.csv\")\nprint(\"Shape of prev_app:\",  prev_app.shape)\nprint(\"\\nColumns of prev_app:\")\nprint(\" --- \".join(prev_app.columns.values))\n", "intent": "---\n<a id=\"prev_app\"></a>\n"}
{"snippet": "for col in (\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\"):\n    pcb[col] = pcb[col].transform(lambda x: x.fillna(x.median()))\npcb = pcb.drop(pcb[pcb.NAME_CONTRACT_STATUS.isin([\"XNA\", \"Canceled\"])].index)\n", "intent": "<a id=\"pos_nan\"></a>\n"}
{"snippet": "pcb[\"nan\"] = np.zeros(len(pcb)).astype(int)\nfor col in (\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\"):\n    pcb[\"nan\"] += pcb[col].map(lambda x: 1 if np.isnan(x) else 0)\n    pcb[col]    = pcb[col].transform(lambda x: x.fillna(x.median()))\npcb = pcb.drop(pcb[pcb.NAME_CONTRACT_STATUS.isin([\"XNA\", \"Canceled\"])].index)\n", "intent": "<a id=\"pos_nan\"></a>\n"}
{"snippet": "install_pay[\"nan\"] = np.zeros(len(install_pay)).astype(int)\nfor col in (\"DAYS_ENTRY_PAYMENT\", \"AMT_PAYMENT\"):\n    install_pay[\"nan\"] = install_pay[col].map(lambda x: 1 if np.isnan(x) else 0)\n    install_pay[col]   = install_pay[col].fillna(0)\n", "intent": "<a id=\"install_nan\"></a>\n"}
{"snippet": "train = pd.read_csv(DATA_PATH + \"train.csv\")\ntest  = pd.read_csv(DATA_PATH + \"test.csv\")\nprint(\"Shape of train:\", train.shape)\nprint(\"Shape of test: \", test.shape)\n", "intent": "---\n<a id=\"final_merge\"></a>\n"}
{"snippet": "kml = pd.DataFrame(kmean.labels_)\n", "intent": "Block Delay is the gate-to-gate delay, so the biggest problem. Let's see what airports have the biggest and least problems with this metric.\n"}
{"snippet": "for col in (\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\"):\n    pcb[col] = pcb[col].transform(lambda x: x.fillna(x.median()))\npcb = pcb.drop(pcb[pcb.NAME_CONTRACT_STATUS.isin([\"XNA\", \"Canceled\"])].index)\n", "intent": "<a id=\"pos_process\"></a>\n"}
{"snippet": "for col in (\"DAYS_ENTRY_PAYMENT\", \"AMT_PAYMENT\"):\n    install_pay[col + \"_nan\"] = install_pay[col].map(lambda x: 1 if np.isnan(x) else 0)\n    install_pay[col] = install_pay[col].fillna(0)\n", "intent": "<a id=\"install_process\"></a>\n"}
{"snippet": "training_x, val_x, training_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=17)\nlgb_train = lgb.Dataset(data=training_x, label=training_y)\nlgb_eval  = lgb.Dataset(data=val_x, label=val_y)\nparams = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n          'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,\n          'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, \n          'min_split_gain':.01, 'min_child_weight':1}\nstart = time.time()\nmodel = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=200)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"feat_reduction\"></a>\n"}
{"snippet": "pd.DataFrame({\n    \"SK_ID_CURR\": test_id,\n    \"TARGET\": sub_preds\n}).to_csv(\"../submissions/lambda_40.csv\", index=False)\n", "intent": "<a id=\"final_pred\"></a>\n"}
{"snippet": "pd.DataFrame({\n    \"SK_ID_CURR\": test_id,\n    \"TARGET\": predictions\n}).to_csv(\"../submissions/not_classifier.csv\", index=False)\n", "intent": "<a id=\"final_pred\"></a>\n"}
{"snippet": "pd.DataFrame({\n    \"SK_ID_CURR\": test_id,\n    \"TARGET\": predictions\n}).to_csv(\"../submissions/more_tuning.csv\", index=False)\n", "intent": "<a id=\"final_pred\"></a>\n"}
{"snippet": "pd.DataFrame({\n    \"SK_ID_CURR\": test_id,\n    \"TARGET\": predictions_2\n}).to_csv(\"../submissions/dart.csv\", index=False)\n", "intent": "<a id=\"final_pred\"></a>\n"}
{"snippet": "pd.DataFrame({\n    \"SK_ID_CURR\": test_id,\n    \"TARGET\": predictions\n}).to_csv(\"../submissions/olivier_params.csv\", index=False)\n", "intent": "<a id=\"final_pred\"></a>\n"}
{"snippet": "bureau[[\"STATUS_0\", \"STATUS_1\", \"STATUS_2\", \"STATUS_3\", \"STATUS_4\", \"STATUS_5\", \"STATUS_C\", \"STATUS_X\"]] = (\n    bureau[[\"STATUS_0\", \"STATUS_1\", \"STATUS_2\", \"STATUS_3\", \"STATUS_4\", \"STATUS_5\", \"STATUS_C\", \"STATUS_X\"]]\n        .fillna(-1))\nbureau.isnull().sum()\n", "intent": "There won't be any effect if we replace NaN with 0\n"}
{"snippet": "memdf1 = pd.read_csv(\"../../projects-capstone/PSP/raw_data/PSP_MembershipData_1.csv\")\nmemdf2 = pd.read_csv(\"../../projects-capstone/PSP/raw_data/PSP_MembershipData_2.csv\")\n", "intent": "Step 2. Import Data\n"}
{"snippet": "bureau[[\"STATUS_0\", \"STATUS_1\", \"STATUS_2\", \"STATUS_3\", \"STATUS_4\", \"STATUS_5\", \"STATUS_C\", \"STATUS_X\"]] = (\n    bureau[[\"STATUS_0\", \"STATUS_1\", \"STATUS_2\", \"STATUS_3\", \"STATUS_4\", \"STATUS_5\", \"STATUS_C\", \"STATUS_X\"]]\n        .fillna(-1))\n", "intent": "There won't be any effect if we replace NaN with -1\nFIXME: make this simpler code\n"}
{"snippet": "full[\"no_prev_app\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\nfor col in merged_cols:\n    not_null  = full[col].notnull()\n    median    = full[not_null][col].median()\n    full[col] = full[col].fillna(median)    \n", "intent": "A load bar would be nice here\n"}
{"snippet": "credit_card = pd.read_csv(DATA_PATH + \"credit_card_balance.csv\")\nprint(\"Shape of credit_card:\",  credit_card.shape)\nprint(\"\\nColumns of credit_card:\")\nprint(\" --- \".join(credit_card.columns.values))\n", "intent": "---\n<a id=\"credit\"></a>\nFIXME: select (1) max months of credit history\n"}
{"snippet": "from scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nnumeric_feats = full.dtypes[full.dtypes != \"object\"].index\nskewed_feats = full[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewed_features = skewness[abs(skewness) > 0.75].index\nlam = 0.15\nfor feat in skewed_features:\n    full[feat] = boxcox1p(full[feat], lam)\n", "intent": "All of this skew analysis is thanks to Serginne\n"}
{"snippet": "ensemble = meta_model_pred*(1/10) + final_predictions['XGBoost']*(1.5/10) + final_predictions['Gradient Boosting']*(2/10) + final_predictions['Bayesian Ridge']*(1/10) + final_predictions['Lasso']*(1/10) + final_predictions['KernelRidge']*(1/10) + final_predictions['Lasso Lars IC']*(1/10) + final_predictions['Random Forest']*(1.5/10)\nsubmission = pd.DataFrame()\nsubmission['Id'] = test_ID\nsubmission['SalePrice'] = ensemble\nprint(\"Submission file, created!\")\n", "intent": "<a id='submission'></a>\n"}
{"snippet": "ensemble = meta_model_pred*(1/10) + final_predictions['XGBoost']*(1.5/10) + final_predictions['Gradient Boosting']*(2/10) + final_predictions['Bayesian Ridge']*(1/10) + final_predictions['Lasso']*(1/10) + final_predictions['KernelRidge']*(1/10) + final_predictions['Lasso Lars IC']*(1/10) + final_predictions['Random Forest']*(1.5/10)\nsubmission = pd.DataFrame()\nsubmission['Id'] = test_ID\nsubmission['SalePrice'] = ensemble\nsubmission.to_csv('../../submissions/patel_submission_tuned_params.csv',index=False)\nprint(\"Submission file, created!\")\n", "intent": "<a id='submission'></a>\n"}
{"snippet": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import Lasso\nfrom xgboost import XGBRegressor\nimport pandas as pd\nmaster_training_data = pd.read_csv('../data/house_prices/train.csv')\nmaster_testing_data = pd.read_csv('../data/house_prices/test.csv')\n", "intent": "^ Huge thank you to Steve DeLano for recommending Lasso!  Good to know what not to use\nhttps://www.youtube.com/watch?v=BHGNvgBATN0\n"}
{"snippet": "train.OutcomeSubtype = train.OutcomeSubtype.fillna(\"NaN\")\ntrain[\"Outcome\"] = train.OutcomeType + \" - \" + train.OutcomeSubtype\nprint(\"Number of unique OutcomeSubtypes: \", len(train.OutcomeSubtype.unique()))\nprint(\"Number of unique Outcomes: \", len(train.Outcome.unique()))\n", "intent": "After the missing values are filled in, let's recreate the Column 'Outcome' again\n"}
{"snippet": "training_data[\"Age\"].fillna(training_data.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\ntesting_data[\"Age\"].fillna(testing_data.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\n", "intent": "- It might help to add a column to the table saying which data I affected\n"}
{"snippet": "pca = decomposition.PCA(n_components=3)\ndelay = pca.fit_transform(X[[i for i in X.columns if \"delay\" in i]])\n", "intent": "- With 3 components we can explain 85% of the variance in the variables\n"}
{"snippet": "train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\ntest[\"Age\"].fillna(test.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\n", "intent": "- It might help to add a column to the table saying which data I affected\n"}
{"snippet": "for dataset in full:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n", "intent": "- This rubs me the wrong way though, I think another column should definitely be added\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVC\ntesting_data = pd.read_csv('../data/talking_data/test_reduced.csv')\n", "intent": "- Removing the device column\n- Using more data\n- Using SVC instead of XGBoost\n- Running the model on the reduced data (excluding month and year)\n"}
{"snippet": "start = dt.datetime.now()\nINPUT_SIZE = 224\nNUM_CLASSES = 16\nSEED = 1987\ndata_dir = '../../data/doge/'\nlabels = pd.read_csv(join(data_dir, 'labels.csv'))\nsample_submission = pd.read_csv(join(data_dir, 'sample_submission.csv'))\nprint(len(listdir(join(data_dir, 'train'))), len(labels))\nprint(len(listdir(join(data_dir, 'test'))), len(sample_submission))\n", "intent": "Using all the images would take more than the 1 hour kernel limit. Let's focus on the most frequent 16 breeds.\n"}
{"snippet": "drop_cols = list()\nfor col in train_x.columns:\n    unique_vals = train_x[col].unique()\n    if len(unique_vals) == 1:\n        drop_cols.append(col)\ntrain.drop(drop_cols, axis=1, inplace=True)\ntest.drop(drop_cols, axis=1, inplace=True)\ntrain.to_csv(\"../../data/santander/train_r.csv\")\ntest.to_csv(\"../../data/santander/test_r.csv\")\n", "intent": "<a id=\"boring\"></a>\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\ntraining_set = sc.fit_transform(training_set)\ntraining_set\n", "intent": "Scaling our features using normalization\n"}
{"snippet": "X_train = training_set[0:1257]\ny_train = training_set[1:1258]\ntoday = pd.DataFrame(X_train[0:5])\ntomorrow = pd.DataFrame(y_train[0:5])\nex = pd.concat([today, tomorrow], axis=1)\nex.columns = (['today', 'tomorrow'])\nex\n", "intent": "Now we split our stock prices by shifting the cells one block. That way, the input would be one day and the output would be the very next day.\n"}
{"snippet": "columns = ['item_id', 'movie title', 'release date', 'video release date', 'IMDb URL', 'unknown', 'Action', 'Adventure',\n          'Animation', 'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',\n          'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies = pd.read_csv('ml-100k/u.item', sep='|', names=columns, encoding='latin-1')\nmovie_names = movies[['item_id', 'movie title']]\nmovie_names.head()\n", "intent": "Note count\t100000.00000 from 1.7M movies is very inefficient\n"}
{"snippet": "scaler = preprocessing.StandardScaler()\ndf3 = pd.DataFrame(df2['Average'], index = df2.index)\ndf3 = df3.to_period(freq='M')\ndf3['Average'] = scaler.fit_transform(df3['Average'])\ndf4 = pd.DataFrame(df['Nacional'], index = df.index)\ndf4 = df4.to_period(freq='M')\ndf4['Nacional'] = scaler.fit_transform(df4['Nacional'])\ndf5 = df4.join(df3,rsuffix='_y')\ndf5.plot()\n", "intent": ">JB Pork shot up in 2013 and stayed high in 2014. However, looks like corn dropped during that time! What's your hypothesis in light of this?\n"}
{"snippet": "pca2 = decomposition.PCA(n_components=1)\ncandd = pca2.fit_transform(X[[i for i in X.columns if \"cancel\" in i or \"diver\" in i]])\n", "intent": "- With just 1 component we can explain more than 80% of the variance in the variables\n"}
{"snippet": "X, Y = datasets.make_blobs(centers=4, cluster_std=0.5, random_state=1)\n", "intent": "For our first example, let's create some synthetic easy-to-cluster data:\n"}
{"snippet": "pipeline = Pipeline((\n    ('vec', TfidfVectorizer(max_df = 0.8, ngram_range = (1, 2), use_idf=True)),\n    ('clf', MultinomialNB(alpha = 0.001)),\n))\n_ = pipeline.fit(twenty_train_small.data, twenty_train_small.target)\n", "intent": "Let's fit a model on the small dataset and collect info on the fitted components:\n"}
{"snippet": "pca_dg = PCA(2)\nX_proj = pca_dg.fit_transform(X)\nprint np.sum(pca_dg.explained_variance_ratio_)\n", "intent": "Therefore, need 29 components from 64 original features to explain 95% of original data\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(x, y, test_size=0.166666, random_state=1)\nprint 'The shape of the x train data set is: ', X_train.shape\nprint 'The shape of the y train data set is: ', y_train.shape\nprint 'The shape of the x validation data set is: ', X_validation.shape\nprint 'The shape of the y validation data set is: ', y_validation.shape\n", "intent": "Split the training set into two pieces - a training set of size 50000, and a separate validation set of size 10000. Also load in the test data.\n"}
{"snippet": "wine = pd.read_csv('wine_original.csv')\nprint(wine.shape)\nwine.head()\n", "intent": "For the questions in this section, load the wine dataset (wine original.csv).\n"}
{"snippet": "from sklearn.decomposition import PCA\ndf_phi_w = pd.DataFrame(phi_w, index=V, columns=C)\npca = PCA(n_components=100)\npca_phi_w = pca.fit_transform(phi_w)\npca_phi_w = pd.DataFrame(pca_phi_w, index=V)\nprint('For PCA(100), ', np.sum(pca.explained_variance_ratio_)* 100.0, '% of the variance is explained.')\n", "intent": "5.Suppose we want a 100-dimensional representation. How would you achieve this?\n"}
{"snippet": "df_test = pd.read_csv(\"../datasets/titanic_test.csv\")\ndf_test.head()\n", "intent": "<i>Read test data, for which we will make predictions</i>\n"}
{"snippet": "d = {'Survived': y_pred}\ndf_to_submit = pd.DataFrame(data=d, index=df_test[\"PassengerId\"])\ndf_to_submit.head()\n", "intent": "*Form a dataset and write it to a file*\n"}
{"snippet": "logins = pd.read_json('logins.json')\nlogins.info()\nlogins.head(10)\n", "intent": "\"logins.json\" file contains (simulated) timestamps of user logins in a particular geographic location\n"}
{"snippet": "pca3 = decomposition.PCA(n_components=3)\nX_decomp = pca3.fit_transform(X)\n", "intent": "- With 3 components we can explain more than 80% of the variance in the variables\n"}
{"snippet": "means_player = pd.DataFrame(playerstats.groupby('yearID')['1B_PA','2B_PA','3B_PA','HR_PA','BB_PA'].mean().reset_index())\nplayerstats_norm = pd.merge(left=playerstats, right=means_player,how='left',left_on='yearID',right_on='yearID')\nplayerstats_norm['1B_norm'] = playerstats_norm['1B_PA_x']-playerstats_norm['1B_PA_y']\nplayerstats_norm['2B_norm'] = playerstats_norm['2B_PA_x']-playerstats_norm['2B_PA_y']\nplayerstats_norm['3B_norm'] = playerstats_norm['3B_PA_x']-playerstats_norm['3B_PA_y']\nplayerstats_norm['HR_norm'] = playerstats_norm['HR_PA_x']-playerstats_norm['HR_PA_y']\nplayerstats_norm['BB_norm'] = playerstats_norm['BB_PA_x']-playerstats_norm['BB_PA_y']\nmeans_player.head()\nplayerstats_norm = playerstats_norm[['teamID','yearID','W','L','playerID','1B_norm','2B_norm','3B_norm','HR_norm','BB_norm']]\nplayerstats_norm.head()\n", "intent": "Show the head of the `playerstats` DataFrame. \n"}
{"snippet": "from scipy.stats import mode\npos_mode = fielding[fielding['yearID'] > 1946].groupby('playerID')['POS'].apply(mode).apply(lambda x: x[0][0])\npositions = pd.DataFrame(pos_mode)\npositions.columns = ['POS']\npos_sal = medianSalaries.merge(positions.reset_index(), on='playerID')\npos_sal.head()\nplayerLS_SAL_POS = playerLS.merge(pos_sal, on='playerID')\nplayerLS_SAL_POS.head() \n", "intent": "Show the head of the `playerLS` DataFrame. \n"}
{"snippet": "print F10.grid_scores_[1:10]\nk_tested = pd.DataFrame(F10.grid_scores_)\nk_tested.head()\n", "intent": "Visualize the result by plotting the score results verus values for $k$. \n"}
{"snippet": "green = pd.read_csv(\"green_taxi_cleaned.csv\")\n", "intent": "Report mean and median trip distance by hour of day\n"}
{"snippet": "green = pd.read_csv(\"green_taxi_cleaned.csv\")\ngreen[\"lpep_pickup_datetime\"] = pd.to_datetime(green[\"lpep_pickup_datetime\"])\ngreen[\"Lpep_dropoff_datetime\"] = pd.to_datetime(green[\"Lpep_dropoff_datetime\"])\n", "intent": "We derive the `tip_percentage` as a percentage of `Total_amount`\n"}
{"snippet": "sample.to_csv(\"sample.csv\", index=False)\n", "intent": "In order to maintain consistency of analysis we save the sample to disk.\n"}
{"snippet": "green = pd.read_csv(\"green_taxi_cleaned.csv\")\ngreen[\"lpep_pickup_datetime\"] = pd.to_datetime(green[\"lpep_pickup_datetime\"])\ngreen[\"Lpep_dropoff_datetime\"] = pd.to_datetime(green[\"Lpep_dropoff_datetime\"])\n", "intent": "We'll start by compressing the data to the average `Trip_distance` and average `Fare_amount` per hour over the course of the month.\n"}
{"snippet": "trend_in_dollars = trip_forecast[\"trend\"] * (1/24.8) * 2.27\ncost_revenue = pd.DataFrame()\ncost_revenue[\"cost\"] = trend_in_dollars\ncost_revenue[\"revenue\"] = fare_forecast[\"trend\"]\ncost_revenue.index = fare_forecast[\"ds\"]\n", "intent": "Fortunately our fare trend is already in dollars.  So we can simply plot the fare trend line against our other line.\n"}
{"snippet": "green = pd.read_csv(\"green_with_hour.csv\")\ngreen[\"lpep_pickup_datetime\"] = pd.to_datetime(green[\"lpep_pickup_datetime\"])\ngreen[\"Lpep_dropoff_datetime\"] = pd.to_datetime(green[\"Lpep_dropoff_datetime\"])\n", "intent": "We'll start by compressing the data to the average `Trip_distance` and average `Fare_amount` per hour over the course of the month.\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = cifar10.load_data() \n", "intent": "Next, we can load the CIFAR-10 data set.\n"}
{"snippet": "with open('input.txt', 'r') as f:\n    read_data = f.read()\n    print read_data[0:100]\nf.closed\n", "intent": "We download the input file, and print a part of it:\n"}
{"snippet": "ratings_df = pd.read_csv('/resources/data/ml-1m/ratings.dat', sep='::', header=None)\nratings_df.head()\n", "intent": "We can do the same for the ratings.dat file:\n"}
{"snippet": "car_images = []\nnot_images = []\ncars = [\"25.png\", \"31.png\", \"53.png\"]\nnot_cars = [\"2.png\", \"3.png\", \"8.png\"]\nfor car in cars:\n    car_images.append(mpimg.imread('test_images/' + car))\nfor not_car in not_cars:\n    not_images.append(mpimg.imread('test_images/' + not_car))\n", "intent": "For each color space, the top three images are cars, and the bottom three are non-cars.  The car and non-car images are shown below:\n"}
{"snippet": "coeff_df = pd.DataFrame(new_train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)\n", "intent": "We can use the coefficients of the logistic regression to verify that our engineered features are good artificial features\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33)\nknn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(X_train, y_train)\n", "intent": "**Split the data into 66% training and 33% testing. Fit a KNN classifier with `n_neighbors=25` on the training data.**\n"}
{"snippet": "conmat = confusion_matrix(y_test, y_pred)\nconfusion = pd.DataFrame(conmat, \n                         index=['is_healthy', 'is_cancer'],\n                         columns=['predicted_healthy', 'predicted_cancer'])\nconfusion\n", "intent": "**Create the confusion matrix for your classfier's performance on the test set.**\n"}
{"snippet": "ss= StandardScaler()\nX = ss.fit_transform(df)\n", "intent": "We'll begin by standardizing the data:\n"}
{"snippet": "cv = CountVectorizer(preprocessor=cleaner)\ncustom_preprocess = pd.DataFrame(cv.fit_transform(space_messages).todense(),\n                                columns=cv.get_feature_names())\nprint(custom_preprocess.sum().sort_values(ascending=False).head(10))\n", "intent": "We can pass this function into `CountVectorizer` as a way to preprocess the text as a part of the fitting.\n"}
{"snippet": "demo_noage_ss = pd.DataFrame(demo_noage_ss, columns=['health', 'income', 'stress'])\n", "intent": "<a id=\"corr\"></a>\nWe will be using the correlation matrix to calculate the eigenvectors and eigenvalues.\n---\n"}
{"snippet": "num_samples = 500 * 1000\nnum_features = 40\nX, y = make_classification(n_samples=num_samples, n_features=num_features)\n", "intent": "First we create a training set of size num_samples and num_features.\n"}
{"snippet": "strategy = 'median' \nage_imputer = Imputer(strategy=strategy)\nage_imputer.fit(df['age'].values.reshape(-1, 1))\nages = age_imputer.transform(\n    df['age'].values.reshape(-1, 1))\nprint(ages[0:5], ages.mean())\n", "intent": "Our results are as follows:\n- Mean: 16.65\n- Median: 17\n- Mode: 16.0\nWhat is most appropriate? Check in on Slack\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n", "intent": "Use `train_test_split()` to create a training set and a test set, split 50/50\n"}
{"snippet": "pca = PCA(n_components= 2)\npca.fit(X_train)\nx_train_pca = pca.transform(X_train)\n", "intent": "ALL: 0  \nAML: 1 higher 'D49818_at', 'M23161_at', 'hum_alu_at', 'AFFX-PheX-5_at'\n"}
{"snippet": "df2_mean = df2.copy()\nfor column in df2.columns:\n    df2_mean[column] = df2[column].fillna(df2_drop[column].mean())\n", "intent": "So tpr is also 1 in since overall accuracy is 1\n"}
{"snippet": "df_s = pd.read_csv('data/security.csv')\ndf_s.head()\n", "intent": "2. Adding one more features\nsecurity\nsource: http://data.ap.org/projects/2016/airport-security-breaches/\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\ndata.sample(n=6)\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nitrain, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask=np.ones(critics.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\n", "intent": "Lets set up the train and test masks first:\n"}
{"snippet": "import pandas as pd\ntracks = pd.read_csv('Data/fma-rock-vs-hiphop.csv')\nechonest_metrics = pd.read_json('Data/echonest-metrics.json')\n", "intent": "First import `pandas` and use `read_csv` and `read_json` to read in the files `fma-rock-vs-hiphop.csv` and `echonest-metrics.json`.\n"}
{"snippet": "data = pd.read_csv('../datasets/10-houses.csv')\n", "intent": "Load a subset of the housing data\n"}
{"snippet": "n_samples = 200000\nn_features = 20\nX, y = make_classification(n_samples=n_samples, n_features=n_features)\n", "intent": "First we create a training set of size n_samples containing n_features each.\n"}
{"snippet": "df_predsFiltered.to_csv(\"solutions.csv\",index =False)\n", "intent": "We can finally save the list of predictions in the format requested by the test\n"}
{"snippet": "df = pd.read_csv('HR_comma_sep.csv')\n", "intent": "To import the dataset we will use Pandas library.It is the best Python library to play with the dataset and has a lot of functionalities. \n"}
{"snippet": "from sklearn.decomposition import PCA \nsklearn_pca = PCA(n_components=6)\nY_sklearn = sklearn_pca.fit_transform(X_std)\n", "intent": "The above plot shows almost 90% variance by the first 6 components. Therfore we can drop 7th component.\n"}
{"snippet": "np.random.seed(9001)\ndf = pd.read_csv('dataset_hw5.csv')\nmsk = np.random.rand(len(df)) < 0.5\ndata_train = df[msk]\ndata_test = df[~msk]\n", "intent": "1. First step is to  split  the observations into an approximate 50-50 train-test split.  Below is some code to do this for \n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"e:/datascience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n", "intent": "We can convert the categorical titles to ordinal.\n"}
{"snippet": "import pandas as pd\nmelbourne_file_path = '../../input/melbourne-housing-snapshot/melb_data.csv'\nmelbourne_data = pd.read_csv(melbourne_file_path) \nfiltered_melbourne_data = melbourne_data.dropna(axis=0)\ny = filtered_melbourne_data.Price\nmelbourne_predictors = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n                        'YearBuilt', 'Lattitude', 'Longtitude']\nX = filtered_melbourne_data[melbourne_predictors]\nfrom sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\n", "intent": "The data is loaded into **train_X**, **val_X**, **train_y** and **val_y** using the code you've already seen (and which you've already written).\n"}
{"snippet": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = pd.read_csv('../../input/melbourne-housing-snapshot/melb_data.csv')\ncols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = data[cols_to_use]\ny = data.Price\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\n", "intent": "We won't focus on the data loading. For now, you can imagine you are at a point where you already have train_X, test_X, train_y and test_y. \n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nencoder = OneHotEncoder()\ny = encoder.fit_transform(y).toarray()\nprint np.max(X)\nX = 1. * X / np.max(X)  \n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\nnames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\ndataset = pandas.read_csv(url, names=names)\n", "intent": "First we load the iris data from task 1 and split it into training and validation set.\n"}
{"snippet": "ultimate_df['phone'].fillna('iPhone', inplace=True)\n", "intent": "Seeing as iPhone makes up nearly 70% of the phones and only 0.8% of the data is missing, it may be a good assumption to fill these values as iPhones.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(ultimate_pred_df,\n                                                    ultimate_tar_df,\n                                                   test_size = 0.3,\n                                                   random_state=24,\n                                                   stratify = ultimate_tar_df)\n", "intent": "Now that we have an idea of what model to use, we can split the data into training and test sets.\n"}
{"snippet": "dataset = pd.read_csv('letter-recognition.data',delimiter=',',header=None)\ndataset.head()\n", "intent": "<h4> Import and pre-process the dataset</h4>\n"}
{"snippet": "dataset = pd.read_csv('fashion-mnist_train.csv')\ndataset = dataset.sample(frac=data_sampling_rate) \nnum_classes = 10\nclasses = {0: \"T-shirt/top\", 1:\"Trouser\", 2: \"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle boot\"}\ndisplay(dataset.head())\n", "intent": "Load the dataset and explore it.\n"}
{"snippet": "X_train_plus_valid, X_test, y_train_plus_valid, y_test \\\n    = train_test_split(X, Y, random_state=0, \\\n                                    train_size = 0.7)\nX_train, X_valid, y_train, y_valid \\\n    = train_test_split(X_train_plus_valid, \\\n                                        y_train_plus_valid, \\\n                                        random_state=0, \\\n                                        train_size = 0.5/0.7)\n", "intent": "Split the data into a **training set**, a **vaidation set**, and a **test set**\n"}
{"snippet": "from sklearn.datasets.samples_generator import make_blobs\nX, y = make_blobs(n_samples=200, centers=4, random_state=0, cluster_std=0.60)\nprint(X)\n", "intent": "First, let's try creating an artificial dataset with 200 items which we will use to test out partitional clustering.\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\ndata = iris.data\ntarget = iris.target\n", "intent": "In our second example, we will apply k-means to the Iris dataset for *k=3* clusters.\n"}
{"snippet": "from sklearn.datasets.samples_generator import make_blobs\nX, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.80)\n", "intent": "First, create a small artificial dataset to test with...\n"}
{"snippet": "import numpy as np\nfrom sklearn import cross_validation\nfrom sklearn import datasets\nfrom sklearn import svm\niris = datasets.load_iris()\n", "intent": "Let's revisit the Iris data set:\n"}
{"snippet": "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\ndataset = pandas.read_csv(url)\n", "intent": "a) We load the breast cancer data set. \n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"Preprocessing_Linear_Regression_Lab.csv\")\n", "intent": "Import \"Preprocessing and Linear Regression Dataset.csv\"\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"Logistic_Regression_Lab.csv\",index_col=False)\n", "intent": "Import \"Logistic Regression Lab.csv\". \n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"Decision_Tree_Lab.csv\",index_col=False)\n", "intent": "Import \"Decision Tree Lab.csv\".\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('Feature Selection Lab.csv')\n", "intent": "Import \"Feature Selection Lab.csv\". \n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "After feature elimination, try fitting linear regression again to see if you have improved or hurt your model performance.\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ncategories = ['comp.graphics', 'rec.motorcycles', 'sci.space', 'talk.politics.mideast', 'talk.religion.misc']\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\ndataset = fetch_20newsgroups(subset='all', categories=categories,\n                             shuffle=True, random_state=42)\n", "intent": "First let's load 20newsgroup data which contain newsletter articles + news categorical labels. We picked 5 news group to to this exercise.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)\nprint(x_train.shape)\nprint(x_test.shape)\n", "intent": "Let's split data into train and test sets.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('twitter-hate-speech.csv', index_col=0)\n", "intent": "We start by loading the dataset, for this we use [Pandas](https://pandas.pydata.org/)\n"}
{"snippet": "vectorizer = CountVectorizer(min_df=10, strip_accents='unicode', analyzer='word',\n                             tokenizer=preprocess_string, stop_words='english')\nX_preprocessed = vectorizer.fit_transform(df['tweet'].values)\n", "intent": "In the examples above we haven't yet used our preprocessing logic, we had just split up words as default. Let's do this now:\n"}
{"snippet": "array = dataset.values\nX = array[:,[0] + list(range(2,32))]\nle = LabelEncoder()\nle.fit([\"M\", \"B\"])\ny = le.transform(array[:,1]) \n", "intent": "b) We split the data into features X and labels y. After that we transform the binary labels to numerical values.\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\noneHot = OneHotEncoder()\noneHot.fit(y.reshape(len(y), -1))\ny = oneHot.transform(y.reshape(len(y), -1))\n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nsample_X, sample_y = shuffle(X, y)\nX_train, X_test, y_train, y_test = train_test_split(sample_X, sample_y.toarray(), \n                                                    test_size = 0.3, stratify = sample_y.toarray())\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nprint boston.data.shape\nprint boston.feature_names\nprint np.max(boston.target), np.min(boston.target), np.mean(boston.target)\nprint boston.DESCR\n", "intent": "Import the Boston House Pricing Dataset (http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html), and show their features.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.25, random_state=33)\n", "intent": "Build, as usual, training and testing sets:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscalerX = StandardScaler().fit(X_train)\nscalery = StandardScaler().fit(y_train)\nX_train = scalerX.transform(X_train)\ny_train = scalery.transform(y_train)\nX_test = scalerX.transform(X_test)\ny_test = scalery.transform(y_test)\nprint np.max(X_train), np.min(X_train), np.mean(X_train), np.max(y_train), np.min(y_train), np.mean(y_train)\n", "intent": "In regression tasks, is very important to normalize data (to avoid that large-valued features weight too much in the final result)\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\nboston = load_boston()\nX = boston.data\ny = boston.target\n", "intent": "Let's peek into the `scikit-learn` API.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"../Documents/DSI-SF-3/datasets/sacramento_real_estate/Sacramentorealestatetransactions.csv\")\n", "intent": "We did this in our previous lab.\n"}
{"snippet": "bcw = pd.read_csv('../../../datasets/breast_cancer_wisconsin/wdbc.data', \n                  header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "ss = StandardScaler()\nXn = ss.fit_transform(X_prop)\nprint Xn.shape\n", "intent": "---\nAlways a necessary step when performing regularization.\n"}
{"snippet": "random_state = 1 \ntest_size = 0.20\ntrain_size = 0.80\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n                                                    train_size=train_size, random_state=random_state)\n", "intent": "c) Next we split the data into a training and a validation set.\n"}
{"snippet": "messages = pandas.read_csv('/Users/Javier/Desktop/DSI-2-TEACH/DSI-SF/datasets/smsspamcollection/SMSSpamCollection', sep='\\t', \n                           quoting=csv.QUOTE_NONE,\n                           names=[\"label\", \"message\"])\nmessages.head(3)\n", "intent": "Instead of parsing TSV (or CSV, or Excel...) files by hand, we can use Python's `pandas` library to do the work for us:\n"}
{"snippet": "listings = pd.read_csv(\"../input/listings.csv\")\n", "intent": "First, let's boot up and examine our data. Since our data comes in a simple CSV file, we load it into a `pandas` `DataFrame`.\n"}
{"snippet": "boston['BNBDensity_Houses'] = boston['NAMELSAD10'].map(houses).fillna(0)\nboston['BNBDensity_Apartments'] = boston['NAMELSAD10'].map(apartments).fillna(0)\n", "intent": "Let's create numerical variables counting how many of each we have per census tract.\n"}
{"snippet": "with open(\"synset_words.txt\") as fp:\n    synset_contents = fp.read()\nclass_labels_data = [{\n    \"line\": index, \n    \"id\": line[0:9], \n    \"label\": line[10:]\n} for index, line in enumerate(synset_contents.split(\"\\n\"))\n]\nvgg_labels = pd.DataFrame(class_labels_data)\n", "intent": "> _This was actually kind of hard to find but these labels should match up to the class prediction as ordinals._\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\npu_df_work_scaled = scaler.fit_transform(pu_df_work)\npu_df_work_scaled = pd.DataFrame(pu_df_work_scaled, columns = (\"Pu239\", \"Pu240\", \"label\"))\npu_df_work_scaled\n", "intent": "<a id='scaling'></a>\n"}
{"snippet": "train_users = pd.read_csv('../cache/train_users.csv')\n", "intent": "Load only the users with known destination\n"}
{"snippet": "train_users.fillna(-1, inplace=True)\n", "intent": "Replace NaN values with -1. \n"}
{"snippet": "y_train = train_users['country_destination']\ntrain_users.drop(['country_destination', 'id'], axis=1, inplace=True)\nx_train = train_users.values\nlabel_encoder = LabelEncoder()\nencoded_y_train = label_encoder.fit_transform(y_train)\n", "intent": "Select proper X and y. The labels should be encoded into integers to be usable by `XGBoost`:\n"}
{"snippet": "train_X, valid_X, train_label, valid_label = train_test_split(train_X, train_Y_one_hot, \n                                                           test_size=0.2, random_state=13)\n", "intent": "For the train further split to <code>train</code> and <code>validate</code>\n"}
{"snippet": "scaler = StandardScaler()\npca = PCA(n_components=2)\nlogistic = LogisticRegression(random_state=1)\npipeline = Pipeline(steps=[('StandardScaler', scaler), ('PCA', pca),\n                           ('LogisticRegression', logistic)])\npipeline.fit(X_train, y_train)\n", "intent": "d) Now we set up and train a pipeline which contains a scaler, dimensionality reduction and a classificator. \n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\ndf['weight_mms'] = mms.fit_transform(df[['Weight']])\ndf['Height_mms'] = mms.fit_transform(df[['Height']])\ndf.describe().round(2)\n", "intent": "2) MinMax normalization :\nscales exactly b/e 0 & 1\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\ndf['weight_ss'] = ss.fit_transform(df[['Weight']])\ndf['Height_ss'] = ss.fit_transform(df[['Height']])\ndf.describe().round(2)\n", "intent": "3) standard normalization :\nscales as mean = 0 & standard deviation = 1\n"}
{"snippet": "df = pd.read_csv(\"HR_comma_sep.csv\")\n", "intent": "1.load the dataset at HR_comma_sep.csv, inspect it with `.head()`, `.info()` and `.describe()`.\n"}
{"snippet": "cluster_2_rows_customers = np.where(customers_prediction == 2)[0]\ncluster_2_rows_general = np.where(predict_general == 2)[0]\ncustomers_undo_pca = pca.inverse_transform(customers_pca[cluster_2_rows_customers])\ncustomers_unscaled = scaler.inverse_transform(customers_undo_pca).round()\ndf = pd.DataFrame(customers_unscaled, columns = customer_columns)\n", "intent": "Note that cluster 2 is vastly overrepresented in the customer data compared to the general population. \n"}
{"snippet": "cluster_5_rows_customers = np.where(customers_prediction == 5)[0]\ncluster_5_rows_general = np.where(predict_general == 5)[0]\ncustomers_undo_pca_2 = pca.inverse_transform(customers_pca[cluster_5_rows_customers])\ncustomers_unscaled_2 = scaler.inverse_transform(customers_undo_pca_2).round()\ndf_2 = pd.DataFrame(customers_unscaled_2, columns = customer_columns)\n", "intent": "Note that cluster 5 is vastly underrepresented in the customer data compared to the general population. \n"}
{"snippet": "data = pd.read_csv('https://serv.cusp.nyu.edu/~cq299/ADS2016/Data/ExtraCredit1.csv')\n", "intent": "- $y = a_1x_1 + b_1$  \n- $y = a_2x_2 + b_2$  \n- $y = a_3x_3 + b_3$\n"}
{"snippet": "path = 'https://serv.cusp.nyu.edu/~cq299/ADS2016/Data/Bayesian/'\ndata4=pd.read_csv(path + \"example4.csv\", low_memory=False)\nlist_311=list(data4.loc[:,\"Adopt A Basket\":].columns)\ndepend_variable=['mean_log','gross_sq_feet_log']+list_311\ndata4['sale_price_log']=np.log(data4['sale_price']).round(decimals=3)\ndata4['gross_sq_feet_log']=np.log(data4['gross_sq_feet']).round(decimals=3)\ndata4['mean_log']=np.log(data4['mean']).round(decimals=3)\nX=data4[depend_variable]\ny=data4['sale_price_log']\n", "intent": "(For simplifing this question, let us ignore the over fitting problem now.)\n"}
{"snippet": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata2 = pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/ML_2017/Q2.csv\")\ntrain, test = train_test_split(data2, random_state=9, test_size=0.4)\n", "intent": "In this question, we use data: \"Q2.csv\" for Bayesian Network Learning.\n"}
{"snippet": "test = \"?,4,93,weighty\\n?,8,70,light\\n?,6,113,medium\\n?,6,95,weighty\\n?,4,115,medium\"\ntest = pd.read_csv(StringIO(unicode(\"MPG,cylinders,HP,weight\\n\" + test)))\n", "intent": "?,4,93,weighty ?,8,70,light ?,6,113,medium ?,6,95,weighty ?,4,115,medium\n"}
{"snippet": "scaler = StandardScaler()\nlogistic = LogisticRegression(random_state=1)\nrfe = RFECV(logistic, scoring='accuracy')\npipeline = Pipeline(steps=[('StandardScaler', scaler), ('rfe', rfe),\n                           ('LogisticRegression', logistic)])\npipeline.fit(X_train, y_train)\n", "intent": "f) Now we use RFE instead of PCA for feature selection. \n"}
{"snippet": "nd.imread(\"esb.jpg\")\n", "intent": "each pixel in the image has a value associated to it. 0 is black, 254 is white if it is an 8-bit image.\n"}
{"snippet": "iris = datasets.load_iris()\ni_data = iris[\"data\"][:,:2]\ntarget = iris[\"target\"]\ntarget = [0 if t == 0 else 1 for t in target]\ndata = MinMaxScaler().fit_transform(i_data)\ntrain_X, test_X, train_y, test_y = train_test_split(data, target, test_size = 0.25, random_state=33)\nN, M = train_X.shape\n", "intent": "<h2>Data Preprocessing </h2>\n"}
{"snippet": "iris = load_iris()\ndata = iris[\"data\"]\nlabels = iris[\"target\"]\nlabels = [0 if target == 0 else 1 for target in  iris.target]\nnormalised_data = MinMaxScaler().fit_transform(data) \ntrain_X, test_X, train_y, test_y = train_test_split(normalised_data, labels, test_size = 0.25, random_state=33)\n", "intent": "<h2>Data Preprocessing </h2>\n"}
{"snippet": "celebrity = pd.read_csv('twitterusers_top_100_celebrity.csv')\ncelebrity['field'] = 'celebrity'\ndata = pd.read_csv('twitterusers_top_100_data.csv')\ndata['field'] = 'data'\ntech = pd.read_csv('twitterusers_top_100_tech.csv')\ntech['field'] = 'tech'\nsports = pd.read_csv('twitterusers_top_100_sports.csv')\nsports['field'] = 'sports'\n", "intent": "Pull the datasets for celebrity, tech, data, and sports top 100 users into pandas dataframes\n"}
{"snippet": "bigdf1 = pd.read_csv('twitter_bigdf_12072013.csv')\n", "intent": "Now that we've got the followers saved and compiled to a csv, we can append that to the dataset.\n"}
{"snippet": "newpd = pd.read_csv('twitter_bigdf_appended_cleanedtweets_averageperuser.csv')\ndef cleanup(x):\n    exclude = set(string.punctuation)\n    s = str(x)\n    s = ''.join(ch for ch in s if ch not in exclude)\n    return s\nnewpd['Tweet'] = newpd['Tweet'].map(lambda x: cleanup(x))\n", "intent": "Create a dataframe and a \"was_retweeted\" column to analyze.  Will try a few different splits.\n"}
{"snippet": "shared = pd.DataFrame(sharedfollowers, columns={'common', 'combo'})\nshared.head()\n", "intent": "Now we have a dataframe with each user combo and a list of their shared followers. Let's take a look.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"D:/DataScience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0) \n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "display(pd.DataFrame({'number_of_clusters':[2, 3, 4, 5, 10, 20, 50], 'silhouette_score': [0.4262, 0.3968, 0.3320, 0.3509, 0.3510, 0.3501, 0.3577    ]}))\n", "intent": "**Answer:**\nSilhouette scores for different number of clusters are displayed below\n"}
{"snippet": "train_data = pd.read_csv(\"../../data/raw/train.csv\")\ntrain_data['Dates'] = pd.to_datetime(train_data['Dates'])\ntest_data = pd.read_csv(\"../../data/raw/test.csv\")\ntest_data['Dates'] = pd.to_datetime(test_data['Dates'])\n", "intent": "First we load and explore the dataset a little.\n"}
{"snippet": "area_df = pd.read_csv(path, names=['State','Area'])\narea_df\n", "intent": "If we wanted to give the columns specific names, we would have to pass another parameter called `names`. We can also omit the header parameter.\n"}
{"snippet": "dates = pd.date_range('20160101', periods=6) \ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))\nprint(dates)\ndf\n", "intent": "We can select data both by their labels and by their position. \n"}
{"snippet": "df[\"preTestScore\"].fillna(df[\"preTestScore\"].mean(), inplace=True)\ndf\n", "intent": "`inplace=True` means that the changes are saved to `df` right away\n"}
{"snippet": "dataset = CreateDataSet(4)\ndf = pd.DataFrame(data=dataset, columns=['location','Status','CustomerCount','StatusDate'])\ndf.head(5)\n", "intent": "Are easy! and are akin to Excel.\n"}
{"snippet": "dataset = CreateDataSet(4)\ndf = pd.DataFrame(data=dataset, columns=['location','Status','CustomerCount','StatusDate'])\ndf.info()\n", "intent": "Are easy! and are akin to Excel.\n"}
{"snippet": "df4 = pd.DataFrame({'B': ['B2', 'B3', 'B6', 'B7'],\n                    'D': ['D2', 'D3', 'D6', 'D7'],\n                    'F': ['F2', 'F3', 'F6', 'F7']},)\ndf4\n", "intent": "What if the columns for both dataframes are not exactly the same (but with some overlaps) ?\n"}
{"snippet": "data = {'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'],\n        'year': [2012, 2012, 2013, 2014, 2014],\n        'reports': [4, 24, 31, 2, 3],\n        'coverage': [25, 94, 57, 62, 70]}\ndf = pd.DataFrame(data, index = ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'])\ndf\n", "intent": "apply() can apply a function along any axis of the dataframe\n"}
{"snippet": "spam_data = np.asarray(pd.read_csv(\"datasets/spambase_data.csv\", header = None))\n", "intent": "Lets load in the dataset and take a look at what we have.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nmedical_data = pd.read_csv(\"datasets/balanced_cleaned_diabetes_data.csv\")\n", "intent": "Lets load in the dataset and take a look at what we have.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "Always execute all the rows here\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nsample_submission = pd.read_csv(\"sample_submission.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nsample_submission = pd.read_csv(\"sample_submission.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nsample_submission = pd.read_csv(\"sample_submission.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\noh_enc = OneHotEncoder(sparse=False)\nall_labels = np.concatenate([train_labels, test_labels])\nprint len(all_labels)\nY = oh_enc.fit_transform(all_labels.reshape(-1, 1))\ntrain_labels = Y[:len(train_labels)]\ntest_labels = Y[len(train_labels):]\nprint len(train_labels), len(test_labels)\n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint X_train.shape,y_train.shape, X_test.shape, y_test.shape\nprint(\"The MNIST database has a training set of %d examples.\" % len(X_train))\nprint(\"The MNIST database has a test set of %d examples.\" % len(X_test))\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the MNIST database.\n"}
{"snippet": "train_df = pd.read_csv(\"feature_engineering/training_sources.csv\")\nX_train = np.array(train_df[[\"mean\", \"nobs\", \"duration\"]])\ny_train = np.array(train_df[\"Class\"])\n", "intent": "**Problem 2a**\nRead in the training set file, and create a feature vector `X` and label array `y`.\n"}
{"snippet": "from astropy.table import Table\nTable.read(\"irsa_catalog_WISE_iPTF14jg_search_results.tbl\", format=\"ipac\")\n", "intent": "**Problem 1b**\nUse [`Table.read()`](http://docs.astropy.org/en/stable/api/astropy.table.Table.html\n"}
{"snippet": "data = pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "def create_submission(probabilities):\n    submission = pd.DataFrame(probabilities, columns=list(le.classes_))\n    submission.insert(0, 'Id', range(0, len(submission)))\n    submission.to_csv(\"submission.csv\", index=False)\n", "intent": "Training MLP with sknn and grid search\n"}
{"snippet": "df = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n", "intent": "Using principal component analysis, we are trying to figure out what components explain the most variance of the dataset.\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "Scale our data so each feature has a single unit variance.\n"}
{"snippet": "df_feat = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n", "intent": "**We're trying to predict whether the tumor is malignant or benign.**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_validation = sc.transform(x_validation)\nx_test = sc.transform(x_test)\n", "intent": "To improve the performance of our model, we scale the dataset.\n"}
{"snippet": "source= '/Users/jamalbacchus/IdeaProjects/Capstone/Mini Projects/Cancer Treatment/data_files'\ndf_variants = pd.read_csv(source+'/training_variants').set_index('ID').reset_index()\ntest_variants_df = pd.read_csv(source+\"/test_variants\")\ndf_text = pd.read_csv(source+\"/training_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\ntest_text_df = pd.read_csv(source+\"/test_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\nprint(\"Train Variant\".ljust(15), df_variants.shape)\nprint(\"Train Text\".ljust(15), df_text.shape)\nprint(\"Test Variant\".ljust(15), test_variants_df.shape)\nprint(\"Test Text\".ljust(15), test_text_df.shape)\ndf_variants.head()\n", "intent": "Let's take a casual look at the *variants* data.\n"}
{"snippet": "source= 'C:\\\\Users\\\\Jameel shaik\\\\Documents\\\\Projects\\\\Personalized Medicine Redefining Cancer Treatment'\ndf_variants = pd.read_csv(source+'/training_variants').set_index('ID').reset_index()\ntest_variants_df = pd.read_csv(source+\"/test_variants\")\ndf_text = pd.read_csv(source+\"/training_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\ntest_text_df = pd.read_csv(source+\"/test_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\nprint(\"Train Variant\".ljust(15), df_variants.shape)\nprint(\"Train Text\".ljust(15), df_text.shape)\nprint(\"Test Variant\".ljust(15), test_variants_df.shape)\nprint(\"Test Text\".ljust(15), test_text_df.shape)\ndf_variants.head()\n", "intent": "Let's take a casual look at the *variants* data.\n"}
{"snippet": "data = pd.read_csv(\"College_Data\")\ndata.set_index('Unnamed: 0', inplace=True)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vec = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "X = count_vec.fit_transform(X)\nX\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "from io import StringIO  \nmovie_txt = requests.get('https://raw.github.com/cs109/cs109_data/master/movies.dat').text\nmovie_file = StringIO(movie_txt) \nmovies = pd.read_csv(movie_file, delimiter='\\t')\nmovies[['id', 'title', 'imdbID', 'year']].irow(0)\n", "intent": "Here's a chunk of the MovieLens Dataset:\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=101)\n", "intent": "**Split the data into testing and training datasets**\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nnumerical = ['budget', 'popularity', 'revenue', 'runtime', 'vote_count', 'release_date']\nfeatures_log_minmax_transform = pd.DataFrame(data = features_log_transformed[numerical])\nfeatures_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])\nfeatures_log_minmax_transform.describe()\n", "intent": "After applying feature scaling to data, we apply **normalization** to ensure that each feature is treated equally when applying supervised learners. \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfeatures_final = features_preprocessed.copy()\nX_train, X_test, y_train, y_test = train_test_split(features_final, \n                                                    target, \n                                                    test_size = 0.30, \n                                                    random_state = 0)\nprint \"Training set has {} samples.\".format(X_train.shape[0])\nprint \"Testing set has {} samples.\".format(X_test.shape[0])\n", "intent": "The 3227 data after preprocessing is split into training sets(2258) and testing sets(969) for model training.\n"}
{"snippet": "cvscore = {}\ncvscore['DT'] = results['Decision Tree']['cv_score']\ncvscore['Ada'] = results['Ada Boost']['cv_score'] \ncvscore['RF'] = results['Random Forest']['cv_score'] \ncvscore['XGB'] = results['XGBoost']['cv_score'] \ncvscore['GB'] = results['Gradient Boosting']['cv_score'] \ncvscore['XGBest'] = grid_results['XGB_Best']['cv_score']\ncvscore['GBest'] = grid_results['GB_Best']['cv_score'] \ndf = pd.DataFrame(cvscore.items(), columns=['model', 'cv_score'])\nsns.barplot(x='model', y='cv_score', data=df, palette=\"cubehelix\", errcolor=\".2\", edgecolor=\".2\");\n", "intent": "- **CV Score:** **XGBoost Best** > XGBoost > Gradient Boosting Best > Gradient Boosting > Ada Boost > Random Forest > Decision Tree\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfeatures_final = features_preprocessed.copy()\nX_train, X_test, y_train, y_test = train_test_split(features_final, \n                                                    target, \n                                                    test_size = 0.30, \n                                                    random_state = 0)\nprint (\"Training set has {} samples.\".format(X_train.shape[0]))\nprint (\"Testing set has {} samples.\".format(X_test.shape[0]))\n", "intent": "The 3227 data after preprocessing is split into training sets(2258) and testing sets(969) for model training.\n"}
{"snippet": "dataframe = pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "df = pd.read_csv(\"{}/lalonde.csv\".format(DATA_PATH))\ndf.set_index('id', drop=True, inplace=True)\n", "intent": "We start by loading the data in a dataframe.\n"}
{"snippet": "nino = pd.read_csv('../data/tao-all2.dat.gz', sep=' ', names=names, na_values='.', \n                   parse_dates=[[1,2,3]])\nnino.columns = [x.replace('.', '_').replace(' ', '_') for x in nino.columns]\nnino['air_temp_F'] = nino.air_temp_ * 9/5 + 32\nwind_cols = [x for x in nino.columns if x.endswith('winds')]\nfor c in wind_cols:\n    nino['{}_mph'.format(c)] = nino[c] * 2.237\npd.to_datetime(nino.date, format='%y%m%d')\nnino = nino.drop('obs', axis=1)\n", "intent": "* Using the nino dataset, see if you can predict what the temperature (``air_temp_F``) will be for the next day \n"}
{"snippet": "y_df = pd.DataFrame(y)\ny_df['pred'] = pred\ny_df['err'] = y_df.pred - y_df.close\n(y_df\n .plot(figsize=(14,10))\n)\n", "intent": "You can plot the actuals and the predicted values. It looks like our model does a pretty poor job\n"}
{"snippet": "selector = SelectKBest(f_regression, k=2).fit(x, y)\nbest_features = np.where(selector.get_support())[0]\nprint(best_features)\n", "intent": "Find the 2 attributes in X that best correlate with y\n"}
{"snippet": "X_ff_train, X_ff_test, y_ff_train, y_ff_test = model_selection.\\\n    train_test_split(X_ff_scaled, y_ff_scaled, test_size=.3,\n                     random_state=42)\n", "intent": "Split the data into test and training data. What is the score on the test data?\n"}
{"snippet": "mush_X = mush_df.iloc[:,1:]\nmush_y = mush_df.class_p\nmush_X_train, mush_X_test, mush_y_train, mush_y_test = model_selection.\\\n    train_test_split(mush_X, mush_y, test_size=.3, random_state=42)\nmush_dt = tree.DecisionTreeClassifier()\nmush_dt.fit(mush_X_train, mush_y_train)\nmush_dt.score(mush_X_test, mush_y_test)\n", "intent": "* Create a testing and training set \n* Check if the model generalizes to the testing set\n* Visualize the tree (if you have graphviz)\n"}
{"snippet": "ss_ff = preprocessing.StandardScaler()\nss_ff.fit(Xff)\nX_ff_scaled = ss_ff.transform(Xff)\nX_ff_scaled = pd.DataFrame(Xff, columns=Xff.columns)\ny_ff_scaled = log(ff.area)\n", "intent": "* Try scaling the input and using the log of the area and see if you get a better score.\n* Examine the coefficients\n"}
{"snippet": "scaled = pd.DataFrame(scaled_feat , columns= df.columns[:-1])\nscaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n", "intent": "Convert Title words to values from 0 to 5\n"}
{"snippet": "test_data['Fare'].fillna(test_data['Fare'].dropna().mode()[0], inplace=True)\ntest_data.head()\n", "intent": "Replacing the sigle Fare missing value in test_data with the mode (most common Fare value)\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n", "intent": "Time to standardize the variables.\n** Import StandardScaler from Scikit learn.**\n"}
{"snippet": "scaled_df = pd.DataFrame(scaler_trans,columns = df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "census_data = pd.read_csv(\"./data/census_demographics.csv\")\ncensus_data.head()\n", "intent": "We'll see an example of the concepts mentioned above by considering a linear regression problem. Let us load the census data set.\n"}
{"snippet": "popdata.to_csv(\"Popular_Datapoints.csv\")\npopdata.head()\n", "intent": "For each of these dataframes, store the data in a .csv for easy future use (running all this code takes a long time!), and display the head of each:\n"}
{"snippet": "popdata = pd.read_csv(\"Popular_Datapoints.csv\")\nunpopdata = pd.DataFrame.from_csv(\"Unpopular_Datapoints.csv\")\n", "intent": "Now, if we want to jump straight to the models without running all of the above, we can just upload the csvs! \n"}
{"snippet": "popdata = pd.read_csv(\"Popular_Datapoints.csv\")\nqualitative = pd.read_csv(\"Qualitative.csv\")\ntop100 = pd.read_csv(\"Top_100.csv\")\nmerged201314 = pd.merge(qualitative, top100)\n", "intent": "We want to now merge our popular dataframe with a previous one that we had created that incorporates various qualitative information.\n"}
{"snippet": "y = df['y']\nX = df.drop('y', 1)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.1, random_state=42)\n", "intent": "b) Set to be the y variable in the dataframe from a and X to be the remaining features.\n"}
{"snippet": "import pandas as pd\npd.set_option('display.max_columns',600)\ndf = pd.read_csv('../data/hw2data.csv')\ndf.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)  \n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "predictions_df.to_csv('house-prices-pred.csv', index=False, float_format='%.0f')\npredictions_int_df.to_csv('house-prices-pred-int.csv', index=False, float_format='%.0f')\n", "intent": "The max is a bit off.  Would be worth investigating in another project. \n"}
{"snippet": "scores_df = pd.DataFrame(test_scores)\nscores_df.sort_values(by='accuracy', ascending=False).head()\n", "intent": "Had time to have a short bath in the Aare and even to make myself an espresso. \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=48)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "pca1 = PCA(n_components=1) \nX_E = pca1.fit_transform(X_HDn)\nX_reconstructed = pca1.inverse_transform(X_E)\n", "intent": "Since the first component is so latge, lets only keep it, and then reconstruct the original data from only this component, setting the others to 0.\n"}
{"snippet": "df = pd.read_csv('https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv', index_col=0)\ndf.head(10)\n", "intent": "1) Load in the dataset `https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv` into a pandas dataframe\n"}
{"snippet": "X= df[['balance', 'income', 'student']]\ny=df['default']\nfor i in range(1,16):\n    print('random state =',i)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n    fpr,tpr,roc_auc,thresholds = generate_auc(X_train,y_train,LogisticRegression, penalty='l1')\n    generate_ROCplot(fpr,tpr,'LR',roc_auc)\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "import pandas as pd\nfrom  sklearn.tree import DecisionTreeClassifier\ntrain=pd.read_csv('train.csv')\ntest=pd.read_csv('test.csv')\n", "intent": "[click here](https://www.kaggle.com/c/titanic) to go to problem description\n"}
{"snippet": "import pandas\nimport numpy as np\ndata = pandas.read_csv(\"./train.csv\")\nseed = 1234\n", "intent": "some description that should be desribed soon\n"}
{"snippet": "digits = datasets.load_digits(n_class=6)\nX = digits.data\ny = digits.target\nn = len(X)\n", "intent": "t-SNE is a technique to visualize high-dimensional data by giving each datapoint a location in a two or three-dimensional map.\n"}
{"snippet": "index_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = []\nfor block_num in full_df['date_block_num'].unique():\n    cur_shops = full_df.loc[full_df['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = full_df.loc[full_df['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])), dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns=index_cols, dtype=np.int32)\nfull_df = grid.merge(full_df, how='left', on=index_cols).fillna(0)\n", "intent": "(shop_id, item_id) for each date_block_num\n"}
{"snippet": "def get_aggragation_feature(df, groupby_cols, agg_col):\n    gb = df[df['ds_type'] == 'trn'].groupby(groupby_cols)[agg_col]\n    fname_fmt = '-'.join(groupby_cols+[agg_col]) + ':%s'\n    agg_df = pd.DataFrame({\n            fname_fmt%'mean': gb.mean(),\n        })\n    new_df = df.join(agg_df, on=groupby_cols).fillna(0)\n    return new_df, agg_df.columns.tolist()\n", "intent": "**aggragation data**\n"}
{"snippet": "prevapp = pd.read_csv('./data/input/previous_application.csv')\nprevapp = prevapp.drop('SK_ID_PREV', axis=1)\nprevapp = summary_extra_data(prevapp, 'prevapp_')\nfull_df = full_df.join(prevapp, on='SK_ID_CURR')\nadd_features(prevapp.columns.tolist())\n", "intent": "**previous_application.csv**\n"}
{"snippet": "pcblc = pd.read_csv('./data/input/POS_CASH_balance.csv')\npcblc = pcblc.drop(['SK_ID_PREV', 'MONTHS_BALANCE'], axis=1)\npcblc = summary_extra_data(pcblc, 'pcblc_')\nfull_df = full_df.join(pcblc, on='SK_ID_CURR')\nadd_features(pcblc.columns.tolist())\n", "intent": "**POS_CASH_balance.csv**\n"}
{"snippet": "def do_pca(d,n):\n    pca = PCA(n_components=n)\n    X = pca.fit_transform(d)\n    print pca.explained_variance_ratio_\n    return X, pca\n", "intent": "We now carryout a 20D PCA, which captures 73% of the variance.\n"}
{"snippet": "installpay = pd.read_csv('./data/input/installments_payments.csv')\ninstallpay = installpay.drop(['SK_ID_PREV'], axis=1)\ninstallpay = summary_extra_data(installpay, 'installpay_')\nfull_df = full_df.join(installpay, on='SK_ID_CURR')\nadd_features(installpay.columns.tolist())\n", "intent": "**installments_payments.csv**\n"}
{"snippet": "usecols = ['id', 'total_distance', 'total_travel_time', 'number_of_steps']\nfr_df = pd.concat([    \n    pd.read_csv('./data/input/fastest_routes_train_part_1.csv', usecols=usecols),\n    pd.read_csv('./data/input/fastest_routes_train_part_2.csv', usecols=usecols),\n    pd.read_csv('./data/input/fastest_routes_test.csv', usecols=usecols),\n])\n", "intent": "* Addition Data\nhttps://www.kaggle.com/oscarleo/new-york-city-taxi-with-osrm\n"}
{"snippet": "import pandas as pd\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n                'TAX', 'PTRATIO', 'B', 'LSTAT']\ndf = pd.DataFrame(train_data, columns=column_names)\ndf.head()\n", "intent": "Use the [pandas](https://pandas.pydata.org) library to display the first few rows of the dataset in a nicely formatted table:\n"}
{"snippet": "distance_df = pd.DataFrame(distance_matrix, index=invocation_matrix.index, columns=invocation_matrix.index)\ndistance_df.iloc[81:85,60:62]\n", "intent": "From this result, we create a `DataFrame` to get a better visual representation of the data.\n"}
{"snippet": "import pandas as pd\ndissimilarity_df = pd.DataFrame(\n    dissimilarity_matrix,\n    index=commit_matrix.index,\n    columns=commit_matrix.index)\ndissimilarity_df.iloc[:5,:2]\n", "intent": "To be able to better understand the result, we add the file names from the `commit_matrix` as index and column index  to the `dissimilarity_matrix`.\n"}
{"snippet": "from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(plot_data['index'])\nplot_data['normalized_index_name'] = le.transform(plot_data['index']) * 10\nle.fit(plot_data['index_from_emails'])\nplot_data['normalized_index_email'] = le.transform(plot_data['index_from_emails']) * 10\nplot_data.head()\n", "intent": "I just add some nicely normalized indexes for plotting (note: there might be a method that's easier)\n"}
{"snippet": "distance_df = pd.DataFrame(distance_matrix, index=invocation_matrix.index, columns=invocation_matrix.index)\ndistance_df.iloc[81:85,60:62]\n", "intent": "From this data, we create a `DataFrame` to get a better representation. You can find the complete `DataFrame` here as excel file as well.\n"}
{"snippet": "n_components=300\nsvd = TruncatedSVD(n_components=n_components)\nX_train_svd = svd.fit_transform(X_train_vectorized)\nX_test_svd = svd.transform(X_test_vectorized)\n", "intent": "Now we have scaled vectorized vector using Tfidf, we can reduce the number of columns using a PCA\n"}
{"snippet": "X_all_reviews = vectorizer.fit_transform(X).todense()\nX_all_reviews_TFIDF_scaled =scaler.fit_transform(X_all_reviews)\n", "intent": "Finally, we can vectorize the full set with Tfidf, scale it, and use PCA to summarize with only 300 vectors :\n"}
{"snippet": "X2, pca2=do_pca(data,2)\n", "intent": "Justfor kicks, because we can plot it, we'll do the 2D PCA\n"}
{"snippet": "df = pd.read_csv('train.csv') \n", "intent": "- Visualize\n- Find Missing Data\n- Look For Correlations\n"}
{"snippet": "data = pd.read_csv('./minute_weather.csv')\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nCreating a Pandas DataFrame from a CSV file<br><br></p>\n"}
{"snippet": "labels_list = pd.read_csv('signnames.csv')\nlabels_name = labels_list['SignName'].tolist()\nlabels_list\n", "intent": "Visualize the German Traffic Signs Dataset using the pickled file(s).\n"}
{"snippet": "digits = datasets.load_digits()\n", "intent": "The data that we are interested in is a set of 1797 images of digits, each one made of $8\\times8$ pixels.\n"}
{"snippet": "fileTrain = 'train-tweets.txt'\ndfTrain = utils.read_textfile(fileTrain,',')\ntweets = dfTrain.tweet.values\nlabels = pd.factorize(dfTrain.sentiment)[0]\nsentiments = pd.factorize(dfTrain.sentiment)[1]\ntweetsTrain, tweetsTest, labelsTrain, labelsTest = cross_validation.train_test_split(tweets, labels, \n                                                                      train_size=0.85, random_state=1234)\n", "intent": "(comment this block when real testing data is given)\n"}
{"snippet": "ss = StandardScaler()\nXs = ss.fit_transform(X)\n", "intent": "Standardize the data and compare at least one of the scatterplots for the scaled data to unscaled above.\n"}
{"snippet": "df['bias'].unique()\nX = df['text'].values\ny = df['bias'].map(lambda x: 1 if x == 'partisan' else 0)\nX_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.33)\n", "intent": "Please split the dataset into a training and test set and convert the `bias` feature into 0s and 1s.\n"}
{"snippet": "sd = pd.read_csv('./datasets/speed_dating.csv')\nsd.head()\nsd.isnull().sum()[sd.isnull().sum() > 200]\n", "intent": "---\n- Remove columns with over 200 missing values.\n- Then, remove rows with missing values.\n- Verify no rows contain NaNs.\n"}
{"snippet": "subjective_pca_transformed_df = subj_pca.transform(subjective_sdt.values)\nsns.pairplot(pd.DataFrame(subjective_pca_transformed_df), kind='reg')\n", "intent": "---\nThe transform function in the PCA will create you new component variable matrix.\n"}
{"snippet": "pca = PCA(n_components=2)\ntrain_x = pca.fit_transform(train_x)\ntest_x = pca.transform(test_x)\n", "intent": "We *fit (find PC's) and transform* the training data, and then use the PC's to transform the test data.\n"}
{"snippet": "from sklearn import cross_validation\nX_dummy = pd.DataFrame([1,2,3,4],columns=['Dummy Data'])\nk_folds = cross_validation.KFold(n=X_dummy.shape[0],n_folds=4,shuffle=False)\nfor train_index,test_index in k_folds:\n    print train_index, test_index\n", "intent": "Example of creating folds manually\n"}
{"snippet": "df = pd.DataFrame()\ndf['ship_type']  =  np.random.choice([\"romulan\", \"human\", \"klingon\", \"borg\", \"red_shirt\", \"jyotsna\"], size=500)\ndf['ship_value'] =  np.random.randint(200000, 10000000, size=500)\ndf['ship_speed'] =  np.random.randint(10, 60, size=500)\ndf['baths']      =  np.random.choice(np.arange(1, 4, 0.5), size=500)\n", "intent": "With 2-3 continious variables, and one categorical.\n"}
{"snippet": "df = pd.read_csv('../../../datasets/breast_cancer_wisconsin/wdbc.data', \n                  header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "bcw = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-2/datasets/breast_cance_wisconsin/wdbc.data', \n                  header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "cancer = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-4/datasets/breast_cancer_wisconsin/breast_cancer.csv')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "def bar_chart(feature) :\n    survived = train[train['Survived'] == 1][feature].value_counts()\n    dead = train[train['Survived'] == 0][feature].value_counts()\n    df = pd.DataFrame([survived, dead])\n    df.index = ['Survived', 'Dead']\n    df.plot(kind = 'bar', stacked = True, figsize = (10, 5))\n", "intent": " - Pclass\n - Sex\n - SibSp (\n - Parch (\n - Embarked\n - Cabin\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_data, test_data = train_test_split(agg_data, test_size=0.2, random_state=1)\n", "intent": "We will split now the dataset into two parts, the training data and the testing data. The test data should be 0.2 of the original data size. \n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nsamples = x_train[:2]\nx_train_counts = count_vect.fit_transform(samples)\nprint(pd.DataFrame(x_train_counts.A, columns=count_vect.get_feature_names()).to_string())\n", "intent": "Now let's see how we can transform our tweets into vectors\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_data, test_data = train_test_split(agg_data, test_size=0.2, random_state= 42)\n", "intent": "We will split now the dataset into two parts, the training data and the testing data. The test data should be 0.2 of the original data size. \n"}
{"snippet": "X5, pca5=do_pca(data, 5)\n", "intent": ">YOUR TURN NOW\nDo a 5 dimensional PCA, get the variance explanation, and display the components.\n"}
{"snippet": "spotify = pd.read_csv(\"../data/spotify_data.csv\", index_col=[0])\nspotify.head()\n", "intent": "Let's use AdaBoost on Spotify data\n"}
{"snippet": "Xr, yr = make_regression(n_samples=1000, n_features=1, noise=7, random_state=4,bias = 0.7)\n", "intent": "Now let's train a neural net on a regression dataset\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split( X, y, \n                                                    test_size = .25,\n                                                   random_state = 3)\n", "intent": "Let's use train/test split with RMSE to decide whether Newspaper should be kept in the model:\n"}
{"snippet": "pd.set_option(\"max.columns\", 30)\nkc = pd.read_csv(\"../data/kc_house_data.csv\")\nkc.head()\n", "intent": "We're going to work together to model housing prices using the king county home sales \n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33,\n                                                    random_state=4)\n", "intent": "Create confusion matrix for the Spotify data and calculate recall and precision scores\n"}
{"snippet": "imp = Imputer(axis=1)\n", "intent": "**Imputation** Replacing nan or null values with the mean or median values of a specific column in a dataset.\n"}
{"snippet": "imp_med = Imputer(axis=1, strategy=\"median\")\nimp_med.fit(X_train.Age)\nX_train.Age.median()\n", "intent": "We can also use the median instead of age\n"}
{"snippet": "path = \"../data/fraud.csv\"\nfraud = pd.read_csv(path, index_col=[0])\nfraud.drop(\"Time\", axis = 1)\nfraud.head()\n", "intent": "Let's move onto the real thing by modeling credit card fraud data\nhttps://www.kaggle.com/dalpozz/creditcardfraud\n"}
{"snippet": "from sklearn import datasets\ndigits_dict = datasets.load_digits()\ndigits_dict[\"DESCR\"].split(\"\\n\")\n", "intent": "The hand written digits of the mnist dataset.\n"}
{"snippet": "titanic.Age.fillna(titanic.Age.mean(), inplace=True)\n", "intent": "Sometimes a better strategy is to **impute missing values**:\n"}
{"snippet": "df = pd.read_table(\"../data/NLP_data/sms.tsv\",encoding=\"utf-8\", names= [\"label\", \"message\"])\ndf.head()\n", "intent": "This is a really helpful technique to find the words most associated with either class.\n"}
{"snippet": "path = \"../data/NLP_data/yelp.csv\"\nyelp = pd.read_csv(path, encoding='unicode-escape')\nyelp.head()\n", "intent": "To wrap our text classification section, we're going to learn how to incorporate stemming and lemmatization in our vectorizers. \n"}
{"snippet": "path = \"../data/NLP_data/ds_articles.csv\"\narticles = pd.read_csv(path, usecols=[\"text\", \"title\"], encoding=\"utf-8\")\narticles.dropna(inplace=True)\narticles.reset_index(inplace=True, drop=True)\narticles.head()\n", "intent": "We're going to build a very simple summarizer that uses tfidf scores on a corpura of data science and artificial intelligence articles\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "Ridge is better for dealing with multicollinearity and Lasso is better for high number of features.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncountvec = CountVectorizer()\nsparse_dtm = countvec.fit_transform(reviews['body_without_digits'])\n", "intent": "Our next step is to turn the text into a document term matrix using the scikit-learn function called `CountVectorizer`.\n"}
{"snippet": "import glob\nDAY2_DATA_DIR = '../../day-2/data'\nAUSTEN_DIR = os.path.join(DAY2_DATA_DIR, 'austen', '*.txt')\nfnames = glob.glob(AUSTEN_DIR)\nbooks = []\nfor fname in fnames:\n    with open(fname) as f:\n        text = f.read()\n    books.append(text)\n", "intent": "Read in all the Jane Austen books from day 2 and turn them into a DTM. What will be the rows and columns?\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidfvec = TfidfVectorizer()\nsparse_tfidf = tfidfvec.fit_transform(reviews['body_without_digits'])\nsparse_tfidf\n", "intent": "To do so, we simply do the same thing we did above with CountVectorizer, but instead we use the function TfidfVectorizer.\n"}
{"snippet": "tfidf_vectorizer = TfidfVectorizer(max_df=0.80, min_df=50,\n                                   max_features=5000,\n                                   stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(df_lit['text'])\n", "intent": "In sklearn, the input to LDA is a DTM (with either counts or TF-IDF scores).\n"}
{"snippet": "import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nlda = LatentDirichletAllocation(n_components=n_topics, max_iter=20, random_state=0)\nlda = lda.fit(tf)\n", "intent": "This is where we fit the model.\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\ntrain_dtm = vect.fit_transform(X_train)\ntrain_dtm.shape\n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(features, response, test_size=0.2)\n", "intent": "We don't want to train our classifier on the same dataset that we test it on, so let's split it into training and test sets.\n"}
{"snippet": "vocab = [(v,k) for k,v in countvectorizer.vocabulary_.items()]\nvocab = sorted(vocab, key=lambda x: x[0])\nvocab = [word for num,word in vocab]\ncoef = list(zip(vocab, lr.coef_[0]))\nimportant = pd.DataFrame(lr.coef_).T\nimportant.columns = lr.classes_\nimportant['word'] = vocab\nimportant.head()\n", "intent": "Now we can interpret the classifier by the features that it found important.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_dev, Y_train, Y_dev = train_test_split(X, Y, random_state = 101, test_size = 0.3)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "X, y = make_regression(n_samples=10000, n_features=100, random_state=2017)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "Wygenerujmy dane, np. 10000 wierszy i 100 kolumn (cech).\n"}
{"snippet": "X, y = make_classification(n_samples=10000, n_features=100, random_state=2017)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "Podobnie, wygenerujmy dane, np. 10000 wierszy i 100 kolumn (cech).\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_col_names = ['num_preg', 'glucose_conc', 'diastolic_bp', 'thickness', 'insulin', 'bmi', 'diab_pred', 'age']\npredicted_class_name = ['diabetes']\nX = df[feature_col_names].values    \ny = df[predicted_class_name].values \nsplit_test_size = 0.30\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_test_size, random_state=42)\n", "intent": "70% for training, 30% for testing\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n", "intent": "<center>![alt text](mnist_plot.png)</center>\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_2 = LabelEncoder()\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()\nX = X[:, 1:]\n", "intent": "As we can see, we have to deal with a classification problem.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_table('smsspamcollection/SMSSpamCollection',\n                   sep='\\t', \n                   header=None, \n                   names=['label', 'sms_message'])\ndf.head()\n", "intent": "Para Problemas de Naives Bayes\n"}
{"snippet": "url = 'https://raw.githubusercontent.com/justmarkham/DAT7/master/data/titanic.csv'\ntitanic = pd.read_csv(url)\ntitanic.head(10)\n", "intent": "We'll build a classification tree using the Titanic data:\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn import manifold\nimport matplotlib.patheffects as PathEffects\ndigits = datasets.load_digits()\ndigits.data.shape\nfrom sklearn.utils.extmath import _ravel\nRS = 20150101\n", "intent": "OK - lets try something more exciting than the Iris dataset (this is taken from https://github.com/oreillymedia/t-SNE-tutorial) \n"}
{"snippet": "X = np.vstack([digits.data[digits.target==i]\n               for i in range(10)])\ny = np.hstack([digits.target[digits.target==i]\n               for i in range(10)])\ndigits_proj = manifold.TSNE(random_state=RS).fit_transform(X)\n", "intent": "Now let's run the t-SNE algorithm on the dataset. It just takes one line with scikit-learn.\n"}
{"snippet": "content_image = scipy.misc.imread(\"images/rsz_content.jpg\")\ncontent_image = reshape_and_normalize_image(content_image)\n", "intent": "Let's load, reshape, and normalize our \"content\" image (the Louvre museum picture):\n"}
{"snippet": "style_image = scipy.misc.imread(\"images/starry_night2.jpg\")\nstyle_image = reshape_and_normalize_image(style_image)\n", "intent": "Let's load, reshape and normalize our \"style\" image (Claude Monet's painting):\n"}
{"snippet": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combined_df:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_df.head()\n", "intent": "We can convert the categorical titles to ordinal\n"}
{"snippet": "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()\n", "intent": "Complete the ``Fare`` feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature\n"}
{"snippet": "azdias_1 = pd.read_csv('Udacity_AZDIAS_Subset.csv', delimiter=';')\nazdias_2 = pd.read_csv('Udacity_AZDIAS_Subset.csv', delimiter=';')\nazdias = clean_data(azdias_1, azdias_2)\nazdias.shape\n", "intent": "- Validate the clean function\n"}
{"snippet": "cs = cosine_similarity(qm, nm)\ncdf = pd.DataFrame(data=cs, index=tst_labels, columns=nms)\ncdf\n", "intent": "Build the cosine similarity matrix between the test data (qm) and the training reference vectors (nm)\n"}
{"snippet": "data2 = pd.DataFrame({'Age':  [17,64,18,20,38,49,55,25,29,31,33], \n                      'Salary': [25,80,22,36,37,59,74,70,33,102,88], \n             'Loan Default': [1,0,1,0,1,0,0,1,1,0,1]})\ndata2\n", "intent": "Let's consider a more complex example by adding a \"Salary\" variable (in the thousands of dollars per year).\n"}
{"snippet": "data_test = pd.read_csv(data_path+'/data_test.csv.gz')\nimages_test = create_images(data_test, \n                            n_theta_bins=10, \n                            n_phi_bins=20, \n                            n_time_bins=6)\nX_test = images_test / 10.\n", "intent": "Make predictions for test data and prepare a submission file.\n"}
{"snippet": "X = np.vstack((car_features, notcar_features)).astype(np.float64)\nX_scaler = StandardScaler().fit(X)\nscaled_X = X_scaler.transform(X)\ny = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n", "intent": "A standard scaler fit was applied to the feature vector to normalize the features.\n"}
{"snippet": "import pandas as pd\nloans_2007 = pd.read_csv(\"loans_2007.csv\")\nprint(\"DF SHAPE pre cleanse: \",loans_2007.shape, '\\n')\nloans_2007.drop_duplicates()\nhalf_count = len(loans_2007) / 2\nloans_2007 = loans_2007.dropna(thresh=half_count, axis=1)\nprint(\"DF SHAPE post cleanse: \",loans_2007.shape, '\\n')\n", "intent": "Read in the data and perform initial data exploration to determine the size and shape of the data set. \n"}
{"snippet": "from sklearn.manifold import MDS\nmds = MDS(n_components=2, random_state=42)\nt0 = time.time()\nX_reduced_mds = mds.fit_transform(X)\nt1 = time.time()\ntime_mds = round(t1-t0,2)\nprint(\"Complete MDS: \",time_mds)\n", "intent": "**Multidimensional Scaling (MDS):** reduces dimensionality while trying to <font color=red> preserve the distances between the instances. </font>\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_tr)\nX_tr = scaler.transform(X_tr)\nX_te = scaler.transform(X_te)\n", "intent": "Similarly, use StandardScaler from sklearn.preprocessing to normalize the training and testing data, using the training data\n"}
{"snippet": "from keras.utils.data_utils import get_file\nimport io\npath = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\nwith io.open(path, encoding='utf-8') as f:\n    text = f.read().lower()\ntext = text.replace(\"\\n\",\" \")\nprint('corpus length:', len(text))\ntext[0:100]\n", "intent": "Should have a character length of ~600k\n"}
{"snippet": "encoding = 'ISO-8859-1'\ndf = pd.read_csv('spam.csv', encoding=encoding)\nprint(df.columns)\ndf.head()\n", "intent": "We are going to use classifiers to predict SMS spam.\n1. load in data\n2. apply ML algo\n3. Assess accuracy\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ntwenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\nprint type(twenty_train)\ntwenty_train.data[0]\n", "intent": "We can now load the list of files matching those categories as follows:\n"}
{"snippet": "sc = StandardScaler()\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=100)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\nrfModel = rf.fit(trainingData)\nrfFeatureImportance = pd.DataFrame([(name, rfModel.featureImportances[idx]) for idx, name in attrs],columns=['feature_name','feature_importance'])\nprint(gbCvFeatureImportance.sort_values(by=['feature_importance'],ascending =False))\n", "intent": "1. Build and train a RandomForestClassifier and print out a table of feature importances from it.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"C:/Users/1asch/udemy-datascience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "def make_submission_df(all_prediction):\n    df = test.merge(all_prediction, on=[\"shop_id\", \"item_id\"], how=\"left\")[[\"ID\", \"item_cnt_month\"]]\n    df[\"item_cnt_month\"] = df[\"item_cnt_month\"].fillna(0).clip(0, 20)    \n    return df\ndef make_submission_file(df, filename):\n    df.to_csv(\"./submission/%s.csv\" % filename, index=False)\ndef make_submission(all_prediction, filename=\"no_name\"):\n    make_submission_file(make_submission_df(all_prediction), filename)\n", "intent": "Utility function makes codes simple, so it's good to make these functions\n"}
{"snippet": "sales = pd.read_csv('../competitive-data-science-final-project/dataset/sales_train.csv.gz')\nshops = pd.read_csv('../competitive-data-science-final-project/dataset/shops.csv')\nitems = pd.read_csv('../competitive-data-science-final-project/dataset/items.csv')\nitem_cats = pd.read_csv('../competitive-data-science-final-project/dataset/item_categories.csv')\n", "intent": "Let's load the data from the hard drive first.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(sales_2015[[\"2015 Sales Q1\", \"Price per Liter mean\", \"2015 Volume Sold (Liters) mean\"]], sales_2015[\"2015 Sales\"], test_size=0.4)\nprint X_train.shape, y_train.shape\nprint X_test.shape, y_test.shape\n", "intent": "Create Model using train test split\n"}
{"snippet": "state_location = pd.read_csv(\"data/State_Location.csv\")\nstate_location.head()\n", "intent": "Impact of de-regulation\n"}
{"snippet": "data = pd.read_csv(\"http://www.ats.ucla.edu/stat/data/binary.csv\")\ndata.head()\n", "intent": "Example - learning the probability of school admission based on marks and school type\n"}
{"snippet": "seq_len = 500\nfrom keras.preprocessing.sequence import pad_sequences\ntrain = pad_sequences(train, maxlen=seq_len, value=0)\ntest = pad_sequences(test, maxlen=seq_len, value=0)\n", "intent": "Keras padding 0 at the beginning.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True)\n", "intent": "Using this, we can conveniently partition the data into training and testing data and report model accuracy.\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston=load_boston()\ndf=pd.DataFrame(boston.data,columns=boston.feature_names)\ndf['Price']=boston.target\ndf.head().T\n", "intent": " - Ridge `MSE + g * sum(a**2)`\n - Lasso `MSE + g * sum(abs(a))`\n - Wehre `g` is the regularization parameter we choose freely\n"}
{"snippet": "df['avg_rating_by_driver'].fillna(df['avg_rating_by_driver'].mean(), inplace=True)\ndf['avg_rating_of_driver'].fillna(df['avg_rating_of_driver'].mean(), inplace=True)\n", "intent": "Although there are few outliers I believe it is safe to impute the missing values with the mean\n"}
{"snippet": "from keras.datasets import mnist\n(X_train0, y_train0), (X_test0, y_test0) = mnist.load_data()\n", "intent": "<img src=\"https://datascienceschool.net/upfiles/90f2752671424cef846839b89ddcf6aa.png\">\n"}
{"snippet": "import pandas as pd\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data'\ncol_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age','label']\npima = pd.read_csv(url, header=None, names=col_names)\n", "intent": "[Pima Indian Diabetes dataset](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) from the UCI Machine Learning Repository\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\n", "intent": "**4 Steps for Vectorization**\n1. Import\n2. Instantiate\n3. Fit\n4. Transform\nThe difference from modelling is that a vectorizer does not predict\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\n", "intent": "Load the digits dataset from sklearn:\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, Y_pred, labels=[1,0]))\nprint conmat\nconfusion = pd.DataFrame(conmat, index=['has_cancer', 'is_healthy'],\n                         columns=['predicted_cancer','predicted_healthy'])\nprint(confusion)\n", "intent": "Let's say again that we are predicting cancer based on some kind of detection measure, as before.\n"}
{"snippet": "df = pd.read_csv('../../assets/datasets/cars.csv')\n", "intent": "Visualize the last tree. Can you make sense of it? What does this teach you about decision tree interpretability?\n"}
{"snippet": "df = pd.DataFrame(data = adult, columns=['workclass', 'education-num', 'hours-per-week', 'income'])\ndf.head()\n", "intent": "Convert the data to a Pandas dataframe to work with the data:\n"}
{"snippet": "df = pd.DataFrame(data = adult)\ndf\n", "intent": "Convert the data to a Pandas dataframe to work with the data:\n"}
{"snippet": "X_scale = StandardScaler().fit_transform(X)  \nX_scale = pd.DataFrame(X_scale, columns = X.columns)\nX_scale\n", "intent": "First, let's standardize the data\n"}
{"snippet": "import pandas as pd\npath = '../data/'\nurl = path + 'titanic.csv'\ntitanic = pd.read_csv(url, index_col='PassengerId')\ntitanic.shape\n", "intent": "scikit-learn models expect that all values are **numeric** and **hold meaning**. Thus, missing values are not allowed by scikit-learn.\n"}
{"snippet": "skewed = ['Avg_Basket_Size','Reward_Points_Earned']\nfeatures_log_transformed_purchage = pd.DataFrame(data = customer_purchase)\nfeatures_log_transformed_purchage[skewed] = features_log_transformed_purchage[skewed].apply(lambda x: np.log(x + 1))\n", "intent": "Aa we can see the data is very skewed. Lets remove the outlier and transform the data using log scale\n"}
{"snippet": "from sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import validation_curve\ndigits = load_digits()\nx = digits.data\ny = digits.target\n", "intent": "- validation_curve\n- GridSearchCV\n- ParameterGrid\n"}
{"snippet": "import pandas as pd\ntrain = pd.read_csv('input/train.csv')\ntest = pd.read_csv('input/test.csv')\n", "intent": "download from kaggle directly [kaggle](https://www.kaggle.com/c/titanic/data)  \n"}
{"snippet": "submission_example = pd.read_csv(Path_submission_example)\nsubmission_example.head()\n", "intent": "Example of a submission file for kaggle\n"}
{"snippet": "from sklearn.decomposition import PCA\ndef get_PCs(X,p):\n    pca = PCA(whiten=True)\n    pca.fit(X)\n    ix=np.where(np.cumsum(pca.explained_variance_ratio_)>p)[0][0]\n    pca = PCA(n_components=ix,whiten=True)\n    return pca.fit_transform(X)\n", "intent": "**PCA_BinF, PCA_BinaryF, PCA_RawCF**: Use projected components which keep more than 99% variations as the features\n"}
{"snippet": "f = open('HomeB/2015/HomeB_meter1_2015.csv')\ndata = pd.read_csv(f,sep=',', header='infer', parse_dates=[1])\nprint(data.head())\n", "intent": "Data Home B for predicition\n"}
{"snippet": "df_users['invited_by_user_id'] = df_users['invited_by_user_id'].fillna(0)\n", "intent": "8823 users have non-null last_session_creation_time, i.e. they have logged into the product.    \n6417 users are invited by other users.\n"}
{"snippet": "rdf = rdf.fillna(0)\n", "intent": "So, we have 9788 records from 1970-01-01 20:15:00 to 1970-04-13 19:00:00.\nThere are null values in count column.\n"}
{"snippet": "df['avg_rating_by_driver'].fillna(df['avg_rating_by_driver'].mean(), inplace=True)\ndf['avg_rating_of_driver'].fillna(df['avg_rating_of_driver'].mean(), inplace=True)\ndf.dropna(axis=0, inplace=True)\n", "intent": "There are missing values in columns 'avg_rating_by_driver', 'avg_rating_of_driver', and 'phone'.\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape\n", "intent": "- Parameter **lowercase:** boolean, True by default\n    - If True, Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "w=84\ncols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\ny_pred_test_mix=(w/100)*y_pred_test_nw2+y_pred_test_nw*((100-w)/100)\npd.DataFrame(y_pred_test_mix, index=dataTesting.index, columns=cols).to_csv('Final_pred_NW_MIX.csv', index_label='ID')\n", "intent": "Export results Neural Network model for mixed model Images Transfer Learning VGG16+Text. Text model weight=0.84\n"}
{"snippet": "image = io.imread(os.path.join(path, 'images_resize_gray', str(dataTraining.index[0]) + '_resize_gray.jpeg'))\n", "intent": "We are going to use the gray images to make the predictions.\n"}
{"snippet": "image = io.imread(os.path.join(path, 'images_resize_gray', str(dataTraining.index[0]) + '_resize_gray.jpeg'))\n", "intent": "Load an image from file.\n"}
{"snippet": "pca = PCA(n_components=24)\nimages_training_pca = pca.fit_transform(images_training)\n", "intent": "To reduce the dimensionality, we start appling PCA previus to make a model with Machine learning\n"}
{"snippet": "w=95\ncols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\ny_pred_test_mix=(w/100)*y_pred_test_genres2+y_pred_test_genres*((100-w)/100)\npd.DataFrame(y_pred_test_mix, index=dataTesting.index, columns=cols).to_csv('Final_pred_RF_MIX.csv', index_label='ID')\n", "intent": "Export results Random Forest model for mixed model Images+Text. Text model weight=0.95\n"}
{"snippet": "dataset = pd.read_csv('Titanic_train.csv', header = 0, dtype={'Age': np.float64})\n", "intent": "<a id=\"dataset_import\"></a>\n"}
{"snippet": "train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=0)\n", "intent": "<a id=\"splitting_dataset\"></a>\n"}
{"snippet": "for set_ in (train_set, test_set):\n    set_['Fare'] = set_['Fare'].fillna(set_['Fare'].median())\n", "intent": "<b>NOTE:</b> Fare feature also has some missing value. <b>We replace those values with the median</b>.\n"}
{"snippet": "dataset = datasets.load_iris()\nX = dataset['data']\ny = dataset['target']\nX.shape, y.shape\n", "intent": "First, we'll load the data and create one-hot vectors\n"}
{"snippet": "from sklearn.datasets import fetch_olivetti_faces\noliv=fetch_olivetti_faces()\nprint oliv.data.shape \n", "intent": "This notebook is based on Shankar Muthuswamy's example at https://shankarmsy.github.io/posts/pca-sklearn.html\n"}
{"snippet": "X, y = load_breast_cancer(return_X_y=True)\n", "intent": "First, we'll split our data int train, test, and validation datasets\n"}
{"snippet": "iris = datasets.load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris[\"data\"], iris[\"target\"], test_size=0.2)\niris[\"feature_names\"], iris[\"target_names\"]\n", "intent": "As in PyTorch, we'll start with the Iris dataset.\n"}
{"snippet": "iris = datasets.load_iris()\nX_train = iris['data']\ny_train = iris['target']\n", "intent": "As in PyTorch, we'll start with the Iris dataset.\n"}
{"snippet": "df_stat = pd.DataFrame(columns=['year','month','nrows'])\nquery = 'SELECT distinct year, month, start_station_id FROM rides WHERE year=? AND month=? '\nfor year in [2014,2015,2016]:\n    for month in range(1,13):\n        df = pd.read_sql_query(query, con, params=[year,month])\n        newdf = pd.DataFrame({'year':[year],'month':[int(month)],'nrows':[int(len(df))]})\n        df_stat = df_stat.append(newdf,ignore_index=True)\ndf_stat.head()\n", "intent": "- See when number of stations changed; this could effect number of rides\n- Need to account for changing \n"}
{"snippet": "bank_full_data = pd.read_csv('bank-full.csv', delimiter=';')\nprint(\"Bank dataset was loaded successfully!\")\n", "intent": "Loading the dataset from the CSV file.\n"}
{"snippet": "def preprocess_features(X):\n    output = pd.DataFrame(index=X.index)\n    for col, col_data in X.iteritems():\n        if col_data.dtype == object:\n            col_data = col_data.replace(['yes', 'no'], [1, 0])\n        if col_data.dtype == object:\n            col_data = pd.get_dummies(col_data, prefix=col)  \n        output = output.join(col_data)\n    return output\n", "intent": "Applying pandas_get_dummies to convert categorical features into binary variables. Also, we'll replace 'yes' -> 1, 'no' -> 0.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.3, random_state=10)\nprint(\"Training set has {} samples with {:.2f}% of 'yes' (subscribed) and {:.2f}% of 'no' (not subscribed).\"\n      .format(X_train.shape[0], \n        100 * len(y_train[y_train == 1])/len(y_train), \n        100 * len(y_train[y_train == 0])/len(y_train)))\nprint(\"Testing set has {} samples with {:.2f}% of 'yes' (subscribed) and {:.2f}% of 'no' (not subscribed).\"\n      .format(X_test.shape[0], \n        100 * len(y_test[y_test == 1])/len(y_test), \n        100 * len(y_test[y_test == 0])/len(y_test)))\n", "intent": "Splitting data into training and testing datasets.\n"}
{"snippet": "sc = StandardScaler()\nX_train = pd.DataFrame(sc.fit_transform(X_train), columns=X_all.columns)\nX_test = pd.DataFrame(sc.transform(X_test), columns=X_all.columns)\n", "intent": "Rescaling the features for them to have standard normal distribution with mean 0 and a standard deviation 1.\n"}
{"snippet": "features = X.columns\nfeature_importances = model.feature_importances_\nfeatures_df = pd.DataFrame({'Features': features, 'Importance Score': feature_importances})\nfeatures_df.sort_values('Importance Score', inplace=True, ascending=False)\nfeatures_df.head()\n", "intent": "Why are random forest models more accurate than decision trees? \n"}
{"snippet": "url = '../data/titanic.csv'\ntitanic = pd.read_csv(url)\ntitanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\ntitanic.Age.fillna(titanic.Age.median(), inplace=True)\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\nembarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\ntitanic = pd.concat([titanic, embarked_dummies], axis=1)\ntitanic.head()\n", "intent": "We'll build a classification tree using the Titanic data:\n"}
{"snippet": "jobs_w_salary.number_reviews.fillna(0, inplace=True)\n", "intent": "It also appears not to be statistically significant, although interestingly the pseudo-r2 did go up significantly when I added this to the model...\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "housing_data = pd.read_csv('USA_Housing.csv')\n", "intent": "Now, I will get idea from the dataset as to what is in it and how can it be fed to the model.\n"}
{"snippet": "b_new_order_item.to_csv('../2_data/explored/order_item.csv', index =False)\n", "intent": "Export file order_item\n"}
{"snippet": "from sklearn.utils import shuffle\nX_train, y_train = shuffle(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)    \n", "intent": "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project.\n"}
{"snippet": "df = pd.read_csv(\"file2.csv\", lineterminator='\\n')\ntweets = list(df[\"tweets\"])\ntimes = list(df[\"times\"])\n", "intent": "Create dataframes, then store them to csv files.\n"}
{"snippet": "cv = CountVectorizer()\nX = cv.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "X = StandardScaler().fit_transform(select_df)\nX[0]\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nScale the Features using StandardScaler\n<br><br></p>\n"}
{"snippet": "coeff_customers = pd.DataFrame(lm.coef_, X.columns, columns=['Coefficient'])\ncoeff_customers\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "df = pd.read_csv('data/multiTimeline.csv', skiprows = 1)\ndf.head()\n", "intent": "* Import data that you downloaded and check out first several rows:\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncn_vt = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "X = cn_vt.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"/Users/Carter/Projects/DataScience/DS-SF-32/lessons/lesson-7/2008.csv\").fillna(\"unk\")\n", "intent": "http://stat-computing.org/dataexpo/2009/the-data.html\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"/Users/Carter/Projects/DataScience/DS-SF-32/lessons/lesson-7/2008.csv\").fillna(0)\n", "intent": "http://stat-computing.org/dataexpo/2009/the-data.html\n"}
{"snippet": "from sklearn.preprocessing import  LabelEncoder\nle = LabelEncoder()\nle.fit(df['UniqueCarrier'].unique().tolist()[0:10])\nle.transform(df['UniqueCarrier'].unique().tolist()[0:10])\n", "intent": "+ THESE ARE NOT DUMMY VARIABLES\n+ Note the fit vs fit_transform function\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nonehot = OneHotEncoder(categorical_features = is_cat_list, sparse=False)\nX =onehot.fit_transform(df2)\n", "intent": "+ BUT YOU NEED TO SPECIFY WHICH VARIABLES ARE CATEGORICAL\n+ Could we use \"integer\" type as a proxy for that?\n"}
{"snippet": "dataset = pd.read_csv('pima-indians-diabetes.data.txt', header=None)\ndataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, np.NaN)\ndataset.fillna(dataset.mean(), inplace = True)\nprint(dataset.isnull().sum())\n", "intent": "Removing rows with missing values can be too limiting on some predictive modeling problems, an alternative is to impute missing values.\n"}
{"snippet": "data['Loan_Amount_Applied'].fillna(data['Loan_Amount_Applied'].median(),inplace = True)\ndata['Loan_Tenure_Applied'].fillna(data['Loan_Tenure_Applied'].median(),inplace=True)\n", "intent": "Loan Amount and Tenure applied:\n"}
{"snippet": "data = pd.read_csv('bloodpressure_males.csv')\nage=data['age'].values\nweight=data['weight'].values\nheight=data['height'].values\nblood=data['bloodpressure'].values\n", "intent": "Run three independent single linear regression models \n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\ndf_iris = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                     columns= iris['feature_names'] + ['target'])\ndf_iris.head()\n", "intent": "* Import the iris dataset from scikit-learn, turn it into a DataFrame and view the head:\n"}
{"snippet": "train, test, train_labels, test_labels = train_test_split(data, labels, random_state=0, test_size = 0.2, train_size = 0.8)\ntrain_labels_wide = keras.utils.to_categorical(train_labels, num_classes)\ntest_labels_wide = keras.utils.to_categorical(test_labels, num_classes)\n", "intent": "Split the data into a training and test partition so we can evaluate at the end\n"}
{"snippet": "dataset = pd.read_csv('fashion-mnist_train.csv')\ndataset = dataset.sample(frac=data_sampling_rate) \nnum_classes = 10\nclasses = {0: \"T-shirt/top\", 1:\"Trouser\", 2: \"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle boot\"}\n", "intent": "Load the dataset and explore it.\n"}
{"snippet": "scaledFeatures = scaler.fit_transform(bank.drop('Class', axis = 1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "dfScaled = pd.DataFrame(scaledFeatures, columns = bank.columns[:-1])\ndfScaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df = pd.read_csv('College_Data', index_col = 0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=42)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "iowa_data = pd.read_csv('../../../../../../iowa_data/Iowa_Liquor_sales_sample_10pct.csv')\n", "intent": "Load your data from project 3\n"}
{"snippet": "import pandas as pd\nfrom sklearn.feature_extraction.text import HashingVectorizer\nhvec = HashingVectorizer()\nhvec.fit([spam])\n", "intent": "Lookup how to do this and then try it.\n"}
{"snippet": "votes = pd.read_csv(votes_file)\nairport_file = pd.read_csv(airport_file)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, x_test, Y_train, Y_test = train_test_split(df['Reviews'], df['Positively Rated'], random_state=0)\n", "intent": "This shows we have imbalanced classes\n"}
{"snippet": "df=pd.read_csv('../Iris.csv')\n", "intent": "Lataa iris-dataset dataframeen totuttuun tapaan:\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimputer = Imputer(strategy=\"median\")\nX_train_num_np = imputer.??(X_train_num_df)\n", "intent": "<span style=\"color:red\"> **Imputer Class for pipeline** </span>\n"}
{"snippet": "median_total_bedrooms = train_set[\"total_bedrooms\"].median()\nmedian_bedrooms_per_room = train_set[\"bedrooms_per_room\"].median()\nX_train_num_df_null[\"total_bedrooms\"].fillna(median_total_bedrooms, inplace=True)\nX_train_num_df_null[\"bedrooms_per_room\"].fillna(median_bedrooms_per_room, inplace=True)\nX_train_num_df_null.head()\n", "intent": "*Option 3: Set the missing values to some values(median)*\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimputer = Imputer(strategy=\"median\")\nX_train_num_np = imputer.fit_transform(X_train_num_df)\n", "intent": "<span style=\"color:red\"> **Imputer Class for pipeline** </span>\n"}
{"snippet": "for i in range(data.shape[1]-1):\n    if int(data.iloc[:,i:i+1].isnull().sum()) > 0:\n        imp = Imputer(missing_values='NaN', strategy='mean')\n        imp.fit(data.iloc[:,i:i+1])\n        data.iloc[:,i:i+1] = imp.transform(data.iloc[:,i:i+1])\ndata.isnull().sum()\n", "intent": "Next. plug missing value by mean.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(stop_words='english')\nfit_vect = vect.fit_transform(X)\n", "intent": "Test out differnt parameters of the vect (e.g. stop_words='english' or None) to observe how it changes the clusters\n"}
{"snippet": "airline_acronyms = pd.read_csv(AIRLINE_ACRONYMS_FILEPATH)\nairline_acronyms.head()\n", "intent": "SOURCE: https://www.faa.gov/airports/resources/acronyms/\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nvect.fit(text)\nprint(vect.get_feature_names())\n", "intent": "- boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "vect = CountVectorizer(max_features=5)\nvect.fit(text)\nprint(vect.get_feature_names())\n", "intent": "- int or None, default=None\n- If not None, build a vocabulary that only consider the top  max_features ordered by term frequency across the corpus.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer().fit(X_train)\n", "intent": "converting the text data into numerical data so that we can use it in sklearn\n"}
{"snippet": "class CountVectorizer:\n    def __init__(self, lowercase=True):\n        self.lowercase = lowercase\n    def fit(self, raw_documents):\n        self.vocabulary_ = raw_documents\n        return self\n    def __repr__(self):\n        return \"CountVectorizer(lowercase={})\".format(self.lowercase)\ncv = CountVectorizer()\ncv.fit(text)\n", "intent": "When we run the cv.fit(text) no output displays. This is because the method does not include a return value or print statements\n"}
{"snippet": "data = pd.read_csv(\"life_satisfaction_vs_gdp_per_capita_all.csv\", thousands=',')\n", "intent": "load \"datasets/lifesat/life_satisfaction_vs_gdp_per_capita_all.csv\"\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=20, include_bias=False)\nX_poly = poly_features.fit_transform(X)\n", "intent": "feature extension with PolynomialFeatures\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nstd_scaler = StandardScaler()\nX_poly_scaled = std_scaler.fit_transform(X_poly)\n", "intent": "Apply StandardScaler\n"}
{"snippet": "import pandas as pd\nurl = '../../assets/dataset/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer\n", "intent": "1. K-means clustering\n2. Clustering evaluation\n3. DBSCAN clustering\n"}
{"snippet": "users = pd.read_csv('takehome_users.csv')\nusers.info()\n", "intent": "We can read in the users file. \n"}
{"snippet": "engagement = pd.read_csv('takehome_user_engagement.csv')\nengagement.info()\n", "intent": "Next, we load in the engagement data. \n"}
{"snippet": "users.last_session_day.fillna(0, inplace=True)\nusers.last_session_month.fillna(0, inplace=True)\nusers.last_session_year.fillna(0, inplace=True)\nusers = pd.get_dummies(users, drop_first=True) \n", "intent": "Next we ensure null values are filled and one-hot encoding the one string/categorical feature. \n"}
{"snippet": "logins = pd.read_json('logins.json')\nlogins.info()\n", "intent": "OK, so we will start by reading in the file and doing some quick exploration. \n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(min_df=5).fit(X_train)\n", "intent": "And features with high Tfidf are vice-versa\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=1221, stratify=y)\nprint('The train data has %.0f rows which is %.2f%% of the total. ' % (len(x_train), len(x_train)*100./len(ultimate)))\nprint('The  test data has %.0f rows which is %.2f%% of the total. ' % (len(x_test),  len(x_test) *100./len(ultimate)))\n", "intent": "Now, we can split the data into training and testing data. Note that we also stratify the split, because there is an imbalance in the data (3:1). \n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the MNIST database.\n"}
{"snippet": "evictions = pd.read_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Projects/Project 4/san_francisco/evictions/sf_eviction_notices.csv')\nbudget = pd.read_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Projects/Project 4/san_francisco/city_budget/budget.csv')\nspending_rev = pd.read_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Projects/Project 4/san_francisco/city_spending_revenue/spending_revenue.csv')\n", "intent": "My hypothesis with this data is that number of evictions can impact the spending budget of San Francisco.\n"}
{"snippet": "park_info = pd.read_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Projects/Project 4/san_francisco/parks/park_info.csv')\npark_scores = pd.read_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Projects/Project 4/san_francisco/parks/park_scores.csv')\n", "intent": "My hypothesis is that the size of a park helps predict the quality score of a park.\n"}
{"snippet": "prices_df.to_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Projects/Project 6/rv_prices.csv') \n", "intent": "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n"}
{"snippet": "bcw = pd.read_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Week 4 Notes & Code/Datasets/wdbc.data', \n                  header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "bc = pd.read_csv('../Datasets/breast_cancer.csv')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "data = pd.read_csv('../Datasets/401ksubs.csv') \ndata.head(2)\n", "intent": "1. Read the 401ksubs.csv data into Pandas.\n2. Explore the data by sorting, plotting, group_by, and any other ideas/techniques you have been using.\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "1. Standarize the data\n"}
{"snippet": "data['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\ndata['Embarked'] = data['Embarked'].fillna('S')\ndata.info()\n", "intent": "* Impute missing values:\n"}
{"snippet": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Other\": 5}\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n", "intent": "After that, we convert the categorical Title values into numeric form.\n"}
{"snippet": "for dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n", "intent": "We find that category \"S\" has maximum passengers. Hence, we replace \"nan\" values with \"S\".\n"}
{"snippet": "for dataset in train_test_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n", "intent": "Replace missing Fare values with the median of Fare.\n"}
{"snippet": "models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines', 'Linear SVC', \n              'KNN', 'Decision Tree', 'Random Forest', 'Naive Bayes', \n              'Perceptron', 'Stochastic Gradient Decent'],\n    'Score': [acc_log_reg, acc_svc, acc_linear_svc, \n              acc_knn,  acc_decision_tree, acc_random_forest, acc_gnb, \n              acc_perceptron, acc_sgd]\n    })\nmodels.sort_values(by='Score', ascending=False)\n", "intent": "Let's compare the accuracy score of all the classifier models used above.\n"}
{"snippet": "with open('trump.txt', 'r') as fp:\n    txt = fp.read()\n", "intent": "<a name=\"getting-the-trump-data\"></a>\nNow let's load the text.  This is included in the repo or can be downloaded from:\n"}
{"snippet": "df.columns = [c.lower() for c in df.columns] \nfrom sqlalchemy import create_engine\nengine = create_engine('postgresql://:@localhost:5432/mydb')\ndf1 = pd.read_csv('../assets/airports.csv')\ndf1.columns = [c.lower() for c in df1.columns] \ndf2 = pd.read_csv('../assets/Airport_operations.csv')\n", "intent": "Load our csv files into tables\n"}
{"snippet": "stuff = pd.DataFrame(sklearn_pca.components_,columns=X.columns,index=['PCA-1','PCA-2'])\nstuff.head()\n", "intent": "Create a writeup on the interpretation of findings including an executive summary with conclusions and next steps\n"}
{"snippet": "df = pd.DataFrame(mtcars)\ndf.head()\n", "intent": "Convert to a Pandas Dataframe for our analysis\n"}
{"snippet": "from sklearn.datasets import load_iris\ndata = load_iris()\ndata.keys\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "import pandas as pd\ndftrain = pd.read_csv('imagery/traindata.csv')\ndftest = pd.read_csv('imagery/testdata.csv')\nprint dftrain.head(5)\n", "intent": "Read in the CSV file as Pandas dataframes\n"}
{"snippet": "PATH_TO_DATA = ('../data')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "train_target = pd.read_csv('../data/medium/train_log1p_recommends.csv', index_col='id')\ny_train = train_target['log_recommends'].values\ny_train = y_train[:X_train_sparse.shape[0]]\n", "intent": "**Read train target and split data for validation.**\n"}
{"snippet": "text = open(path_to_file).read()\nprint ('Length of text: {} characters'.format(len(text)))\n", "intent": "We can take a look and listen to get a better sense of the dataset:\n"}
{"snippet": "mnist = tf.keras.datasets.mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\ntrain_images = np.expand_dims(train_images, axis=-1)/255.\ntrain_labels = np.int64(train_labels)\ntest_images = np.expand_dims(test_images, axis=-1)/255.\ntest_labels = np.int64(test_labels)\n", "intent": "Let's download and load the dataset and display a few random samples from it:\n"}
{"snippet": "from IPython.display import HTML\nimport io, base64\nvideo = io.open('./pong_agent.mp4', 'r+b').read()\nencoded = base64.b64encode(video)\n", "intent": "And display the result:\n"}
{"snippet": "df = pd.read_csv('Classified Data', index_col=0)\n", "intent": "Set index_col=0 to use the first column as the index.\n"}
{"snippet": "from sklearn.decomposition import  TruncatedSVD\nlsa = TruncatedSVD(n_components=5)\nlsaDocTopic = lsa.fit_transform(X)\nprint(\"Document topic shape\", lsaDocTopic.shape)\nprint (\"Topics and word shape\", lsa.components_.shape)\n", "intent": "PLSA topic modeling in scikit-learn is implemented in the same way as LDA but uses a TruncatedSVD class.\n"}
{"snippet": "sbp_data = pd.read_csv(\"Ex03_SystolicBP_Regreesion.csv\")\n", "intent": "Reading the data file\n"}
{"snippet": "df_model=df_ORG\nstringIndexer2 = StringIndexer(inputCol=\"Dest\", outputCol=\"destIndex\")\nmodel_stringIndexer = stringIndexer2.fit(df_model)\nindexedDest = model_stringIndexer.transform(df_model)\nencoder2 = OneHotEncoder(dropLast=False, inputCol=\"destIndex\", outputCol=\"destVec\")\ndf_model = encoder2.transform(indexedDest)\n", "intent": "In the next two cell we select the features that we need to create the model.\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn import tree\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport random as rnd\niris = datasets.load_iris()\ndf = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n                  columns=iris['feature_names'] + ['target'])\ndf.head()\n", "intent": "This is a companion notebook for the new [Data Science Solutions](https://strtupsci.com) book. The code is explained in the book.\n"}
{"snippet": "def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind = 'bar', stacked = True,figsize=(10,5))\n", "intent": "bar chart for categorical features\n-> Pclass ->Sex -> SbSp(\n"}
{"snippet": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain.head()\n", "intent": "We can convert the categorical titles to ordinal.\n"}
{"snippet": "url = 'https://raw.githubusercontent.com/cs109/2014_data/master/diamonds.csv'\ndiamonds = pd.read_csv(url, index_col=0) \ndiamonds.head()\n", "intent": "http://pandas.pydata.org/pandas-docs/stable/indexing.html\n"}
{"snippet": "X, X_test, y, y_test = train_test_split(stat.iloc[:,2:-1], stat.iloc[:,16], \n                                                    test_size=0.33, random_state=42)\nX.reset_index(inplace=True)\ny = y.reset_index()\nX_test.reset_index(inplace=True)\ny_test = y_test.reset_index()\nprint(\"Training set::{}{}\".format(X.shape,y.shape))\nprint(\"Testing set::{}\".format(X_test.shape))\n", "intent": "<h1> Train-Test Split\n"}
{"snippet": "from sklearn.tree import DecisionTreeRegressor\nX, X_test, y, y_test = train_test_split(stats.iloc[:,2:-3], stats.iloc[:,16], \n                                                    test_size=0.33, random_state=42)\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(X, y)\n", "intent": "<h1> Decision Tree Regression on All the Features\n"}
{"snippet": "X, X_test, y, y_test = train_test_split(stats.iloc[:,2:-1], stats.iloc[:,-1], \n                                                    test_size=0.33, random_state=42)\nX.reset_index(inplace=True)\ny = y.reset_index()\nX_test.reset_index(inplace=True)\ny_test = y_test.reset_index()\nprint(\"Training set::{}{}\".format(X.shape,y.shape))\nprint(\"Testing set::{}\".format(X_test.shape))\n", "intent": "<h1> Train Test Split\n"}
{"snippet": "X, X_test, y, y_test = train_test_split(stat.iloc[:,0:-1], stat.iloc[:,-1], \n                                                    test_size=0.33, random_state=42)\nX.reset_index(inplace=True)\ny = y.reset_index()\nX_test.reset_index(inplace=True)\ny_test = y_test.reset_index()\nprint(\"Training set::{}{}\".format(X.shape,y.shape))\nprint(\"Testing set::{}\".format(X_test.shape))\n", "intent": "<h1> Train-Test Split\n"}
{"snippet": "submit.to_csv('log_reg_baseline.csv', index = False)\n", "intent": "The predictions as given in Target variable should be assessed by the Client and may decide to classify applicant whether can repay loan or can not.\n"}
{"snippet": "from sklearn.datasets import make_classification\nX, y = make_classification(\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n    n_informative=3, n_redundant=1, flip_y=0,\n    n_features=20, n_clusters_per_class=1,\n    n_samples=100, random_state=10\n)\ndf = pd.DataFrame(X)\ndf['TARGET'] = y\ndf.TARGET.value_counts().plot(kind='bar', title='Count (TARGET)');\n", "intent": "For ease of visualization, let's create a small unbalanced sample dataset using the make_classification method:\n"}
{"snippet": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_df.head()\n", "intent": "We can convert the categorical titles to ordinal.\n"}
{"snippet": "engagement = pd.read_csv('takehome_user_engagement.csv')\nengagement.info()\n", "intent": "Next, we load in the engagement data.\n"}
{"snippet": "users.last_session_day.fillna(0, inplace=True)\nusers.last_session_month.fillna(0, inplace=True)\nusers.last_session_year.fillna(0, inplace=True)\nusers = pd.get_dummies(users, drop_first=True) \n", "intent": "Next we ensure null values are filled and one-hot encoding the one string/categorical feature.\n"}
{"snippet": "with open('ultimate_data_challenge.json') as f:\n    ultimate = json.load(f)\nultimate = pd.DataFrame(ultimate)\nultimate.signup_date = pd.to_datetime(ultimate.signup_date)\nultimate.last_trip_date = pd.to_datetime(ultimate.last_trip_date)\nultimate.head()\n", "intent": "We start of course by loading and then briefly exploring the data.\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=1221, stratify=y)\nprint('The train data has %.0f rows which is %.2f%% of the total. ' % (len(x_train), len(x_train)*100./len(ultimate)))\nprint('The  test data has %.0f rows which is %.2f%% of the total. ' % (len(x_test),  len(x_test) *100./len(ultimate)))\n", "intent": "Now, we can split the data into training and testing data. Note that we also stratify the split, because there is an imbalance in the data (3:1).\n"}
{"snippet": "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2)\nX += np.random.random(X.shape)\ndatasets = [make_moons(noise=0.1), make_circles(noise=0.1, factor=0.5), (X, y)]\n", "intent": "<h1 align=\"center\">Warm Up: 3 datasets</h1> \n"}
{"snippet": "D = pd.DataFrame({'area': area, 'length': length})\nD.head()\n", "intent": "As is shown above, the average explanation length for 99% is ~428.\n"}
{"snippet": "table.to_csv('table',header=None)\n", "intent": "Finally, we have a table for all informations include stations, weather and area-id.\n"}
{"snippet": "header=['station','count']\ndata=pd.read_csv(\"data/counts_year\", header=None,names=header,\n                 delim_whitespace=True,\n                 skipinitialspace=True)\n", "intent": "**Then table of measurements are joined with latitude/longitude information below.**\n"}
{"snippet": "header=['station','year'] + range(1,731)\ndata = pd.read_csv(\"data/concenate_data\", header=None,names=header,delim_whitespace=True)\n", "intent": "**Due to the fact that the file is too big, it cannot load into memory in one time.**\n"}
{"snippet": "train_df = pd.read_csv('data/titanic-kaggle/train.csv')\ntest_df = pd.read_csv('data/titanic-kaggle/test.csv')\n", "intent": "The Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames.\n"}
{"snippet": "f = open('stations_weight.txt', 'rb')\nd = []\nfor line in f.readlines():\n    data = re.compile(r\"[\\.\\w-]+\").findall(line)\n    d.append(data)\nheader = ['station', 'weight']  \ndf = pd.DataFrame(d,columns=header)\ndf['weight'] = df['weight'].astype(float)\nDjoined = df.join(station_info, on='station')\n", "intent": "Import the weight information calculated from the above process (stations_weight.txt) and perform a join on station table\n"}
{"snippet": "dfMeasurementsYear = pd.read_csv('TMINcounts_year',delimiter='\\t',header=None,names=['year','N_TMIN'])\nmeasurements = ['TMAXcounts_year','TMIN365counts_year','TMAX365counts_year']\nnames = ['N_TMAX','N_TMIN365','N_TMAX365']\nfor j in range(len(measurements)):\n    df = pd.read_csv(measurements[j],delimiter='\\t',index_col=0,header=None,names=[names[j]])\n    dfMeasurementsYear = dfMeasurementsYear.join(df,on='year')\n", "intent": "Let us first look at the recording statistics by year. For this purpose, we read all required tables into a single dataframe.\n"}
{"snippet": "dfMean = pd.read_csv('TMINmean',delimiter=r\"[\\[,\\]\\t]+\",header=None,names=['prefix','partitionID','day','TMINmean'])\ndfMean = dfMean[dfMean.ix[:,'prefix'] != '\"header\"']\ndfMean = dfMean[dfMean.ix[:,'prefix'] != '\"other\"']\ndfMean = dfMean[dfMean.ix[:,'prefix'] != '\"incomplete\"'] \ndfMean = pd.pivot_table(dfMean,values='TMINmean',rows='partitionID',cols='day')\ndfMean.head()\n", "intent": "After successfully running the job, we can load the results into a dataframe\n"}
{"snippet": "dfCov = pd.read_csv('TMINcov',delimiter=r\"[\\[,\\]\\t]+\",header=None,names=['prefix','partitionID','i','j','cov'])\ndfCov = dfCov[2:].reset_index(drop=True).drop('prefix',1) \ndfCov = pd.pivot_table(dfCov,values='cov',rows=['partitionID','i'],cols='j')\ndfCov = dfCov.reset_index('i')\ndfCov.to_pickle('covTable.pkl')\ndfCov.head(10)\n", "intent": "We load the covariance matrix elements for each partition into a dataframe.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "First we'll load the iris data as we did before:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X.transpose())\nV=pca.components_ \nprint 'eigenvectors=\\n',V\nprint 'rotation matrix=\\n',rotation\nprint 'Product=\\n',V*rotation\n", "intent": "For more documentation on sklearn.decomposition.PCA see http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "def get_files(f):\n    df = pd.read_csv(f,header=None)\n    df.columns = ['file']\n    df.head(2)\n    return df\n", "intent": "process chipseq signals\n"}
{"snippet": "def map_features_to_colors(df_track, map_dict1, clrs):\n    map_dict2 = {}\n    for n, k in enumerate(map_dict1.keys()):\n        map_dict2[k] = clrs[n]\n    assert list(map_dict2.values()) == clrs\n    ndf_track = df_track.apply(lambda x: x.map(map_dict1)).fillna(0).T\n    return (ndf_track, map_dict2)\n", "intent": "plot hiv and histology track\n"}
{"snippet": "nt = '{}numeric_track.txt'.format(wkdir)\nntdf = pd.read_csv(nt)\nntdf.head(2)\n", "intent": "get meta data about library ids, hiv status, histology, total reads sequenced etc\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n        reshaped_segments, labels, test_size=0.2, random_state=RANDOM_SEED)\n", "intent": "Finally, let's split the data into training and test (20%) set:\n"}
{"snippet": "f1 = '/home/szong/projects/resource/chrominfo.txt'\nchrominfo = pd.read_csv(f1, sep='\\t', index_col='\nchrominfo.columns = ['size', 'file_name']\nchrominfo.head(2)\n", "intent": "look at GISTIC amp and del peak length distribution\n"}
{"snippet": "f = '/projects/trans_scratch/validations/workspace/szong/Cervical/cnv/bin_segs/amps_bins_1kb_patients.txt'\nrecur_amps.to_csv(f, index=False, sep='\\t')\nrecur_amps = pd.read_csv(f, sep='\\t')\n", "intent": "find systematic cnv noise\n"}
{"snippet": "f = '/projects/trans_scratch/validations/workspace/szong/POG/pog_skin_normals_and_ffpe.tsv'\ndf = pd.read_csv(f, sep='\\t')\ndf.head(2)\n", "intent": "look at skin normal and ffpe samples\n"}
{"snippet": "x, y = 3, 10\ndf = pd.DataFrame(np.random.randn(x, y),\n                  index=['sample_{}'.format(i) for i in range(1, x + 1)],\n                  columns=['gene_{}'.format(i) for i in range(1, y + 1)])\n", "intent": "We should create some simple example data for the purpose of illustration.\n"}
{"snippet": "f1 = '/projects/trans_scratch/validations/workspace/szong/EXPANDS_500/pog500_annotated_all_biopsies.vcf'\ndf1 = pd.read_csv(f, sep='\\t')\ndf1.head(2)\n", "intent": "determine number of variants to sample\n"}
{"snippet": "f = '/projects/trans_scratch/validations/workspace/szong/EXPANDS_500/cnv_files.txt'\ndf21 = pd.read_csv(f, sep='\\t', header=None)\ndf21.columns = ['lib', 'cnv']\ndf21.head(2)\n", "intent": "make cnv input file\n"}
{"snippet": "f = '/projects/trans_scratch/validations/workspace/szong/EXPANDS_500/all_cnv_files.txt'\ndf = pd.read_csv(f, sep='\\t', header=None)\ndf.head()\n", "intent": "processing ploidy/tc corrected cnv, get rid of negative values, replace with 0\n"}
{"snippet": "f = '/projects/trans_scratch/validations/workspace/szong/EXPANDS_500/sub_populations.txt'\ndf = pd.read_csv(f, sep='\\t', header=None)\ndf.head()\n", "intent": "visualize subpopulations\n"}
{"snippet": "f1 = '/projects/trans_scratch/validations/workspace/szong/Cervical/mutsig2cv/118_patients.txt'\npatients = pd.read_csv(f1, header=None)[0].values.tolist()\nassert len(patients) == 118\npatients[:3]\n'HTMCP-03-06-02007' in patients\n'HTMCP-03-06-02026' in patients\n", "intent": " 53 samples, need to remove 2 samples: 'HTMCP-03-06-02007', and 'HTMCP-03-06-02026'\n"}
{"snippet": "df_dropped['price'].fillna(value=np.round(df.price.mean(),decimals=2),\n                                inplace=True)\n", "intent": "Fill Missing Price values with mean price\n"}
{"snippet": "df1 = pd.DataFrame(np.random.randint(1,10,size=(10,5)), columns=list('abcde'))\ndf2 = pd.DataFrame(100+np.random.randint(1,10,size=(10,5)), columns=list('abcde'))\ndf = pd.concat([df1,df2])\ndf = df.reset_index(drop=True)\ndf3 = df+100\ndf4 = pd.merge(df, df3, left_index=True, right_index=True)\ndf4.head()\n", "intent": "https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/\n"}
{"snippet": "count_vectorizer = CountVectorizer(\n    analyzer=\"word\", tokenizer=nltk.word_tokenize,\n    preprocessor=None, stop_words='english', max_features=None)    \n", "intent": "Start with a simple baseline. Bag of words\n"}
{"snippet": "f1 = '/projects/mwarren_prj/mwarren_prj_results/VDB-241/all_annotated.tsv'\ndf1 = pd.read_csv(f1, sep='\\t',dtype={'chrom': str}) \ndf1 = df1[df1.chrom != 'MT']\nnew_chr = df1.chrom.str.replace('X', '23').replace('Y', '24')\ndf1.chrom = new_chr\ndf1.tail()\n", "intent": "159 tert mutations in a region of 51kb, this can be used to sanity check\n"}
{"snippet": "f3 = '/home/szong/projects/resource/centromere.pos.txt'\ncentromeres = pd.read_csv(f3, sep='\\t')\ncentromeres.head(2)\n", "intent": "exclude centromeres\n"}
{"snippet": "f1 = '/projects/mwarren_prj/mwarren_prj_results/VDB-241/all_annotated.tsv'\ndf1 = pd.read_csv(f1, sep='\\t',dtype={'chrom': str}) \ndf1 = df1[df1.chrom != 'MT']\nnew_chr = df1.chrom.str.replace('X', '23').replace('Y', '24')\ndf1.chrom = new_chr\ndf1.tail()\n", "intent": "pro-processing mutations: pick only modifier mutations, exclude mutations at centromere. convert X>23, Y>24, remove other unplaced contigs and MT\n"}
{"snippet": "f = '/projects/trans_scratch/validations/workspace/szong/Cervical/ml/melb_data.csv'\nmelbourne_data = pd.read_csv(f)\nmelbourne_data.head()\n", "intent": "data can be download from here: https://www.kaggle.com/dansbecker/starting-your-ml-project/data\n"}
{"snippet": "f1 = '/projects/trans_scratch/validations/workspace/szong/Cervical/cnv/cleaned_cnv_files.txt'\ndf1 = pd.read_csv(f1, sep='\\t', header=None, comment='\ndf1.columns = ['patient', 'seg_path']\nassert df1.shape[0] == 124\ndf1.head(2)\n", "intent": "visualize all cnvs in one plot\n"}
{"snippet": "f1 = '/projects/trans_scratch/validations/workspace/szong/Cervical/cnv/Cervical_124_patients_adjusted/cnv_files_for_acen_removal.txt'\ndf1 = pd.read_csv(f1, sep='\\t', header=None, comment='\ndf1.columns = ['patient', 'seg_path']\nassert df1.shape[0] == 123\ndf1.head(2)\n", "intent": "visualize all cnvs in one plot\n"}
{"snippet": "tf = '/projects/trans_scratch/validations/workspace/szong/Cervical/tcga_cn/gdac.broadinstitute.org_CESC.Merge_snp__genome_wide_snp_6__broad_mit_edu__Level_3__segmented_scna_minus_germline_cnv_hg19__seg.Level_3.2016012800.0.0/CESC.snp__genome_wide_snp_6__broad_mit_edu__Level_3__segmented_scna_minus_germline_cnv_hg19__seg.seg.txt'\ntdf = pd.read_csv(tf, sep='\\t')\ntdf.head(2)\n", "intent": "hiv status, need to run for hiv pos and neg seprately and then plot together\n"}
{"snippet": "df_dropped['user_type'].fillna(method='ffill',inplace=True)\n", "intent": "Fill Missing user_type values with value from previous row (forward fill) \n"}
{"snippet": "f = '/projects/trans_scratch/validations/workspace/szong/Cervical/cnv/Cervical_124_patients_new/del_genes.conf_99.txt'\ndf = pd.read_csv(f, sep='\\t', header=None)\ndf.head(5)\n", "intent": "look at GISTIC results\n"}
{"snippet": "itrain, itest = train_test_split(range(df.shape[0]), train_size=0.6)\nmask=np.ones(df.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\n", "intent": "Here we'll consider classification but Decision trees can be use for regression as we know.\n"}
{"snippet": "data = pd.read_csv( 'nba_2013.csv' )\ndata.head().T\n", "intent": "pos:\n- SF small forward\n- C center\n- PF power forward\n- SG shooting guard\n- PG point guard\n- G guard\n- F forward\n"}
{"snippet": "df2 = pd.read_csv('haberman.data',names=['age','yearoperation','posauxnodes','survival'])\ndf2_train, df2_test = train_test_split(df2,test_size=0.25)\ndf2.head()\n", "intent": "Draw the ROC curve (and calculate AUC) for the logistic regression classifier from challenge 12\n"}
{"snippet": "titanicdf = pd.read_csv('train.csv',header=0)\nprint titanicdf[titanicdf.PassengerId==38]\n", "intent": "Tackle the Titanic Survivors kaggle competition4 with decision trees. Look at your splits, how does your tree decide?\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nbindf_train, bindf_test = train_test_split(bindf,test_size=0.25)\n", "intent": "Split the data into a test and training set. But this time, use this function: from sklearn.cross_validation import train_test_split\n"}
{"snippet": "iris = datasets.load_iris()\nirisdf = pd.DataFrame(iris.data, columns=iris.feature_names)\nirisdf['target'] = iris.target\nirisdf.head()\n", "intent": "We'll be using the Iris dataset. Read more about the Iris dataset [here](https://en.wikipedia.org/wiki/Iris_flower_data_set)\n"}
{"snippet": "iris = datasets.load_iris()\nirisdf = pd.DataFrame(iris.data, columns=iris.feature_names)\nirisdf['target'] = iris.target\nirisdf.head()\n", "intent": "API Docs for [sklearn.neighbors.KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier)\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n", "intent": "Use `CountVectorizer` to generate vectorized text features.\n"}
{"snippet": "df_dropped['user_type'].fillna(method='bfill',inplace=True)\n", "intent": "Fill Missing user_type values with value from next row (backward fill)\n"}
{"snippet": "vectorizer = CountVectorizer(max_features = 1000,\n                             ngram_range=(1, 2),\n                             stop_words='english',\n                             binary=True)\nvectorizer.fit(data['body'])\nX_new_text_features = vectorizer.transform(data['body'])\n", "intent": "Use `CountVectorizer` to generate vectorized text feature for `data['body']`\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(max_features = 1000, \n                             ngram_range=(1, 2), \n                             stop_words='english')\nvectorizer.fit(data['body'])\nX_new_text_features = vectorizer.transform(data['body'])\n", "intent": "Use `TfidfVectorizer` to generate vectorized text features.\nDoes it perform better than `CountVectorizer`?\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ntitles = data['title'].fillna('')\nvectorizer = CountVectorizer(max_features = 1000, \n                             ngram_range=(1, 2), \n                             stop_words='english',\n                             binary=True)\n", "intent": "We previously used the Count Vectorizer to extract text features for this classification task\n"}
{"snippet": "from sklearn.preprocessing import MaxAbsScaler\nscaler = MaxAbsScaler()\n", "intent": "[Docs](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html) for `MaxAbsScaler`\n"}
{"snippet": "from sklearn import datasets, cross_validation, metrics\niris = datasets.load_iris()\n", "intent": "We will build a 3 layer network with 10, 20 and 10 hidden units respectively for the iris dataset\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nimport re\nspam_data = pd.read_csv('spam.csv')\nspam_data['target'] = np.where(spam_data['target']=='spam',1,0)\nspam_data.head(10)\n", "intent": "In this assignment you will explore text message data and create models to predict if a message is spam or not. \n"}
{"snippet": "import face_recognition\ndef face_detector2(img_path):\n    img = cv2.imread(img_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    try:\n        result = len(face_recognition.face_locations(gray, model='cnn')) > 0 \n    except RuntimeError:\n        result = len(face_recognition.face_locations(gray)) > 0 \n    return result\n", "intent": "I'll be using the face recognition package provided here: https://github.com/ageitgey/face_recognition\n"}
{"snippet": "def best_reg_method(X, best_regulari):\n    method_coefs = pd.DataFrame({'variable':X.columns, \n                                 'coef':best_regulari.coef_, \n                                 'abs_coef':np.abs(best_regulari.coef_)})\n    method_coefs.sort_values('abs_coef', inplace=True, ascending=False)\n    return method_coefs.head(27)\n", "intent": "    What are the features with coefficients greater than 0\n---\n"}
{"snippet": "from sklearn.model_selection import train_test_split \nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=1734)\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train);\n", "intent": "**Part A**: First, let's set a baseline by performing a train-validation split on the data and then fitting a logistic regression model. \n"}
{"snippet": "df_normalized = df.dropna().copy()\nmin_max_scaler = preprocessing.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(df_normalized['price'].values.reshape(-1,1))\ndf_normalized['price'] = np_scaled.reshape(-1,1)\n", "intent": "Normalize price values using  **Min-Max Scaler**\n"}
{"snippet": "import pandas as pd\nyelp = pd.read_csv('../data/yelp.csv')\nyelp.head(10)\n", "intent": "Read `yelp.csv` into a DataFrame.\n"}
{"snippet": "vectorized_count_train_data = vectorizer_count.fit_transform(train_data)\ntype(vectorized_count_train_data)\n", "intent": "Fit the instance to the training data\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_tf_idf = TfidfVectorizer()\n", "intent": "Let's try it again with tf-idf\n"}
{"snippet": "vectorized_train_data = vectorizer_tf_idf.fit_transform(train_data)\nclf_tf_idf = MultinomialNB()\nclf_tf_idf.fit(vectorized_train_data, train_target)\n", "intent": "Now fit a new NB classifer instance with the tf-idf vectorized data\n"}
{"snippet": "vectorized = CountVectorizer(max_features=1000, max_df=0.95, min_df=2, stop_words='english')\na = vectorized.fit_transform(df.content)\na.shape\n", "intent": "What is the size of the document-term matrix?\n"}
{"snippet": "W = model.fit_transform(a)\nH = model.components_\n", "intent": "Get the factors $\\text{W}$ and $\\text{H}$ from the resulting model.\n"}
{"snippet": "lda = LatentDirichletAllocation(n_topics=10,\n                                max_iter=5,\n                                learning_method='online',\n                                learning_offset=50.,\n                                random_state=42)\nlda.fit(vectorized)\nprint(\"Topics in LDA model:\")\nvectorizer = tfidf_vectorizer\ntf_feature_names = vectorizer.get_feature_names()  \nprint_top_words(lda, tf_feature_names)\n", "intent": "Experiment with the number of topics. What patterns emerge?\nWhat is the best number of topics?\n"}
{"snippet": "train_A = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\ntrainA = pd.read_csv(train_A)\ntrain_B = os.path.join(os.getcwd(), 'datasets', 'train_20news_partB.csv')\ntrainB = pd.read_csv(train_B)\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'train_auto_numeric.csv')\nauto_numeric = pd.read_csv(data_path, delimiter = ',')\nprint(auto_numeric.shape[0])\nprint(auto_numeric.shape[1])\n", "intent": "<font color='red'>TASK MARK: 1</font>\n<br>\n<font color='green'>COMMENT:  - </font>\n"}
{"snippet": "df_normalized = df.dropna().copy()\nrobust_scaler = preprocessing.RobustScaler()\nrs_scaled = robust_scaler.fit_transform(df_normalized['quantity_purchased'].values.reshape(-1,1))\ndf_normalized['quantity_purchased'] = rs_scaled.reshape(-1,1)\n", "intent": "Normalize quantity purchased values using  **Robust Scaler**\n"}
{"snippet": "auto_full_edit = auto_full.copy(deep = True)\ncols = [\"make\", \"fuel-type\", \"aspiration\" , \"num-of-doors\", \"body-style\" , \"drive-wheels\" , \"engine-location\" , \"engine-type\" , \"num-of-cylinders\", \"fuel-system\", \"symboling\"]\nfor i in cols:\n    le = LabelEncoder()\n    auto_full_edit[i] = le.fit_transform(auto_full_edit[i]) \nenc = OneHotEncoder(categorical_features = [1,2,3,4,5,6,7,12,13,15,23]) \nX_enc = enc.fit_transform(auto_full_edit.drop('price',axis=1))\nX_enc.shape\n", "intent": "<font color='red'>TASK MARK: 5</font>\n<br>\n<font color='green'>COMMENT:   Interestingly, you do one-hot-encode *all* the categories.</font>\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nseed = 11\nclean_dataset = 'data/galaxies-clean.csv'\ndf = pd.read_csv(clean_dataset)\nfeatures = ['mag_u', 'mag_g', 'mag_r', 'mag_i', 'mag_z']\nX = df[features]\ny = df['redshift']\n", "intent": "Load and prepare the dataset for using the scikit-learn library.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=seed)\n", "intent": "Create a training and test dataset using the `train_test_split` function.\n"}
{"snippet": "from sklearn.model_selection import train_test_split \nX_tr, X_te, Y_tr, Y_te = train_test_split(X, Y, test_size=0.25, random_state=42)\ntrain_points = go.Scatter(name=\"Train Data\", \n                          x=X_tr, y=Y_tr, mode='markers',  marker=dict(color=\"blue\", symbol=\"o\"))\ntest_points = go.Scatter(name=\"Test Data\",\n                         x=X_te, y=Y_te, mode='markers', marker=dict(color=\"red\", symbol=\"x\"))\npy.iplot([train_points, test_points])\n", "intent": "It is always a good habbit to split data into training and test sets.\n"}
{"snippet": "df = pd.DataFrame({\n    \"name\": [\"Goldy\", \"Scooby\", \"Brian\", \"Francine\", \"Goldy\"],\n    \"kind\": [\"Fish\", \"Dog\", \"Dog\", \"Cat\", \"Dog\"],\n    \"age\": [0.5, 7., 3., 10., 1.]\n}, columns = [\"name\", \"kind\", \"age\"])\ndf\n", "intent": "Here we create a toy dataframe of pets including their name and kind:\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\nvec_enc = DictVectorizer()\nvec_enc.fit(df.to_dict(orient='records'))\n", "intent": "The `DictVectorizer` encodes dictionaries by taking keys that map to strings and applying a one-hot encoding.\n"}
{"snippet": "flavor_enc = DictVectorizer()\nflavor_enc.fit(icecream[[\"flavor\"]].to_dict(orient='records'))\nonehot_flavor = flavor_enc.transform(icecream[[\"flavor\"]].to_dict(orient='records'))\n", "intent": "Here we will construct one hot encodings for the flavor and toppings in seperate calls so we know which columns correspond to each:\n"}
{"snippet": "train_data = pd.read_csv(\"toy_training_data.csv\")\nprint(train_data.describe())\ntrain_data.head()\n", "intent": "To illustrate the potential for feature transformations consider the following *synthetic dataset*:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ndata_tr, data_te = train_test_split(data, test_size=0.25, random_state=42)\nprint(\"Training Data Size: \", len(data_tr))\nprint(\"Test Data Size: \", len(data_te))\n", "intent": "Always split your data into training and test groups.  \n"}
{"snippet": "import pandas as pd \ndf = pd.read_csv(\"adult.csv\").drop('Unnamed: 0',axis=1)\n", "intent": "read_csv has all kinds of important tuning parameters which can make the reading and the saving of the data more efficient\n"}
{"snippet": "dfRenew  = pd.read_csv('./properatti.csv')  \ndfRenew.loc[acomprar]\n", "intent": "PROPIEDADES A COMPRAR SEGUN MI PRESUPUESTO\n"}
{"snippet": "df = pd.read_csv('../data/all_stocks_5yr.csv.zip', compression='zip')\ndf.head()\n", "intent": "The data was downloaded from https://www.kaggle.com/camnugent/sandp500\n"}
{"snippet": "result = pd.DataFrame({\n    'datetime':X_test_datetime.values,\n    'count':np.ceil(np.abs(pred))\n})\nresult.to_csv('sol.csv',index=False)\n", "intent": "1. hour proved to be working great than time\n2. using months instead of seasons\n3. using atemp and temp both\n4. using workingday and holiday both\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\ntrain_y_onehot = np.float32( enc.fit_transform(train_y.reshape(-1,1)) \\\n                   .toarray() )\nval_y_onehot = np.float32( enc.fit_transform(val_y.reshape(-1,1)) \\\n                 .toarray() )\ntest_y_onehot = np.float32( enc.fit_transform(test_y.reshape(-1,1)) \\\n                  .toarray() )\n", "intent": "Onehot-encoding the labels:\n"}
{"snippet": "import numpy as np\nfrom sklearn import preprocessing\nlabels = np.array([1,5,3,2,1,4,2,1,3])\nlb = preprocessing.LabelBinarizer()\nlb.fit(labels)\none_hot = lb.transform(labels)\nprint(one_hot)\n", "intent": "* Transforming your labels into one-hot encoded vectors is pretty simple with scikit-learn using <b style='color: red'>LabelBinarizer</b>. \n"}
{"snippet": "cluster.fillna('').head(20)\n", "intent": "And the actual ratings in the cluster look like this:\n"}
{"snippet": "content_image = scipy.misc.imread(\"images/Shawnee.jpg\")\nimshow(content_image)\n", "intent": "Here is my input image:\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ncategories = ['alt.atheism', 'soc.religion.christian']\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\nnewsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\nclass_names = ['atheism', 'christian']\n", "intent": "For this tutorial, we'll be using the [20 newsgroups dataset](http://scikit-learn.org/stable/datasets/\nIn [2]:\n"}
{"snippet": "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\ntrain_vectors = vectorizer.fit_transform(newsgroups_train.data)\ntest_vectors = vectorizer.transform(newsgroups_test.data)\n", "intent": "Let's use the tfidf vectorizer, commonly used for text.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=train.shape[1])\ntrain_pca = pca.fit_transform(train, )\ntest_pca = pca.transform(test)\n", "intent": "Data is projected into a new space where the new transformed features has no mutual correlation.\n"}
{"snippet": "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n", "intent": "We are going to use the MNIST dataset to train the generator and the discriminator. The generator will then generate handwritten digits.\n"}
{"snippet": "import numpy as np\nlabels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\nimagenet_labels = np.array(open(labels_path).read().splitlines())\nlabels_batch = imagenet_labels[np.argmax(result_batch, axis=-1)]\nlabels_batch\n", "intent": "Fetch the `ImageNet` labels, and decode the predictions\n"}
{"snippet": "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n                'Acceleration', 'Model Year', 'Origin'] \nraw_dataset = pd.read_csv(dataset_path, names=column_names,\n                      na_values = \"?\", comment='\\t',\n                      sep=\" \", skipinitialspace=True)\ndataset = raw_dataset.copy()\ndataset.tail()\n", "intent": "Import it using pandas\n"}
{"snippet": "hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()\n", "intent": "Visualize the model's training progress using the stats stored in the `history` object.\n"}
{"snippet": "text = open(path_to_file).read()\nprint ('Length of text: {} characters'.format(len(text)))\n", "intent": "First, look in the text.\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42)\nX.shape\n", "intent": "Clustering\n=============\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nX, y = digits.data, digits.target\n", "intent": "A less trivial example\n-------------------------\n"}
{"snippet": "from sklearn.datasets import load_iris\nfrom sklearn.neighbors import KNeighborsClassifier\niris = load_iris()\nX, y = iris.data, iris.target\nn_samples = X.shape[0]\nprint(X.shape)\nprint(y.shape)\nprint(y)\n", "intent": "Cross-Validation\n=====================================\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nboston.keys()\n", "intent": "Load the boston dataset:\n"}
{"snippet": "tr_data = pd.read_csv('../input/train.csv')\nte_data = pd.read_csv('../input/test.csv')\nprint 'train shape is: {} \\r\\n\\\ntest shape is: {}'.format(tr_data.shape,te_data.shape)\n", "intent": "now lets load our data set for this tutorial:\nthe Otto dataset \n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\ncv.fit(text_train)\nlen(cv.vocabulary_)\n", "intent": "<img src=\"bag_of_words.svg\" width=80%>\n"}
{"snippet": "from helpers import Timer\nfrom sklearn.datasets import load_digits\nfrom sklearn.cross_validation import train_test_split\ndigits = load_digits()\nX, y = digits.data / 16., digits.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n", "intent": "$$\\hat{y} \\approx w^T \\phi(\\mathbf{x})> 0$$\n"}
{"snippet": "cars, noncars = carSVM.load_dataset()\nprint(\"Cars' images:           {}\".format(len(cars)))\nprint(\"Non-cars' images:       {}\\n\".format(len(noncars)))\nindex = np.random.randint(0,len(cars))\ncar_img = mpimg.imread(cars[index])\nnoncar_img = mpimg.imread(noncars[index])\nprint(\"Car image size:         {}x{}x{}\".format(car_img.shape[0], car_img.shape[1], car_img.shape[2]))\nprint(\"Non-car image size:     {}x{}x{}\\n\".format(noncar_img.shape[0], noncar_img.shape[1], noncar_img.shape[2]))\nprint(\"Car image data type:    {}\".format(car_img.dtype))\nprint(\"Noncar image data type: {}\".format(noncar_img.dtype))\n", "intent": "Loading the data to show a brief summary of the data sets\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\nprint X_train_dtm.shape\nprint X_test_dtm.shape\n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\ndisplay(data.head())\ndata[data.columns[1:]].describe()\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(X_features, Y, test_size=.3, random_state=42,\n                                                   stratify = ultimate_ml['retained'])\n", "intent": "Let's start by splitting the dataset into train and test data, and then we can SMOTE technique on training dataset for resampling.\n"}
{"snippet": "import pandas as pd\ndata_df = pd.read_csv('data/hourly_wages.csv')\n", "intent": "My version of this thing\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n", "intent": "Splitting the data:\n"}
{"snippet": "from sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nX, y, true_coefficient = make_regression(n_samples=80, n_features=30, n_informative=10, noise=100, coef=True, random_state=5)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)\nprint(X_train.shape)\nprint(y_train.shape)\n", "intent": "```\ny_pred = x_test[0] * coef_[0] + ... + x_test[n_features-1] * coef_[n_features-1] + intercept_\n```\n"}
{"snippet": "tr_data = pd.read_csv('../input/train.csv')\nte_data = pd.read_csv('../input/test.csv')\nprint('train shape is: {} \\r\\n\\ test shape is: {}'.format(tr_data.shape, te_data.shape))\n", "intent": "now lets load our data set for this tutorial:\nthe Otto dataset \n"}
{"snippet": "import numpy as np\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.keys()\n", "intent": "Get some data to play with\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(digits.data,\n                                                    digits.target)\n", "intent": "Split the data to get going\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\n    \"adult.data\", header=None, index_col=False,\n    names=['age', 'workclass', 'fnlwgt', 'education',  'education-num',\n           'marital-status', 'occupation', 'relationship', 'race', 'gender',\n           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n           'income'])\ndata = data.drop(\"fnlwgt\", axis=1)\ndata.head()\n", "intent": "Apply dummy encoding and scaling to the \"adult\" dataset consisting of income data from the census.\nBonus: visualize the data.\n"}
{"snippet": "from sklearn.datasets import load_diabetes\ndiabetes = load_diabetes()\ndata = diabetes.data\ntarget = diabetes.target\n", "intent": "Load the diabetes dataset using ``sklearn.datasets.load_diabetes``. Apply ``LinearRegression`` and ``Ridge`` and visualize the coefficients.\n"}
{"snippet": "y = data['income']\nX = data.drop('income', axis=1)\nX = pd.get_dummies(X)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n", "intent": "Do it again keeping a test apart\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\n", "intent": "Let's try with Digits\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "Let's try with Boston data (a RandomForestRegressor with different n_estimators and max_depth)\n"}
{"snippet": "df_train = pd.read_csv('../train.csv')\ndf_store = pd.read_csv('../store.csv')\n", "intent": "also the procedure of how I went thorugh of doing all the load and transform jobs\n"}
{"snippet": "func = lambda x: 2 + 0.5 * x + 3 * x**2 + 5 * stats.norm.rvs(0, 10)\ndf = pd.DataFrame()\ndf[\"x\"] = list(range(0, 30))\ndf[\"y\"] = list(map(func, df[\"x\"])) \ndf.plot(x='x', y='y', kind='scatter')\n", "intent": "Next we look at a data set that needs a quadratic fit. Let's do both a linear and quadratic fit and compare.\n"}
{"snippet": "TARGET_VAR= 'target'\nTOURNAMENT_DATA_CSV = 'numerai_tournament_data.csv'\nTRAINING_DATA_CSV = 'numerai_training_data.csv'\nBASE_FOLDER = 'numerai/'\ndf_train = pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV)\ndf_train.head(5)\n", "intent": "- Numerai provides a data set that is allready split into train, validation and test sets. \n"}
{"snippet": "url = '../assets/dataset/bikeshare.csv'\nbikes = pd.read_csv(url, parse_dates=True)\nbikes['dteday'] = pd.to_datetime(bikes['dteday'])\nbikes.set_index('dteday',inplace=True)\n", "intent": "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X[feature_cols], y, train_size = 0.7)\nlr.fit(X_train, Y_train)\nprint('R^2 for training data')\nprint(lr.score(X_train, Y_train))\nprint('R^2 for testing data')\nlr.score(X_test, Y_test)\n", "intent": "Load the Boston housing data.  Fix any problems, if applicable.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X[feature_cols], y, train_size = 0.7)\nlr.fit(X_train, Y_train)\nprint('R^2 for training data')\nprint(lr.score(X_train, Y_train))\nprint('R^2 for testing data')\nlr.score(X_test, Y_test)\n", "intent": "- Score and plot.  \n- How do your metrics change?\n"}
{"snippet": "df = pd.read_csv(\"../assets/datasets/iris.csv\")\nprint(df['Name'].value_counts())\ndf.head(5)\n", "intent": "Let's do some clustering with the iris dataset.\n"}
{"snippet": "X_scaled = preprocessing.MinMaxScaler().fit_transform(df[cols]) \n", "intent": "Next, since each of our features have different units and ranges, let's do some preprocessing:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\npca.fit(df.drop('inflation', axis=1))\npca.explained_variance_ratio_\n", "intent": "The below code shows that with two \"principal components\" you can capture > 97% of the variation!\n"}
{"snippet": "pca = PCA(n_components = None)\nX_train_pca = pca.fit_transform(X_train)\nexplained_variance = pca.explained_variance_ratio_\n", "intent": "implementing pca to reduce to an lower dimension to increase the readability\n"}
{"snippet": "data_path_partA = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\ndata_path_partB = os.path.join(os.getcwd(), 'datasets', 'train_20news_partB.csv')\nnews_A = pd.read_csv(data_path_partA, delimiter = ',')\nnews_B = pd.read_csv(data_path_partB, delimiter = ',')\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'splice_train.csv')\nsplice_train = pd.read_csv(data_path, delimiter = ',')\ndata_path = os.path.join(os.getcwd(), 'datasets', 'splice_test.csv')\nsplice_test = pd.read_csv(data_path, delimiter = ',')\nprint('Number of instances: {}, number of attributes: {}'.format(splice_train.shape[0], splice_train.shape[1]))\nsplice_train.head(5)\nprint('Number of instances: {}, number of attributes: {}'.format(splice_test.shape[0], splice_test.shape[1]))\nsplice_test.head(5)\n", "intent": "Load the `splice_train.csv` and `splice_test.csv` into two separate dataframes. Display the shape and first 10 instances for each dataframe.\n"}
{"snippet": "from kaggle_data import load_data, preprocess_data, preprocess_labels\nX_train, labels = load_data('data/kaggle_ottogroup/train.csv', train=True)\nX_train, scaler = preprocess_data(X_train)\nY_train, encoder = preprocess_labels(labels)\nX_test, ids = load_data('data/kaggle_ottogroup/test.csv', train=False)\nX_test, _ = preprocess_data(X_test, scaler)\nnb_classes = Y_train.shape[1]\nprint(nb_classes, 'classes')\ndims = X_train.shape[1]\nprint(dims, 'dims')\n", "intent": "See: [Data Description](1.2 Introduction - Tensorflow.ipynb\n"}
{"snippet": "everything.to_csv(\"../assets/everything.csv\",encoding=\"utf-8\")\n", "intent": "save \"everything\" as a csv for use in Tableau later\n"}
{"snippet": "dummyColumns = dataWithDummies.columns\npca = PCA()\npipe = Pipeline([('pca', pca)])\ntmp = np.array(dataWithDummies)\ndataOriginal = pipe.fit_transform(tmp)\ndataOriginal = pd.DataFrame(dataOriginal,columns=dummyColumns)\ndataOriginal.head()\n", "intent": "This set of data contains all the columns from the operations table + the airports as dummies\n"}
{"snippet": "df = pd.DataFrame(pca.components_,columns=dummyColumns)\nsns.heatmap(df)\n", "intent": "PC0 and PC2 is conal.\n"}
{"snippet": "corr = df.corr()\nyXCorr = corr.iloc[1,2:]\nyXCorr = abs(yXCorr)\nyXCorr = pd.DataFrame(yXCorr)\nyXCorr.sort_values(by=yXCorr.columns[0],inplace=True)\nyXCorr.index[0:3]\nX = df.iloc[:,yXCorr.index[-3:]]\nX.head()\n", "intent": "What sort of strategy might one take to drop features?\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df.Car)\nle.transform(df.Car)\ndf[\"Car\"] = le.transform(df.Car)\ndf.head()\n", "intent": "Loop through Cars List and Convert to Numeric. **HINT:** Reference the lesson for help with this!\n"}
{"snippet": "df = pd.DataFrame(adult)\ndf.head()\n", "intent": "Convert the data to a Pandas dataframe to work with the data:\n"}
{"snippet": "df[\"workclass\"] = preprocessing.LabelEncoder().fit_transform(df[\"workclass\"])\ndf[\"marital-status\"] = preprocessing.LabelEncoder().fit_transform(df[\"marital-status\"])\ndf[\"occupation\"] = preprocessing.LabelEncoder().fit_transform(df[\"occupation\"])\ndf[\"relationship\"] = preprocessing.LabelEncoder().fit_transform(df[\"relationship\"])\ndf[\"race\"] = preprocessing.LabelEncoder().fit_transform(df[\"race\"])\ndf[\"sex\"] = preprocessing.LabelEncoder().fit_transform(df[\"sex\"])\ndf[\"income\"] = preprocessing.LabelEncoder().fit_transform(df[\"income\"])\ndf[\"native-country\"] = preprocessing.LabelEncoder().fit_transform(df[\"native-country\"])\n", "intent": "Convert the categorical Data to numeric for our analysis. **HINT:** Refer to lesson 1.1 for writing a function of this sort\n"}
{"snippet": "X = pd.DataFrame(iris.data,columns=iris.feature_names)\ny = pd.DataFrame(iris.target,columns=[\"Class\"])\nX.head()\n", "intent": "Define the \"x\" and \"y\" variables. *Remember*, y is our classifer, and x is our attributes.\n"}
{"snippet": "df = pd.read_csv(\"../../assets/datasets/iris.csv\")\n", "intent": "We're going to load the iris data from the scikit \"datasets\" package\n"}
{"snippet": "fpred=[]\nfor i in xrange(len(predictions)):\n img=imresize((predictions[i][0]), (420,580))/255\n fpred.append((img,predictions[i][1]))\n", "intent": "But the original size of the images was 420x580, so let's resize our predictions \n"}
{"snippet": "def movie_parser(m):\n    return [int(m['num_votes']), float(m['rating']),m['tconst'],m['title'],int(m['year'])]\nparsed = [movie_parser(m) for m in top250]\nmovies = pd.DataFrame(parsed, columns=['num_votes','rating','tconst','title','year'])\n", "intent": "Keep the fields:\n    num_votes\n    rating\n    tconst\n    title\n    year\nAnd discard the rest\n"}
{"snippet": "path = '../rsc/_.txt'\ntext = open(path).read().lower()\nprint('corpus length:', len(text))\n", "intent": "Pick text corpora you would like to work on.\nTo chose: pantadeusz, potop, linux, nietzsche\n"}
{"snippet": "diabetic_data.to_csv('diabetic_data_clean.csv',index=False)\ndiabetes3 = pd.read_csv('diabetic_data_clean.csv')\nreadmitted_dummies = pd.get_dummies(diabetes3.readmitted, prefix='readmitted').iloc[:, 1:]\ndiabetes3 = pd.concat([diabetes3, readmitted_dummies], axis=1)\nfeature_cols = ['admission_type_id','readmitted', 'gender', 'age','diabetesMed', 'race', 'num_medications', 'number_diagnoses', 'time_in_hospital']\ndiabetes3.head(10)\n", "intent": "* Increases model accuracy but decreases the interpretability of the model\n"}
{"snippet": "import pandas as pd\ntitanic = pd.read_csv('titanic.csv', index_col='PassengerId')\ntitanic.head(5)\n", "intent": "For a description of the Titanic dataset see this Kaggle page: https://www.kaggle.com/c/titanic/data\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nXsubset, _, ysubset, _ = train_test_split(X, y, train_size=0.1)\n", "intent": "Let's start by working with just a subset of the data, because these things can be pretty slow:\n"}
{"snippet": "traindf = pd.read_csv('../input/train.csv')\ntestdf = pd.read_csv('../input/test.csv')\ntraindf.head()\n", "intent": "As a first start we will read in the training and testing data.\n"}
{"snippet": "weatherdf = pd.read_csv('../input/weather.csv').drop(['Station','CodeSum'],axis = 1)\nweatherdf = cleanse_data(weatherdf)\nweatherdf = weatherdf.set_index('Date')\nweatherdf.head()\n", "intent": "We are also provided weather data, which is believed to have an important impact on WNV occurrence. Let's take a look.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2017)\n", "intent": "Split the data into a test (20%) and train set.\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.999)\nprint ('\\nExamples in training set: ' , str(len(x_train)))\nprint ('Examples in testing set: ' , str(len(x_test)))\nprint ('Dimensions of training set: ' , str(np.asarray(x_train).shape))\nprint ('Dimensions of testing set: ' , str(np.asarray(x_test).shape))    \n", "intent": "Now let's create a test/train data set split.\n"}
{"snippet": "tic = time()\ncnt_vec = CountVectorizer(tokenizer=tokenizer.tokenize, analyzer='word', ngram_range=(1,1), max_df=0.8, min_df=2, max_features=1000, stop_words=stop_words)\ncnt_vec.fit(X_train)\ntoc = time()\nprint \"elapsed time: %.2f sec\" %(toc - tic)\nvocab = cnt_vec.vocabulary_\nidx2word = {val: key for (key, val) in vocab.items()}\nprint \"vocab size: \", len(vocab)\nX_train_vec = cnt_vec.transform(X_train) \nX_test_vec = cnt_vec.transform(X_test)\n", "intent": "We'll use a count vectorizer to produce a vector of word counts for each document while filtering stop and low-frequency words.\n"}
{"snippet": "data = pd.read_csv(\"adult.csv\", index_col=0)\n", "intent": "Apply dummy encoding and scaling to the \"adult\" dataset consisting of income data from the census.\nBonus: visualize the data.\n"}
{"snippet": "df2015a.to_csv('project3.csv')\n", "intent": "Present your conclusions and results, including a Tableau Storyboard. If you have more than one interesting model feel free to include.\n"}
{"snippet": "sf_crime = pd.read_csv('../../assets/datasets/sf_crime_train.csv')\nsf_crime = sf_crime.dropna(inplace=True)\n", "intent": "Multinomial Logit\nBasically we have more than two(binomial) outcomes or classes. Let's look at recent San Francisco data set\n"}
{"snippet": "subset = ['ASSAULT','VANDALISM'] \nsf_crime_sub = sf_crime[sf_crime['Category'].str.contains('|'.join(subset))]\nX = patsy.dmatrix('~ C(hour) + C(DayOfWeek) + C(PdDistrict) + X + Y', sf_crime_sub) \nY = sf_crime_sub.Category.values\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, stratify=Y, random_state=5)\n", "intent": "Setting Thresholds for binary Classes Let's rebuild our target to have two classes, and review how to optimize thresholds\n"}
{"snippet": "subset = ['ASSAULT','VANDALISM'] \nsf_crime_sub = sf_crime[sf_crime['Category'].str.contains('|'.join(subset))]\nX = patsy.dmatrix('~ C(hour) + C(DayOfWeek) + C(PdDistrict) + X + Y', sf_crime_sub) \nY = sf_crime_sub.Category.values\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, stratify=Y, random_state=5)\n", "intent": "Setting Thresholds for binary Classes\nLet's rebuild our target to have two classes, and review how to optimize thresholds\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_norm =  StandardScaler().fit_transform(X)\nmodel.fit(X_norm, y)\ncoeffs = pd.DataFrame(model.coef_, columns = iris.feature_names, index =iris.target_names)\ncoeffs\n", "intent": "Let's normalize the data and repeat the exercise:\n"}
{"snippet": "lb = preprocessing.LabelBinarizer()\nlb.fit_transform(['yes', 'no', 'no', 'yes'])\n", "intent": "Binary targets transform to a column vector\n"}
{"snippet": "cdf = pd.read_csv('../../assets/datasets/cars.csv')\n", "intent": "Visualize the last tree. Can you make sense of it? What does this teach you about decision tree interpretability?\n"}
{"snippet": "pd.DataFrame(cm, index=[le.classes_, columns=['Predicted_'+ X for i in le.classes_]])\n", "intent": "now to build a dataframe...\n"}
{"snippet": "DATA_PATH = '/data/vision/fisher/data1/vsmolyakov/time_series/ECG5000/'\ntrain_df = pd.read_csv(DATA_PATH + \"/ECG5000_TRAIN\")  \ntest_df = pd.read_csv(DATA_PATH + \"/ECG5000_TEST\")\nX_train, y_train = train_df.iloc[:,1:].values, train_df.iloc[:,0].values\nX_test, y_test = test_df.iloc[:,1:].values, test_df.iloc[:,0].values\n", "intent": "The ECG5000 dataset can be downloaded from http://www.cs.ucr.edu/~eamonn/time_series_data/\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfeatures = [c for c in df.columns if c != 'acceptability']\nfor c in df.columns:\n    df[c] = le.fit_transform(df[c])\nX = df[features]\ny = df['acceptability']\n", "intent": "Since most of the features are categorical text we will need to encode them as numbers using the LabelEncoder:\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\nprint y_train[:3]\ny_train, y_test = ohe.fit_transform(y_train), ohe.fit_transform(y_test)\nprint y_train[:3]\n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "X = data[0]\ny = data[1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nY_train = np_utils.to_categorical(y_train, 2) \nY_test = np_utils.to_categorical(y_test, 2)\nprint(\"y_train labels: \",y_train.shape)\nprint(\"Y_train one-hot labels: \\n\",Y_train.shape)\n", "intent": "Convert to one-hot labels.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)\n", "intent": "Split your data in to train and test sets.\n"}
{"snippet": "import keras\nfrom keras.datasets import cifar10\nfrom tflearn.datasets import cifar10\n(x_train, y_train), (x_test, y_test) =cifar10.load_data(dirname=\"/tmp/data\")\n", "intent": "---\nIn this notebook, we train a CNN on augmented images from the CIFAR-10 database.\n"}
{"snippet": "merged_routes['Codeshare'] = merged_routes.Codeshare.fillna('N')\n", "intent": "Fill in Nan values:\n"}
{"snippet": "pd.DataFrame(stats, columns=['Average Degree', \n                             'Standard Dev Degree', \n                              \"Maxium degree\",\n                             'Density', \n                             'Diameter', \n                             'Average path lenth']\n            ).describe()\n", "intent": "And here we calculate statistical measures of different graph properties:\n"}
{"snippet": "network_stats = np.array(Stats)\nx = StandardScaler().fit_transform(network_stats)\n", "intent": "Put everything into a matrix, do PCA for dimensionality reduction & cluster:\n"}
{"snippet": "scatter_data = pd.DataFrame(principalComponents)\nscatter_data['name'] = airlines\nscatter_data['labels'] = cluster.labels_\nscatter_data['country'] = scatter_data.name.map(airline_to_countryid)\nscatter_data['ontime'] = scatter_data.name.map(Delays_data.set_index('On-time')['On-time (A14)'].to_dict())\nscatter_data['delay'] = scatter_data.name.map(Delays_data.set_index('On-time')['Avg. Delay'].to_dict())\nscatter = scatter_data[scatter_data.delay.notna()].copy()\n", "intent": "**Analyzing inference from eignevalues to Delays**\n"}
{"snippet": "np.random.seed(0)\ndata, target = make_classification(\n    n_samples=1000,\n    n_features=45,\n    n_informative=12,\n    n_redundant=7\n)\ntarget = target.ravel()\n", "intent": "Load dataset and target values:\n"}
{"snippet": "proj_main = pd.read_csv('project_main.csv', delimiter =';').set_index('project_id') \nproj_main.head(3)\n", "intent": "Now let's move to the next data, **project_main.csv**\nStarting with importing the data into a dataframe with pandas.\n"}
{"snippet": "proj_main.to_csv('cleaned_project_main.csv')\n", "intent": "Lastly, let's save it to new csv file, **cleaned_project_main.csv**.\n"}
{"snippet": "project_unit = pd.read_csv('cleaned_project_unit.csv')\nproject_main = pd.read_csv('cleaned_project_main.csv')\nproject_unit_main = project_main.merge(project_unit, on = ['project_id'])\nproject_unit_main.head(3)\n", "intent": "So, let's get our **cleaned_project_unit** and **cleaned_project_main** to be ready to use.\n"}
{"snippet": "item_feature.to_csv('item_feature.csv', index = False)\n", "intent": "Finally, let's save it to new csv file, **item_feature.csv**.\n"}
{"snippet": "userLog = pd.read_csv('userLog_201801_201802_for_participants.csv', delimiter = ';')\n", "intent": "Let's apply the same process to **userLog_201801_201802_for_participants.csv**.\n"}
{"snippet": "userLog.to_csv('user_feature.csv', index = False)\n", "intent": "Lastly, let's save it to new csv file, **user_feature.csv**.\n"}
{"snippet": "imp = Imputer(strategy='mean')\nX_replace_with_mean = imp.fit_transform(X_new)\n", "intent": "Then we replace the missing values with mean of the corresponding columns.\n"}
{"snippet": "col = ['rss','intercept'] + ['coef_X_%d'%i for i in range(1,30)]\nind = ['model_pow_%d'%i for i in range(1,30)]\ncoef_matrix_simple = pd.DataFrame(index=ind, columns=col)\nmodels_to_plot = {1:231,3:232,6:233,12:234,18:235,29:236}\nfor i in range(1,30):\n    coef_matrix_simple.iloc[i-1,0:i+2] = linear_regression(data, power=i, models_to_plot=models_to_plot)\n", "intent": "In the cell below we show the fit for a number of polynomials of varying degrees. \nWhich do you think show bias? And which over-fitting?\n"}
{"snippet": "scaler = StandardScaler()\nX = scaler.fit_transform(X)\n", "intent": "Scale the training data X using the standard scalar. Store in X\n"}
{"snippet": "scaler = MinMaxScaler(feature_range=(0,1))\ndataset = scaler.fit_transform(dataset)\n", "intent": "We scale the closing price to 0 to 1 range:\n"}
{"snippet": "loans = pd.read_csv('loan_data.csv')\nloans.head()\n", "intent": "Read loan_data.csv as a dataframe called loans\n"}
{"snippet": "df = pd.read_csv('KNN_Project_Data')\ndf.head(3)\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "** Create a StandardScaler() object**\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head(3)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)\n", "intent": "**Creating and Training and Test data **\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=77)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "train = pd.read_csv('titanic_train.csv')\ntrain.head(2)\n", "intent": "We will be working with the [Titanic Data Set from Kaggle](https://www.kaggle.com/c/titanic) downloaded as titanic_train.csv file\n"}
{"snippet": "data = pd.read_csv('data/data.csv')\ndata.info()\n", "intent": "   - Download [spotify dataset from Kaggle](https://www.kaggle.com/geomack/spotifyclassification)\n"}
{"snippet": "node_model = manifold.LocallyLinearEmbedding(n_components=2, n_neighbors=6, eigen_solver='dense')\nembedding = node_model.fit_transform(X.T).T\n", "intent": "Compute a 2D embedding for visualization:\n"}
{"snippet": "messages = pd.read_csv('smsspamcollection/SMSSpamCollection',\n                       sep='\\t',names=['label','message'])\nmessages[:5]\n", "intent": "We'll use **read_csv** and make note of the **sep** argument, we can also specify the desired column names by passing in a list of *names*.\n"}
{"snippet": "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n", "intent": "We'll go ahead and check what is the IDF (inverse document frequency) of the word **`months`**\n"}
{"snippet": "X0 = df.values.astype(float)\nscaler = StandardScaler()\nX = scaler.fit_transform(X0)\n", "intent": "First, let us consider the case of scaled data via `StandardScaler`.\n"}
{"snippet": "pca = PCA()\npca.fit(X)\nfeatures = range(1,pca.n_components_+1)\n", "intent": "We fit use PCA and fit to the data.\n"}
{"snippet": "pca = PCA(n_components=3)\npca_transformed = pca.fit_transform(df.values.astype(float))\n", "intent": "Now we proceed to perform PCA reduction to $D=3$.\n"}
{"snippet": "X0 = df.values.astype(float)\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X0)\n", "intent": "Finally, we attempt to perform a clustering of the complete data set without performing a dimensional reduction. We start with a scaling:\n"}
{"snippet": "df = pd.read_csv('processed_data/df.csv')\ndf_type = pd.read_csv('processed_data/df_type.csv')\n", "intent": "`df` contains the complete data set we used in clustering and `df_type` contains the identification for 911 fire calls `Type`:\n"}
{"snippet": "matrix = df_transactions.pivot_table(index=['customer_name'], columns=['offer_id'], values='n').rename_axis(None, axis=1)\nmatrix = matrix.fillna(0).reset_index().rename(columns={'customer_name':'C_Name'})\nmatrix.head()\n", "intent": "Let us obtain a dataframe with such characteristics. We may use `pivot_table`:\n"}
{"snippet": "data2 = pd.DataFrame({'Age':  [17,64,18,20,38,49,55,25,29,31,33], 'Salary': [25,80,22,36,37,59,74,70,33,102,88], 'Loan Default': [1,0,1,0,1,0,0,1,1,0,1]})\ndata2\n", "intent": "Let's consider a more complex example by adding the \"Salary\" variable (in the thousands of dollars per year).\n"}
{"snippet": "calendar = pd.read_csv('calendar.csv.gz')\nprint('We have', calendar.date.nunique(), 'days and', calendar.listing_id.nunique(), 'unique listings in the calendar data.')\n", "intent": "How busy is it for Airbnb hosts in Toronto?\n"}
{"snippet": "train_raw = pd.read_csv('datasets/dataset_5_train.txt' , header = None)\ntest_raw = pd.read_csv('datasets/dataset_5_test.txt', header = None)\nex_table = pd.read_csv('datasets/dataset_5_description.txt', skiprows = 12 , delimiter = '\\t')\nex_text = pd.read_csv('datasets/dataset_5_description.txt', delimiter = ':', skiprows = range(11,26))\ntrain_raw.head()\nex_census = pd.read_csv('datasets/dataset_5_description.txt', delimiter = '\\t', skiprows = 12)\ntrain_raw.head()\n", "intent": "<font color = 'blue'>\n</font>\n<br>\n<br>\n"}
{"snippet": "results_array = np.array([[scorea_test , prun_score , dfeat_score] ,\n                          [gina_test , gin1_test , gin2_test] , \n                          [enta_test , ent1_test , ent2_test]])\nresults_df = pd.DataFrame(results_array , columns = ['Simple' , 'Prune' , 'Double_Feat'] ,\n                          index = ['score' , 'Gini' , 'Entropy'])\nresults_df\n", "intent": "<br>\n** c. Aggregating Results:**\n<br>\n"}
{"snippet": "data_raw = pd.read_csv('datasets\\dataset_1.txt')\ndata_raw.head(10)\n", "intent": "<br>\n**1. Load Data set and inspect it:**\n"}
{"snippet": "data_2 = pd.read_csv('datasets\\dataset_2.txt')\ndata_2.head(10)\n", "intent": "<br>\n**1. Read dataset and examine columns:**\n"}
{"snippet": "data_2_expanded = pd.DataFrame({}) \nfor column in data_2_new.columns:\n    if(data_2_new[column].dtype != np.dtype('object')):\n        data_2_expanded = pd.concat([data_2_expanded, data_2_new[column]], axis=1)\n    else:\n        encoding = pd.get_dummies(data_2_new[column])\n        data_2_expanded = pd.concat([data_2_expanded, encoding], axis=1)\nprint '\\n The number of predictors after the one-hot binary encoding is : ', data_2_expanded.iloc[:,:-1].shape[1]\nprint ' Which is the required number as stated\\n'\n", "intent": "**3. Apply get dummies for the rest of the categorical variables and expand the dataframe:**\n"}
{"snippet": "itrain, itest = train_test_split(xrange(df.shape[0]), train_size=0.6)\nmask=np.ones(df.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\n", "intent": "Here we'll consider classification but Decision trees can be use for regression as we know.\n"}
{"snippet": "def references_organisation(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == \"ORG\" for word in parsed])\ndata[\"references_organisation\"] = data[\"title\"].fillna(u\"\").map(references_organisation)\ndata[data[\"references_organisation\"]][[\"title\"]].head()\n", "intent": "Let's see if we can find organisations in our page titles\n"}
{"snippet": "titles = data[\"title\"].fillna(\"\")\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectoriser = CountVectorizer(max_features = 1000,\n                             ngram_range = (1, 2),\n                             stop_words = \"english\",\n                             binary = True)\nvectoriser.fit(titles)\nX = vectoriser.transform(titles)\n", "intent": "We previously used the `CountVectorizer` to extract text features for this classification task\n"}
{"snippet": "import numpy as np\nfrom sklearn import preprocessing\nlabels = np.array([1,5,3,2,1,4,2,1,3])\nlb = preprocessing.LabelBinarizer()\nlb.fit(labels)\nlb.transform(labels)\n", "intent": "Transform labels into binary vectors using scikit learn LabelBinarizer.\n"}
{"snippet": "listings['security_deposit'] = listings['security_deposit'].fillna(value=0)\nlistings['security_deposit'] = listings['security_deposit'].replace( '[\\$,)]','', regex=True ).astype(float)\nlistings['cleaning_fee'] = listings['cleaning_fee'].fillna(value=0)\nlistings['cleaning_fee'] = listings['cleaning_fee'].replace( '[\\$,)]','', regex=True ).astype(float)\n", "intent": "Same way to clean up the other money value columns.\n"}
{"snippet": "for col in df.columns:\n    try:\n        mean = df[col].mean()\n        df[col] = df[col].fillna(value=mean)\n    except:\n        pass\ndf.head()\n", "intent": "Old: Fill all the null values with zero.\n"}
{"snippet": "scaled_features = pd.DataFrame(scaled_features, columns=bank_notes.drop('Class',axis=1).columns)\nscaled_features.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "colleges = pd.read_csv('College_Data', index_col = 0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "classified_data = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "data_features = pd.DataFrame(scaled_features, columns=classified_data.columns[:-1])\ndata_features.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "import json\nimport urllib.request\nurl = \"http://api.fixer.io/2014-01-03\"\nresponse_bytes = urllib.request.urlopen(url).read()\nprint(response_bytes)\n", "intent": "The following snippet demonstrates how to fetch exchange rate EUR to X for a given date using a so called \"web service\".\n"}
{"snippet": "import pandas as pd\ndataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\ndata = pd.read_csv(dataset_url, delimiter=\";\")\nprint(\"shape (rows, cols) is\", data.shape)\nprint(\"column names are\", data.columns)\nprint(data.head())  \nprint(data.tail())  \n", "intent": "pandas allows reading different file formats from different sources:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconmat = np.array(confusion_matrix(Y_test, Y_pred))\nconfusion = pd.DataFrame(conmat, index=['is_healthy', 'has_cancer'],\n                         columns=['predicted_healthy', 'predicted_cancer'])\nconfusion\n", "intent": "Look at the confusion matrix\n"}
{"snippet": "df_with_year = df['age'] > 1000\ndf.loc[df_with_year, 'age'] = 2015 - df.loc[df_with_year, 'age']\ndf.loc[df.age > 95, 'age'] = np.nan\ndf.loc[df.age < 16, 'age'] = np.nan\ndf['age'].fillna(-1, inplace=True)\n", "intent": "Convert year to age, set limits to age, and fill NaNs with -1.\n"}
{"snippet": "lasso_coefs = pd.DataFrame({'variable':X.columns, 'coef':lasso.coef_,\n                            'abs_coef':np.abs(lasso.coef_)})\nlasso_coefs.sort_values('abs_coef', inplace=True, ascending=False)\nlasso_coefs.head(30)\n", "intent": "Many indicators have been zeroed out, leaving only 86 out of over a thousand remaining to have an effect.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXn = ss.fit_transform(X)\n", "intent": "Lasso regularization selects for only a subset of the provided variables. It is very useful for removing the effects of redundant variables\n"}
{"snippet": "from sklearn.linear_model import ElasticNetCV\ntarget = 'SalePrice'\nnon_target = [col for col in res_house.columns.tolist() if col != target]\nformula = target + ' ~ ' + ' + '.join(non_target) + ' -1'\ny, X = patsy.dmatrices(formula, data=res_house, return_type=\"dataframe\")\nXn = ss.fit_transform(X)\noptimal_enet = ElasticNetCV(l1_ratio=np.linspace(0.01, 1.0, 50), n_alphas=500, cv=5, verbose=1)\noptimal_enet.fit(Xn, y)\nprint optimal_enet.alpha_\nprint optimal_enet.l1_ratio_\n", "intent": "The Elastic Net is a combination of Ridge and Lasso regression.\n"}
{"snippet": "candidates = \"realDonaldTrump BernieSanders\".split(\" \")\ncandidatesRDD = sc.parallelize(candidates)\\\n    .flatMap(lambda s: [(t.user.screen_name, t.text.encode('ascii', 'ignore')) for t in getTweets(s)])\\\n    .map(lambda s: (s[0], s[1]))\\\n    .reduceByKey(lambda s,t: s + '\\n' + t)\\\n    .filter(lambda s: len(re.findall(r'\\w+', s[1])) > 100 )\\\n    .map(lambda s: [s[0]] + getPersonalityInsight(s[1]))\ncandidatesPIDF = sqlContext.createDataFrame(\n   candidatesRDD, schema\n)\n", "intent": "For this project, the users are Trump and Bernie\n"}
{"snippet": "def select_item(x):\n    x = x.iloc[0]\n    if len(x) == 0:\n        return np.nan\n    else:\n        return x\nbusw = pd.pivot_table(bus, index=['business_id', 'name', 'review_count', 'city', 'stars', 'categories'],\n                      columns='variable', values='value', aggfunc=select_item)\nbusw.reset_index(inplace=True)\nbusw.head(3)\n", "intent": "Obviously, multiple rows refer to the same business. The data is in 'long' format to identify the variables.\nConverting to 'wide' format:\n"}
{"snippet": "busw.fillna(0, inplace=True)\nbusw.replace('None', 0, inplace=True)\nbusw.replace(True, 1, inplace=True)\nbusw.replace(False, 0, inplace=True)\nbusw.info()\n", "intent": "True, None, and False values need to be broken up to binary values\n"}
{"snippet": "use.fillna(0, inplace=True)\nuse.drop('Unnamed: 0', axis=1, inplace=True)\nuse.head(3)\n", "intent": "Empty values and elite identification need to be converted to binary values.\n"}
{"snippet": "df = pd.read_csv('data/WA_Fn-UseC_-HR-Employee-Attrition.csv')\n", "intent": "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset\n"}
{"snippet": "scaler = sklearn.preprocessing.MinMaxScaler()\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "sessions.loc[sessions.action == 'lookup', 'action_type'] = 'lookup'\nsessions.loc[sessions.action == 'lookup', 'action_detail'] = 'lookup'\nsessions.loc[sessions.action == 'track_page_view', 'action_type'] = 'track_page_view'\nsessions.loc[sessions.action == 'track_page_view', 'action_detail'] = 'track_page_view'\nsessions.action_type = sessions.action_type.fillna('missing')\nsessions.action_detail = sessions.action_detail.fillna('missing')\n", "intent": "The rest are easy, and lastly, we fill \"missing\" to the ones we can't find them a home.\n"}
{"snippet": "umportant_features = pd.DataFrame(final_clf.feature_importances_).T\numportant_features.columns = list(train.columns)\numportant_features = umportant_features.T\numportant_features.columns = ['feature_importance']\numportant_features.sort_values('feature_importance', ascending=False)\n", "intent": "79% Accuracy on test set\n"}
{"snippet": "df['avg_rating_by_driver'].fillna(df['avg_rating_by_driver'].mean(), inplace=True)\ndf['avg_rating_of_driver'].fillna(df['avg_rating_of_driver'].mean(), inplace=True)\ndf['phone'].fillna('iPhone', inplace=True)\ndf.info()\n", "intent": "There are few missing values. We can impute these columns with mean of the column\n"}
{"snippet": "ex_user = pd.DataFrame([0.77, 5., 4.3, 1., 0., 3., 100.,\n          1., 0., 0., 0., 1., 1., 0.])\nex_user = ex_user.T\nex_user.columns = df_encoded.columns\nex_user\n", "intent": "Here we test our neural network predicting feature with an example user.\n"}
{"snippet": "embed_tsne = tsne.fit_transform(np.random.random((63000,200)))\n", "intent": "embed_tsne = tsne.fit_transform(random.random(63000,200))\n"}
{"snippet": "g=d[d['type'] == 'EMS' ]\np=pd.pivot_table(g, values='e', index=['timeStamp'], columns=['title'], aggfunc=np.sum)\npp=p.resample('W', how=[np.sum]).reset_index()\npp.head()\n", "intent": "Pivot Table\n-----------\n"}
{"snippet": "teams_df = pd.DataFrame(Teams)\nprint(teams_df.head())\n", "intent": "Using pandas, convert the results to a DataFrame and print the first 5 rows using the head() method to ensure it looks right.\n"}
{"snippet": "house = pd.read_csv(\"C:/ProgramData/Anaconda3/SeattleHousing/kc_house_data.csv\")\nhouse.head()\n", "intent": "Let's first read in the house data as a dataframe \"house\" and inspect the first 5 rows\n"}
{"snippet": "X = scaled_features \ny = df['TARGET CLASS']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state =101)\n", "intent": "do \"shift-tab\" so that you can scroll-down and find example\n"}
{"snippet": "X = yelp_class['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "sessions[\"secs_elapsed\"] = sessions.groupby(\"action\").transform(lambda x: x.fillna(x.median()))\n", "intent": "Fill the missing secs_elapsed with the median secs_elapsed for each action.\n"}
{"snippet": "scaled_df_bank = pd.DataFrame(data=scaled_bank,columns=bank.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "soccer_data = pd.read_csv('CrowdstormingDataJuly1st.csv', sep=',', parse_dates=['birthday'])\nsoccer_data.head()\n", "intent": "In this part, we review obtained data and clean them before doing any manipulation with (un)supervised classifiers.\n"}
{"snippet": "soccer_data_clean[['height', 'weight']] = soccer_data_clean[['height', 'weight']].fillna(soccer_data_clean[['height', 'weight']].mean())\n", "intent": "For height and weight, we decide to use mean of values to replace null values.\n"}
{"snippet": "y = preprocessing.LabelEncoder().fit_transform(soccer_data_all_features[classes_column])\n", "intent": "Let's encode the classes again, for further comparisons.\n"}
{"snippet": "df_aliases = pd.read_csv('hillary-clinton-emails/Aliases.csv', index_col=0)\n", "intent": "First, we import all data as DataFrames.\n"}
{"snippet": "vect = CountVectorizer()\nvect.fit(X_train)\nX_train_dtm = vect.transform(X_train)\nX_test_dtm = vect.transform(X_test)\nX_test_dtm\n", "intent": "Use CountVectorizer to create **document-term matrices** from X_train and X_test.\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "Scale the data so high values won't impact on the distance between the data points.\n"}
{"snippet": "X = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\ny = iris['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "** Split your data into a training set and a testing set.**\n"}
{"snippet": "from sklearn import datasets, svm, metrics\ndigits = datasets.load_digits()\nprint(digits.data.shape)\ndigits.data \n", "intent": "The [digits dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\n"}
{"snippet": "df.first_affiliate_tracked = df.first_affiliate_tracked.fillna(\"untracked\")\n", "intent": "Set the missing values for \"first_affiliate_tracked\" to \"untracked\".\n"}
{"snippet": "train, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000, valid_portion=0.1)\ntrainX, trainY = train\ntestX, testY = test\n", "intent": "Siraj's [code](https://github.com/llSourcell/How_to_do_Sentiment_Analysis)\n"}
{"snippet": "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n", "intent": "Firstly, we have to create two data sets: training set and test set using built-in method from scilearn.\n"}
{"snippet": "df = pd.DataFrame(tp250)\n", "intent": "Keep the fields:\n    num_votes\n    rating\n    tconst\n    title\n    year\nAnd discard the rest\n"}
{"snippet": "M = len(df[male_mask].Sex.values)\nF = len(df[female_mask].Sex.values)\ncolumn_gender = [\"Male\",\"Female\"]\nrows_gender = [M,F]\nindex_gender = [0]\nMale_Female_Hist = dict(zip(column_gender,rows_gender))\nMale_Female_Hist = pd.DataFrame(Male_Female_Hist, index = index_gender)\nMale_Female_Hist.head()\nMale_Female_Hist_plot = sns.barplot(data = Male_Female_Hist )\nsns.axlabel(\"Gender\", \"Count\")\n", "intent": "** Not representative of total travelers in the dataset**\n"}
{"snippet": "name = ('id number','thickness','uniformity size','uniformity shape','adhesion','cell size','nuclei','chromatin','nucleoli','mitoses','class')\ndf = pd.read_csv('../../assets/datasets/breast-cancer-wisconsin.csv', names = name, na_values='?')\ndf.head()\ndf.dropna(inplace = True)\ndf.head()\n", "intent": "The column names are taken from the dataset info file. Create an array\nwith the column names and assign them as the header when loading the\ncsv.\n"}
{"snippet": "conmat = np.array(confusion_matrix(Ytest, Ypred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['over_200k', 'under_200k'],\n                        columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "df = pd.read_csv('../../assets/datasets/cars.csv')\ndf.head()\n", "intent": "Visualize the last tree. Can you make sense of it? What does this teach you about decision tree interpretability?\n"}
{"snippet": "wine = pd.read_csv('../../assets/datasets/wine_v.csv')\n", "intent": "First, let's import our data. We'll be using *Iris* again for this exercise. \n"}
{"snippet": "for c in pure_geo_columns:\n    dat.loc[:, c] = dat[c].cat.add_categories('N/A').fillna('N/A')\n", "intent": "---------------------------------------\n"}
{"snippet": "min_max_scaler = preprocessing.MinMaxScaler()\nfor feature in cont_features:\n    df.loc[:,feature] = min_max_scaler.fit_transform(df[[feature]])\n", "intent": "Preprocessing continuous variables\n"}
{"snippet": "jlt = Cats.Catalog()\nlogm, logsfr, w = jlt.Read('tinkergroup')\n", "intent": "First lets import a catalog\n"}
{"snippet": "nsa = Cats.Catalog()\nlogm, logsfr, w = nsa.Read('nsa_dickey')\n", "intent": "This seems promising for high masses. Lets try the NSA\n"}
{"snippet": "df_married = pd.DataFrame()\ndf_married['not_treated'] = [0, 0]\ndf_married['treated'] = [0, 0]\ndf_married.index = ['not_married', 'married']\ndf_married.loc['not_married', 'not_treated'] = len(df_not_treated[df_not_treated['married'] == 0])\ndf_married.loc['married', 'not_treated'] = len(df_not_treated[df_not_treated['married'] == 1])\ndf_married.loc['not_married', 'treated'] = len(df_treated[df_treated['married'] == 0])\ndf_married.loc['married', 'treated'] = len(df_treated[df_treated['married'] == 1])\ndf_married\n", "intent": "In order to present this data, we need to calculate number of married and not married subjet's for both groups.\n"}
{"snippet": "df_no_degree = pd.DataFrame()\ndf_no_degree['not_treated'] = [0, 0]\ndf_no_degree['treated'] = [0, 0]\ndf_no_degree.index = ['no_degree', 'have_degree']\ndf_no_degree.loc['no_degree', 'not_treated'] = len(df_not_treated[df_not_treated['nodegree'] == 0])\ndf_no_degree.loc['have_degree', 'not_treated'] = len(df_not_treated[df_not_treated['nodegree'] == 1])\ndf_no_degree.loc['no_degree', 'treated'] = len(df_treated[df_treated['nodegree'] == 0])\ndf_no_degree.loc['have_degree', 'treated'] = len(df_treated[df_treated['nodegree'] == 1])\ndf_no_degree\n", "intent": "In order to present this data, we need to calculate number of subjet's with and without degree for both groups.\n"}
{"snippet": "conf_matrix = confusion_matrix(y, y_predictions)\ndf_conf_matrix = pd.DataFrame(conf_matrix)\ndf_conf_matrix.index = ['true_not_treated','true_treated']\ndf_conf_matrix.columns = ['predicted_not_treated', 'predicted_treated']\nsn.set(font_scale=1.3)\nsn.heatmap(df_conf_matrix, annot=True, cmap=\"Greens\", fmt='g')\n", "intent": "To see how good our model is we have decided to present it's confusion matrix.\n"}
{"snippet": "df_married = pd.DataFrame()\ndf_married['not_treated'] = [0, 0]\ndf_married['treated'] = [0, 0]\ndf_married.index = ['not_married', 'married']\ndf_married.loc['not_married', 'not_treated'] = len(df_match[df_match['married_not_treated'] == 0])\ndf_married.loc['married', 'not_treated'] = len(df_match[df_match['married_not_treated'] == 1])\ndf_married.loc['not_married', 'treated'] = len(df_match[df_match['married_treated'] == 0])\ndf_married.loc['married', 'treated'] = len(df_match[df_match['married_treated'] == 1])\ndf_married\n", "intent": "Same as in Task 2 we calculate the number of married and not married subjet's for both groups.\n"}
{"snippet": "df_no_degree = pd.DataFrame()\ndf_no_degree['not_treated'] = [0, 0]\ndf_no_degree['treated'] = [0, 0]\ndf_no_degree.index = ['no_degree', 'have_degree']\ndf_no_degree.loc['no_degree', 'not_treated'] = len(df_match[df_match['nodegree_not_treated'] == 0])\ndf_no_degree.loc['have_degree', 'not_treated'] = len(df_match[df_match['nodegree_not_treated'] == 1])\ndf_no_degree.loc['no_degree', 'treated'] = len(df_match[df_match['nodegree_treated'] == 0])\ndf_no_degree.loc['have_degree', 'treated'] = len(df_match[df_match['nodegree_treated'] == 1])\ndf_no_degree\n", "intent": "And finally, we need to calculate number of subjet's with and without degree for both groups.\n"}
{"snippet": "df = pd.DataFrame(importances[np.where(importances>0)])\nplot = df.plot.hist(figsize=(10, 5), bins=50)\nplot.set(xlabel=\"Feature Importance\", ylabel=\"Count\")\nplot\n", "intent": "We can even verify this asusmption by using a histogram to present the values of importance of features who have a non-zero importance.\n"}
{"snippet": "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', header=0, sep=';')\n", "intent": "Data: https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n"}
{"snippet": "us_canada_user_rating_pivot2 = us_canada_user_rating.pivot(index = 'userID', columns = 'bookTitle', values = 'bookRating').fillna(0)\n", "intent": "Perfect! \"Green Mile Series\" books are definitely should be recommended one after another.   \n"}
{"snippet": "noise = [0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.25,1.5,1.75,2.0]\nn_clusters = []\nfor i in noise:\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=200, centers=centers, cluster_std=i,random_state=101)\n    af_model=AffinityPropagation(preference=-50,max_iter=500,convergence_iter=15,damping=0.5).fit(X)\n    n_clusters.append(len(af_model.cluster_centers_indices_))  \n", "intent": "Create data sets with varying degree of noise std. dev and run the model to detect clusters. Also, play with damping parameter to see the effect.\n"}
{"snippet": "df = pd.read_csv('Datasets/loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n", "intent": "**Instantiate a scaler standardizing estimator**\n"}
{"snippet": "noise = [0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.25,1.5,1.75,2.0]\nn_clusters = []\nfor i in noise:\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=200, centers=centers, cluster_std=i,random_state=101)\n    ms_model=MeanShift().fit(X)\n    n_clusters.append(len(ms_model.cluster_centers_))\n", "intent": "Create data sets with varying degree of noise std. dev and run the model to detect clusters.\n"}
{"snippet": "df['Cancer'] = pd.DataFrame(cancer['target'])\ndf.head()\n", "intent": "** Adding the target data to the DataFrame**\n"}
{"snippet": "import pandas as pd\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names).assign(species=iris.target_names[iris.target])\n", "intent": "We probably want to convert the data into a more convenient structure, namely, a `DataFrame`.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(iris.data)\nX_std[:5]\n", "intent": "Let's apply a standardization transformation from scikit-learn:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=3, whiten=True).fit(iris.data)\nX_pca = pca.transform(iris.data)\n", "intent": "`scikit-learn` provides a PCA transformation in its `decomposition` module. \n"}
{"snippet": "wine = pd.read_table('../data/wine.dat', sep='\\s+')\nwine.head()\n", "intent": "Import the wine dataset and perform PCA on the predictor variables, and decide how many principal components would you select.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ny = indicator.Ease_Bus\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n", "intent": "Split the data into training and test\n"}
{"snippet": "X, y = datasets.make_moons(50, noise=0.20)\nX = X.astype(np.float32)\ny = y.astype(np.int32)\n", "intent": "Since we used the scikit-learn interface, its easy to take advantage of the `metrics` module to evaluate the MLP's performance.\n"}
{"snippet": "from sklearn import linear_model\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\nY = iris.target\nprint( iris.DESCR)\n", "intent": "Load the data set from Scikit Learn\n"}
{"snippet": "coeff_poly = pd.DataFrame(model_poly.coef_,index=df_poly.drop('y',axis=1).columns, \n                          columns=['Coefficients polynomial model'])\ncoeff_poly\n", "intent": "** Recall that the originating  function  is: ** \n$ y= 5x_1^2+13x_2+0.1x_1x_3^2+2x_4x_5+0.1x_5^3+0.8x_1x_4x_5+noise $\n"}
{"snippet": "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n", "intent": "- As with the Iris data: split the data into training and testing sets, then fit a Gaussian naive Bayes model against the test data.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "- Let's demonstrate the naive approach to validation using the Iris data.\n"}
{"snippet": "import pandas as pd\npd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n", "intent": "- It is easier to inspect if we convert this to a ``DataFrame`` with labeled columns:\n"}
{"snippet": "categories = ['talk.religion.misc', 'soc.religion.christian',\n              'sci.space', 'comp.graphics']\ntrain = fetch_20newsgroups(subset='train', categories=categories)\ntest = fetch_20newsgroups(subset='test', categories=categories)\n", "intent": "- For simplicity we select a few of these categories and download the training and testing set:\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nx = np.array([2, 3, 4])\npoly = PolynomialFeatures(3, include_bias=False)\npoly.fit_transform(x[:, None])\n", "intent": "- Polynomial projection is built into Scikit-Learn using the ``PolynomialFeatures`` transformer:\n"}
{"snippet": "from pandas.tseries.holiday import USFederalHolidayCalendar\ncal = USFederalHolidayCalendar()\nholidays = cal.holidays('2012', '2016')\ndaily = daily.join(pd.Series(1, index=holidays, name='holiday'))\ndaily['holiday'].fillna(0, inplace=True)\n", "intent": "- We expect riders to behave differently on holidays. let's add an indicator of this as well:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=324)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nSplit the Dataset into Training and Test Datasets\n<br><br></p>\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=42)\n", "intent": "- Split the data into training and testing sets\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.keys()\n", "intent": "- Let's reuse the hand-written digits dataset here to see how the random forest classifier can be used.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.data.shape\n", "intent": "- The benefit of dimensionality reduction becomes much more apparent when looking at high-dimensional data. Let's apply PCA to the digits dataset.\n"}
{"snippet": "pca = PCA(2)  \nprojected = pca.fit_transform(digits.data)\nprint(digits.data.shape)\nprint(projected.shape)\n", "intent": "- Use PCA to project the 8x8 (64-dimensional) images to a more manageable number.\n"}
{"snippet": "pca = PCA(0.50).fit(noisy)\npca.n_components_\n", "intent": "- It's clear that the images are noisy. Let's train a PCA on the noisy data, requesting that the projection preserve 50% of the variance.\n"}
{"snippet": "from sklearn.datasets.samples_generator import make_blobs\nX, y_true = make_blobs(n_samples=400, centers=4,\n                       cluster_std=0.60, random_state=0)\nX = X[:, ::-1] \n", "intent": "- We already known that given simple, well-separated data, *k*-means finds suitable clustering results.\n"}
{"snippet": "df_scaled = pd.DataFrame(scaled,columns=df.columns[:-1])\ndf_scaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"C:/Downloads/Data Science and Machine Learning with Python/DataScience/DataScience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\n_, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itest] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "df = pd.read_csv('train.csv.gz', sep=',').dropna()\ndest = pd.read_csv('destinations.csv.gz')\ndf = df.sample(frac=0.01, random_state=99)\ndf.shape\n", "intent": "To be able to process locally, we randomly sample 1% of the records. After that, we still have a large number of records at 241,179.\n"}
{"snippet": "columns = data['alchemy_category'].unique()\nnew_df = pd.DataFrame(columns=columns)\n", "intent": "> \nPlot the rate of evergreen sites for all Alchemy categories.\n"}
{"snippet": "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\nX_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\nprint('Train dimension:');print(X_train.shape)\nprint('Test dimension:');print(X_test.shape)\nlb = LabelBinarizer()\ny_train = lb.fit_transform(y_train)\ny_test = lb.transform(y_test)\nprint('Train labels dimension:');print(y_train.shape)\nprint('Test labels dimension:');print(y_test.shape)\n", "intent": "As we can see our current data have dimension N * 28*28, we will start by flattening the image in N*784, and one-hot encode our target variable.\n"}
{"snippet": "df=pd.read_csv(\"../../data/raw/yelp.csv\")\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "ss = StandardScaler() \n", "intent": "** Crie um objeto StandardScaler() chamado scaler. **\n"}
{"snippet": "ad_data = pd.read_csv('advertising.csv')\nad_data.head()\n", "intent": "** Leia o arquivo advertising.csv e grave-o em um DataFrame chamado ad_data. **\n"}
{"snippet": "loans = pd.read_csv('loan_data.csv')\nloans.head()\n", "intent": "** Use pandas para ler loan_data.csv como um DataFrame chamado loans. **\n"}
{"snippet": "loans = pd.read_csv('')\n", "intent": "** Use pandas para ler loan_data.csv como um DataFrame chamado loans. **\n"}
{"snippet": "    from sklearn.datasets import load_boston\n    boston = load_boston()\n    print(boston.DESCR)\n    boston_df = boston.data\n", "intent": "    from sklearn.datasets import load_boston\n    boston = load_boston()\n    print(boston.DESCR)\n    boston_df = boston.data\n"}
{"snippet": "data = pd.read_csv('bank.csv', header=0)\ndata = data.dropna()\nprint(data.shape)\nprint(list(data.columns))\n", "intent": "This dataset provides the customer information. It includes 41188 records and 21 fields.\n"}
{"snippet": "SP500_price = pd.read_csv(\"SP500_price.csv\", sep = \",\")\n", "intent": "Second Dataset: the S&P500 Index Price download\n"}
{"snippet": "aapl_fb_goog_prices = pd.read_csv(\"aapl_fb_goog_prices.csv\", sep = \",\")\n", "intent": "Third Dataset: Stock prices of Facebook, Apple and Google\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import preprocessing\nlabel_encoder=preprocessing.LabelEncoder()\n", "intent": "- Prepare your dataset for modeling\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(features_df, target_df, test_size=0.33, random_state=5) \n", "intent": "Split into 66% training set and 33% testing set\n"}
{"snippet": "df_feat = pd.DataFrame(scaler_features, columns=df.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "import numpy as np\nfrom sklearn import preprocessing\nlabels = np.array([1,5,3,2,1,4,2,1,3])\nlb = preprocessing.LabelBinarizer()\nlb.fit(labels)\nlb.transform(labels)\n", "intent": "Transforming your labels into one-hot encoded vectors is pretty simple with scikit-learn using `LabelBinarizer`. Check it out below!\n"}
{"snippet": "std_scale = preprocessing.StandardScaler().fit(drop[['SQFT', 'BDRMS', 'AGE']])\ndf_std = std_scale.transform(drop[['SQFT', 'BDRMS', 'AGE']])\n", "intent": "[StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\n", "intent": "Use an SVM, GridSearchCV and the iris dataset to find an optimal model.\nMake sure to search for optimal paramaters for kernel, C, \n"}
{"snippet": "df = pd.read_csv('train.tsv', sep = '\\t')\n", "intent": "Split the dataset in to train and test. We are using training data only for EDA.\n"}
{"snippet": "amazon = np.asarray(pd.read_csv(\"amazon.csv\", encoding = \"ISO-8859-1\"))\nrotten_tomatoes = np.asarray(pd.read_csv(\"rotten_tomatoes.csv\", encoding = \"ISO-8859-1\"))\ntrain = np.asarray(pd.read_csv(\"train.csv\", encoding = \"ISO-8859-1\"))\ntest = np.asarray(pd.read_csv(\"test.csv\", encoding = \"ISO-8859-1\"))\nholdout = np.asarray(pd.read_csv(\"holdout.csv\", encoding = \"ISO-8859-1\"))\n", "intent": "Let's start by reading the given files.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_s = scaler.fit_transform(X)\n", "intent": "- Plot the standardized mean cross-validated accuracy against the unstandardized. Which is better?\n- Why?\n"}
{"snippet": "table = pd.DataFrame({'probability':[0.0, 0.1, 0.2, 0.25, 0.5, 0.6, 0.8, 0.9,1.0]})\ntable['odds'] = table.probability/(1 - table.probability)\ntable\n", "intent": "**As an example we can create a table of probabilities vs. odds, as seen below.**\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nlabels = ['CRIM','RM','B','LSTAT']\nX_train, X_test, y_train, y_test = train_test_split(X[labels], y, train_size=0.7, random_state=8)\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nlr.score(X_test, y_test)\n", "intent": "- Score and plot.  \n- How do your metrics change?\n"}
{"snippet": "from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33) \n", "intent": "Split your data in to train and test sets.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('H:/Technology/python/Dataset/pydata-book-master/ch06/ex1.csv')\nprint( df )\nprint( df.index )\nprint( df.columns )\nprint( df.values )\ndf\n", "intent": "<a id=\"csv\"></a>\n<hr>\n"}
{"snippet": "data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\n        'year': [2000, 2001, 2002, 2001, 2002],\n        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\nframe = pd.DataFrame(data)\nprint(\"TO STANDARD OUTPUT  with SEP\")\nframe.to_csv(sys.stdout, sep='|')\nprint(\"TO STANDARD OUTPUT  without HEADER \")\nframe.to_csv(sys.stdout, index=False, header=False)\n", "intent": "<a id=\"standardoutput\"></a>\n<hr>\n"}
{"snippet": "data = load_boston()\nX_ = data['data']\ny_ = data['target']\n", "intent": "__TODO__:\n  1. apply fixes for the data, to ensure statistical validity of the test and to prevent \"multiplicity\" issues\n"}
{"snippet": "def merge_columns(main, other):\n    result = pd.merge(left=main,right=other, how='outer', left_on='date', right_on='date')\n    return result\ntrends = pd.read_csv('bitcoin_trends.csv')\ndata = merge_columns(data, trends)\n", "intent": "Read in the Google Trends result and merge with the main data set.\n"}
{"snippet": "cv = CountVectorizer(min_df=NAME_MIN_DF)\nX_name = cv.fit_transform(merge['name'])\n", "intent": "Count vectorize name and category name columns.\n"}
{"snippet": "X = df_delay.loc[:, df_delay.columns != 'delay_log']\ny = df_delay[\"delay_log\"]\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.25, random_state=42)\n", "intent": "- Split the dataset into training and test, where test size is 0.25:\n"}
{"snippet": "lambdas = [.001,.005,1,5,10,50,100,500,1000]\nclfl = LassoCV(cv=10, alphas=lambdas, fit_intercept=True, normalize=False)\npreprocessing.StandardScaler()\nclfl.fit(X_train2, y_train2)\nprint('Lasso Train Score', clfl.score(X_train2, y_train2))\nprint('Lasso Test Score', clfl.score(X_test2, y_test2))\n", "intent": "- Use LASSO cross validation (10-fold) to select predictors:\n"}
{"snippet": "betas = np.transpose(logitCV.coef_[:,logitCV.coef_[0,:]>0])\nfeatures = X_train.columns[logitCV.coef_[0,:]>0]\nsignif_betas=pd.DataFrame([features, betas])\nsignif_betas\n", "intent": "5..(5pts) Given your model, comment on the importance of factors as related to whether a flight is delayed.\n"}
{"snippet": "df_sales_sample = pd.read_csv('df_sales_sample.csv')\n", "intent": "**Modeling & Evaluation**\n"}
{"snippet": "df_sales = pd.read_csv('df_sales.csv')\ndf_crime_summary_zip = pd.read_csv(\"crime_summary_zip.csv\")\ndf_crime_summary_zip\n", "intent": "**Modeling with Full sales data and crime counts for property zip-code for Year_Sold**\n"}
{"snippet": "reviews = []\nsentiments = []\nwith open(os.path.join('..', 'datasets', 'amazon-reviews.txt')) as f:\n    for line in f.readlines():\n        line = line.strip('\\n')\n        review, sentiment = line.split('\\t')\n        sentiment = np.nan if sentiment == '' else int(sentiment)\n        reviews.append(review)\n        sentiments.append(sentiment)\ndf = pd.DataFrame({'review': reviews, 'sentiment': sentiments})\n", "intent": "The data is about sentiments on Amazon reviews.\n"}
{"snippet": "reviews = []\nsentiments = []\nwith open(os.path.join('..', 'datasets', 'amazon-reviews.txt')) as f:\n    for line in f.readlines():\n        line = line.strip('\\n')\n        review, sentiment = line.split('\\t')\n        sentiment = np.nan if sentiment == '' else int(sentiment)\n        reviews.append(review.lower())\n        sentiments.append(sentiment)\ndf = pd.DataFrame({'review': reviews, 'sentiment': sentiments})\n", "intent": "The data is about sentiments on Amazon reviews.\n"}
{"snippet": "if not is_labels_encod:\n    encoder = LabelBinarizer()\n    encoder.fit(train_labels)\n    train_labels = encoder.transform(train_labels)\n    test_labels = encoder.transform(test_labels)\n    train_labels = train_labels.astype(np.float32)\n    test_labels = test_labels.astype(np.float32)\n    is_labels_encod = True\nprint('Labels One-Hot Encoded')\n", "intent": "** One-Hot Encoding:**\n"}
{"snippet": "assert is_features_normal, 'You skipped the step to normalize the features'\nassert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\ntrain_features, valid_features, train_labels, valid_labels = train_test_split(\n    train_features,\n    train_labels,\n    test_size=0.05,\n    random_state=832289)\nprint('Training features and labels randomized and split.')\n", "intent": "** Split Training and Validation Sets:**\n"}
{"snippet": "tv = TfidfVectorizer(max_features=MAX_FEATURES_ITEM_DESCRIPTION, ngram_range=(1, 3), stop_words='english')\nX_description = tv.fit_transform(merge['item_description'])\n", "intent": "TFIDF Vectorize item_description column.\n"}
{"snippet": "sac = pd.read_csv('datasets/Sacramentorealestatetransactions.csv')\nsac\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "sacr = pd.read_csv('../../DSI-SYD-1/classes/week-02/3.2-lab/assets/datasets/Sacramentorealestatetransactions.csv')\n", "intent": "<h1>Multiple Linear Regression</h1>\n"}
{"snippet": "import pandas as pd\nwheel = pd.read_csv('datasets/wheel.csv')\n", "intent": "The data is in wheel.csv.\nWe would like to understand the relationship between _seconds_ and _signal_\n"}
{"snippet": "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()\ntfids = tfidf.fit_transform(articles)\ntfids_test = tfids[:len(test),:]\ntfids_train = tfids[len(test):,:]\ncountvect = sklearn.feature_extraction.text.CountVectorizer()\ntfids_test\n", "intent": "<h1> Vectorizers </h1>\n"}
{"snippet": "data=pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/Bonus/titanic3.csv\");\nprint(\"Here are the first three rows:\")\ndata.iloc[0:3,:]\n", "intent": "Consider the Titanic dataset below\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import datasets\niris = datasets.load_iris()\n", "intent": "Import packages and read data.\n"}
{"snippet": "call_adjmatrix = pd.read_csv('./call.adjmatrix', index_col=0)\ncall_graph     = nx.from_numpy_matrix(call_adjmatrix.as_matrix())\n", "intent": "You are going to read the graph from an adjacency list saved in the earlier exercises.\n"}
{"snippet": "(pca.fit_transform(X_nrm)).round(2)\n", "intent": "Compare the 2 below: they are the embedding of users\n"}
{"snippet": "df = pd.DataFrame({'img_name': i2fn, 'age': i2age, 'gender': i2gender, 'race': i2race})\ndf.shape\n", "intent": "Combine the above maps into a pandas data frame.\n"}
{"snippet": "lb = LabelBinarizer(sparse_output=True)\nX_brand = lb.fit_transform(merge['brand_name'])\n", "intent": "Label binarize brand_name column.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=100)\n", "intent": "Divide the data into training and test sets with 80-20 split using scikit-learn. Print the shapes of training and test feature sets.\n"}
{"snippet": "GPU3 = pd.DataFrame([9,10,9,11,10,13,12,9,12,12,13,12,13,10,11])\n", "intent": "He is trying a third GPU - GPU3.\n"}
{"snippet": "import pandas as pandas\nSource = pandas.read_csv(\"bigcity.csv\")\n", "intent": "Read the dataset given in file named 'bigcity.csv'.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=100)\n", "intent": "Check: train_test_split function\n"}
{"snippet": "insurance = pd.read_csv('insurancedata.csv')\n", "intent": "We'll work with the Insurance Data csv file.\n** Read in the Insurance Data csv file as a DataFrame called insurance.**\n"}
{"snippet": "train = pd.read_csv('titanic_train.csv')\n", "intent": "Load the data reading in the titanic_train.csv file into a pandas dataframe.\n"}
{"snippet": "wine_df = pandas.read_csv(\"winequality-red.csv\")\n", "intent": "Let us assume the data frame is named wine_df\n"}
{"snippet": "Y = pandas.DataFrame(wine_df[\"quality\"])\nX = wine_df.iloc[:,0:11]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3, random_state=100)\n", "intent": "The above mixture suggests, overall accuracy may not make sense for this use case and class level accuracy will provide more insights\n"}
{"snippet": "ratio_train = pandas.DataFrame(Y_train[\"quality\"].value_counts())\nratio_train[\"Ratio\"] = (ratio_train[\"quality\"]/1119)*100\nratio_train\n", "intent": "Post train test split, ensure the ratio of quality levels in the data set is maintained\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(taxi, test_size=0.3, random_state=42)\n", "intent": "To be quick, let's create a baseline model, without Machine learning, just a simple rate calculation\n"}
{"snippet": "Train = pandas.read_csv(\"train.csv\")\nTest = pandas.read_csv(\"test.csv\")\n", "intent": "Use read_csv to read csv file. This is similar to read.csv in R\n"}
{"snippet": "train = pd.read_csv(\"train.csv\")\ntest    = pd.read_csv(\"test.csv\")\nfull = train.append( test , ignore_index = True )\ntitanic = full[ :891 ]\ndel train , test\nprint ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)\n", "intent": "Now that our packages are loaded, let's read in and take a peek at the data.\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "link = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/1200px-Correlation_examples2.svg.png\"\nio.imshow(io.imread(link))\nio.show()\n", "intent": "* Looking for Correlations\n"}
{"snippet": "enc = preprocessing.OneHotEncoder()\nenc.fit([[0, 3, 4], [1, 1, 0], [0, 2, 1], [1, 0, 2]])  \nenc.transform([[0, 1, 3]]).toarray()\n", "intent": "One Hot Encoding - Useful for inputing categorical data into SVM and linear models.\n"}
{"snippet": "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,5))\nX_train = vectorizer.fit_transform(twenty_train_subset.data)\n", "intent": "We can put this together with our other tricks as well...but notice the running time hit\n"}
{"snippet": "vectorizer = TfidfVectorizer(stop_words='english')\nX_train = vectorizer.fit_transform(twenty_train_subset.data)\n", "intent": "We can use predict using our 20-newsgroup dataset above\n"}
{"snippet": "mean = spambase.mean()\nstd = spambase.std()\nmean_std_df = pd.DataFrame({'Mean': mean, 'Std': std})\nmean_std_df\n", "intent": "**c)** Display the mean and standard deviation of each attribute.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'spambase_test.csv')\nspambase_test = pd.read_csv(data_path, delimiter = ',')\nspambase_test.head(10)\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure\n"}
{"snippet": "df = pd.read_csv('train.csv.gz', sep=',').dropna()\ndf = df.sample(frac=0.01, random_state=99)\n", "intent": "To be able to process locally, we will use 1% of data. After that, we still have a large number of 241,179 records.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'train_images_partB.csv')\ntrain_B = pd.read_csv(data_path, delimiter = ',')\ndata_path = os.path.join(os.getcwd(), 'datasets', 'valid_images_partB.csv')\nvalid_B = pd.read_csv(data_path, delimiter = ',')\nattributes = train_B.columns[1:501].tolist()\n", "intent": "Let's load the data.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'train_20news')\nnews_A = pd.read_csv(data_path + '_partA.csv', delimiter = ',')\nnews_B = pd.read_csv(data_path + '_partB.csv', delimiter = ',')\n", "intent": "<span style=\"color:red\">OK\n"}
{"snippet": "def get_dataset(path):\n    dataset = pd.read_csv(path)\n    np.random.seed(42)\n    dataset = dataset.reindex(np.random.permutation(dataset.index))\n    return dataset\ndataset = get_dataset('data/dataset.csv')\ndataset.head()\n", "intent": "First, we want to grab the dataset from the CSV file. Load it as a Pandas Dataframe so we can easily work with it in further steps.\n"}
{"snippet": "two = ['2.txt']+[0]*len(vocab)\nwith open('2.txt') as f:\n    x = f.read().lower().split()\nfor word in x:\n    two[vocab[word]]+=1\n", "intent": "<font color=green>We can see that most of the words in 1.txt appear only once, although \"cats\" appears twice.</font>\n"}
{"snippet": "import numpy as np\nimport pandas as pd\ndf = pd.read_csv('../TextFiles/moviereviews2.tsv', sep='\\t')\ndf.head()\n", "intent": "For this exercise you can load the dataset from `'../TextFiles/moviereviews2.tsv'`.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df['review']\ny = df['label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "You may use whatever settings you like. To compare your results to the solution notebook, use `test_size=0.33, random_state=42`\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\ntext_clf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC()),\n])\ntext_clf.fit(X_train, y_train)  \n", "intent": "You may use whatever model you like. To compare your results to the solution notebook, use `LinearSVC`.\n"}
{"snippet": "data_db = pd.io.parsers.read_csv(db,header=0)\ndata_db\n", "intent": "import into pandas dataframe and view it\n"}
{"snippet": "data_db = pd.io.parsers.read_csv(db,header=0)\ndata_db.head(5)\n", "intent": "import into pandas dataframe and view it\n"}
{"snippet": "scaler = StandardScaler()\nX=scaler.fit_transform(X)\nX\n", "intent": "Standardize the dataset\n"}
{"snippet": "url = '../assets/dataset/bikeshare.csv'\nbikes = pd.read_csv(url, index_col='datetime', parse_dates=True)\n", "intent": "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n"}
{"snippet": "bank_md1 = pd.get_dummies(bank_a[['age','job','education','day_of_week','y']], drop_first = True)\nLogReg1 = LogisticRegression()\nX1 = bank_md1.drop('y', axis =1)\ny1 = bank_md1['y']\nx_train1, x_test1, y_train1, y_test1 = train_test_split(X1,y1, random_state =42)\nLogReg1.fit(x_train1, y_train1)\n", "intent": "**Build a Model**  \n*Model 1, using `age`, `job`, `education`, and `day_of_week`*\n"}
{"snippet": "columns = \"age sex bmi map tc ldl hdl tch ltg glu\".split()\ndiabetes = datasets.load_diabetes()\ndf = pd.DataFrame(diabetes.data, columns=columns)\ny = diabetes.target  \ndf.head()\n", "intent": "* Luego, cargamos el dataset y definimos los nombres de colmnas\n"}
{"snippet": "df_cor = pd.DataFrame.from_dict(cuentos_cor, orient=\"index\")\ndf_cor = pd.DataFrame.from_dict(cuentos_bor, orient=\"index\")\ndf_cor.to_csv(\"df_cor.csv\")\ndf_cor.to_csv(\"df_bor.csv\")\n", "intent": "***6. Guardalos en .csv***\n"}
{"snippet": "df_cortazar.to_csv('cortazar.csv')\n", "intent": "***6. Guardalos en .csv***\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=42)\n", "intent": "Haremos el split en training y test, para poder evaluar el clasificador.\n"}
{"snippet": "from sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=150, whiten=True, svd_solver='randomized', random_state=42)\n", "intent": "Sobre los datos de entrenamiento podemos entrenamos el modelo PCA para reducir las dimensiones.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "Hacemos el split entre train y test:\n"}
{"snippet": "sac = pd.read_csv('Sacramentorealestatetransactions.csv')\n", "intent": "Carguemos el dataset de Sacramento:\n"}
{"snippet": "pca = PCA(n_components=23)\npca.fit(X)\n", "intent": "Apply PCA. And we have 23 features in our data.\n"}
{"snippet": "sac = pd.read_csv('../Data/Sacramentorealestatetransactions.csv')\n", "intent": "Carguemos el dataset de Sacramento:\n"}
{"snippet": "df = pd.read_csv('breast-cancer.csv', header = None)\ndf.columns = ['ID', 'clump_Thickness', 'unif_cell_size', 'unif_cell_shape', 'adhesion', 'epith_cell_Size', 'bare_nuclei',\n              'bland_chromatin ','norm_nucleoli', 'mitoses', 'class_t']\n", "intent": "Importamos el dataset\n"}
{"snippet": "X = (df.iloc[:,1:9])\ny = df['class_t']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n", "intent": "Hacemos el split entre target y features\n"}
{"snippet": "df = pd.read_csv('../Data/breast-cancer.csv', header = None)\ndf.columns = ['ID', 'clump_Thickness', 'unif_cell_size', 'unif_cell_shape', 'adhesion', 'epith_cell_Size', 'bare_nuclei',\n              'bland_chromatin ','norm_nucleoli', 'mitoses', 'class_t']\n", "intent": "Importamos el dataset\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nimport pandas as pd\nimport json\ndata = pd.read_csv(\"../Data/stumbleupon.tsv\", sep='\\t')\ndata['boilerplate'].head()\n", "intent": "* Primero importaremos los datos, paquetes, etc.\n"}
{"snippet": "import pandas as pd\ndf = pd.DataFrame(c.execute(\"SELECT * from adult\").fetchall(), columns = adult_cols)\n", "intent": "** Transformar los datos en un dataframe de pandas**\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('car.csv')\ndf.head()\n", "intent": "El primer paso es leer los datos en Pandas.\n"}
{"snippet": "pca = PCA(n_components=1)\npca.fit(X)\nX_pca = pca.transform(X)\nprint(\"original shape:   \", X.shape)\nprint(\"transformed shape:\", X_pca.shape)\n", "intent": "Ahora utilizamos PCA para reducir la cantidad de dimensiones de 2 a 1\n"}
{"snippet": "pca = PCA(0.50).fit(noisy)\npca.n_components_\n", "intent": "Vamos a correr PCA para retener con la cantidad de componentes principales necesaria para explicar el 50% de la varianza.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n", "intent": "Standardization of the data\n"}
{"snippet": "hide_code\nboston_data = datasets.load_boston()\nboston_df = pd.DataFrame(boston_data.data, columns=boston_data.feature_names)\nboston_df['MEDV'] = boston_data.target\n(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\nx_valid, y_valid = x_test[:51], y_test[:51]\nx_test, y_test = x_test[51:], y_test[51:]\n", "intent": "This database is very popular for studying regression and can be downloaded in several ways. Let's display the easiest ones of them.\n"}
{"snippet": "hide_code\nx_train, x_test, y_train, y_test = train_test_split(images, cat_brands, \n                                                    test_size = 0.2, \n                                                    random_state = 1)\nn = int(len(x_test)/2)\nx_valid, y_valid = x_test[:n], y_test[:n]\nx_test, y_test = x_test[n:], y_test[n:]\n", "intent": "Apply the function train_test_split and split the data into training and testing sets. Set up the size of the testing set - 20%.\n"}
{"snippet": "retail = pd.read_csv('data/retail_data.csv', index_col='CustomerID')\nheader = retail.columns.values\nretail_array = np.array(retail)\nX = retail_array[:,:-1].astype(float)\ny = retail_array[:,-1]\ny = preprocessing.LabelEncoder().fit_transform(y)\nXTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state=1)\n", "intent": "We will use the same previous dataset customer retails to evaluate AdaBoost\n"}
{"snippet": "retail = pd.read_csv('data/retail_data.csv', index_col='CustomerID')\nheader = retail.columns.values\nretail_array = np.array(retail)\nX = retail_array[:,:-1].astype(float)\ny = retail_array[:,-1]\nXTrain, XTest, yTrain, yTest = train_test_split(X, preprocessing.LabelEncoder().fit_transform(Y), random_state=1)\n", "intent": "We will use the `Pipeline` module to build the complete classification system for the customer retail we encountered in the previous sessions\n"}
{"snippet": "def get_matrix():\n    df = pd.merge(df_offers, df_transactions, how='left')\n    response_df = df.pivot_table(index=['customer_name'], columns=['offer_id'], values='n')\n    response_df.fillna(0, inplace=True)\n    matrix_df = response_df.reset_index()\n    del matrix_df['customer_name']\n    return matrix_df\n", "intent": "> - **Exercise**: Construct a plot showing $SS$ for each $K$  and pick $K$ using this plot. For simplicity, test $2 \\le K \\le 10$.\n"}
{"snippet": "with open('data/die_leiden_des_jungen_werthers_kapitel_1.txt','r') as f:\n    read_data=f.read()\nf.close\n", "intent": "__Given:__ Chapter 1 of 'die Leiden des jungen Werthers' (data/die_leiden_des_jungen_werthers_kapitel_1.txt)\n"}
{"snippet": "recons = np.dot(proj[:,:50],W[:,:50].T) + df[feat_cols].values.mean(axis=0)\ndf_recons = pd.DataFrame(recons,columns=feat_cols)\nplotMNIST(df_recons,  maxN=20, print_digits=False)\n", "intent": "<h2>Take a look at the reconstruction</h2>\n"}
{"snippet": "pca = repres.PCA(n_components=784)\nres = pca.fit_transform(df[feat_cols].values)\ndf['pca-one'] = res[:,0]\ndf['pca-two'] = res[:,1] \ndf['pca-three'] = res[:,2]\nsns.pairplot(df, hue=\"label\", x_vars=[\"pca-one\",\"pca-two\"], y_vars=[\"pca-two\",\"pca-three\"], plot_kws={\"alpha\": 0.2},  size=5,)\n", "intent": "<h2>Now, let's use scikit.learn's PCA </h3>\n"}
{"snippet": "fa = repres.FactorAnalysis(n_components=3)\nres = fa.fit_transform(df[feat_cols].values)\ndf['fa-one'] = res[:,0]\ndf['fa-two'] = res[:,1] \ndf['fa-three'] = res[:,2]\nsns.pairplot(df, hue=\"label\", x_vars=[\"fa-one\",\"fa-two\"], y_vars=[\"fa-two\",\"fa-three\"], plot_kws={\"alpha\": 0.2},  size=4,)\n", "intent": "<h2>Now, let's use scikit.learn's Factor Analysis</h3>\n"}
{"snippet": "data = pd.read_csv('./weather/daily_weather.csv')\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nCreating a Pandas DataFrame from a CSV file<br></p>\n"}
{"snippet": "from sklearn.manifold import TSNE\ntsne = TSNE(n_components=3, perplexity=40, n_iter=300)\nres = tsne.fit_transform(proj[:,:50])\ndf['tsne-one'] = res[:,0]\ndf['tsne-two'] = res[:,1] \ndf['tsne-three'] = res[:,2] \n", "intent": "<h2>What next?</h3>\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX2D = pca.fit_transform(X)\n", "intent": "Apply PCA to reduce to 2D.\n"}
{"snippet": "X2D_inv = pca.inverse_transform(X2D)\n", "intent": "Recover the 3D points projected on the plane (PCA 2D subspace).\n"}
{"snippet": "ev = pd.read_csv('/users/patricksmith/desktop/evergreen.tsv', delimiter='\\t')\nev.head()\n", "intent": "First, let's load in the dataset. It's always important to visually examine the data, so let's go ahead and give it a look\n"}
{"snippet": "X_newr = []\nfor i in in_images_names:\n    X_newr.append (cv2.imread('./new_images_resized/'+ i)[:,:,:3] )\nX_new = np.asarray( X_newr )\n", "intent": "All new images are uploaded in a single mini data set variable with name *X_new*.\n"}
{"snippet": "df_probs = pd.DataFrame(data=probabilities) \ndf_probs.head()\n", "intent": "df_probs = pd.DataFrame(data=probabilities)\ndf_probs.head()\n"}
{"snippet": "pca2 = PCA(n_components=2)\nnorm_dist_pca2 = pca2.fit_transform(norm_dist)  \n", "intent": "- How much variance is explained using the first two components?\n- Does the explained variance number make sense?\n"}
{"snippet": "pca = PCA(n_components=2)\nnorm_dist_pca = pca.fit_transform(norm_dist)\n", "intent": "- How much variance is explained using the first two components?\n- Does the explained variance number make sense?\n"}
{"snippet": "df = pd.read_csv('data/blood_donations.csv', index_col = 'Unnamed: 0')\n", "intent": "https://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center\n"}
{"snippet": "data = pd.read_csv('./weather/minute_weather.csv')\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nCreating a Pandas DataFrame from a CSV file<br><br></p>\n"}
{"snippet": "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\nXtis = vectorizer.fit_transform(df['quote'])\nY = (df['fresh'] == 'fresh').values.astype(np.int8)\nxtraintis, xtesttis, ytraintis, ytesttis = train_test_split(Xtis, Y, random_state=1)\n", "intent": "Do both together help?\n"}
{"snippet": "vectorizer = CountVectorizer(max_features=5000)\nXtis = vectorizer.fit_transform(df['quote'])\nY = (df['fresh'] == 'fresh').values.astype(np.int8)\nxtraintis, xtesttis, ytraintis, ytesttis = train_test_split(Xtis, Y, random_state=1)\n", "intent": "Do both together help?\n"}
{"snippet": "url='https://health.data.ny.gov/api/views/h8yk-ufg9/rows.csv?accessType=DOWNLOAD'\ncareplan_utilization = pd.read_csv(url)\n", "intent": "We read in the file.\n"}
{"snippet": "filtered = pd.DataFrame(combined.pivot_table(index=['Year','Payer','Plan','Measure'],values='Events',aggfunc=np.sum))\nfiltered = filtered.reset_index()\nfiltered = filtered[filtered.Measure == 'ER Visits']\n", "intent": "Now that we've seen the overall trending data, we are going to drill down to our ER visits.\n"}
{"snippet": "indicator = pd.DataFrame()\nindicator['indicator_number'] = CommInd['Indicator Number']\nindicator['indicator'] = CommInd['Indicator']\nindicator = indicator.drop_duplicates(subset='indicator_number',take_last=True)\nindicator\n", "intent": "Explore the indicators in the data file to help us select those we want to explore\n"}
{"snippet": "import csv\nf=open('Hospital_Inpatient_Discharges__SPARCS_De-Identified___2012 (1).csv')\ndata = pd.read_csv(f)\n", "intent": "First we read in the data.\n"}
{"snippet": "needs = pd.read_csv(os.path.join(DERIVED_DATA_PATH,'C16B_PUF_cleaned_needs.csv'), na_values=' ', index_col='ID')\nwellness = pd.read_csv(os.path.join(DERIVED_DATA_PATH,'C16B_PUF_cleaned_wellness.csv'), na_values=' ', index_col='ID')\nkm_X = needs.join(wellness).dropna() \n", "intent": "Below we're going to join the needs and wellness domains, as it's our intended feature space.\n"}
{"snippet": "profile = pd.read_csv(os.path.join(DERIVED_DATA_PATH,'profile.csv'))\nprofile.head()\n", "intent": "The dataset consists of XXXX elements comprised of XXXX unique songs from XXXX artists, spanning   XXX weeks.\n"}
{"snippet": "profile = pd.read_csv(os.path.join(DERIVED_DATA_PATH,'C16B_PUF_cleaned_profile.csv'), na_values=' ', index_col='ID')\nprofile.head(3)\n", "intent": "The dataset consists of XXXX elements comprised of XXXX unique songs from XXXX artists, spanning   XXX weeks.\n"}
{"snippet": "X = StandardScaler().fit_transform(select_df)\nX\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nScale the Features using StandardScaler\n<br><br></p>\n"}
{"snippet": "import os.path\ncolumns = [u'ID', u'AGEGRP', u'RACE', u'GENDER', u'MRSTAT', u'EDUC', u'BMI', u'GENHTH', u'MODACT', u'CLMBSV', u'PACMPL', u'PLMTKW', u'EACMPL', u'ENTCRF', u'PNINTF', u'PCEFUL', u'ENERGY', u'BLSAD', u'SCLACT', u'ASHLTH', u'ASEHLTH', u'DIFBTH', u'DIFDRS', u'DIFEAT', u'DIFCHR', u'DIFWLK', u'DIFTOL', u'DIFPRM', u'DIFMON', u'DIFMED', u'PHYHTH', u'MENHTH', u'PORHTH', u'BLIND', u'DEAF', u'DIFCON', u'DIFERR', u'MEMINT', u'HIGHBP', u'ANGCAD', u'CHF', u'AMI', u'OTHHRT', u'STROKE', u'COPD_E', u'GI_ETC', u'ATHHIP', u'ATHHAN', u'OSTEOPO', u'SCIATC', u'DIABET', u'DEPRES', u'ANYCAN', u'COLNCA', u'LUNGCA', u'BRSTCA', u'PROSCA', u'OTHCAN', u'PNIACT', u'PNISOC', u'AVGPN', u'FELTNP', u'FELTSD', u'CMPHTH', u'SMKFRQ', u'URNLKG', u'URNMAG', u'URNDOC', u'URNTRT', u'PAOTLK', u'PAOADV', u'BALTLK', u'FELL12MO', u'BAL12MO', u'FALLTLK', u'OSTTEST', u'WHOCMP', u'SRVDSP', u'RNDNUM', u'PCTCMP', u'COHORT', u'PLREG', u'SVLANG']\ndata = pd.read_csv(os.path.join(SOURCE_DATA_PATH,'C16B_PUF.csv'), names=columns, na_values=' ', index_col='ID')\n", "intent": "Good.  So let's read it into pandas so we can begin our data inspection.\n"}
{"snippet": "wellness.to_csv(os.path.join(DERIVED_DATA_PATH,'C16B_PUF_cleaned_wellness.csv'))\n", "intent": "Note: Values Ascending from not a problem at all to all the time\n"}
{"snippet": "data = pd.read_csv(CSV_PATH, na_values=' ')\n", "intent": "Good.  So let's read it into pandas so we can begin our data inspection.\n"}
{"snippet": "wellness.to_csv(os.path.join(DERIVED_DATA_PATH,'wellness.csv'))\n", "intent": "Note: Values Ascending from not a problem at all to all the time\n"}
{"snippet": "x = np.sort(x, axis=0)\nU_vectors = []\nfor i in range(N):\n    U_vectors.append( get_linear_transform(x[i,:][np.newaxis], theta['W'], theta['b'])[:,0] )\nU_vectors = np.array(U_vectors).T\n", "intent": "Next we calculate all the $\\tilde{\\mathbf{U}}$ vectors...\n"}
{"snippet": "audio_fname_template = '../data/SpeechEmotion-master/data/{:02d}.wav'\ndef load_wave(i):\n    fname = audio_fname_template.format(i)\n    rate, wave = scipy.io.wavfile.read(fname)\n    wave = (wave + 0.5) / (0x7FFF + 0.5)  \n    wave *= 1.0 / np.max(np.abs(wave)) \n    return wave, rate\nwave, rate = load_wave(0)\n", "intent": "The next functions are similar to the ESC-50 session, loading, plotting and plying an audio clip.\n"}
{"snippet": "sns.pairplot(pd.DataFrame({'arousal':arousal, 'valence': valence}), height=3, plot_kws=dict(marker='o'));\n", "intent": "Let's also look at the distribution of arousal and valence values (on the main diagonal) and their joint distributions (on the off-diagonal).\n"}
{"snippet": "import keras\n(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\n", "intent": "We start by loading the MNIST digits data we used in the [softmax model session](softmax-model.ipynb).\n"}
{"snippet": "(X_train, _), (_, _) = keras.datasets.mnist.load_data()\nX_train = (X_train.astype(np.float32) - 127.5) / 127.5\nX_train = np.expand_dims(X_train, axis=3)\n", "intent": "Let's load the MNIST dataset through Keras' API.\nWe only need the train set as we don't actually use the labels.\n"}
{"snippet": "def pd_centers(featuresUsed, centers):\n\tcolNames = list(featuresUsed)\n\tcolNames.append('prediction')\n\tZ = [np.append(A, index) for index, A in enumerate(centers)]\n\tP = pd.DataFrame(Z, columns=colNames)\n\tP['prediction'] = P['prediction'].astype(int)\n\treturn P\n", "intent": "Let us first create some utility functions which will help us in plotting graphs:\n"}
{"snippet": "img_path = '../data/Kobe_Bryant_2014.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n", "intent": "That's really nice.\nLoad an image, convert it to array, and preprocess it for ResNet50.\n"}
{"snippet": "df = pd.DataFrame(data=X, columns=diabetes.feature_names)\nsns.pairplot(df, plot_kws=dict(alpha=0.25));\n", "intent": "Let's look at the features (`X`):\n"}
{"snippet": "df['Mas Vnr Type'].fillna(value='None',inplace=True)\ndf['Mas Vnr Area'].fillna(value=0.0,inplace=True)\n", "intent": "- I will replace the NaN with 0.0 and None for Msn Vnr Area and Msn Vnr Type respectively. \n"}
{"snippet": "from sklearn.preprocessing import LabelBinarizer\nlabel_binarizer = LabelBinarizer()\ny_one_hot = label_binarizer.fit_transform(y_train)\nprint (y_one_hot.shape)\n", "intent": "Visualizing the normalized data set after being normalized and grayscale\n"}
{"snippet": "tfv=TfidfVectorizer(min_df=0, max_features=None, strip_accents='unicode',lowercase =True,\n                    analyzer='word', token_pattern=r'\\w{3,}', ngram_range=(1,1),\n                    use_idf=True,smooth_idf=True, sublinear_tf=True, stop_words = \"english\")  \n", "intent": "First, we will not go more in details in tweets and just apply a TF-IDF and a model to have a baseline.\n"}
{"snippet": "df = pd.read_csv(\"preprocessed_audio/mean_cqt.csv\", index_col=0)\ndf = df.div(df.sum(axis=1)+1e-6, axis=0)\ndf = df.dropna(axis=0, how='any')\n", "intent": "As we mentionned previously, we can use Mini Batch Kmeans but we have to ensure that our clusters are well balanced in term of qty and homogeneous.\n"}
{"snippet": "df = pd.read_csv(\"preprocessed_audio/sample_content.csv\", index_col=0)\n", "intent": "Now let's try to fin d a similar audio to a given audio\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nenc = LabelBinarizer()\ny_enc = enc.fit_transform(y.reshape(-1, 1))\nX_train, X_test, y_train, y_test = train_test_split(X_reduced, y_enc, test_size=0.20, random_state=42)\n", "intent": "If we want to keep 90 of the variance we are now at 400 dimensions, this should be fine for a simple ANN.\n"}
{"snippet": "tracks = tracks[tracks.index.isin(list_index)]\ngenres.to_csv(\"preprocessed_meta/genres.csv\")\ntracks.to_csv(\"preprocessed_meta/tracks.csv\")\nechonest[echonest.index.isin(list_index)].to_csv(\"preprocessed_meta/echonest.csv\")\nfeatures[features.index.isin(list_index)].to_csv(\"preprocessed_meta/features.csv\")\nunique_album = np.unique(tracks[('album', 'id')].values)\nunique_artist = np.unique(tracks[('artist', 'id')].values)\nartists[artists.index.isin(unique_artist)].to_csv(\"preprocessed_meta/artists.csv\")\nalbums[albums.index.isin(unique_album)].to_csv(\"preprocessed_meta/albums.csv\")\n", "intent": "Now let's filter our datasets and save them.\n"}
{"snippet": "def read_data(filename):\n  f = zipfile.ZipFile(filename)\n  for name in f.namelist():\n    return f.read(name).split()\n  f.close()\nwords = read_data(filename)\nprint 'Data size', len(words)\n", "intent": "Read the data into a string.\n"}
{"snippet": "X = np.load(\"preprocessed_audio/mfcc_norm.npy\")\nX_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.20, random_state=42)\n", "intent": "For this model, it's easier, a simple fully connected model will do the job. The complete dataset fit into memory and the training will be very fast.\n"}
{"snippet": "dataset = pd.read_csv(\"creditcard.csv\")\n", "intent": "First let's explore quickly the dataset. I won't explain all what we have as it is described on above\n"}
{"snippet": "dataset_raw = pd.read_csv(\"HR.csv\")\n", "intent": "Let's first import the dataset and take a quick look to the content\n"}
{"snippet": "X = dataset.drop(\"left\", axis=1)\ny = dataset[\"left\"]\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n", "intent": "We can now, scale the dataset and prepare our train and test dataset\n"}
{"snippet": "control = pd.read_csv(\"features_test.csv\", header=[0, 1, 2], skipinitialspace=True, index_col=0).reset_index()[[\"index\"]]\ncontrol.columns = [\"fname\"]\ncontrol = control.set_index(\"fname\")\n", "intent": "Now we have our prediction. The only point is that we removed 3 lines of bugged audio. We have now to add them with a random class\n"}
{"snippet": "house_data = pd.read_csv(\"house_data.csv\")\n", "intent": "On va commencer par explorer rapidement le dataset car il est assez simple\n"}
{"snippet": "tracks_OHE.to_csv(\"F:/Nicolas/DNUPycharmProjects/machine_learning/audio/FMA/fma_metadata/classes.csv\")\n", "intent": "The next step will require lot of memory so the best is to save this dataset, clean the memroy and restart from here\n"}
{"snippet": "enc = LabelBinarizer()\ny = enc.fit_transform(y.reshape(-1, 1))\n", "intent": "Now our dataset should be balanced. Let'(s apply a Label Bisarizer and count each class\n"}
{"snippet": "pca = PCA(n_components=0.99)\nnew_X = pca.fit_transform(X)\n", "intent": "Now, let's reduce dimensions as much as possible\n"}
{"snippet": "df_test = pd.read_csv('../data/titanic/test.csv')\ndf_test.head()\n", "intent": "Read the test data:\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(X_moons, y_moons, test_size=0.2)\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)\n", "intent": "Now let's split the model with a train dataset and test dataset\n"}
{"snippet": "scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n", "intent": "As we goona use the sigmoid function as activation function, we need to have centered datas so we can use StandardScaler from sklearn\n"}
{"snippet": "X = pd.DataFrame(data = dataset.data, columns =dataset.feature_names)\ny = pd.Series(data = dataset.target)\ny.name = \"Label\"\ny = y.map({i : dataset.target_names[i] for i in range(3)}).to_frame()\ny_ohe = pd.get_dummies(dataset.target)\ny_ohe.columns = dataset.target_names\n", "intent": "The dataset provided by sklearn is not a usual dataset so let's first create dataframes as we usually have\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.25, random_state=42)\n", "intent": "Now we can split both datas with a trian and validation set\n"}
{"snippet": "analyzer = CountVectorizer().build_analyzer()\nCV = CountVectorizer(lowercase=True, stop_words=\"english\", analyzer=stem_and_filter)\nTF_matrix = CV.fit_transform(X)\n", "intent": "For tests, we can also generate the TF Matrix but we won't use it.\n"}
{"snippet": "svd = TruncatedSVD(n_components = 100)\nnormalizer = Normalizer(copy=False)\nTF_embedded = svd.fit_transform(TF_matrix)\nTF_embedded = normalizer.fit_transform(TF_embedded)\nTF_IDF_embedded = svd.fit_transform(TF_IDF_matrix)\nTF_IDF_embedded = normalizer.fit_transform(TF_IDF_embedded)\n", "intent": "Let's now apply the LSA and normalize every row afterward.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nTFVectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\nTF_train = TFVectorizer.fit_transform(X_train)\nTF_val = TFVectorizer.transform(X_val)\ninv_voc = {v: k for k, v in TFVectorizer.vocabulary_.items()}\nsum_ = TF_train.sum(axis=0)\n", "intent": "Let's first have a look of the count of every ingredients.\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ny_train_enc = lb.fit_transform(y_train_filtered)\nohe =  OneHotEncoder(dtype=np.bool)\ny_train_OHE = ohe.fit_transform(y_train_enc.reshape(-1,1))\n", "intent": "Just for information, We can also look at the balance for every origines. The dataset is not very balanced and can create troubles\n"}
{"snippet": "df = pd.read_csv(\"F:/data/trading/stat_history.csv\", header =[0, 1], index_col=[0, 1])\ndf = df.reset_index()\ndf.columns = [' '.join(col).strip() for col in df.columns.values]\n", "intent": "Let's start from the initial dataframe of price we prepared in the first Notebook\n"}
{"snippet": "df_test['Survived'] = test_y\ndf_test[['PassengerId', 'Survived']] \\\n    .to_csv('../data/titanic/results-rf.csv', index=False)\n", "intent": "Create a DataFrame by combining the index from the test data with the output of predictions, then write the results to the output:\n"}
{"snippet": "scaled_data = pd.DataFrame(scaled_features, columns=bank_note.columns[:-1])\nscaled_data.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "milk = pd.read_csv('monthly-milk-production-pounds-p.csv', index_col='Month')\nmilk.columns\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('census_data.csv')\n", "intent": "** Read in the census_data.csv data with pandas**\n"}
{"snippet": "data = pd.read_csv('../datasets/santander.csv')\ndata.shape\n", "intent": "Load the Santander Customer Satisfaction dataset\n"}
{"snippet": "data = pd.read_csv('../datasets/santander.csv')\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "In the following, we will code the VarianceThreshold from scratch\n"}
{"snippet": "X_train, X_test, y_train, y_test, = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "Transposing a dataframe is costly if the dataframe is big. Therefore, we can use the alternative loop to find duplicated columns in bigger datasets.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    num_data.drop(labels=['target', 'ID'], axis=1),\n    num_data['target'],\n    test_size=0.3,\n    random_state=0\n)\nX_train.shape, X_test.shape\n", "intent": "In all feature selection procedures, it is good practice to select the features by examining only the training set and to avoid overfit.\n"}
{"snippet": "correlation_matrix = X_train.corr()\ncorrelation_matrix = correlation_matrix.abs().unstack()\ncorrelation_matrix = correlation_matrix.sort_values(ascending=False)\ncorrelation_matrix = correlation_matrix[correlation_matrix >=0.8]\ncorrelation_matrix = correlation_matrix[correlation_matrix < 1]\ncorrelation_matrix = pd.DataFrame(correlation_matrix).reset_index()\ncorrelation_matrix.columns = ['feature1', 'feature2', 'corr']\ncorrelation_matrix.head()\n", "intent": "Build a dataframe with the correlation between features; remember that the absolute value is the important part, not the sign.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nfeatures = list(group.feature2.unique()) + ['v17']\nrf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\nrf.fit(X_train[features].fillna(0), y_train)\n", "intent": "Loooots of NULL values in here. We can build a ML algorithm using all the features from the above list, and select the most predictive ones.\n"}
{"snippet": "from keras.datasets import mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n", "intent": "The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays:\n"}
{"snippet": "df = pd.DataFrame(scaled_data, columns = project_data.columns[:-1])\ndf.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncountvectorizer = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "X = countvectorizer.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "df = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n", "intent": "With principal component analysis, we are finding out those that are the most variant\n"}
{"snippet": "movie_titles = pd.read_csv('Movie_Id_Titles')\nmovie_titles.head()\n", "intent": "We can use the Movie_Id_Titles to grab the movie names and merge it with the dataframe.\n"}
{"snippet": "vect = CountVectorizer(ngram_range = (1,2), stop_words='english', min_df =2)\ndoc_matrix = pd.DataFrame(vect.fit_transform(df.text).toarray(), columns=vect.get_feature_names())\ntype(df)\n", "intent": "Note that most of the sentiments (473 out of 979 are neutral) and that this can potentially skew the data\n"}
{"snippet": "month = bikes.index.month\npd.pivot_table(bikes, index='season', columns=month, values='temp', aggfunc=np.count_nonzero).fillna(0)\n", "intent": " seasons: \n  *  1 = spring\n  * 2 = summer \n  * 3 = fall \n  * 4 = winter \n"}
{"snippet": "import pandas as pd\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/titanic.csv'\ntitanic = pd.read_csv(url, index_col='PassengerId')\ntitanic.head()\n", "intent": "Read the data into Pandas\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n", "intent": "Split the data into training and testing sets\n"}
{"snippet": "img_path = '/Users/fchollet/Downloads/cats_and_dogs_small/test/cats/cat.1700.jpg'\nfrom keras.preprocessing import image\nimport numpy as np\nimg = image.load_img(img_path, target_size=(150, 150))\nimg_tensor = image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\nimg_tensor /= 255.\nprint(img_tensor.shape)\n", "intent": "This will be the input image we will use -- a picture of a cat, not part of images that the network was trained on:\n"}
{"snippet": "import pandas as pd\nyelp = pd.read_csv('yelp.csv')\nyelp.head(1)\n", "intent": "Using the yelp reviews database  create a Naive Bayes model to predict the star rating for reviews\nRead `yelp.csv` into a DataFrame.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X.values, y.values, random_state=123)\n", "intent": "* Split the data in train and test\n* Train a KNN model (K=5)\n* Evaluate the accuracy\n"}
{"snippet": "import pandas as pd  \ndata_temp = pd.DataFrame(iris.data, columns=iris.feature_names)\ndata_temp['target'] = iris.target\ndata_temp['target'] = data_temp['target'].astype('category')\ndata_temp['target'].cat.categories = iris.target_names\nsns.pairplot(data_temp, hue='target', palette=sns.color_palette(\"hls\", 3))\n", "intent": "This data is four dimensional, but we can visualize two of the dimensions\nat a time using a simple scatter-plot:\n"}
{"snippet": "X_poly = PolynomialFeatures(degree=2).fit_transform(X)\nX_test_poly = PolynomialFeatures(degree=2).fit_transform(X_test)\n", "intent": "Now we'll use this to fit a quadratic curve to the data.\n"}
{"snippet": "from sklearn.feature_selection import SelectPercentile, f_classif\nsel = SelectPercentile(f_classif, percentile=50)\nsel.fit(X, y)\nsel.get_support()\n", "intent": "There is still the question of how to select the parameter k\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\ncol_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\niris = pd.read_csv(url, header=None, names=col_names)\niris.head()\n", "intent": "We'll read the iris data into a DataFrame, and **round up** all of the measurements to the next integer:\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nX_dtm = vect.fit_transform(X)\nX_dtm.shape\n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "seq_len = 500\ntrn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\ntest = sequence.pad_sequences(test, maxlen=seq_len, value=0)\n", "intent": "Making every trn idx consistent and choosing max len and padding sequences less than this length to zeros\n"}
{"snippet": "data = pd.read_csv('../data/aquastat/aquastat.csv.gzip', compression='gzip')\ndata.region = data.region.apply(lambda x: simple_regions[x])\ndata = data.loc[~data.variable.str.contains('exploitable'),:]\ndata = data.loc[~(data.variable=='national_rainfall_index')]\n", "intent": "http://www.fao.org/nr/water/aquastat/data/query/index.html\n"}
{"snippet": "import cv2\nimg = cv2.imread(img_path)\nheatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\nheatmap = np.uint8(255 * heatmap)\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\nsuperimposed_img = heatmap * 0.4 + img\ncv2.imwrite('/Users/fchollet/Downloads/elephant_cam.jpg', superimposed_img)\n", "intent": "Finally, we will use OpenCV to generate an image that superimposes the original image with the heatmap we just obtained:\n"}
{"snippet": "clf_train, clf_test = train_test_split(txclf,test_size=0.4)\n", "intent": "Let's split the data into a training set and a test set\n"}
{"snippet": "vectorizer = TfidfVectorizer(max_df=0.5,\n                                 min_df=2, stop_words='english')\ntrans = vectorizer.fit_transform(txclf.Proposal)\n", "intent": "Create a text vectorizer\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntext_clf = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', SGDClassifier(loss='hinge', penalty='l2',  \n                        alpha=1e-3, n_iter=5, random_state=42)),])\ntext_clf.fit(clf_train.Proposal,clf_train.Bucket)\n", "intent": "Wait but shouldn't we do this in a more extensible way so it's easy for us to add data later? \nYes let's make a pipeline!!!\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "Housing Prices Dataset\n"}
{"snippet": "cv = CountVectorizer(tokenizer=myTokenizer,ngram_range=(1,1))\nvectorized_corpus = cv.fit_transform(corpus)\nvc = vectorized_corpus.toarray()\nprint(cv.vocabulary_)\n", "intent": "- Unigrams\n- Bigrams, Trigrams\n- N-Grams\n"}
{"snippet": "def load_data():\n    column_names = ['digit', 'intensity', 'symmetry']\n    sep = '\\s+'\n    features_train = pd.read_table('http://www.amlbook.com/data/zip/features.train', sep=sep, names=column_names)\n    features_test = pd.read_table('http://www.amlbook.com/data/zip/features.test', sep=sep, names=column_names)\n    return features_train, features_test\n", "intent": "Load training and testing data in pandas dataframes\n"}
{"snippet": "def create_one_vs_all_dataframe(df, classifiers):\n    one_vs_all = pd.DataFrame(df, copy=True)\n    for class_label, digit in classifiers.items():\n        labels = one_vs_all.loc[one_vs_all['digit'] == digit, 'digit']\n        labels.loc[:] = 1.0\n        one_vs_all[class_label] = labels\n    one_vs_all.fillna(-1.0, inplace=True)\n    return one_vs_all\n", "intent": "Create one-versus-all dataframe by adding outputs for the different classifiers to the training dataframe\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features,dfr['Helpful'], test_size=0.33, random_state=42)\n", "intent": "We will set up some basic functions to test our classifiers through them and choose the best performer based on the accuracy. \n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features,dfr['Helpful'], test_size=0.3, random_state=42)\n", "intent": "We will set up some basic functions to test our classifiers through them and choose the best performer based on the accuracy. \n"}
{"snippet": "import keras\nimport numpy as np\npath = keras.utils.get_file(\n    'nietzsche.txt',\n    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\ntext = open(path).read().lower()\nprint('Corpus length:', len(text))\n", "intent": "Let's start by downloading the corpus and converting it to lowercase:\n"}
{"snippet": "custdata14 = pd.read_csv('C:\\Springboard Data\\LoanStats14.csv',low_memory=False)\n", "intent": "We are going to be combining 2014 and 2015 dataset in one dataframe and call it \"custdata\"\n"}
{"snippet": "import pandas as pd\nbos = pd.DataFrame(boston.data)\nbos.head()\n", "intent": "Now let's explore the data set itself. \n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\nprint (\"y_train.shape = {}\".format(y_train.shape))\nprint (\"x_train.shape = {}\".format(x_train.shape))\nprint(len(y_train), 'train samples')\nprint(len(y_test), 'test samples')\n", "intent": "We'll use the same data as before, so we included this step already.\n"}
{"snippet": "tdm = cv.fit_transform(text)\n", "intent": "<img src=\"img/13.PNG\">\n"}
{"snippet": "imread(all_images_list[0]).shape\n", "intent": " - np.expand_dims -> Expand the shape of an array.\n  - Insert a new axis, corresponding to a given position in the array shape.\n"}
{"snippet": "le = preprocessing.LabelEncoder()\n", "intent": "- Encode labels with value between 0 and n_classes-1.\n"}
{"snippet": "dataset = load_boston()\nprint(dataset.keys())\n", "intent": "<h3>  Problem \nIn this part, you should download and analyze **\"Boston House Prices\"** dataset. <br>\nHere use a code below to download the  dataset: \n"}
{"snippet": "X_train, X_test, Y_train, Y_test= train_test_split(features, target, random_state= 0, test_size=0.4)\nprint('The shape of X_train is', X_train.shape)\nprint('The shape of X_test is', X_test.shape)\nprint('The shape of Y_train is', Y_train.shape)\nprint('The shape of Y_test is', Y_test.shape)\n", "intent": "<h4> Problem \n- Report the shape of each (training and test) datasets.\n"}
{"snippet": "X_train, X_test, Y_train, Y_test= train_test_split(features, target, random_state= 0, test_size=0.333)\nprint('The shape of X_train is', X_train.shape)\nprint('The shape of X_test is', X_test.shape)\nprint('The shape of Y_train is', Y_train.shape)\nprint('The shape of Y_test is', Y_test.shape)\n", "intent": "<h4> Problem \n- Report the shape of each (training and test) dataset.\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nhousing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\nhousing_cat_1hot\n", "intent": "We can convert each categorical value to a one-hot vector using a `OneHotEncoder`:\n"}
{"snippet": "dataset = load_boston()\nprint(dataset.keys())\n", "intent": "<h4> Problem \nIn this part, you should download and analyze **\"Boston House Prices\"** dataset. <br>\nUse a code below to download the  dataset: \n"}
{"snippet": "from sklearn.datasets import load_boston\nimport pandas as pd\nimport numpy as np\ndataset = load_boston()\nprint(dataset.keys())\n", "intent": "<h3>  Problem \nIn this part, you should download and analyze **\"Boston House Prices\"** dataset. <br>\nHere use a code below to download the  dataset: \n"}
{"snippet": "df = pd.read_csv('College_data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "X = cntvct.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "import numpy as np\nfrom sklearn import datasets\nfrom sklearn.linear_model import LinearRegression\ndataset = datasets.load_diabetes()\nmodel = LinearRegression()\nmodel.fit(dataset.data, dataset.target)\nprint(model)\n", "intent": "* Classical Regression\n* Handling Nonlinearity\n* Regularization\n* Dimensionality\n"}
{"snippet": "diab = datasets.load_diabetes()\n", "intent": "For more information on the dataset, see the original paper at \nhttp://web.stanford.edu/%7Ehastie/Papers/LARS/LeastAngle_2002.pdf.\n"}
{"snippet": "def examine_coefficients(model, df):\n    df = pd.DataFrame(\n        { 'Coefficient' : model.coef_[0] , 'Feature' : df.columns}\n    ).sort_values(by='Coefficient')\n    return df[df.Coefficient !=0 ]\n", "intent": "- You can either create the dummy features manually or use the `dmatrix` function from `patsy`\n- Remember to scale the feature variables as well!\n"}
{"snippet": "df.to_csv(\"all_data.csv\", encoding=\"utf-8\")\n", "intent": "Save all data to file, so that I can start by reading it in the future.\n"}
{"snippet": "names = ['sample_id','clump thikness','uniformity_size','uniformity_shape','adhesion','cell_size','bare_nuclei','bland_chromatin','normal_nucleoti','mitoses','class']\ndf = pd.read_csv(\"../../assets/datasets/breast-cancer-wisconsin.csv\",names=names,na_values=['?'])\ndf.head()\n", "intent": "The column names are taken from the dataset info file. Create an array\nwith the column names and assign them as the header when loading the\ncsv.\n"}
{"snippet": "cat_encoder = CategoricalEncoder()\nhousing_cat_reshaped = housing_cat.values.reshape(-1, 1)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)\nhousing_cat_1hot\n", "intent": "The `CategoricalEncoder` expects a 2D array containing one or more categorical input features. We need to reshape `housing_cat` to a 2D array:\n"}
{"snippet": "model2 = LogisticRegression(penalty='l1')\nmodel2.fit(X_norm, y)\ncoeffs = pd.DataFrame(model2.coef_, columns = iris.feature_names, index =iris.target_names)\ncoeffs\n", "intent": "**Check** Try changing the penalty to `l1`, do the coefficients change?\n**Optional Check** Check score with `cross_val_score` and select best model\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconmat = np.array(confusion_matrix(y, kmeans.labels_))\nconfusion = pd.DataFrame(conmat, index=['setosa','versicolor','virginica'],columns=['pred. setosa','pred. versicolor','pred. virginica'])\nconfusion\n", "intent": "Compute the Confusion Matrix to test the performance of the clustering analysis\n"}
{"snippet": "df_rep = df[df.Class==\"republican\"]\ndf_dem = df[df.Class==\"democrat\"]\ndf_rep.fillna(method='pad',axis=0,inplace=True)\ndf_dem.fillna(method='pad',axis=0,inplace=True)\ndf_rep.fillna(method='bfill',axis=0,inplace=True)\ndf_dem.fillna(method='bfill',axis=0,inplace=True)\ndf = pd.concat([df_rep,df_dem],axis=0)\n", "intent": "Next, let's define the x and y variables: \n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXs = ss.fit_transform(X)\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "Xdf = pd.DataFrame(X_reduced,columns=['PC1','PC2','PC3'])\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "PCA_set = PCA(n_components=5)\nY = PCA_set.fit_transform(xStand)\nY\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "airport_operations = pd.read_csv(\"./assets/datasets/airport_operations.csv\")\nairport_operations.head()\n", "intent": "You might want to restart your kernel between parts of this lab, to avoid crossover of variable names.\n"}
{"snippet": "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=10000, stop_words='english')\nX = vectorizer.fit_transform(docs_raw)\n", "intent": "Ok, let's fit a vectorizer to get some features. You could use a CountVectorizer or a TfidfVectorizer.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = pre_poll[['state','edu','age']]\ny = pre_poll['inclusion']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3)\n", "intent": "Split the data in the ordinary way, making sure you have a 70/30 split.\n"}
{"snippet": "cat_encoder = CategoricalEncoder(encoding=\"onehot-dense\")\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)\nhousing_cat_1hot\n", "intent": "Alternatively, you can specify the encoding to be `\"onehot-dense\"` to get a dense matrix rather than a sparse matrix:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n", "intent": "You should keep 25% of the data in the test set.\n"}
{"snippet": "cv = CountVectorizer(ngram_range=(1,2), max_features=2500, binary=True, stop_words='english')\nwords = cv.fit_transform(rt[\"quote\"])\n", "intent": "It is up to you what ngram range you want to select. **Make sure that `binary=True`**\n"}
{"snippet": "Xtrain, Xtest, ytrain, ytest = train_test_split(words.values, rt[\"fresh\"].values, test_size=0.25)\n", "intent": "You should keep 25% of the data in the test set.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime, random_state=0)\nlinreg = LinearRegression().fit(X_train, y_train)\nprint('Crime Dataset')\nprint('Linear model intercept : {}'.format(linreg.intercept_))\nprint('Linear model coeff :\\n {}'.format(linreg.coef_))\nprint('R squared score (training) : {:.3f}'.format(linreg.score(X_train, y_train)))\nprint('R squared score (test) : {:.3f}'.format(linreg.score(X_test, y_test)))\n", "intent": "Linear Regression on Crime data\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nMinMaxScaler().fit_transform(df)\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "print(lm.coef_)\ncdf=pd.DataFrame(lm.coef_,x.columns,columns=['Coef'])\ncdf\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "df=pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "df_new=pd.DataFrame(df_scaled,columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data=pd.read_csv(\"train.csv\")\ndata.shape\ndata.loc[11]\n", "intent": "Load the \"train.csv\" dataset. You will be using the same file for sampling training and testing points.\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nnum_pipeline = Pipeline([\n        ('imputer', Imputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\n", "intent": "Now let's build a pipeline for preprocessing the numerical attributes:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1)\nX_test['Sex'].value_counts\n", "intent": "Split the dataset into train and test with a test size of 20% of total dataset.\n"}
{"snippet": "data =pd.read_csv(\"train.csv\")\n", "intent": "Load the \"train.csv\" dataset. You will be using the same file for sampling training and testing points.\n"}
{"snippet": "l = LabelEncoder()\nX['Sex'] = l.fit_transform(X.Sex)\n", "intent": "The values in the 'Sex' column are 'Male/Female'. So convert them into 1/0 using Label Encoding.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.4)\n", "intent": "Split the dataset into train and test with a test size of 20% of total dataset.\n"}
{"snippet": "data=pd.read_csv(\"train.csv\")\ndata.shape\n", "intent": "Load the \"train.csv\" dataset. You will be using the same file for sampling training and testing points.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1)\n", "intent": "Split the dataset into train and test with a test size of 20% of total dataset.\n"}
{"snippet": "pred_data = pd.read_table('./prediction.txt', delim_whitespace = True)\nX_pred = pred_data[['G', 'P','Z']].values\nX_pred = np.concatenate((np.ones((X_pred.shape[0], 1), dtype = X_pred.dtype), X_pred), axis = 1)\nX_pred\n", "intent": "Import the prediction dataset\n"}
{"snippet": "top_10_zip=pd.DataFrame(data['zip'].value_counts().head(10))\ntop_10_zip.reset_index(inplace=True)\ntop_10_zip.columns=['ZIP','Count']\ntop_10_zip\n", "intent": "Top 10 Zipcodes for Emergency Calls\n"}
{"snippet": "top_10_twp=pd.DataFrame(data['twp'].value_counts().head(10))\ntop_10_twp.reset_index(level=0,inplace=True)\ntop_10_twp.columns=['Township','Count']\ntop_10_twp\n", "intent": "Top 10 townships for 911 calls\n"}
{"snippet": "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.most_frequent = pd.Series([X[c].value_counts().index[0] for c in X],\n                                       index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.most_frequent)\n", "intent": "We will also need an imputer for the string categorical columns (the regular `Imputer` does not work on those):\n"}
{"snippet": "poly=PolynomialFeatures(2)\nclus_quad=pd.DataFrame(poly.fit_transform(clus_info),columns=['1','lat','lng','lat^2','lat*lng','lng^2'])\n", "intent": "Lets find the average population density of each of the clusters \n"}
{"snippet": "top_10_zip=pd.DataFrame(data['zip'].value_counts().head(10))\ntop_10_zip.reset_index(level=0,inplace=True)\ntop_10_zip.columns=['ZIP','Count']\ntop_10_zip\n", "intent": "Top 10 Zipcodes for Emergency Calls\n"}
{"snippet": "loan=pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "data=pd.read_csv('College_Data',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "dataframe=pd.read_csv(\"KNN_Project_Data\")\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "dataframe_feat=pd.DataFrame(scaled,columns=dataframe.columns[:-1])\ndataframe.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer().fit(X)\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "X=yelp_class['text']\ny=yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "from sklearn.pipeline import FeatureUnion\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", num_pipeline),\n        (\"cat_pipeline\", cat_pipeline),\n    ])\n", "intent": "Finally, let's join the numerical and categorical pipelines:\n"}
{"snippet": "def impute(col,value):\n    trainSet[col] = trainSet[col].fillna(value)\n    testSet[col] = testSet[col].fillna(value)\nimpute('PoolQC','No')\n", "intent": "First of all, I'll try to impute missing data for features identified earlier especially for categoric features\n"}
{"snippet": "X = ratings[['user', 'movie']].values\ny = ratings['rating'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n", "intent": "Create a traditional (X, y) pairing of data and label, then split the data into training and test data sets.\n"}
{"snippet": "import json\nimport brewer2mpl\ndata = json.loads(open('mplrc.json','r').read())\nfor x in data.keys():\n    try:\n        mpl.rcParams[x] = data[x]\n    except ValueError:\n        pass\ncolors = brewer2mpl.get_map('Set1', 'qualitative', 8).mpl_colors\nmpl.rcParams['axes.color_cycle'] = colors\n", "intent": "A little style from the previous session.\n"}
{"snippet": "import sys\nsys.path.append('mglearn')\nimport make_blobs\nX, y = make_blobs.make_blobs(centers=2, n_samples=1000, random_state=0)\n", "intent": "At first the blobs are made for evaluation\n"}
{"snippet": "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())\n", "intent": "Keras returns classes as a single column, so we convert to one hot encoding\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n", "intent": "In order to check the effect of regularization on test data, split the data into train and test using sklearn.\n"}
{"snippet": "grouped_by_flight_df = pd.DataFrame(flights_df.groupby([flights_df.carrier,flights_df.flight, flights_df.dest]).count())\ngrouped_by_flight_df[grouped_by_flight_df.day==365]\n", "intent": "Which flights (i.e. carrier + flight + dest) happen every day? Where do they fly to?\n"}
{"snippet": "weather_df = pd.read_csv('weather.csv.bz2')\nweather_df.shape\n", "intent": "What weather conditions are associated with flight delays leaving NYC? Use graphics to explore.\n"}
{"snippet": "a = flights_df[flights_df.dest=='SEA']\nb = a.origin.value_counts(dropna=False)\nc = pd.DataFrame(b)\nc.reset_index(level=0,inplace=True)\nprint(c)\nprint('JFK',c.origin.loc[0]/c.origin.sum()*100)\nprint('EWR',c.origin.loc[1]/c.origin.sum()*100)\n", "intent": "(e) What proportion of flights to Seattle come from each NYC airport?\n"}
{"snippet": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nX = np.array(ham_emails + spam_emails)\ny = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "Okay, before we learn too much about the data, let's not forget to split it into a training set and a test set:\n"}
{"snippet": "weather_df=pd.read_csv('weather.csv.bz2')\nweather_df.head()\n", "intent": "What weather conditions are associated with flight delays leaving NYC? Use graphics to explore.\n"}
{"snippet": "rotated_df = pd.DataFrame(PCA().fit_transform(X.as_matrix()))\nrotated_df.columns = ['pc'+str(i) for i in range(33)]\n", "intent": "Majority of the variance is explained by the first two components.\n"}
{"snippet": "housing_data = pd.read_csv(\"RealEstate.csv\")\nhousing_data.head()\n", "intent": "We are going to solve the problem of predicting **house prices** based on historical data. \n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nprint(digits.DESCR)\n", "intent": "Apply PCA (or RandomizedPCA?) to the Hand Written Digits dataset.\n"}
{"snippet": "from statsmodels.sandbox.tools.tools_pca import pca\nprcomp = pca(X.T)\nprint \"Standard deviations:\\n{}\".format(np.sqrt(prcomp[2]))\nprint \"\\nRotation:\\n{}\".format(prcomp[3])\n", "intent": "Principal Components\n========================================================\nThe easy way:\n--------------------------------------------------------\n"}
{"snippet": "def pca_summary(prcomp):\n    return pd.DataFrame([np.sqrt(prcomp.explained_variance_), \n              prcomp.explained_variance_ratio_, \n              prcomp.explained_variance_ratio_.cumsum()],\n             index = [\"Standard deviation\", \"Proportion of Variance\", \"Cumulative Proportion\"], \n             columns = (map(\"PC{}\".format, range(1, len(prcomp.components_)+1))))\n", "intent": "Percent Variance Accounted For\n========================================================\n"}
{"snippet": "heptathlon_unscaled_pca = PCA().fit(X_heptathlon.copy())\npca_summary(heptathlon_unscaled_pca)\n", "intent": "* Why is it important? (in other words, why might unscaled variables cause a problem with PCA?)\n"}
{"snippet": "df = pd.read_csv('data/apib12tx.csv')\ndf\n", "intent": "Let's load the data and give it a quick look.\n"}
{"snippet": "df = pd.read_csv('data/apib12tx.csv')\n", "intent": "Let's load the data and give it a quick look.\n"}
{"snippet": "X_few = X_train[:3]\nX_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\nX_few_wordcounts\n", "intent": "Let's try this transformer on a few emails:\n"}
{"snippet": "df = pd.read_csv('sentiment.csv', usecols=[1,7,8,9,10])\ndf.Date = pd.to_datetime(df.Date)\ndf.Date = df.Date.dt.date\ndf = df.set_index(pd.DatetimeIndex(df.Date))\nprint(df.shape)\ndf.head()\n", "intent": "-------------------------\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(df3_mean[[('XRP','Price'),('XRP','Change %')]])\nprice_norm = scaler.transform(df3_mean[[('XRP','Price'),('XRP','Change %')]])\n", "intent": "Next to keep everything standardized, lets normalize the XRP Price data to 1\n"}
{"snippet": "xold=np.arange(0,10,0.5)\nxtrain, xtest = train_test_split(xold)\nxtrain = np.sort(xtrain)\nxtest = np.sort(xtest)\nprint(xtrain, xtest)\n", "intent": "Calculate covariance based on the kernel\n"}
{"snippet": "df = pd.read_csv(\"data/Kline2.csv\", sep=';')\ndf.head()\n", "intent": "We read back the Oceanic tools data\n"}
{"snippet": "dfd = pd.read_csv(\"data/distmatrix.csv\", header=None)\ndij=dfd.values\ndij\n", "intent": "And read in the distance matrix\n"}
{"snippet": "dfcorr = pd.DataFrame(medcorrij*100).set_index(df.culture.values)\ndfcorr.columns = df.culture.values\ndfcorr\n", "intent": "We'll data frame it to see clearly\n"}
{"snippet": "ofdata=pd.read_csv(\"data/oldfaithful.csv\")\nofdata.head()\n", "intent": "We start by considering waiting times from the Old-Faithful Geyser at Yellowstone National Park.\n"}
{"snippet": "hm2df=pm.trace_to_dataframe(tracehm2)\nhm2df.head()\n", "intent": "The slope and intercept are very highly correlated:\n"}
{"snippet": "dffull=pd.read_csv(\"data/religion.csv\")\ndffull.head()\n", "intent": "Let us assume that we have a \"population\" of 200 counties $x$:\n"}
{"snippet": "from sklearn.pipeline import Pipeline\npreprocess_pipeline = Pipeline([\n    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n])\nX_train_transformed = preprocess_pipeline.fit_transform(X_train)\n", "intent": "We are now ready to train our first spam classifier! Let's transform the whole dataset:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nztrain, ztest, xtrain, xtest = train_test_split(z,x)\n", "intent": "Now we split into a training set and a test set.\n"}
{"snippet": "from sklearn.datasets import load_boston \nimport pandas as pd\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns = boston.feature_names)\ndf['MEDV'] = boston.target\nprint(boston.DESCR)\n", "intent": "Now it's time to learn how to perform linear regression. We will analyze the Boston housing dataset loaded in the cell below.\n"}
{"snippet": "dfFeat = pd.DataFrame(scaledFeatures,columns=df.columns[:-1])\ndfFeat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y,\n                                test_size=0.3,random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "import sklearn.cross_validation\ndata_train, data_test, target_train, target_test = sklearn.cross_validation.train_test_split(\n    data.data, data.target, test_size=0.20, random_state = 5)\nprint(data.data.shape, data_train.shape, data_test.shape)\n", "intent": "well not that great. Let's use a supervised classifier\nFirst, split our data in train and test set\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\ntype(boston)\n", "intent": "    from sklearn.datasets import load_boston\n    boston = load_boston()\n    print(boston.DESCR)\n    boston_df = boston.data\n"}
{"snippet": "dataframe = pd.read_csv(\"conversion_data.csv\")\ndataframe.head(10)\n", "intent": "Load data and data exploration\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nitrain, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask=np.ones(critics.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "bonds['industry'] = bonds['industry'].fillna('Financial')\nbonds['subindustry'] = bonds['subindustry'].fillna('Commer Banks Non-US')\n", "intent": "Clean industry columns.\n"}
{"snippet": "from sklearn.svm import SVC\nfrom sklearn import datasets\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  \ny = iris[\"target\"]\nsetosa_or_versicolor = (y == 0) | (y == 1)\nX = X[setosa_or_versicolor]\ny = y[setosa_or_versicolor]\nsvm_clf = SVC(kernel=\"linear\", C=float(\"inf\"))\nsvm_clf.fit(X, y)\n", "intent": "The next few code cells generate the first figures in chapter 5. The first actual code sample comes after:\n"}
{"snippet": "data['interest_mean_by_bond'].fillna(data['interest_mean_by_bond'].mean(), inplace=True)\ndata['interest_mean_by_customer'].fillna(data['interest_mean_by_customer'].mean(), inplace=True)\ndata['interest_mean_by_bond_and_action'].fillna(data['interest_mean_by_bond'].mean(), inplace=True)\ndata['interest_mean_by_customer_and_action'].fillna(data['interest_mean_by_customer'].mean(), inplace=True)\n", "intent": "Handle missing values.\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  \ny = iris[\"target\"]\nsetosa_or_versicolor = (y == 0) | (y == 1)\nX = X[setosa_or_versicolor]\ny = y[setosa_or_versicolor]\n", "intent": "Let's use the Iris dataset: the Iris Setosa and Iris Versicolor classes are linearly separable.\n"}
