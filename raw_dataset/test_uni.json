{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "Split it into a training set and a test set:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n", "intent": "Don't forget to scale the data:\n"}
{"snippet": "df = pd.read_csv('/home/data_scientist/data/misc/sim.reg.data')\ndf.head()\n", "intent": "The cell below reads in a simulated dataset where y (labels) is an unknown function of a,b, and c (your features).\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "b. Split it into a training set and a test set using `train_test_split()`.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX2D = pca.fit_transform(X)\n", "intent": "With Scikit-Learn, PCA is really trivial. It even takes care of mean centering for you:\n"}
{"snippet": "X3D_inv = pca.inverse_transform(X2D)\n", "intent": "Recover the 3D points projected on the plane (PCA 2D subspace).\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=0.95)\nX_train_reduced = pca.fit_transform(X_train)\n", "intent": "*Exercise: Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%.*\n"}
{"snippet": "from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, random_state=42)\nX_reduced = tsne.fit_transform(X)\n", "intent": "Now let's use t-SNE to reduce dimensionality down to 2D so we can plot the dataset:\n"}
{"snippet": "idx = (y == 2) | (y == 3) | (y == 5) \nX_subset = X[idx]\ny_subset = y[idx]\ntsne_subset = TSNE(n_components=2, random_state=42)\nX_subset_reduced = tsne_subset.fit_transform(X_subset)\n", "intent": "Let's see if we can produce a nicer image by running t-SNE on these 3 digits:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_housing_data = scaler.fit_transform(housing.data)\nscaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n", "intent": "Gradient Descent requires scaling the feature vectors first. We could do this using TF, but let's just use Scikit-Learn for now.\n"}
{"snippet": "from sklearn.datasets import make_moons\nm = 1000\nX_moons, y_moons = make_moons(m, noise=0.1, random_state=42)\n", "intent": "First, let's create the moons dataset using Scikit-Learn's `make_moons()` function:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(data[:100])\nX_test = scaler.transform(data[100:])\n", "intent": "Normalize the data:\n"}
{"snippet": "df2 = pd.read_csv('/home/data_scientist/data/misc/sim.reg.w4p3.data')\nX = df2.drop('y', axis=1)\ny = df2['y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\ndf2.head()\n", "intent": "The cell below reads in a more complex simulated dataset, where y is some unknown function of all (or a subset of) the features a,b,c,d,e, and f.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler_all = StandardScaler()\ndf_data_scaled = scaler_all.fit_transform(df_data)\n", "intent": "DBSCAN does not have the concept of 'predict'. \nWe'll use the \"full\" dataset (first 300 values), without splitting train and test.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../data/research-and-development-expenditure-by-type-of-cost/research-and-development-expenditure-by-type-of-cost.csv',\n                 usecols=['sector', 'type_of_expenditure', 'type_of_cost', 'rnd_expenditure'])\ndf.head()\n", "intent": "1. Encode string labels to numbers\n2. Ensure dataset is balanced\n"}
{"snippet": "dimensions = [2, 3, 5, 7]\nfor n in dimensions:\n    print('========= Projection into %d dimensions =========' % n)\n    pca = KernelPCA(n_components=n)\n    train_X_pca = pca.fit(train_X).transform(train_X)\n    test_X_pca = pca.transform(test_X)\n    print('PCA explained variance ratio: %s' % str(pca.explained_variance_ratio_))\n    TrainRFClassifier(train_X_pca, train_y, test_X_pca, test_y)        \n", "intent": "The code below performs PCA projections before fitting the RandomForestClassifier.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX = df.loc[:, ['quota', 'premium']]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n", "intent": "1. Scale the features\n"}
{"snippet": "from sklearn.datasets import make_classification\nX, y = make_classification(\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n    n_informative=3, n_redundant=1, flip_y=0,\n    n_features=20, n_clusters_per_class=1,\n    n_samples=100, random_state=10\n)\ndf = pd.DataFrame(X)\ndf['target'] = y\ndf.target.value_counts().plot(kind='bar', title='Count (target)');\n", "intent": "For ease of visualization, let's create a small unbalanced sample dataset using the <code>make_classification</code> method:\n"}
{"snippet": "df = pd.read_csv('D:/tmp/graduate-employment-survey-ntu-nus-sit-smu-sutd/graduate-employment-survey-ntu-nus-sit-smu-sutd.csv',\n                 encoding='ISO-8859-1',\n                 usecols=['university', 'employment_rate_overall', 'gross_monthly_median'])\ndf = df.loc[df.university == 'National University of Singapore']\ndf.columns\n", "intent": "The first step is to inspect the data to see what transformation/cleaning is needed.\n"}
{"snippet": "data = {\n    'gross_monthly_median': pd.to_numeric(df.gross_monthly_median, errors='coerce'),\n    'employment_rate_overall': pd.to_numeric(df.employment_rate_overall, errors='coerce')\n}\ndf_dataset = pd.DataFrame(data).dropna()\ndf_dataset.head()\n", "intent": "The `gross_monthly_median` column is handled the same way.\nLet's now perform the data transformation and cleaning.\n"}
{"snippet": "df = pd.read_csv('D:/tmp/poker-hand/poker-hand-training-true.data',\n                 names=['S1', 'C1', 'S2', 'C2', 'S3', 'C3', 'S4', 'C4', 'S5', 'C5', 'CLASS'])\ndf.head()\n", "intent": "1. read_csv for both training and test set\n"}
{"snippet": "df = pd.read_csv('D:/tmp/beijing-pmi/PRSA_data_2010.1.1-2014.12.31.csv')\ndf.head()\n", "intent": "Check:\n- Did the data import correctly?\n- What are the types of each column?\n- How large is the dataset?\n"}
{"snippet": "df = pd.read_csv('/home/data_scientist/data/misc/sim.data')\ndf.head()\n", "intent": "The cell below reads in a simulated dataset where y(labels) are a unknown function of a, b, and c.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n", "intent": "4. train-test split\n"}
{"snippet": "with open(anchors_path, 'r') as f:\n    print(f.read())\n", "intent": "These are the pre-defined anchors, chosen to be representative of the ground truth detection boxes.\nThese are found using K-means clustering.\n"}
{"snippet": "with open(classes_path, 'r') as f:\n    print(f.read())\n", "intent": "These are the classes of objects that the detector will recognize.\n"}
{"snippet": "encoder = LabelEncoder()\ncategory_enc = encoder.fit_transform(df.Category)\ndf['y'] = category_enc\ndf\n", "intent": "Label encode the 'Category' column into numbers to form our output (Y).\n"}
{"snippet": "df = pd.read_csv('d:/tmp/news-article/Full-Economic-News-DFE-839861.csv', encoding='latin1',\n                usecols=['relevance', 'text'])\ndf.head()\n", "intent": "1. Load the data into pandas\n2. Check for NaNs (if any) and decide what you would do with them\n"}
{"snippet": "def play_midi(filename):\n    from music21 import midi\n    mf = midi.MidiFile()\n    mf.open(filename)\n    mf.read()\n    mf.close()\n    stream = midi.translate.midiFileToStream(mf)\n    stream.show('midi')\nplay_midi('Classical-Piano-Composer/test_output.mid')\n", "intent": "This produces the following file:\n`test_output.mid`\n"}
{"snippet": "df_train = pd.read_csv('fifa_processed_train.csv')\ndf_test = pd.read_csv('fifa_processed_test.csv')\n", "intent": "Load pre-processed data from Day 9 workshop.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n   'This is the first document.',\n   'This document is the second document.',\n   'And this is the third one.',\n   'Is this the first document?',\n]\ncv = CountVectorizer()\nresult = cv.fit_transform(corpus)\nprint(result) \n", "intent": "* Words that are used more frequently get a higher count\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus = [\n   'This is the first document.',\n   'This document is the second document.',\n   'And this is the third one.',\n   'Is this the first document?',\n]\ntfidf = TfidfVectorizer()\nresult = tfidf.fit_transform(corpus)\nresult.todense()\n", "intent": "- TF: Term Frequency: rewards words commonly used in a document\n- IDF: Inverse Document Frequency: penalises words commonly used in all documents\n"}
{"snippet": "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=.7, random_state=0)\n", "intent": "The code cell below creates a validation set.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n", "intent": "- Train test split\n- Iteratively:\n - Select model parameters\n - Cross validate\n - Plot learning curve\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nencoders = dict()\nfor f in features:\n    if (df[f].dtypes == 'object'):\n        encoder = LabelEncoder()\n        encoder.fit(df[f])        \n        df[f] = encoder.transform(df[f])\n        encoders[f] = encoder\n", "intent": "For simplicity, let's label encode these categorical features.\nHow to one-hot encode, for reference:\n```\npd.get_dummies(df['marital'])\n```\n"}
{"snippet": "Z_train, Z_test, y_train, y_test = train_test_split(Z_scaled, y)\n", "intent": "- Train/test split\n- Train a Decision Tree Classifier\n- Evaluation Metrics\n- Visualise the decision tree\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../data/imdb-review-dataset/imdb_master.csv',\n                 encoding='latin_1', index_col=0)\ndf1 = df.loc[df.label != 'unsup']\ndf1.label.value_counts() \n", "intent": "Dataset: https://www.kaggle.com/utathya/imdb-review-dataset/version/1\n"}
{"snippet": "df = pd.read_csv('./bitcoin/bitcoin/bitflyerJPY_1-min_data_2018-06-01_to_2018-06-27.csv')\ndf.head()\n", "intent": "```\npd.read_csv()\n```\n"}
{"snippet": "df1 = df.loc[df.Timestamp < '2018-06-02 00:00']\ndf1.to_csv('bitcoin_June1_2018.csv', index=False) \ndf1.head()\n", "intent": "```\ndf.to_csv()\n```\n"}
{"snippet": "array1 = df['Close'].values\nprint(array1)\ndf1 = pd.DataFrame(array1)\ndf1.head()\n", "intent": "Create Dataframe from Numpy array\n```\ndf = pd.DataFrame(numpy_array)\n```\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n", "intent": "We will perform feature scaling.\nBefore features can be scaled, we need to holdout (separate out) the test dataset from the training dataset. \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n", "intent": "No imbalance, go ahead and train a decision tree classifier\n"}
{"snippet": "df = pd.read_csv('/home/data_scientist/data/misc/sim.data')\ndf.drop('y', axis=1, inplace=True)\ndf.head()\n", "intent": "The cell below reads in a simulated dataset with the features a, b, and c.\n"}
{"snippet": "df_kmeans = pd.concat([pd.DataFrame(X, columns=['sepal length (cm)', 'sepal width (cm)',\n                                                'petal length (cm)', 'petal width (cm)']),\n                       pd.DataFrame(cluster_kmeans, columns=['cluster_id'])], axis=1)\n", "intent": "Let's explore the clusters using scatter matrix to see if we discover patterns.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, cluster_kmeans, random_state=42)\n", "intent": "Train a decision tree using any set of clusters from earlier workshop.\n"}
{"snippet": "df.to_csv('./dataset1_dataset2_solution.csv',index=False)\n", "intent": "Since there are no more duplicates, we can output the file\n"}
{"snippet": "_, bp = df4.drop(columns=['id','date','yr_renovated','waterfront','lat','long']).boxplot(figsize=(15,10),return_type='both');\noutliers = []\nfor flier in bp[\"fliers\"]:\n    outliers.append(list(flier.get_ydata()))\noutlier_no = []\nfor l in outliers:\n    outlier_no.append(len(l))\npd.DataFrame(np.array(outlier_no).reshape(1,13),columns=df4.drop(columns=['id','date','yr_renovated','waterfront','lat','long']).columns)    \n", "intent": "After the deletion, we check the number of outliers once more\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(input = 'content', analyzer = 'word')\ntfidf_vectors = tfidf_vectorizer.fit_transform(sents_words)\ntfidf_vectors.shape\n", "intent": "Write your code below to generate TF-IDF vector.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(boston_df.iloc[:,:-1],boston_df.iloc[:,-1:], random_state = 1)\n", "intent": "we will select a feature to degrade (remove some values as missing data)\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(boston_df.iloc[:,:-1],boston_df.iloc[:,-1:],random_state=111)\n", "intent": "\\begin{equation}\ny_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\n\\end{equation}\n"}
{"snippet": "CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\nFEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS) - 1]\nLABEL = CSV_COLUMNS[0]\ndf_train = pd.read_csv('./taxi-train.csv', header = None, names = CSV_COLUMNS)\ndf_valid = pd.read_csv('./taxi-valid.csv', header = None, names = CSV_COLUMNS)\ndf_test = pd.read_csv('./taxi-test.csv', header = None, names = CSV_COLUMNS)\n", "intent": "Read data created in the previous chapter.\n"}
{"snippet": "def get_distinct_values(column_name):\n    return bq.Query(sql).execute().result().to_dataframe()\n", "intent": "Let's write a query to find the unique values for each of the columns and the count of those values.\n"}
{"snippet": "df = pd.read_csv('exercise_dataset_SLU13.csv')\nprint('Shape:', df.shape)\ndf.head()\n", "intent": "First, let us load some data to fit into a classifier.\n"}
{"snippet": "stationsdf = bq.Query(stations).execute().result().to_dataframe()\n", "intent": "Let's explore the embeddings for some stations. Let's look at stations with overall similar numbers of trips. Do they have similar embedding values?\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('{}/batch_predict/predictions-00000-of-00001.csv'.format(OUTDIR), names=('key','true_cost','predicted_cost'))\ndf['true_cost'] = df['true_cost'] * 20000\ndf['predicted_cost'] = df['predicted_cost'] * 20000\ndf.head()\n", "intent": "<h3> Prediction </h3>\n"}
{"snippet": "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\nprint(\"Done\")\n", "intent": "Let's download  Fashion MNIST data and examine the shape. Take note of the numbers you will get. You will use them throughout this notebook.\n"}
{"snippet": "df = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep=\",\")\n", "intent": "Next, we'll load our data set.\n"}
{"snippet": "df = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep = \",\")\n", "intent": "Next, we'll load our data set.\n"}
{"snippet": "import shutil\nshutil.rmtree('data/sines', ignore_errors=True)\nos.makedirs('data/sines/')\nnp.random.seed(1) \nfor i in range(0,10):\n  to_csv('data/sines/train-{}.csv'.format(i), 1000)  \n  to_csv('data/sines/valid-{}.csv'.format(i), 250)\n", "intent": "<h3> Cloud ML Engine </h3>\nNow to train on Cloud ML Engine with more data.\n"}
{"snippet": "import os, shutil\nDATADIR='data/txtcls'\nshutil.rmtree(DATADIR, ignore_errors=True)\nos.makedirs(DATADIR)\ntraindf.to_csv( os.path.join(DATADIR,'train.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')\nevaldf.to_csv( os.path.join(DATADIR,'eval.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')\n", "intent": "Finally we will save our data, which is currently in-memory, to disk.\n"}
{"snippet": "import shutil\nshutil.rmtree('data/sines', ignore_errors=True)\nos.makedirs('data/sines/')\nfor i in range(0,10):\n  to_csv('data/sines/train-{}.csv'.format(i), 1000)  \n  to_csv('data/sines/valid-{}.csv'.format(i), 250)\n", "intent": "<h3> Cloud ML Engine </h3>\nNow to train on Cloud ML Engine with more data.\n"}
{"snippet": "categories_list = open(\"categories.txt\").read().splitlines()\nauthors_list = open(\"authors.txt\").read().splitlines()\ncontent_ids_list = open(\"content_ids.txt\").read().splitlines()\nmean_months_since_epoch = 523\n", "intent": "To start, we'll load the list of categories, authors and article ids we created in the previous **Create Datasets** notebook.\n"}
{"snippet": "import keras\nfrom keras.datasets import cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n", "intent": "---\nIn this notebook, we train a CNN on augmented images from the CIFAR-10 database.\n"}
{"snippet": "test_set_df = bq.Query(sql).execute().result().to_dataframe()\ntest_set_df.to_csv('test_set.csv', header=False, index=False, encoding='utf-8')\ntest_set_df.head()\n", "intent": "Repeat the query as above but change outcome of the farm fingerprint hash to collect the remaining 10% of the data for the test set.\n"}
{"snippet": "CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\nFEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS) - 1]\nLABEL = CSV_COLUMNS[0]\ndf_train = pd.read_csv('./taxi-train.csv', header = None, names = CSV_COLUMNS)\ndf_valid = pd.read_csv('./taxi-valid.csv', header = None, names = CSV_COLUMNS)\n", "intent": "Read data created in the previous chapter.\n"}
{"snippet": "style_image = scipy.misc.imread(\"images/monet_800600.jpg\")\nimshow(style_image)\n", "intent": "For our running example, we will use the following style image: \n"}
{"snippet": "content_image = scipy.misc.imread(\"images/louvre_small.jpg\")\ncontent_image = reshape_and_normalize_image(content_image)\n", "intent": "Let's load, reshape, and normalize our \"content\" image (the Louvre museum picture):\n"}
{"snippet": "style_image = scipy.misc.imread(\"images/monet.jpg\")\nstyle_image = reshape_and_normalize_image(style_image)\n", "intent": "Let's load, reshape and normalize our \"style\" image (Claude Monet's painting):\n"}
{"snippet": "customer_log_df = np.log(customer_features)\nscaler.fit(customer_log_df)\ncustomer_log_sc = scaler.transform(customer_log_df)\ncustomer_log_sc_df = pd.DataFrame(customer_log_sc, columns=customer_features.columns)\n", "intent": "Many times the skew of data can be easily removed by taking the log of the data. Let's do so here.\nWe will then scale the data after deskewing.\n"}
{"snippet": "scaler = StandardScaler()\ncustomer_sc = scaler.fit_transform(customer_features)\ncustomer_sc_df = pd.DataFrame(customer_sc, columns=customer_features.columns)\nsc_stats = customer_features.describe().T\nsc_stats['skew'] = st.skew(customer_features)\nsc_stats['kurt'] = st.kurtosis(customer_features)\ndisplay(stats)\ndisplay(sc_stats)\n", "intent": "$$Z = \\frac{X-\\mu}{\\sigma}$$\n"}
{"snippet": "customer_log_df = np.log(1+customer_features)\nscaler.fit(customer_log_df)\ncustomer_log_sc = scaler.transform(customer_log_df)\ncustomer_log_sc_df = pd.DataFrame(customer_log_sc, columns=customer_features.columns)\n", "intent": "Many times the skew of data can be easily removed by taking the log of the data. Let's do so here.\nWe will then scale the data after deskewing.\n"}
{"snippet": "import keras\nfrom keras.datasets import cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n", "intent": "---\nIn this notebook, we train a CNN to classify images from the CIFAR-10 database.\n"}
{"snippet": "column_names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\", \n                \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\ndata = pd.read_csv(\"abalone.data\", names=column_names)\nprint(\"Number of samples: %d\" % len(data))\ndata.head()\n", "intent": "There are no column labels in the data, so we copy them from the documentation and use `pandas` to read and print few lines of the dataset.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\n_, i_test = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[i_test] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "merged = pd.merge(df_transactions, df_offers)\npivoted = merged.pivot_table(index='customer_name', columns='offer_id', values='n', fill_value=0)\npivoted\n", "intent": "<div class=\"span5 alert alert-success\">\n<h4>SOLUTIONS: Exercise Set I</h4>\n    </div>\n"}
{"snippet": "golf_df = pd.read_csv(\"golf_data.csv\")\ngolf_df\n", "intent": "<b>Naive Bayesian Classifier</b><br>\nHere, we'll train a Naive Bayesian classifier to make a prediction as to whether or not we'll play golf.\n"}
{"snippet": "golf_df[\"Outlook\"] = LabelEncoder().fit_transform(golf_df[\"Outlook\"])\ngolf_df[\"Wind\"] = LabelEncoder().fit_transform(golf_df[\"Wind\"])\ngolf_df[\"Play\"] = LabelEncoder().fit_transform(golf_df[\"Play\"])\ngolf_df\n", "intent": "The first thing we have to do is encode the catagorical and binary variables\n"}
{"snippet": "X_Train, X_Test, Y_Train, Y_Test = train_test_split(golf_df[[\"Outlook\", \"Temperature\", \"Humidity\", \"Wind\"]], \n                                                    golf_df[\"Play\"], test_size = 0.3, random_state = 0)\n", "intent": "Next we split the data\n"}
{"snippet": "df=pd.DataFrame({\"age\": x_data, \"bmi\": y_data})\nmodel = ols(\"bmi ~ age\", data=df).fit()\nprint(model.summary())\n", "intent": "Next we use <a href=\"http://www.statsmodels.org/stable/index.html\" target=\"\n"}
{"snippet": "mk_df = pd.read_csv(\"mariokart.csv\")\n", "intent": "<b>MarioKart</b><br>\nThe MarioKart dataset contains data from 141 online auctions, some of games in new condition, others in used condition.\n"}
{"snippet": "mk_df = pd.read_csv(\"MarioKart.csv\")\nmk_df.head()\n", "intent": "<b>Example 1: Mariokart</b><br>\nHere we'll build a multiple regression model using the Mariokart data that we first saw last class.\n"}
{"snippet": "import keras\nfrom keras.datasets import cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the CIFAR-10 database.\n"}
{"snippet": "diabetes = datasets.load_diabetes()\ndiabetes_df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\ndiabetes_df.head()\ndiabetes_target = diabetes.target\ndiabetes_target_binary = (diabetes_target > 140).astype(int)\n", "intent": "Using the diabetes data, build a logistic regression model to predict diabetes (defined clinically as blood glucose above 140)\n"}
{"snippet": "weights = pandas.read_table(\"/home/skipper/school/talks/538model/\"\n                            \"data/pollster_weights.csv\")\n", "intent": "These are old weights obtained from the 538 web site. New weights are not published anywhere to my knowledge.\n"}
{"snippet": "pvi = pandas.read_csv(\"/home/skipper/school/talks/538model/data/partisan_voting.csv\")\npvi.set_index(\"State\", inplace=True);\npvi.PVI = pvi.PVI.replace({\"EVEN\" : \"0\"})\npvi.PVI = pvi.PVI.str.replace(\"R\\+\", \"-\")\npvi.PVI = pvi.PVI.str.replace(\"D\\+\", \"\")\npvi.PVI = pvi.PVI.astype(float)\npvi.PVI\n", "intent": "Partisan voting index\n"}
{"snippet": "party_affil = pandas.read_csv(\"/home/skipper/school/talks/538model/\"\n                              \"data/gallup_electorate.csv\")\nparty_affil.Democrat = party_affil.Democrat.str.replace(\"%\", \"\").astype(float)\nparty_affil.Republican = party_affil.Republican.str.replace(\"%\", \"\").astype(float)\nparty_affil.set_index(\"State\", inplace=True);\nparty_affil.rename(columns={\"Democrat Advantage\" : \"dem_adv\"}, inplace=True);\nparty_affil[\"no_party\"] = 100 - party_affil.Democrat - party_affil.Republican\nparty_affil[[\"dem_adv\", \"no_party\"]]\n", "intent": "Gallup party affiliation (Poll Jan.-Jun. 2012)\n"}
{"snippet": "pvi = pandas.read_csv(\"/home/skipper/school/seaboldgit/talks/pydata/data/partisan_voting.csv\")\n", "intent": "Partican Voting Index data obtained from [Wikipedia](http://en.wikipedia.org/wiki/Cook_Partisan_Voting_Index)\n"}
{"snippet": "party_affil = pandas.read_csv(\"/home/skipper/school/seaboldgit/talks/pydata/data/gallup_electorate.csv\")\n", "intent": "Party affliation of electorate obtained from [Gallup](http://www.gallup.com/poll/156437/Heavily-Democratic-States-Concentrated-East.aspx\n"}
{"snippet": "obama_give = pandas.read_csv(\"/home/skipper/school/seaboldgit/talks/pydata/data/obama_indiv_state.csv\", \n                             header=None, names=[\"State\", \"obama_give\"])\nromney_give = pandas.read_csv(\"/home/skipper/school/seaboldgit/talks/pydata/data/romney_indiv_state.csv\",\n                             header=None, names=[\"State\", \"romney_give\"])\n", "intent": "Campaign Contributions from FEC.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.text, df.sentiment, test_size=0.3)\n", "intent": "Now that we have the 1000 labeled reviews, let's split into a 70/30 train/test set with `sklearn`:\n"}
{"snippet": "data = pd.read_csv(\"https://raw.githubusercontent.com/pburkard88/DS_BOS_07/master/Data/Spam%20Classification/sms.csv\")\ndata.head()\n", "intent": "Now read in the data with `pandas` `read_csv()` and check it out with `head()`:\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint(\"The MNIST database has a training set of %d examples.\" % len(X_train))\nprint(\"The MNIST database has a test set of %d examples.\" % len(X_test))\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the MNIST database.\n"}
{"snippet": "df = pd.read_csv(\"~/git/GA/DS_BOS_07/Data/Spam Classification/sms.csv\")\ndf.head(10)\n", "intent": "Now read in the data with `pandas` `read_csv()` and check it out with `head()`:\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer(decode_error = 'ignore')\nvect.fit(X_train)\nvect.get_feature_names()\n", "intent": "Use `sklearn.feature_extraction.text.CountVectorizer` on the training set to create a vectorizer called `vect`.\n"}
{"snippet": "tdf = pd.read_csv(\"https://raw.githubusercontent.com/pburkard88/DS_BOS_07/master/Data/Titanic/titanic.csv\")\ntdf.head()\n", "intent": "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle.\n"}
{"snippet": "titanic = pd.read_csv('~/git/GA/DS_BOS_07/Data/Titanic/titanic.csv')\ntitanic.head(10)\n", "intent": "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle.\n"}
{"snippet": "df = pd.read_csv('~/git/GA/DS_BOS_07/Data/Heart Disease/heart_disease.csv', names=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num'])\n", "intent": "Note: You'll have to manually add column labels\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) \n", "intent": "Use `train_test_split()` to generate a 70/30 train/test split and store this in `X_train`, `X_test`, `y_train`, `y_test`.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3)\n", "intent": "Use `train_test_split()` to generate a 70/30 train/test split and store this in `X_train`, `X_test`, `y_train`, `y_test`.\n"}
{"snippet": "df_wide = df_wide.fillna(0)\n", "intent": "Set Nans to zero with the `fillna()` function.\n"}
{"snippet": "ratings = pd.read_table('../Data/movielens/ratings.dat', sep='::', names= ['UserID','MovieID','Rating','Timestamp'])\n", "intent": "Load the ratings.dat data into a `ratings` variable with the same separator, and the column names UserID, MovieID, Rating, Timestamp.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\ndata\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "df = pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "df = pd.read_csv('College_Data')\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df_scaled = pd.DataFrame(data=scaled_features,columns=df.columns[:-1])\ndf_scaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df = pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nX_full, y_full = make_classification(n_samples=2400, n_features=20, n_informative=18,\n                                     n_redundant=0, n_classes=2, random_state=2319)\n", "intent": "In this assignment you are asked to apply different classifiers to the dataset:\n"}
{"snippet": "from sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nX, y = make_moons(n_samples=300, shuffle=True, noise=0.05, random_state=1011)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1011)\n", "intent": "Let's generate a toy dataset for classification.\n"}
{"snippet": "cars_info = pd.read_csv('data/cars.csv', index_col=0, dtype=np.float)\nX = cars_info.speed.values.reshape(-1, 1)\n", "intent": "For our experiments we will use \"Cars\" dataset, which contains information about braking distances for several cars from the 1920s.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_scaler = StandardScaler()\nX_train = X_scaler.fit_transform(X)\ny_train = np.array(cars_info.dist)\n", "intent": "Normalization leads to better convergence\n"}
{"snippet": "y_train = train_data['human-generated']\ncontextVectorizer = CountVectorizer().fit(train_data['context'])\nx_context = contextVectorizer.transform(train_data['context'])\nresponseVectorizer = CountVectorizer().fit(train_data['response'])\nx_response = responseVectorizer.transform(train_data['response'])\nx_train = sparse.hstack([x_context,x_response])\n", "intent": "Generate feature matricies for the context and the response. Fill free to create additional\nfeature extraction objects if you like.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer()\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "kpcaTransform = skdec.KernelPCA(n_components=2,kernel=\"rbf\",gamma=0.023)\nkpcaTransform.fit(X)\nXkpca = kpcaTransform.transform(X)\n", "intent": "<br/> <!--Intentionally left blank-->\n"}
{"snippet": "kpcaTransformCircles=skdec.KernelPCA(n_components=1,kernel=\"rbf\",gamma=1.65)\nkpcaTransformCircles.fit(X)\nXkpca = kpcaTransformCircles.transform(X)\n", "intent": "After PCA we still have not separable sets.\n"}
{"snippet": "split = train_test_split(X, y, test_size=0.5,\n                         random_state=42, stratify=y)\ntrain_X, test_X, train_y, test_y = split\n", "intent": "Split data into training sample and test sample\n"}
{"snippet": "X, y = load_iris(return_X_y=True)\nX_train_non_normalized = X[-100:, [1, 2]]\ny_train = y[-100:]\ny_train[y_train == 1] = 1\ny_train[y_train == 2] = -1\nX_train = scale(X_train_non_normalized)\nn_dim = X_train.shape[1]\nn_elements = X.shape[0]\n", "intent": "Out of box sklearn supports various techniques for data preprocessing.\nhttp://scikit-learn.org/stable/modules/preprocessing.html\n"}
{"snippet": "features = features.fillna(0.0)\nfeatures_kaggle = features_kaggle.fillna(0.0) \ndisplay(features.head())\n", "intent": "And now we'll fill in any blanks with zeroes.\n"}
{"snippet": "transformations_pipeline = FeatureUnion(transformer_list=[\n        (\"cap_gain_loss_pipeline\", cap_gain_loss_pipeline),\n        (\"num_pipeline\", num_pipeline),\n        (\"cat_pipeline\", append_categoricals),\n    ])\n", "intent": "Below is the pipeline for combining all of the other pipelines\n"}
{"snippet": "tpm = pd.read_csv('bmdc-data/lps_tpm.csv', index_col=0)\n", "intent": "Included in this repo is a TPM table which only contains expression from ~300 cells which was part of the time course we are interested in.\n"}
{"snippet": "sample_info = pd.read_csv('bmdc-data/sample_info_lps.csv', index_col=0)\n", "intent": "We also have meta data about these cells\n"}
{"snippet": "gene_annotation = pd.read_csv('bmdc-data/mouse_annotation.csv', index_col=0)\ngene_annotation.head()\n", "intent": "Let's load some gene annotations as well, since our TPM table is just indexed by indecipherable Ensembl IDs.\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "**Create a StandardScaler() object called scaler.**\n"}
{"snippet": "cluster_membership = pd.DataFrame({'cluster': np.argmax(mohgp.phi,1)}, index=varying)\n", "intent": "We can extract the genes belonging to the clusters. Here we will look at a few of them.\n"}
{"snippet": "path = './data/vehicles_test.csv'\ntest = pd.read_csv(path)\ntest['vtype'] = test.vtype.map({'car':0, 'truck':1})\ntest\n", "intent": "<a id=\"testing-preds\"></a>\n"}
{"snippet": "path = './data/titanic.csv'\ntitanic = pd.read_csv(path)\ntitanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\ntitanic.Age.fillna(titanic.Age.median(), inplace=True)\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\nembarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\ntitanic = pd.concat([titanic, embarked_dummies], axis=1)\ntitanic.head()\n", "intent": "We'll build a classification tree using the Titanic survival data set:\n"}
{"snippet": "import pandas as pd\npath = './data/vehicles_train.csv'\ntrain = pd.read_csv(path)\ntrain['vtype'] = train.vtype.map({'car':0, 'truck':1})\ntrain\n", "intent": "<a id=\"manual-bagged\"></a>\n"}
{"snippet": "df = pd.read_csv(\"assets/datasets/iris.csv\")\nprint df['Name'].value_counts()\ndf.head(5)\n", "intent": "Let's do some clustering with the iris dataset.\n"}
{"snippet": "import pandas as pd\nurl = './data/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer\n", "intent": "<a id=\"k-means-demo\"></a>\n---\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "Both sklearn and seaborn have ways to import the iris data:\n- `sklearn.datasets.load_iris()`\n- `sns.load_dataset(\"iris\")`\nThe seaborn way is easier.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(y)\nle.classes_\n", "intent": "- Compare the predicted labels vs. the actual labels.\n"}
{"snippet": "Xs = StandardScaler().fit_transform(X)\nXs = pd.DataFrame(Xs, columns=X.columns)\n", "intent": "Standardize the data and compare at least one of the scatterplots for the scaled data to unscaled above.\n"}
{"snippet": "scaled_features = scaler.fit_transform(data.drop('Class',axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "import pandas as pd\nurl = '../data/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer\n", "intent": "<a id=\"k-means-demo\"></a>\n---\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\ndf = pd.read_csv('assets/datasets/titanic.csv')\ninclude = ['Pclass', 'Sex', 'Age', 'Fare', 'SibSp', 'Survived']\ndf['Sex'] = df['Sex'].apply(lambda x: 0 if x == 'male' else 1)\ndf = df[include].dropna()\nX = df[['Pclass', 'Sex', 'Age', 'Fare', 'SibSp']]\ny = df['Survived']\nPREDICTOR = RandomForestClassifier(n_estimators=100).fit(X, y)\n", "intent": "Our classifier algorithm will be a random forest, which as you know is relatively slow to train.\n"}
{"snippet": "import sklearn.model_selection\n(Xtrain, Xtest, Ytrain, Ytest) = sklearn.model_selection.train_test_split(salaries[['AnnualSalary']],\n                                                                          salaries.GrossPay)\n", "intent": "It seems like there is a linear relationship in there, but it's obscured by a lot of noise.\nSplit the data into training and testing data sets.\n"}
{"snippet": "pd.DataFrame(sanders).head()\n", "intent": "> *Hint: this is as easy as passing it to the DataFrame constructor!*\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)\n", "intent": "Notice that we create the train/test split first. This is because we will reveal information about our testing data if we standardize right away.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXs = ss.fit_transform(X)\n", "intent": "- Plot the standardized mean cross-validated accuracy against the unstandardized. Which is better?\n- Why?\n"}
{"snippet": "ss = StandardScaler()\nXs = ss.fit_transform(X)\n", "intent": "**10.C: Standardize the predictor matrix.**\n"}
{"snippet": "url = './data/bikeshare.csv'\nbikes = pd.read_csv(url, index_col='datetime', parse_dates=True)\n", "intent": "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n"}
{"snippet": "y = kobe.SHOTS_MADE.values\nX = kobe.iloc[:,1:]\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXs = ss.fit_transform(X)\n", "intent": "---\nWhy is normalization necessary for regularized regressions?\nUse the `sklearn.preprocessing` class `StandardScaler` to standardize the predictors.\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features,columns=data.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "table = pd.DataFrame({'probability':[0.1, 0.2, 0.25, 0.5, 0.6, 0.8, 0.9]})\ntable['odds'] = table.probability / (1 - table.probability)\ntable\n", "intent": "**As an example we can create a table of probabilities vs. odds, as seen below.**\n"}
{"snippet": "X = admissions[['gre']]\ny = admissions['admit']\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=46)\nlogit_simple = linear_model.LogisticRegression(C=1e9).fit(X_train, y_train)\n", "intent": "**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**\n"}
{"snippet": "bank_md1 = pd.get_dummies(bank_a[['age','job','education','day_of_week','y']], drop_first = True)\nbank\nLogReg1 = LogisticRegression()\nX1 = bank_md1.drop('y', axis =1)\ny1 = bank_md1['y']\nx_train1, x_test1, y_train1, y_test1 = train_test_split(X1,y1, random_state =42)\nLogReg1.fit(x_train1, y_train1)\n", "intent": "**Build a Model**  \n*Model 1, using `age`, `job`, `education`, and `day_of_week`*\n"}
{"snippet": "name = bank_md1.columns.drop('y')\ncoef = LogReg1.coef_[0]\npd.DataFrame([name,coef],index = ['Name','Coef']).transpose()\n", "intent": "**Get the Coefficient for each feature.**\n- Be sure to make note of interesting findings.\n*Seems like `job_entrepreneur` carries that largest coef.*\n"}
{"snippet": "yelp = pd.read_csv(csv_file)\nyelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\nX = yelp_best_worst.text\ny = yelp_best_worst.stars\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n", "intent": "- Select only 5-star and 1-star reviews.\n- The text will be the features, the stars will be the target.\n- Create a train-test split.\n"}
{"snippet": "vect = TfidfVectorizer(stop_words='english')\ndtm = vect.fit_transform(yelp.text)\nfeatures = vect.get_feature_names()\ndtm.shape\n", "intent": "> **Note:** Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF!\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nX_train, X_test, y_train, y_test = train_test_split(X[predictors], y, train_size=0.7, random_state=8)\nlr2 = LinearRegression()\nlr2.fit(X_train,y_train)\nlr2.score(X_test, y_test)\n", "intent": "- Score and plot.  \n- How do your metrics change?\n"}
{"snippet": "from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.5, random_state =8 ) \n", "intent": "Score and plot your predictions.\n"}
{"snippet": "college = pd.read_csv('./College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "loans = pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "ad = pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "data = pd.read_csv('./yelp.csv')\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "c = CountVectorizer().fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "from keras.datasets import mnist\ndata_dir=\"C:/Training/udacity/DeepLearningNanoDegree/Projects/mnist-mlp/data/\"\n(X_train, y_train), (X_test, y_test) = mnist.load_data(path=data_dir+\"mnist.npz\")\nprint(\"The MNIST database has a training set of %d examples.\" % len(X_train))\nprint(\"The MNIST database has a test set of %d examples.\" % len(X_test))\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the MNIST database.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX = vote_features\ny = vote_target\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                   test_size = 0.3,\n                                                   random_state = 4444)\n", "intent": "Split the data into test and training sets.\nUse this function: `from sklearn.cross_validation import train_test_split`\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data(path=\"C:/Training/udacity/AI_NanoDegree/Term2/3.Convolutional Neural Networks Videos/datasets/mnist.npz\")\nprint(\"The MNIST database has a training set of %d examples.\" % len(X_train))\nprint(\"The MNIST database has a test set of %d examples.\" % len(X_test))\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the MNIST database.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nLabel_X_1 = LabelEncoder()\nX.iloc[:,1] =  Label_X_1.fit_transform(X.iloc[:,1])\nLabel_X_2 = LabelEncoder()\nX.iloc[:,2] =  Label_X_2.fit_transform(X.iloc[:,2])\n", "intent": "Encode the categorical data\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.1, stratify=y, random_state=0)\n", "intent": "Given that the dependent variable is not uiform, we will use the stratified split\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "Feature scaling, critical for training the deep neural network, even for 0/1 categorical variables\n"}
{"snippet": "df = pd.read_csv('College_Data',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n", "intent": "b. Split it into a training set and a test set using `train_test_split()`.\n"}
{"snippet": "from sklearn.datasets import make_moons\nm = 1000\nX_moons, y_moons = make_moons(m, noise=0.1)\n", "intent": "First, let's create the moons dataset using Scikit-Learn's `make_moons()` function:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\n", "intent": "Normalize the data:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(train_reduced)\nX_test_scaled = scaler.transform(test_reduced)\n", "intent": "Here we will try to use an artificial neural network to predict\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n", "intent": "Here we will use the backward elimination, which is the fastest one\n"}
{"snippet": "data = pd.read_csv('Position_Salaries.csv')\n", "intent": "The basic idea is to reduce the informatio entropy. The algorithm looks for the best split to ...?\n"}
{"snippet": "filename = '/data/flights/On_Time_On_Time_Performance_2014_1.csv'\ndata_fl = pd.read_csv(filename)\n", "intent": "First, we will take an example table and inspect the memory usage for each data type: float, int and object\n"}
{"snippet": "x = np.array(data_pred_delay)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True, random_state=1)\n", "intent": "From the correlation coefficient, we could see that features are not correlated between each pair.\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "without using Scikit-Learn\n"}
{"snippet": "df = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "mat = pd.DataFrame(\n    [ df[f].value_counts() for f in list(cat_features) ],\n    index=list(cat_features)\n    ).stack()\npd.DataFrame(mat.values, index=mat.index)\n", "intent": "Results in a data frame:\n"}
{"snippet": "import scipy\npca_data = doPCA(test_data, 11)\nprint(test_data.shape)\nprint(pca_data.shape)\n", "intent": "To apply PCA-based dimensionality reduction to the letter data we can now type:\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nfrom utils import load_wine\nX, y = load_wine()\nclf = RandomForestClassifier()\nclf.fit(X[3:], y[3:])\n", "intent": "---\nIndividual trees output the class index, `RandomForestClassifier` has already\ntranslated back to class label. This is a bit confusing.\n"}
{"snippet": "pca = PCA(n_components=1)\npca.fit(X)\nX_pca = pca.transform(X)\nprint('shape of transformed data:', X_pca.shape)\nprint('First five data points after transform:')\nprint(X_pca[:5])\n", "intent": "---\nWe can use PCA to reduce the number of dimensions of our data by setting the smallest components to zero.\n"}
{"snippet": "import shutil\nshutil.rmtree('data/sines', ignore_errors=True)\nos.makedirs('data/sines/')\nfor i in xrange(0,10):\n  to_csv('data/sines/train-{}.csv'.format(i), 1000)  \n  to_csv('data/sines/valid-{}.csv'.format(i), 250)\n", "intent": "<h3> Cloud ML Engine </h3>\nNow to train on Cloud ML Engine with more data.\n"}
{"snippet": "def cleanup_nulls(df, variablename):\n  df2 = df.pivot_table(variablename, 'date', 'stationid', fill_value=np.nan)\n  print('Before: {} null values'.format(df2.isnull().sum().sum()))\n  df2.fillna(method='ffill', inplace=True)\n  df2.fillna(method='bfill', inplace=True)\n  df2.dropna(axis=1, inplace=True)\n  print('After: {} null values'.format(df2.isnull().sum().sum()))\n  return df2\n", "intent": "One way to fix this is to do a pivot table and then replace the nulls by filling it with nearest valid neighbor\n"}
{"snippet": "df_std = pd.DataFrame(std, columns=df.columns[:-1])\ndf_std.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\n(training_inputs,\n testing_inputs,\n training_classes,\n testing_classes) = train_test_split(all_inputs, all_classes, train_size=0.75, random_state=1)\n", "intent": "Now our data is ready to be split.\n"}
{"snippet": "train = pd.read_csv('./data/train.csv')\ntest = pd.read_csv('./data/test.csv')\n", "intent": "* Exploratory Data Analysis\n* How to deal with data that has anonymous features\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "cars_cancel_test = pd.read_csv('./data/Kaggle_YourCabs_score.csv', index_col='id')\ncars_cancel_test.head()\n", "intent": "[[ go back to the top ]](\n"}
{"snippet": "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\ntext = open(path).read()\nprint('corpus length:', len(text))\n", "intent": "We're going to download the collected works of Nietzsche to use as our data for this class.\n"}
{"snippet": "tables = [pd.read_csv(fname+'.csv', low_memory=False) for fname in table_names]\n", "intent": "We're going to go ahead and load all of our csv's as dataframes into a list `tables`.\n"}
{"snippet": "joined.CompetitionOpenSinceYear = joined.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)\njoined.CompetitionOpenSinceMonth = joined.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)\njoined.Promo2SinceYear = joined.Promo2SinceYear.fillna(1900).astype(np.int32)\njoined.Promo2SinceWeek = joined.Promo2SinceWeek.fillna(1).astype(np.int32)\n", "intent": "Next we'll fill in missing values to avoid complications w/ na's.\n"}
{"snippet": "joined.to_csv('joined.csv')\n", "intent": "We'll back this up as well.\n"}
{"snippet": "joined = pd.read_csv('joined.csv', index_col=0)\njoined[\"Date\"] = pd.to_datetime(joined.Date)\njoined.columns\n", "intent": "We now have our final set of engineered features.\n"}
{"snippet": "train = pd.DataFrame(utils.load_array(data_path+'train/train_features.bc'),columns=['TRIP_ID', 'CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID',\n       'TIMESTAMP', 'DAY_TYPE', 'MISSING_DATA', 'POLYLINE', 'LATITUDE', 'LONGITUDE', 'DAY_OF_WEEK',\n                            'QUARTER_HOUR', \"WEEK_OF_YEAR\", \"TARGET\", \"COORD_FEATURES\"])\n", "intent": "Meanshift clustering as performed in the paper\n"}
{"snippet": "train = pd.DataFrame(utils.load_array(data_path+'train/train_features.bc'),columns=['TRIP_ID', 'CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID',\n       'TIMESTAMP', 'DAY_TYPE', 'MISSING_DATA', 'POLYLINE', 'LATITUDE', 'LONGITUDE', 'TARGET',\n                            'COORD_FEATURES', \"DAY_OF_WEEK\", \"QUARTER_HOUR\", \"WEEK_OF_YEAR\"])\n", "intent": "Load training data and cluster centers\n"}
{"snippet": "from sklearn import model_selection\nfr_train, fr_test, en_train, en_test = model_selection.train_test_split(\n    fr_padded, en_padded, test_size=0.1)\n[o.shape for o in (fr_train, fr_test, en_train, en_test)]\n", "intent": "And of course we need to separate our training and test sets...\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "veczr =  CountVectorizer(ngram_range=(1,2), tokenizer=tokenize)\ntrn_term_doc = veczr.fit_transform(trn)\nval_term_doc = veczr.transform(val)\n", "intent": "Similar to the model before but with bigram features.\n"}
{"snippet": "df_raw = pd.read_csv(f'{PATH}Train.csv', low_memory=False, \n                     parse_dates=[\"saledate\"])\n", "intent": "*Question*\nWhat stands out to you from the above description?  What needs to be true of our training and validation sets?\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_words,test_words = train_test_split(all_words,test_size=0.1,random_state=42)\n", "intent": "We hold out 10% of all words to be used for validation.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_words,test_words = train_test_split(all_words,test_size=0.1,random_state=42)\n", "intent": "We hold out 20% of all words to be used for validation.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.images.shape\n", "intent": "We'll use Scikit-Learn's data access interface and take a look at this data:\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "Let's demonstrate the naive approach to validation using the Iris data, which we saw in the previous section.\nWe will start by loading the data:\n"}
{"snippet": "categories = ['talk.religion.misc', 'soc.religion.christian',\n              'sci.space', 'comp.graphics']\ntrain = fetch_20newsgroups(subset='train', categories=categories)\ntest = fetch_20newsgroups(subset='test', categories=categories)\n", "intent": "For simplicity here, we will select just a few of these categories, and download the training and testing set:\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nx = np.array([2, 3, 4])\npoly = PolynomialFeatures(3, include_bias=False)\npoly.fit_transform(x[:, None])\n", "intent": "This polynomial projection is useful enough that it is built into Scikit-Learn, using the ``PolynomialFeatures`` transformer:\n"}
{"snippet": "from pandas.tseries.holiday import USFederalHolidayCalendar\ncal = USFederalHolidayCalendar()\nholidays = cal.holidays('2012', '2016')\ndaily = daily.join(pd.Series(1, index=holidays, name='holiday'))\ndaily['holiday'].fillna(0, inplace=True)\n", "intent": "Similarly, we might expect riders to behave differently on holidays; let's add an indicator of this as well:\n"}
{"snippet": "df = pd.read_csv(\"Classified Data\",index_col=0)\n", "intent": "Set index_col=0 to use the first column as the index.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n                                                random_state=42)\n", "intent": "For the sake of testing our classifier output, we will split the data into a training and testing set:\n"}
{"snippet": "digits_new = pca.inverse_transform(data_new)\nplot_digits(digits_new)\n", "intent": "Finally, we can use the inverse transform of the PCA object to construct the new digits:\n"}
{"snippet": "X = vectorizer.fit_transform(corpus)\nX\n", "intent": "Now we have the training corpus defined as a list of raw text documents.  We can pass this to our vectorizer to build our bag of words matrix.\n"}
{"snippet": "path = os.getcwd() + '\\data\\ex2data2.txt'\ndata2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])\ndata2.head()\n", "intent": "Similar to part 1, let's start by visualizing the data.\n"}
{"snippet": "def read_data(filename):\n  f = zipfile.ZipFile(filename)\n  for name in f.namelist():\n    return tf.compat.as_str(f.read(name)).split()\n  f.close()\nwords = read_data(filename)\nprint('Data size %d' % len(words))\n", "intent": "Read the data into a string.\n"}
{"snippet": "import pandas as pd\nfile_path = 'train.csv'\ndf = pd.read_csv(file_path) \ny = df.SalePrice\n", "intent": "The varialbe we want to predict is the SalePrice. So:\n"}
{"snippet": "import pandas as pd\nfile_path = 'train.csv'\ndf = pd.read_csv(file_path) \ny = df.SalePrice\npredictors = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', \n                        'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = df[predictors]\nfrom sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\n", "intent": "Let's load the data again and split it:\n"}
{"snippet": "sub = pd.DataFrame()\nsub['ImageId'] = new_test_ids\nsub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\nsub.to_csv('sub-dsbowl2018-1.csv', index=False)\n", "intent": "... and then finally create our submission!\n"}
{"snippet": "from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\ndata = np.asarray(digits.data, dtype='float32')\ntarget = np.asarray(digits.target, dtype='int32')\nX_train, X_test, y_train, y_test = train_test_split(\n    data, target, test_size=0.15, random_state=37)\nscaler = preprocessing.StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "- normalization\n- train/test split\n"}
{"snippet": "ad_data = pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\ndata = np.asarray(digits.data, dtype='float32')\ntarget = np.asarray(digits.target, dtype='int32')\nX_train, X_test, y_train, y_test = train_test_split(\n    data, target, test_size=0.15, random_state=37)\nscaler = preprocessing.StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "- Normalization\n- Train / test split\n"}
{"snippet": "import pandas as pd\nraw_ratings = pd.read_csv(op.join(ML_100K_FOLDER, 'u.data'), sep='\\t',\n                      names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\nraw_ratings.head()\n", "intent": "Each line contains a rated movie: \n- a user\n- an item\n- a rating from 1 to 5 stars\n"}
{"snippet": "from sklearn.manifold import TSNE\nitem_tsne = TSNE(perplexity=30).fit_transform(item_embeddings)\n", "intent": "- we use scikit learn to visualize items embeddings\n- Try different perplexities, and visualize user embeddings as well\n- What can you conclude ?\n"}
{"snippet": "import pandas as pd\nall_ratings = pd.read_csv(op.join(ML_100K_FOLDER, 'u.data'), sep='\\t',\n                          names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\nall_ratings.head()\n", "intent": "Each line contains a rated movie: \n- a user\n- an item\n- a rating from 1 to 5 stars\n"}
{"snippet": "names = [\"name\", \"date\", \"genre\", \"url\"]\nnames += [\"f\" + str(x) for x in range(19)]  \nitems = pd.read_csv(op.join(ML_100K_FOLDER, 'u.item'), sep='|', encoding='latin-1',\n                    names=names)\nitems.fillna(value=\"01-Jan-1997\", inplace=True)\nitems.head()\n", "intent": "The item metadata file contains metadata like the name of the movie or the date it was released\n"}
{"snippet": "from sklearn.manifold import TSNE\nimg_emb_tsne = TSNE(perplexity=30).fit_transform(out_tensors)\n", "intent": "Let's find a 2D representation of that high dimensional feature space using T-SNE:\n"}
{"snippet": "from keras.utils.data_utils import get_file\nURL = \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\ncorpus_path = get_file('nietzsche.txt', origin=URL)\ntext = open(corpus_path).read().lower()\nprint('Corpus length: %d characters' % len(text))\n", "intent": "Let's use some publicly available philosopy:\n"}
{"snippet": "idx = 0\nwith open(filenames_train[idx], 'rb') as f:\n    print(\"class:\", target_names[target_train[idx]])\n    print()\n    print(f.read().decode('latin-1')[:500] + '...')\n", "intent": "Let's check that text of some document have been loaded correctly:\n"}
{"snippet": "texts_train = [open(fn, 'rb').read().decode('latin-1') for fn in filenames_train]\ntexts_test = [open(fn, 'rb').read().decode('latin-1') for fn in filenames_test]\n", "intent": "This dataset is small so we can preload it all in memory once and for all to simplify the notebook.\n"}
{"snippet": "ad_data=pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "PATH = 'data/dbpedia/'\nURL_BASE = 'http://downloads.dbpedia.org/3.5.1/en/'\nfilenames = [\"redirects_en.nt.bz2\", \"page_links_en.nt.bz2\"]\nfor filename in filenames:\n    if not os.path.exists(PATH+filename):\n        print(\"Downloading '%s', please wait...\" % filename)\n        open(PATH+filename, 'wb').write(urlopen(URL_BASE+filename).read())\n", "intent": "Note: this takes a while\n"}
{"snippet": "from sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, random_state=123)\nalphas, lambdas = stepwise_kpca(X, gamma=15, n_components=1)\n", "intent": "Now, let's make a new half-moon dataset and project it onto a 1-dimensonal subspace using the RBF kernel PCA:\n"}
{"snippet": "from sklearn.decomposition import PCA as sklearnPCA\nsklearn_pca = sklearnPCA(n_components=2)\nY_sklearn = sklearn_pca.fit_transform(X_std)\n", "intent": "For educational purposes, we went a long way to apply the PCA to the Iris dataset. But luckily, there is already implementation in scikit-learn. \n"}
{"snippet": "tfidf = TfidfTransformer(use_idf=True, smooth_idf=False, norm='l2')\ntfidf.fit_transform(tf).toarray()[-1]\n", "intent": "And finally, we compare the results to the results that the `TfidfTransformer` returns.\n"}
{"snippet": "tfidf = TfidfTransformer(use_idf=True, smooth_idf=True, norm=None)\ntfidf.fit_transform(tf).toarray()[-1][:3]\n", "intent": "To confirm that we understand the `smooth_idf` parameter correctly, let us walk through the 3-word example again:\n"}
{"snippet": "import pandas as pd\ndf = pd.DataFrame([\n            ['green', 'M', 10.1, 'class1'], \n            ['red', 'L', 13.5, 'class2'], \n            ['blue', 'XL', 15.3, 'class1']])\ndf.columns = ['color', 'size', 'prize', 'class label']\ndf\n", "intent": "First, let us create a simple example dataset with 3 different kinds of features:\n- color: nominal\n- size: ordinal\n- prize: continuous\n"}
{"snippet": "class_le.inverse_transform(df['class label'])\n", "intent": "The class labels can be converted back from integer to string via the `inverse_transform` method:\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndvec = DictVectorizer(sparse=False)\nX = dvec.fit_transform(df.transpose().to_dict().values())\nX\n", "intent": "Now, we can use the `DictVectorizer` to turn this\nmapping into a matrix:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3, \n                     stratify=y,\n                     random_state=0)\n", "intent": "Splitting the data into 70% training and 30% test subsets.\n"}
{"snippet": "train = pd.read_csv('titanic_train.csv')\ntest = pd.read_csv('titanic_test.csv')\n", "intent": "Let's start by reading in the titanic_train.csv file into a pandas dataframe.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\nlr = LogisticRegression()\nlr = lr.fit(X_train_pca, y_train)\n", "intent": "Training logistic regression classifier using the first 2 principal components.\n"}
{"snippet": "df.to_csv('movie_data.csv', index=False, encoding='utf-8')\n", "intent": "Optional: Saving the assembled data as CSV file:\n"}
{"snippet": "with open('./sckit-model-to-json/params.json', 'r', encoding='utf-8') as infile:\n    print(infile.read())\n", "intent": "When we read the file, we can see that the JSON file is just a 1-to-1 copy of our Python dictionary in text format:\n"}
{"snippet": "with open('./sckit-model-to-json/attributes.json', 'r', encoding='utf-8') as infile:\n    print(infile.read())\n", "intent": "If everything went fine, our JSON file should look like this -- in plaintext format:\n"}
{"snippet": "if Version(sklearn_version) < '0.18':\n    from sklearn.cross_validation import train_test_split\nelse:\n    from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0)\n", "intent": "Splitting data into 70% training and 30% test data:\n"}
{"snippet": "if Version(sklearn_version) < '0.18':\n    from sklearn.cross_validation import train_test_split\nelse:\n    from sklearn.model_selection import train_test_split\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3, random_state=0)\n", "intent": "Splitting the data into 70% training and 30% test subsets.\n"}
{"snippet": "df.to_csv('./movie_data.csv', index=False)\n", "intent": "Optional: Saving the assembled data as CSV file:\n"}
{"snippet": "X, y = make_hastie_10_2(n_samples=8000, random_state=42)\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\ngs = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                  param_grid={'min_samples_split': range(2, 403, 10)},\n                  scoring=scoring, cv=5, refit='AUC')\ngs.fit(X, y)\nresults = gs.cv_results_\n", "intent": "Running ``GridSearchCV`` using multiple evaluation metrics\n----------------------------------------------------------\n"}
{"snippet": "from IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()\n", "intent": "1. [The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3](http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/) by Thomas Wiecki\n"}
{"snippet": "train = pd.read_csv('titanic_train.csv')\n", "intent": "Let's start by reading in the titanic_train.csv file into a pandas dataframe.\n"}
{"snippet": "filenames = tf.train.match_filenames_once('./audio_dataset/*.wav')\ncount_num_files = tf.size(filenames)\nfilename_queue = tf.train.string_input_producer(filenames)\nreader = tf.WholeFileReader()\nfilename, file_contents = reader.read(filename_queue)\nchromo = tf.placeholder(tf.float32)\nmax_freqs = tf.argmax(chromo, 0)\n", "intent": "Select the location for the audio files:\n"}
{"snippet": "import pandas as pd\ndf = pd.DataFrame({'feature': x, 'label': y})\nprint(df)\n", "intent": "```pandas``` is the essential package for working with dataframes.  Make a convenient dataframe for using our feature to predict our label.\n"}
{"snippet": "all_X = all_X.fillna(all_X.mean())\n", "intent": "We can approximate the missing values by the mean values of the current feature.\n"}
{"snippet": "(doremi_sample_rate, doremi) = scipy.io.wavfile.read(\"audio_files/do-re-mi.wav\")\n", "intent": "* x-axis: time\n* y-axis: frequency\n* z-axis (color): strength of each frequency\n"}
{"snippet": "df = pd.read_csv(\"data/ebaytitles.csv\")\ndf = df.sample(frac=0.1) \ndf.head()\n", "intent": "Read csv file into a dataframe\n"}
{"snippet": "X = df.title.values\ny = df.category_name.values\nX_tr, X_te, y_tr, y_te = train_test_split(X, \n                                          y,\n                                          test_size=0.1,\n                                          random_state=0)\n", "intent": "Split the data into train and test observations - there is a column\n"}
{"snippet": "df = pd.read_csv(\"data/ebaytitles.csv\")\ndf = df.iloc[:100000,:]\ndf.head()\n", "intent": "Read csv file into a dataframe\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\nimputer = imputer.fit(X[:, 1:3])\nX[:, 1:3] = imputer.transform(X[:, 1:3])\n", "intent": "X is matrix of Features\nY dependent vector\n"}
{"snippet": "combined_data = pd.DataFrame()\ncombined_data = combined_data.append(titanic_train_org)\ncombined_data = combined_data.append(titanic_test_org)\ncombined_data.drop(['PassengerId'], axis=1, inplace=True)\ncombined_data.reset_index(drop=True, inplace=True)\ntrain_idx = len(titanic_train_org)\ntest_idx = len(combined_data) - len(titanic_test_org)\nprint('Combined dataset dimension: {} rows, {} columns'.format(combined_data.shape[0], combined_data.shape[1]))\ncombined_data.head()\n", "intent": "- Imputing null values\n- Transform categorical features\n- Identify new features based on existing features\n"}
{"snippet": "messages = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t',\n                           names=[\"label\", \"message\"])\nmessages.head()\n", "intent": "We'll use **read_csv** and make note of the **sep** argument, we can also specify the desired column names by passing in a list of *names*.\n"}
{"snippet": "numeric_feats = all_df.dtypes[all_df.dtypes != \"object\"].index\nskewed_feats = all_df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nsns.barplot(skewness.index,skewness.Skew);\n", "intent": "**Lets see the highly skewed features we have**\n"}
{"snippet": "regressor = LinearRegression()  \nregressor.fit(X_train, y_train)  \ncoeff_df = pd.DataFrame(regressor.coef_, X.columns, columns=['Coefficient'])  \ncoeff_df  \n", "intent": "Let's build & train the linear regressor model with mutliple features and label.\n"}
{"snippet": "films = { 'title': titles, 'movie_idx': movie_idx, 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\nframe = pd.DataFrame(films, index = [clusters] , columns = ['movie_idx', 'title', 'cluster', 'genre'])\nprint(frame['cluster'].value_counts()) \ngrouped = frame['movie_idx'].groupby(frame['cluster']) \nprint(grouped.mean()) \n", "intent": "Now, we can create a dictionary of titles, ranks, the synopsis, the cluster assignment, and the genre\n"}
{"snippet": "def compute_tf_idf(tf, idf):\n    tf_idf = dict.fromkeys(tf.keys(), 0)\n    for word, v in tf.items():\n        tf_idf[word] = v * idf[word]\n    return tf_idf\ntf_idf_A = compute_tf_idf(tf_A, idf)\ntf_idf_B = compute_tf_idf(tf_B, idf)\ntf_idf_C = compute_tf_idf(tf_C, idf)\nprint('TF-IDF bag of words:')\npd.DataFrame([tf_idf_A, tf_idf_B, tf_idf_C])\n", "intent": "Now, tf-idf is the product of tf to idf. For our python example, tf-idf is dictionary with the corresponding products.\n"}
{"snippet": "import pandas as pd\npath = '../data/yelp.csv'\nyelp = pd.read_csv(path)\n", "intent": "Read **`yelp.csv`** into a Pandas DataFrame and examine it.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\n", "intent": "Use CountVectorizer to create **document-term matrices** from X_train and X_test.\n"}
{"snippet": "import pandas as pd\npath = '../data/yelp.csv'\nyelp = pd.read_csv(path)\n", "intent": "- \"corpus\" = collection of documents\n- \"corpora\" = plural form of corpus\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape\n", "intent": "- **lowercase:** boolean, True by default\n    - Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\ntokenize_test(vect)\n", "intent": "**Approach 3:** Don't convert to lowercase\n"}
{"snippet": "yelp = pd.read_csv('yelp.csv')\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer()\nvect\n", "intent": "[TfidfVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values=1, strategy='median')\n", "intent": "[Imputer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html)\n"}
{"snippet": "origdf = df.merge(df_transactions, on='customer_name').merge(df_offers, on='offer_id')\nwinesdf = origdf[['cluster_id', 'varietal', 'past_peak']]\nwinesdf.pivot_table(index=['cluster_id'], columns=['varietal'], aggfunc='count')\n", "intent": "It looks like 3 is the best value for K, which was the same value suggested by the elbow method.\n"}
{"snippet": "df_d4_predicted = pd.read_csv(\n    os.path.join(data_path,'output_deg_4',\n    'bp-GDbDfIfQef8-fit_degree_4_example_test30.csv.gz'))\ndf_d4_predicted.columns = [\"Row\",\"y_predicted\"]\n", "intent": "<h4>Model with degree 4 features</h4>\n"}
{"snippet": "df_d15_predicted = pd.read_csv(\n    os.path.join(data_path,'output_deg_15',\n    'bp-wr0EvVL9UA5-fit_degree_15_example_test30.csv.gz'))\ndf_d15_predicted.columns = [\"Row\",\"y_predicted\"]\n", "intent": "<h4>Model with degree 15 features</h4>\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "Scikit-learn has a bunch of example datasets. Here, we'll use the iris dataset: which contains data about different species of plants. \n"}
{"snippet": "sales = pd.read_csv('/Users/kiefer/Desktop/Iowa_Liquor_Sales_reduced.csv')\n", "intent": "Load your data from project 3\n"}
{"snippet": "df = pd.DataFrame()\ndf['size'] = np.random.choice([\"big\", \"med\", \"large\", \"gigantic\"], size=500)\ndf['price'] = np.random.choice(np.arange(100, 1000, 1), size=500)\ndf['baths'] = np.random.choice(np.arange(1, 4, 0.5), size=500)\n", "intent": "With 2-3 continious variables, and one categorical.\n"}
{"snippet": "df = pd.DataFrame()\ndf['size'] = np.random.choice([\"big\", \"med\", \"large\", \"giantic\"], size=500)\ndf['baths'] = np.random.choice(np.arange(1, 4, 0.5), size=500)\n", "intent": "With 2-3 continious variables, and one categorical.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "bcw = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-2/datasets/breast_cancer_wisconsin/wdbc.data', \n                  header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "bcw = pd.read_csv('/Users/alex/Desktop/DSI-SF-2-akodate/datasets/breast_cancer_wisconsin/wdbc.data', \n                  header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nconfusion = pd.DataFrame(confmat, index=['is_over_200k', 'is_not_over_200k'],\n                         columns=['predicted_is_over_200k','predicted_is_not_over_200k'])\nprint(confusion)\n", "intent": "What do these mean?\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"../../../datasets/sacramento_real_estate/Sacramentorealestatetransactions.csv\")\n", "intent": "We did this in our previous lab.\n"}
{"snippet": "ss = StandardScaler()\nXn = ss.fit_transform(X)\nprint Xn.shape\n", "intent": "---\nAlways a necessary step when performing regularization.\n"}
{"snippet": "df = pd.read_csv('/Users/alex/Desktop/DSI-SF-2-akodate/datasets/breast_cancer_wisconsin/breast_cancer.csv', na_values='?')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "df = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-2/datasets/breast_cancer_wisconsin/breast_cancer.csv', na_values='?')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "grad_test = pd.read_csv('/Users/alex/Desktop/DSI-SF-2-akodate/datasets/data_for_diplomas/grad_test.csv')\n", "intent": ".\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "data = pd.read_csv('~/Desktop/DSI-SF-2/datasets/401_k_abadie/401ksubs.csv')\ndata.head(2)\n", "intent": "1. Read the data into Pandas.\n2. Explore the data by sorting, plotting, group_by, and any other ideas/techniques you have been using.\n"}
{"snippet": "X = cv.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "sstrain = StandardScaler()\nsstest = StandardScaler()\nlr_sep = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\nXtrain_n = sstrain.fit_transform(Xtrain)\nXtest_n = sstest.fit_transform(Xtest)\n", "intent": "For the sake of example, standardize the Xtrain and Xtest separately and show that their normalization parameters differ.\n"}
{"snippet": "Xn = ss.fit_transform(X)\n", "intent": "Import the LogisticRegression and StandardScaler classes.\n"}
{"snippet": "docs = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\ndocs\n", "intent": "What is being counted?\n_Warning: big ugly sparse matrix ahead._\n"}
{"snippet": "data = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-2/datasets/adult_salary/adult.csv')\n", "intent": "Gradient boosting vs. AdaBoost: 7\nHinge Loss: 2\nPipelines: 5\nFeature importances: 0\nLDA: 2\nNLP: 2\n"}
{"snippet": "csv_file = \"https://vincentarelbundock.github.io/Rdatasets/csv/cluster/pluton.csv\"\ndf = pd.read_csv(csv_file)\ndf.head(5)\n", "intent": "We have a nice [data dictionary](https://vincentarelbundock.github.io/Rdatasets/doc/cluster/pluton.html)\n"}
{"snippet": "stats_pcs = pd.DataFrame(stats_pcs, columns=['PC'+str(i) for i in range(1,8)])\nstats_pcs['athlete'] = hep.iloc[:,0]\nstats_pcs['score'] = hep.score\n", "intent": "---\nAdd back in the athelete and score columns from the original data.\n"}
{"snippet": "wine_pca = PCA().fit(wine_cont_n)\nwine_pcs = wine_pca.transform(wine_cont_n)\nwine_pcs = pd.DataFrame(wine_pcs, columns=['PC'+str(i) for i in range(1, wine_pcs.shape[1]+1)])\nwine_pcs['red_wine'] = wine.red_wine\n", "intent": "---\nCreate a new dataframe with the principal components and the `red_wine` column added back in from the original data.\n"}
{"snippet": "cv = CountVectorizer(ngram_range=(1,2), max_features=2500, binary=True, stop_words='english')\nwords = cv.fit_transform(rt.quote)\n", "intent": "---\nIt is up to you what ngram range you want to select. **Make sure that `binary=True`**\n"}
{"snippet": "Xtrain, Xtest, ytrain, ytest = train_test_split(words.values, rt.fresh.values, test_size=0.25)\n", "intent": "---\nYou should keep 25% of the data in the test set.\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "cvt = CountVectorizer(stop_words=\"english\", ngram_range=(2,3))\nmatrix = cvt.fit_transform(insults_df[\"Comment\"])\nfreqs = [(word, matrix.getcol(idx).sum()) for word, idx in cvt.vocabulary_.items()]\nprint sorted (freqs, key = lambda x: -x[1])[:75]\n", "intent": "Display the top 75 ngrams with frequencies.\n"}
{"snippet": "cvt = CountVectorizer()\nX_all = cvt.fit_transform(insults_df[\"Comment\"])\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "iowa_file = '/Users/alex/Desktop/DSI-SF-2-akodate/datasets/iowa_liquor/Iowa_Liquor_sales_sample_10pct.csv'\niowa = pd.read_csv(iowa_file)\n", "intent": "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n---\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size = 0.2, random_state = 7)\n", "intent": "Batch size of 6 was chosen!\n"}
{"snippet": "df_d4_predicted = pd.read_csv(\n    os.path.join(data_path,'output_deg_4',\n    'bp-W4oBOhwClbH-fit_degree_4_example_test30.csv.gz'))\ndf_d4_predicted.columns = [\"Row\",\"y_predicted\"]\n", "intent": "<h4>Model with degree 4 features</h4>\n"}
{"snippet": "df_d15_predicted = pd.read_csv(\n    os.path.join(data_path,'output_deg_15',\n    'bp-rBWxcnPN3zu-fit_degree_15_example_test30.csv.gz'))\ndf_d15_predicted.columns = [\"Row\",\"y_predicted\"]\n", "intent": "<h4>Model with degree 15 features</h4>\n"}
{"snippet": "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()\n", "intent": "Loading training and testing datasets.\n"}
{"snippet": "data = pd.read_csv(\"./data/monthly-milk-production.csv\", index_col = 'Month')\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "effect_size(pd.DataFrame([1, 2, 3, 4]), pd.DataFrame([3, 3, 1, 2]))\n", "intent": "It is calculated with delta degree of freedom = 1!\n"}
{"snippet": "dfDelayData = pd.read_csv('airlineData/flightDelays2.csv') \n", "intent": "Due to the size of my original dataset, I utilized a slice of that which included only the columns that I needed for my below predictions. \n"}
{"snippet": "from sklearn import preprocessing\nstd_scale = preprocessing.StandardScaler().fit(df[['Alcohol', 'Malic acid']])\ndf_std = std_scale.transform(df[['Alcohol', 'Malic acid']]) \n", "intent": "We use sklearn linrary for standardise data (mean=0, SD=1). \n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(x_cols)\nx_2 = pca.transform(x_cols)\ndf_x_2 = pd.DataFrame(x_2, columns=['C1','C2'])\ndf_x_2.head()\n", "intent": "Use PCA  for dimension reduction to 2 components\n"}
{"snippet": "flights_df['carrier_flight_dest'] = flights_df.carrier + flights_df.flight.astype(str) + \"  \"\\\n                                + flights_df.dest\ntotal = pd.DataFrame(flights_df.groupby('carrier_flight_dest').count())\ntotal.reset_index(level=0, inplace=True)\nprint(total.head())\nflights_365 = total[total['year']==365]['carrier_flight_dest']\nprint(flights_365)\n", "intent": "Which flights (i.e. carrier + flight + dest) happen every day? Where do they fly to?\n"}
{"snippet": "weather_df = pd.read_csv('../Problem_Set_1/weather.csv.bz2')\nweather_df.head()\n", "intent": "What weather conditions are associated with flight delays leaving NYC? Use graphics to explore.\n"}
{"snippet": "flights_df3=pd.DataFrame()\nflights_df3['count_delayed']=flights_df.groupby('date').apply( lambda x:x[x['dep_delay']>0]['dep_delay'].count())\nprint flights_df3.sort(columns='count_delayed',ascending=False).head(n=10)\nflights_df3_temp=flights_df3.reset_index()\n", "intent": "(b) What was the worst day to fly out of NYC in 2013 if you dislike delayed flights?\n"}
{"snippet": "flights_df4=pd.DataFrame()\nflights_df4['avg_dep_delay']=flights_df1.groupby('month')['dep_delay'].mean()\nprint flights_df4.shape\nflights_df4\n", "intent": "(c) Are there any seasonal patterns in departure delays for flights from NYC?\n"}
{"snippet": "flights_df5=pd.DataFrame()\nflights_df5['avg_dep_delay']=flights_df1.groupby('hour')['dep_delay'].mean()\nprint flights_df5.shape\nflights_df5\n", "intent": "(d) On average, how do departure delays vary over the course of a day?\n"}
{"snippet": "flights_df6=pd.DataFrame()\nflights_df6=flights_df.groupby(['carrier','flight','dest']).size().reset_index()\nflights_df6.columns=['carrier','flight','dest','count']\nflights_df6\nprint flights_df6[flights_df6['count']==365]\n", "intent": "Which flights (i.e. carrier + flight + dest) happen every day? Where do they fly to?\n"}
{"snippet": "flights_df= pd.read_csv('C:\\\\Users\\\\root\\\\Desktop\\\\Winter2015\\\\CoreMethods\\\\PS1\\\\flights.csv')\nweather_df= pd.read_csv('C:\\\\Users\\\\root\\\\Desktop\\\\Winter2015\\\\CoreMethods\\\\PS1\\\\weather.csv')\n", "intent": "What weather conditions are associated with flight delays leaving NYC? Use graphics to explore.\n"}
{"snippet": "df08 = pd.read_csv('airlineData/DelayedFlights.csv') \n", "intent": "In the section below, I am looking at the impact of \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfeatures = [item for item in data.columns[:-1]]\ndata = data.replace({'g':1, 'b': 0})\nX = data[features]\ny = data[\"target\"]\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nlen(x_train)\n", "intent": "In order to check the effect of regularization on test data, split the data into train and test using sklearn.\n"}
{"snippet": "print(\"TFIDFVectorizer is being applied...\")\ntfidf_vectorizer = TfidfVectorizer(stop_words='english')\ntfidf_train = tfidf_vectorizer.fit_transform(X_train_1)\nprint(X_train_1.shape)\nprint(tfidf_train.shape)\ntfidf_test = tfidf_vectorizer.transform(X_test_1)\n", "intent": "**Create Vectorizers, I will be using only TFIDFVectorizer.**\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = random_state)\nnn.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)\n", "intent": "**Training the model 1**\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = random_state)\nnn.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)\n", "intent": "**Training the model 2**\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = random_state)\nnn.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)\n", "intent": "**Training the model 3**\n"}
{"snippet": "df = pd.read_csv('/Users/Carsten/GitRepos/NLP-LAB/Carsten_Solutions/sets/fact checking/fake_or_real_news.csv')\n", "intent": "Read in File fake_or_real_news.csv\n"}
{"snippet": "bin_count_vectorizer = CountVectorizer(stop_words='english')\nbin_count_train = bin_count_vectorizer.fit_transform(bin_X_train)\nbin_count_test = bin_count_vectorizer.transform(bin_X_test)\n", "intent": "generate two different vectorizers\n"}
{"snippet": "mul_count_vectorizer = CountVectorizer(stop_words='english')\nmul_count_train = mul_count_vectorizer.fit_transform(mul_X_train)\nmul_count_test = mul_count_vectorizer.transform(mul_X_test)\nmul_count_valid = mul_count_vectorizer.transform(mul_X_valid)\n", "intent": "* same again\n* only additional transform for validation set\n"}
{"snippet": "concat_count_vectorizer = CountVectorizer(stop_words='english')\nconcat_count_train = concat_count_vectorizer.fit_transform(concat_X_train)\nconcat_count_test = concat_count_vectorizer.transform(concat_X_test)\n", "intent": "* again the same ...\n"}
{"snippet": "func = lambda x: 2 + 0.5 * x + 3 * x ** 2 + 5 * stats.norm.rvs(0, 10)\ndf = pd.DataFrame()\ndf[\"x\"] = list(range(0, 30))\ndf[\"y\"] = map(func, df[\"x\"])\ndf.plot.scatter(x='x', y='y')\n", "intent": "Next we look at a data set that needs a quadratic fit. Let's do both a linear and quadratic fit and compare.\n"}
{"snippet": "size=10000\nX, y = transform_to_dataset(training_sentences)\nclf = Pipeline([\n    ('vectorizer', DictVectorizer(sparse=False)),\n    ('classifier', LogisticRegression())\n])\nclf.fit(X[:size], y[:size])\nprint('training OK')\nX_test, y_test = transform_to_dataset(test_sentences)\nperformances['performancesTreebank'].update({'Classifier': clf.score(X_test, y_test)})\n", "intent": "<h4>Task 1.1: Train classifier with the extracted features on the treebank corpus</h4>\n"}
{"snippet": "size=10000\nX, y = transform_to_dataset(training_sentences)\nclf = Pipeline([\n    ('vectorizer', DictVectorizer(sparse=False)),\n    ('classifier', LogisticRegression())\n])\nclf.fit(X[:size], y[:size])\nprint('training OK')\nX_test, y_test = transform_to_dataset(test_sentences)\nperformancesAlpino.update({'Classifier': clf.score(X_test, y_test)})\n", "intent": "<h4>Task 2.1: Train classifier on the extracted features</h4>\n"}
{"snippet": "ds1 = pd.read_csv('../fake_or_real_news.csv', sep=',', usecols=['title','text','label'])\nds1['claim'] = ds1[['title', 'text']].apply(lambda x: '. '.join(x), axis=1)\ndel ds1['title']\ndel ds1['text']\nds1.rename(index=str, columns={'label': 'y'}, inplace=True)\nds1['y'] = np.where(ds1['y'] == 'FAKE', 'false', 'true')\nds1.head()\n", "intent": "<h4>Read and preprocess datasets</h4>\n"}
{"snippet": "df = pd.read_csv('./task2_datasets1_2/fake_or_real_news.csv')\n", "intent": "only FAKE and REAL labels\n"}
{"snippet": "path='./task2_datasets1_2/liar_liar_paints_on_fire/liar_dataset/'\nheaders=['id','label','statement','subject','speaker','speakerstitle','stateinfo','party','barely-true','false','half-true','mostly-true','pants-on-fire','context']\nds2_train_df = pd.read_csv(path+'train.tsv',sep='\\t', names=headers)\nds2_test_df = pd.read_csv(path+'test.tsv',sep='\\t', names=headers)\nds2_valid_df = pd.read_csv(path+'valid.tsv',sep='\\t',names=headers)\n", "intent": "labels of a different range: pants-fire,barely-\ntrue,  false, half-true, mostly-true, and true\n"}
{"snippet": "count_vector = TfidfVectorizer(stop_words='english')\ncount_vector.fit_transform(df['text'])\nds1_x = count_vector.transform(df['text'])\nds1_y = df.label\n", "intent": "Clf1: Simple MultonomialNB after this [article](https://github.com/kjam/random_hackery/blob/master/Attempting%20to%20detect%20fake%20news.ipynb)\n"}
{"snippet": "print(length)\nsteps = ['original', 'dropped NA 1', 'dropped NA 2', 'dropped too short paragrphs', 'merged overlapping paragraphs']\ndataframe = pd.DataFrame(\n    {'length': length,\n     'steps': steps}, \n    index = steps)\ndataframe.plot(kind='bar');\n", "intent": "The plot shows that there was a considerable amount of NAs as well as a considerable amount of overlapping paragraphs.\n"}
{"snippet": "b = pd.DataFrame()\nfor att in X:\n    a = []\n    a = hypothesisTest(df, att, 'TREAT')\n    b = pd.concat([b,a],0)\nb\n", "intent": "We can carry out t-tests to evaluate whether these means are statistically distinguishable:\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.keys()\n", "intent": "We previously saw the **hand-written digits** data. Let's use that here to test the efficacy of the SVM and Random Forest classifiers.\n"}
{"snippet": "sub.to_csv('submissions/lgbm_{:.5f}_{:.5f}.csv'.format(val_mean, val_std), index=False)\n", "intent": "Now let's save the submission. I like annotating the submission with the local score to compare it with the leaderboard score.\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nknn_pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\nknn_params = {'knn__n_neighbors': range(1, 10)}\nknn_grid = GridSearchCV(knn_pipe, knn_params,\ncv=5, n_jobs=-1,\nverbose=True)\nknn_grid.fit(X_train, y_train)\nknn_grid.best_params_, knn_grid.best_score_\n", "intent": "Now, let's tune the number of neighbors $k$ for k-NN:\n"}
{"snippet": "time_df = pd.DataFrame(index=train_df.index)\ntime_df['target'] = train_df['target']\ntime_df['min'] = train_df[times].min(axis=1)\ntime_df['max'] = train_df[times].max(axis=1)\ntime_df['seconds'] = (time_df['max'] - time_df['min']) / np.timedelta64(1, 's')\ntime_df.head()\n", "intent": "Now, let us look at the timestamps and try to characterize sessions by timeframes:\n"}
{"snippet": "def fill_nan(table):\n    for col in table.columns:\n        table[col] = table[col].fillna(table[col].median())\n    return table   \n", "intent": "Let's write the function that will replace *NaN* values with a median for each column.\n"}
{"snippet": "PATH_TO_DATA = ('../../data')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'websites_train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'websites_test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "train_target = pd.read_csv('../data/train_log1p_recommends.csv', \n                           index_col='id')\ny_train = train_target['log_recommends'].values\n", "intent": "**Read train target and split data for validation.**\n"}
{"snippet": "PATH_TO_DATA = ('../../../data')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "from sklearn import datasets\ncancer = datasets.load_breast_cancer()\nX_can = cancer.data\ny_can = cancer.target\nprint(X_can.shape)\nprint(y_can.shape)\n", "intent": "Let's try to import sklearn and check some available datasets. \n"}
{"snippet": "pca16 = PCA(n_components=16).fit(X_dig)\nX_reduced16 =pca16.transform(X_dig)\nprint(X_reduced16.shape)\n", "intent": "Let's try to repeat K-means analysis taking 16 dimensions/components:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfmri_tr, fmri_ts, cond_tr, cond_ts = train_test_split(fmri_masked_2lb, conditions_2lb)\nsvc.fit(fmri_tr, cond_tr)\n", "intent": "Let's split our data and fit the model using the training set:\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"e:/sundog-consult/udemy/datascience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "from sklearn.decomposition import PCA\niris = datasets.load_iris()\nX_ir = iris.data\ny_ir = iris.target\nn_components = 2\npca = PCA(n_components=n_components)\nX_pca = pca.fit_transform(X_ir)\n", "intent": "Let's recreate X_pca from our previous notebook:\n"}
{"snippet": "X, y = get_dataset2(shift=0)\ndf = pd.DataFrame(np.hstack((X, y[:, np.newaxis])), columns=['x0', 'x1', 'x2', 'y'])\nsns.pairplot(df, hue='y', vars=['x0', 'x1', 'x2'])\n", "intent": "Let us make the last component fully irrelevant and try a L1-based method again\n"}
{"snippet": "from sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np\niris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf.head()\n", "intent": "In this example we will use dataset and the random forest classifer from Scikit-learn.\nExample from: Chris Albon: https://github.com/chrisalbon/notes\n"}
{"snippet": "data.Age = ____\ndata.Fare = ____\ndata['Embarked'] = data['Embarked'].fillna('S')\ndata.info()\n", "intent": "* Impute missing values:\n"}
{"snippet": "df_data = pd.read_csv(os.path.join(workspace_path, 'data/census.csv'), dtype=str)\nprint '%d rows' % len(df_data)\ndf_data.head()\n", "intent": "Its a good idea to load data and inspect it to build an understanding of the structure, as well as preparation steps that will be needed.\n"}
{"snippet": "import pandas as pd\nadver = pd.read_csv('data/adver.csv', index_col=0)\nadver.head()\n", "intent": "<p><a name=\"ex2\"></a></p>\nThe Advertising data includes Sales vs TV, Radio and Newspaper.\nWe start with importing the data and visualizing it.\n"}
{"snippet": "colnames = df.columns\nresult = pd.DataFrame(ols.coef_).transpose()\nresult.columns = colnames.tolist()\nresult['intercept'] = ols.intercept_ \nresult = result.transpose()\nresult.columns = ['coefficient']\nresult\n", "intent": "Below we list the coefficients for each variable.\n"}
{"snippet": "path_to_file = \"./data/pml2tumor.csv\"\ndata = pd.read_csv(path_to_file)\nx_tm = data[[\"Size\"]]\ny_tm = data[\"Malignant\"]\nx_tm2 = np.copy(x_tm)\nx_tm2[-3,0] = 13\nx_tm2[-1,0] = 14\n", "intent": "We demonstrate **LDA** in `sklearn` with the tumor data.\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\npair = (2,3)\nxlabel = iris.feature_names[pair[0]]\nylabel = iris.feature_names[pair[1]]\niris_x = iris.data[:,pair]\niris_y = iris.target\n", "intent": "<p><a name=\"ex3\"></a></p>\nWe will work on the `iris` data\n"}
{"snippet": "X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\nclf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)   \n", "intent": "A single train/test split is made easy with the train_test_split function in the cross_validation library:\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = pd.read_csv('data/data.csv')\nprint data.head()\nX = np.array(data[['x1', 'x2', 'x3', 'x4', 'x5', 'x6']])\ny = np.array(data[['y']])\n", "intent": "<p><a name=\"ex3\"></a></p>\nRun the follwing code to obatian the data:\n"}
{"snippet": "best2 = fs.SelectKBest(fs.chi2, k=2).fit_transform(iris.data, iris.target)\nbest2.shape\n", "intent": "We use the function **SelectKBest** to select the best 2 features using the chi-square test:\n"}
{"snippet": "percent80 = fs.SelectPercentile(fs.chi2, 80).fit_transform(iris.data, iris.target) \npercent80.shape\n", "intent": "We can see here: compared to the original predictors, the selected features are the last two features.\n"}
{"snippet": "import numpy as np\nnp.random.seed(1)\nm = 100\nx = np.random.randn(m) \ny = x + 2 * x**2 - 3 * x**3 + np.random.randn(m)\nX = np.column_stack((x, x**2, x**3, x**4, x**5, x**6))\ndata = pd.DataFrame(np.column_stack((X, y)))\ndata.columns = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'y']\ndata.to_csv('data/data_new.csv', index=False)\n", "intent": "** Appendix ** Here is how the data is generated:\n"}
{"snippet": "import numpy as np\nfrom sklearn import datasets\niris = datasets.load_iris()\nindex = range(100)\niris.x = iris.data[index, :]\niris.y = iris.target[index]\nsvm_model.fit(iris.x[:, 0:2], iris.y)\n", "intent": "<p><a name=\"1case1\"></a></p>\nWe demonstrate the `svm` model on the first 100 observation of the iris data, which include two species.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ntrain = pd.read_csv('data/spam_train.csv')\ntest = pd.read_csv('data/spam_test.csv')\nx_train = np.array(train.iloc[:, 0:57])\ny_train = np.ravel(train.iloc[:, -1])\nx_test = np.array(test.iloc[:, 0:57])\ny_test = np.ravel(test.iloc[:, -1])\n", "intent": "<p><a name=\"2case2\"></a></p>\nIn this case we try to fit a decision tree on the spam data set.\n"}
{"snippet": "import pandas as pd\nimport sklearn.cross_validation as cv\nspam_train = pd.read_csv('./data/spam_train.csv')\nspam_test = pd.read_csv('./data/spam_test.csv')\nx_train = np.array(spam_train.iloc[:, :57])\ny_train = np.array(spam_train.iloc[:, -1])\nx_test = np.array(spam_test.iloc[:, :57])\ny_test = np.array(spam_test.iloc[:, -1])\n", "intent": "<p><a name=\"3case2\"></a></p>\nIn this case we fit a random forest on the spam data. We first prepare the data:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA()\niris.data2 = pca.set_params(n_components = 2).fit_transform(iris.data)\n", "intent": "We can also fit the principal components to the KMeans algorithm. \n- First transform the data to principal components\n"}
{"snippet": "from sklearn import datasets\niris= datasets.load_iris()\nhier.set_params(n_clusters = 3)\nhier.fit(iris.data)\n", "intent": "<p><a name=\"hcase1\"></a></p>\nLet's try to fit the hierarchical clustering model with the iris data.\n"}
{"snippet": "with open('anna.txt', 'r') as f:\n    text=f.read()\nvocab = set(text)\nvocab_to_int = {c: i for i, c in enumerate(vocab)}\nint_to_vocab = dict(enumerate(vocab))\nchars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n", "intent": "First we'll load the text file and convert it into integers for our network to use.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA()\n", "intent": "<p><a name=\"pca-sklearn\"></a></p>\nIn order to implement PCA in Python, import the class `PCA` from *sklearn.decomposition*.\n"}
{"snippet": "pca.set_params(n_components = 2)\niris.data2 = pca.fit_transform(iris.data)\n", "intent": "- Fit `pca` with the data:  \n"}
{"snippet": "dataframe  = pd.read_csv('../data/projects.csv')\nprint dataframe.shape[0]\n", "intent": "First, check the size of the datasets:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.20, random_state=17)\n", "intent": "Split the data into a training set and a test set.\n"}
{"snippet": "if os.path.exists('data_september_2015.csv'): \n    data = pd.read_csv('data_september_2015.csv')\nelse: \n    url = \"https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2015-09.csv\"\n    data = pd.read_csv(url)\n    data.to_csv(url.split('/')[-1])\nprint \"Number of rows:\", data.shape[0]\nprint \"Number of columns: \", data.shape[1]\n", "intent": "***Let's first download the dataset and print out the its size***\n"}
{"snippet": "colleges = pd.read_csv('College_Data')\ncolleges.set_index('Unnamed: 0', inplace=True)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "secret = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaled_df = pd.DataFrame(data=scaled_features, columns = secret.columns[:-1])\nscaled_df.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df = pd.read_table('data/winequality-red.csv', sep=';')\nfeatures = df.columns - ['quality']\ntarget = 'quality'\n", "intent": "We are going to briefly revisit the dataset used for the \"wine regression\" exercise to show the built-in parallel processing capabilities of sklearn.\n"}
{"snippet": "csv_url = \"http://www.stat.berkeley.edu/~ledell/data/eeg_eyestate_splits.csv\"\ndata = pd.read_csv(csv_url)\n", "intent": "Let's import the same dataset directly with pandas\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(\n    stop_words='english', lowercase=True, binary=False, min_df=0.01)\nfeatures_train = vectorizer.fit_transform(reviews_train)\n", "intent": "To extract features from the reviews, we are going to use word counts after normalizing to lowercase and removing stopwords.\n"}
{"snippet": "from sklearn import preprocessing\nscaler = preprocessing.StandardScaler()\nscaler.fit(x_train)\nx_test = scaler.transform(x_test)\nx_train = scaler.transform(x_train)\n", "intent": "Now we're ready to put together our regression workflow. Let's start by rescaling the data set so that all variables have the same range.\n"}
{"snippet": "daily_vol_df = ticker_daily_vol.to_pandas_dataframe()\ndaily_vol_df.plot()\n", "intent": "**Plot daily volatility in stocks over time.**\n"}
{"snippet": "most_volatile_on_ave_stocks_set = set([x[0] for x in most_volatile_on_ave_stocks])\nticker_pandas_df = ticker_daily_vol.filter(lambda x: x[0] in most_volatile_on_ave_stocks_set) \\\n    .to_pandas_dataframe()\nticker_pandas_df.plot()\n", "intent": "** Plot stocks with the highest average daily volatility over time. **\n"}
{"snippet": "from io import StringIO  \nmovie_txt = requests.get('https://????/movies.dat').text\nmovie_file = StringIO(movie_txt) \nmovies = pd.read_csv(movie_file, delimiter='\\t')\nmovies\n", "intent": "Here's a chunk of the MovieLens Dataset:\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ntwenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\nprint type(twenty_train)\ntwenty_train['data']\n", "intent": "We can now load the list of files matching those categories as follows:\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(['Data programming is cool', 'If you and only you pass the course!'])\ncount_vect.get_feature_names()\n", "intent": "Now back to our text data example\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ntwenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\ntype(twenty_train.data[0])\n", "intent": "We can now load the list of files matching those categories as follows:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = store_spend[['APR_SPEND', 'MAY_SPEND']]\ny = store_spend['JUNE_SPEND']\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n", "intent": "First, separate the response and the features into their own NumPy arrays.\n"}
{"snippet": "pca = H2OPCA(k=5)\npca.fit(X_train_norm)\nX_train_norm_pca = pca.transform(X_train_norm)\nX_test_norm_pca  = pca.transform(X_test_norm)\n", "intent": "Then, we can apply PCA and keep the top 5 components. A user warning is expected here.\n"}
{"snippet": "pd.DataFrame(train.groupby('gender').stroke.value_counts())\n", "intent": "** Some numerical EDAs to quench my curiosity**\n"}
{"snippet": "electoral_votes = pd.read_csv(\"data/electoral_votes.csv\").set_index('State')\nelectoral_votes.head(5)\n", "intent": "*As a matter of convention, we will index all our dataframes by the state name*\n"}
{"snippet": "from io import StringIO  \nmovie_txt = requests.get('https://raw.github.com/cs109/cs109_data/master/movies.dat').text\nmovie_file = StringIO(movie_txt) \nmovies = pd.read_csv(movie_file, delimiter='\\t')\nprint movies[['id', 'title', 'imdbID', 'year']].iloc[0]\nmovies.head(3)\n", "intent": "Here's a chunk of the MovieLens Dataset:\n"}
{"snippet": "from io import StringIO  \nmovie_txt = requests.get('https://raw.github.com/cs109/cs109_data/master/movies.dat').text\nmovie_file = StringIO(movie_txt) \nmovies = pd.read_csv(movie_file, delimiter='\\t')\nprint movies[['id', 'title', 'imdbID', 'year']].iloc[0]\nmovies\n", "intent": "Here's a chunk of the MovieLens Dataset:\n"}
{"snippet": "im = io.imread('../images/chapel_floor.png')\nim_lab = color.rgb2lab(im)\ndata = np.array([im_lab[..., 0].ravel(),\n                 im_lab[..., 1].ravel(),\n                 im_lab[..., 2].ravel()])\nkmeans = KMeans(n_clusters=4, random_state=0).fit(data.T)\nsegmentation = kmeans.labels_.reshape(im.shape[:-1])\n", "intent": "Of course we can generalize this method to more than two clusters.\n"}
{"snippet": "with open(model_location, 'rb') as f:\n    ciphered_model = f.read()\n", "intent": "We will now read the encrypted model from the file Flor has stored it in:\n"}
{"snippet": "data = None\nHTML(open(\"input.html\").read())\n", "intent": "**Exercise**: Try to write a digit into the box below. This will automatically save your input in a variable `data` behind the scenes.\n"}
{"snippet": "with open('text8') as f:\n    words = f.read().split()\nn_words = len(words)\nprint('Number of words in the dataset: {}'.format(n_words))\n", "intent": "Read the dataset and split it into words\n"}
{"snippet": "def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\nmatrix = fill_na(matrix)\n", "intent": "Producing lags brings a lot of nulls.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\nclf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)   \n", "intent": "A single train/test split is made easy with the train_test_split function in the cross_validation library:\n"}
{"snippet": "sales = pd.read_csv('../readonly/final_project_data/sales_train.csv.gz')\nshops = pd.read_csv('../readonly/final_project_data/shops.csv')\nitems = pd.read_csv('../readonly/final_project_data/items.csv')\nitem_cats = pd.read_csv('../readonly/final_project_data/item_categories.csv')\n", "intent": "Let's load the data from the hard drive first.\n"}
{"snippet": "y_val_pred_inversed = mlb.inverse_transform(y_val_predicted_labels_tfidf)\ny_val_inversed = mlb.inverse_transform(y_val)\nfor i in range(3):\n    print('Title:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n        X_val[i],\n        ','.join(y_val_inversed[i]),\n        ','.join(y_val_pred_inversed[i])\n    ))\n", "intent": "Now take a look at how classifier, which uses TF-IDF, works for a few examples:\n"}
{"snippet": "sample_size = 200000\ndialogue_df = pd.read_csv('data/dialogues.tsv', sep='\\t').sample(sample_size, random_state=0)\nstackoverflow_df = pd.read_csv('data/tagged_posts.tsv', sep='\\t').sample(sample_size, random_state=0)\n", "intent": "Now, load examples of two classes. Use a subsample of stackoverflow data to balance the classes. You will need the full data later.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"https://www.dropbox.com/s/1k9cgsd7bzce0yk/housing-data.csv?dl=1\")\nprint df\n", "intent": "https://www.dropbox.com/s/1k9cgsd7bzce0yk/housing-data.csv?dl=1\n"}
{"snippet": "cvec = CountVectorizer()\ncvec.fit(data_train['data'])\n", "intent": "Initialize a standard CountVectorizer and fit the training data\n"}
{"snippet": "dfv = pd.read_csv('../assets/datasets/votes.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "xStand = StandardScaler().fit_transform(x)\n", "intent": "use a map to get the values into 0 and 1.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"/Users/Debjani/Downloads/IMDB.csv\")\n", "intent": "Source of the data: [IMBD movies data](https://dpalit.github.io/)\n"}
{"snippet": "data = pd.read_csv('./bank-additional/bank-additional-full.csv', sep=';')\npd.set_option('display.max_columns', 500)     \npd.set_option('display.max_rows', 20)         \ndata\n", "intent": "Now lets read this into a Pandas data frame and take a look.\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nyencode = LabelEncoder().fit(dataset.target)\ncensus = Pipeline([\n        ('encoder',  EncodeCategorical(dataset.categorical_features.keys())),\n        ('imputer', ImputeCategorical(['GRINST','MIGMTR3','MIGMTR4','MIGSAME','NOEMP','PEMNTVTY','PENATVTY','PRCITSHP'])),\n        ('classifier', SVC())\n    ])\ncensus.fit(dataset.data, yencode.transform(dataset.target))\n", "intent": "Model comparison using Support Vector Machine\n"}
{"snippet": "pca = PCA(n_components=2)\n", "intent": "Create a new PCA-object and set the target array-length to 2.\n"}
{"snippet": "transfer_values_reduced = pca.fit_transform(transfer_values)\n", "intent": "Use PCA to reduce the transfer-value arrays from 2048 to 2 elements.\n"}
{"snippet": "log_q_values.read()\nlog_reward.read()\n", "intent": "We can now read the logs from file:\n"}
{"snippet": "continuous_feature_assembler = VectorAssembler(inputCols=continuous_features, outputCol=\"unscaled_continuous_features\")\ncontinuous_feature_scaler = StandardScaler(inputCol=\"unscaled_continuous_features\", outputCol=\"scaled_continuous_features\", \\\n                                           withStd=True, withMean=False)\n", "intent": "**Step 4**: Continous Feature Pipeline\n"}
{"snippet": "categorical_feature_indexers = [StringIndexer(inputCol=x, \\\n                                              outputCol=\"{}_index\".format(x)) \\\n                                for x in categorical_features]\ncategorical_feature_one_hot_encoders = [OneHotEncoder(inputCol=x.getOutputCol(), \\\n                                                      outputCol=\"oh_encoder_{}\".format(x.getOutputCol() )) \\\n                                        for x in categorical_feature_indexers]\n", "intent": "**Step 5**: Categorical Feature Pipeline\n"}
{"snippet": "n_samples = 2000\nrandom_state=42\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05, random_state=random_state)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05, random_state=random_state)\nblobs = datasets.make_blobs(n_samples=n_samples, centers=2, random_state=random_state)\n", "intent": "Generate three different datasets, with two classes\n"}
{"snippet": "def get_distinct_values(column_name):\n  return bq.Query(sql).execute().result().to_dataframe()\n", "intent": "Let's write a query to find the unique values for each of the columns and the count of those values.\n"}
{"snippet": "CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\nFEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS)-1]\nTARGET = CSV_COLUMNS[0]\ndf_train = pd.read_csv('../datasets/taxi-train.csv', header=None, names=CSV_COLUMNS)\ndf_valid = pd.read_csv('../datasets/taxi-valid.csv', header=None, names=CSV_COLUMNS)\n", "intent": "Read data created in the previous chapter.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\n_, itest = train_test_split(range(critics.shape[0]), train_size=0.7,random_state=42)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itest] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "scaler = StandardScaler()\nscaler.fit(X_train)\nX_scaled_train = scaler.transform(X_train)\nX_scaled_train = pd.DataFrame(X_scaled_train)\nX_scaled_train.columns = list(df.columns)[1:]\ndf3 = X_scaled_train.join(y_train.reset_index())\n", "intent": "`2.` Now, use [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n"}
{"snippet": "student_data = pd.read_csv(\"student-data.csv\")\ntarget = student_data['passed']\nprint \"Student data read successfully!\"\ndata = pd.read_csv('student-data.csv', header=0)\noutcome_passed = data['passed']\noutcome_passed_num = outcome_passed.replace(['yes', 'no'], [1,0])\nfeatures = data.drop('passed', axis = 1)\nfeatures_num = preprocess_features(features)\n", "intent": "2) Load the student data from a CSV file and save as new dataframe @student_data\n"}
{"snippet": "user_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\nusers = pd.read_table('data/u.user', sep='|', header=None, names=user_cols)\n", "intent": "Documentation for [**`read_table`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html)\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX = representatives[range(1,17)]\nY = representatives[0]\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=.30, random_state=4444)\n", "intent": "Split the data into a test and training set. But this time, use this function: from sklearn.cross_validation import train_test_split\n"}
{"snippet": "movies = pd.read_csv('2013_movies.csv')\n", "intent": "For your movie classifiers, calculate the precision and recall for each class.\n"}
{"snippet": "movies = pd.read_csv('../../challenges_data/2013_movies.csv')\n", "intent": "Fit and evaluate a decision tree classifier for your movie dataset. Examine the rules your tree uses.\n"}
{"snippet": "movie=pd.read_csv('../../../challenges/challenges_data/2013_movies.csv')\nmovie=movie.dropna()\nmovie.describe()\n", "intent": "The error term must have constant variance\n"}
{"snippet": "max_features = 2000\n(X_train, y_train), (X_test, y_test) = reuters.load_data(\n    num_words=max_features)\nmaxlen = 10\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nnb_epoch = 20\n", "intent": "- reuter newswire ~ loads the Reuters newswire classification dataset\n- 46 classes\n"}
{"snippet": "from keras.datasets import cifar10\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n", "intent": "- The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes\n<img src ='../imgs/cifar.png'/>\n"}
{"snippet": "max_features = 2000\n(X_train, y_train), (X_test, y_test) = reuters.load_data(\n    num_words=max_features)\nmaxlen = 10\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nnb_epoch = 20\n", "intent": "- We'll work with reuter newswire to classification dataset.   \n- 46 classes\n- We will model this using RNNs, but first let's try with an ANN. \n"}
{"snippet": "train_x, train_y, test_x, test_y = load_data(\"../data/iris.data\")\n", "intent": "Note that we need the data in a different format. Each instance is a list of values (not a dict, as before).\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ntext = ['That is should come to this!', 'This above all: to thine own self be true.', 'Something is rotten in the state of Denmark.']\nvectorizer = CountVectorizer(ngram_range=(1,2))\nvectorizer.fit(text)\nprint(vectorizer.get_feature_names())\nx = vectorizer.transform(text)\n", "intent": "CountVectorizer:  Convert a collection of text documents to a matrix of token counts\nThis implementation produces a sparse representation.\n"}
{"snippet": "pd.DataFrame(dtm_lsa.round(5), index = example, columns = [\"component_1\",\"component_2\" ])\n", "intent": "Each document is a linear combination of the LSA components\n"}
{"snippet": "similarity = np.asarray(numpy.asmatrix(dtm_lsa) * numpy.asmatrix(dtm_lsa).T) \npd.DataFrame(similarity.round(6),index=(range(7)), columns=(range(7))).head(10)\n", "intent": "Document similarity using LSA\n"}
{"snippet": "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball']\nng_train = datasets.fetch_20newsgroups(subset='train', categories=categories, \n                                      remove=('headers', 'footers', 'quotes'))\n", "intent": "Let's retain only a subset of the 20 categories in the original 20 Newsgroups Dataset.\n"}
{"snippet": "ratings = pd.read_table('~/data/movielens/ratings.dat', sep='::', names= ['UserID','MovieID','Rating','Timestamp'])\n", "intent": "Load the ratings.dat data into a `ratings` variable with the same separator, and the column names UserID, MovieID, Rating, Timestamp.\n"}
{"snippet": "maxlen=100\nX_train=sequence.pad_sequences(X_train,maxlen=maxlen)\nX_test=sequence.pad_sequences(X_test,maxlen=maxlen)\n", "intent": "**Overfit?** Yes, but perhaps even more importantly: our loss rate \nis increasing     \nTry again\n[ref](http://cs231n.github.io/neural-networks-3/)\n"}
{"snippet": "from sklearn.feature_selection import RFE\nselect = RFE(LinearRegression(n_jobs=-1), n_features_to_select=4)\nselect.fit(X_train, y_train)\nX_train_rfe= select.transform(X_train)\nprint('X_train.shape: {}'.format(X_train.shape))\nprint('X_train_l1.shape: {}'.format(X_train_rfe.shape))\nX_test_rfe= select.transform(X_test)\nmodel = LinearRegression(n_jobs=-1).fit(X_train_rfe, y_train)\nprint('Test score: {:.3f}'.format(model.score(X_test_rfe, y_test)))\n", "intent": "And also some recursive feature elimination:\n"}
{"snippet": "def log_transform(arr):\n    return np.log(arr)\n", "intent": "**Logarithmic model**\n"}
{"snippet": "y=df.republican\nX=df.drop('republican',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=4444)\n", "intent": "Split the data into a test and training set. Use this function:\nfrom sklearn.cross_validation import train_test_split\n"}
{"snippet": "N = 200\npoints, clusters = make_blobs(n_samples=N, centers=3, n_features=2, cluster_std=0.8, random_state=0)\npoints = np.array(points).tolist()\n", "intent": "Generate random data points\n"}
{"snippet": "df = pd.read_csv('2013_movies.csv')\ndel df['Title']\nfrom datetime import datetime\ndf['ReleaseDate'] = df.apply(lambda x: (datetime.now() \n                                        - datetime.strptime(x['ReleaseDate'],'%Y-%m-%d %H:%M:%S'))\n                                         .total_seconds(),\n                             axis=1)\ndf = pd.get_dummies(df, columns = ['Director'], drop_first=True)\ndf.dropna(inplace=True)\n", "intent": "* Movie Dataset\n* Bar Graph of each Rating\n* KNN/LogReg Prediction Accuracies\n* Stupid Predictor Accuracy\n* Comparison\n* LogReg Coefficients\n"}
{"snippet": "X = congress_votes_df.iloc[:, 1:].copy()\ny = congress_votes_df.iloc[:, 0].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "**Challenge 2**\nSplit the data into a test and training set. Use this function:\n```\nfrom sklearn.cross_validation import train_test_split\n```\n"}
{"snippet": "X = haberman_df.iloc[:, :-1].copy()\ny = haberman_df.iloc[:, -1].copy()\ny[y == 2] = 0\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "The most recent year of surgery in the dataset is 1969.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nc = {'logit__C': np.arange(.01, 2.02, .05)}\npipe = Pipeline([('scaler', StandardScaler()),\n                 ('logit', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))])\ngrid = GridSearchCV(pipe, param_grid=c, cv=4, return_train_score=True)\ngrid.fit(X_train, y_train)\nprint('Highest cross-validation accuracy: {:0.3f}'.format(grid.best_score_))\nprint('Test set score: {:0.3f}'.format(grid.score(X_test, y_test)))\nprint('Optimal regularization strength: {}'.format(grid.best_params_['logit__C']))\n", "intent": "<font color=\"blue\">\nMake a similar model but with `LogisticRegression` instead, calculate test accuracy.\n</font>\n"}
{"snippet": "neighbors = {'knn__n_neighbors': np.arange(1, 21, dtype=int)}\npipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])\ngrid = GridSearchCV(pipe, param_grid=neighbors, cv=4, return_train_score=True)\ngrid.fit(X_train, y_train)\nn = grid.best_params_['knn__n_neighbors']  \n", "intent": "<font color=\"blue\">\nDraw the learning curve for KNN with the best k value as well.\n</font>\n"}
{"snippet": "pipe = Pipeline([('scaler', StandardScaler()), ('clf', GaussianNB())])\npipe.fit(X_train, y_train)\nprint('Gaussian Naive Bayes test score: {:0.3f}'.format(pipe.score(X_test, y_test)))\n", "intent": "Lastly, let's also try GaussianNB.  We excluded it from the grid search because we don't really have any parameters over which to search.\n"}
{"snippet": "param_grid = dict(\n    max_depth=range(2, 10, 2),\n    max_features=range(1, df.shape[1] - 1, 2)\n    )\ngrid = GridSearchCV(RandomForestClassifier(n_estimators=1000,\n                                           n_jobs=-1, random_state=RANDOM_STATE), \n                    param_grid, cv=4, return_train_score=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    df, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\ngrid.fit(X_train, y_train)\n", "intent": "Let's give this a try for random forest classificatoin with varying tree depth and features.\n"}
{"snippet": "movie_df=pd.read_csv('2013_movies.csv')\n", "intent": "For your movie classifiers, calculate the precision and recall for each class.\n"}
{"snippet": "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data'\ndf = pd.read_csv(url, names = ['Class Name','handicapped-infants','water-project-cost-sharing',\n                               'adoption-of-the-budget-resolution','physician-fee-freeze','el-salvador-aid',\n                              'religious-groups-in-schools','anti-satellite-test-ban','aid-to-nicaraguan-contras',\n                              'mx-missile','immigration','synfuels-corporation-cutback','education-spending',\n                              'superfund-right-to-sue','crime','duty-free-exports','export-administration-act-south-africa'])\n", "intent": "For the house representatives data set, fit and evaluate a decision tree classifier. Examine the rules your tree uses.\n"}
{"snippet": "N = 300\npoints, clusters = make_blobs(n_samples=N, centers=5, n_features=2, cluster_std=0.8, random_state=0)\npoints = np.array(points).tolist()  \nclusters = np.array(clusters).tolist()  \n", "intent": "Generate random data points\n"}
{"snippet": "url2 = 'https://storage.googleapis.com/kaggle-competitions-data/kaggle/3136/train.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1519266715&Signature=QAmWZ44uU2dmteoEgWcV%2BRLslJrMwCknx5I4Kk8evvjI69ajUWimlwojf5hEe8HTavVeEUdvYqqrXIJZpSB4wHahv%2FiIshNigAim2iV4GnNEro9NwQUh2HapuOt1l%2FfdRkFdLFO66mXVA5EhY4NghBXNvsY2TOKDpOZecFKWRuIwv0jdBHcttcCZS%2F5xjJf9IC1qRdLUc2TAFRzAL08BssfpdV%2F86QVhZrzKIq6bBwJeyOZmMeG0twqUuhXnfy0MnOuzUbUyJBlV3TyEeEHfq6vgdW%2BPCFxi6XI%2FGkgUtuXxMJMviWAFDOFsqt4PKQ20CBOFfSnlv8iU0FlQBEU2dg%3D%3D'\ndf3 = pd.read_csv(url2)\ndf3.head()\n", "intent": "**Challenge Number 3**\n"}
{"snippet": "import pandas as pd\nimport numpy as np\npath='https://ibm.box.com/shared/static/q6iiqb1pd7wo8r3q28jvgsrprzezjqk3.csv'\ndf = pd.read_csv(path)\n", "intent": "<p></p>\n<li>Model Evaluation</li>\n<li>Over-fitting, Under-fitting and Model Selection</li>\n<li>Ridge Regression</li>\n<li>Grid Search</li>\n<p></p>\n"}
{"snippet": "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\nnames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\ndataset = pandas.read_csv(url, names=names)\n", "intent": "> Iris Path: Dataset is available on many sites but we are taking it from UCI ML\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "plyfile = PlyData.read('Datasets/stanford_armadillo.ply')\narmadillo = pd.DataFrame({\n  'x':plyfile['vertex']['z'][::reduce_factor],\n  'y':plyfile['vertex']['x'][::reduce_factor],\n  'z':plyfile['vertex']['y'][::reduce_factor]\n})\n", "intent": "Load up the scanned armadillo:\n"}
{"snippet": "pca = do_PCA(armadillo, 'full')\n", "intent": "Let's see how long it takes PCA to execute:\n"}
{"snippet": "city = df.groupby('City')['Zip'].count().reset_index().sort_values('Zip', ascending=False).reset_index(drop=True)\ncity_community = df[df['Community School?'] == 'Yes'].groupby('City')['Zip'].count().reset_index().sort_values('Zip', ascending=False).reset_index(drop=True)\ncity_merge = pd.merge(city, city_community, how='left', on='City')\ncity_merge.fillna(0, inplace=True)\ncity_merge['Zip_y'] = city_merge['Zip_y'].astype(int)\ntop_10_city = city_merge.iloc[:10,]\ntop_10_city = top_10_city.rename(columns={\"Zip_x\":'Total Count', \"Zip_y\":'Community Count'})\ntop_10_city\n", "intent": "<a id='school_per_city'></a>\n***\nThe **Top 3 Cities** with the most schools are:\n1. **Brooklyn**: 411\n2. **Bronx**: 297\n3. **New York **: 232\n"}
{"snippet": "registration_per_year = pd.DataFrame(shsat.groupby('Year of SHST')['Number of students who registered for the SHSAT'].sum()).reset_index()\nregistration_per_year\n", "intent": "<a id='registration_per_year'></a>\nYear **2014** has the highest amount of registrations at **838**, but then it falls off in the following years.\n"}
{"snippet": "data = np.loadtxt('house_pricing.csv')\nscaler = StandardScaler()\nX = scaler.fit_transform(data[:, :-1])\ny = data[:, -1:]\ny_log = np.log(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n", "intent": "Let's load house pricing data again.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nspam_data = pd.read_csv('spam.csv')\nspam_data['target'] = np.where(spam_data['target']=='spam',1,0)\nspam_data.head(10)\n", "intent": "In this assignment you will explore text message data and create models to predict if a message is spam or not. \n"}
{"snippet": "milk_prod = pd.read_csv('monthly-milk-production.csv', index_col='Month')\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", \\\n                   quoting=3 )\nprint test.shape\nnum_reviews = len(test[\"review\"])\n", "intent": "Gets over 90% on the test data\n"}
{"snippet": "import pandas as pd\npath = 'data/yelp.csv'\nyelp = pd.read_csv(path)\n", "intent": "Read **`yelp.csv`** into a pandas DataFrame and examine it.\n"}
{"snippet": "n=len(df) \nm=len(df.iloc[0,:])\ndf_matrix = df.as_matrix()\nnx = df_matrix[:,0:(m-1)]\nny = df_matrix[:,m-1]\nny = ny[:,np.newaxis] \nW1 = np.linalg.inv(np.dot(nx.T,nx)) \nW2 = np.dot(nx.T,ny)\nW = np.dot(W1,W2)\nweights_df = pd.DataFrame(W,index=['beta0','beta1'])\n", "intent": "Matrix multiplication Linear Regression to obtain the weights\n$ W = (X^T X)^{-1} X^T Y $\n$ W_1 = (X^T X)^{-1} $\n$ W_2 = X^T Y $\n"}
{"snippet": "d_train.Embarked = d_train.Embarked.fillna(\"None\")\nd_train.Cabin = d_train.Cabin.fillna(\"None\")\nd_train.Age = d_train.Age.fillna(d_train.Age.mean())\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nd_train['Sex'] = encoder.fit_transform(d_train.Sex)\nd_train['Cabin'] = encoder.fit_transform(d_train.Cabin)\nd_train['Embarked'] = encoder.fit_transform(d_train.Embarked)\nd_train.head()\n", "intent": "**Transformar dados categoricos e limpando algumas colunas**\n"}
{"snippet": "df = pd.read_csv(\"insurance.csv\", header=None, names=['n_rei', 'pgtTot'])\n", "intent": "ALGORITMO BASELINE:\n"}
{"snippet": "data = datasets.load_boston() \n", "intent": "Loads Boston dataset from datasets library:\n"}
{"snippet": "df = pd.DataFrame(data.data, columns=data.feature_names) \n", "intent": "Define the data/predictors as the pre-set feature names:\n"}
{"snippet": "target = pd.DataFrame(data.target, columns=[\"MEDV\"]) \n", "intent": "Put the target (housing value -- MEDV) in another DataFrame:\n"}
{"snippet": "from IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"styles/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()\n", "intent": "- Email: <kevin@dataschool.io>\n- Website: http://dataschool.io\n- Twitter: [@justmarkham](https://twitter.com/justmarkham)\n"}
{"snippet": "target = pd.DataFrame(data.target, columns=[\"MEDV\"])\n", "intent": "Put the target (housing value -- MEDV) in another DataFrame:\n"}
{"snippet": "import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\ndata = datasets.load_iris()\n", "intent": "> Put your code here\n"}
{"snippet": "log_model.score(X = pd.DataFrame(encoded_sex) ,\n                y = df[\"Survived\"])\n", "intent": "We can also get the accuracy of a model using the scikit-learn model.score() function:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nmodel2 = LogisticRegression()\nmodel2.fit(X_train, y_train)\n", "intent": "So far, we have trained and tested on the same set. Let's instead split the data into a training set and a testing set.\n"}
{"snippet": "loans = pd.read_csv(\"/Users/racheldyap/Desktop/BTS 1st Semester/CDA/Session 8/loan_data.csv\")\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "conf_mat = confusion_matrix(y_test, y_hat_test)\nconf_df = pd.DataFrame(conf_mat, columns = ['y_hat=0', 'y_hat = 1'], index = ['y=0', 'y=1'])\nconf_df\n", "intent": "**3\\. Compute the confusion table for both the fitted classifier and the classifier that predicts all 0's.**\n"}
{"snippet": "conf_mat_1 = confusion_matrix(y_test, y_hat_test)\nconf_df_1 = pd.DataFrame(conf_mat_1, columns = ['y_hat=0', 'y_hat = 1'], index = ['y=0', 'y=1'])\nconf_df_1\n", "intent": "We now create the confusion matrix.\n"}
{"snippet": "conf_mat_2 = confusion_matrix(y_test2, y_hat_test2)\nconf_df_2 = pd.DataFrame(conf_mat_2, columns = ['y_hat=0', 'y_hat = 1'], index = ['y=0', 'y=1'])\nconf_df_2\n", "intent": "We now create the confusion matrix for Problem 2. \n"}
{"snippet": "X_train_quad = PolynomialFeatures(2).fit_transform(X_train)\nX_test_quad = PolynomialFeatures(2).fit_transform(X_test)\ny_train.shape, X_train_quad.shape, y_test.shape, X_test_quad.shape\n", "intent": "> \n> Both Multinomial & OvR implemented for reference.\n"}
{"snippet": "import pandas as pd\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data'\ncol_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\npima = pd.read_csv(url, header=None, names=col_names)\n", "intent": "[Pima Indian Diabetes dataset](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) from the UCI Machine Learning Repository\n"}
{"snippet": "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train,\n                                                stratify = labels_train, random_state = 123)\n", "intent": "Train/Validation Split\n"}
{"snippet": "Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(Xtrain, Ytrain,\n                                                  stratify=Ytrain, random_state=123)\n", "intent": "Train/Validation Split\n"}
{"snippet": "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train, \n                                                stratify = labels_train,\n                                                random_state = 123)\n", "intent": "Train/Validation Split\n"}
{"snippet": "car = pd.read_csv('../datasets/car_evaluation/car.csv')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "np.random.seed(101)\nfrom sklearn.model_selection import train_test_split\ndatatrain, datatest = train_test_split(data2.ix[comind&demoind], test_size=0.5)\n", "intent": "Now split the dataset, train the model over one half and apply to the other\n"}
{"snippet": "df = pd.read_csv('HtWt.csv')\ndf.head()\n", "intent": "We have observations of height and weight and want to use a logistic model to guess the sex.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = deputes[['place_en_hemicycle']]\ny = deputes['groupe_sigle']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25)\n", "intent": "Which [algorithm](http://scikit-learn.org/stable/tutorial/machine_learning_map/) should I use ? \n"}
{"snippet": "from sklearn.datasets import fetch_lfw_people\nfaces = fetch_lfw_people(min_faces_per_person=60)\nprint(faces.target_names)\nprint(faces.images.shape)\n", "intent": "This example applies a PCA projection to facial image data \nusing the Labeled Faces in the Wild dataset made available through Scikit-Learn:\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.keys()\n", "intent": "We previously looked at the hand-written digits data, and we use that again here to see how the random forest classifier can be used in this context.\n"}
{"snippet": "iris = datasets.load_iris()\nx_vals = np.array([x[3] for x in iris.data])\ny_vals = np.array([y[0] for y in iris.data])\n", "intent": "Next we load the Iris data from the Scikit-Learn library.\n"}
{"snippet": "from pandas import DataFrame\nlabels = [\"hot\", \"warm\", \"cool\", \"cold\"] * 25\nrandom.shuffle(labels)\nN = 100\ndata = DataFrame(np.random.randint(0, 100, size=(N, 2)), columns = [\"x\", \"y\"], index = labels)\ndata.count\n", "intent": "Create the training data\n"}
{"snippet": "test_data = replace_strings(pd.read_csv('/root/labs/demos/titanic/test.csv'))\ntest_data\n", "intent": "Not bad! We have some test data to run it against in train.csv\n"}
{"snippet": "x, y = datasets.make_classification(n_samples=20000, n_features=20, n_classes=2, n_informative=15)\nprint numpy.percentile(x[y == 1 ],  [0, 25, 50, 75, 100])\nprint numpy.percentile(x[y == 0 ],  [0, 25, 50, 75, 100])\n", "intent": "**Let us now build a much larger matrix for use in our classification initiatives.**\n"}
{"snippet": "train = pd.read_csv('data/titanic_train.csv')\ntest = pd.read_csv('data/titanic_test.csv')\n", "intent": "2. We load the train and test datasets with Pandas.\n"}
{"snippet": "(X_train, X_test, \n y_train, y_test) = cv.train_test_split(X, y, test_size=.05)\n", "intent": "6. Let's try to train a `LogisticRegression` classifier. We first need to create a train and a test dataset.\n"}
{"snippet": "submission = generate_submission(threshold=0.04)\nsubm_name = \"FADL1-L3CA-submission-RN34-06\"\nsubmission.to_csv(f'{SUBM}{subm_name}.csv.gz', compression='gzip', index=False)\nFileLink(f'{SUBM}{subm_name}.csv.gz')\n", "intent": "This scores: **0.90181 -- 517/938** Private, when using a threshold of `0.035` and Log predictions.\n"}
{"snippet": "df = pd.read_csv(path + 'train.csv')\nlen(df['id'])\n", "intent": "Getting length of dataset. Dataset is my custom-built set for my G-LOC-Detector.\n"}
{"snippet": "PATH = 'data/nietzsche/'\nget_data(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", f'{PATH}nietzsche.txt')\ntext = open(f'{PATH}nietzsche.txt').read()\nprint('corpus length:', len(text))\n", "intent": "> We're going to download the collected works of Nietzsche to use as our data for this class.\n"}
{"snippet": "movie_names = pd.read_csv(path+'movies.csv').set_index('movieId')['title'].to_dict()\n", "intent": "Just for display purposes, let's read in the movie names too:\n"}
{"snippet": "iris = datasets.load_iris()\nx_vals = np.array([x[3] for x in iris.data])\ny_vals = np.array([y[0] for y in iris.data])\n", "intent": "Now we load the Iris data.\n"}
{"snippet": "centers = [[1,1], [6,6]]\nX, _ = make_blobs(n_samples=400, n_features=2, centers=centers, cluster_std=3.)\nrun_MS();\n", "intent": "However, if there's an outlier point that doesn't converge with the rest, Mean Shift will make it it's own cluster.\n"}
{"snippet": "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntrain = pd.read_csv(comp_path/TRAIN_DATA_FILE)\n", "intent": "Labels are already binary encoded, so no need to numericalize. Therefore `use_vocab=False`.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df, y, random_state=42)\n", "intent": "**We won't be scaling the data for this model, because it's not required here.**\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nprint(digits.DESCR)\n", "intent": "Let's load a simple dataset of 8x8 gray level images of handwritten digits (bundled in the sklearn source code):\n"}
{"snippet": "from sklearn.decomposition import RandomizedPCA\npca = RandomizedPCA(n_components=2)\nX_pca.shape\n", "intent": "Let's visualize the dataset on a 2D plane using a projection on the first 2 axis extracted by Principal Component Analysis:\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits_split_filenames = persist_cv_splits(digits.data, digits.target,\n    name='digits', random_state=42)\ndigits_split_filenames\n", "intent": "Let's try it on the digits dataset, we can run this from the :\n"}
{"snippet": "TfidfVectorizer()\n", "intent": "The text vectorizer has many parameters to customize it's behavior, in particular how it extracts tokens:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntext_train_small, text_validation, target_train_small, target_validation = train_test_split(\n    text_train_all, target_train_all, test_size=.5, random_state=42)\n", "intent": "Let's split the training CSV file into a smaller training set and a validation set with 100k random tweets each:\n"}
{"snippet": "from sklearn.feature_extraction.text import HashingVectorizer\nh_vectorizer = HashingVectorizer(charset='latin-1')\ndv['h_vectorizer'] = h_vectorizer\ndv['read_csv'] = read_csv\ndv['training_csv_file'] = training_csv_file\ndv['n_partitions'] = len(client)\n", "intent": "Let's send all we need to the engines\n"}
{"snippet": "iris = datasets.load_iris()\nx_vals = np.array([x[3] for x in iris.data]) \ny_vals = np.array([y[0] for y in iris.data]) \n", "intent": "We load the iris data.\n"}
{"snippet": "pca.fit(_90s_z)\n_90s_existing_nd = pca.transform(_90s_z)\n_90s_existing_df_nd = pd.DataFrame(_90s_existing_nd)\n_90s_existing_df_nd.index = _90s.index\n_90s_existing_df_nd.columns = _90s.columns\n_90s_existing_df_nd.head()\n", "intent": "Repeat the same steps for the other dataset.\n"}
{"snippet": "c1means = pd.DataFrame(cluster1.mean())\nc2means = pd.DataFrame(cluster2.mean())\nc3means = pd.DataFrame(cluster3.mean())\nc4means = pd.DataFrame(cluster4.mean())\nc5means = pd.DataFrame(cluster5.mean())\nc6means = pd.DataFrame(cluster6.mean())\nc7means = pd.DataFrame(cluster7.mean())\n", "intent": "Now, lets look at the characteristics of each cluster by calculating means of each clusters.\n"}
{"snippet": "pca.fit(_10s_z)\n_10s_existing_nd = pca.transform(_10s_z)\n_10s_existing_df_nd = pd.DataFrame(_10s_existing_nd)\n_10s_existing_df_nd.index = _10s.index\n_10s_existing_df_nd.columns = _10s.columns\n_10s_existing_df_nd.head()\n", "intent": "Repeat the same steps for the other dataset.\n"}
{"snippet": "from sklearn.datasets import load_digits\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nX, y = load_digits(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n", "intent": "<h3>Two Layer Fully Connected Neural Net with SGD</h3>\n"}
{"snippet": "trainDF = pd.read_csv('train.csv')\ntrainDF = trainDF.dropna(how=\"any\").reset_index(drop=True)\ntrainDF.ix[:7,3:]\nprint trainDF.shape\n", "intent": "Load data and show some samples of the data\n-------------------------------------------\n"}
{"snippet": "submissionName = 'shallowBenchmark2'\nsubmission = pd.DataFrame()\nsubmission['test_id'] = testDF['test_id']\nsubmission['is_duplicate'] = testPredictions\nsubmission.to_csv(submissionName + '.csv', index=False)\n", "intent": "Create a Submission\n-------------------\n"}
{"snippet": "trainDF = pd.read_csv('train.csv')\ntrainDF = trainDF.dropna(how=\"any\").reset_index(drop=True)\ntrainDF.ix[:7,3:]\n", "intent": "Load data and show some samples of the data\n-------------------------------------------\n"}
{"snippet": "submissionName = 'shallowBenchmark'\nsubmission = pd.DataFrame()\nsubmission['test_id'] = testDF['test_id']\nsubmission['is_duplicate'] = testPredictions\nsubmission.to_csv(submissionName + '.csv', index=False)\n", "intent": "Create a Submission\n-------------------\n"}
{"snippet": "masked_epi = masker.inverse_transform(samples)\nmax_zscores = nl.image.math_img(\"np.abs(img).max(axis=3)\", img=masked_epi)\nnl.plotting.plot_stat_map(max_zscores, bg_img=anat, dim=-.5)\n", "intent": "To recover the original data shape (giving us a masked and z-scored BOLD series), we simply use the masker's inverse transform:\n"}
{"snippet": "(x_vals, y_vals) = datasets.make_circles(n_samples=350, factor=.5, noise=.1)\ny_vals = np.array([1 if y==1 else -1 for y in y_vals])\nclass1_x = [x[0] for i,x in enumerate(x_vals) if y_vals[i]==1]\nclass1_y = [x[1] for i,x in enumerate(x_vals) if y_vals[i]==1]\nclass2_x = [x[0] for i,x in enumerate(x_vals) if y_vals[i]==-1]\nclass2_y = [x[1] for i,x in enumerate(x_vals) if y_vals[i]==-1]\n", "intent": "For this example, we will generate fake non-linear data.  The data we will generate is concentric ring data.\n"}
{"snippet": "import google.datalab.bigquery as bq\nsql = \"SELECT image_url, short_label, label FROM demos.cervical_truncated_images\"\ndf = bq.Query(sql).execute().result().to_dataframe()\nprint 'Have a total of {} labeled images'.format(len(df))\ndf.tail()\n", "intent": "This is what the labeled dataset looks like (To train on a different set of images, start with a BigQuery table that has these two columns).\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer \ncount_vector = CountVectorizer()\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "data = pd.read_csv('311-service-requests.csv', parse_dates=['Created Date'], low_memory=False)\n", "intent": "Check first that you have the CSV in the same folder as the .ipynb notebook.\n"}
{"snippet": "data = pandas.read_csv('datasets/training.csv')\ndata_correlation = pandas.read_csv('datasets/check_correlation.csv')\n", "intent": "Download \n* `training.csv`, \n* `check_correlation.csv`, \nto the folder `datasets/` from https://www.kaggle.com/c/flavours-of-physics/data\n"}
{"snippet": "df2 = pd.DataFrame({'A' : 1.,\n                  'B' : pd.Timestamp('20130102'),\n                  'C' : pd.Series(1,index=list(range(4)),dtype='float32'),\n                  'D' : np.array([3] * 4,dtype='int32'),\n                  'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]),\n                  'F' : 'foo' })\ndf2\n", "intent": "Creating a DataFrame by passing a dict of objects that can be converted to series-like.\n"}
{"snippet": "df1.fillna(value=5)\n", "intent": "Filling missing data\n"}
{"snippet": "events = pd.read_csv(\"data/train.csv\")\n", "intent": "Now let's plot some events.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve\ndata = pandas.read_csv('datasets/training.csv')\n", "intent": "Compare ROC curve stability for simple Tree and for any ensemble method. Do they have different confidence intervals for ROC curves and AUC indeed?\n"}
{"snippet": "from sklearn.datasets import fetch_covtype\ncovtype = fetch_covtype()\n", "intent": "http://habrahabr.ru/post/249759/\nhttp://nbviewer.ipython.org/github/aguschin/kaggle/blob/master/forestCoverType_featuresEngineering.ipynb\n"}
{"snippet": "print('Getting Test Set Accuracy For {} Sentences.'.format(len(texts_test)))\ntest_acc_all = []\nfor ix, t in enumerate(vocab_processor.fit_transform(texts_test)):\n    y_data = [[target_test[ix]]]\n    if (ix+1)%50==0:\n        print('Test Observation \n    [[temp_pred]] = sess.run(prediction, feed_dict={x_data:t, y_target:y_data})\n    test_acc_temp = target_test[ix]==np.round(temp_pred)\n    test_acc_all.append(test_acc_temp)\nprint('\\nOverall Test Accuracy: {}'.format(np.mean(test_acc_all)))\n", "intent": "Now that we have a logistic model, we can evaluate the accuracy on the test dataset.\n"}
{"snippet": "data_agreement = pandas.read_csv('datasets/check_agreement.csv')\ndata_MC = pandas.concat([data_agreement[data_agreement.signal == 1], data[data.signal == 1]])\ndata_MC['signal'] = numpy.array([0] * sum(data_agreement.signal.values == 1) + [1] * sum(data.signal.values == 1))\n", "intent": "Use U-test to compare different ND pdfs\n"}
{"snippet": "columns = ['hSPD', 'pt_b', 'pt_phi', 'vchi2_b']\noriginal = root_numpy.root2array('datasets/MC_distribution.root', branches=columns)\ntarget = root_numpy.root2array('datasets/RD_distribution.root', branches=columns)\noriginal = pandas.DataFrame(original)\ntarget = pandas.DataFrame(target)\noriginal_weights = numpy.ones(len(original))\n", "intent": "Pay attention that here we work with `.root` files and `root_numpy` can help\n"}
{"snippet": "train_index, test_index = train_test_split(range(len(data)))\ntrain = data.iloc[train_index, :]\ntest = data.iloc[test_index, :]\n", "intent": "The worst features are `SPDhits`, `FlightDistance`, `IP_p1p2`\n"}
{"snippet": "PATH_TO_DATA = ('./data')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "ohe = OneHotEncoder()\nohe_columns = ['weekday'] + hour\nohe.fit(X[ohe_columns])\nX_ohe = ohe.transform(X[ohe_columns])\ntest_ohe = ohe.transform(test[ohe_columns])\n", "intent": "Scale this features and combine then with Tf-Idf based on sites (you'll need `scipy.sparse.hstack`)\n"}
{"snippet": "x_train_multi, x_test_multi, y_train_multi, y_test_multi = Multi_log_reg.split(x_multi,y_multi,rand=None)\nnot_bi = Multi_log_reg.not_bi(x_multi)\nscaler = StandardScaler()\nscaler.fit(x_train_multi[not_bi]) \nx_train_scaled_multi=x_train_multi\nx_test_scaled_multi=x_test_multi\nx_train_scaled_multi[not_bi] = scaler.transform(x_train_multi[not_bi])\nx_test_scaled_multi[not_bi]  = scaler.transform(x_test_multi[not_bi])\nmodel_multi = Multi_log_reg.reg(x_train_scaled_multi,y_train_multi)\nprint(\"Mean accuracy score is %f\" % model_multi.score(x_test_scaled_multi,y_test_multi,sample_weight=None))\n", "intent": "Run Multi-Class(Ridge)\n"}
{"snippet": "def pca_k(k):\n    pca = PCA(n_components=k)\n    principalComponents = pca.fit_transform(X)\n    pca.explained_variance_ratio_\n    print(\"The percentage of Variance retained by K = \",k, \" : \", sum(pca.explained_variance_ratio_))\n", "intent": "   - My first attempt with K = 30 yield 0.761394, which is not a very good result.\n   - When K goes up to 70 The result seems promising.\n"}
{"snippet": "from sklearn.model_selection import train_test_split , cross_val_score\ndef split(x,y,rand=0):\n        y = np.ravel(y)\n        x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.25,random_state=rand)\n        return x_train, x_test, y_train, y_test \n", "intent": "Manually generate validation dataset (in case of clf is not useful...)\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X.reshape([len(X),-1]),\n                                                 y.reshape([len(y),-1]),\n                                                 test_size=0.05,random_state=42)\nprint(\"X_train.shape=\",X_train.shape)\nprint(\"X_test.shape=\",X_test.shape)\n", "intent": "Now let's get through the usual pipeline: split data between train and test; fit linear regression\n"}
{"snippet": "def tokenizer(text):\n    words = nltk.word_tokenize(text)\n    return words\ntfidf = TfidfVectorizer(tokenizer=tokenizer, stop_words='english', max_features=max_features)\nsparse_tfidf_texts = tfidf.fit_transform(texts)\n", "intent": "Define tokenizer function and create the TF-IDF vectors with SciKit-Learn.\n"}
{"snippet": "from sklearn.random_projection import GaussianRandomProjection\nXrp = GaussianRandomProjection(n_components=2).fit_transform(X)\n", "intent": " * Pick several random axes from normal distribution\n * Projects data to these axis\n * Mostly useless for our task.\n"}
{"snippet": "from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=2)\nXsvd = svd.fit_transform(X)\n", "intent": "* Idea: try to compress the data in a way that you can then restore it\n* Equivalent to minimizing MSE: $|| X  - U \\cdot \\Sigma \\cdot V^T ||$\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(X.values, y.values,\n                                                                train_size=0.85, random_state=1234)\n", "intent": "First of all, lets split our train data to train and validation sets:\n"}
{"snippet": "import pandas\ndf2 = pandas.DataFrame(data=memo_time)\ndf2 = df2.set_index(\"legend\").sort_values(\"average\")\ndf2\n", "intent": "C'est beaucoup plus rapide.\n"}
{"snippet": "iris = datasets.load_iris()\nX_iris = iris.data\ny_iris = iris.target\n", "intent": "Nous partirons classiquement du dataset Iris (classification de 3 fleurs sur la base de certaines de leurs mesures) :\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "``scikit-learn`` embeds a copy of the iris CSV file along with a helper function to load it into numpy arrays:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(faces.data,\n        faces.target, random_state=0)\nprint(X_train.shape, X_test.shape)\n", "intent": "We'll perform a Support Vector classification of the images.  We'll\ndo a typical train-test split on the images to make this happen:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n", "intent": "Simple [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n"}
{"snippet": "from pyensae.datasource import download_data\nif False:\n    file = download_data(\"Divvy_Trips_2016_Q3Q4.zip\",\n                         url=\"https://s3.amazonaws.com/divvy-data/tripdata/\")\nelse:\n    file = download_data(\"Divvy_Trips_2016_Q3.zip\")    \n", "intent": "Une carte des stations un jour de semaine.\n"}
{"snippet": "original_image = scipy.misc.imread(original_image_file)\nstyle_image = scipy.misc.imread(style_image_file)\ntarget_shape = original_image.shape\nstyle_image = scipy.misc.imresize(style_image, target_shape[1] / style_image.shape[1])\n", "intent": "Read in the images.\n"}
{"snippet": "train_df[\"avg_item_cat_price\"] = train_df[\"avg_item_cat_price\"].fillna(train_df[\"avg_all_item_cat_price\"]) \ntrain_df[\"avg_all_item_price\"] = train_df[\"avg_all_item_price\"].fillna(train_df[\"avg_all_item_cat_price\"]) \ntrain_df[\"avg_item_price\"] = train_df[\"avg_item_price\"].fillna(train_df[\"avg_all_item_price\"]) \ntrain_df[\"avg_shop_price\"] = train_df[\"avg_shop_price\"].fillna(train_df[\"avg_all_shop_price\"]) \ntrain_df[\"avg_all_shop_item_price\"] = train_df[\"avg_all_shop_item_price\"].fillna(train_df[\"avg_all_item_price\"]) \n", "intent": "Some items in the test dataset do not exist in train dataset. Imputation is needed. \n"}
{"snippet": "from itertools import product\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = []\nfor block_num in train_df['date_block_num'].unique():\n    cur_shops = train_df.loc[train_df['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = train_df.loc[train_df['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n", "intent": "Create grid using all the shop_id, item_id combiation for each date_block_num\n"}
{"snippet": "for df in train_df, test_df:\n    for feat in df.columns[4:]:\n        if 'sales' in feat:\n            df[feat]=df[feat].fillna(0)            \n        else:\n            df[feat]=df[feat].fillna(df[feat].median())           \n", "intent": "Some items in the test dataset do not exist in train dataset. Imputation is needed. \n"}
{"snippet": "index_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\ngb = sales.groupby(index_cols, as_index=False)['item_cnt_day'].agg('sum').rename(columns={'sum':'target'})\n", "intent": "This is necessary because shop and items could be different from one month to the next\n"}
{"snippet": "sales_submission_prev_month = pd.merge(sales_test, grid.loc[grid.date_block_num == 33], how='left', on=['shop_id', 'item_id'])\nsales_submission_prev_month = sales_submission_prev_month.loc[:,['ID', 'item_cnt_day']].fillna(0).rename(columns={'item_cnt_day':'item_cnt_month'})\nsales_submission_prev_month.to_csv('../readonly/sales_submission_prev_month.csv.gz', index=False, compression='gzip')\n", "intent": "Submit the sales for 2015 October (date_block_num = 33)\n"}
{"snippet": "index_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n", "intent": "We now need to prepare the features. This part is all implemented for you.\n"}
{"snippet": "summary_hour_duration = pd.DataFrame(train_data.groupby(['day_of_week','hour'])['trip_duration'].mean())\nsummary_hour_duration.reset_index(inplace = True)\nsummary_hour_duration['unit']=1\nsns.set(style=\"white\", palette=\"muted\", color_codes=False)\nsns.set_context(\"poster\")\nsns.tsplot(data=summary_hour_duration, time=\"hour\", unit = \"unit\", condition=\"day_of_week\", value=\"trip_duration\")\nsns.despine(bottom = False)\n", "intent": "- Simple lineplots can explain how the trip duration is changing with time for different days of week\n- very easy to interpret\n"}
{"snippet": "test_df = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv')\ntest_fr = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_test.csv')\ntest_fr_new = test_fr[['id', 'total_distance', 'total_travel_time', 'number_of_steps']]\ntest_df = pd.merge(test_df, test_fr_new, on = 'id', how = 'left')\ntest_df.head()\n", "intent": " 1. Loading test data from competition and OSRM features for test data\n"}
{"snippet": "train = pd.read_csv('../data/train.csv')\ntest = pd.read_csv('../data/test.csv')\n", "intent": "<h2>1.3 Loading the Data</h2><br>\n<p>Load the data using the Pandas `read_csv` function:</p>\n"}
{"snippet": "cols = train.columns.values\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n", "intent": "Create a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n", "intent": "`TfidfVectorizer` = `CountVectorizer` + `TfidfTransformer`\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(documents, lowercase = True, token_pattern = '(?u)\\\\b\\\\w\\\\w+\\\\b' )\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "features = features.fillna(0.0)\ndisplay(features.head())\n", "intent": "And now we'll fill in any blanks with zeroes.\n"}
{"snippet": "df_mm = p.MinMaxScaler().fit_transform(df) \n", "intent": "`3.` Now it's your turn.  Try fitting the **MinMaxScaler** transformation to this dataset. You should be able to use the previous example to assist.\n"}
{"snippet": "pca, X_pca = do_pca(15, X)\n", "intent": "Now let's fit PCA with 15 components, and take a look at a few of the main features that live on the pca object we get back.\n"}
{"snippet": "df_train['Embarked'] = df_train['Embarked'].fillna(\"\")\nembarked_locs = sorted(df_train['Embarked'].unique())\nembarked_locs_mapping = dict(zip(embarked_locs, \n                                 range(0, len(embarked_locs) + 1)))\nembarked_locs_mapping\n", "intent": "Prepare to map Embarked from a string to a number representation:\n"}
{"snippet": "df_test = pd.read_csv('data/test.csv')\ndf_test.head()\n", "intent": "Read the test data:\n"}
{"snippet": "df_test['Survived'] = test_y\ndf_test[['PassengerId', 'Survived']] \\\n    .to_csv('data/results-rf.csv', index=False)\n", "intent": "Create a DataFrame by combining the index from the test data with the output of predictions, then write the results to the output:\n"}
{"snippet": "dataset = pandas.read_csv(\"tennis.csv\")\n", "intent": "** Loading data, generated locally for example 'John plays tennis?' **\n"}
{"snippet": "train_x, train_y, test_x, test_y = load_data(\"data/basketball.train.csv\")\n", "intent": "And then we can use it to load the data.\n"}
{"snippet": "import pandas\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = pandas.read_csv(url, names=names)\ndataframe.head()\n", "intent": "Now let's have a look at the dataset.\n"}
{"snippet": "import pandas\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = pandas.read_csv(url, names=names)\ndataframe.head()\n", "intent": "Pima Indians Diabetes Data Set: https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes\n"}
{"snippet": "messages = pandas.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t',\n                           names=[\"label\", \"message\"])\nmessages.head()\n", "intent": "We'll use **read_csv** and make note of the **sep** argument, we can also specify the desired column names by passing in a list of *names*.\n"}
{"snippet": "columns = ['id number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', \n           'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class']\ndf = pd.DataFrame(data, columns = columns)\ndf.head(10)\n", "intent": "Take a look at the dataset.\n"}
{"snippet": "dataset = pandas.read_csv(\"Iris.csv\")\ndataset.drop(columns=['Id'], inplace =True)\ndataset.head()\n", "intent": "** Loading data, generated locally for example 'John plays tennis?' **\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntrain, test = train_test_split(dataset, train_size = 0.7)\nfeature_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n", "intent": "** Train / Test set split: Dividing sub-set of test and training **\n"}
{"snippet": "columns = ['id number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', \n           'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class']\ndf = pd.DataFrame(cancer_data, columns = columns)\ndf.head()\n", "intent": "Let's have a look at the dataset.\n"}
{"snippet": "column = ['Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', \n           'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class']\ndf = pd.DataFrame(cancer_data, columns = column)\ndf.head()\n", "intent": "Print the dataset again to confirm the id number column is deleted.\n"}
{"snippet": "data = pd.read_csv('Advertising.csv', index_col=0)\ndata.head()\n", "intent": "Read data into a DataFrame\n"}
{"snippet": "dicts_test_x = []\nvectorizer_test = DictVectorizer()\nvec_test_x = \n", "intent": "We do similarly for vectorizing `test_x`.\n"}
{"snippet": "model = NMF(init=\"nndsvd\",\n            n_components=4,\n            max_iter=200)\n", "intent": "Apply NMF with SVD-based initialization to the document-term matrix $\\text{A}$ generate 4 topics.\n"}
{"snippet": "W = model.fit_transform(vectorized)\nH = model.components_\n", "intent": "Get the factors $\\text{W}$ and $\\text{H}$ from the resulting model.\n"}
{"snippet": "train = pd.read_csv('./input_bikesharing/train.csv',  parse_dates=['datetime'])\nprint(train.shape)\nprint(train.columns)\nprint(train.info())\ntrain.head()\n", "intent": "1. Data Load\n=============\n"}
{"snippet": "submit = pd.read_csv('./input_bikesharing/sampleSubmission.csv')\nprint(submit.shape)\nsubmit.head()\n", "intent": "4. Submit\n=========\n"}
{"snippet": "train_df = pd.read_csv(DATA_PATH + TRAIN_CSV)\ntest_df = pd.read_csv(DATA_PATH + TEST_CSV)\nstops = set(stopwords.words('english'))\ndef text_to_word_list(text):\n", "intent": "Create Embedding Matrix\n"}
{"snippet": "df_train = pd.read_csv(DATA_PATH + 'train.csv')\ndf_train.head()\n", "intent": "Set Training Data\n=================\n"}
{"snippet": "train = pd.read_csv('./input/train.csv', index_col='PassengerId')\nprint(train.shape)\ntrain.head(2)\n", "intent": "1. Load Dataset\n===============\n"}
{"snippet": "import pandas as pd\ntrain= pd.read_csv('./input/train.csv')\ntest = pd.read_csv('./input/test.csv')\n", "intent": "**load train, test dataset using Pandas**\n"}
{"snippet": "def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived, dead])\n    df.index = ['Survived', 'Dead']\n    df.plot(kind='bar', stacked=True, figsize=(10,5))\n", "intent": "* Pclass\n* Sex\n* Sibsp(\n* Parch(\n* Embarked\n* Cabin\n"}
{"snippet": "digits = datasets.load_digits()\n", "intent": "Load a sample dataset\n"}
{"snippet": "train[\"Embarked\"].fillna('S', inplace=True)\ntest[\"Embarked\"].fillna('S', inplace=True)\n", "intent": "**fill out missing embark with S embark**\n"}
{"snippet": "matchedData = cleanData[(cleanData['PurchasedUnits'] == 1) & (cleanData['DeliveredUnits'] == 1)]\nmatchedData.ProfitabilityScore.fillna(matchedData.ProfitabilityScore.mean(), inplace=True)\nmatchedData.ClearanceScore.fillna(matchedData.ProfitabilityScore.mean(), inplace=True)\n", "intent": "Matched Dataset\n===============\nCreate the matched subset of the data  \nClean `ProfitabilityScore` and `ClearanceScore`\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndv = DictVectorizer()\nX = dv.fit_transform(data[['Category']].T.to_dict().values())\nprint X, X.shape\n", "intent": "Another option which we haven't seen before is to use another feature extraction tool from sklearn, the DictVectorizer\n"}
{"snippet": "drinks = pd.read_csv('http://bit.ly/drinksbycountry')\ndrinks.loc[0:2]\ndrinks.iloc[0:2,0:2]\nufo.fillna(method='bfill').tail()\nufo.fillna(method='ffill').tail()\n", "intent": "    drinks.loc[[0:2,9:10],:]\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.20, random_state=17)\n", "intent": "Split the data into a training set and a test set.\n"}
{"snippet": "iris = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\n", "intent": "https://en.wikipedia.org/wiki/Iris_flower_data_set\n"}
{"snippet": "import pandas as pd\ncrime = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data', header=None, na_values=['?'])\ncrime = crime.iloc[:, 5:]\ncrime.dropna(inplace=True)\ncrime.head()\nX = crime.iloc[:, :-1]\ny = crime.iloc[:, -1]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n", "intent": "This data set contains data on violent crimes within a community.\n"}
{"snippet": "import statsmodels.api as sm\ntarget = pd.DataFrame(df_rookie_averages['Salary_2018'])\nindependent_vars = df_rookie_averages.loc[:,['PTS','DWS','Age']]\nindependent_vars.astype(float)\nindependent_vars.dropna(inplace=True)\nindependent_vars.describe()\nindependent_vars = sm.add_constant(independent_vars)\ngen_model = sm.OLS(list(target['Salary_2018'].astype(float)),independent_vars.astype(float)).fit()\ngen_model.summary()\n", "intent": "This is the part of the ensemble model that is not position specific.  It does not use variables used by the position specific sub models.\n"}
{"snippet": "data = pd.read_csv('../../DSI-CHI-1/lessons/week-04/3.2-logistic-regression-lab/assets/datasets/train.tsv', sep='\\t', na_values={'is_news' : '?'}).fillna(0)\ndata['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\ndata['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n", "intent": "- These are websites that always relevant like recipes or reviews (as opposed to current events)\n- Look at some examples\n"}
{"snippet": "df = pd.read_csv('HtWt.csv')\ndf.head()\n", "intent": "We will look at the effect of strongly correlated variabels using a data set from Kruschke's book.\n"}
{"snippet": "data = pd.read_csv('../../DSI-CHI-1/lessons/week-04/4.2-sklearn-regularization-logistic-lab/assets/datasets/train.tsv', sep='\\t', na_values='?')\ndata['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', '')).fillna('')\ndata['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', '')).fillna('')\n", "intent": "- These are websites that always relevant like recipies or reviews (as opposed to current events)\n- Look at some examples\n"}
{"snippet": "dimensions = ['mean', 'se', 'worst']\nattributes = ['radius', 'texture', 'perimeter', 'area', 'smoothness', 'compactness',\n              'concavity', 'concave_points', 'symmetry', 'fractal_dimension']\nattribute_names = ['{}-{}'.format(x, y) for x in attributes for y in dimensions]\ncell_data_filepath = 'https://s3-us-west-2.amazonaws.com/ga-dat-2015-suneel/datasets/breast-cancer.csv'\ncol_names = ['id', 'diagnosis'] + attribute_names\ncell_df = pd.read_csv(cell_data_filepath, header=None, names=col_names)\ncell_df.head()\n", "intent": "Return to the Wisconsin breast cancer data. Clean it up as we did before.\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(features_df, target_df, test_size=0.33, random_state=5)\n", "intent": "Split into 66% training set and 33% testing set\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconmat = np.array(confusion_matrix(Y_test, Y_pred))\nconfusion = pd.DataFrame(conmat, index=['is_healthy', 'has_cancer'],\n                         columns=['predicted_healthy', 'predicted_cancer'])\nprint(confusion)\n", "intent": "Look at the confusion matrix\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(dummies)\nX.shape\n", "intent": "Cluster the Data to our our target groups. **BONUS**: Perform multiple clustering tests on various indicator variables\n"}
{"snippet": "df = pd.read_csv('../../DSI-CHI-1/lessons/week-07/1.3-tuning-clusters/assets/datasets/iris.csv')\ndf.head()\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_standard = StandardScaler().fit_transform(X)\n", "intent": "Run the clustering analysis using scikit-learn.\n*Hint*: Estimate the number of clusters, k, based on your visual examination of the distributions.\n"}
{"snippet": "df = pd.read_csv('../../DSI-CHI-1/lessons/week-07/1.3-tuning-clusters/assets/datasets/wine.csv')\ndf.head()\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "df = pd.read_csv('../../DSI-CHI-1/lessons/week-07/2.3-pca-lab-1/assets/datasets/votes.csv', index_col=0)\nprint df.shape\ndf.head()\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "standardizer = StandardScaler(withMean=True, withStd=True, \n                              inputCol='features', \n                              outputCol='std_features')\nmodel = standardizer.fit(output)\noutput = model.transform(output)\n", "intent": "Scale features to have zero mean and unit standard deviation\n"}
{"snippet": "X_standard = StandardScaler().fit_transform(X)\nX_standard.shape\n", "intent": "Next, create the covariance matrix from the standardized x-values and decompose these values to find the eigenvalues and eigenvectors\n"}
{"snippet": "pca = PCA(n_components=8)\npca_x = pca.fit_transform(X_standard)\npca_x[0:5]\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "X_standard = StandardScaler().fit_transform(X)\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "pca = PCA(n_components=4)\npca_x = pca.fit_transform(X_standard)\npca_x[0:5]\n", "intent": "Finally, conduct the PCA - use the results about to guide your selection of \"n\" componants\n"}
{"snippet": "df_unseen = pd.read_csv('../data/bank-marketing-data/bank-unseen-data.csv')\n", "intent": "Load new data from '../data/bank-marketing-data/bank-unseen-data.csv'\n"}
{"snippet": "df_unseen = pd.read_csv('../data/bank-marketing-data/bank-unseen-data.csv', index_col=0)\n", "intent": "Load new data from '../data/bank-marketing-data/bank-unseen-data.csv'\n"}
{"snippet": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nnb_samples = 100  \nnb_features = 20  \nbatch_size = 16\nX = np.random.random((nb_samples, nb_features))\ny = np.random.randint(0, 2, nb_samples)  \nX_train, X_test, y_train, y_test = train_test_split(X, y)\ny_train.shape, X_test.shape\n", "intent": "Scikit-learn has a `train_test_split` function.\n"}
{"snippet": "df = pd.read_csv(\"./College_Data\",index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df = pd.read_csv(\"advertising.csv\")\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "from sklearn import svm, grid_search, datasets\nfrom spark_sklearn import GridSearchCV\niris = datasets.load_iris()\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\nsvr = svm.SVC()\nclf = GridSearchCV(sc, svr, parameters)\nclf.fit(iris.data, iris.target)\n", "intent": "Install if necessary\n```\n```\n"}
{"snippet": "X = bow_transformer.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "n_samples = 100\nrandom_state = 0\ndatasets = collections.OrderedDict([\n    ('Blobs', make_blobs(n_samples=n_samples, centers=2, cluster_std=0.5, random_state=random_state)),\n    ('Circles', make_circles(n_samples=n_samples, factor=.5, noise=.03, random_state=random_state)),\n    ('Moons', make_moons(n_samples=n_samples, noise=.03, random_state=random_state))\n])\n", "intent": "Create a few toy datasets for binary classification. 'Blobs' is linearly seperable, the others are not.\n"}
{"snippet": "hours = [0.0,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0]\nph = pd.DataFrame({'Hours Researched':hours,'Prob. of Hiring':[special.expit(a*x+b) for x in hours]})\nph\n", "intent": "We can use this function to predict the *probability* that a given amount of research time will result in being hired.\n"}
{"snippet": "le = LabelEncoder()\nle.fit([\"red\", \"orange\", \"yellow\"])\n", "intent": "We make an instance of the `preprocessing.LabelEncoder()` class, and then fit a given data set.\n"}
{"snippet": "list(le.inverse_transform([2, 2, 1]))\n", "intent": "This operation can easily be undone with `.inverse_transform()`:\n"}
{"snippet": "fit.apply(lambda x: d[x.name].inverse_transform(x))\n", "intent": "And then can reverse the encoding:\n"}
{"snippet": "test = pd.DataFrame(columns = c.columns)\ntest.loc[0] = ['red','square']\ntest\n", "intent": "We can then use the dictionary to label future data `df` of the correct layout with the code `df.apply(lambda x: d[x.name].transform(x))`:\n"}
{"snippet": "an = pd.read_csv('animals.csv',index_col=0)\nan\n", "intent": "Let's look at the following data about animals:\n"}
{"snippet": "user_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\nusers = pd.read_table('http://bit.ly/movieusers', sep='|', header=None, names=user_cols)\n", "intent": "Documentation for [**`read_table`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html)\n"}
{"snippet": "from sklearn import model_selection, metrics, linear_model, datasets, feature_selection, tree\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\nd = defaultdict(LabelEncoder)\nanimals = tree.DecisionTreeClassifier()\nle = LabelEncoder()\ntn1 = (train_c[['Cold or Warm Blooded','Covering','Aquatic','Aerial','Lays Eggs']]).apply(lambda x: d[x.name].fit_transform(x))\ntrain_encoded = pd.concat([train_c['Legs'],tn1],axis=1)\ntrain_encoded\n", "intent": "Now we will use the LabelEncoder on each, and create a model on the training set.\n"}
{"snippet": "iris = sklearn.datasets.load_iris()\n", "intent": "For this part, we'll use the Iris dataset, and we'll train a random forest. \n"}
{"snippet": "lfw_people = datasets.fetch_lfw_people(min_faces_per_person=50, \n                resize=0.4, data_home='../../data/faces')\nprint('%d objects, %d features, %d classes' % (lfw_people.data.shape[0],\n      lfw_people.data.shape[1], len(lfw_people.target_names)))\nprint('\\nPersons:')\nfor name in lfw_people.target_names:\n    print(name)\n", "intent": "Let's load a dataset of peoples' faces and output their names. (This step requires stable, fast internet connection.)\n"}
{"snippet": "boston = datasets.load_boston()\nX = boston.data\n", "intent": "For the next question, load the housing prices dataset:\n"}
{"snippet": "sales = pd.read_csv('Iowa_Liquor_sales_sample_10pct.csv')\n", "intent": "Load your data from project 3\n"}
{"snippet": "bcw = pd.read_csv('../assets/datasets/breast-cancer-wisconsin.csv',\n                 names=column_names, na_values=['?'])\n", "intent": "We can see that there are '?' fields in column 6 that are making some columns strings. Reload the dataset specifying na_values.\n"}
{"snippet": "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", header=None)\n", "intent": "Read in Wisconsin Breast Cancer Dataset\n"}
{"snippet": "sac = pd.read_csv('../../assets/datasets/Sacramentorealestatetransactions.csv')\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "X = pd.DataFrame(data=sX,columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "ufo = pd.read_csv('http://bit.ly/uforeports')\nufo.columns\n", "intent": "**Question:** When reading from a file, how do I read in only a subset of the columns?\n"}
{"snippet": "from sklearn import preprocessing\ndf_numerical = df.copy()\nencoder_dict = {}\nfor col in string_variables:\n    le = preprocessing.LabelEncoder()\n    le.fit(df[col])\n    encoder_dict[col] = le\n    df_numerical[col] = le.transform(df[col])\ndf_numerical.head()\n", "intent": "Learn about label encoders here: http://scikit-learn.org/stable/modules/preprocessing.html\n"}
{"snippet": "df_additional_full = pd.read_csv('homework/Hw3/bank-additional-full.csv', sep = ';')\n", "intent": "Extend the analysis and cross-validation to bank-additional-full.csv. How does the performance change?\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features_array, target, test_size=0.20, random_state=0)\n", "intent": "Let's take the 80% of the data for training a first model and keep 20% for computing is generalization score:\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\nX = iris['data']\nNames = iris['feature_names']\ntarget_names = iris['target_names']\ny = iris['target']\n", "intent": "Import the data from the iris dataset in sklearn, generate X and y arrays\n"}
{"snippet": "oos = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT4/master/data/used_vehicles_oos.csv')\noos['type'] = oos.type.map({'car':0, 'truck':1})\noos\n", "intent": "How accurate is scikit-learn's regression tree at predicting the out-of-sample data?\n"}
{"snippet": "titanic = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT4/master/data/titanic.csv')\ntitanic.head(10)\n", "intent": "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle.\n"}
{"snippet": "pd.DataFrame(x_back, columns=vectorizer.get_feature_names())\n", "intent": "If we wanted the extra overhead, we could use the feature names as columns to a dataframe:\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(min_df = 1)\ntfidf_matrix = tfidf_vectorizer.fit_transform(mydoclist)\nprint tfidf_matrix.todense()\n", "intent": "In fact, you can do this just by combining the steps into one: the TfidfVectorizer\n"}
{"snippet": "allbeers = pd.read_csv(\"beer_reviews.tar.gz\", compression='gzip')\n", "intent": "Import data in a pandas dataframe called \"allbeers\". Use the compression keyword\n"}
{"snippet": "ufo = pd.read_csv('http://bit.ly/uforeports', nrows=3)\nufo\n", "intent": "**Question:** When reading from a file, how do I read in only a subset of the rows?\n"}
{"snippet": "movies = pd.read_table('movielens/movies.dat', sep='::', names= ['ITEMID', 'Title', 'Genres'], index_col= 'ITEMID')\n", "intent": "Import 'movies.dat' to a 'movies' pandas dataframe. Make sure you name the columns, use the correct separator and define the index.\n"}
{"snippet": "ratings = pd.read_table('movielens/ratings.dat', sep='::', names= ['UserID','MovieID','Rating','Timestamp'])\n", "intent": "Import 'ratings.dat' to a 'ratings' pandas dataframe. Make sure you name the columns, use the correct separator.\n"}
{"snippet": "svd.load_data(filename='./movielens/ratings.dat', sep='::', format={'col':0, 'row':1, 'value':2, 'ids': int})\n", "intent": "Populate it with the data from the ratings dataset, using the built in load_data method\n"}
{"snippet": "def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "tsne = TSNE(random_state=17)\ntsne_representation = tsne.fit_transform(X_scaled)\n", "intent": "Now, build a t-SNE representation:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_train_all=train[:1460]\nfeature_test=train[1460:]\ntarget_data=np.log(target_data)\nX_train, X_test, y_train, y_test = train_test_split(feature_train_all,target_data, test_size = 0.2, random_state = 0)\n", "intent": "4.Shuffle and Split Data\n"}
{"snippet": "from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ncategorical=list(raw_data.select_dtypes(include=['object']).columns.values)\nprint(categorical)\nfor value in categorical:\n    le.fit(raw_data[value])\n    raw_data[value]=le.transform(raw_data[value])\nraw_data.head()\n", "intent": "3.Converting a categorical feature with labelEncoder\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_train_all=raw_data[:1460]\nfeature_test=raw_data[1460:]\nX_train, X_test, y_train, y_test = train_test_split(feature_train_all,target_data, test_size = 0.2, random_state = 0)\n", "intent": "4.Shuffle and Split Data\n"}
{"snippet": "from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ncategorical=list(raw_data.select_dtypes(include=['object']).columns.values)\nprint(categorical)\nfor value in categorical:\n    le.fit(raw_data[value])\n    raw_data[value]=le.transform(raw_data[value])\nraw_data.head()\n", "intent": "3.Converting a categorical feature\n"}
{"snippet": "drinks = pd.read_csv('http://bit.ly/drinksbycountry')\ndrinks.dtypes\n", "intent": "**Question:** How do I drop all non-numeric columns from a DataFrame?\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_train_all=raw_data_pca[:1460]\nfeature_test=raw_data_pca[1460:]\nX_train, X_test, y_train, y_test = train_test_split(feature_train_all,target_data, test_size = 0.2, random_state = 0)\n", "intent": "4.Shuffle and Split Data\n"}
{"snippet": "from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ncategorical=list(raw_data.select_dtypes(include=['object']).columns.values)\nprint(categorical)\nfor value in categorical:\n    le.fit(raw_data[value])\n    raw_data[value]=le.transform(raw_data[value])\nraw_data.head()\n", "intent": "3.Converting the categorical features\n"}
{"snippet": "from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ncategorical=list(raw_data.select_dtypes(include=['object']).columns.values)\nprint(categorical)\nfor value in categorical:\n    le.fit(raw_data[value])\n    raw_data[value]=le.transform(raw_data[value])\nraw_data.head()\n", "intent": "Converting a categorical feature with labelEncoder\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_train_all=raw_data[:4209]\nfeature_test=raw_data[4209:]\nX_train, X_test, y_train, y_test = train_test_split(feature_train_all,target_data, test_size = 0.2, random_state = 0)\n", "intent": "Shuffle and Split Data\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_train_all=raw_data_pca[:4209]\nfeature_test=raw_data_pca[4209:]\nX_train, X_test, y_train, y_test = train_test_split(feature_train_all,target_data, test_size = 0.2, random_state = 0)\n", "intent": "Shuffle and Split Data\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_train_all=raw_data_ica[:4209]\nfeature_test=raw_data_ica[4209:]\nX_train, X_test, y_train, y_test = train_test_split(feature_train_all,target_data, test_size = 0.2, random_state = 0)\n", "intent": "Shuffle and Split Data\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n", "intent": "1. randomly choose 30% of the data for testing\n2. encode the class\n3. standaridize the features\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test = train_test_split(X, test_size=0.3, random_state=0, stratify=X)\n", "intent": "1. randomly choose 30% of the data for testing\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nStdScaler = StandardScaler()\nX_train_std_sklearn = StdScaler.fit_transform(X_train)\nX_test_std_sklearn = StdScaler.transform(X_test)\n", "intent": "Here we compare the performance of our standard scaler with the StandardScaler in sklearn\n"}
{"snippet": "drinks = pd.read_csv('http://bit.ly/drinksbycountry', dtype={'beer_servings':float})\ndrinks.dtypes\n", "intent": "Documentation for [**`astype`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html)\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nStdScaler = StandardScaler()\nX_train_std_sklearn = StdScaler.fit_transform(X_train)\nX_test_std_sklearn = StdScaler.transform(X_test)\n", "intent": "Here we compare the performance of our StandardScaler with the StandardScaler in sklearn\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"c:\\pydat\\PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\n", "intent": "Find the best model to classify the digits dataset.\nUse a 10-fold cross-validation for the model selection phase.\n"}
{"snippet": "cross_validation.train_test_split(X, y, test_size=0.3)\n", "intent": "Find the best model to classify the digits dataset.\nUse a 10-fold cross-validation for the model selection phase.\n"}
{"snippet": "from sklearn import datasets\nX, _ = datasets.make_blobs(n_samples=150, random_state=8, centers=4)\nplot(X[:, 0], X[:, 1], 'bx')\n", "intent": "Prepare a sinthetic dataset for this example.\n"}
{"snippet": "from sklearn import datasets\nX, _ = datasets.make_circles(n_samples=150, factor=.5, noise=.05)\nplot(X[:, 0], X[:, 1], 'bx')\n", "intent": "Test the different clustering approaches with a \"circles\" dataset.\n"}
{"snippet": "from sklearn import decomposition\nlda = decomposition.LatentDirichletAllocation(n_topics=6)  \nlda.fit(tfidf)\n", "intent": "Compare the results of Nonnegative Matrix Factorization (NMF) with Latent Dirichlet Allocation (LDA).\n"}
{"snippet": "X_train, X_test, y_train, y_test =cross_validation.train_test_split(X,y,test_size = 0.3)\n", "intent": "Preparazione dei test e dei training set\n"}
{"snippet": "db_train = pd.read_csv('CONTEST_TRAINING_SET_PUBBLICO.CSV')\n", "intent": "Importo il dataset di training:\n"}
{"snippet": "ufo['Shape Reported'].fillna(value='VARIOUS', inplace=True)\n", "intent": "Documentation for [**`value_counts`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html)\n"}
{"snippet": "pd.DataFrame(metrics.confusion_matrix(db_result['FLG_DEF_6M'].tolist(),y_test1))\n", "intent": "Provo la bonta' del modello attraverso la confusion matrix:\n"}
{"snippet": "training_data = pd.read_csv(\"CONTEST_TRAINING_SET_PUBBLICO.CSV\",delimiter=\",\",decimal=\".\")\ntest_data = pd.read_csv(\"CONTEST_TEST_SET_PUBBLICO.csv\",delimiter=\",\",decimal=\".\")\nresults_data = pd.read_csv(\"CONTEST_TEST_RESULTS_AN_PRIVATO.csv\",delimiter=\",\",decimal=\".\")\n", "intent": "CARICO I DATASET IMPOSTANDO LE OPZIONI SUL DELIMITER E SUL SEPARATORE DECIMALE \n"}
{"snippet": "digits = datasets.load_digits()\n", "intent": "We will use the digits dataset from [scikit-learn](http://scikit-learn.org/stable/).\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\nprint('Train size: {}'.format(len(X_train)))\nprint('Test size: {}'.format(len(X_test)))\n", "intent": "In order to be able to measure the performance of an estimator, we need to split the data into train and test data sets. Shuffling is not necessary.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_all = data[['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit','temp', 'atemp', 'hum', 'windspeed']]\ny_all = data['cnt']\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, \n    y_all,\n    random_state=1,\n    test_size=0.2)\nprint('Train size: {}'.format(len(X_train)))\nprint('Test size: {}'.format(len(X_test)))\n", "intent": "Prepare train and test data sets.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_all = data[['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit','temp', 'atemp', 'hum', 'windspeed', 'hist']]\ny_all = data['cnt']\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, \n    y_all,\n    random_state=1,\n    test_size=0.2)\nprint('Train size: {}'.format(len(X_train)))\nprint('Test size: {}'.format(len(X_test)))\n", "intent": "Prepare train and test data sets.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "Standardize the features\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_all = data[['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit','temp', 'atemp', 'hum', 'windspeed', 'hist']]\ny_all = data['cnt']\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, \n    y_all,\n    random_state=1,\n    test_size=0.2)\nprint('Train size: {}'.format(len(X_train)))\nprint('Test size: {}'.format(len(X_test)))\n", "intent": "Prepare train and test data sets.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\ntrain_shape = X_train.shape\ntest_shape = X_test.shape\nscaler = StandardScaler()\nscaler.fit(X_train.reshape(train_shape[0]*train_shape[1], train_shape[2]))\nX_train = scaler.transform(X_train.reshape(train_shape[0]*train_shape[1], train_shape[2])).reshape(train_shape)\nX_test = scaler.transform(X_test.reshape(test_shape[0]*test_shape[1], test_shape[2])).reshape(test_shape)\n", "intent": "Standardize the features\n"}
{"snippet": "df = pd.DataFrame({'ID':[100, 101, 102, 103], 'quality':['good', 'very good', 'good', 'excellent']})\ndf\n", "intent": "The **category** data type should only be used with a string Series that has a **small number of possible values**.\n"}
{"snippet": "import lda\nvect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \nsentences_train = vect.fit_transform(sentences)\n", "intent": "What:  Way of automatically discovering topics from sentences\nWhy:   Much quicker than manually creating and identifying topic clusters\n"}
{"snippet": "import pandas\nwheel = pandas.read_csv('wheel.csv')\n", "intent": "The data is in wheel.csv.\nWe would like to understand the relationship between _seconds_ and _signal_\n"}
{"snippet": "confusion_matrix = np.array(confusion_matrix(test_y, Y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(confusion_matrix, index=['over_200k', 'under_200k'],\n                         columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "fare_pipe = make_pipeline(ColumnSelector('Fare'),\n                          StandardScaler())\nfare_pipe.fit_transform(df.head())\n", "intent": "The `Fare` attribute can be scaled using one of the scalers from the preprocessing module. \n"}
{"snippet": "union = make_union(age_pipe,\n                   one_hot_pipe,\n                   gender_pipe,\n                   fare_pipe)\nunion.fit_transform(df.head())\n", "intent": "Use the `make_union` function from the `sklearn.pipeline` module to combine all the pipes you have created.\n"}
{"snippet": "df = pd.DataFrame(data=adult, columns=['workclass', 'education-num', 'hours-per-week', 'income'])\ndf.head(n=5)\n", "intent": "Convert the data to a Pandas dataframe\n"}
{"snippet": "df_new = pd.DataFrame(data=df, columns=['workclass_num', 'education-num', 'hours-per-week', 'income_num'])\ndf_new2 = pd.DataFrame(data=df, columns=['workclass_num', 'education-num'])\ndf_new.head(n=5)\n", "intent": "Create a New Dataframe with just numerical data\n"}
{"snippet": "iris = pd.read_csv('../../assets/datasets/iris.csv')\n", "intent": "First, let's import our data. We'll be using *Iris* again for this exercise. \n"}
{"snippet": "x_standard = StandardScaler().fit_transform(X)\n", "intent": "First, let's standardize the data\n"}
{"snippet": "test = pd.read_csv('http://bit.ly/kaggletest')\ntest.head()\n", "intent": "**Video series:** [Introduction to machine learning with scikit-learn](https://www.youtube.com/playlist?list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A)\n"}
{"snippet": "iris = pd.read_csv('iris.csv')\n", "intent": "First, let's import our data. We'll be using *Iris* again for this exercise. \n"}
{"snippet": "PCA_set = PCA(n_components=5)\nY = PCA_set.fit_transform(xStand)\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "votes = pd.read_csv('../assets/datasets/votes.csv', index_col=0)\nvotes.fillna('n', inplace=True)\nvotes\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "X_standard = StandardScaler().fit_transform(X)\n", "intent": "Next, create the covariance matrix from the standardized x-values and decompose these values to find the eigenvalues and eigenvectors\n"}
{"snippet": "pcask = PCA(n_components=5)\ny_sk = pcask.fit_transform(X_standard)\ny_sk\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "airports = pd.read_csv('../../assets/datasets/airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "xStand = StandardScaler().fit_transform(x)\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "Ydf = pd.DataFrame(Y, columns=[\"PC1\", \"PC2\"])\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "airports = pd.read_csv('../../assets/datasets/airport_operations.csv')\nairports.head()\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "pd.DataFrame({'PassengerId':test.PassengerId, 'Survived':new_pred_class}).set_index('PassengerId').head()\n", "intent": "Documentation for the [**`DataFrame`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) constructor\n"}
{"snippet": "airports = pd.read_csv('airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "We're going to load the iris data from the scikit \"datasets\" package\n"}
{"snippet": "X, y = load_iris().data, load_iris().target\n", "intent": "Define your \"X\" and \"y\" variables for the analysis\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_stand, y, test_size=0.3, random_state = 42)\n", "intent": "Split the data in the ordinary way, making sure you have a 70/30 split.\n"}
{"snippet": "the_normalizer(v_merge_re['comments'][5])\n", "intent": "Test out your normalizer on a comment to ensure it's working. \n"}
{"snippet": "feature_set = []\nlen1 = v_merge_re[\"comments\"].size\nfor i in xrange(0,len1):\n    feature_set.append(the_normalizer(v_merge_re['comments'][i]))\n", "intent": "Use the normalizer that you created to build a 'stripped' down comments array/vector/list (however you did it)\n"}
{"snippet": "normalizer(comment)\n", "intent": "Test out your normalizer on a comment to ensure it's working. \n"}
{"snippet": "iris_train_1_2 = iris_train.loc[iris_train['Species'].isin(['versicolor','virginica'])]\nx_1_2 = iris_train_1_2.iloc[:,:4]\ny_1_2 = iris_train_1_2.iloc[:,4]\nx_1_2 = StandardScaler().fit_transform(x_1_2)\n", "intent": "**Fitting Fisher projection by discriminating classes 1 and 2**\n"}
{"snippet": "x_test = StandardScaler().fit_transform(iris_test.iloc[:,:4])\ny_test_3_4 = iris_test.apply(lambda row : return_class(row),axis=1)\n", "intent": "**Projecting test data to above two projections**\n"}
{"snippet": "ufo = pd.read_csv('http://bit.ly/uforeports')\nufo.head()\n", "intent": "**Question:** What is the difference between **`ufo.isnull()`** and **`pd.isnull(ufo)`**?\n"}
{"snippet": "np.sum(img_to_array(img0)* img_to_array(img0_))\n", "intent": "- Compute these quantities to verify your answer\n"}
{"snippet": "import numpy as np\nfrom keras.datasets import mnist\nfrom keras.preprocessing.image import array_to_img, img_to_array\nimport keras as keras\n[X, y], _ = mnist.load_data()\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nX = np.expand_dims(X, axis=1).astype(np.float)\nimgs = [array_to_img(x, data_format='channels_first') for x in X]\nX = np.array([img_to_array(img, data_format='channels_last') for img in imgs]) / 255. \ny = np.array([img_to_array(img, data_format='channels_last') for img in imgs]) / 255\n", "intent": "Since tensorflow expects images with shape `(height, width, channels)` we need to convert the dimensions of the MNIST images.\n"}
{"snippet": "import numpy as np\nfrom keras.datasets import mnist\nfrom keras.preprocessing.image import array_to_img, img_to_array\n[X_mnist, y], _ = mnist.load_data()\nX_mnist = X_mnist.astype(np.float) / 255.\nX = np.expand_dims(X_mnist, axis=-1)\nX.shape\n", "intent": "Since tensorflow expects images with shape `(height, width, channels)` we need to convert the dimensions of the MNIST images.\n"}
{"snippet": "import pandas as pd\nY = np.zeros([nb_train, 10])\nfor i, (d1, d2) in enumerate(y):\n    Y[[i, i], [d1, d2]] = 1\nX_train, Y_train = X[:-nb_train//10], Y[:-nb_train//10]\nX_val, y_val = X[-nb_train//10:], y[-nb_train//10:]\nhistory = model.fit(X_train, Y_train, callbacks=[ThreeAccuracyCallback(X_val, y_val)])\npd.DataFrame(history.history).plot();\n", "intent": "The following code fits a multi-label logistic regression model to the multi-digit MNIST images and uses an `AccuracyCallback` for evaluation.\n"}
{"snippet": "import numpy as np\nfrom keras.datasets import mnist\nfrom keras.preprocessing.image import array_to_img, img_to_array\n[X, y], _ = mnist.load_data()\nX = np.expand_dims(X, axis=1).astype(np.float)\nimgs = [array_to_img(x, data_format='channels_first') for x in X]\nX = np.array([img_to_array(img, data_format='channels_last') for img in imgs]) / 255.\n", "intent": "Since tensorflow expects images with shape `(height, width, channels)` we need to convert the dimensions of the MNIST images.\n"}
{"snippet": "data_raw = pd.read_csv('data/adult.data.txt')\ndata_raw.head(20)\n", "intent": "Let us import and look at the data.\n"}
{"snippet": "display(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2']))\n", "intent": "Let's now see how the log-transformed sample data has changed after having a PCA transformation applied to it using only two dimensions.\n"}
{"snippet": "from sklearn import cross_validation\nfrom sklearn.linear_model import LinearRegression\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size = 0.2)\nregr = LinearRegression()\nregr.fit(X_train, y_train)\naccuracy = regr.score(X_test, y_test)\naccuracy\n", "intent": "Here we will partition up the training and testing sets as well as fit our model.\n"}
{"snippet": "count_vectorizer_100 = CountVectorizer(max_features=100, stop_words='english')\n", "intent": "Okay, it's **far** too big to even look at. Let's try to get a list of features from a new `CountVectorizer` that only takes the top 100 words.\n"}
{"snippet": "train = pd.read_csv('http://bit.ly/kaggletrain')\ntrain.head()\n", "intent": "Documentation for [**`concat`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html)\n"}
{"snippet": "idf_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\nTop_100_tokens_idf = idf_vectorizer.fit_transform(All_speeches)\nidf_df = pd.DataFrame(Top_100_tokens_idf.toarray(), columns=idf_vectorizer.get_feature_names())\nidf_df['china trade'] = idf_df['china'] + idf_df['trade']\n", "intent": "Now what if I'm using a `TfidfVectorizer`?\n"}
{"snippet": "meta = pd.read_csv('./data_v_7_stc/meta/meta.txt', sep='\\t', header=None, names=['name', 'class'], usecols=[0, 4], index_col=0)\nle = LabelEncoder()\nmeta['class'] = le.fit_transform(meta['class'])\nmeta.head()\n", "intent": "Load train database\n"}
{"snippet": "training_set = pd.read_csv('./Boltzmann_Machines/ml-100k/u1.base',\n                           delimiter='\\t', names=['UserID','MovieID','Rating','Timestamp'])\nprint training_set.shape\ntraining_set.head()\n", "intent": "* from 100k dataset\n"}
{"snippet": "training_set = pd.read_csv('./ml-100k/u1.base',\n                           delimiter='\\t', names=['UserID','MovieID','Rating','Timestamp'])\nprint training_set.shape\ntraining_set.head()\n", "intent": "* from 100k dataset\n"}
{"snippet": "df = pd.read_csv('predict/bidder1_gender_predict.csv')\nprint df.shape\ndone1 = df['file']\ndf2 = pd.read_csv('predict/bidder1_gender_diff_predict_1.csv')\nprint df2.shape\ndone2 = df2['file']\nshould = pd.read_fwf('bidder1.txt',names =['url'])\nwanted=should['url']\ndiff = list(set(wanted)-set(done1)-set(done2))\nlen(diff)\n", "intent": "CPU times: user 2min 18s, sys: 17 s, total: 2min 35s\nWall time: 1h 34min 33s\n"}
{"snippet": "df = pd.read_csv('bidder1_gender_predict.csv')\ndf.shape\n", "intent": "CPU times: user 2min 18s, sys: 17 s, total: 2min 35s\nWall time: 1h 34min 33s\n"}
{"snippet": "df = pd.read_csv('boss1_gender_predict.csv')\ndf.shape\n", "intent": "CPU times: user 2min 18s, sys: 17 s, total: 2min 35s\nWall time: 1h 34min 33s\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(df, mark, test_size=0.25, random_state=0)\ndim(X_train, X_test, y_train, y_test)\n", "intent": "* train on 75% of and test on the remaining 25% of the data\n"}
{"snippet": "cols = ['f1', 'f2', 'f3', 'f4', 'species']\ndata = pd.read_csv('../data/iris.csv', names=cols)\n", "intent": "This portion of the notebook is a carry over from the \"example1_\" notebook.\n"}
{"snippet": "pd.DataFrame([[100, 'red'], [101, 'blue'], [102, 'red']], columns=['id', 'color'])\n", "intent": "Documentation for [**`DataFrame`**](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n"}
{"snippet": "f = z.open('ml-1m/movies.dat')\ntmp = f.read()\ntmp = tmp.decode('latin')\nmovies = np.array([np.array(t.split('::')) for t in tmp.splitlines()])\nf.close()\n", "intent": "Movies' information\n* MovieID, Title, Genres\n"}
{"snippet": "f = z.open('ml-1m/ratings.dat')\ntmp = f.read()\ntmp = tmp.decode()\nratings = np.array([np.array(t.split('::')) for t in tmp.splitlines()])\nratings = ratings.astype(np.float)\nratings[:, 2] /= 5\nf.close()\n", "intent": "Ratings' information\n* UserID, MovieID, Rating, Timestamp\n"}
{"snippet": "print('\nnow = datetime.now()\noof_result = pd.DataFrame(avreal, columns=['target'])\noof_result['prediction'] = avpred\noof_result['id'] = avids\noof_result.sort_values('id', ascending=True, inplace=True)\noof_result = oof_result.set_index('id')\nsub_file = 'train_5fold-keras-run-01-v1-oof_' + str(score) + '_' + str(now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\nprint('\\n Writing out-of-fold file:  %s' % sub_file)\noof_result.to_csv(sub_file, index=True, index_label='id')\n", "intent": "Save the file with out-of-fold predictions. For easier book-keeping, file names have the out-of-fold gini score and are are tagged by date and time.\n"}
{"snippet": "result = pd.DataFrame(mpred, columns=['target'])\nresult['id'] = te_ids\nresult = result.set_index('id')\nprint('\\n First 10 lines of your 5-fold average prediction:\\n')\nprint(result.head(10))\nsub_file = 'submission_5fold-average-keras-run-01-v1_' + str(score) + '_' + str(now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\nprint('\\n Writing submission:  %s' % sub_file)\nresult.to_csv(sub_file, index=True, index_label='id')\n", "intent": "Save the final prediction. This is the one to submit.\n"}
{"snippet": "data = pd.read_csv('../../data/aquastat/aquastat.csv.gzip', compression='gzip')\ndata.region = data.region.apply(lambda x: simple_regions[x])\ndata = data.loc[~data.variable.str.contains('exploitable'),:]\ndata = data.loc[~(data.variable=='national_rainfall_index')]\n", "intent": "http://www.fao.org/nr/water/aquastat/data/query/index.html\n"}
{"snippet": "from sklearn.datasets import load_digits\nimport seaborn as sns\ndataset = load_digits()\nX, y = dataset.data, dataset.target\n", "intent": "What are the difficult digits to see?\n"}
{"snippet": "x = df.CRIM.values.reshape(-1,1)\ny = y\nX_train, X_test, y_train, y_test = train_test_split(x, y)\n", "intent": "- `train_test_split`\n- `mean_squared_error`\n- `cross_val_score`\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\ndf = pd.read_csv('data/titanic.csv')\ninclude = ['cclass', 'sex', 'age', 'fare', 'sibsp', 'survived']\ndf['sex'] = df['sex'].apply(lambda x: 0 if x == 'male' else 1)\ndf = df[include].dropna()\nX = df[['pclass', 'sex', 'age', 'fare', 'sibsp']]\ny = df['survived']\nPREDICTOR = RandomForestClassifier(n_estimators=100).fit(X, y)\n", "intent": "Our classifier algorithm will be a random forest, which as you know is relatively slow to train.\n"}
{"snippet": "import pandas as pd\nyelp = pd.read_csv('../data/yelp.csv')\nyelp.head(3)\n", "intent": "Read `yelp.csv` into a DataFrame.\n"}
{"snippet": "for num_feats in np.arange(1,65, dtype = int):\n    pca = PCA(n_components = num_feats)\n    pca.fit(digits.data)\n    variance_explained = sum(pca.explained_variance_ratio_)\n    if variance_explained >= .90:\n        break\nprint(\"{:d} features are needed to explain 90% of the variance\".format(num_feats))\n", "intent": "**Problem 2e** How many components are needed to explain 90% of the variance in the data.\n"}
{"snippet": "training_data, validation_data, test_data = load_data()\n", "intent": "Let's see how the data looks:\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n", "intent": "Let's load the MNIST dataset from `keras.datasets`. The download may take a few minutes.\n"}
{"snippet": "from sklearn.feature_selection import RFE\nclf_rf_3 = RandomForestClassifier()      \nrfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)\nrfe = rfe.fit(x_train, y_train)\n", "intent": "Like previous method, we will use 5 features. However, which 5 features will we use ? We will choose them with RFE method.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=27)\npca.fit(X_train)\n", "intent": "There are 784 features, it will be difficult and time consuming task to reduce the features using p-value and VIF score.\nSo let's use PCA to do that.\n"}
{"snippet": "pcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':X_colnames})\npcs_df.head()\n", "intent": "- We'll plot original features on the first 2 principal components as axes\n"}
{"snippet": "del X\nX= train[:6000].values\ndel train\nX_std = StandardScaler().fit_transform(X)\npca = PCA(n_components=5)\npca.fit(X_std)\nX_5d = pca.transform(X_std)\nTarget = target[:6000]\n", "intent": "Now using the Sklearn toolkit, we implement the Principal Component Analysis algorithm as follows:\n"}
{"snippet": "pca = PCA(svd_solver='randomized', random_state=27)\npca.fit(X_train)\n", "intent": "There are 154 features, it will be difficult and time consuming task to reduce the features using p-value and VIF score.\nSo let's use PCA to do that.\n"}
{"snippet": "pca_final = IncrementalPCA(n_components=19)\nX_train_pca = pca_final.fit_transform(X_train)\nX_test_pca = pca_final.transform(X_test)\nX_train_pca.shape\n", "intent": "- We'll choose 19 components for our modeling\nActual Dimensions 89, after PCA, 19\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(twenty_train.data)\nX_train_counts.shape\n", "intent": "CountVectorizer segment each text file into words (for English splitting by space), and count \n"}
{"snippet": "X_pca_train = pca.fit_transform(X_train)\nX_pca_test = pca.transform(X_test)\nRFmod = RandomForestClassifier()\npars = {\"n_estimators\": [10, 100, 300],\n        \"max_features\": [1, 2], \n        \"min_samples_leaf\": [1,10]}\ngrid_results = GridSearchCV(RandomForestClassifier(), \n                            pars,\n                            cv = 5)\ngrid_results.fit(X_pca_train, y_train)\n", "intent": "**Exercise 5**: Re-do the classification on the PCA components instead of the original features.\n"}
{"snippet": "scaler_a = p.StandardScaler()\nscaler_c = p.StandardScaler()\nscaler_a.fit(azdias_10)\nscaler_c.fit(customers_10)\nazdias_ss = pd.DataFrame(scaler_a.transform(azdias_10), columns = azdias_10.columns.values)\ncustomers_ss = pd.DataFrame(scaler_c.transform(customers_10), columns = customers_10.columns.values)\n", "intent": "Use StandardScaler module scales each feature to mean 0 and standard deviation 1.\n"}
{"snippet": "var_list = list()\ncomp_list= list()\nfor _ in [100, 150, 200, 250, 300, 350, 400, 450,  500, 550, 600, 700, 800, 860]:\n    pca = PCA(_)\n    X_pca = pca.fit_transform(customers_ss)\n    var = np.sum(pca.explained_variance_ratio_)\n    var_list.append(var)\n    comp_list.append(_)\n", "intent": "Main idea is to find balance between reduced number of features and remaining variance.\n"}
{"snippet": "centers = pca_a.inverse_transform(kmeans.cluster_centers_)\ndf_temp = pd.DataFrame(centers[8,:])\ndf_temp.columns= [\"Weight\"]\ndf_temp[\"Column_name\"] = azdias_ss.columns.values\ndf_temp.nlargest(10, 'Weight')\n", "intent": "This cluster indicates that customers of a mail-order sales company are:\n* `LP_LEBENSPHASE_FEIN_income_high` wealthy people,\n*\n"}
{"snippet": "centers = pca_a.inverse_transform(kmeans.cluster_centers_)\ndf_temp = pd.DataFrame(centers[14,:])\ndf_temp.columns= [\"Weight\"]\ndf_temp[\"Column_name\"] = azdias_ss.columns.values\ndf_temp.nlargest(10, 'Weight')\n", "intent": "This cluster indicates that customers of a mail-order sales company are:\n* \n"}
{"snippet": "df_attr = pd.read_csv('info_attributes_processed.csv', sep=',', low_memory=False, index_col=0)\ndf_attr.head()\n", "intent": "Helper DataFrame with attribute descriptions\n"}
{"snippet": "ord_col_names = [_[0] for _ in pd.read_csv(\"ordinal_names.csv\", header=None, index_col=0).values]\nord_col_names = set(ord_col_names)\n", "intent": "Intentify columns with ordinal data using information from part 1.\n"}
{"snippet": "bi_cat_col_names = [_[0] for _ in pd.read_csv(\"binary_categorical_names.csv\", header=None, index_col=0).values]\nbi_cat_col_names = set(bi_cat_col_names)\nbi_cat_col_names.remove(\"KBA05_SEG6\")\n", "intent": "First encode binary categorical data.\n"}
{"snippet": "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\ntrain_values = imp.fit_transform(X_train)\ntest_values = imp.fit_transform(X_test)\n", "intent": "Because less than 1% of data is missing, I decided to simply impute missing values with averages for each column with missing values.\n"}
{"snippet": "scaler = p.StandardScaler()\nscaler.fit(X_train)\nX_train_ss = pd.DataFrame(scaler.transform(X_train), columns = col_order)\nX_test_ss = pd.DataFrame(scaler.transform(X_test), columns = col_order)\n", "intent": "Use StandardScaler module scales each feature to mean 0 and standard deviation 1.\n"}
{"snippet": "from sklearn.pipeline import FeatureUnion\ntransformers = [(\"pca\", PCA(n_components=2)),\n                (\"pt\", PSFMagThreshold(p=1.45))]\nfeat_union = FeatureUnion(transformers)\nX_transformed = feat_union.fit_transform(X_train)\n", "intent": "Now let's make a feature set that combines this feature with the PCA features:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_col_names = ['num_preg', 'glucose_conc', 'diastolic_bp', 'thickness', 'insulin', 'bmi', 'diab_pred', 'age']\npredicted_class_names = ['diabetes']\nX = df[feature_col_names].values     \ny = df[predicted_class_names].values \nsplit_test_size = 0.30\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_test_size, random_state=42) \n", "intent": "70% for training, 30% for testing\n"}
{"snippet": "df_mm = p.MinMaxScaler().fit_transform(df)\n", "intent": "`3.` Now it's your turn.  Try fitting the **MinMaxScaler** transformation to this dataset. You should be able to use the previous example to assist.\n"}
{"snippet": "X = tfidf_vect.fit_transform(dta.violations)\n", "intent": "Notice here that we can combine the fitting and the transformation by taking advantage of the `fit_transform` method.\n"}
{"snippet": "movie_df = pd.read_csv('imdb.csv', delimiter='\\t')\nmovie_df.head()\n", "intent": "Import the IMDb data.\n"}
{"snippet": "df = pd.DataFrame(X_r, columns=['PC1', 'PC2'])\ndf.head()\ndf['species'] = Y\ndf.head()\n", "intent": "Now we can assemble the two components and the `species` column into a DataFrame.\n"}
{"snippet": "n_components=6\npca = PCA(n_components=n_components)\n", "intent": "Set the number of components to 6:\n"}
{"snippet": "(input_train, input_test, labels_train, labels_test) = train_test_split(input_, labels_, test_size=0.1)\n", "intent": "Now, let us split our data into train and test set.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\ndata.head()\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"/Users/christyhe/Documents/Data_Scientist/DataScience-Python3/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.ensemble import RandomForestClassifier\niris = datasets.load_iris()\nRFclf = RandomForestClassifier().fit(iris.data, iris.target)\n", "intent": "To test the slides on your local machine you need to run the following: \n    ipython nbconvert Session2FinalProject.ipynb --to slides --post serve\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(kick[numerical_columns], response, test_size = 0.3, random_state=465)\n", "intent": "Train test split after only considering the columns extracted above\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(kick[numerical_columns],\n                                                    response, random_state=143)\n", "intent": "Train test split after only considering the columns extracted above\n"}
{"snippet": "titanic_df = pd.read_csv('/Users/avkashchauhan/learn/seattle-workshop/titanic_list.csv')\n", "intent": "http://www.titanicfacts.net/titanic-passengers.html\nTotal Passangers: 1317\nDetails:\nhttps://blog.socialcops.com/engineering/machine-learning-python/\n"}
{"snippet": "titanic_df = pd.read_csv('/Users/avkashchauhan/learn/comcast/titanic_list.csv')\n", "intent": "http://www.titanicfacts.net/titanic-passengers.html\nTotal Passangers: 1317\nDetails:\nhttps://blog.socialcops.com/engineering/machine-learning-python/\n"}
{"snippet": "data = pd.read_csv('train.csv')\nX = data[['X']].as_matrix()\ny = data['y'].as_matrix()\nX.shape, y.shape\n", "intent": "Let's load in the data:\n"}
{"snippet": "df=pd.read_csv(\"religion.csv\")\ndf.head()\n", "intent": "Let us assume that we have a \"population\" of 200 counties $x$:\n"}
{"snippet": "def make_features(train_set, test_set, degrees):\n    traintestlist=[]\n    for d in degrees:\n        traintestdict={}\n        traintestdict['train'] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n        traintestdict['test'] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n        traintestlist.append(traintestdict)\n    return traintestlist\n", "intent": "Lets put this alltogether. Below we create multiple datasets, one for each polynomial degree:\n"}
{"snippet": "CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\nFEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS)-1]\nTARGET = CSV_COLUMNS[0]\ndf_train = pd.read_csv('./taxi-train.csv', header=None, names=CSV_COLUMNS)\ndf_valid = pd.read_csv('./taxi-valid.csv', header=None, names=CSV_COLUMNS)\n", "intent": "Read data created in the previous chapter.\n"}
{"snippet": "advertising = pd.read_csv('Data/Advertising.csv', usecols=[1,2,3,4])\nadvertising.info()\n", "intent": "Datasets available on http://www-bcf.usc.edu/~gareth/ISL/data.html\n"}
{"snippet": "train_df = pd.read_csv(\"training_sources.csv\")\nX_train = np.array(train_df[[\"mean\", \"nobs\", \"duration\"]])\ny_train = np.array(train_df[\"Class\"])\n", "intent": "**Problem 2a**\nRead in the training set file, and create a feature vector `X` and label array `y`.\n"}
{"snippet": "print(X_reduced.shape)\npd.DataFrame(X_reduced).loc[:4,:5]\n", "intent": "The above loadings are the same as in R.\n"}
{"snippet": "df = pd.read_csv('Data/Wage.csv')\ndf.head(3)\n", "intent": "Using write.csv in R, I exported the dataset from package 'ISLR' to a csv file.\n"}
{"snippet": "X1 = PolynomialFeatures(1).fit_transform(df.age.values.reshape(-1,1))\nX2 = PolynomialFeatures(2).fit_transform(df.age.values.reshape(-1,1))\nX3 = PolynomialFeatures(3).fit_transform(df.age.values.reshape(-1,1))\nX4 = PolynomialFeatures(4).fit_transform(df.age.values.reshape(-1,1))\nX5 = PolynomialFeatures(5).fit_transform(df.age.values.reshape(-1,1))\ny = (df.wage > 250).map({False:0, True:1}).values\nprint('X4:\\n', X4[:5])\nprint('y:\\n', y[:5])\n", "intent": "Create polynomials for 'age'. These correspond to those in R, when using raw=TRUE in poly() function.\n"}
{"snippet": "df = pd.read_csv('Data/Hitters.csv').dropna()\ndf.info()\n", "intent": "In R, I exported the dataset from package 'ISLR' to a csv file.\n"}
{"snippet": "df2 = pd.read_csv('Data/Heart.csv').drop('Unnamed: 0', axis=1).dropna()\ndf2.info()\n", "intent": "Dataset available on http://www-bcf.usc.edu/~gareth/ISL/data.html\n"}
{"snippet": "df3 = pd.read_csv('Data/Carseats.csv').drop('Unnamed: 0', axis=1)\ndf3.head()\n", "intent": "In R, I exported the dataset from package 'ISLR' to a csv file.\n"}
{"snippet": "boston_df = pd.read_csv('Data/Boston.csv')\nboston_df.info()\n", "intent": "In R, I exported the dataset from package 'MASS' to a csv file.\n"}
{"snippet": "X_train = pd.read_csv('Data/Khan_xtrain.csv').drop('Unnamed: 0', axis=1)\ny_train = pd.read_csv('Data/Khan_ytrain.csv').drop('Unnamed: 0', axis=1).as_matrix().ravel()\nX_test = pd.read_csv('Data/Khan_xtest.csv').drop('Unnamed: 0', axis=1)\ny_test = pd.read_csv('Data/Khan_ytest.csv').drop('Unnamed: 0', axis=1).as_matrix().ravel()\n", "intent": "In R, I exported the dataset from package 'ISLR' to csv files.\n"}
{"snippet": "labels = pd.read_csv(\"b37d3960-6909-472b-9ce1-c33b07dbdb66.csv\")\nids = labels.id.tolist()\nlabels = labels.genus.tolist()\nfilenames = [\"%s.jpg\" % id for id in ids]\nX = load_bee_images(\"data/train\", filenames)\nX = prep_images(X)\n", "intent": "Load images and labels for model training\n"}
{"snippet": "test_df = pd.read_csv(\"test_sources.csv\")\nX_test = np.array(test_df[[\"mean\", \"nobs\", \"duration\"]])\ny_test = np.array(test_df[\"Class\"])\n", "intent": "You can load the test set using the commands below.\n"}
{"snippet": "def convert_timedelta_to_float(df, feature):\n    lst = []\n    for i in df[feature]:\n        lst.append(i.total_seconds())\n    new_feature = pd.DataFrame(lst, index=df.index).rename(columns={0:feature})\n    return new_feature\nfor col in tdeltas:\n    df[col] = convert_timedelta_to_float(df, col)\ndf.head()\n", "intent": "First I'll need to convert the timedeltas to a float so I can further analyze the timestamp data.\n"}
{"snippet": "scaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df_2)\n", "intent": "We need to normalize data first\n"}
{"snippet": "PATH_TO_DATA = ('/Users/owner/data/project_alice')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "vcrz = CountVectorizer(ngram_range=(1,3), max_features=100000)\ncounts_train = vcrz.fit_transform(full_df['str0'][:idx_train].ravel(), y) \ncounts_test = vcrz.fit_transform(full_df['str0'][idx_train:].ravel(), y) \n", "intent": "Build Tf-Idf features based on sites. You can use `ngram_range`=(1, 3) and `max_features`=100000 or more\n"}
{"snippet": "pipeline = Pipeline([(\"vectorize\", TfidfVectorizer(ngram_range=(1, 3), max_features=100000)), (\"tfidf\", TfidfTransformer())])\npipeline.fit(full_df['str'][:idx].ravel(),y)\nX_train = pipeline.transform(full_df['str'][:idx].ravel())\nX_test = pipeline.transform(full_df['str'][idx:].ravel())\nfor C in range(3,6):\n    print(C, get_auc_lr_valid(X_train, y, C=C))\n", "intent": "Perform cross-validation with logistic regression.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nstandardScaler = StandardScaler()\nX = standardScaler.fit_transform(X[:,1:])  \ny = standardScaler.fit_transform(y)\n", "intent": "Whoa!  What happened??\nThis illustrates an issue with Gradient Descent.  Not scaling of variables can wreak havoc on the optimization\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nstandardScaler = StandardScaler()\nX = standardScaler.fit_transform(X[:,1:])\nX = np.matrix(X)\nX = sma.add_constant(X)\nlogistic_regression = GradientDescent(BCE, tolerance=1e-7)\nb = logistic_regression.optimize(X,g,learning_rate=10) \nprint(\"iterations: {}\".format(logistic_regression.iter))\nprint(logistic_regression.beta.T)\nBCE.valueAt(X,g,b)\n", "intent": "That's alot of iterations!  We can improve it by standardizing the features and playing with the learning rate\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nstandardScaler = StandardScaler()\nX = standardScaler.fit_transform(X[:,1:])  \ny = standardScaler.fit_transform(y)\n", "intent": "Estimate the regression with regularization\n"}
{"snippet": "X = standardScaler.fit_transform(X[:,1:])\nX = np.matrix(X)\nX = sma.add_constant(X)\nlogistic_regression = GradientDescent(BCE, tolerance=1e-7)\nb = logistic_regression.optimize(X,g,learning_rate=10) \nprint(\"iterations: {}\".format(logistic_regression.iter))\nprint(logistic_regression.beta.T)\nBCE.valueAt(X,g,b)\n", "intent": "That's alot of iterations!  We can improve it by standardizing the features and playing with the learning rate\n"}
{"snippet": "(x_train_temp, y_train_temp), (x_test_temp, y_test_temp) = mnist.load_data()\n", "intent": "(ooh look it's all stored on Amazon's AWS!)\n(pssst, we're in the cloooud, it's the future!)\n"}
{"snippet": "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.4,random_state=20)\n", "intent": "Before we build our model, lets generate train/test splits of our data:\n"}
{"snippet": "cement_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/slump_test.data\",index_col=0)\ncement_data.columns = cement_data.columns.str.lower().str.replace(\" \",\"_\").str.replace(\".\",\"\")\n", "intent": "Lets get the data directly off the web this time:\n"}
{"snippet": "special_missing_category = kidney_data[kidney_columns[14:-1]].fillna(\"missing\")\nspecial_missing_category.head()\n", "intent": "Now let's do **2.** (Which is much easier):\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target  \n", "intent": "- 50 samples of 3 different species of iris (150 samples total)\n- Measurements: sepal length, sepal width, petal length, petal width\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "- 50 samples of 3 different species of iris (150 samples total)\n- Measurements: sepal length, sepal width, petal length, petal width\n"}
{"snippet": "sc = StandardScaler()\nhousing_data_scaled  = housing_data.copy()\nhousing_data_scaled[housing_columns[:-1]]  = sc.fit_transform(housing_data[housing_columns[:-1]])\nprint(housing_data_scaled.shape)\nhousing_data_scaled.head()\n", "intent": "And normalizing it per-column:\n"}
{"snippet": "sc = StandardScaler()\nhousing_data_scaled  = housing_data.copy()\nhousing_data_scaled[housing_columns[:-1]]  = sc.fit_transform(housing_data[housing_columns[:-1]])\n", "intent": "And normalizing it per-column:\n"}
{"snippet": "vectorizer = CountVectorizer(ngram_range=(1,6))\ntransformed_text = vectorizer.fit_transform(some_text.text)\nprint(transformed_text)\n", "intent": "Think of it as large-scale one-hot encoding with some catches:\n"}
{"snippet": "vectorizer = CountVectorizer(ngram_range=(1,6))\ntransformed_text = vectorizer.fit_transform(some_text.text)\n", "intent": "Think of it as large-scale one-hot encoding with some catches:\n"}
{"snippet": "(x_train_temp, y_train_temp), (x_test_temp, y_test_temp) = mnist.load_data()\n", "intent": "(ooh look it's all stored on Amazon's AWS!)\n(pssst, we're in the cloooud)\n"}
{"snippet": "scaled = pd.DataFrame(scaled,columns=df.columns.values[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data = pd.read_csv('College_Data',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "final_zips = final_filtered.sort_values(\"zip\").zip.unique()\nfinal_kmeans_labels = final_kmeans.labels_\nfinal_dbscan_labels = final_dbscan.labels_\nto_merge = pd.DataFrame({\"ZIPCODE\": final_zips, \"kmeans\": final_kmeans_labels, \"dbscan\": final_dbscan_labels})\nzips_with_labels = zipcodes.merge(to_merge, on=\"ZIPCODE\").drop_duplicates(subset=[\"ZIPCODE\"])\nax = zips_with_labels.plot('kmeans', figsize=(16, 9), legend=True,categorical=True)\nax.get_xaxis().set_visible(False)\nax.get_yaxis().set_visible(False)\n", "intent": "    5. overlay your data on a NYC map: you can use shapefiles for the zip codes and different colors for different clusters\n"}
{"snippet": "final_zips = final_filtered.sort_values(\"zip\").zip.unique()\nfinal_kmeans_labels = final_kmeans.labels_\nfinal_dbscan_labels = final_dbscan.labels_\nto_merge = pd.DataFrame({\"ZIPCODE\": final_zips, \"kmeans\": final_kmeans_labels, \"dbscan\": final_dbscan_labels})\nzips_with_labels = zipcodes.merge(to_merge, on=\"ZIPCODE\").drop_duplicates(subset=[\"ZIPCODE\"])\nax = zips_with_labels.plot('kmeans', cmap='coolwarm', figsize=(16, 9), legend=True,categorical=True)\nax.get_xaxis().set_visible(False)\nax.get_yaxis().set_visible(False)\n", "intent": "    5. overlay your data on a NYC map: you can use shapefiles for the zip codes and different colors for different clusters\n"}
{"snippet": "dir_test = 'datasets/energy_efficiency_test.csv'\ndf_test = pd.read_csv(dir_test, index_col=False)\n", "intent": "Run the following to load the test set into a pandas dataframe.\n"}
{"snippet": "df_training = pd.read_csv(dir_training)\n", "intent": "First let's load the dataset into a pandas dataframe. [(Hint)](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)\n"}
{"snippet": "df = pandas.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "df_feat = pandas.DataFrame(scaled_features,columns = df.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "ad_data = pandas.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "from astropy.table import Table\nTable.read( \n", "intent": "**Problem 1b**\nUse [`Table.read()`](http://docs.astropy.org/en/stable/api/astropy.table.Table.html\n"}
{"snippet": "advert = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\nadvert.head()\n", "intent": "Let's compute these evaluation metrics for the linear model we have developed for our advertising dataset\n"}
{"snippet": "hr = pd.read_csv(\"data/HR_comma_sep.csv\")\nhr.head()\n", "intent": "We will revisit the HR dataset and see if we can use KNN to predict whether an employee is likely to leave the company\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"}
{"snippet": "of_df = pd.read_csv(\"data/old-faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "data = pd.read_csv(\"../../lesson-11/code/data/stumbleupon.tsv\", sep='\\t', encoding=\"utf-8\")\ndata['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\ndata['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n", "intent": "Importing the stumbleupon dataset from last week\n"}
{"snippet": "titles = data['title'].fillna('')\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features = 1000, \n                             ngram_range=(1, 2), \n                             stop_words='english',\n                             binary=False)\nvectorizer.fit(titles)\nX = vectorizer.transform(titles)\n", "intent": " We previously used the Count Vectorizer to extract text features for this classification task\n"}
{"snippet": "df = pd.read_csv(\"../assets/titanic.csv\")\n", "intent": "- Include visualizations, descriptive statistics, etc.\n"}
{"snippet": "scaler = preprocessing.StandardScaler()\n", "intent": "- Depending on the classification model we're using, we may want to scale our features (in my case, only Age)\n"}
{"snippet": "import pandas as pd\nfrom sklearn import datasets\niris = datasets.load_iris()\nprint(iris.data.shape)\nprint(iris.target.shape)\n", "intent": "<b>Shape and representation<b>\n"}
{"snippet": "sdss = pd.read_csv(\"DSFP_SDSS_spec_train.csv\")\nsdss[:5]\n", "intent": "**Problem 4a**\nDownload and read in the [training set](https://northwestern.box.com/s/sjegm0tx62l2i8dkzqw22s4gmq1a9sg1) for the model. \n"}
{"snippet": "with open('./tweets.pickle', 'rb') as f:\n    char2int, int2char = pickle.load(f)\njson_file = open('./architecture.trump.json', 'rt')\narchitecture = json_file.read()\njson_file.close()\nmodel2 = model_from_json(architecture)\nmodel2.load_weights('./weights.trump.h5')\n", "intent": "To load the model run the following:\n"}
{"snippet": "index_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n", "intent": "We now need to prepare the features. This part is all implemented for you.\n"}
{"snippet": "def query_to_df(session,query):\n    result = session.execute(query)\n    d = DataFrame(result.fetchall())\n    d.columns = result.keys()\n    return d\n", "intent": " Load data via sqlalchemy from the sql lite database forjar.db\n ===================================================================================\n"}
{"snippet": "pca_model = PCA(n_components=n_latent, whiten=True)\npsth_pca_latent = pca_model.fit_transform(psth)\n", "intent": "Below, we'll calculate the latent structure present in the data with a few algorithms.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42)\n", "intent": "Split into a training set and a test set using a stratified k fold\n"}
{"snippet": "import pandas as pd       \ntrain = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, \\\n                    delimiter=\"\\t\", quoting=3)\n", "intent": "from https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n"}
{"snippet": "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", header=None)\ndf.head()\n", "intent": "Read in Wisconsin Breast Cancer Dataset\n"}
{"snippet": "california_housing_dataframe = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep=\",\")\n", "intent": "Next, we'll load our data set.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\n    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n    header=None, \n    sep=',')\ndf.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\ndf.dropna(how=\"all\", inplace=True) \ndf.tail()\n", "intent": "In order to load the Iris data directly from the UCI repository, we are going to use the [pandas](http://pandas.pydata.org) library.\n"}
{"snippet": "from astropy.table import Table\nTable.read('irsa_catalog_WISE_iPTF14jg_search_results.tbl', format='ipac')\n", "intent": "**Problem 1b**\nUse [`Table.read()`](http://docs.astropy.org/en/stable/api/astropy.table.Table.html\n"}
{"snippet": "from sklearn import datasets\ndiabetes = datasets.load_diabetes()\nprostrate = pd.read_table(\"data/prostrate.data\").iloc[:,1:]\n", "intent": "We will be using a dataset of prostrate cancer occurances.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nimport matplotlib.pylab as pl\npl.gray();\n", "intent": "Import the dataset and use `pylab` to explore.\n"}
{"snippet": "pca = PCA(2)\n", "intent": "- Lower accuracy for k = 10\n"}
{"snippet": "X_train, X_test, y_train, y_test =train_test_split(reviews.data,reviews.target)\n", "intent": "What is the accuracy? \n<br>\n<details><summary>\nClick here for my accuracy...\n</summary>\n<br>\naccuracy = 82.00%  <br>\n</details>\n"}
{"snippet": "W = model.fit_transform(doc_term)\nH = model.components_\n", "intent": "Get the factors $\\text{W}$ and $\\text{H}$ from the resulting model.\n"}
{"snippet": "import pandas as pd\npath_data = '../../data/clean_json.json'\ndef load_json_data(path_to_file):\n    data_DF = pd.read_json(path_to_file,encoding='ascii')\n    data_DF['from'] = data_DF['from'].str.lower()\n    data_DF['body'] = data_DF['body'].apply(lambda x: \" \".join(str(x).split()))\n    return data_DF \n", "intent": "In the folder you will find a json file.\n"}
{"snippet": "import pandas as pd\ndef load_json_data(path_to_file):\n    data_DF = pd.read_json(path_to_file,encoding='ascii')\n    data_DF['from'] = data_DF['from'].str.lower()\n    data_DF['body'] = data_DF['body'].apply(lambda x: \" \".join(str(x).split()))\n    return data_DF\n", "intent": "In the folder you will find a json file.\n"}
{"snippet": "from scipy import misc, ndimage\nim = misc.imread(\"handwrite-small.png\")\nim = (255-im)/16.0\n", "intent": "<img src=\"handwrite-me.jpg\">\n<img src=\"handwrite-large.png\" width=\"50%\"> \n"}
{"snippet": "model = model_from_json(open('mnist_cnn.json').read())\nmodel.load_weights('mnist_cnn_weights.h5')\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "First we will load the MNIST model we trained beforehand.\n"}
{"snippet": "n = 10000\np = 2\nX, y = make_blobs(n_samples=n, n_features=p, centers=2, cluster_std=1.05, random_state=23)\nX = np.c_[np.ones(len(X)), X]\ny = y.astype('float')\n", "intent": "**Data set for classification**\n"}
{"snippet": "import pandas as pd\nimport numpy as np\npath = path='https://ibm.box.com/shared/static/q6iiqb1pd7wo8r3q28jvgsrprzezjqk3.csv'\ndf = pd.read_csv(path)\n", "intent": "<p></p>\n<li><a href=\"\n<li><a href=\"\n<li><a href=\"\n<li><a href=\"\n<p></p>\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=1)\nprint(\"number of test samples :\", x_test.shape[0])\nprint(\"number of training samples:\",x_train.shape[0])\n", "intent": " now we randomly split our data into training and testing data  using the function **train_test_split** \n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)\nprint(\"done\")\n", "intent": "Let's use 55 percent of the data for testing and the rest for training:\n"}
{"snippet": "pr=PolynomialFeatures(degree=5)\nx_train_pr=pr.fit_transform(x_train[['horsepower']])\nx_test_pr=pr.fit_transform(x_test[['horsepower']])\npr\n", "intent": "We will perform a degree 5 polynomial transformation on the feature **'horse power'**. \n"}
{"snippet": "def f(order,test_data):\n    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)\n    pr=PolynomialFeatures(degree=order)\n    x_train_pr=pr.fit_transform(x_train[['horsepower']])\n    x_test_pr=pr.fit_transform(x_test[['horsepower']])\n    poly=LinearRegression()\n    poly.fit(x_train_pr,y_train)\n    PollyPlot(x_train[['horsepower']],x_test[['horsepower']],y_train,y_test,poly,pr)\n", "intent": " The following function will be used in the next section; please run the cell.\n"}
{"snippet": "pr=PolynomialFeatures(degree=2)\nx_train_pr=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\nx_test_pr=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\n", "intent": " Let's perform a degree two polynomial transformation on our data. \n"}
{"snippet": "log_centers = pca.inverse_transform(centers)\ntrue_centers = scaler.inverse_transform(log_centers)\ntrue_centers = np.exp(true_centers)\nsegments = ['Segment {}'.format(i) for i in range(0,len(centers))]\ntrue_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())\ntrue_centers.index = segments\ndisplay(true_centers)\n", "intent": "\"If you do this, you will need to apply the following to transform your data back in Implementation: Data Recovery.\"\n"}
{"snippet": "subm = np.column_stack((np.asarray(test_data.index.values), np.asarray(test_data_pred, dtype=np.float32)))\nnp.savetxt('kaggle_submissions/sub_ridge_01.csv',subm, delimiter=',', comments='',  newline='\\n', fmt='%s', \n           header = 'id,trip_duration')\nsubmission = pd.read_csv('kaggle_submissions/sub_ridge_01.csv', index_col= 'id')\nprint(submission.shape)\nsubmission.head()\n", "intent": "Now we create the submission file.\n"}
{"snippet": "subm = np.column_stack((np.asarray(test_data.index.values), np.asarray(test_data_pred, dtype=np.float32)))\nnp.savetxt('kaggle_submissions/sub_rfr.csv',subm, delimiter=',', comments='',  newline='\\n', fmt='%s', \n           header = 'id,trip_duration')\nsubmission = pd.read_csv('kaggle_submissions/sub_rfr.csv', index_col= 'id')\nprint(submission.shape)\nsubmission.head()\n", "intent": "Now we create the submission file.\n"}
{"snippet": "standardizer = StandardScaler(withMean=True, withStd=True, \n                              inputCol='raw_features', \n                              outputCol='features')\nmodel = standardizer.fit(output)\noutput = model.transform(output)\n", "intent": "Scale features to have zero mean and unit standard deviation\n"}
{"snippet": "subm = np.column_stack((np.asarray(test_data.index.values), np.asarray(test_data_pred, dtype=np.float32)))\nnp.savetxt('kaggle_submissions/submission_grad_boost.csv',subm, delimiter=',', comments='',  newline='\\n', fmt='%s', \n           header = 'id,trip_duration')\nsubmission = pd.read_csv('kaggle_submissions/submission_grad_boost.csv', index_col= 'id')\nprint(submission.shape)\nsubmission.head()\n", "intent": "Creating the submission file.\n"}
{"snippet": "key_pts_frame = pd.read_csv('/data/training_frames_keypoints.csv')\nprint(key_pts_frame.shape)\nn = 5\nimage_name = key_pts_frame.iloc[n, 0]\nkey_pts = key_pts_frame.iloc[n, 1:].as_matrix()\nkey_pts = key_pts.astype('float').reshape(-1, 2)\nprint('Image name: ', image_name)\nprint('Landmarks shape: ', key_pts.shape)\nprint('First 4 key pts: {}'.format(key_pts[:4]))\n", "intent": "Then, let's load in our training data and display some stats about that data to make sure it's been loaded in correctly!\n"}
{"snippet": "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n", "intent": "The MNIST dataset contains thousands of grayscale images of handwritten digits.\n"}
{"snippet": "import helper\ndata_dir = './data/simpsons/moes_tavern_lines.txt'\ntext = helper.load_data(data_dir)\ntext = text[81:]\n", "intent": "Play around with `view_sentence_range` to view different parts of the data.\n"}
{"snippet": "boston = load_boston()\n", "intent": "Next we'll download the data set\n"}
{"snippet": "coeff_df = DataFrame(boston_df.columns)\ncoeff_df.columns = ['Features']\ncoeff_df[\"Coefficient Estimate\"] = pd.Series(lreg.coef_)\ncoeff_df\n", "intent": "What we'll do next is set up a DataFrame showing all the Features and their estimated coefficients obtained form the linear regression.\n"}
{"snippet": "df = pd.DataFrame()\ndf['ship_type'] = np.random.choice([\"romulan\", \"human\", \"klingon\", \"borg\", \"red_shirt\", \"ovid\"], size=500)\ndf['ship_value'] = np.random.randint(200000, 10000000, size=500)\ndf['ship_speed'] = np.random.randint(10, 60, size=500)\ndf['baths'] = np.random.choice(np.arange(1, 4, 0.5), size=500)\ny = df[['baths']]\nX = df[['ship_type','ship_value','ship_speed']]\n", "intent": "With 2-3 continious variables, and one categorical.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncolumns = ['ship_type','ship_value',]\nstand_columns = \"'stand_ship_type','stand_ship_value'\"\ndf['stand_ship_type','stand_ship_value'] = scaler.fit_transform(df[columns])\n", "intent": "More code will be written...\n"}
{"snippet": "my_alphas = []\nmy_score = []\nfor x in results.grid_scores_:\n    y = tuple(x)\n    my_score.append(y[1])\n    my_alphas.append(y[0]['alpha'])\nmy_df = pd.DataFrame(data=[my_alphas,my_score])\nmy_df = my_df.T\nmy_df.columns=[['Alphas','Score']]\nmy_df.plot(x= 'Alphas',y = 'Score' ,kind='scatter')\n", "intent": "Figure out how to plot the impact of your score given the range of parameters used.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\ndata = pd.read_csv(\"data.csv\") \nlabels = data['Dataset']  \ndata\n", "intent": "In this step, we will be analyzing the data given to us. It gives us an idea of what features are important to the determination of liver disease. \n"}
{"snippet": "cm = pd.DataFrame(confusion_matrix(y_test, predictions), columns=['predicted is cheap','predicted is expensive'],\n                 index = ['actual is cheap','actual is expensive'])\ncm\n", "intent": "What do these mean?\n"}
{"snippet": "iowa_file = '/Users/ryandunlap/Desktop/DSI-SF-2/datasets/iowa_liquor/Iowa_Liquor_sales_sample_10pct.csv'\niowa = pd.read_csv(iowa_file)\n", "intent": "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n---\n"}
{"snippet": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nairline_data = pd.read_csv('airline_data.csv', header=None, names=cols, low_memory=False)\nairline_data.fillna(airline_data.mean(axis=1))\nairline_data.drop[airline_data.index[4]]\n", "intent": "Other crash example\n"}
{"snippet": "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/titanic.csv'\ntitanic = pd.read_csv(url)\ntitanic.head()\n", "intent": "We'll deal with Titanic dataset again, this time with XGBClassifier\n"}
{"snippet": "import pandas as pd\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/titanic.csv'\ntitanic = pd.read_csv(url)\ntitanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\ntitanic.Age.fillna(titanic.Age.median(), inplace=True)\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\nembarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\ntitanic = pd.concat([titanic, embarked_dummies], axis=1)\ntitanic.head()\n", "intent": "We'll build a classification tree using the Titanic data:\n"}
{"snippet": "from sklearn.decomposition import PCA as sklearnPCA\nsklearn_pca = sklearnPCA(n_components=2)\nY_sklearn = sklearn_pca.fit_transform(X_std) \n", "intent": "For educational purposes, we went a long way to apply the PCA to the Iris dataset. But luckily, there is already implementation in scikit-learn. \n"}
{"snippet": "X, y = make_blobs(n_samples=10000, \n                  n_features=10, centers=100, random_state=0)\n", "intent": "X, y = make_blobs...\nfig = plt.figure(figsize=(10,8))\nax = fig.add_subplot(111)\nax.scatter...\n"}
{"snippet": "authorship = pd.read_csv(\"http://people.stern.nyu.edu/jsimonof/AnalCatData/Data/Comma_separated/authorship.csv\")\nprint authorship.columns\n", "intent": "- \"http://people.stern.nyu.edu/jsimonof/AnalCatData/Data/Comma_separated/authorship.csv\"\n- Print the columns, print df.head()\n"}
{"snippet": "le = preprocessing.LabelEncoder()\nle.fit(authors)\nauthorship['Author_num'] = le.transform(authorship['Author']) \nprint authorship['Author_num']\n", "intent": "Use the LabelEncoder to encode Authors to integers...\n1. What does the LabelEncoder do for us? \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val = train_test_split(new_data,labels,test_size=0.33,random_state=42)\n", "intent": "Q8. Create training and validation split on data. Check out train_test_split() function from sklearn to do this.\n"}
{"snippet": "import pandas as pd\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer\n", "intent": "1. K-means clustering\n2. Clustering evaluation\n3. DBSCAN clustering\n"}
{"snippet": "from sklearn.datasets import make_blobs\nnum_blobs = 8\nX, Y = make_blobs(centers=num_blobs, cluster_std=0.5, random_state=2)\n", "intent": "The $k$-Means Algorithm\n====================\nLet's start by generating some artificial blobs of data:\n"}
{"snippet": "X = pd.DataFrame(X, columns=df.drop('Class', axis=1).columns)\nX.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df = pd.read_csv('College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "mean_distances = np.apply_along_axis(np.mean, 1, dist_mtx)\ndf_valid_popularity = pd.DataFrame({'coupon_id_hash': coupons_valid_ids,\n    'popularity': 1-mean_distances})\ndf_valid_popularity.head()\n", "intent": "And finally the validation coupons \"popularity\", expressed as how similar are the validation coupons to the most popular coupons during training.\n"}
{"snippet": "coupons_train_feat_num = df_coupons_train_feat[num_cols].values\ncoupons_valid_feat_num = df_coupons_valid_feat[num_cols].values\nscaler = MinMaxScaler()\ncoupons_train_feat_num_norm = scaler.fit_transform(coupons_train_feat_num)\ncoupons_valid_feat_num_norm = scaler.transform(coupons_valid_feat_num)\n", "intent": "And the numeric ones\n"}
{"snippet": "ncomp = 50\nnmf_model = NMF(n_components=ncomp, init='random', random_state=1981)\nuser_factors = nmf_model.fit_transform(interactions_mtx)\nitem_factors = nmf_model.components_.T\n", "intent": "None negative matrix factorization with default values and n_comp (50 to start with) components/factors.\n"}
{"snippet": "mean_value = data['abdomo_protein'].mean()\ndata['abdomo_protein'] = data['abdomo_protein'].fillna(mean_value)\ndata['abdomo_protein']\n", "intent": "Q3. Fill all NaN values in column 'abdomo_protein' with the mean value of 'abdomo_protein'.\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "** Crie um objeto StandardScaler() chamado scaler. **\n"}
{"snippet": "yelp = pd.read_csv('yelp.csv')\n", "intent": "** Leia o arquivo yelp.csv e configure-o como um dataframe chamado yelp. **\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\n", "intent": "** Import CountVectorizer e crie um objeto CountVectorizer. **\n"}
{"snippet": "yelp = pd.read_csv(\"yelp.csv\")\n", "intent": "** Leia o arquivo yelp.csv e configure-o como um dataframe chamado yelp. **\n"}
{"snippet": "ad_data = pd.read_csv('advertising.csv')\n", "intent": "** Leia o arquivo advertising.csv e grave-o em um DataFrame chamado ad_data. **\n"}
{"snippet": "loans = pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas para ler loan_data.csv como um DataFrame chamado loans. **\n"}
{"snippet": "data_path_test = os.path.join(os.getcwd(), 'datasets', 'spambase_test.csv')\nspambase_test = pd.read_csv(data_path_test, delimiter = ',')\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure\n"}
{"snippet": "label_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(train_targets)\ny_valid = label_encoder.transform(valid_targets)\ny_test = label_encoder.transform(test_targets)\ny_train_1_hot = to_categorical(y_train, N_CLASSES)\ny_valid_1_hot = to_categorical(y_valid, N_CLASSES)\nwith open('label_encoder.p', 'wb') as f:\n    pickle.dump(label_encoder, f)\n", "intent": "    also convert targets to 1-hot format required by using categorical_crossentropy loss \n"}
{"snippet": "dfT = pd.read_csv('C:/Users/BeckyC/Desktop/Data Science - GA files/titanic.csv')\n", "intent": "Create a cluster based on Pclass and Age from the Titanic dataset.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()  \ndata['outcome'] = le.fit_transform(data['outcome'])  \n", "intent": "Q9. Using LabelEncoder, encode 'outcome'.\n"}
{"snippet": "from sklearn.model_selection import train_test_split, GridSearchCV\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n", "intent": "I will use a random forest classification to look for important features.\n"}
{"snippet": "fi = pd.DataFrame(rf.feature_importances_, columns = ['importance'])\nlabel = pd.DataFrame(X.columns, columns = ['feature'])\nfeature_imp = pd.concat([label, fi], axis = 1)\n", "intent": "The random forest model was able to predict the active status with an accuracy score of 78.3 %.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = 'data/iris.data'\niris = pd.read_csv(data)\n", "intent": "<a id=\"overview-of-the-iris-dataset\"></a>\n---\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = '../data/iris.data'\niris = pd.read_csv(data)\n", "intent": "<a id=\"overview-of-the-iris-dataset\"></a>\n---\n"}
{"snippet": "table = pd.DataFrame({'probability':[0.1, 0.2, 0.25, 0.5, 0.6, 0.8, 0.9]})\ntable['odds'] = table.probability/(1 - table.probability)\ntable\n", "intent": "**As an example we can create a table of probabilities vs. odds, as seen below.**\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nprint (cancer.feature_names)\nprint (cancer.DESCR)\n", "intent": "***\nA frequently used data set for ML is a data set for *breast cancer diagnosis*\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\nforest = RandomForestClassifier(n_estimators=100, random_state=0)\nforest.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))\n", "intent": "Effectively, boundaries are more complex\n***\n"}
{"snippet": "from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n", "intent": "Neural networks are quite sensitive to feature scaling, so let's try to scale the features.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.images.shape\n", "intent": "***\nAnother classic example case for ML is handwritten digits data.\nA suitable dataset is included with sklearn, first we look into it:\n"}
{"snippet": "import numpy as np\nimport pandas as pd\ndata = pd.read_csv('abalone.data',sep = ' ')\n", "intent": "In this lab, we will be faced with a regression problem. We have to guess the age of abalone(a type of marine snail) based on other given data. \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n", "intent": "***\nOf course what we not just clustering but classification, so let's try our two models we had used before also on digits:\n"}
{"snippet": "pca = PCA(n_components=2)\n", "intent": "What is `n_components`?\n"}
{"snippet": "use_cols = ['Cabin', 'Cabin_reduced', 'Sex']\nX_train, X_test, y_train, y_test = train_test_split(\n    data[use_cols], \n    data.Survived,  \n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "We reduced the number of different labels from 148 to 9.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data[['Age', 'Fare']].fillna(0),\n    data.Survived,\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "Age contains 20 % of missing data. For simplicity, I will fill the missing values with 0.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "coef = model.coef_.reshape(1,4)\npd.DataFrame(data=coef, columns=['Avg. Session Length', 'Time on App',\n       'Time on Website', 'Length of Membership'])\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "X = yelp['text']\ny = yelp['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=77)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "df = pd.read_csv('train.csv')\n", "intent": "Use `pandas` to load the file `train.csv`.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\n", "intent": "Split the data into a training and a test set:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.33,random_state=42)\n", "intent": "Q8. Split training and testing set.\n"}
{"snippet": "df = pd.read_csv(\"train806.csv\")\n", "intent": "Verwende `pandas`, um die Datei `train.csv` einzulesen.\n"}
{"snippet": "import pandas as pd\ndata = pd.DataFrame()\ndata['gender'] = ['male','male','male','male','female','female','female','female']\ndata['height'] = [6,5.92,5.58,5.92,5,5.5,5.42,5.75]\ndata['weight'] = [180,190,170,165,100,150,130,150]\ndata['shoe size'] = [12,11,12,10,6,8,7,9]\ndata\n", "intent": "Consider the following dataset:\n"}
{"snippet": "person = pd.DataFrame()\nperson['height'] = [6]\nperson['weight'] = [130]\nperson['shoe size'] = [8]\nperson\n", "intent": "Given this dataset, implement the naive bayes classification algorithm and use it classify the following person as either a male or female:\n"}
{"snippet": "import pandas as pd\ndata = pd.DataFrame()\ndata['gender'] = ['male','male','male','male','female','female','female','female']\ndata['height'] = [6,5.92,5.58,5.92,5,5.5,5.42,5.75]\ndata['weight'] = [180,190,170,165,100,150,130,150]\ndata['shoe size'] = [12,11,12,10,6,8,7,9]\ndata.head()\n", "intent": "Consider the following dataset:\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n", "intent": "Practice: Apply PCA to the iris dataset.\n"}
{"snippet": "labels =[\"Survived\", \"Perished\"]\ndef splitData(features):\n    titanic_predictors = titanic[features].as_matrix()\n    titanic_labels = titanic[\"Survived\"].as_matrix()\n    XTrain, XTest, yTrain, yTest = train_test_split(titanic_predictors, titanic_labels, random_state=1, test_size=0.5)\n    return XTrain, XTest, yTrain, yTest\n", "intent": "Here is some code that splits the data into training and test sets for cross-validation and selects features.\n"}
{"snippet": "contraception_df = pd.read_csv(\"cmc.csv\")\ncontraception_df.head()\n", "intent": "And then load and explore the dataset:\n"}
{"snippet": "features = contraception_df.columns[:-1]\ndef splitData(features):\n    contraception_labels = contraception_df[\"Contraceptive-method-used\"].as_matrix()\n    contraception_predictors =  contraception_df[features].as_matrix()\n    XTrain, XTest, yTrain, yTest = train_test_split(contraception_predictors, contraception_labels, \n                                                    random_state=1, test_size=0.5)\n    return XTrain, XTest, yTrain, yTest\n", "intent": "Here is some code that splits the data into training and test sets for cross-validation and selects features.\n"}
{"snippet": "XTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state=1, test_size=0.2)\nmodel.fit(XTrain, yTrain)\n", "intent": "So, let's see what our algorithm things about the Mod Squad! \n"}
{"snippet": "df=pd.read_csv(\"loan_data.csv\")\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nlist(digits.keys())\n", "intent": "Let's load a simple dataset of 8x8 gray level images of handwritten digits (bundled in the sklearn source code):\n"}
{"snippet": "from sklearn.decomposition import RandomizedPCA\npca = RandomizedPCA(n_components=2)\nX_pca = pca.fit_transform(X)\nX_pca.shape\n", "intent": "Let's visualize the dataset on a 2D plane using a projection on the first 2 axis extracted by Principal Component Analysis:\n"}
{"snippet": "from sklearn.feature_extraction.text import HashingVectorizer\nh_vectorizer = HashingVectorizer(encoding='latin-1')\ndv['h_vectorizer'] = h_vectorizer\ndv['names'] = names\ndv['training_csv_file'] = training_csv_file\ndv['n_partitions'] = len(client)\n", "intent": "Let's send all we need to the engines\n"}
{"snippet": "train = pd.DataFrame()\nfor i in range(5):\n    df = load_month(i, True)\n    train = pd.concat([train, df], axis=0)\ntest = load_month(5, True)\n", "intent": "Load 2015-01 - 2015-05 as training and test on 2015-06\n"}
{"snippet": "def encode_cat_data(df):\n    string_data = df.select_dtypes(include=[\"object\"])\n    for c in string_data.columns:\n        le = LabelEncoder()    \n        le.fit(df[c])\n        df[c] = le.transform(df[c])\n", "intent": "Encode categorical columns\n"}
{"snippet": "from datetime import datetime\nimport csv\nlogging.info('- Generate submission')\nsubmission_file = '../results/submission_' + \\\n                  str(datetime.now().strftime(\"%Y-%m-%d-%H-%M\")) + \\\n                  '.csv'\nsubmission.to_csv(submission_file, index=False, index_label=False)\n", "intent": "Get submission DataFrame and write csv file\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nll = 5\ntrain_df = pd.DataFrame({'ncodpers': range(ll) + range(ll), 'fecha_dato': [201501]*ll + [201502]*ll, 'a': range(2*ll), 'b': np.random.randn(2*ll)}, columns=['ncodpers', 'fecha_dato', 'a', 'b'])\ntrain_df\n", "intent": "Append last choice as columns\n"}
{"snippet": "data = pd.read_csv('TaiwaneseDefaultData.csv')\ndata.head()\n", "intent": "Note, the last column is the label\n"}
{"snippet": "scaler = StandardScaler().fit(features)\nscaled_features = scaler.transform(features)\nprint(scaled_features)\n", "intent": "Super, now we're finally ready for our Scikit-Learn magic; let's apply the `StandardScaler` to the feature set:\n"}
{"snippet": "df=pd.read_csv(\"College_Data\",index_col=0)\ndf.head()\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_norm =  StandardScaler().fit_transform(X)\nmodel.fit(X_norm, y)\ncoeffs = pd.DataFrame(model.coef_, columns = iris.feature_names, index =iris.target_names)\ncoeffs\n", "intent": "Let's normalize the data and repeat the exercise:\n> Student should do this:\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "Scikit Learn implements support vector machine models in the `svm` package.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfeatures = [c for c in df.columns if c != 'acceptability']\nfor c in df.columns:\n    df[c] = le.fit_transform(df[c])\nX = df[features]\ny = df['acceptability']\n", "intent": "Since most of the features are categorical text we will need to encode them as numbers using the `LabelEncoder`:\n"}
{"snippet": "feature_importances = pd.DataFrame(dt.feature_importances_,\n                                   index = X.columns,\n                                    columns=['importance']).sort_values('importance',\n                                                                        ascending=False)\nfeature_importances.head()\n", "intent": "Great, now we are ready to look at feature importances in our tree:\n"}
{"snippet": "cars=pd.DataFrame(mtcars)\n", "intent": "Convert to a Pandas Dataframe for our analysis\n"}
{"snippet": "iris=pd.read_csv(\"iris.csv\")\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "First, let's import our data. We'll be using *Iris* again for this exercise. \n"}
{"snippet": "x = pd.DataFrame(iris.data)\nx.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\ny = pd.DataFrame(iris.target)\ny.columns = ['Targets']\n", "intent": "As in 2.1, let's split the set into two parts. \"X\" will be the data and \"Y\" will be the class labels.\n"}
{"snippet": "x = pd.DataFrame(iris.data)\nx.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\ny = pd.DataFrame(iris.target)\ny.columns = ['Targets']\n", "intent": "Define your \"X\" and \"y\" variables for the analysis\n"}
{"snippet": "df=pd.read_csv(\"KNN_Project_Data\")\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "data = datasets.load_boston()\nprint data.DESCR \n", "intent": "**Load the boston housing data with the `datasets.load_boston()` function.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size= 0.25)\n", "intent": "---\n- Make predictor matrix `X` and target variable `y`\n- Split your data into a validation set using `train_test_split`\n"}
{"snippet": "test_df = test_df.drop('PassengerId', axis=1)\ntest_df.to_csv('new_test_data.csv', index=False)\ntest_df\n", "intent": "And the test dataset.\n"}
{"snippet": "A.fillna('missing data')\n", "intent": "That actually works with any data type.\n"}
{"snippet": "A.fillna(A.mean())\n", "intent": "We can use this functionality to fill in the gaps with the average value computed across the non-missing values.\n"}
{"snippet": "print(B)\nprint()\nprint(B.fillna(method='pad'))\n", "intent": "You can fill values forwards and backwards with the flags *pad* / *ffill* and *bfill* / *backfill*\n"}
{"snippet": "B.fillna(method='bfill', limit=1)\n", "intent": "We can set a limit if we only want to replace consecutive gaps.\n"}
{"snippet": "np.random.seed(2)\nser = pd.Series(np.arange(1, 10.1, .25)**2 + np.random.randn(37))\nbad = np.array([4, 13, 14, 15, 16, 17, 18, 20, 29])\nser[bad] = np.nan\nmethods = ['linear', 'nearest', 'quadratic']\ndf = pd.DataFrame({m: ser.interpolate(method=m) for m in methods})\ndf.plot()\n", "intent": "Below we compare three different methods.\n"}
{"snippet": "file_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\nnames = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Name']\ndf_iris = pd.read_csv(file_url, names=names, header=None)\nprint(df_iris.head())\n", "intent": "Now let's load and take a look at the Iris dataset again.\n"}
{"snippet": "ss=StandardScaler()\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "cdf = pd.DataFrame(lm.coef_, X.columns,columns=['Coeffecient'])\n", "intent": "**Interpretting the coefficients and answering the question at hand.**\n"}
{"snippet": "loans_df = pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "loans_df = pd.read_csv('loan_data.csv')\n", "intent": "** Check out the info(), head(), and describe() methods on loans.**\n"}
{"snippet": "fscaled_df = pd.DataFrame(feats_scaled, columns=feats.columns)\nfscaled_df.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "ad_df = pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer()\nprint(count_vector)\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "five_year_returns = \\\n    get_pricing(\n        data_portal,\n        trading_calendar,\n        universe_tickers,\n        universe_end_date - pd.DateOffset(years=5),\n        universe_end_date)\\\n    .pct_change()[1:].fillna(0)\nfive_year_returns\n", "intent": "Let's get returns data for our risk model using the `get_pricing` function. For this model, we'll be looking back to 5 years of data.\n"}
{"snippet": "num_factor_exposures = 20\npca = fit_pca(five_year_returns, num_factor_exposures, 'full')\npca.components_\n", "intent": "Let's see what the model looks like. First, we'll look at the PCA components.\n"}
{"snippet": "df1=pd.DataFrame(data=scaled_Value,columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "def factor_returns(pca, returns, factor_return_indices, factor_return_columns):\n    assert len(factor_return_indices.shape) == 1\n    assert len(factor_return_columns.shape) == 1\n    return pd.DataFrame(pca.transform(returns),  factor_return_indices, factor_return_columns)\nproject_tests.test_factor_returns(factor_returns)\n", "intent": "Implement `factor_returns` to get the factor returns from the PCA model using the returns data.\n"}
{"snippet": "def idiosyncratic_var_matrix(returns, factor_returns, factor_betas, ann_factor):\n    common_returns = pd.DataFrame(np.dot(factor_returns, factor_betas.T), returns.index, returns.columns)\n    s_returns = returns - common_returns\n    return pd.DataFrame(np.diag(np.var(s_returns))*ann_factor, returns.columns, returns.columns)\nproject_tests.test_idiosyncratic_var_matrix(idiosyncratic_var_matrix)\n", "intent": "Implement `idiosyncratic_var_matrix` to get the idiosyncratic variance matrix.\n"}
{"snippet": "def idiosyncratic_var_vector(returns, idiosyncratic_var_matrix):\n    return pd.DataFrame(np.diag(idiosyncratic_var_matrix), returns.columns)\nproject_tests.test_idiosyncratic_var_vector(idiosyncratic_var_vector)\n", "intent": "Implement `idiosyncratic_var_vector` to get the idiosyncratic variance Vector.\n"}
{"snippet": "all_weights = pd.DataFrame(np.repeat(1/len(universe_tickers), len(universe_tickers)), universe_tickers)\npredict_portfolio_risk(\n    risk_model['factor_betas'],\n    risk_model['factor_cov_matrix'],\n    risk_model['idiosyncratic_var_matrix'],\n    all_weights)\n", "intent": "Let's see what the portfolio risk would be if we had even weights across all stocks.\n"}
{"snippet": "ls_factor_returns = pd.DataFrame()\nfor factor, factor_data in clean_factor_data.items():\n    ls_factor_returns[factor] = al.performance.factor_returns(factor_data).iloc[:, 0]\n(1+ls_factor_returns).cumprod().plot()\n", "intent": "Let's view the factor returns over time. We should be seeing it generally move up and to the right.\n"}
{"snippet": "sector_lookup = pd.read_csv(\n    os.path.join(os.getcwd(), '..', '..', 'data', 'project_7_sector', 'labels.csv'),\n    index_col='Sector_i')['Sector'].to_dict()\nsector_lookup\nsector_columns = []\nfor sector_i, sector_name in sector_lookup.items():\n    secotr_column = 'sector_{}'.format(sector_name)\n    sector_columns.append(secotr_column)\n    all_factors[secotr_column] = (all_factors['sector_code'] == sector_i)\nall_factors[sector_columns].head()\n", "intent": "For the model to better understand the sector data, we'll one hot encode this data.\n"}
{"snippet": "ls_factor_returns = pd.DataFrame()\nfor factor_name, data in factor_data.items():\n    ls_factor_returns[factor_name] = al.performance.factor_returns(data).iloc[:, 0]\n(1 + ls_factor_returns).cumprod().plot()\n", "intent": "Let's view the factor returns over time. We should be seeing it generally move up and to the right.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X)\nprint(pca.components_)  \nprint(pca.explained_variance_)  \n", "intent": "[The inspiration](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)\n"}
{"snippet": "prices = pd.read_csv('stock_prices_advanced_optimization.csv', parse_dates=['date'], index_col=0)\n", "intent": "Load the data from the file `stock_prices_advanced_optimization.csv`.\n"}
{"snippet": "X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "rm = RiskModelPCA(2) \nrm.fit(rets) \n", "intent": "Let's fit the risk model with 2 factors (i.e., we'll keep 2 PCs).\n"}
{"snippet": "unemp_03_14 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/unemp_tot_03_14.csv')\nunemp_03_14 = unemp_03_14.iloc[:,1:]\npov_county_year_03_14 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/pov_county_year_03_14.csv')\npov_county_year_03_14 = pov_county_year_03_14.iloc[:,1:]\ncdc_03_14 = pd.read_csv('/Users/HudsonCavanagh/Documents/csv_out_duplicate/cdc_03_14.csv')\ncdc_03_14 = cdc_03_14.iloc[:,2:]\ncdc_03_14.head(12)\n", "intent": "https://www.census.gov/geo/reference/county-changes.html\n"}
{"snippet": "wine = wine.drop('type', 1)\ndf_X = pd.DataFrame(wine, columns=wine.columns)\ndf_X = df_X.drop('quality', 1)\ndf_y = wine[\"quality\"]\nX_train, X_test, y_train, y_test = train_test_split(df_X,df_y, test_size=.4)\nprint \"       X Shape  Y Shape\"\nprint \"Train\", X_train.shape, y_train.shape\nprint \"Test \", X_test.shape, y_test.shape\n", "intent": "Using scikit-learn or statsmodels, build the necessary models for your scenario. Evaluate model fit.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(Q1_2015_X, Q234_2015_Y, test_size=.33)\nprint \"       X Shape  Y Shape\"\nprint \"Train\", X_train.shape, y_train.shape\nprint \"Test \", X_test.shape, y_test.shape\n", "intent": "Using scikit-learn or statsmodels, build the necessary models for your scenario. Evaluate model fit.\n"}
{"snippet": "df = pd.read_csv('assets/dataset/flight_delays.csv')\ndf.head(3)\n", "intent": "Using a dataset of flight delays let's:\n- try to predict whether a flight will be delayed by 15 minutes\n- visualize our predictions\n"}
{"snippet": "df = pd.read_csv('assets/dataset/collegeadmissions.csv')\ndf.head()\n", "intent": "Here's a dataset of grad school admissions, based on GPA, rank, and GRE score. In the admit column \"1\" is admit.\n"}
{"snippet": "sac = pd.read_csv('../assets/datasets/Sacramentorealestatetransactions.csv')\nsac.head()\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\ny = iris.target\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "df_new = pd.DataFrame(data=df, columns=['workclass_num', 'education-num', 'hours-per-week', 'income_num'])\ndf_new2 = pd.DataFrame(data=df, columns=['workclass_num', 'education-num'])\ndf_new.head(5)\n", "intent": "Create a New Dataframe with just numerical data\n"}
{"snippet": "df=pd.read_csv(\"advertising.csv\")\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "airports = pd.read_csv('/Users/HudsonCavanagh/Documents/airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "xStand = StandardScaler().fit_transform(x)\n", "intent": "Then, standardize the features for analysis\n"}
{"snippet": "PCdf = pd.DataFrame(xPC, columns=['PC1','PC2'])\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "air_pca_2_features = pd.DataFrame(air_pca_2_features)\nair_pca_2_features.head()\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "airports = pd.read_csv('./assets/datasets/airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "X_scaled = preprocessing.MinMaxScaler().fit_transform(df)\n", "intent": "Next, since each of our features have different units and ranges, let's do some preprocessing:\n"}
{"snippet": "from sklearn import preprocessing\nencode = preprocessing.LabelEncoder()\npre_poll['age'] = encode.fit_transform(pre_poll.age) \npre_poll['state'] = encode.fit_transform(pre_poll.state)\npre_poll['edu'] = encode.fit_transform(pre_poll.edu)\npre_poll.head()\n", "intent": "Some of our ML work will be better suited if the input data is contained in a numpy object.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split, cross_val_score\nX_train, X_test, y_train, y_test = train_test_split(poll_nd, bush_null, test_size=0.30, random_state=50)\n", "intent": "Split the data in the ordinary way, making sure you have a 70/30 split.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"data_training.csv\")\nimport numpy as np\nX = np.array(data[['x1', 'x2']])\ny = np.array(data['y'])\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, \n\t\t\t\t\t\t\t\t\t\t\t\t\ty, \n\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size = 0.25)\n", "intent": "Point of the exercise: it is not easy to fit these parameters manually.\n**Testing in sklearn**\n"}
{"snippet": "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=101)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline([('std_scaler', StandardScaler()),\n                    ('poly_features', PolynomialFeatures(degree = 2))])\nhousing_prep = pipeline.fit_transform(X)\n", "intent": "<b>SCALING FEATURES</b>\n"}
{"snippet": "customer_log_df = np.log(customer_df)\nscaler.fit(customer_log_df)\ncustomer_log_sc = scaler.transform(customer_log_df)\ncustomer_log_sc_df = pd.DataFrame(customer_log_sc, columns=customer_df.columns)\n", "intent": "Many times the skew of data can be easily removed by taking the log of the data. Let's do so here.\nWe will then scale the data after deskewing.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('car.csv') \ndf.head()\n", "intent": "Vamos a comenzar con la lectura del dataset de aceptabilidad de autos.\n"}
{"snippet": "subte = pd.read_csv('estaciones-de-subte.csv',delimiter=',')\nmetrobus = pd.read_csv('estaciones-de-metrobus.csv',delimiter=';')\nferrocarril = pd.read_csv('estaciones-de-ferrocarril.csv',delimiter=';')\n", "intent": "Datos de servicios de transporte por Buenos Aires Data:\n"}
{"snippet": "loans=pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "df=pd.read_csv('College_Data',index_col=0)\ndf.head()\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df=pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaler=StandardScaler()\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "scaled_features=pd.DataFrame(scaled_features,columns=df.columns.drop('TARGET CLASS'))\nscaled_features.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint('Size of training data: {}'.format(len(X_train)))\nprint('Size of testing data: {}'.format(len(X_test)))\n", "intent": "- From the dataset, we will split into training and testing data.\n"}
{"snippet": "from dopy.doplot.selection import add_log_to_dataframe, add_max_to_dataframe, add_min_to_dataframe\nadd_min_to_dataframe(real_dataframe_wrongPV, 'B0_FitDaughtersConst_KS0_min_PT', ['B0_FitDaughtersConst_KS0_P1_PT_flat', 'B0_FitDaughtersConst_KS0_P0_PT_flat'])\nadd_min_to_dataframe(signal_dataframe_wrongPV, 'B0_FitDaughtersConst_KS0_min_PT', ['B0_FitDaughtersConst_KS0_P1_PT_flat', 'B0_FitDaughtersConst_KS0_P0_PT_flat'])\nadd_min_to_dataframe(real_dataframe_wrongPV, 'test_IP', ['B0_FitDaughtersConst_KS0_P0_IPCHI2_flat', 'B0_FitDaughtersConst_KS0_P1_IPCHI2_flat'])\nadd_min_to_dataframe(signal_dataframe_wrongPV, 'test_IP', ['B0_FitDaughtersConst_KS0_P0_IPCHI2_flat', 'B0_FitDaughtersConst_KS0_P1_IPCHI2_flat'])\nadd_min_to_dataframe(real_dataframe_wrongPV, 'B0_FitDaughtersConst_J_psi_1S_min_PT', ['B0_FitDaughtersConst_J_psi_1S_P0_PT_flat', 'B0_FitDaughtersConst_J_psi_1S_P1_PT_flat'])\nadd_min_to_dataframe(signal_dataframe_wrongPV, 'B0_FitDaughtersConst_J_psi_1S_min_PT', ['B0_FitDaughtersConst_J_psi_1S_P0_PT_flat', 'B0_FitDaughtersConst_J_psi_1S_P1_PT_flat'])\nreal_dataframe_wrongPV['B0_FitPVConst_KS0_tau_dimless'] = real_dataframe_wrongPV['B0_FitPVConst_KS0_tau_flat']/real_dataframe_wrongPV['B0_FitPVConst_KS0_tauErr_flat']\nsignal_dataframe_wrongPV['B0_FitPVConst_KS0_tau_dimless'] = signal_dataframe_wrongPV['B0_FitPVConst_KS0_tau_flat']/signal_dataframe_wrongPV['B0_FitPVConst_KS0_tauErr_flat']\n", "intent": "Generate new features\n------------------------------------\n"}
{"snippet": "from dopy.doplot.selection import add_log_to_dataframe, add_max_to_dataframe, add_min_to_dataframe\nadd_min_to_dataframe(real_dataframe, 'B0_FitDaughtersConst_KS0_min_PT', ['B0_FitDaughtersConst_KS0_P1_PT', 'B0_FitDaughtersConst_KS0_P0_PT'])\nadd_min_to_dataframe(signal_dataframe, 'B0_FitDaughtersConst_KS0_min_PT', ['B0_FitDaughtersConst_KS0_P1_PT', 'B0_FitDaughtersConst_KS0_P0_PT'])\nadd_min_to_dataframe(real_dataframe, 'test_IP', ['B0_FitDaughtersConst_KS0_P0_IPCHI2', 'B0_FitDaughtersConst_KS0_P1_IPCHI2'])\nadd_min_to_dataframe(signal_dataframe, 'test_IP', ['B0_FitDaughtersConst_KS0_P0_IPCHI2', 'B0_FitDaughtersConst_KS0_P1_IPCHI2'])\nadd_min_to_dataframe(real_dataframe, 'B0_FitDaughtersConst_J_psi_1S_min_PT', ['B0_FitDaughtersConst_J_psi_1S_P0_PT', 'B0_FitDaughtersConst_J_psi_1S_P1_PT'])\nadd_min_to_dataframe(signal_dataframe, 'B0_FitDaughtersConst_J_psi_1S_min_PT', ['B0_FitDaughtersConst_J_psi_1S_P0_PT', 'B0_FitDaughtersConst_J_psi_1S_P1_PT'])\nreal_dataframe['B0_FitPVConst_KS0_tau_dimless'] = real_dataframe['B0_FitPVConst_KS0_tau']/real_dataframe['B0_FitPVConst_KS0_tauErr']\nsignal_dataframe['B0_FitPVConst_KS0_tau_dimless'] = signal_dataframe['B0_FitPVConst_KS0_tau']/signal_dataframe['B0_FitPVConst_KS0_tauErr']\n", "intent": "Generate new features\n------------------------------------\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../datasets/ch1ex1.csv')\npoints = df.values\nnew_df = pd.read_csv('../datasets/ch1ex2.csv')\nnew_points = new_df.values\n", "intent": "**Step 1:** Load the dataset _(written for you)_.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../datasets/ch1ex1.csv')\npoints = df.values\n", "intent": "**Step 1:** Load the dataset _(written for you)_.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../datasets/fish.csv')\ndel df['species']\n", "intent": "**Step 1:** Load the dataset _(this bit is written for you)_.\n"}
{"snippet": "import pandas as pd\nfn = '../datasets/company-stock-movements-2010-2015-incl.csv'\nstocks_df = pd.read_csv(fn, index_col=0)\n", "intent": "**Step 1:** Load the data _(written for you)_\n"}
{"snippet": "import pandas as pd\nfn = '../datasets/company-stock-movements-2010-2015-incl.csv'\nstocks_df = pd.read_csv(fn, index_col=0)\n", "intent": "**Step 1:** Load the data _(written for you)_.\n"}
{"snippet": "companies = list(stocks_df.index)\nmovements = stocks_df.values\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import make_pipeline\nnormalizer = Normalizer()\nkmeans = KMeans(n_clusters=14)\npipeline = make_pipeline(normalizer, kmeans)\npipeline.fit(movements)\n", "intent": "**Step 2:** Run your code from the previous exercise _(filled in for you)_.\n"}
{"snippet": "import pandas as pd\nseeds_df = pd.read_csv('../datasets/seeds.csv')\ndel seeds_df['grain_variety']\n", "intent": "**Step 1:** Load the dataset _(written for you)_.\n"}
{"snippet": "data_path = votre_path + \"features.txt\"\nfeatures_names =    pd.read_csv(data_path,delim_whitespace=True,header=None)\ndata_path = votre_path + \"train/X_train.txt\"\nactivity_features = pd.read_csv(data_path,delim_whitespace=True,header=None,names=features_names.values[:,1])\nactivity_features.head()\n", "intent": "Importation de la base d'apprentissage de donnees de features\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../datasets/fish.csv')\nspecies = list(df['species'])\ndel df['species']\n", "intent": "**Step 1:** Load the dataset, extracting the species of the fish as a list `species` _(done for you)_\n"}
{"snippet": "normalizer = Normalizer()\n", "intent": "**Step 3:** Create an instance of `Normalizer` called `normalizer`.\n"}
{"snippet": "client_df = pd.DataFrame(np.array(client_data).reshape(3,3), columns=['RM','LSTAT','PTRATIO'])\ncorr = client_df.corr()\nsns.heatmap(corr,annot=True,xticklabels=['RM','LSTAT','PTRATIO'], yticklabels =['RM','LSTAT','PTRATIO'])\n", "intent": "Heatmap of the client data shows the positive and negative correlations.\n"}
{"snippet": "import matplotlib.image as mpimg\nimage1 = mpimg.imread('external-data/right-of-way.png')\nimage2 = mpimg.imread('external-data/pedestrians_1.png')\nimage3 = mpimg.imread('external-data/general_caution_1.png')\nimage4 = mpimg.imread('external-data/children-crossing_1.png')\nimage5 = mpimg.imread('external-data/stop.png')\nimage6 = mpimg.imread('external-data/no-truck-passing.png')\nX_test_new = [image1, image2, image3, image4, image5, image6]\ny_test_new = [11,27,18,28,14,10]\n", "intent": "Load the new  images\n"}
{"snippet": "import pandas as pd\nfn = '../datasets/company-stock-movements-2010-2015-incl.csv'\nstocks_df = pd.read_csv(fn, index_col=0)\ncompanies = list(stocks_df.index)\nmovements = stocks_df.values\n", "intent": "**Step 1:** Load the data _(written for you)_\n"}
{"snippet": "import pandas as pd\nscores_df = pd.read_csv('../datasets/eurovision-2016-televoting.csv', index_col=0)\ncountry_names = list(scores_df.index)\n", "intent": "**Step 1:** Load the DataFrame _(written for you)_\n"}
{"snippet": "data = pd.read_csv('data/weather-non-agg-DFE.csv')\nprint(data.shape)\ndata.head()\n", "intent": "To begin with let's load the data into a format we can work with.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nsamples = x_train[:2]\nx_train_counts = ...\nprint(pd.DataFrame(x_train_counts.A, columns=count_vect.get_feature_names()).to_string())\n", "intent": "Now let's see how we can transform our tweets into vectors\n"}
{"snippet": "df_pivot = df.pivot(values='n', columns='offer_id', index='customer_name').fillna(0)\n", "intent": "All the value ranges look ok. Let's get the pivot table.\n"}
{"snippet": "data_path = votre_path + \"train/y_train.txt\"\nactivity  =    pd.read_csv(data_path,delim_whitespace=True,header=None)\nactivity  =  activity.values[:,0] -  1\nactivity_names = ['WALKING','WALKING_UPSTAIRS','WALKING_DOWNSTAIRS','SITTING','STANDING','LAYING']\n", "intent": "Recuperations des types d'activites\n"}
{"snippet": "X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(X_res, y_res,\n                                                    stratify=y_res, test_size=0.3, random_state=42)\n", "intent": "We can see that the resampled variables have increased in size. Let's check the class balance.\n"}
{"snippet": "col_names = ['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', \n        'drive-wheels', 'engine-location', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type', \n        'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-rate', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\ncars = pd.read_csv(\"/Users/chesterjohn/Desktop/dataquest/jupyter-1-ml-fundamentals/imports-85.data\",names=col_names)\nprint(cars.head(2))\n", "intent": "Looks like our data do not contain headers names, so we have to manually add them.\n"}
{"snippet": "dataset = pd.read_csv(\"train.csv\")\nX = dataset.iloc[:,:-1].values\nY=dataset.iloc[:,-1].values\n", "intent": "Import dataset to train model\n"}
{"snippet": "X_train, X_test, y_train ,y_test = train_test_split(X,Y,test_size = 0.2,random_state=0)\n", "intent": "Split X to Train and Test set by 80% and 20% respectively\n"}
{"snippet": "scaler = StandardScaler()\nscaler.fit(X)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "Perform feature scalling on features to improve the performance of model\n"}
{"snippet": "dataset = pd.read_csv('train.csv')\n", "intent": "Import dataset to train model\n"}
{"snippet": "dataset = dataset.fillna(-1)\nX = dataset.iloc[:,1:-1].values\ny = dataset.iloc[:,-1].values\n", "intent": "Fill NaN with -1 and Split dataset to X and y\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)\n", "intent": "Split X to Train and Test set by 80% and 20% respectively\n"}
{"snippet": "dataset = pd.read_csv('train.csv')\nX = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values\n", "intent": "Import dataset to train model and split dataset to X and y\n"}
{"snippet": "data_path = votre_path + \"train/subject_train.txt\"\nsujet =  pd.read_csv(data_path,delim_whitespace=True,header=None)\nsujet =sujet.values[:,0]\n", "intent": "Recuperations des sujets\n"}
{"snippet": "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n", "intent": "Split X to Train and Test set by 80% and 20% respectively\n"}
{"snippet": "from sklearn.datasets import fetch_california_housing\nhousing = fetch_california_housing()\n", "intent": "for california housing dataset\n"}
{"snippet": "data = pd.read_csv('bitcoin_price.csv', index_col='trading_day')\nnext_day = data['price_usd'].iloc[1:]\ndata = data.iloc[:-1,:]\ndata['next_day'] = next_day.values\nprint data.shape\n", "intent": "Create the outcome variable `next_day`.\n"}
{"snippet": "def merge_columns(main, other):\n    result = pd.merge(left=main,right=other, how='outer', left_on='date', right_on='date')\n    return result\ntrends = pd.read_csv('bitcoin_trends.csv')\ndata = merge_columns(data, trends)\n", "intent": "<img src='bitcoin_trend2.png' width=400px>\n"}
{"snippet": "features = list(set(selected_columns) - set(['next_day']))\ny = df['next_day']\nX = df[features]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "Split the data into training and test set.\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nmin_max_scaler = MinMaxScaler()\nlongitude_minmax = min_max_scaler.fit_transform(X['X'])\nlongitude_minmax_df = pd.DataFrame(longitude_minmax, columns=['scaled_longitude'])\npos_X = X.copy()\ndel pos_X['X']\npos_X = pd.concat([longitude_minmax_df, pos_X], axis=1)\npos_X.head()\n", "intent": "We can use MinMaxScaler() with a range of [0,1] to address this problem.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n", "intent": "Use `train_test_split()` to generate a 70/30 train/test split and store this in `X_train`, `X_test`, `y_train`, `y_test`.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "Use `train_test_split()` to generate a 70/30 train/test split and store this in `X_train`, `X_test`, `y_train`, `y_test`.\n"}
{"snippet": "scaled_features = pd.DataFrame(scaled_data, columns=['Image.Var', 'Image.Skew', 'Image.Curt', 'Entropy'])\nscaled_features.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data_path = votre_path + \"train/Inertial Signals/body_acc_x_train.txt\"\nacc_x = pd.read_csv(data_path,delim_whitespace=True,header=None).values\ndata_path = votre_path + \"train/Inertial Signals/body_acc_y_train.txt\"\nacc_y = pd.read_csv(data_path,delim_whitespace=True,header=None).values\ndata_path = votre_path + \"train/Inertial Signals/body_acc_z_train.txt\"\nacc_z = pd.read_csv(data_path,delim_whitespace=True,header=None).values\n", "intent": "Importation des donnees brute accelerometre\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "import pandas as pd\nimport glob\npath =r'/Users/mtlee/Code/DAT_mtl/nytfiles/' \nallFiles = glob.glob(path + \"/nyt*.csv\")\nframe = pd.DataFrame()\nlist_ = []\nfor file_ in allFiles:\n    df = pd.read_csv(file_,index_col=None, header=0)\n    list_.append(df)\nframe = pd.concat(list_, axis=0)\n", "intent": "Put together the NYT data from 30 different files into one file. Run df.shape and report the dimensions of your final dataframe.\n"}
{"snippet": "from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy = 'mean')\n", "intent": "[sklean impute docs](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n"}
{"snippet": "from sklearn.datasets import load_iris \niris = load_iris() \nnumSamples, numFeatures = iris.data.shape \n", "intent": "<h3>Dimensional Reduction for IRIS data set</h3>\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nX_r  = pca.fit_transform(X)\nprint(np.shape(X), np.shape(X_r))\n", "intent": "Tools for data mining and analysis. For now we will just focus on dimensionality reduction \nhttp://scikit-learn.org/stable/index.html\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nprint (digits.data.shape)\nimagea = digits.images\ntargeta = digits.target\nprint (imagea.shape, targeta.shape)\n", "intent": "<h3> Visualizing Handwritten Digits </h3>\nload the digit data set\n"}
{"snippet": "df = pd.read_csv('resources/train_house.csv')\ndf.head()\n", "intent": "Working with Real Estate Dataset\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_housing_data = scaler.fit_transform(housing.data)\nscaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n", "intent": "When using Gradient Descent, recall that it is important to first normalize the input feature vectors, or else training may be much slower.\n"}
{"snippet": "air_quality = pd.read_csv(fname, header=4, skipfooter=4, na_values='No data', engine='python')\n", "intent": "Let's try to read the data using `pandas.read_csv()` function.\n"}
{"snippet": "pca_features_myact = PCA(n_components=2).fit(preprocessing.scale(features_myact)).transform(features_myact)\npca_Comp0 = pca_features_myact[:,0]\npca_Comp1 = pca_features_myact[:,1]\npca_Comp0.shape\n", "intent": "Projection sur 2 axes de l'ACP\n"}
{"snippet": "age_gender_bkts_data = pd.read_csv(\"age_gender_bkts.csv\")\n", "intent": "This file contains the census data of age and gender distribution for the 10 destination countries as of 2015.\n"}
{"snippet": "countries_data = pd.read_csv(\"countries.csv\")\n", "intent": "This file contains the latitude, longitude, distance to US, area, and language of 10 destination countries.\n"}
{"snippet": "train.pivot_table(values='Loan_Status',index=['Credit_History'],aggfunc=lambda x: x.map({'Y':1,'N':0}).mean())\n", "intent": "Probability of getting loan for each Credit History class:\n"}
{"snippet": "ss = MinMaxScaler()\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "pca = PCA(n_components=X.shape[1])\npca.fit(X)\n", "intent": "Compute a PCA with the maximum number of components.\n"}
{"snippet": "print(pca.explained_variance_ratio_)\nK = 2\npca = PCA(n_components=X.shape[1])\npca.fit(X)\nPC = pca.transform(X)\n", "intent": "Retrieve the explained variance ratio. Determine $K$ the number of components.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('Advertising.csv', index_col=0)\ndata.rename(columns = {'radio':'Radio','newspaper':'Newspaper','sales':'Sales'},inplace = True)\ndata.head()\n", "intent": "The data set we are using is: Advertising.csv\n"}
{"snippet": "data = data.iloc[np.random.permutation(len(data))]\ndata.reset_index(drop = True, inplace = True)\nx = data.drop(['Target'], axis = 1)\ny = data.Target\ntrain_examples = 100000\nfrom sklearn.cross_validation import train_test_split\nx_train, x_test, y_train, y_test = train_test_split( x, y, train_size = train_examples )\n", "intent": "Recreating training and test sets:\n"}
{"snippet": "train = pd.read_csv('train.csv')\n", "intent": "Simple data loading and transformations:\n"}
{"snippet": "pca = PCA(n_components=3)\npca.fit(basket_data_num_nor)\npcaBasket3 = pca.transform(basket_data_num_nor)\nComp0 = pcaBasket3[:,0]\nComp1 = pcaBasket3[:,1]\nComp2 = pcaBasket3[:,2]\n", "intent": "Extraction des trois premieres composantes\n"}
{"snippet": "survived_sex = data[data['Survived']==1]['Sex'].value_counts()\ndead_sex = data[data['Survived']==0]['Sex'].value_counts()\ndf = pd.DataFrame([survived_sex,dead_sex])\ndf.index = ['Survived','Dead']\ndf.plot(kind='bar',stacked=True, figsize=(13,8))\n", "intent": "Perfect.\nLet's now make some charts.\nLet's visualize survival based on the gender.\n"}
{"snippet": "survived_embark = data[data['Survived']==1]['Embarked'].value_counts()\ndead_embark = data[data['Survived']==0]['Embarked'].value_counts()\ndf = pd.DataFrame([survived_embark,dead_embark])\ndf.index = ['Survived','Dead']\ndf.plot(kind='bar',stacked=True, figsize=(13,8))\n", "intent": "Let's now see how the embarkation site affects the survival.\n"}
{"snippet": "def recover_train_test_target():\n    global combined\n    train0 = pd.read_csv('../data/train.csv')\n    targets = train0.Survived\n    train = combined.ix[0:890]\n    test = combined.ix[891:]\n    return train,test,targets\n", "intent": "Recovering the train set and the test set from the combined dataset is an easy task.\n"}
{"snippet": "features = pd.DataFrame()\nfeatures['feature'] = train.columns\nfeatures['importance'] = clf.feature_importances_\n", "intent": "Let's have a look at the importance of each feature.\n"}
{"snippet": "def embarked_impute(train, test):\n    for i in [train, test]:\n        i['Embarked'] = i['Embarked'].fillna('S')\n    return train, test\n", "intent": "We fill the null values in the `Embarked` column with the most commonly occuring value, which is 'S.'\n"}
{"snippet": "train = pd.read_csv(\"train.csv\")\ntest    = pd.read_csv(\"test.csv\")\nfull = train.append( test , ignore_index = True )\ntitanic = full[ :891 ]\nprint train.shape\nprint test.shape\ndel train , test\nprint ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)\n", "intent": "Now that our packages are loaded, let's read in and take a peek at the data.\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "titanic = pd.read_csv(\"train.csv\")\ntitanic.head()\n", "intent": "Load train & test data\n======================\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\nlabelEnc=LabelEncoder()\ncat_vars=['Embarked','Sex',\"Title\",\"FsizeD\",\"NlengthD\",'Deck']\nfor col in cat_vars:\n    titanic[col]=labelEnc.fit_transform(titanic[col])\n    titanic_test[col]=labelEnc.fit_transform(titanic_test[col])\ntitanic.head()\n", "intent": "Convert Categorical variables into Numerical ones\n=================================================\n"}
{"snippet": "print gb_features\ncols = trainX.columns.values[1:]\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })\n", "intent": "Create a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package.\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\ndigits.images.shape\n", "intent": "We'll use scikit-learn's data access interface and take a look at this data:\n"}
{"snippet": "from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(numTrees=100, maxDepth=2, labelCol=\"label\", seed=42)\nrfModel = rf.fit(trainingData)\nrfFeatureImportance = pd.DataFrame([(name, rfModel.featureImportances[idx]) for idx, name in attrs],columns=['feature_name','feature_importance'])\nprint(rfFeatureImportance.sort_values(by = ['feature_importance'], ascending = False))\n", "intent": "1. Build and train a RandomForestClassifier and print out a table of feature importances from it.\n"}
{"snippet": "from sklearn import linear_model\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\nY = iris.target\nprint iris.DESCR\n", "intent": "Load the data set from Sci Kit Learn\n"}
{"snippet": "iris_data = DataFrame(X,columns=['Sepal Length','Sepal Width','Petal Length','Petal Width'])\niris_target = DataFrame(Y,columns=['Species'])\n", "intent": "Let's put the data into a pandas DataFrame.\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\nX = iris.data\nY = iris.target\nprint iris.DESCR\n", "intent": "First we'll start by importing the Data set we are already very familiar with, the Iris Data Set\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n", "intent": "Now we will split the data into a training set and a testing set and then train our model.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n", "intent": "Now that we have our model, we will continue by seperating into training and testing sets:\n"}
{"snippet": "train = read_csv(\"data/training.csv\")\nprint(train.count())\n", "intent": "Load the training data from the local file. This model will make use of a broken out dev set, so it won't need the test data.\n"}
{"snippet": "new_train = read_csv(\"data/training.csv\")\nall_data, all_labels = load(new_train, complete=False)\n", "intent": "Finding which images have points closest to the edges.\n"}
{"snippet": "PATH_TO_DATA = ('.')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n                                                random_state=2)\nprint(Xtrain.shape, Xtest.shape)\n", "intent": "Let's try a classification task on the digits. The first thing we'll want to do is split the digits into a training and testing sample:\n"}
{"snippet": "import pandas as pd\nbus = pd.read_csv(\"data/san_francisco/businesses.csv\", \n                             encoding='ISO-8859-1')\nins = pd.read_csv(\"data/san_francisco/inspections.csv\")\nvio = pd.read_csv(\"data/san_francisco/violations.csv\")\n", "intent": "One of the useful features of Pandas is its ability to load fairly messy \"CSV\" files:\n"}
{"snippet": "feature_weights = pd.DataFrame({\n        'features': training_data.drop('latest_score', axis=1).columns, \n        'weights': model.coef_})\nprint(feature_weights.sort_values('weights'))\n", "intent": "The most negative coefficient determine the the violations that most contributed to a reduction in score.\n"}
{"snippet": "import statsmodels.api as sm\nfrom statsmodels.formula.api import logit, glm, ols\ndat = pd.DataFrame(data, columns = ['Temperature', 'Failure'])\nlogit_model = logit('Failure ~ Temperature',dat).fit()\nprint logit_model.summary()\n", "intent": "Lets plot this data\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\nX = digits.data\ny = digits.target\n", "intent": "Load up the digits dataset, and create matrix X and vector y containing the predictor and response variables\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y)\n", "intent": "Break up the dataset into a training and testing set\n"}
{"snippet": "cv = CountVectorizer()\nX = cv.fit_transform(df.all_data)\n", "intent": "Now, lets try a basic CountVectorizer\n"}
{"snippet": "cv = CountVectorizer(stop_words='english')\nX = cv.fit_transform(df.all_data)\n", "intent": "We saw there was limited success there. We should enhance it by removing stop words\n"}
{"snippet": "cv = TfidfVectorizer(stop_words='english')\nX = cv.fit_transform(df.all_data)\n", "intent": "Perhaps we can try weighing words inversly by their frequency\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n", "intent": "Split the data to get going\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(xData, yData, test_size=0.3, random_state=0)\nmodel2 = LogisticRegression()\nmodel2.fit(X_train, y_train)\n", "intent": "So far, we have trained and tested on the same set. Let's instead split the data into a training set and a testing set.\n"}
{"snippet": "heroes[\"attack_type\"] = LabelEncoder().fit_transform(heroes[\"attack_type\"])\nheroes[\"primary_attr\"] = LabelEncoder().fit_transform(heroes[\"primary_attr\"])\nhero_categorical_features = [\"attack_type\", \"primary_attr\"]\n", "intent": "*Prepare categorical features*\n"}
{"snippet": "for i in range(1,6):\n    for team in [\"r\", \"d\"]:\n        for feature in [\"hero_id\"]:\n            fights[\"%s%s_%s\" % (team, i, feature)] = fights[\"%s%s_%s\" % (team, i, feature)].fillna(200)\n            fights[\"%s%s_%s\" % (team, i, feature)] = fights[\"%s%s_%s\" % (team, i, feature)].astype(\"uint8\")\n", "intent": "*Replace NaN and Float to Int32*\n"}
{"snippet": "for team in [\"radiant\", \"dire\"]:\n    player_features = [\"%s_%s_mean\" % (team, feature) for feature  in players_characteristics]\n    poly = PolynomialFeatures(2)\n    player_features_values = poly.fit_transform(fights[player_features].fillna(0))\n    player_features_names = [\"%s_feature_%s\" % (team, j) for j in range(player_features_values.shape[1])]\n    player_features_df = pd.DataFrame(player_features_values, columns=player_features_names)\n    fights = pd.concat([fights, player_features_df], axis=1)\n", "intent": "*Calculate polynom*\n"}
{"snippet": "for i in range(1,6):\n    for team in [\"r\", \"d\"]:\n        for feature in [\"hero\", \"items\", \"role\"]:\n            fights[\"%s%s_%s\" % (team, i, feature)] = fights[\"%s%s_%s\" % (team, i, feature)].fillna(200)\n            fights[\"%s%s_%s\" % (team, i, feature)] = fights[\"%s%s_%s\" % (team, i, feature)].astype(\"uint8\")\n", "intent": "*Replace NaN and Float to Int32*\n"}
{"snippet": "import numpy as np\nfrom sklearn import preprocessing\nlabels = np.array([1, 5, 3, 2, 1, 4, 2, 1, 3])\nlb = preprocessing.LabelBinarizer()\nlb.fit(labels)\nlb.transform(labels)\n", "intent": "Transforming your labels into one-hot encode vectors is pretty simple with scikit-learn.\n"}
{"snippet": "california_housing_dataframe = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\n", "intent": "Next, we'll load our data set.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n", "intent": "So, we want to have 2 datasets, train and test, each used for the named purpose exclusively.\n"}
{"snippet": "min_max_scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-1, 1))\nfeatures_scaled = min_max_scaler.fit_transform(features)\nprint(features_scaled.shape)\nprint(features_scaled.min(axis=0))\nprint(features_scaled.max(axis=0))\n", "intent": "Scale the features from -1 to 1\n"}
{"snippet": "print(\"Size of training set before: \", x_train.shape)\n(x_train, y_train), (x_test, y_test) = boston_housing.load_data(test_split=0.10)\nprint(\"Size of training set after: \", x_train.shape)\n", "intent": "You can also choose the proportion of training data you would like.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=1, stratify=y)\n", "intent": "Splitting data into 70% training and 30% test data:\n"}
{"snippet": "iris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\n", "intent": "First load the iris dataset into a pandas dataframe.\n"}
{"snippet": "pca = PCA(n_components=36)\npca.fit(X_train)\n", "intent": "It seems like the largest eigenvalues are roughly the first 40. We will use 36 prinicple components so the number is square like the original data.\n"}
{"snippet": "X_train_inv = pca.inverse_transform(X_train_pca)\n", "intent": "The data after PCA is not as interpretable because each\n"}
{"snippet": "specObj = Table.read(path.join(data_path, 'sdss', 'specObj-merged.hdf5'), \n                     path='specObj')\nspec_class = specObj['CLASS'].astype(str)\nspec_classes = np.unique(spec_class)\nfor cls in spec_classes:\n    print(cls, (spec_class == cls).sum())\n", "intent": "Let's take a look at how the spectral data was classified by SDSS into galaxies, QSOs, or stars\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\nnewsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\nnewsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)\n", "intent": "The dataset we'll use is the 20newsgroup dataset that is available from sklearn. This dataset has news articles grouped into 20 news categories\n"}
{"snippet": "import pandas as pd\nstemmer = SnowballStemmer(\"english\")\noriginal_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n           'traditional', 'reference', 'colonizer','plotted']\nsingles = [stemmer.stem(plural) for plural in original_words]\npd.DataFrame(data={'original word':original_words, 'stemmed':singles })\n", "intent": "Let's also look at a stemming example. Let's throw a number of words at the stemmer and see how it deals with each one:\n"}
{"snippet": "label_df = pd.read_csv(label_csv)\nlabel_df.head()\n", "intent": "In this data set the labels associated with each image are stored as a csv file\n"}
{"snippet": "label_df.pivot_table(index='breed', aggfunc=len).sort_values('id', ascending=False)\n", "intent": "Lets get an idea of class representation in the dataset\n"}
{"snippet": "label_df = pd.read_csv(f'{PATH}train.csv')\nlabel_df.head()\n", "intent": "The \"test\" and \"train\" folders contain images for the test set and training set. Lets look at the format the train.csv file:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n", "intent": "Standardizing the features:\n"}
{"snippet": "probs_embedded = TSNE(n_components=2).fit_transform(probs)\n", "intent": "Now to use TSNE to decompose the data into two dimensions\n"}
{"snippet": "test = pd.read_csv('test.csv')\ntrain = pd.read_csv('train.csv')\nfull_set = train.append(test, ignore_index=True, sort=True)\ndata = full_set.copy()\ndata = data.drop('PassengerId',1)\n", "intent": "Data is provided by Kaggle, available at https://www.kaggle.com/c/titanic/data\n"}
{"snippet": "family = pd.DataFrame()\nfamily[ 'FamilySize' ] = full_set[ 'Parch' ] + full_set[ 'SibSp' ] + 1\n", "intent": "Both these values refer to the family size of the passenger on board, so lets wrap them up into a single variable.\n"}
{"snippet": "movies = pd.read_csv(path+'movies.csv')\nmovies.head()\n", "intent": "We can also connect movie Ids to movie titles.\n"}
{"snippet": "pca2 = PCA(n_components=2)\nmovie_cluster = pca2.fit_transform(movie_emb)\nmovie_cluster.shape\n", "intent": "We can also use PCA to dimension reduce and cluster the movie values themselves instead of looking at principal components.\n"}
{"snippet": "probs_trans = manifold.TSNE(n_components=2, perplexity=15).fit_transform(preds)\n", "intent": "We can perform t-SNE on our model's output vectors. As these vectors are from the final classification, we would expect them to cluster well.\n"}
{"snippet": "df = pd.read_csv(path/'train.csv')\ndf.head()\n", "intent": "The dataset in question contains text quenstions from Quora. Questions are labeled as either sincere or insincere.\n"}
{"snippet": "class_desc = pd.read_csv(f'{PATH}/challenge-2018-class-descriptions-500.csv', header=None, names=['Code', 'Class'])\nclass_desc.head()\n", "intent": "Classes in this data set are all represented by a code. There are 500 classes in the entire data set.\n"}
{"snippet": "newDF = pd.DataFrame(scaled_features,columns=df.columns[:-1])\nnewDF.info()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df_new = pd.read_csv('./data/bank-marketing-data/bank-unseen-data.csv')\n", "intent": "Load new data from './data/bank-marketing-data/bank-unseen-data.csv'\n"}
{"snippet": "print(\"total variance:\", np.sum(np.var(X,0)))\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X)\nprint(\"variance explained via the first and second components:\\n\" , pca.explained_variance_)\nprint(\"principal components:\\n\", pca.components_)\n", "intent": "(in this particular example ~97.6% of the variance is preserved if we project down to the leading principal component)\n"}
{"snippet": "data=pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/Bonus/titanic3.csv\").dropna(subset=['age'])\ntarget = data['survived']\nX = data[['age', 'sex', 'pclass', 'sibsp', 'parch']]\nX = pd.get_dummies(X)\nX_train, X_test, target_train, target_test = train_test_split(\n    X, target, test_size=0.25, random_state=1\n)\n", "intent": "Consider the Titanic dataset below\n"}
{"snippet": "bcs = pd.read_csv('https://github.com/estimand/teaching-datasets/raw/master/british-crime-survey/bcs.csv')\n", "intent": "Read in the British Crime Survey 2007-2008 dataset.\n"}
{"snippet": "whites = pd.read_csv(WHITES_URL, sep=';')\n", "intent": "Read in the Wine Quality dataset.\n"}
{"snippet": "iris = pd.read_csv(IRIS_URL, header=None, names=var_names)\n", "intent": "Read in the Iris dataset.\n"}
{"snippet": "crypto = pd.read_csv('https://raw.githubusercontent.com/estimand/teaching-datasets/master/cryptocurrencies/cryptocurrencies.csv')\n", "intent": "Read in the cryptocurrencies dataset.\n"}
{"snippet": "crypto_close = crypto.pivot_table(values='close', index='date', columns='symbol')\n", "intent": "Pivot close prices.\n"}
{"snippet": "boroughs = pd.read_csv(BOROUGHS_URL, encoding='iso-8859-1')\n", "intent": "Read in the London Borough Profiles datasets.\n"}
{"snippet": "boroughs_mds = smds.fit_transform(boroughs)\n", "intent": "Two-dimensional projection ('embedding') of 'boroughs'\n"}
{"snippet": "url = 'https://gist.githubusercontent.com/podopie/5ea0c35ecc556d6cbae3/raw/c56f694bf4e7bbeeec92e24d33a8f49f7da37be8/mammals.csv'\nanimals = pd.read_csv(url)\nprint animals.describe()\n", "intent": "Here we'll work with a very simple data set of one input (animal body weight) to find the relationship with a response (animal brain weight)\n"}
{"snippet": "h2020 = pd.read_csv(H2020_URL, sep=';', decimal=',')\n", "intent": "Read in the H2020 dataset.\n"}
{"snippet": "vectorizer = fe.text.CountVectorizer(\n    stop_words='english',\n    ngram_range=(1, 2),\n    min_df=5\n)\n", "intent": "Count words and 2-grams (combinations of two words) in the 'objective', keeping only those that occur at least 5 times.\n"}
{"snippet": "importances = pd.DataFrame({\n    'variable': vectorizer.get_feature_names() + ['totalCost'] + list(country_dummies.columns),\n    'importance': rf1.feature_importances_\n})\nimportances.sort_values('importance', ascending=False, inplace=True)\nimportances.head(10)\n", "intent": "Extract variable importances and sort in descending order.\n"}
{"snippet": "X_tfidf = vectorizer.fit_transform(h2020.objective)\nX_tfidf = hstack([X_tfidf, np.asmatrix(h2020.totalCost).T, country_dummies])\n", "intent": "Prepare the data (as above).\n"}
{"snippet": "importances = pd.DataFrame({\n    'variable': vectorizer.get_feature_names() + ['totalCost'] + list(country_dummies.columns),\n    'importance': rf2.feature_importances_\n})\nimportances.sort_values('importance', ascending=False, inplace=True)\nimportances.head(10)\n", "intent": "Extract variable importances and sort in descending order.\n"}
{"snippet": "vectorizer = fe.text.CountVectorizer(\n    stop_words='english',\n    min_df=5\n)\nX = vectorizer.fit_transform(h2020.objective)\n", "intent": "Count words in the 'objective', keeping only those that occur at least 5 times.\n"}
{"snippet": "with ZipFile(io.BytesIO(requests.get(SMS_URL).content)) as zf:\n    sms = pd.read_table(zf.open('SMSSpamCollection'), names=['class', 'text'])\n", "intent": "Read in the SMS spam dataset.\n"}
{"snippet": "def pd_centers(featuresUsed, centers):\n    colNames = list(featuresUsed)\n    colNames.append('prediction')\n    Z = [np.append(A, index) for index, A in enumerate(centers)]\n    P = pd.DataFrame(Z, columns=colNames)\n    P['prediction'] = P['prediction'].astype(int)\n    return P\n", "intent": "Let us first create some utility functions which will help us in plotting graphs:\n"}
{"snippet": "train_data = pd.read_csv('./train.csv', parse_dates=['datetime'])\ntest_data = pd.read_csv('./test.csv', parse_dates=['datetime'])\ntest_count = pd.read_csv('./test_solution.csv', parse_dates=['datetime'])\ntrain_data.head()\n", "intent": "The bike dataset is devided into training and test sets. The former is used to learn the model, whereas the later is left for evaluation.\n"}
{"snippet": "url = 'https://gist.githubusercontent.com/podopie/5ea0c35ecc556d6cbae3/raw/c56f694bf4e7bbeeec92e24d33a8f49f7da37be8/mammals.csv'\nanimals = pd.read_csv(url)\nanimals['body'].values\n", "intent": "Here we'll work with a very simple data set of one input (animal body weight) to find the relationship with a response (animal brain weight)\n"}
{"snippet": "pcask = PCA(n_components=3)\nY = pcask.fit_transform(xStand) \nprint(pcask.fit(xStand).components_) \n", "intent": "Now that we have discovered the principal componants, we have an educated idea on how many componants to pass to the function.\n"}
{"snippet": "X_piv = pd.pivot_table(X_fin, index='full_name', aggfunc=np.mean).drop(['season', 'max_pt_season_bin'], axis=1)\ny_piv = pd.pivot_table(y, index='full_name', aggfunc=np.mean).drop(['season'], axis=1)\ndraft_eligible_17 = pd.pivot_table(draft_eligible_17, index='full_name', aggfunc=np.mean).drop(['season', 'max_pt_season_bin'], axis=1, errors='ignore')\nlen(X_piv), len(y_piv), len(draft_eligible_17)\n", "intent": "This will allow us to see a players full body of work and use an average of those metrics. As you can see it ends up to about \n"}
{"snippet": "df = pd.read_csv(\"../assets/datasets/votes.csv\")\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "df.to_csv(\"votes_corrected.csv\")\n", "intent": "Next, let's define the x and y variables: \n"}
{"snippet": "confusion_mat = np.array(confusion_matrix(y_test, predictions, labels = [1,0]))\nconfusion = pd.DataFrame(confusion_mat, index=['over_200k', 'under_200k'],\n                         columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "dataset = load_iris()\ndata =  pd.DataFrame(dataset.data, columns = dataset.feature_names)\ntarget =  pd.Series(dataset.target)\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "results = pd.DataFrame(data = [labels, target], index = [\"predicted\", \"actual\"]).T\nresults['difference'] = results['predicted'] - results['actual']\ncolored_error = results[results['difference'] != 0]['difference'].abs().map({1: 'b', 2: 'r'})\nerror_size = results['difference'].abs().map({0: 0, 1: 100, 2: 100})\n", "intent": "First, go ahead and plot the results of your clustering analysis\n"}
{"snippet": "def nb_cross_validation(x_data = X, y_data = y):\n    train_X, test_X, train_y, test_y = model_selection.train_test_split(X, y)\n    naive_bayes_model.fit(train_X, train_y)\n    return [naive_bayes_model.score(test_X, test_y), naive_bayes_model]\n", "intent": "Define the target and feature set for the test data\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nscaled_X = ss.fit_transform(X)\nprint X.shape\nscaled_X.shape\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\nlabel_encoder = enc.fit(titanic_X[:, 2])\nprint \"Categorical classes:\", label_encoder.classes_\ninteger_classes = label_encoder.transform(label_encoder.classes_)\nprint \"Integer classes:\", integer_classes\nt = label_encoder.transform(titanic_X[:, 2])\ntitanic_X[:, 2] = t\n", "intent": "Class and sex are categorical classes. Sex can be converted to a binary value (0=female,1=male):\n"}
{"snippet": "dir_vec = CountVectorizer(ngram_range=(2,3), strip_accents = 'ascii')\ndir_vec.fit(movies['Actors'])\ndirector_bigrams = pd.DataFrame(dir_vec.transform(movies['Director']).todense(),\n                      columns = dir_vec.get_feature_names())\n", "intent": "These are the most frequently occuring 20 actors to appear in the NYT top 1000 movies.\n"}
{"snippet": "rfecv = RFECV(estimator = DecisionTreeRegressor(), cv = 5, scoring = 'mean_squared_error')\nrfecv.fit(X,y)\nrfecv_cols = X.columns[rfecv.support_]\n", "intent": "To narrow down the number of features to consider when building a tree, I'll use cross-validated recursive feature elimination.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import train_test_split\nX = vectorizer.transform(tomatoes.Phrase)\ny = tomatoes.Sentiment\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5)\nmodel = LogisticRegression(multi_class='multinomial', solver='newton-cg')\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n", "intent": "Next, we transform the individual phrases using our vectorizer, create a train test split, and fit a logistic regression model.\n"}
{"snippet": "X_crime = pd.get_dummies(crime.DayOfWeek)\ny_crime = crime.Category\nX_crime_train, X_crime_test, y_crime_train, y_crime_test = \\\n    train_test_split(X_crime, y_crime, train_size=.05)\nmodel_crime = LogisticRegression(multi_class='multinomial', solver='newton-cg')\nmodel_crime.fit(X_crime_train, y_crime_train)\n", "intent": "Next, we'll do a quick spin through the SF Crime dataset to see another example of visualizing a learned model.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_with_num_words, y, train_size=0.5)\ntree_model_with_num_words = DecisionTreeClassifier()\ntree_model_with_num_words.fit(X_train, y_train)\ntree_model_with_num_words.score(X_test, y_test)\n", "intent": "This has been good to visualize what is going on with the decision tree, but let's see how well we can do if we don't limit the depth.\n"}
{"snippet": "scaler = StandardScaler()\nscaler.fit(customer_features)\ncustomer_sc = scaler.transform(customer_features)\ncustomer_sc_df = pd.DataFrame(customer_sc, columns=customer_features.columns)\nsc_stats = customer_sc_df.describe().T\nsc_stats['skew'] = st.skew(customer_features)\nsc_stats['kurt'] = st.kurtosis(customer_features)\ndisplay(stats)\ndisplay(sc_stats)\n", "intent": "$$Z = \\frac{X-\\mu}{\\sigma}$$\n"}
{"snippet": "iris = load_iris()\n", "intent": "In this step we will load and store the iris data set into a variable.\n"}
{"snippet": "data.pivot_table(index=['platform','release_year'],columns=['editors_choice'],values=['title'],aggfunc='count')\n", "intent": "Does number of games by a platform in a given year have any effect on these awards?\n"}
{"snippet": "data = data.fillna(data.dropna().median())\ndata.shape\n", "intent": "It delete all those rows which contain missing values. Now we take its median and impute values to the null\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(titanic_X, titanic_y, test_size=0.25, random_state=33)\n", "intent": "Separate training and test sets\n"}
{"snippet": "cdf = pd.DataFrame(lm.coef_[0], index=X_train.columns, columns=['coefficients'])\ncdf\n", "intent": "Create a table showing the coefficient (influence) of each of the columns\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n", "intent": "Now let us use scikit-learn to easily transform this dataset\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "df = pd.read_csv('data/Train.zip')\ndf.head(2)\n", "intent": "Putting it all together: creating a matrix of heterogeneous data types.\n"}
{"snippet": "df = pd.read_csv('data/train.csv')\n", "intent": "Load the data: train.csv\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "Create test and training set.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "Create test and training set.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(lowercase = True,token_pattern= \"(?u)\\\\b\\\\w\\\\w+\\\\b\")\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "df = pd.read_csv('College_Data.csv', index_col = 0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "pca = PCA()\npca.fit(data_transposed_scaled)\n", "intent": "Now that all the data has been generated, we can build the PCA model.\n"}
{"snippet": "rf.fit(train[features_to_use], train['final_status'])\nimportances = pd.DataFrame({'feature':train[features_to_use].columns,'importance':np.round(rf.feature_importances_,3)})\nimportances = importances.sort_values(by = 'importance', ascending=False)\nimportances\n", "intent": "Lets do feature selection using Random forest and we are going to choose 34 most important features\n"}
{"snippet": "tfidf = TfidfVectorizer(max_features=150)\n", "intent": "The maximum number of features for tfidf vectorizer would be taken as 150\n"}
{"snippet": "exercise = pd.read_csv('myfitnesspal/Exercise-Summary.csv', index_col = 0)\nmeasurement = pd.read_csv('myfitnesspal/Measurement-Summary.csv', index_col = 0)\nnutrition = pd.read_csv('myfitnesspal/Nutrition-Summary.csv', index_col = 0)\n", "intent": "First I will read the data I have into a pandas dataframes:\n"}
{"snippet": "measurement_agg['Height'] = 1.76  \nmeasurement_agg['BMI'] = np.round(measurement_agg['Weight'] / measurement_agg['Height']**2,2)\nmeasurement_agg['Age'] = np.round((measurement_agg.index - pd.Timestamp('1988-06-07')) / pd.Timedelta(days=365),1)\nmeasurement_agg['dW'] = measurement_agg['Weight'].diff(periods=1)\nmeasurement_agg.at[measurement_agg.index[0],'dW'] = 0\nmeasurement_agg.fillna(value=0, inplace=True)\nmeasurement_agg.head()\n", "intent": "Now let's fill in the data in the rest of the columns:\n"}
{"snippet": "itrain, itest = train_test_split(xrange(df.shape[0]), train_size=0.6)\nmask=np.ones(df.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\n", "intent": "In this lab we'll consider classification but Decision trees can be use for regression (prediction of continuous outcomes) as well.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = pd.read_csv(\"../data/house_prices.csv\")\ndata.head()\n", "intent": "***\n- Now, let's have a look at the data \n - Every row displays the Price and Area of each house\n"}
{"snippet": "labels_df = pd.read_csv(IMAGE_NET_LABELS_PATH, sep='\\\\t', header=None, names=['id','labels'])\nlabels_df.head(5)\n", "intent": "Lettura del file words di ImageNet come PandaDF. A ogni id (cartella che contiene immagini per le classi fornite) vengono assegnati i label\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\ndigits = datasets.load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n", "intent": "In this section we study how different estimators maybe be chained.\n"}
{"snippet": "from sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\npca = PCA(n_components=5)\npca.fit(X_train)\nX_train_reduced = pca.transform(X_train)\nX_test_reduced = pca.transform(X_test)\nlogistic = LogisticRegression()\nlogistic.fit(X_train_reduced, y_train)\nlogistic.score(X_test_reduced, y_test)\n", "intent": "You can apply the dimensionality reduction manually, like so:\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ncategories = ['alt.atheism',\n              'talk.religion.misc',\n              'comp.graphics',\n              'sci.space']\ntwenty_train_subset = fetch_20newsgroups(subset='train', categories=categories)\ntwenty_test_subset = fetch_20newsgroups(subset='test', categories=categories)\n", "intent": "We will take a look at some of the twenty newsgroups dataset, another common dataset for classification. Note that the data is fetched from.\n"}
{"snippet": "s4 = pandas.read_csv('data/spectra_4.csv')\nf = pandas.read_csv('data/freq.csv')\nc4 = s4['concentration']\nm4 = s4['molecule']\ns4 = s4['spectra']\n", "intent": "We are going to use `spectra_4.csv` for testing a model learned on the previous data\n"}
{"snippet": "def read_spectra(path_csv):\n    s = pandas.read_csv(path_csv)\n    c = s['concentration']\n    m = s['molecule']\n    s = s['spectra']\n    x = []\n    for spec in s:\n        x.append(numpy.fromstring(spec[1:-1], sep=','))\n    s = pandas.DataFrame(x)\n    return s, c, m\n", "intent": "We can define a function which will read the data and process them.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,train_size=0.7)\nx_train\n", "intent": "<ul>\n<li><code>train_size</code></li>\n<li><code>test_size</code></li>\n</ul>\n"}
{"snippet": "filedata='SVM_Dataset2.csv'\ndata2=pd.read_csv(filedata)\ndata2\n", "intent": "Now we upload the second dataset.\n"}
{"snippet": "import pandas as pd\npath = 'data/yelp.csv'\nreview = pd.read_csv(path, index_col = 0)\n", "intent": "Read **`yelp.csv`** into a pandas DataFrame and examine it.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nvect.fit(X_train)\n", "intent": "Use CountVectorizer to create **document-term matrices** from X_train and X_test.\n"}
{"snippet": "Y_pred = Y_pred_6\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('titanic.csv', index=False, columns=[\"Survived\", \"PassengerId\"])\n", "intent": "Here is our code to output our solution to a .csv file. Just modify the Y_pred variable to test each classifier. \n"}
{"snippet": "data['Cabin'].fillna('U0', inplace=True)\ndata['CabinSection'] = LabelEncoder().fit_transform(data['Cabin'].map(lambda x: x[0]))\ndata['CabinDistance'] = data['Cabin'].map(lambda x: x[1:])\ndata['CabinDistance'] = data['CabinDistance'].map(lambda x: x.split(' ')[0])\ndata['CabinDistance'].where(data['CabinDistance'] != '', '0', inplace=True)\ndata['CabinDistance'] = data['CabinDistance'].map(lambda x: int(x))\ndata.head()\n", "intent": "In this attempt we include all the cleaning inside one function and included comment to explain what we do in each step\n"}
{"snippet": "data.drop('PassengerId', axis=1, inplace=True)\ntraining_survived = data['Survived'].dropna()\ndata['Survived'].fillna(-1, inplace=True)\n", "intent": "This attemp fo similar data cleaning as the previous attemp. However, we introduce new columns and also standardizing almost all the column.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = CountVectorizer()\nX_train = vectorizer.fit_transform(twenty_train_subset.data)\n", "intent": "Here are some ways to generate features from the text:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0)\n", "intent": "Use different data sets for training and testing a model (generalization)\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n", "intent": "It is better to scale the data so that different features/channels have similar mean/std.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nclass_le = LabelEncoder()\ny = class_le.fit_transform(df['classlabel'].values)\ny\n", "intent": "We can use LabelEncoder in scikit learn to convert class labels automatically.\n"}
{"snippet": "import pandas as pd\ndf_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'\n                      'machine-learning-databases/wine/wine.data',\n                      header=None)\ndf_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n                   'Color intensity', 'Hue',\n                   'OD280/OD315 of diluted wines', 'Proline']\ndf_wine.head()\n", "intent": "Use the wine data set as it has 13 features for dimensionality reduction\n"}
{"snippet": "from distutils.version import LooseVersion as Version\nfrom sklearn import __version__ as sklearn_version\nif Version(sklearn_version) < '0.18':\n    from sklearn.cross_validation import train_test_split\nelse:\n    from sklearn.model_selection import train_test_split\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3, random_state=0)\n", "intent": "Splitting the data into 70% training and 30% test subsets.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA()\nX_train_pca = pca.fit_transform(X_train_std)\npca.explained_variance_ratio_\n", "intent": "PCA is actually part of scikit-learn, so we can use it directly instead of going through the code above.\n"}
{"snippet": "import pandas as pd\nwdbc_source = '../datasets/wdbc/wdbc.data'\ndf = pd.read_csv(wdbc_source, header=None)\n", "intent": "Malignant versus benign tumor cells based on 30 features (large enough for experiments in this chapter)\n"}
{"snippet": "import pandas as pd\nwdbc_source = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\ndf = pd.read_csv(wdbc_source, header=None)\n", "intent": "We will use the Wisconsin breast cancer dataset for the following questions\n"}
{"snippet": "if not os.path.isfile(csv_file):\n    df.to_csv(os.path.join(basepath, csv_filename), index=False, encoding='utf-8')\n", "intent": "Optional: Saving the assembled data as CSV file:\n"}
{"snippet": "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,5))\nX_train = vectorizer.fit_transform(twenty_train_subset.data)\n", "intent": "We can put this together with our other tricks as well.\n"}
{"snippet": "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True) \nraw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1] \nraw_tfidf \n", "intent": "As we can see, the results match the results returned by scikit-learn's `TfidfTransformer` (below).\n"}
{"snippet": "df = pd.read_csv('data/multiTimeline.csv', skiprows=1)\ndf.head()\n", "intent": "* Import data that you downloaded and check out first several rows:\n"}
{"snippet": "from keras.datasets import cifar10\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\nprint(\"Complete\")\n", "intent": "Start by importing the data from the pickle file.\n"}
{"snippet": "iris_data = pd.read_csv('./data/iris.csv') \n", "intent": "We can read the data with the help of pandas using the `read_csv` method\n"}
{"snippet": "f = open(\"data/brief_comments.txt\", \"r\") \ndogs = f.read() \nf.close() \n", "intent": "An alternative way of reading data from a file is to use the `with` statement.\n"}
{"snippet": "dfcars=pd.read_csv(\"data/mtcars.csv\")\ndfcars.head()\n", "intent": "Now let's read in some automatible data as a pandas *dataframe* structure.  \n"}
{"snippet": "dfcars=pd.read_csv(\"data/mtcars.csv\")\ndfcars.head()\n", "intent": "Now let's read in some automobile data as a pandas *dataframe* structure.  \n"}
{"snippet": "import pandas as pd\ndfcars = pd.read_csv(\"data/mtcars.csv\")\ndfcars = dfcars.rename(columns={\"Unnamed: 0\":\"car name\"})\ndfcars.head()\n", "intent": "We begin by loading up the `mtcars` dataset and cleaning it up a little bit.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntraindf, testdf = train_test_split(dfcars, test_size=0.2, random_state=42)\n", "intent": "Next, let's split the dataset into a training set and test set.\n"}
{"snippet": "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\nXtis = vectorizer.fit_transform(df['quote'])\nY = (df['fresh'] == 'fresh').values.astype(np.int8)\nxtraintis, xtesttis, ytraintis, ytesttis = train_test_split(Xtis, Y)\n", "intent": "Do both together help?\n"}
{"snippet": "sample_df = pd.DataFrame(dict(x=x[indexes],f=f[indexes],y=y[indexes])) \nsample_df.head()\n", "intent": "Moving on, let's get the $60$ random samples from our dataset.\n"}
{"snippet": "fold = 0\nfor train, valid in KFold(n_folds, shuffle=True).split(range(48)): \n    for d in degrees:\n        train_set = PolynomialFeatures(d).fit_transform(xtrain[train].reshape(-1,1))\n        valid_set = PolynomialFeatures(d).fit_transform(xtrain[valid].reshape(-1,1))\n        train_errors[d, fold], valid_errors[d, fold] = compute_MSE(train_set, ytrain[train], valid_set, ytrain[valid])\n    fold += 1\n", "intent": "Now let's try to run things and see what we get.\n"}
{"snippet": "wines_df = pd.read_csv(\"data/wines.csv\", index_col=0)\nwines_df.head()\n", "intent": "We do the usual read-in and verification of the data:\n"}
{"snippet": "from IPython.core.display import HTML\ndef css_styling(): styles = open(\"cs109.css\", \"r\").read(); return HTML(styles)\ncss_styling()\n", "intent": "**Harvard University**<br/>\n**Summer 2018**<br/>\n**Instructors**: Pavlos Protopapas, Kevin Rader\n<hr style=\"height:2pt\">\n"}
{"snippet": "dataDIR = './data'\nlogFile = os.path.join(dataDIR, 'driving_log.csv')\nraw_data=pd.read_csv(logFile)\nprint('Number of images per camera in the log file : {}'.format(raw_data.shape[0]))\nraw_data.head(5)\n", "intent": "First we load the log file with the steering angles\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nprint('X_train shape:', X_train.shape)\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\ninput_shape =  (3,64,64)\n", "intent": "I do not use a validation split, but I have to validate that the model works well in localization independently anyway.\n"}
{"snippet": "def get_russian_holidays(year):\n    url = f'https://www.timeanddate.com/holidays/russia/{year}'\n    html = requests.get(url).content\n    table_df = pd.read_html(html)[0]\n    table_df = table_df.rename(columns={'Date': 'date'})\n    holidays = pd.to_datetime(table_df['date'], format='%b %d')\n    holidays = holidays.apply(lambda x: x.replace(year=year))\n    return holidays\n", "intent": "We will here generate the number of holidays in the previous month, the current month and the next month\n"}
{"snippet": "col_names = [f'item_tf_idf_{i}' for i in range(tf_idf_item.shape[1])]\ntf_idf_item_df = pd.DataFrame(tf_idf_item, columns=col_names)\nitem_nlp = pd.concat([item_nlp, tf_idf_item_df], axis=1)\nitem_nlp.drop(['item_name', 'item_category_id', 'item_name_nlp'], axis=1, inplace=True)\n", "intent": "Combine the TF-IDF results with the corresponding data frames\n"}
{"snippet": "on = ['shop_id', 'item_id']\nid_df = pd.merge(sales_train.loc[:, on], sales_test, how='outer', on=['shop_id', 'item_id'])\nid_df.loc[:,'ID'].fillna(-1, inplace=True)\nid_df.loc[:,'ID'] = id_df.loc[:,'ID'].astype('int32')\n", "intent": "**NOTE**: We do an outer join here as some combinations of `shop_id` and `item_id` is only present in the test-set\n"}
{"snippet": "text, Y = zip(*corpus)\ntext, Y = [], []\nfor pair in corpus:\n    text.append(pair[0])\n    Y.append(pair[1])\ntext = tuple(text)\nY = tuple(Y)\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(text)\n", "intent": "Here, we convert the data that came out of my function into the vector representation that we introduced before.\n"}
{"snippet": "best_xg_prediction = test_dt.loc[:, ['ID']]\nbest_xg_prediction.loc[:, 'item_cnt_month'] = xg_lvl_1_pred.clip(0, 20)\nbest_xg_prediction.set_index('ID', inplace=True)\nbest_xg_prediction.to_csv(generated_data.joinpath('best_xg_prediction.csv'))\nbest_xg_prediction.loc[:, 'item_cnt_month'].describe()\n", "intent": "This gives a score of $1.06557$, which is sligthly worse then the previous neural network submission. It is likely that overfitting is the reason.\n"}
{"snippet": "data = pd.read_csv(\"adult_us_postprocessed.csv\")\ndata.head()\n", "intent": "Let's read the dataset. This is a post-processed version of the [UCI Adult dataset](http://archive.ics.uci.edu/ml/datasets/Adult).\n"}
{"snippet": "from IPython.display import clear_output\nfrom tqdm import trange\nfrom pandas import DataFrame\newma = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\nenv_batch = EnvBatch(10)\nbatch_states = env_batch.reset()\nrewards_history = []\nentropy_history = []\n", "intent": "Just the usual - play a bit, compute loss, follow the graidents, repeat a few million times.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nimgs_train, imgs_val, points_train, points_val = train_test_split(imgs, points, test_size=0.1)\n", "intent": "Run the following code to obtain train/validation split for training neural network.\n"}
{"snippet": "sample_size = 200000\ndialogue_df = pd.read_csv(Path('data', 'dialogues.tsv'), sep='\\t').sample(sample_size, random_state=0)\nstackoverflow_df = pd.read_csv(Path('data', 'tagged_posts.tsv'), sep='\\t').sample(sample_size, random_state=0)\n", "intent": "Now, load examples of two classes. Use a subsample of stackoverflow data to balance the classes. You will need the full data later.\n"}
{"snippet": "test = pandas.read_csv('test.csv.gz')\n", "intent": "Select your best classifier and prepare submission file.\n"}
{"snippet": "train_ada = pandas.read_csv('reference/training.csv', sep=',')\ntest_ada = pandas.read_csv('reference/test.csv', sep=',', index_col='id')\n", "intent": "`training.csv` is a mixture of simulated signal, real background.\nIt has the following columns.\n`test.csv` has the following columns:\n"}
{"snippet": "pd.DataFrame(data=forest.feature_importances_,\n             index=list(bcw_train.columns),\n             columns=['importance']).sort_values(by='importance', ascending=False)\n", "intent": "5 - Determine the most important three features in your model, and then fit a new model using only those three features.  Comment on your results.\n"}
{"snippet": "pd.DataFrame(data=treereg.feature_importances_,\n             index=list(Xtrain.columns),\n             columns=['importance']).sort_values(by='importance', ascending=False)\n", "intent": "4 - Determine the top three features, and fit a new decision tree regressor to the data.  Comment on your results.\n"}
{"snippet": "text, Y = zip(*corpus)\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(text)\n", "intent": "How are we doing now?\n"}
{"snippet": "pd.DataFrame(data=forestreg.feature_importances_,\n             index=list(Xtrain.columns),\n             columns=['importance']).sort_values(by='importance', ascending=False)\n", "intent": "8 - Determine the top three features, and fit a new random forest regressor to the data.  Comment on your results.\n"}
{"snippet": "errors = pd.read_csv('./data/errors.csv')\nerrors.head()\n", "intent": "I would use a binomial distribution with $p=0.005$.\n"}
{"snippet": "iris_train = pd.read_csv('./data/iris_train.csv')\niris_test = pd.read_csv('./data/iris_test.csv')\n", "intent": "This is a very common dataset (slightly modified) of three varieties of the iris flower; it will work well for clustering.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nheader = ['user_id', 'item_id', 'rating', 'timestamp']\ndata_movie_raw = pd.read_csv('../data/ml-100k/u.data', sep='\\t', names=header)\ndata_movie_raw.head()\n", "intent": "First familiarize yourself with the data you downloaded, and then import the `u.data` file and take a look at the first few rows.\n"}
{"snippet": "pm2_regr_imp = pm2_regr.dropna(subset=['year', 'month', 'day', 'hour', 'pm2'])\nimp = Imputer(strategy = 'median')\npm2_regr_imp = imp.fit_transform(pm2_regr_imp)\n", "intent": "I'm going to drop rows with NAs for the columns year, month and hour, pm2; I'm imputing the median for all other columns:\n"}
{"snippet": "mms = MinMaxScaler()\nXtrain_norm = mms.fit_transform(Xtrain)\nXtest_norm = mms.transform(Xtest)\nknn.fit(Xtrain_norm, ytrain)\nprint(knn.score(Xtrain_norm, ytrain))\nprint(knn.score(Xtest_norm, ytest))\n", "intent": "Normalized dataset:\n"}
{"snippet": "ssc = StandardScaler()\nXtrain_std = ssc.fit_transform(Xtrain)\nXtest_std = ssc.transform(Xtest)\nknn.fit(Xtrain_std, ytrain)\nprint(knn.score(Xtrain_std, ytrain))\nprint(knn.score(Xtest_std, ytest))\n", "intent": "Standardized dataset:\n"}
{"snippet": "pca = PCA(n_components=1)\npca.fit(X)\nX_pca = pca.transform(X)\nprint(X_pca.mean(axis=0))\nprint(X_pca.var(axis=0))\nprint(np.corrcoef(X_pca.T))\n", "intent": "The two variables are highly correlated, so almost all the variance is explained by the first principal component.\n"}
{"snippet": "pca = PCA(n_components=20)\npca.fit(communities_scaled[:, :-1])\nXtrainpca = pca.transform(Xtrain)\nXtestpca = pca.transform(Xtest)\n", "intent": "In the solutions he has the data scaled, all the features imputed and so he chooses 40 components.\nThe models chosen are linear regression and SVM.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test, images_train, images_test = train_test_split(\n        data, digits.target, digits.images,  test_size=0.25, random_state=42)\nn_samples, n_features = X_train.shape\nn_digits = len(np.unique(y_train))\nlabels = y_train\nprint_digits(images_train, y_train, max_n=20)\nprint_digits(images_test, y_test, max_n=20)\nprint(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n      % (n_digits, n_samples, n_features))\n", "intent": "Build training and test set\n"}
{"snippet": "from sklearn.manifold import Isomap\niso = Isomap(n_components=2)\nmadelon_iso = iso.fit_transform(madelon)\n", "intent": "3 - Perform variable reduction, to two components, via Isomap, and plot the data to show the latent clusters.  Comment on your results.\n"}
{"snippet": "from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2)\nmadelon_tsne = tsne.fit_transform(madelon)\n", "intent": "5 - Perform variable reduction, to two components, via TSNE, and plot the data to show the latent clusters.  Comment on your results.\n"}
{"snippet": "pca = PCA(n_components=2)\nmadelon_pca = pca.fit_transform(madelon)\n", "intent": "6 - Perform variable reduction, to two components, via PCA, and plot the data to show the latent clusters.  Comment on your results.\n"}
{"snippet": "le = LabelEncoder()\nle.fit([2, 4])\nbreast['class'] = le.transform(breast['class'])\n", "intent": "Also, I'm encoding the class column with 0s and 1s:\n"}
{"snippet": "scl = StandardScaler()\nsfm = SelectFromModel(LassoCV())\nXtrain_sc = scl.fit_transform(Xtrain)\nXtest_sc = scl.transform(Xtest)\nXtrain_lm = sfm.fit_transform(Xtrain_sc, ytrain)\nXtest_lm = sfm.transform(Xtest_sc)\ntrain_scores, test_scores = validation_curve(Ridge(), Xtrain_lm, ytrain,\n                                             param_name='alpha',\n                                             param_range=[0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000])\n", "intent": "In solution he uses LassoCV with SelectFromModel:\n"}
{"snippet": "player_batting_cols = ['ab', 'r', 'h', 'double', 'triple', 'hr', 'rbi', 'sb', 'cs', 'bb']\nplayer = pd.read_csv('player.csv')\nbatting = pd.read_csv('batting.csv')\nplayer_batting = pd.merge(left=player, right=batting, left_on='player_id', right_on='player_id')\nplayer_batting = player_batting.loc[player_batting.year >= 2004, ['name_given', 'name_last', 'year'] + player_batting_cols]\n", "intent": "**Correlation doesn't imply causation**\n"}
{"snippet": "person = pd.DataFrame()\nperson['Height'] = [72]\nperson['Weight'] = [188]\nperson\n", "intent": "create a new person for whom we know their feature values but not their gender. Our goal is to predict their gender.\n"}
{"snippet": "from sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import f_classif\nfeat_selector= SelectKBest(score_func=f_classif, k=3)\nX_new= feat_selector.fit_transform(X, y)\nprint(X_new.shape)\nprint(feat_selector.pvalues_)\nprint(feat_selector.scores_)\n", "intent": "(https://en.wikipedia.org/wiki/F-test)\n"}
{"snippet": "from sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import mutual_info_classif\nfeat_selector= SelectKBest(score_func=mutual_info_classif, k=4)\nX_new= feat_selector.fit_transform(X, y)\nprint(X_new.shape)\nprint(feat_selector.pvalues_)\nprint(feat_selector.scores_)\n", "intent": "(http://scikitlearn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html\n"}
{"snippet": "vectorizer = CountVectorizer(max_features=5000)\nXtis = vectorizer.fit_transform(df['quote'])\nY = (df['fresh'] == 'fresh').values.astype(np.int8)\nxtraintis, xtesttis, ytraintis, ytesttis = train_test_split(Xtis, Y)\n", "intent": "Do both together help?\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "Primeiro carregue o dataset. Vamos dar uma roubada e usar o dataset da biblioteca do sklearn.\n"}
{"snippet": "boston_data = pd.DataFrame(boston.data, columns=boston.feature_names)\n", "intent": "Jeito 2 - Com os nomes dos atributos\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = 42)\n", "intent": "Dividir o dataset em treino e teste\n"}
{"snippet": "Galton = pd.read_csv('C:/Users/crrodr/Documents/1. Conhecimento/Tera - Data Science/TURMA_4_201808/Regressao_Linear/Exemplo_Galton_simples/Galton.csv')\n", "intent": "importando dados do Galton\n"}
{"snippet": "x, y = utils.load_dataset('default')\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, stratify=y, random_state=1)\n", "intent": "Vamos usar o `VotingClassifier` do `sklearn` para compor alguns classificadores diferentes.\n"}
{"snippet": "xbase, xmeta, ybase, ymeta = train_test_split(xtrain, ytrain, test_size=0.5, random_state=1)\n", "intent": "Agora precisamos fazer um novo split no conjunto de treino para reservar dados para nosso meta-learner.\n"}
{"snippet": "lemmas_df = pd.DataFrame(lemmas, columns={'token'})\n", "intent": "Vamos construir um *dataframe* com os tokens *lemmetizados*.\n"}
{"snippet": "train_df, test_df = train_test_split(train_set, test_size=0.3)\n", "intent": "Vamos dividir nossa base em **treino** e **teste**.\n"}
{"snippet": "df_route = pd.read_csv(os.path.join(DATASET_FOLDER, 'route_clustering_elo7_dataset.csv'), sep=';')\ndf_route.head()\n", "intent": "Vamos tentar aplicar o mesmo algoritmo para o problema de cluster de frete.\n"}
{"snippet": "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\nXtis = vectorizer.fit_transform(df['quote'])\nY = (df['fresh'] == 'fresh').values.astype(np.int8)\nxtraintis, xtesttis, ytraintis, ytesttis = train_test_split(Xtis, Y)\n", "intent": "Do both together help?\n"}
{"snippet": "X = tfidf.fit_transform(df_reason['reason'].values)\n", "intent": "Cria a matriz de embeddings.\n"}
{"snippet": "df = pd.DataFrame({'reason': df_reason['reason'], 'labels': labels})\ndf.head()\n", "intent": "Vamos agora visualizar os clusters criados.\n"}
{"snippet": "pca = PCA()\n", "intent": "Vamos aplicar o PCA e verificar o resultado.\n"}
{"snippet": "scaled_feat = scaler.fit_transform(data.drop('Class', axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "scaled_data = pd.DataFrame(data=scaled_feat)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data = pd.read_csv('College_Data')\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "scaled_feat = scaler.fit_transform(data.drop('TARGET CLASS', axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "scaled_df = pd.DataFrame(scaled_feat, columns=data.columns.drop('TARGET CLASS'))\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X = countVec.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "ds = pd.read_csv('./train.csv')\ndata = ds.values\nprint data.shape\n", "intent": "1. GPU (Nvidia)\n2. Cuda (8.0 with ubuntu 16) [install from .run or .deb file]\n3. NVCC\n4. Theano\nwebsite: floydhub.com\n"}
{"snippet": "df = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n", "intent": "Let's create the DataFrame\n"}
{"snippet": "X = scaled_df\ny = cancer['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n", "intent": "Following we'll do the same train/test split for all the algorithms\n"}
{"snippet": "yelp = pd.read_csv('C://Users//Sai//Desktop//floyd//Natural-Language-Processing/yelp.csv')\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "env = pd.read_csv('data/ces3results_environment.csv')\ndemog = pd.read_csv('data/ces3results_demographics.csv')\n", "intent": "---\n<br>\nNow that we have a solid understanding of how SVM works, let's begin applying our classroom knowledge on real-life data. \n"}
{"snippet": "airquality_path = Path('annual_conc_by_monitor_2017.zip')\nzf = zipfile.ZipFile(airquality_path, 'r')\nf_name = 'annual_conc_by_monitor_2017.csv'\nwith zf.open(f_name) as fh:\n    annual_2017 = pd.read_csv(fh, low_memory=False)\nprint(annual_2017.columns)\nannual_2017\n", "intent": "Let's try to get a sense of what our data looks like. Run the next cell to see the 2017 dataset.\n"}
{"snippet": "pm25_ca = pd.read_csv('data/pm25_ca.csv', low_memory=False)\npm25_ca.shape\n", "intent": "This is what our resulting annual California PM2.5 dataset looks like.\n"}
{"snippet": "airquality_path = Path('annual_conc_by_monitor_2017.zip')\nzf = zipfile.ZipFile(airquality_path, 'r')\nf_name = 'annual_conc_by_monitor_2017.csv'\nwith zf.open(f_name) as fh:\n    annual_2017 = pd.read_csv(fh, low_memory=False)\nprint(annual_2017.columns)\n", "intent": "Let's try to get a sense of what our data looks like. Run the next cell to see the 2017 dataset.\n"}
{"snippet": "pm25_ca = pd.read_csv('pm25_ca.csv', low_memory=False)\npm25_ca.head()\n", "intent": "This is what our resulting annual California PM2.5 dataset looks like.\n"}
{"snippet": "import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', None)\nchurn_df = pd.read_csv('data/churn.all')\n", "intent": "<ul>\n<li>Data Source: https://www.sgi.com/tech/mlc/db/churn.all\n<li>Data info: https://www.sgi.com/tech/mlc/db/churn.names\n</ul>\n"}
{"snippet": "env = pd.read_csv('ces3results_environment.csv')\ndemog = pd.read_csv('ces3results_demographics.csv')\n", "intent": "Let's import the environmental and demographic datasets from CES:\n"}
{"snippet": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nm_train = pd.read_csv(\"mod_train.csv\", encoding = \"ISO-8859-1\", index_col=0)\nprint(m_train.dtypes,\"\\n\")\nprint(m_train.shape,\"\\n\")\nprint(m_train.label.value_counts(),\"\\n\")\n", "intent": "Lets recall the structure of our modified train data \n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nstandardize_data = StandardScaler().fit_transform(data)\nprint(standardize_data.shape)\n", "intent": "Before applying PCA we need to standardize the data set\n"}
{"snippet": "pd.Series(le.inverse_transform(y)).value_counts().sort_index()\n", "intent": "http://www.webgraphviz.com\n"}
{"snippet": "data = pd.read_csv( 'nba_2013.csv' )\ndata.head()\n", "intent": "pos:\n- SF small forward\n- C center\n- PF power forward\n- SG shooting guard\n- PG point guard\n- G guard\n- F forward\n"}
{"snippet": "from sklearn.svm import SVC\npoly_kernel_svm_clf = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n])\npoly_kernel_svm_clf.fit(X, y)\n", "intent": "$K(\\mathbf{a}, \\mathbf{b}) = (\\gamma\\mathbf{a}^T \\cdot \\mathbf{b} + r)^d$\n"}
{"snippet": "actual = [row[1:] for row in test]\nactual = inverse_transform(df, actual, scaler, n_test + 2)\n", "intent": "Get the actual test target and transform the data to their original scale\n"}
{"snippet": "import pandas as pd\nmovies_df = pd.DataFrame(movies)\nmovies_df['year'].hist()\n", "intent": "Now let's do this with pandas\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX = representatives[range(1,17)]\nY = representatives[0]\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=.30, random_state=4444)\n", "intent": "Split the data into a test and training set. Use this function: `from sklearn.cross_validation import train_test_split`\n"}
{"snippet": "from sklearn.feature_selection import RFE\nrfe_l1 = RFE(LRmodel_l1, n_features_to_select=1) \nrfe_l1.fit(X, y)\nprint(\"Logistic Regression (L1) RFE Result\")\ndf = pd.DataFrame({'name' :churn_feat_space.columns, 'rank' :rfe_l1.ranking_}, columns=['name', 'rank'])\ndf.sort_values(by='rank', ascending= True).reset_index()\n", "intent": "The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer \ncv = CountVectorizer()\ntrain_features = cv.fit_transform(posts_frame['text']).toarray() \ntrain_frame = posts_frame.join(pd.DataFrame(train_features, columns=cv.get_feature_names())) \ntrain_frame.drop(['likes_number','text'],inplace=True,axis=1,errors='ignore') \ntrain_frame\n", "intent": "Creating object-feature matrix\n"}
{"snippet": "train_frame.to_csv('traindata.csv')\n", "intent": "Saving train frame to file\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer \ncv = TfidfVectorizer(norm='l1', max_features = 200, analyzer = 'word', strip_accents='unicode', binary=True)\ntrain_features = cv.fit_transform(posts_frame['post_text']).toarray() \ntrain_frame = posts_frame.join(pd.DataFrame(train_features, columns=cv.get_feature_names())) \nall_data = train_frame.copy()\nvalue_frame = train_frame['likes_number']\ntrain_frame.drop(['likes_number','post_text'],inplace=True,axis=1,errors='ignore') \n", "intent": "Creating object-feature matrix\n"}
{"snippet": "scaler = StandardScaler()\ntrain_frame = scaler.fit_transform(train_frame)\n", "intent": "Scaling our features\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_frame, value_frame, test_size=0.3, random_state=42)\n", "intent": "Splitting into train and test samples\n"}
{"snippet": "X = data[\"review\"]\ny = data[\"label\"]\nclass_name = np.unique(y)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "You may use whatever settings you like. To compare your results to the solution notebook, use `test_size=0.33, random_state=42`\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\ntext_clf = Pipeline([(\"tfidf\",TfidfVectorizer()),(\"svc\",LinearSVC())])\ntext_clf.fit(X_train,y_train)\n", "intent": "You may use whatever model you like. To compare your results to the solution notebook, use `LinearSVC`.\n"}
{"snippet": "X_train,X_test,Y_train,Y_test = train_test_split(X,Y)\nlog_model2 = LogisticRegression()\nlog_model2.fit(X_train,Y_train)\n", "intent": "Testing and Training data\n"}
{"snippet": "df_scaled = pd.DataFrame(scaler.transform(df.drop(\"Class\",axis=1)),columns=df.drop(\"Class\",axis=1).columns)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.feature_selection import RFE\nrfe_l1 = RFE(LRmodel_l1, n_features_to_select=1) \nrfe_l1.fit(X, y)\nprint \"Logistic Regression (L1) RFE Result\"\nfor k,v in sorted(zip(map(lambda x: round(x, 4), rfe_l1.ranking_), churn_feat_space.columns)):\n    print v + \": \" + str(k)\n", "intent": "The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features.\n"}
{"snippet": "cm = pd.DataFrame(confusion_matrix(y_test,predicted_y),columns= class_names)\nprint(cm)\n", "intent": "**Show the Confusion Matrix for the predictions.**\n"}
{"snippet": "df = pd.read_csv(\"./KNN_Project_Data\")\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaler_feature = pd.DataFrame(scaler.transform(df.drop(\"TARGET CLASS\",axis=1)),columns=df.drop(\"TARGET CLASS\",axis=1).columns)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "yelp = pd.read_csv(\"./yelp.csv\")\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(yelp_class[\"text\"],yelp_class[\"stars\"], test_size=0.3,random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "user_engagement_df = pd.read_csv('takehome_user_engagement.csv',encoding=\"utf-8\")\nuser_engagement_df.head()\n", "intent": "We begin by reading in both datasets and displaying the first several rows of data.   \n"}
{"snippet": "car_gears = pd.DataFrame(car_gears)\ncar_gears.columns = ['vehicle_id','labels']\n", "intent": "Adding disturbing records\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\ndata.head(13)\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "gen = np.random.RandomState(seed=123)\ndata = pd.DataFrame()\ndata['x'] = gen.uniform(-1, +1, 50)\ndata['y'] = np.zeros_like(data['x'])\nsns.jointplot('x', 'y', data, stat_func=None, size=5);\n", "intent": "As an extreme example, consider the following 2D data which is effectively 1D since one feature has a constant value (zero):\n"}
{"snippet": "(feature_train, feature_test,\n target_train, target_test) = cross_validation.train_test_split(X, target, test_size=.25)\n", "intent": "We are going to use train_test_split to split the data up into training, testing, and validation sets\n"}
{"snippet": "df_img_train, df_img_test, nsrc_true_train, nsrc_true_test = model_selection.train_test_split(\n    df_img, nsrc_true, test_size=0.25, random_state=123)\ndf_iso_train, df_iso_test, e_true_train, e_true_test = model_selection.train_test_split(\n    df_iso, e_true, test_size=0.25, random_state=123)\n", "intent": "Split our image and label datasets using the sklearn default 25% test fraction. Note the use of `random_state` to ensure reproducible \"randomness\":\n"}
{"snippet": "assert np.allclose(e_true_test, e_scaler.inverse_transform(e_scaled_test))\n", "intent": "Note that a scaler transformation is invertible, which is useful for interpreting scaled predictions:\n"}
{"snippet": "iso_scaler = preprocessing.StandardScaler().fit(df_iso_train)\ndf_iso_train_scaled = iso_scaler.transform(df_iso_train)\ndf_iso_test_scaled = iso_scaler.transform(df_iso_test)\n", "intent": "Perform the same normalization on the subset of images with a single source:\n"}
{"snippet": "df.Age = df.Age.fillna(df.Title.map(title_to_age))\n", "intent": "with title_to_age value (the mean for that title)\n"}
{"snippet": "df_feat = pd.DataFrame(scaler_features,columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "OzoneImputeMean=OzoneData.copy()\nOzoneImputeMean['Ozone'].fillna(value = np.mean(OzoneImputeMean['Ozone']), inplace = True)\nOzoneImputeMean['Solar.R'].fillna(value = np.mean(OzoneImputeMean['Solar.R']), inplace = True)\n", "intent": "Hint: copy OzoneData this way:\nOzoneImputeMean = OzoneData.copy()\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler() \nXn = ss.fit_transform(X)\n", "intent": "Now we standardize our Xs\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(Xn, y, test_size=0.3, random_state=10)\nprint Xtrain.shape, Xtest.shape\nprint \"\\n======\\n\"\nprint ytrain.shape, ytest.shape\n", "intent": "Creating Train and Split Samples\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(Xn, y, test_size=0.3, random_state=10)\nprint X_train.shape, X_test.shape\nprint \"\\n======\\n\"\nprint y_train.shape, y_test.shape\n", "intent": "---\ntest size = 30% and random_state = 10\n"}
{"snippet": "(feature_train, feature_validate,\n target_train, target_validate) = cross_validation.train_test_split(feature_train, \n                                                                    target_train, \n                                                                    test_size=.33)\n", "intent": "We can now split the training data into training and validation sets \n"}
{"snippet": "Universities = pd.read_csv('College_Data',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "data_feat = pd.DataFrame(scaled_features,columns=data.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\ntext = open(path).read().lower()\nprint('corpus length:', len(text))\n", "intent": "Nietzsche's Complete works is publicly available on Amazon:\n"}
{"snippet": "pd.DataFrame(sorted(zip(Xt.columns, abs(estimator.coef_[0])), key=lambda (k, v): (abs(v), k), reverse=True)).plot(kind='bar')\n", "intent": "Use a bar chart to display the logistic regression coefficients. Start from the most negative on the left.\n"}
{"snippet": "df_new = pd.DataFrame(data=df, columns=['workclass_num', 'education-num', 'hours-per-week', 'income_num'])\ndf_new2 = pd.DataFrame(data=df, columns=['workclass_num', 'education-num'])\ndf_new.head(5)\n", "intent": "Create a New Dataframe with just numerical data for the analysis\n"}
{"snippet": "normalizer(big_table.Comments[4])\n", "intent": "Test out your normalizer on a comment to ensure it's working. \n"}
{"snippet": "fare_pipe = Pipeline([('selector', ColumnSelector('Fare')),\n                      ('scaler', StandardScaler())])\nfare_pipe.fit_transform(df)\n", "intent": "The `Fare` attribute can be scaled using one of the scalers from the preprocessing module. \n"}
{"snippet": "from sklearn.pipeline import make_union\nunion = make_union(age_pipe,\n                   one_hot_pipe,\n                   gender_pipe,\n                   fare_pipe)\nunion.fit_transform(df.head())\n", "intent": "Use the `make_union` function from the `sklearn.pipeline` modeule to combine all the pipes you have created.\n"}
{"snippet": "data, target = datasets.load_iris(return_X_y=True)\niris = datasets.load_iris()\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "* Let's look again at the Iris datasets\n"}
{"snippet": "sac = pd.read_csv('CSV/Sacramentorealestatetransactions.csv')\nsac = sac.copy()\nsac.head()\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, Y_pred))\nconfusion = pd.DataFrame(conmat, index=['under_200k', 'over_200k'],\n                         columns=['predicted_under_200k', 'predicted_over_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "import pandas\nwheel = pandas.read_csv('CSV/wheel.csv')\nwheel\n", "intent": "The data is in wheel.csv.\nWe would like to understand the relationship between _seconds_ and _signal_\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n", "intent": "Split the data in the ordinary way, making sure you have a 70/30 split.\n"}
{"snippet": "votes = pd.read_csv('../CSV/votes.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "PCA_sk = PCA(n_components=16)\nY_sk = PCA_sk.fit_transform(X_standard)\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "PCA_sk = PCA(n_components=7)\nY_sk = PCA_sk.fit_transform(X_standard)\n", "intent": "Finally, conduct the PCA - use the results about to guide your selection of \"n\" componants\n"}
{"snippet": "airports = pd.read_csv('../CSV/Airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "new_X = pd.DataFrame(Y_sk, columns=['PC1', 'PC2'])\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "scaler = preprocessing.StandardScaler().fit(data)\n", "intent": "* Doing the pre-processing this way allows the *same* transform to be applied to different data later\n"}
{"snippet": "X_standard = StandardScaler().fit_transform(X)\n", "intent": "Since we don't know what units our data was measured in, let's standardize \"x\"\n"}
{"snippet": "item_sum = pd.DataFrame(item_sum)\nitem_sum.index.name = ''\nitem_sum['item_id'] = item_sum.index.values\nitem_sum['First_letter'] = item_sum.index.to_series().map(items.item_name.apply(lambda x: x[0]))\n", "intent": "From .head() and .tail() we see that item id sorted by alphabet. What if it is depend on frequency of letters?\n"}
{"snippet": "data = {\n    'province': ['BXL', 'BXL', 'BXL', 'ANT', 'ANT', 'ANT'],\n    'year': [2013, 2014, 2015, 2013, 2014, 2015],\n    'customers': [150000, 200000, 250000, 200000, 150000, 100000]\n}\nframe = DataFrame(data, columns=['year', 'province', 'customers'])\nframe\n", "intent": "**Ex 1.4.2 Build a DataFrame that captures how the population in each  province of Flanders changed in the years 2013, 2014 and 2015.**\n"}
{"snippet": "names1880 = pd.read_csv('names/yob1880.txt', names=['name', 'sex', 'births'])\n", "intent": "**Ex 1.4.7 How many different male and female names were used in 1880?**\n"}
{"snippet": "def read_one_year(year):\n    df = pd.read_csv('names/yob{0}.txt'.format(year),\n                     names=['name', 'sex', 'births'])\n    df['year'] = int(year)\n    return df\nnames = pd.concat([read_one_year(y) for y in range(1880, 2014+1)],\n                 ignore_index=False)\n", "intent": "**Ex 1.4.8 Write a function to read all names data from 1880 to 2014 into a single `DataFrame`.  Store the result in `names`**\n"}
{"snippet": "total_births = top1000.pivot_table('births', index='year', columns='name',\n                                  aggfunc=sum)\ntotal_births.head()\n", "intent": "**Ex 1.4.11 Plot the proportion in each year of babies named John, Harry, Mary and Marilyn, all in one figure.**\n"}
{"snippet": "import load_spam\nspam_data = load_spam.spam_data_loader()\nspam_data.load_data()\n", "intent": "Let's put all this loading process into a separate file so that we can reuse it in other experiments.\n"}
{"snippet": "from sys import path\npath.append('../2 - Text data preprocessing')\nimport load_spam\nspam_data = load_spam.spam_data_loader()\nspam_data.load_data()\nprint(\"data loaded\")\nXtrain, ytrain, Xtest, ytest = spam_data.split(2000)\nXtrain.shape\n", "intent": "Let's illustrate that on the Ling-spam database.\n"}
{"snippet": "from keras.datasets import fashion_mnist\n(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n", "intent": "One nice thing is that Fashion MNIST has become part of the standard computer vision benchmarks and is included with most libraries, including Keras.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\ndata = iris.data\ntarget = iris.target\n", "intent": "* Fisher developed LDA to deal with the Iris dataset\n* So let's try that one\n* What do you get using the Iris data?\n"}
{"snippet": "data = pd.read_csv(\"data/mushrooms.csv\")\ndata.head(6)\n", "intent": "We now try to do binary classification on the Mushrooms data set loaded below:\n"}
{"snippet": "countvec = CountVectorizer()\nnovels_df = pandas.DataFrame(countvec.fit_transform(text_list).toarray(), columns=countvec.get_feature_names())\nnovels_df\n", "intent": "Create a DTM from these two novels, force it into a pandas DF, and inspect the output:\n"}
{"snippet": "import pandas\nimport nltk\nfrom nltk import word_tokenize\nimport string\ndf = pandas.read_csv(\"../Data/BDHSI2016_music_reviews.csv\", encoding='utf-8', sep = '\\t')\ndf\n", "intent": "First, read in our Music Reviews corpus as a Pandas dataframe.\n"}
{"snippet": "topic_dist_df = pandas.DataFrame(topic_dist)\ndf_w_topics = topic_dist_df.join(df_lit)\ndf_w_topics\n", "intent": "Merge back in with the original dataframe.\n"}
{"snippet": "churntrain, churntest = train_test_split(xrange(dfchurn.shape[0]), train_size=0.6)\nchurnmask=np.ones(dfchurn.shape[0], dtype='int')\nchurnmask[churntrain]=1\nchurnmask[churntest]=0\nchurnmask = (churnmask==1)\nchurnmask\n", "intent": "We havent made any calculations yet! Lets fix that omission and create our training and test sets.\n"}
{"snippet": "dfhw=pd.read_csv(\"https://dl.dropboxusercontent.com/u/75194/stats/data/01_heights_weights_genders.csv\")\nprint dfhw.shape\ndfhw.head()\n", "intent": "(I encountered this dataset in Conway, Drew, and John White. Machine learning for hackers. \" O'Reilly Media, Inc.\", 2012.)\n"}
{"snippet": "itrain, itest = train_test_split(xrange(df.shape[0]), train_size=0.6)\nmask=np.ones(df.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\nmask[:10]\n", "intent": "We split the data into training and test sets...\n"}
{"snippet": "from sklearn.decomposition import TruncatedSVD\nfrom sklearn.utils import as_float_array\ndata_centered = as_float_array(data)\nmean = np.mean(data_centered, axis=0)\ndata_centered -= mean\nU,S,V = np.linalg.svd(data_centered,full_matrices=False)\npd.DataFrame(U[:,:60] *S[:60]).head()\n", "intent": "<h1 ><font color='red'>SVD WORKS LIKE PCA ONLY IF YOU NORMALIZE THE DATA BEFORE !!!! (PCA does it automatically)</font></h1> \n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=0)\n", "intent": "Splitting data into 70% training and 30% test data:\n"}
{"snippet": "y = pca.fit_transform(rvs)\n", "intent": "Reduce the dimension\n"}
{"snippet": "from sklearn.datasets import make_s_curve\ndata, colors = make_s_curve(n_samples=1000)\nprint(data.shape)\nprint(colors.shape)\n", "intent": "One dataset often used as an example of a simple nonlinear dataset is the S-cure:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(lfw_people.data, lfw_people.target, random_state=0)\nprint X_train.shape, X_test.shape\n", "intent": "We'll perform a Support Vector classification of the images.  We'll\ndo a typical train-test split on the images to make this happen:\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\n", "intent": "Apply PCA LocallyLinearEmbedding, and Isomap to project the data to two dimensions.\nWhich visualization technique separates the classes most cleanly?\n"}
{"snippet": "from sklearn import datasets, feature_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nnews = datasets.fetch_20newsgroups()\nX, y = news.data, news.target\nvectorizer = TfidfVectorizer()\nvectorizer.fit(X)\nvector_X = vectorizer.transform(X)\nprint vector_X.shape\n", "intent": "For some types of data, for instance text data, a feature extraction step must be applied to convert it to numerical features.\n"}
{"snippet": "novels_df = pandas.DataFrame(countvec.fit_transform(text_list).toarray(), columns=countvec.get_feature_names())\nnovels_df\n", "intent": "Creat a DTM from these two novels, force it into a pandas DF, and inspect the output:\n"}
{"snippet": "import pandas\nfrom sklearn.feature_extraction.text import CountVectorizer\ncon_score = pandas.read_csv('Concreteness_ratings_Brysbaert_et_al.csv')\nprint(con_score)\n", "intent": "* Do the additional exercises **without** copying and pasting code from lecture notebook.\n"}
{"snippet": "import pandas\nfrom sklearn.feature_extraction.text import CountVectorizer\ncon_score = pandas.read_csv('Concreteness_ratings_Brysbaert_et_al.csv')\nprint(con_score)\n", "intent": "* Do the additional exercises without copying and pasting code from lecture notebook.\n"}
{"snippet": "text_list = []\nmachiavelli_string = open('Machiavelli_ThePrince.txt', encoding='utf-8').read()\nmarx_string = open('Marx_CommunistManifesto.txt', encoding='utf-8').read()\ntext_list.append(machiavelli_string)\ntext_list.append(marx_string)\ncountvec = CountVectorizer(stop_words=\"english\")\nnovels_df = pandas.DataFrame(countvec.fit_transform(text_list).toarray(), columns=countvec.get_feature_names())\nnovels_df\n", "intent": "* open the **Machiavelli_ThePrince.txt** and **Marx_CommunistManifesto.txt**.\n* make a data frame that contains both of them.\n"}
{"snippet": "tfidfvec = TfidfVectorizer(stop_words = 'english', min_df = 1, binary=True)\ncountvec = CountVectorizer(stop_words = 'english', min_df = 1, binary=True)\ntraining_dtm_tf = countvec.fit_transform(df_train.body)\ntest_dtm_tf = countvec.transform(df_test.body)\ntraining_labels = df_train.label\ntest_labels = df_test.label\ntest_labels.value_counts()\n", "intent": "Next we need to create a dtm for each review, and an array containing the classification label for each review (for us, this is called 'label')\n"}
{"snippet": "X_pca = PCA().fit_transform(X)\n", "intent": "* PCA fails to linearly separate this data\n"}
{"snippet": "n_topics = 20\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nlda = LatentDirichletAllocation(n_topics=n_topics, \n                                max_iter=5,\n                                learning_method='online', \n                                learning_offset=50.,\n                                random_state=0)\n", "intent": "1. Fit the `lda` model below with the features you extracted above.\n"}
{"snippet": "start = time.time()\nlda = LatentDirichletAllocation( n_topics = N_TOPICS )\ndoctopic = lda.fit_transform( bag_of_words_all )\nend = time.time() \nprint(\"Processing took {}s\".format(end- start)) \n", "intent": "To create our topics we will use the LatentDirichletAllocation algorithm \n"}
{"snippet": "import statsmodels.api as sm\nfrom statsmodels.formula.api import logit, glm, ols\ndat = pd.DataFrame(data, columns = ['Temperature', 'Failure'])\nlogit_model = logit('Failure ~ Temperature',dat).fit()\nprint(logit_model.summary())\n", "intent": "Lets plot this data\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ndf_default = pd.read_excel('Data/Default.xlsx')\ndf_default.default= df_default.default.map({'No':0,'Yes':1})\ndf_default = df_default.iloc[:,[0,2,3]]\ndf_default.head()\ny = df_default.default.values\nX = df_default.iloc[:,[1,2]].values\nX_train, X_test, y_train,y_test = train_test_split(X,y,test_size =0.3, random_state =1,stratify =y)\n", "intent": "- We will be using the default data set to test\n"}
{"snippet": "print(\"\ntrain = pd.read_csv('numerai_datasets/numerai_training_data.csv', header=0)\ntournament = pd.read_csv('numerai_datasets/numerai_tournament_data.csv', header=0)\n", "intent": "We will be using the numerai data set for example\n"}
{"snippet": "df1 = pd.DataFrame(np.arange(24).reshape(4,6))\nnp.sin(df1)\n", "intent": "For element-wise function application, the most straightforward thing to do is to apply numpy functions to these objects:\n"}
{"snippet": "time_df = pd.DataFrame(np.random.rand(len(business_days)),\n                    index=business_days,\n                   columns=['random'])\ntime_df.head()\n", "intent": "This can in turn be used in as a DataFrame index:\n"}
{"snippet": "pd.pivot_table(df, index='State_Code', columns='County_Code',\n               values=['NUM_ALL', 'NUM_FHA'], aggfunc=np.sum).head()\n", "intent": "This can be done with one step with the `pivot_table()` function.\n"}
{"snippet": "df = pd.read_csv('HR_data.csv')\n", "intent": "***\nTenemos suerte y nos han pasado los datos en un csv. Importa el fichero como DataFrame de pandas y guardalo en la variable `df`\n"}
{"snippet": "kpca = KernelPCA(kernel=\"rbf\", fit_inverse_transform=True, gamma=10)\nX_kpca = kpca.fit_transform(X)\n", "intent": "* Kernel PCA, on the other hand\n"}
{"snippet": "from sklearn.datasets import load_boston\ndata = load_boston()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf['y'] = data.target\nprint(data.get('DESCR'))\n", "intent": "** Ejercicio 1: Carga el dataset de ejemplo por defecto de las casas de boston **\n"}
{"snippet": "df_cv_results = pd.DataFrame(gs_reg_DT.cv_results_)\ndf_cv_results.head()\n", "intent": "** Ejercicio 15: Guarda en un dataframe los resultados de las iteraciones. Los encontraras en .cv_results_ **\n"}
{"snippet": "from sklearn.datasets import load_iris\nimport numpy as np\ndata = load_iris()\nX = data.data\ny = data.target\nmask = np.random.choice([0,1], p=[0.99, 0.01], size=X.shape).astype(np.bool)\nX[mask] = np.nan\n", "intent": "Preparamos los datos con unos cuantos nan's...\n"}
{"snippet": "from sklearn.pipeline import FeatureUnion\ntransformer_list = [('num_pipe', num_pipe),\n                    ('cat_pipe', cat_pipe)]\nfull_pipe = FeatureUnion(transformer_list=transformer_list)\n", "intent": "** Join the two pipes using feature union **\n"}
{"snippet": "local_paths = ['data/othello.txt', 'data/mcbeth.txt']\ndocs = []\nfor path in local_paths:\n    with open(path, 'r') as f:\n        docs.append(f.read())\n", "intent": "O bien, desde local...\n"}
{"snippet": "pca_orig = PCA(n_components=2)\npca_orig.fit(original_imgs)\n", "intent": "Run PCA with 2 components on the original images\n"}
{"snippet": "pca_featurized = PCA(n_components=2)\npca_featurized.fit(featurized)\n", "intent": "Run PCA with 2 components on the featurizations\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])\ndf_feat.head(3)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nprint(boston.DESCR)\nboston_df = boston.data\n", "intent": "    from sklearn.datasets import load_boston\n    boston = load_boston()\n    print(boston.DESCR)\n    boston_df = boston.data\n"}
{"snippet": "pca = PCA().fit(X)\n", "intent": "* Say we want to know the five underlying factors\n"}
{"snippet": "from matplotlib.image import imread\ncustom_images = []\nfor i in range(15):\n    custom_images.append(imread(\"Captured-Image-Data/image\"+str(i)+\".jpg\",))\n", "intent": "* I load a set of **15** custom images from google that I resized to **32x32** by hand. (Loading JPEG images requires `pillow`).\n"}
{"snippet": "from urllib.request import urlretrieve\nfrom os.path import isfile\nfrom tqdm import tqdm\nfrom keras.datasets import cifar10\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\nprint('Training and Test data downloaded.')\n", "intent": "Download and load the Cifar10 dataset.\n"}
{"snippet": "df_bank = pd.DataFrame(data = bankf, columns = bank.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "** Use SciKit Learn to create training and testing sets of the data as we've done in previous lectures:**\n"}
{"snippet": "df = pd.read_csv('KNN_Project_Data')\ndf.head()\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features, columns = df.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "msg2_train, msg2_test, label2_train, label2_test = train_test_split(yelp_class['text'], yelp_class['stars'], test_size = 0.3, random_state = 101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"./PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "pca = UnsupervisedSpatialFilter(PCA(30), average=False)\npca_data = pca.fit_transform(X)\nev = mne.EvokedArray(np.mean(pca_data, axis=0),\n                     mne.create_info(30, epochs.info['sfreq'],\n                                     ch_types='eeg'), tmin=tmin)\nev.plot(show=False, window_title=\"PCA\")\n", "intent": "Transform data with PCA computed on the average ie evoked response\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\ndfp_subsampled = df[0:5000]\nX = MinMaxScaler().fit_transform(dfp_subsampled[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\ny = dfp_subsampled['is_duplicate'].values\n", "intent": "<h3>3.5.2 Visualization </h3>\n"}
{"snippet": "X_train,X_test, y_train, y_test = train_test_split(data, y_true, stratify=y_true, test_size=0.3)\n", "intent": "<h2> 4.3 Random train test split( 70:30) </h2>\n"}
{"snippet": "vectorizer = CountVectorizer(tokenizer = lambda x: x.split())\ntag_dtm = vectorizer.fit_transform(tag_data['Tags'])\n", "intent": "<h3> 3.2.1 Total number of unique tags </h3>\n"}
{"snippet": "start = datetime.now()\nvectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", \\\n                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))\nx_train_multilabel = vectorizer.fit_transform(x_train['question'])\nx_test_multilabel = vectorizer.transform(x_test['question'])\nprint(\"Time taken to run this cell :\", datetime.now() - start)\n", "intent": "<h2>4.3 Featurizing data </h2>\n"}
{"snippet": "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\nmultilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])\n", "intent": "__ Converting string Tags to multilable output variables __ \n"}
{"snippet": "start = datetime.now()\nvectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", \\\n                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))\nx_train_multilabel = vectorizer.fit_transform(x_train['question'])\nx_test_multilabel = vectorizer.transform(x_test['question'])\nprint(\"Time taken to run this cell :\", datetime.now() - start)\n", "intent": "<h3> 4.5.2 Featurizing data with TfIdf vectorizer </h3>\n"}
{"snippet": "import numpy as np\nimport pandas as pd\ndf = pd.DataFrame(data1.data, columns=data.feature_names)\ntarget = pd.DataFrame(data1.target, columns=[\"MEDV\"])\n", "intent": "First, we should load the data as a pandas data frame for easier analysis and set the median home value as our target variable\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nenc=OneHotEncoder(sparse=False)\nX_train_1=X_train\nX_test_1=X_test\ncolumns=['Gender', 'Married', 'Dependents', 'Education','Self_Employed',\n          'Credit_History', 'Property_Area']\nfor col in columns:\n     data=X_train[[col]].append(X_test[[col]])\n     enc.fit(data)\n", "intent": "One-Hot Encoding transforms each categorical feature with n possible values into n binary features, with only one active.\n"}
{"snippet": "y=pd.DataFrame(telco['Churn'])\ntelco=telco.drop('Churn',axis=1)\n", "intent": "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfitting.\n"}
{"snippet": "clf = LogisticRegression()\nscaler = StandardScaler()\nmodel = LinearModel(clf)\nX = scaler.fit_transform(meg_data)\nmodel.fit(X, labels)\nfor name, coef in (('patterns', model.patterns_), ('filters', model.filters_)):\n    coef = scaler.inverse_transform([coef])[0]\n    coef = coef.reshape(len(meg_epochs.ch_names), -1)\n    evoked = EvokedArray(coef, meg_epochs.info, tmin=epochs.tmin)\n    evoked.plot_topomap(title='MEG %s' % name)\n", "intent": "Decoding in sensor space using a LogisticRegression classifier\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(data, data.SalePrice, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "There are 4 variables with missing data.\n"}
{"snippet": "def impute_na(df, variable, median):\n    df[variable+'_NA'] = np.where(df[variable].isnull(), 1, 0)\n    df[variable].fillna(median, inplace=True)\n", "intent": "We observed that the numerical variables are not normally distributed. In particular, most of them apart from YearBuilt are skewed.\n"}
{"snippet": "def impute_na(df_train, df_test, variable):\n    most_frequent_category = df_train.groupby([variable])[variable].count().sort_values(ascending=False).index[0]\n    df_train[variable].fillna(most_frequent_category, inplace=True)\n    df_test[variable].fillna(most_frequent_category, inplace=True)\n", "intent": "Two of the variables have missing data, so let's replace by the most frequent category as we saw on previous lectures.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data[['Cabin', 'Survived']], data.Survived, test_size=0.3, random_state=0)\nX_train.shape, X_test.shape\n", "intent": "The ordering of the labels should be done considering the target ONLY on the training set, and then expanded it to the test set.\nSee below.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(data[['Cabin', 'Survived']], data.Survived, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "The risk factor should be calculated per label considering ONLY on the training set, and then expanded it to the test set.\nSee below.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(data[['Pclass', 'Age', 'Fare']],\n                                                    data.Survived, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "Age contains missing information, so I will fill those observations with the median in the next cell.\n"}
{"snippet": "scaler = StandardScaler() \nX_train_scaled = scaler.fit_transform(X_train) \nX_test_scaled = scaler.transform(X_test) \n", "intent": "StandardScaler from scikit-learn removes the mean and scales the data to unit variance. \n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(data[['Age', 'Fare', 'Survived']],\n                                                    data.Survived, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "The tree should be built using the training dataset, and then used to replace the same feature in the testing dataset, to avoid over-fitting.\n"}
{"snippet": "data = pd.read_csv('titanic.csv', usecols=['Cabin', 'Survived'])\ndata.Cabin.fillna('Missing', inplace=True)\ndata['Cabin'] = data['Cabin'].astype(str).str[0]\ndata.head()\n", "intent": "Discretisation using trees can also be used on categorical variables, to capture some insight into how well they predict the target.\n"}
{"snippet": "pca = UnsupervisedSpatialFilter(PCA(30), average=False)\npca_data = pca.fit_transform(X)\nev = mne.EvokedArray(np.mean(pca_data, axis=0),\n                     mne.create_info(30, epochs.info['sfreq'],\n                                     ch_types='eeg'), tmin=tmin)\nev.plot(show=False, window_title=\"PCA\", time_unit='s')\n", "intent": "Transform data with PCA computed on the average ie evoked response\n"}
{"snippet": "for df in [X_train, X_test, submission]:\n    for var in ['LotFrontage', 'GarageYrBlt']:\n        df[var+'_NA'] = np.where(df[var].isnull(), 1, 0)\n        df[var].fillna(X_train[var].median(), inplace=True) \nfor df in [X_train, X_test, submission]:\n    df.MasVnrArea.fillna(X_train.MasVnrArea.median(), inplace=True)\n", "intent": "- LotFrontage and GarageYrBlt: create additional variable with NA + median imputation\n- CMasVnrArea: median imputation\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfitting.\n"}
{"snippet": "data = pd.read_csv('santander.csv', nrows=50000)\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "In the following cells, I will show an alternative to the VarianceThreshold function of sklearn.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['target', 'ID'], axis=1),\n    data['target'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data[['Pclass', 'Sex', 'Embarked']],\n    data['Survived'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    data[['Pclass', 'Sex', 'Embarked', 'Cabin', 'Survived']],\n    data['Survived'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.\n"}
{"snippet": "scaler = StandardScaler()\nscaler.fit(X_train.fillna(0))\ncoefs_df = []\nfor c in [1, 10, 100, 1000]:\n    logit = LogisticRegression(C=c, penalty='l2')\n    logit.fit(scaler.transform(X_train.fillna(0)), y_train)\n    coefs_df.append(pd.Series(logit.coef_.ravel()))\n", "intent": "Let's fit a few logistic regression models decreasing the penalty of the regularisation.\n"}
{"snippet": "data = pd.read_csv('training/training_variants')\nprint('Number of data points : ', data.shape[0])\nprint('Number of features : ', data.shape[1])\nprint('Features : ', data.columns.values)\ndata.head()\n", "intent": "<h3>3.1.1. Reading Gene and Variation Data</h3>\n"}
{"snippet": "clf = LogisticRegression()\nscaler = StandardScaler()\nmodel = LinearModel(clf)\nX = scaler.fit_transform(meg_data)\nmodel.fit(X, labels)\nfor name, coef in (('patterns', model.patterns_), ('filters', model.filters_)):\n    coef = scaler.inverse_transform([coef])[0]\n    coef = coef.reshape(len(meg_epochs.ch_names), -1)\n    evoked = EvokedArray(coef, meg_epochs.info, tmin=epochs.tmin)\n    evoked.plot_topomap(title='MEG %s' % name, time_unit='s')\n", "intent": "Decoding in sensor space using a LogisticRegression classifier\n"}
{"snippet": "y_true = result['Class'].values\nresult.Gene      = result.Gene.str.replace('\\s+', '_')\nresult.Variation = result.Variation.str.replace('\\s+', '_')\nX_train, test_df, y_train, y_test = train_test_split(result, y_true, stratify=y_true, test_size=0.2)\ntrain_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)\n", "intent": "<h4>3.1.4.1. Splitting data into train, test and cross validation (64:20:16)</h4>\n"}
{"snippet": "df = pd.read_csv('emails.csv')\n", "intent": "Load the file emails.csv\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n", "intent": "- Plot the standardized mean cross-validated accuracy against the unstandardized. Which is better?\n- Why?\n"}
{"snippet": "kmeans_df = pd.DataFrame(results_list_dict)\nkmeans_df.head()\n", "intent": "cluster_picker(X, range_setter)\n"}
{"snippet": "hsq = pd.read_csv('./datasets/hsq_data.csv')\n", "intent": "It is worth reading over the description of the data columns above for this.\n"}
{"snippet": "hsq = pd.read_csv('../datasets/hsq_data.csv')\n", "intent": "It is worth reading over the description of the data columns above for this.\n"}
{"snippet": "conmat = np.array(confusion_matrix(y_test, yhat_ridge, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['is_male', 'is_female'],\n                         columns=['predicted_male','predicted_female'])\nconfusion\n", "intent": "**9.C Construct the confusion matrix for the Ridge LR.**\n"}
{"snippet": "df['bias'] = df['bias'].apply(lambda x: 1 if x == 'partisan' else 0)\nX_train, X_test, y_train, y_test = train_test_split(df['text'].values,\n                                                   df['bias'].values)\n", "intent": "Please split the dataset into a training and test set and convert the `bias` feature into 0s and 1s.\n"}
{"snippet": "df = pd.read_csv('train.csv')\n", "intent": "Load `train.csv` from Kaggle into a pandas DataFrame.\n"}
{"snippet": "clf = make_pipeline(StandardScaler(), LogisticRegression(solver='lbfgs'))\ntime_gen = GeneralizingEstimator(clf, scoring='roc_auc', n_jobs=1,\n                                 verbose=True)\ntime_gen.fit(X=epochs['Left'].get_data(),\n             y=epochs['Left'].events[:, 2] > 2)\n", "intent": "We will train the classifier on all left visual vs auditory trials\nand test on all right visual vs auditory trials.\n"}
{"snippet": "test = pd.read_csv('test.csv')\n", "intent": "Be sure to do the **same** preprocessing you did for your training `X`.\n"}
{"snippet": "pd.DataFrame({\"ImageId\": list(range(1,len(Label)+1)), \"Label\": Label}).to_csv('predictions.csv', \\\n                                                                              index=False, header=True)\n", "intent": "Remember to set `index=False`!\n"}
{"snippet": "test = pd.read_csv('test.csv')\ntest = test / 255.\n", "intent": "Be sure to do the **same** preprocessing you did for your training `X`.\n"}
{"snippet": "test[['ImageId', 'Label']].to_csv('submission.csv', index=False)\n", "intent": "Remember to set `index=False`!\n"}
{"snippet": "data = load_iris()\n", "intent": "Scikit Learn datasets are actually functions that return an object containing the data we need.\n"}
{"snippet": "target_df = pd.DataFrame(data.target, columns=['species'])\ntarget_df.head()\n", "intent": "We'll use `data.target` to create our first dataframe, which is just a single column.\n"}
{"snippet": "sd = pd.read_csv('./datasets/speed_dating.csv')\n", "intent": "---\n- Remove columns with over 200 missing values.\n- Then, remove rows with missing values.\n- Verify no rows contain NaNs.\n"}
{"snippet": "sd = pd.read_csv('../datasets/speed_dating.csv')\n", "intent": "---\n- First, remove columns with over 200 missing values.\n- Then, remove rows with missing values.\n"}
{"snippet": "fu = FeatureUnion([\n    ('has_money_symbol_tf', has_money_symbol_tf),\n    ('has_number_large_number_tf', has_number_large_number_tf),\n    ('has_yelling_tf', has_yelling_tf),\n    ('is_exclaiming_tf', is_exclaiming_tf),\n    ('has_domain_name_tf', has_domain_name_tf),\n    ('has_special_characters_tf', has_special_characters_tf)\n])\n", "intent": "Combine all your function transformers into a feature union\n"}
{"snippet": "clf = LogisticRegression(solver='lbfgs')\nscaler = StandardScaler()\nmodel = LinearModel(clf)\nX = scaler.fit_transform(meg_data)\nmodel.fit(X, labels)\nfor name, coef in (('patterns', model.patterns_), ('filters', model.filters_)):\n    coef = scaler.inverse_transform([coef])[0]\n    coef = coef.reshape(len(meg_epochs.ch_names), -1)\n    evoked = EvokedArray(coef, meg_epochs.info, tmin=epochs.tmin)\n    evoked.plot_topomap(title='MEG %s' % name, time_unit='s')\n", "intent": "Decoding in sensor space using a LogisticRegression classifier\n"}
{"snippet": "df = pd.read_csv('../datasets/breast_cancer_wisconsin/breast_cancer.csv', na_values='?')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "scaler = preproc.StandardScaler()\nXs = scaler.fit_transform(X)\n", "intent": "- Plot the standardized mean cross-validated accuracy against the unstandardized. Which is better?\n- Why?\n"}
{"snippet": "df = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['MEDV'] = boston.target\n", "intent": "Load the Boston housing data.  Fix any problems, if applicable.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 142, test_size = 0.1)\n", "intent": "- Score and plot.  \n- How do your metrics change?\n"}
{"snippet": "wine = pd.read_csv('../datasets/winequality_merged.csv')\n", "intent": "**1. Load and examine the data.**\n"}
{"snippet": "ss = StandardScaler()\nmms = MinMaxScaler()\nXs = ss.fit_transform(X)\nXn = mms.fit_transform(X)\n", "intent": "**4. Create a standardized and normalized version of your predictor matrix.**\n"}
{"snippet": "boston_data = datasets.load_boston()\n", "intent": "**Load the boston housing data with the `datasets.load_boston()` function.**\n"}
{"snippet": "hsq = pd.read_csv('/Users/david.yan/hello_world/week-05/evaluation-classifiers_confusion_matrix_roc-lab/datasets/hsq_data.csv')\n", "intent": "It is worth reading over the description of the data columns above for this.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\nknn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(X_train, y_train)\n", "intent": "**Split the data into 66% training and 33% testing. Fit a KNN classifier with `n_neighbors=25` on the training data.**\n"}
{"snippet": "clf = make_pipeline(StandardScaler(),\n                    LinearModel(LogisticRegression(solver='lbfgs')))\ntime_decod = SlidingEstimator(clf, n_jobs=1, scoring='roc_auc', verbose=True)\ntime_decod.fit(X, y)\ncoef = get_coef(time_decod, 'patterns_', inverse_transform=True)\nevoked = mne.EvokedArray(coef, epochs.info, tmin=epochs.times[0])\njoint_kwargs = dict(ts_args=dict(time_unit='s'),\n                    topomap_args=dict(time_unit='s'))\nevoked.plot_joint(times=np.arange(0., .500, .100), title='patterns',\n                  **joint_kwargs)\n", "intent": "You can retrieve the spatial filters and spatial patterns if you explicitly\nuse a LinearModel\n"}
{"snippet": "df = pd.read_csv('./datasets/titanic_train.csv')\n", "intent": "We'll be working with the titanic datasets - go ahead and import it from the dataset folder (or query for it as described above). \n"}
{"snippet": "df = pd.read_csv('../datasets/titanic_train.csv')\n", "intent": "We'll be working with the titanic datasets - go ahead and import it from the dataset folder (or query for it as described above). \n"}
{"snippet": "prop = pd.read_csv('./datasets/assessor_sample.csv')\n", "intent": "Examine the columns.\n"}
{"snippet": "prop = pd.read_csv('../datasets/assessor_sample.csv')\n", "intent": "Examine the columns.\n"}
{"snippet": "Xs = StandardScaler().fit_transform(X)\n", "intent": "**10.C Standardize the predictor matrix.**\n"}
{"snippet": "sf_time = pd.DataFrame(sf_crime['Dates'].str.split(' ',1).tolist(),columns = ['date','time'])\nsf_date = pd.DataFrame(sf_time['date'].str.split('/').tolist(),columns = ['month','day','year'])\n", "intent": "> *Hint: `pd.to_datetime` may or may not be helpful.*\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=12)\n", "intent": "**Split data into training and testing with 50% in testing.**\n"}
{"snippet": "file_path = './datasets/breast_cancer_wisconsin/breast_cancer.csv'\ndf = pd.read_csv(file_path)\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "iris_data = datasets.load_iris()\n", "intent": "Both sklearn and seaborn have ways to import the iris data:\n- `sklearn.datasets.load_iris()`\n- `sns.load_dataset(\"iris\")`\nThe seaborn way is easier.\n"}
{"snippet": "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\ndata.head()\n", "intent": "Let's take a look at some data, ask some questions about that data, and then use linear regression to answer those questions!\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX = wine.iloc[:, :-1]\nXs = StandardScaler().fit_transform(X)\n", "intent": "**Select the other variables to use for clustering.**\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfeatures = [c for c in df.columns if c != 'acceptability']\nfor c in df.columns:\n    df[c] = le.fit_transform(df[c])\nX = df[features]\ny = df['acceptability']\n", "intent": "Since most of the features are categorical text we will need to encode them as numbers using the ***LabelEncoder***.\n"}
{"snippet": "sd = pd.read_csv('./datasets/speed_dating.csv')\nsd.info()\n", "intent": "---\n- Remove columns with over 200 missing values.\n- Then, remove rows with missing values.\n- Verify no rows contain NaNs.\n"}
{"snippet": "pd.DataFrame({'PC1': pc1_ev, 'PC2': pc2_ev},\n             index=EVENT_NAMES)\n", "intent": "---\nBased on how the original variables are weighted to calculate the components, how would you describe PC1 and PC2?\n"}
{"snippet": "lang = pd.read_csv(\"./datasets/lang.csv\")\nlang.head()\n", "intent": "We're going to be using **scipy** for our analysis. Let's load in the dataset using Pandas ```read.csv()``` and check the head to see it's structure\n"}
{"snippet": "cvt      =  CountVectorizer(strip_accents='unicode', ngram_range=(1,1))\nX_all    =  cvt.fit_transform(tweets_df['TEXT'])\ncolumns = np.array(cvt.get_feature_names())\nX_all\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "cvt      =  CountVectorizer(strip_accents='unicode', ngram_range=(1,1))\nX_all    =  cvt.fit_transform(tweets_df['TEXT'])\ncolumns  =  np.array(cvt.get_feature_names())          \nX_all\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "cvt = CountVectorizer(stop_words=\"english\", ngram_range=(2,4))\nX_all = cvt.fit_transform(tweets_df['TEXT'])\ncolumns  =  np.array(cvt.get_feature_names())\nfreq_words = get_freq_words(X_all, columns)\nfreq_words\n", "intent": "Display the top 75 ngrams with frequencies.\n"}
{"snippet": "from sklearn import naive_bayes\nimport numpy as np\nimport pandas as pd\ndata = pd.read_csv('./datasets/spam_base.csv')\n", "intent": "<a id=\"guided-practice-scikit-learn-implementation\"></a>\n"}
{"snippet": "X_new = pd.DataFrame({'TV': [50]})\nX_new.head()\n", "intent": "Thus, we would predict Sales of **9,409 widgets** in that market.\nOf course, we can also use Statsmodels to make the prediction:\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\n", "intent": "<a id=\"an-example-with-real-data\"></a>\n- We'll use the boston data set to implement multiple regression in TensorFlow\n"}
{"snippet": "data = pd.read_csv('assets/datasets/titanic_train.csv')\nX = data.drop('Survived', axis=1)\ny = data[['Survived']]\n", "intent": "<a id=\"load-in-the-titanic-data\"></a>\n"}
{"snippet": "class FeatureExtractor(TransformerMixin):\n    def __init__(self, column):\n        self.column = column\n    def fit(self, x, y=None):\n        return self\n    def transform(self, x, y=None):\n        return x[self.column].values.reshape(-1, 1)\nFeatureExtractor('Fare').fit_transform(X_train)[0:5]\n", "intent": "<a id=\"do-a-bit-of-data-cleaning\"></a>\n"}
{"snippet": "stocks = pd.DataFrame(stock_close)\n", "intent": "This converts the data into daily changes in stock price.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = pd.read_csv('assets/datasets/train.csv')\ndata.set_index('Date', inplace=True)\ndata.head()\n", "intent": "- Assemble observations and graphs as well as timeseries models in a notebook.\n"}
{"snippet": "pred_train, pred_test, tar_train, tar_test  =   train_test_split(predictors, targets, test_size=.3)\nprint( \"Predictor - Training : \", pred_train.shape, \"Predictor - Testing : \", pred_test.shape )\n", "intent": "We now split the model into training and testing data in the ratio of 70:30\n"}
{"snippet": "predictors = cleaned_data.drop(\"CLV\",axis=1)\ntargets = cleaned_data.CLV\npred_train, pred_test, tar_train, tar_test  =   train_test_split(predictors, targets, test_size=.1)\nprint( \"Predictor - Training : \", pred_train.shape, \"Predictor - Testing : \", pred_test.shape )\n", "intent": "Let us split the data into training and testing datasets in the ratio 90:10.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nnumeric_df_ss = pd.DataFrame(ss.fit_transform(numeric_df), index = numeric_df.index, columns= numeric_df.columns)\n", "intent": "The numeric_df looks to have missing values and we now have as sense of distributions. We can go ahead and standardize those values\n"}
{"snippet": "def cat_imputer(df):\n    for col in df.columns:\n        df[col].fillna(value=df[col].mode()[0], inplace=True)\n    return df\n", "intent": "let's impute the missing values for the categorcal with the mode \n"}
{"snippet": "X_new = pd.DataFrame({'TV': [data.TV.min(), data.TV.max()]})\nX_new.head()\n", "intent": "Let's make predictions for the **smallest and largest observed values of x**, and then use the predicted values to plot the least squares line:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconmat = confusion_matrix(y_test,y_pred)\nconfusion = pd.DataFrame(conmat, index = ['is_healthy', 'is_cancer'], columns =['predicted_healthy', 'predicted_cancer'])\nconfusion\n", "intent": "**Create the confusion matrix for your classfier's performance on the test set.**\n"}
{"snippet": "conmat = np.array(confusion_matrix(y_test, y_pred, labels=[0,1]))\nconfusion = pd.DataFrame(conmat, index=['is_healthy', 'is_cancer'],\n                         columns=['predicted_healthy', 'predicted_cancer'])\nconfusion\n", "intent": "**Create the confusion matrix for your classfier's performance on the test set.**\n"}
{"snippet": "selectkbest = SelectKBest(score_func=f_classif, k=5)\n", "intent": "Next, let's instantiate a `SelectKBest` object, using `f_classif` as our scoring function and a `k` of 5:\n"}
{"snippet": "non_quality_columns = [col for col in wine.columns if 'quality' not in col]\nX = wine[non_quality_columns].copy()\nss = StandardScaler()\nss.fit(X)\nstandard_X = ss.transform(X)\ny = wine['quality_0'].copy()\n", "intent": "Split up wine into your features (`X`) and target column (`y`), then standardize your features.\n"}
{"snippet": "df = pd.read_csv('datasets/boston.csv')\ndf.head()\n", "intent": "We'll be using the Boston housing dataset for this section. As a reminder, the first five rows are as follows:\n"}
{"snippet": "iris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf.head()\n", "intent": "First, let's import the `iris` dataset, loading it in from the `sklearn.datasets` module:\n"}
{"snippet": "df = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf.head()\n", "intent": "We'll show a brief example using the Iris dataset:\n"}
{"snippet": "ss = StandardScaler()\nss.fit(df)\nX = ss.transform(df)\n", "intent": "We'll begin by standardizing the data:\n"}
{"snippet": "cv = CountVectorizer()\ncv.fit(space_messages)\nstopped_words = pd.DataFrame(cv.transform(space_messages).todense(),\n                              columns=cv.get_feature_names())\nprint(stopped_words.sum().sort_values(ascending=False).head(10))\n", "intent": "We can also use `CountVectorizer` to remove common stopwords for us by setting the `stop_words` keyword argument to `english`\n"}
{"snippet": "def fill_nan(table):\n    for col in table.columns:\n        table[col] = table[col].fillna(table[col].median())\n    return table   \n", "intent": "Let's write the function that will replace *NaN* values with the median for each column.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(econ['text'].values, \n                                                   econ['relevance'].values,\n                                                   test_size=0.33,\n                                                   random_state=2017)\n", "intent": "And let's split into a training set and a test set, using the `text` of the document.\n"}
{"snippet": "cv = CountVectorizer(ngram_range=(1, 2))\nanswer = answer_df_questions(space_messages, cv)\n", "intent": "```\nInstructor answer:\nThis gives us all of the possible features from vectorizing the 100 messages\n```\n"}
{"snippet": "y = iris['species'].copy()\nX = iris[[col for col in iris.columns if col !='species']].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    random_state=2017)\n", "intent": "We'll split `species` into its own `y` variable and assign the remaining features to `X`, then train-test split (using 2017 as the `random_state`):\n"}
{"snippet": "econ = pd.read_csv('datasets/economic_news.csv',\n                  usecols=[14])\necon['text'] = econ['text'].apply(lambda x: x.replace('</br>', ''))\necon.head()\n", "intent": "First, let's reimport all of the economic news data instead of just the first 200 rows:\n"}
{"snippet": "cv = CountVectorizer(stop_words='english')\ncv.fit(econ['text'].values)\nX = cv.transform(econ['text'].values)\nX\n", "intent": "Next, we'll transform the data using `CountVectorizer` and removing stop words:\n"}
{"snippet": "from sklearn.decomposition import PCA\nX = demo_noage\npca = PCA()\npca = pca.fit(X)\nprint(pca.explained_variance_)\nprint(pca.components_)\n", "intent": "<a id=\"eigen\"></a>\n---\n"}
{"snippet": "Z = pca.transform(demo_noage)\nfeatures_pca = ['PC'+str(i+1) for i in range(pca.n_components_)]\nZ = pd.DataFrame(Z, columns=features_pca)\nZ.head(5)\n", "intent": "<a id=\"transformed\"></a>\n---\n"}
{"snippet": "age_imputer = Imputer(strategy='median')\nage_imputer.fit(df['age'].values.reshape(-1, 1))\nages = age_imputer.transform(\n    df['age'].values.reshape(-1, 1))\nprint(ages[0:5], ages.mean())\n", "intent": "Our results are as follows:\n- Mean: 16.65\n- Median: 17\n- Mode: 16.0\nWhat is most appropriate? Check in on Slack\n"}
{"snippet": "mm_scaler = MinMaxScaler()\nmm_scaler.fit(g1_g3)\nss_scaler = StandardScaler()\nss_scaler.fit(g1_g3)\nrb_scaler = RobustScaler()\nrb_scaler.fit(g1_g3)\n", "intent": "Let's use `MinMaxScaler`, `StandardScaler`, and `RobustScaler` to transform these values and see if visually, there is any difference:\n"}
{"snippet": "data = pd.read_csv('../../data/credit_scoring_sample.csv', sep =';')\ndata.head()\n", "intent": "Now, read the data:\n"}
{"snippet": "data = load_boston()\nprint(data.DESCR)\n", "intent": "Import data, take `TAX`, `CRIM`, and `PTRATIO` and assign to `X`. Assign target to `y`.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, boston_y, test_size=0.33)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n", "intent": "Create `X_train, X_test, y_train, y_test` using train-test split with a 33% holdout\n"}
{"snippet": "X = salary[['yd', 'yr']]\ny = salary['sl']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n", "intent": "Use `train_test_split()` to create a training set and a test set, split 50/50\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(boston, boston_y, test_size=0.33)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n", "intent": "Create `X_train, X_test, y_train, y_test` using train-test split with a 33% holdout\n"}
{"snippet": "def make_features(train_set, test_set, degrees):\n    train_dict = {}\n    test_dict = {}\n    for d in degrees:\n        traintestdict={}\n        train_dict[d] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n        test_dict[d] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n    return train_dict, test_dict\n", "intent": "We'll write a function to encapsulate what we learnt about creating the polynomial features.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ndatasize=sample_df.shape[0]\nitrain, itest = train_test_split(np.arange(60),train_size=0.8)\nxtrain= sample_df.x[itrain].values\nftrain = sample_df.f[itrain].values\nytrain = sample_df.y[itrain].values\nxtest= sample_df.x[itest].values\nftest = sample_df.f[itest].values\nytest = sample_df.y[itest].values\n", "intent": "We split the sample data into training and testing sets.\n"}
{"snippet": "from sklearn.datasets.california_housing import fetch_california_housing\ncal_housing = fetch_california_housing()\n", "intent": "First, the data. This one is built into sklearn, its a dataset about california housing prices. Its quite skewed as we shall see.\n"}
{"snippet": "train, test =  train_test_split(iris, test_size=.3)\nx_train, x_test = train[features_cols], test[features_cols]\ny_train, y_test = train['target'], test['target']\n", "intent": "We will now use the **sklearn** package to implement kNN:\nHere, we will split our data using the train_test_split function from sklearn.\n"}
{"snippet": "degree = 2\npoly = PolynomialFeatures(degree)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.fit_transform(x_test)\npd.DataFrame(x_train_poly).shape\n", "intent": "We first need to create the PolynomialFeatures object and specify to what degree we wish to take our polynomial to:\n"}
{"snippet": "lasso1_coef = pd.DataFrame({'coef': lasso1.coef_, 'coef_abs': np.abs(lasso1.coef_)},\n                          index=data.columns.drop('quality'))\nlasso1_coef.sort_values(by='coef_abs', ascending=False)\n", "intent": "**Which feature is the least informative in predicting wine quality, according to this LASSO model?**\n"}
{"snippet": "train, test =  train_test_split(crime_df, test_size=.2, random_state=123)\ntrain.shape,test.shape\n", "intent": "Now, let's split this dataset up into a testing and training set.\n"}
{"snippet": "X_test_numerical_powers = np.hstack((X_test_df[numerical_columns]**(i+1) for i in range(3)))\nX_test_np_powers = np.concatenate((X_test_numerical_powers,X_test_df.drop(numerical_columns, axis=1)),axis=1)\nX_test_df_powers = pd.DataFrame(X_test_np_powers)\nnewcolname = ['Temp', 'Dewpoint', 'Windspeed', 'Pressure', 'Precipitation', 'TMAX_C', 'TMIN_C', 'Temp^2', 'Dewpoint^2', 'Windspeed^2', 'Pressure^2', 'Precipitation^2', 'TMAX_C^2', 'TMIN_^2','Temp^3', 'Dewpoint^3', 'Windspeed^3', 'Pressure^3', 'Precipitation^3', 'TMAX_C^3', 'TMIN_C^3'] + list(X_train_df.drop(numerical_columns, axis=1))\nX_test_df_powers.columns = newcolname\nX_test_df_powers.head()\n", "intent": "We can do the same with the test set:\n"}
{"snippet": "all_poly_terms = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\nX_train_full_poly = all_poly_terms.fit_transform(X_train_df)\nX_test_full_poly = all_poly_terms.fit_transform(X_test_df)\nprint('number of total predictors', X_train_full_poly.shape[1])\n", "intent": "Let's now create a design matrix that includes all polynomial terms up to the third order, including all interactions. \n"}
{"snippet": "pca = PCA(n_components=5)\npca.fit(X_train_full_poly)\ntrain_pca = pca.transform(X_train_full_poly)\ntest_pca = pca.transform(X_test_full_poly)\nprint('Explained variance ratio:', pca.explained_variance_ratio_)\n", "intent": "Now we can fit our PCA model:\n"}
{"snippet": "X_test_numerical_powers = \nX_test_np_powers = np.concatenate((X_test_numerical_powers,X_test_df.drop(numerical_columns, axis=1)),axis=1)\nX_test_df_powers = pd.DataFrame(X_test_np_powers)\nnewcolname = ['Temp', 'Dewpoint', 'Windspeed', 'Pressure', 'Precipitation', 'TMAX_C', 'TMIN_C', 'Temp^2', 'Dewpoint^2', 'Windspeed^2', 'Pressure^2', 'Precipitation^2', 'TMAX_C^2', 'TMIN_^2','Temp^3', 'Dewpoint^3', 'Windspeed^3', 'Pressure^3', 'Precipitation^3', 'TMAX_C^3', 'TMIN_C^3'] + list(X_train_df.drop(numerical_columns, axis=1))\nX_test_df_powers.columns = newcolname\nX_test_df_powers.head()          \n", "intent": "We can do the same with the test set:\n"}
{"snippet": "x_train, x_test, y_train_val, y_test_val = train_test_split(x, y, test_size=0.6, random_state=42)\n", "intent": "Let's split our dataset into train and test.\n"}
{"snippet": "conf_mat_1 = confusion_matrix(y_test, y_hat_test)\nconf_df_1 = pd.DataFrame(conf_mat_1, columns = ['y_hat=0', 'y_hat = 1'], index = ['y=0', 'y=1'])\nconf_df_1\n", "intent": "We again rely on the confusion matrix to gain a clearer picture of how the classification did.\n"}
{"snippet": "conf_mat_2 = confusion_matrix(y_test2, y_hat_test2)\nconf_df_2 = pd.DataFrame(conf_mat_2, columns = ['y_hat=0', 'y_hat = 1'], index = ['y=0', 'y=1'])\nconf_df_2\n", "intent": "The test accuracy is better than before, but let us see how well it does for the cancer classification.\n"}
{"snippet": "poly_degree = 3\npoly = PolynomialFeatures(poly_degree, include_bias=False)\nX_train_poly_cubic = poly.fit_transform(X_train_2)\nX_test_poly_cubic = poly.fit_transform(X_test_2)\nlogregcv_cubic = LogisticRegressionCV()\nlogregcv_cubic.fit(X_train_poly_cubic, y_train_2)\nlogregcv_2_poly_cubic = LogisticRegressionCV(multi_class='multinomial')\nlogregcv_2_poly_cubic.fit(X_train_poly_cubic, y_train_2)\n", "intent": "Now, we show how to fit a Multiclass Logistic Regression model with cubic terms:\n"}
{"snippet": "lasso_cv_coef = pd.DataFrame({'coef': lasso_cv.coef_, 'coef_abs': np.abs(lasso_cv.coef_)},\n                          index=data.columns.drop('quality'))\nlasso_cv_coef.sort_values(by='coef_abs', ascending=False)\n", "intent": "**<font color='red'>Question 3:</font> Which feature is the least informative in predicting wine quality, according to the tuned LASSO model?**\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"G:\\DOCUMENTOS\\Data Science and Machine Learning with python\\DataScience-Python3/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\nclf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)\n", "intent": "A single train/test split is made easy with the train_test_split function in the cross_validation library:\n"}
{"snippet": "dataset = pd.read_csv(\"./data/dataset.csv\")\n", "intent": "Reading dataset with feature engineering done! Splitting dataset into train and test set.\n"}
{"snippet": "predictions = pd.DataFrame({\"PassengerId\":pd.concat([id_trainW,id_trainM,id_testW,id_testM], axis = 0),\n                            \"Survived\":list(pd.concat([pd.Series(train[train[\"Sex\"] == 1][\"Survived\"].astype(int)),\n                                                       pd.Series(train[train[\"Sex\"] == 0][\"Survived\"].astype(int)),\n                                                       pd.Series(predictions_cluster_1),\n                                                       pd.Series(predictions_cluster_2)],axis=0))})\n", "intent": "Sorting predictions according to delivery format\n"}
{"snippet": "train = pd.read_csv(\"./data/train.csv\")\n", "intent": "Let's get started!!!\n"}
{"snippet": "combined['Fare'].fillna(combined['Fare'].median(),inplace=True)\n", "intent": "We have replaced one missing Fare value by the median.\n"}
{"snippet": "train = pd.read_csv(\"./data/train.csv\")\n", "intent": "Reading and exploring data:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n", "intent": "**Create X and y train and test splits in one command using a split ratio and a random seed**\n"}
{"snippet": "df_2 = DataFrame(data_1, columns=['year', 'state', 'pop'])\ndf_2\n", "intent": "Create a DataFrame specifying a sequence of columns:\n"}
{"snippet": "data_demo = pd.read_csv('../../data/weights_heights.csv')\n", "intent": "Let's test out the algorithm on height/weight data. We will predict heights (in inches) based on weights (in lbs).\n"}
{"snippet": "data_2 = {'VA' : df_4['VA'][1:],\n          'MD' : df_4['MD'][2:]}\ndf_5 = DataFrame(data_2)\ndf_5\n", "intent": "Create a DataFrame from a dict of Series:\n"}
{"snippet": "df_13 = DataFrame({'foo' : [7, -5, 7, 4, 2, 0, 4, 7],\n                   'bar' : [-5, 4, 2, 0, 4, 7, 7, 8],\n                   'baz' : [-1, 2, 3, 0, 5, 9, 9, 5]})\ndf_13\n", "intent": "DataFrames can rank over rows or columns.\n"}
{"snippet": "df_14 = DataFrame(np.random.randn(5, 4),\n                  index=['foo', 'foo', 'bar', 'bar', 'baz'])\ndf_14\n", "intent": "Select DataFrame elements:\n"}
{"snippet": "data_2 = {'state' : ['NY', 'NY', 'NY', 'FL', 'FL'],\n          'year' : [2012, 2013, 2014, 2014, 2015],\n          'population' : [6.0, 6.1, 6.2, 3.0, 3.1]}\ndf_3 = DataFrame(data_2)\ndf_3\n", "intent": "Concatenate two DataFrames:\n"}
{"snippet": "df_1 = pd.read_csv(\"../data/ozone.csv\")\n", "intent": "Read data from a CSV file into a DataFrame (use sep='\\t' for TSV):\n"}
{"snippet": "df_1.to_csv('../data/ozone_copy.csv', \n            encoding='utf-8', \n            index=False, \n            header=False)\n", "intent": "Create a copy of the CSV file, encoded in UTF-8 and hiding the index and header labels:\n"}
{"snippet": "from sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=16)\n", "intent": "- We'll choose 16 components for our modeling\n"}
{"snippet": "ipl_data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings',\n         'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'],\n         'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2],\n         'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017],\n         'Points':[876,789,863,673,741,812,756,788,694,701,804,690]}\ndf = pd.DataFrame(ipl_data)\ndf.head()\n", "intent": "<a id='pandas-groupby'></a>\n"}
{"snippet": "cars=pd.read_csv('./EDA-getting&cleaningdata/CarPrice_Assignment.csv',encoding='iso-8859-1')\n", "intent": "[categorical encoding](http://pbpython.com/categorical-encoding.html)\n"}
{"snippet": "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                     test_size=0.3,\n                                                     random_state=17)\n", "intent": "Perform train/test split and scale data.\n"}
{"snippet": "ratings_data = pd.read_csv('C:/Users/xz2654/Downloads/ratings.csv')\nraw_movies_data = pd.read_csv('C:/Users/xz2654/Downloads/CleanedMovies.csv', encoding='latin-1')\nselected_movies=pd.read_csv('C:/Users/xz2654/Downloads/selectedmovies_infor.csv')\n", "intent": "We first read the files with pandas.\n"}
{"snippet": "genres_df=pd.read_csv('C:/Users/xz2654/Downloads/Selectedmoviesgenres.csv', encoding='latin-1')\nmovie_df=pd.read_csv('C:/Users/xz2654/Downloads/selectedmovies_infor.csv')\n", "intent": "First load the Selectedmovies and Selectedmoviesgenres datasets we got from previous data gathering and preparation process.\n"}
{"snippet": "col_names = ['labels','text']\ndf_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, columns=col_names)\ndf_val = pd.DataFrame({'text':val_texts, 'labels':val_labels}, columns=col_names)\n", "intent": "Fastai adopts a recent (LeCun paper) \"standard\" format for NLP datasets: label followed by text columns. \nFor IMDB, there is only one text column.\n"}
{"snippet": "import pandas as pd\nIMAGES,ANNOTATIONS,CATEGORIES = ['images', 'annotations', 'categories']            \ncategories = pd.DataFrame(trn_j[CATEGORIES])\nannotations = pd.DataFrame(trn_j[ANNOTATIONS])\nimages = pd.DataFrame(trn_j[IMAGES])\nlen(categories), len(images), len (annotations)\n", "intent": "We use pandas to organize the data. See the [Gist](https://gist.github.com/binga/1bc4ebe5e41f670f5954d2ffa9d6c0ed) for details. \n"}
{"snippet": "largest_bbox = data.pivot_table(index='file_name', values='area', aggfunc=max).reset_index()\nlargest_bbox = largest_bbox.merge(data[[AREA, BBOX, IMG_ID, FILE_NAME, NAME]], how='left')\nprint(\"If these are not equal, we may have duplicates\",  largest_bbox.shape[0], len(trn_fns)) \nlargest_bbox.head(1)\n", "intent": "**Best Practice:** Use pandas to create a CSV of the data to model, rather than trying to create a custom dataset. \n"}
{"snippet": "rf = path+'/ratings.csv'\nratings = pd.read_csv(rf)\nratings.head()\n", "intent": "The movielens data contains one user-rating-movie per row:\n"}
{"snippet": "mf = path+'/movies.csv'\nmovies = pd.read_csv(mf)\nmovies.head()\n", "intent": "Just for display purposes, let's read in the movie names too.\n"}
{"snippet": "param_test1 = {'n_estimators':range(120,201,10)}\ngs1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, \n                                                               min_samples_leaf=50,\n                                                               max_features=9,\n                                                               subsample=0.8,\n                                                               random_state=101010), \n                        param_grid = param_test1, \n                        scoring='roc_auc', cv=5)\ngs1.fit(train_set2[predictors3],train_set2[target])\npd.DataFrame(gs1.cv_results_)[cols].sort_values('rank_test_score').head(10)\n", "intent": "Its still increasting with n_estimators so we need to try more\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n     data.drop('subscribed', axis=1).values, \n     data['subscribed'].values,\n     stratify=data['subscribed'].values,\n     random_state=0)\n", "intent": "Let's now split our data into a train set and a test set.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(full_df,shuffle = True, test_size = 0.25, random_state=17)\ntrain_df=train_df.copy()\ntest_df=test_df.copy()\nprint(train_df.shape)\nprint(test_df.shape)\n", "intent": "Let's divide dataset into train (75%) and test (25%).\n"}
{"snippet": "dc_df = pd.get_dummies(c_df)\ndc_df.loc[dc_df['prev_days'] == 999, 'prev_days'] = np.nan\ndc_df = dc_df.drop(['subscribed_no'], axis=1)\nidc_df = pd.DataFrame(Imputer(strategy=\"mean\").fit_transform(dc_df.drop(['subscribed_yes'], axis=1).values))\nidc_df.columns = list(dc_df.drop(['subscribed_yes'], axis=1).columns)\nidc_df['subscribed'] = dc_df['subscribed_yes'].values\netrain_df = idc_df.iloc[:train_df.shape[0],:]\netest_df = idc_df.iloc[train_df.shape[0]:,:]\netrain_df.head()\n", "intent": "Now I get the dummies.\n"}
{"snippet": "vectorizer_ngrams = CountVectorizer(ngram_range=(1,10), stop_words=\"english\", lowercase=True, min_df=2)\nx_train_vectorized_ngrams = vectorizer_ngrams.fit_transform(X_train)\nx_test_vectorized_ngrams = vectorizer_ngrams.transform(X_test)\n", "intent": "Improve the model using more complex text features, including n-grams, character n-grams and possibly domain-specific features.\n"}
{"snippet": "Res = pd.DataFrame(x_16['Team 1'])\nRes['Team 2'] = x_16['Team 2']\nRes = Res.reset_index().drop(['Game ID'],axis=1)\nRes['Predicted_winner'] = y_pred['Predicted_winner']\nRes.head()\n", "intent": "Probability scores for both teams according to the predicted winner value are represented by probs1 and probs2\n"}
{"snippet": "td = pd.read_json('t.json.gz', orient='records', lines=True)\ntd['available_price'].fillna(td['available_price'].mean(),inplace=True)\ntd['mrp'].fillna(td['mrp'].mean(),inplace=True)\ntd.describe()\ntd.head()\n", "intent": "Reading both files and cleaning the columns which are required for now.\n"}
{"snippet": "t = td[['mrp']].values.astype(float)\nmin_max_scaler = MinMaxScaler()\nt_scaled = min_max_scaler.fit_transform(t)\nt_mrp_normalized = pd.DataFrame(t_scaled)\nt_mrp_normalized[0].replace((0,' '),np.nan,inplace=True)\nprint (\"The normalized mrp values are: \")\nt_mrp_normalized\n", "intent": "Normalization using SKLEARN library: for both files\n"}
{"snippet": "from sklearn import preprocessing\nscaler = preprocessing.StandardScaler()\nX2 = X.as_matrix().astype(np.float)\nX2 = scaler.fit_transform(X)\n", "intent": "In order to measure coefficients, recall that we must\n- a) Standarize coefficients\n- b) Remove collinear features\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('./data/hw2data.csv')\ndf.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "j = [10, 20, 30]\nfor s in j:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=s)\n    clf = LogisticRegression(penalty='l1', C=100)\n    clf.fit(X_train, y_train)\n    generate_ROCplot(fpr,tpr,'LR',roc_auc)\n    print \"The ROC curve for random state is:\", s\n", "intent": "For the different random state:10, 20 and 30. The roc_auc models are always keep consistent, therefore, the model is robust.\n"}
{"snippet": "def load_dataset(split):\n    dataset = datasets.load_iris()\n    X, y = dataset['data'], dataset['target']\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=123, test_size=(1 - split))\n    return X_train, X_test, y_train, y_test\n", "intent": "The iris data set (https://en.wikipedia.org/wiki/Iris_flower_data_set) is loaded and split into train and test parts by the function `load_dataset`.\n"}
{"snippet": "new_features_train_df=pd.DataFrame(index=train_df.index)\nnew_features_test_df=pd.DataFrame(index=test_df.index)\nnew_features_train_df['1/distance_to_SF']=1/(train_df['distance_to_SF']+0.001)\nnew_features_train_df['1/distance_to_LA']=1/(train_df['distance_to_LA']+0.001)\nnew_features_train_df['log_distance_to_SF']=np.log1p(train_df['distance_to_SF'])\nnew_features_train_df['log_distance_to_LA']=np.log1p(train_df['distance_to_LA'])\nnew_features_test_df['1/distance_to_SF']=1/(test_df['distance_to_SF']+0.001)\nnew_features_test_df['1/distance_to_LA']=1/(test_df['distance_to_LA']+0.001)\nnew_features_test_df['log_distance_to_SF']=np.log1p(test_df['distance_to_SF'])\nnew_features_test_df['log_distance_to_LA']=np.log1p(test_df['distance_to_LA'])\n", "intent": "Visually is not obvious so let's try to create a couple of new variables and check:\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nimport sklearn.linear_model as lm\ndf=pd.read_csv('/Users/apple/Documents/GitHub/APMAE4990-/data/hw2data.csv', index_col=0)\ndf=pd.DataFrame(df)\ndf.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "regr = Lasso(alpha=alpha_optim)\nregr.fit(X_train,y_train)\ndf_coeffs = pd.DataFrame({'coeffs':regr.coef_, 'name':X.columns.values})\ndf_coeffs=df_coeffs.sort_values(['coeffs'])\ndf_coeffs.iloc[0:110,:].plot(x='name',y='coeffs',kind='bar')\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntrain,test =train_test_split(df,test_size=0.2) \nprint len(train)\nprint len(test)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "kaggle_data = pd.DataFrame({'PassengerId':test.PassengerId, 'Survived':new_pred_class}).set_index('PassengerId')\nkaggle_data.to_csv('sub.csv')\n", "intent": "**Save DataFrame to csv**\n"}
{"snippet": "from sklearn.datasets import make_s_curve\ndata, colors = make_s_curve(n_samples=1000)\nprint(data.shape)\nprint(colors.shape)\n", "intent": "One dataset often used as an example of a simple nonlinear dataset is the S-curve:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n", "intent": "We split our data in a training and a test set again:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n", "intent": "To use a preprocessing method, we first import the estimator, here StandardScaler and instantiate it:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA()\n", "intent": "As always, we instantiate our PCA model. By default all directions are kept.\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42)\nX.shape\n", "intent": "Let's start of with a very simple and obvious example:\n"}
{"snippet": "new_features_train_df=pd.DataFrame(scaler.fit_transform(new_features_train_df),\n                            columns=new_features_train_df.columns, index=new_features_train_df.index)\nnew_features_test_df=pd.DataFrame(scaler.transform(new_features_test_df),\n                            columns=new_features_test_df.columns, index=new_features_test_df.index)\n", "intent": "And finally let's scale all this features\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    lfw_people.data, lfw_people.target, random_state=0)\nprint(X_train.shape, X_test.shape)\n", "intent": "We'll perform a Support Vector classification of the images.  We'll\ndo a typical train-test split on the images to make this happen:\n"}
{"snippet": "gram_vectorizer = CountVectorizer(ngram_range=(1, 2))\ngram_vectorizer.fit(X)\n", "intent": "Often we want to include unigrams (sigle tokens) and bigrams:\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nvectorizer = TfidfVectorizer()\nvectorizer.fit(text_train)\nX_train = vectorizer.transform(text_train)\nX_test = vectorizer.transform(text_test)\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n", "intent": "Previously, we applied the feature extraction manually, like so:\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\npipeline = make_pipeline(TfidfVectorizer(), LogisticRegression())\nparams = {'logisticregression__C': [.1, 1, 10, 100],\n          \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (2, 2)]}\ngrid = GridSearchCV(pipeline, param_grid=params, cv=5)\ngrid.fit(text_train, y_train)\nprint(grid.best_params_)\ngrid.score(text_test, y_test)\n", "intent": "Another benefit of using pipelines is that we can now also search over parameters of the feature extraction with ``GridSearchCV``:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntext_train_small, text_validation, target_train_small, target_validation = train_test_split(\n    text_train_all, np.array(target_train_all), test_size=.5, random_state=42)\n", "intent": "Let's split the training CSV file into a smaller training set and a validation set with 100k random tweets each:\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nprint digits.keys()\ndata         = digits[\"data\"]\nimages       = digits[\"images\"]\ntarget       = digits[\"target\"]\ntarget_names = digits[\"target_names\"]\n", "intent": "Explore the data and filter for instances of ones and sevens.\n"}
{"snippet": "test = test.fillna({'Fare' : test['Fare'].mean()}) \n", "intent": "As there is only one missing value, we can just input the mean of the column without any impact on the set.\n"}
{"snippet": "train = train.fillna({'Embarked' : 'S'})\n", "intent": "For the variable **Embarked**, as almost everyone came from Southampton we can directly put that.\n"}
{"snippet": "baja_si = df.loc[df['Baja'] == 'Baja SI', select]\nbaja_no = df.loc[df['Baja'] == 'Baja NO', select]\npca = PCA(n_components=3)\nprint(np.concatenate((baja_si, baja_no)).shape)\nPCA = pca.fit_transform(np.concatenate((baja_si,baja_no)))\n", "intent": "Using the elbow rule on the screeplot, it seems that 3 components might be enough to explain the most part of the information in the data.\n"}
{"snippet": "results_df=pd.DataFrame(columns=['model','CV_results', 'holdout_results'])\n", "intent": "Lets sum up the results of our project. We will compute RMSE on cross validation and holdout set and compare them.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.2, random_state=42)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata1=pd.read_csv(\"/home/zhejing/APMAE4990--master/data/hw2data.csv\")    \n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "regr = Lasso(alpha=opt_alpha)\nregr.fit(X_train, y_train)\ndf_coeffs = pd.DataFrame({'coeffs':regr.coef_, 'name':X.columns.values})\ndf_coeffs = df_coeffs.sort(columns=['coeffs'])\ndf_coeffs[::-1].head(50).plot(x='name', \n                              y='coeffs', \n                              kind='bar', \n                              title='First ten coefficients',\n                              figsize=(16, 8),\n                              fontsize=15)\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "X = df_final[['balance', 'income','student_Yes']]\ny = df_final.default_Yes\nseed = [10, 20, 30,40,50,60]\nfor s in seed:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=s)\n    clf = LogisticRegression(penalty='l1', C=100)\n    clf.fit(X_train, y_train)\n    plot_roc(clf, X_test, y_test, 'under random_state = %s' %s)\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "X = df_final[['balance', 'income','student_Yes']]\ny = df_final.default_Yes\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ntree = DecisionTreeClassifier(criterion='entropy',  max_depth=5)\ntree.fit(X_train, y_train)\nexport_graphviz(tree, out_file='tree.dot')                \n", "intent": "7) Train a Decision Tree classifier with maximum depth 5 and plot the decision tree. How does performance compare?\n"}
{"snippet": "from keras.preprocessing.sequence import pad_sequences\npad_tweet_list = pad_sequences(tokenized_tweet_list, maxlen=120)\n", "intent": "3\\. Use `pad_sequences` from `keras.preprocessing.sequence` to pad each sequence with zeros to **make the sequence length 120**. [2 pts]\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(pad_tweet_list, df['sentiments'][:5000], test_size=0.33, shuffle=True)\n", "intent": "4\\. Split the above data (the sequence and the label) into training (67%) and validation (33%) sets. [3 pts]\n"}
{"snippet": "df2015 = df.ix[(df['year']==2015)]\ndf2016 = df.ix[(df['year']==2016)]\ndf2015q1 = df2015.ix[(df2015['quarter']==1)]\ndf2015q234 = df2015.ix[(df2015['quarter']==2)|(df2015['quarter']==3)|(df2015['quarter']==4)]\ndf2015q1_s = df2015q1.pivot_table(index = 'store_number',aggfunc = sum)\ndf2015q234_s = df2015q234.pivot_table(index = 'store_number',aggfunc = sum)\ndf2016s = df2016.pivot_table(index = 'store_number',aggfunc = sum)\n", "intent": "Look for any statistical relationships, correlations, or other relevant properties of the dataset.\n"}
{"snippet": "import pandas as pd\ndata2=pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/ML_2017/Q2.csv\")\nfrom sklearn.cross_validation import train_test_split\ntrain,test=train_test_split(data2,random_state=100,test_size=0.4)\n", "intent": "In this question, we use data: \"Q2.csv\" for Bayesian Network Learning.\n"}
{"snippet": "train_part, test_part = train_test_split(data[['name','Manufacturer']], test_size=0.2, random_state=21, stratify=data[\"Manufacturer\"])\n", "intent": "At the first time let's build a model with only Description of Manufacturer as feature\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n", "intent": "Let's explore the dataset a little bit\n"}
{"snippet": "spam_np_vecs = np.zeros((len(spam_data1), 100))\nfor i, vec in enumerate(spam_data1.vecs):\n    spam_np_vecs[i, :] = vec\nspam_w2v_data = pd.concat([spam_data1.reset_index().label, pd.DataFrame(spam_np_vecs)], axis=1)\n", "intent": "Now let's just convert the format that we have into a final DataFrame with 100 features and 1 label for use in our document classification task:\n"}
{"snippet": "df_Beijing = pd.read_csv('../Data/FiveCitiesPM/Beijing.csv')\ndf_Beijing = df_Beijing[df_Beijing.year >= 2015]\ndf_Beijing.head(10)\n", "intent": "We'll start by working with Beijing data, and filter the dataset down to records from 2015. \n"}
{"snippet": "scaler = MinMaxScaler()\ndata = df.values\ndata = data.astype('float32')\nscaler.fit(data)\ndata = scaler.transform(data)\ntrain_size = int(len(data) * 0.67)\ntest_size = len(data) - train_size\ntrain, test = data[0:train_size,:], data[train_size:len(data),:]\nprint('Train Set contains:', len(train),'datapoints')\nprint('Test Set contains:', len(test),'datapoints')\n", "intent": "Use the MinMaxScaler to normalize the training and testing set betwee 0 and 1.  Name the normalized outputs norm_train and norm_test.\n"}
{"snippet": "df = df.fillna(df.median())\ndf.head()\n", "intent": "Fill the missing numbers with the median of the whole dataset! (hint: df.fillna?) Check to see if the counts for all columns is now 5000.\n"}
{"snippet": "from sklearn.preprocessing import LabelBinarizer\nlb = LabelBinarizer()\nfor col in ['churned']:\n    df[col] = lb.fit_transform(df[col])\nfrom sklearn.preprocessing import MinMaxScaler\nmsc = MinMaxScaler()\ndf = pd.DataFrame(msc.fit_transform(df),  \n                    columns=df.columns)\ny = df['churned']\nX = df[[col for col in df.columns if col!=\"churned\"]]\n", "intent": "Separate the data into the features and labels. Assign the features to variable X and labels to variable y.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.30, random_state=42)\nprint(\"Size of train dataset: {} rows\".format(X_train.shape[0]))\nprint(\"Size of test dataset: {} rows\".format(X_test.shape[0]))\n", "intent": "Split the data into 70% training set and 30% test set and assign them to variables named X_train, X_test, y_train, y_test (hint: train_test_split?)\n"}
{"snippet": "ames_df = pd.read_csv(\"http://www.amstat.org/publications/jse/v19n3/Decock/AmesHousing.txt\", delimiter=\"\\t\")\names_df.head()\n", "intent": "We'll be using the Ames Housing dataset during this exercise. Below we'll read the data in and display the first five rows.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ndata[float_columns] = sc.fit_transform(data[float_columns])\ndata.head(4)\n", "intent": "Perform feature scaling.\n"}
{"snippet": "columns_nans = ['race', 'diag_1', 'diag_2', 'diag_3']\nimp_most_frequent = SimpleImputer(missing_values='?', strategy='most_frequent', verbose=1)\nX_train_nan_most_frequent = pd.DataFrame(imp_most_frequent.fit_transform(X_train[columns_nans]),\n                                         columns=[el+'_mf' for el in columns_nans] )\nX_test_nan_most_frequent = pd.DataFrame(imp_most_frequent.transform(X_test[columns_nans]),\n                                         columns=[el+'_mf' for el in columns_nans] )\nX_train = pd.concat([X_train, X_train_nan_most_frequent], axis=1)\nX_test = pd.concat([X_test.reset_index(drop=True), X_test_nan_most_frequent], axis=1).set_index(X_test.index)\n", "intent": " - Baseline model: fill with most frequent value\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data, test_size=0.3, random_state=42)\n", "intent": "Next, split the data in train and test data sets.\n"}
{"snippet": "from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold=(.7 * (1 - .7)))\ndata2 = pd.concat([X_train,X_test])\ndata_new = pd.DataFrame(sel.fit_transform(data2))\ndata_y = pd.concat([y_train,y_test])\nfrom sklearn.model_selection import train_test_split\nX_new,X_test_new = train_test_split(data_new)\nY_new,Y_test_new = train_test_split(data_y)\n", "intent": " Identify highly correlated columns and drop those columns before building models\n"}
{"snippet": "skew = pd.DataFrame(data.skew())\nskew.columns = ['skew']\nskew['too_skewed'] = skew['skew'] > .75\nskew\n", "intent": "Notice that aside from the predictor variable, everything is float.\n"}
{"snippet": "print('Input features: \\n', datasets.load_iris().feature_names)\nprint()\nprint('Target classes: \\n', datasets.load_iris().target_names)\niris = datasets.load_iris().data\niris = preprocessing.scale(iris)\nspecies = datasets.load_iris().target\n", "intent": "* 4 dimensions is too many to plot\n"}
{"snippet": "data_path_train = os.path.join(os.getcwd(), 'datasets', 'splice_train.csv')\nsplice_train = pd.read_csv(data_path_train, delimiter = ',')\nprint('Number of instances: {}, number of attributes: {}'.format(splice_train.shape[0], splice_train.shape[1]))\nsplice_train.head(10)\n", "intent": "Load the `splice_train.csv` and `splice_test.csv` into two separate dataframes. Display the shape and first 10 instances for each dataframe.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nprint(digits.DESCR)\n", "intent": "Run the cell below to load the digits object and print its description.\n**Do not change any of the code in this question**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=42)\n", "intent": "Finally, let's make the train and test split. Again, don't alter this code.\n"}
{"snippet": "categories_df = pd.read_csv(\"categories.csv\", index_col=0)\ntrain_offsets_df = pd.read_csv(\"train_offsets.csv\", index_col=0)\ntrain_images_df = pd.read_csv(\"train_images.csv\", index_col=0)\nval_images_df = pd.read_csv(\"val_images.csv\", index_col=0)\n", "intent": "First load the lookup tables from the CSV files (you don't need to do this if you just did all the steps from part 1).\n"}
{"snippet": "from sklearn.datasets import fetch_lfw_people\nfaces = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n", "intent": "The data we'll use is a number of snapshots of the faces of world leaders. We'll fetch the data as follows:\n"}
{"snippet": "adf_kpss_returns = pd.DataFrame(index=asset_names, columns=['ADF','KPSS'])\nadf_returns = []\nkpss_returns = []\nfor i in range(len(asset_names)):\n    adf_returns.append(ts.adfuller(returns.iloc[:,i])[1])\n    kpss_returns.append(kpss(returns.iloc[:,i])[1])\n", "intent": "ACF and PACF demonstrate that time-series of returns should be stationary. Let's support this by performing ADF and KPSS tests.\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ntwenty_train = fetch_20newsgroups(subset='train',\n    categories=['comp.graphics', 'sci.med'], shuffle=True, random_state=0)\nprint(twenty_train.target_names)\n", "intent": "Working through an example.\n"}
{"snippet": "df=pd.read_csv('train.csv')\n", "intent": "Load `train.csv` from Kaggle into a pandas DataFrame.\n"}
{"snippet": "df = pd.read_csv('data/weather_2010-2016.csv');\n", "intent": "<h3>Time series prediction method</h3>\n"}
{"snippet": "df = pd.read_csv('data/weather_2010-2016.csv');\nfrom dateutil.parser import parse\ndf['Date'] = df['Date'].apply(lambda d: parse(d, dayfirst=True).date())\ndf = df.groupby('Date', as_index=False, sort=True)['Temp'].mean()\ndf['month'] = df['Date'].apply(lambda d: d.month)\ndf['day'] = df['Date'].apply(lambda d: d.day)\ndf.drop('Date', axis=1, inplace=True)\n", "intent": "<h3>Traditional method</h3>\n"}
{"snippet": "train_csv = pd.read_csv('data/train/train.csv')\nprint(train_csv.shape)\ntest_csv = pd.read_csv('data/test/test.csv')\nprint(test_csv.shape)\n", "intent": "Load dataset and check the size of train data and test data whichi is provided by the website.\n"}
{"snippet": "train_rows.to_csv('data/train_rows.csv', sep=',', index=False)\nvalid_rows.to_csv('data/valid_rows.csv', sep=',', index=False)\n", "intent": "For the convenience, both data frame are stored. They can be loaded in the future.\n"}
{"snippet": "train_input = train_rows.append(train_agument)\ntrain_input_stat = pd.DataFrame(train_input.landmark_id.value_counts())\ntrain_input_stat.reset_index(inplace=True)\ntrain_input_stat.columns = ['landmark_id','count']\ndisplay(train_input_stat.head())\ndisplay(train_input_stat.tail())\n", "intent": "After generating agumnet images, check if the train dataset is balance or not.\n"}
{"snippet": "train_reduce = pd.DataFrame()\nfor i in range(int(train_input.shape[0]/100)):\n    train_reduce = train_reduce.append(train_input.iloc[i*100:i*100+3])\ndisplay(train_reduce.shape)\ndisplay(train_reduce.head())\ndisplay(train_reduce.tail())\n", "intent": "There are too many train images so there are about 45000 images which will be used in this project.\n"}
{"snippet": "valid_reduce = pd.DataFrame()\nvalid_rows.sort_values(by=['landmark_id'])\ntmp_id = -1\nfor i in range(int(valid_rows.shape[0])):\n    if tmp_id != valid_rows.iloc[i]['landmark_id']:\n        valid_reduce = valid_reduce.append(valid_rows.iloc[i:i+1])\n        tmp_id = valid_rows.iloc[i]['landmark_id']\ndisplay(valid_reduce.shape)\ndisplay(valid_reduce.head())\n", "intent": "There are only two images for each lankmark to validate the model because the computer does not have enough memory.\n"}
{"snippet": "from sklearn.preprocessing import  StandardScaler\nscaler = StandardScaler()\n", "intent": "When we use linear model, **it's nesessary to standatrize** our data. Let's use **StandardScaler**:\n"}
{"snippet": "audio_file_path = '../Datasets'\nfilename = '/5e1b34a6_nohash_0.wav'\nsample_rate, samples = wavfile.read(str(audio_file_path) + filename)\n", "intent": "Let us read a sample audio file from this dataset: \n"}
{"snippet": "X = encode[:25]\ntsne_data = manifold.TSNE(n_components=2).fit_transform(X)\n", "intent": "We will take first 50 values to visualize the data.\n"}
{"snippet": "audio_file_path = '../Datasets'\nfilename = '/5e1b34a6_nohash_0.wav'\nsample_rate, samples = wavfile.read(str(audio_file_path) + filename)\nprint(sample_rate)\n", "intent": "Let us read a sample audio file from this dataset: \n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"Datasets/amazon.csv\")\nprint(data.describe())\ndata = data.dropna()\nprint(data.describe())\n", "intent": "**Objectives:** Create a linear regression based product rating solution.\n"}
{"snippet": "from sklearn import decomposition\npca = decomposition.PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X)\nprint(principalComponents.shape)\nplot_clustering(principalComponents,Y,Y,\"Distribution of clusters\")\n", "intent": "**Excerise 1:** Apply PCA on the data\n"}
{"snippet": "tsne_data = manifold.TSNE(n_components=2).fit_transform(X)\n", "intent": "**Excerise 4:** Apply t-SNE on the data\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"../Datasets/amazon_reviews.csv\")\nprint(data.describe())\ndata = data.dropna()\nprint(data.describe())\n", "intent": "**Objectives:** Create a non-linear regression based product rating solution.\n"}
{"snippet": "url = \"http://api.sdss3.org/spectrum?id=boss.3588.55184.511.v5_4_45&format=json\"\nresponse = urllib2.urlopen(url)\ndata = response.read()\ndata_dict = json.loads(data)\n", "intent": "With `urllib2`, have to format a string with some parameters, remember the ? and & in the URL, etc:\n"}
{"snippet": "print(\"total variance:{}\".format(np.sum(np.var(Xn,0))))\npca = PCA(2)\npca1 = pca.fit(Xn).transform(Xn)\nprint(\"variance explained via the first and second components:{}\\n\".format(pca.explained_variance_))\n", "intent": "Visualize the data using PCA (two dimensions). Color the points by the day of the week\n"}
{"snippet": "train_df, test_df, y_train, y_test = train_test_split(data_df.drop(['casual', 'registered', 'count'], axis=1), data_df[['casual', 'registered', 'count']], \n                                                      test_size=0.3, random_state=17, shuffle=True)\n", "intent": "I split data into train and hold-out samples \n"}
{"snippet": "print(\"total variance:{}\".format(np.sum(np.var(X,0))))\npca = PCA(2)\npca.fit(X)\nprint(\"variance explained via the first and second components:{}\\n\".format(pca.explained_variance_))\nprint(\"principal components:\\n{}\".format(pca.components_))\n", "intent": "(in this particular example ~97.6% of the variance is preserved if we project down to the leading principal component)\n"}
{"snippet": "admission = pd.read_csv('data/admission.csv')  \nadmission.head()\n", "intent": "Example - learning the probability of school admission based on two exams\n"}
{"snippet": "results.to_csv('./titanic_results.csv')\n", "intent": "To save your results (possibly for submission to a Kaggle competition), you can use the `.to_csv()` method to write the dataframe to a file.\n"}
{"snippet": "data = pd.read_csv('../data/housing-data.csv')\n", "intent": "Load a subset of the housing data\n"}
{"snippet": "forest_fires = pd.read_csv('../data/forestfires.csv')\nforest_fires.head()\n", "intent": "https://archive.ics.uci.edu/ml/datasets/Forest+Fires\n"}
{"snippet": "en = pd.read_csv('../data/ENB2012_data.csv',header=0)\nen.info()\n", "intent": "Back to our confusion matrix.  Let's refresh our data first.\n"}
{"snippet": "import pandas as pd\ntitanic = pd.read_csv('../data/titanic.csv')\ntitanic.head(10)\n", "intent": "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle.\n"}
{"snippet": "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\", header = None)\ndata.columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"]\ndata.head()\n", "intent": "Investigate the data and check for missing values - we've used .info() before:\n"}
{"snippet": "from sklearn.cross_validation import ShuffleSplit\ntitanic = pd.read_csv('../data/titanic.csv')\ntitanic.info()\n", "intent": "Random forests are a type of ensemble method (which we hinted at above) that build out groups of decision trees\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y, test_size=0.2, random_state=666)\n", "intent": "Split our train dataset to part, which we are going to train (X_train; same name), and to part with which we will check an error.\n"}
{"snippet": "mush_code = pd.DataFrame(mush['edible?'].map({'p':0,'e':1}))\nfor column in mush.columns:\n    if column == 'edible?':\n        continue\n    temp = pd.get_dummies(mush[column],prefix=column)\n    mush_code[temp.columns] = temp\nmush_code.head()\n", "intent": "For each new column, we'll preface it with the original column name\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "Load the sklearn `iris` dataset.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n", "intent": "So a concern if we're taking features away is are we losing too much information? And if we're losing information, are we gaining speed?\n"}
{"snippet": "data2 = pd.read_csv('C:\\Users\\Tam\\Google Drive\\Data\\GA\\Eviction_Notices.csv',header=0)\ndata2.info()\n", "intent": "Import Eviction Data\n"}
{"snippet": "all_data['GarageCars'] = all_data['GarageCars'].fillna((all_data['GarageCars'].mean()))\nall_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].fillna((all_data['TotalBsmtSF'].mean()))\n", "intent": "Fill missing values with the mean of the column.\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\ndegree = 20 \npf = PolynomialFeatures(degree)\nlr = LinearRegression() \nX_data = data[['x']]\nY_data = data['y']\nX_poly = pf.fit_transform(X_data)\n", "intent": "* Using the `PolynomialFeatures` class from Scikit-learn's preprocessing library, create 20th-order polynomial features \n"}
{"snippet": "data_path = ['C:/Users/japor/Desktop/Machine Learning 501 Nervana/Intel-ML101-Class4/data']\nfilepath = os.sep.join(data_path + ['Ames_Housing_Sales.csv'])\ndata = pd.read_csv(filepath, sep = ',')\n", "intent": "* Import the data with pandas, remove null values, and one-hot encode categoricals \n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data, test_size = 0.3, random_state = 42)\n", "intent": "Split the data into train and test data sets \n"}
{"snippet": "from sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold = 0.7 * (1 - 0.7))\ndata2 = pd.concat([X_train, X_test])\ndata_new = pd.DataFrame(sel.fit_transform(data2))\n", "intent": "Identify highly correlated columns and drop these columns before building models \n"}
{"snippet": "X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y, test_size=0.2, random_state=666)\n", "intent": "Split our train dataset again\n"}
{"snippet": "df = pd.DataFrame(zip(Y, labels), columns=['true','pred'])\ndf['correct'] = df.apply(lambda x: 1 if x['true'] == x['pred'] else 0, axis=1)\ndf\n", "intent": "Plot the predicted vs actual classifcations to see how our clustering analysis compares\n"}
{"snippet": "estimator3 = GradientBoostingRegressor()\nselector3 = RFECV(estimator3, step=1, cv=5)\nselector3 = selector3.fit(X, y)\nrfecv_columns3 = X.columns[selector3.support_]\nrfecv_columns3\n", "intent": "Variables relating to the Gradient Boosting Regressor will be marked either 'gbr' or '3'.\n"}
{"snippet": "estimator4 = ExtraTreesRegressor()\nselector4 = RFECV(estimator4, step=1, cv=5)\nselector4 = selector4.fit(X, y)\nrfecv_columns4 = X.columns[selector4.support_]\nrfecv_columns4\n", "intent": "Variables relating to the Extra Trees Regressor will be marked either 'etr' or '4'.\n"}
{"snippet": "train.workclass.value_counts(sort=True)\ntrain.workclass.fillna('Private', inplace=True)\n", "intent": "Fix missing values in 'workclass', 'occupation', and 'native country'\n"}
{"snippet": "housing_df = pd.read_csv('WestRoxbury.csv')\n", "intent": "Load the West Roxbury data set\n"}
{"snippet": "Amtrak_df = pd.read_csv('Amtrak.csv', squeeze=True)\nAmtrak_df['Date'] = pd.to_datetime(Amtrak_df.Month, format='%d/%m/%Y')\nridership_ts = pd.Series(Amtrak_df.Ridership.values, index=Amtrak_df.Date)\n", "intent": "Load the Amtrak data and convert them to be suitable for time series analysis\n"}
{"snippet": "universal_df = pd.read_csv('UniversalBank.csv')\nuniversal_df.plot.scatter(x='Income', y='CCAvg', \n                          c=['black' if c == 1 else 'grey' for c in universal_df['Securities Account']],\n                          ylim = (0.05, 10), alpha=0.2,\n                          logx=True, logy=True)\n", "intent": "Use `alpha` to add transparent colors \n"}
{"snippet": "import gmaps\nSCstudents = pd.read_csv('SC-US-students-GPS-data-2016.csv')\ngmaps.configure(api_key=os.environ['GMAPS_API_KEY'])\nfig = gmaps.figure(center=(39.7, -105), zoom_level=3)\nfig.add_layer(gmaps.symbol_layer(SCstudents, scale=2, \n                                 fill_color='red', stroke_color='red'))\nfig\n", "intent": "To run this example you need an API key for Google maps. Go to https://cloud.google.com/maps-platform/ to find out how to getone\n"}
{"snippet": "pd.DataFrame({'mean': bostonHousing_df.mean(),\n              'sd': bostonHousing_df.std(),\n              'min': bostonHousing_df.min(),\n              'max': bostonHousing_df.max(),\n              'median': bostonHousing_df.median(),\n              'length': len(bostonHousing_df),\n              'miss.val': bostonHousing_df.isnull().sum(),\n             })\n", "intent": "Compute mean, standard dev., min, max, median, length, and missing values for all variables\n"}
{"snippet": "X_train, X_train_test, y_train, y_train_test = train_test_split(df_train_store_mlc, y, \n                                                                test_size=0.2, random_state=17)\n", "intent": "> Now we will divide out train set on **80% training and 20% validation**.\n"}
{"snippet": "pcsSummary = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n                           'Proportion of variance': pcs.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\npcsSummary = pcsSummary.transpose()\npcsSummary.columns = ['PC1', 'PC2']\npcsSummary.round(4)\n", "intent": "The importance of components can be assessed using the explained variance.\n"}
{"snippet": "scores = pd.DataFrame(pcs.transform(cereals_df[['calories', 'rating']]), \n                      columns=['PC1', 'PC2'])\nscores.head()\n", "intent": "Use the `transform` method to get the scores.\n"}
{"snippet": "pcs = PCA()\npcs.fit(cereals_df.iloc[:, 3:].dropna(axis=0))\npcsSummary_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n                           'Proportion of variance': pcs.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\npcsSummary_df = pcsSummary_df.transpose()\npcsSummary_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_df.columns) + 1)]\npcsSummary_df.round(4)\n", "intent": "Perform a principal component analysis of the whole table ignoring the first three non-numerical columns.\n"}
{"snippet": "scaler = preprocessing.StandardScaler()\nscaler.fit(trainData[['Income', 'Lot_Size']])  \nmowerNorm = pd.concat([pd.DataFrame(scaler.transform(mower_df[['Income', 'Lot_Size']]), \n                                    columns=['zIncome', 'zLot_Size']),\n                       mower_df[['Ownership', 'Number']]], axis=1)\ntrainNorm = mowerNorm.iloc[trainData.index]\nvalidNorm = mowerNorm.iloc[validData.index]\nnewHouseholdNorm = pd.DataFrame(scaler.transform(newHousehold), columns=['zIncome', 'zLot_Size'])\n", "intent": "Initialize normalized training, validation, and complete data frames. Use the training data to learn the transformation.\n"}
{"snippet": "utilities_df = pd.read_csv('Utilities.csv')\nutilities_df.set_index('Company', inplace=True)\nutilities_df = utilities_df.apply(lambda x: x.astype('float64'))\nutilities_df.head()\n", "intent": "Load the data, set row names (index) to the utilities column (company) and remove it. Convert all columns to `float`\n"}
{"snippet": "d = pairwise.pairwise_distances(utilities_df, metric='euclidean')\npd.DataFrame(d, columns=utilities_df.index, index=utilities_df.index).head(5)\n", "intent": "Compute Euclidean distance matrix (to compute other metrics, change the name of `metric` argument)\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nbinary_vectorizer = CountVectorizer(binary=True)\nX = binary_vectorizer.fit_transform(corpus)\nprint X.todense()\n", "intent": "Now let's use scikit-learn to create our feature representations. `CountVectorizer` can be used to convert a corpus to a matrix of token counts.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=4)\n", "intent": "Train/Test split\nScikitlearn has its own methodology\nThere is a slightly criptics command that we need.\n"}
{"snippet": "path = os.getcwd() + '/data/ex2data2.txt'\ndata2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])\ndata2.head()\n", "intent": "Similar to part 1, let's start by visualizing the data.\n"}
{"snippet": "num_cols = key_cols+['prev_flux']\ntrain_ohe = pd.concat([pd.get_dummies(train_df['Year'], prefix = 'Year'), train_df[num_cols]], axis=1)\ntransformers = [('num', StandardScaler(), num_cols)]\nct = ColumnTransformer(transformers=transformers, remainder = 'passthrough' )\nlogit_pipe = Pipeline([('transform', ct), ('logit', LogisticRegression(C=0.01, class_weight='balanced', random_state=17))])\nlogit_pipe.fit(train_ohe.loc[:idx_split,:], train_df.loc[:idx_split,'bin_target'])\n", "intent": "Now time to fit model on the whole dataset and test it on the hold out\n"}
{"snippet": "chilledw_train = pd.DataFrame(data=normalized_chilledWater, index=np.arange('2012-01', '2014-01', dtype='datetime64[D]')).dropna()\nchilledw_test = pd.DataFrame(data=normalized_chilledWater, index=np.arange('2014-01', '2014-11', dtype='datetime64[D]')).dropna()\nXX_chilledw_train = chilledw_train.drop('chilledWater-TonDays', axis = 1).reset_index().drop('index', axis = 1)\nXX_chilledw_test = chilledw_test.drop('chilledWater-TonDays', axis = 1).reset_index().drop('index', axis = 1)\nYY_chilledw_train = chilledw_train['chilledWater-TonDays']\nYY_chilledw_test = chilledw_test['chilledWater-TonDays']\nprint XX_chilledw_train.shape, XX_chilledw_test.shape\n", "intent": "Analysis of chilled water data.\n"}
{"snippet": "steam_train = pd.DataFrame(data=normalized_steam, index=np.arange('2012-01', '2014-01', dtype='datetime64[D]')).dropna()\nsteam_test = pd.DataFrame(data=normalized_steam, index=np.arange('2014-01', '2014-11', dtype='datetime64[D]')).dropna()\nXX_steam_train = steam_train.drop('steam-LBS', axis = 1).reset_index().drop('index', axis = 1)\nXX_steam_test = steam_test.drop('steam-LBS', axis = 1).reset_index().drop('index', axis = 1)\nYY_steam_train = steam_train['steam-LBS']\nYY_steam_test = steam_test['steam-LBS']\nprint XX_steam_train.shape, XX_steam_test.shape\n", "intent": "Analysis of steam data.\n"}
{"snippet": "turnstile_data = pd.read_csv(\"turnstile_data_master_with_weather.csv\")\nturnstile_data.head()\n", "intent": "However, we're going to use the pandas library to make it much easier to analyse and chart the data contained in this CSV file.\n"}
{"snippet": "from sklearn.decomposition import PCA\nboth = [X[i] for i in range(len(y)) if y[i] == 0 or y[i] == 1]\nlabels = [y_ for y_ in y if y_ == 0 or y_ == 1]\npca = PCA(n_components=3)\nXproj3d = pca.fit_transform(both)\n", "intent": "Let's try to approximate the relative positions of our points in $\\mathbb{R}^3$, then:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(2)\nXproj = pca.fit_transform(X)\n", "intent": " How effective do you think logistic regression will be on the entire `digits` dataset?\n"}
{"snippet": "corpus_raw = u\"\"\nfor book_filename in book_filenames:\n    print(\"Reading '{0}'...\".format(book_filename))\n    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n        corpus_raw += book_file.read()\n    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n    print()\n", "intent": "**Combine the books into one string**\n"}
{"snippet": "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)\n", "intent": "**Train t-SNE, this could take a minute or two...**\n"}
{"snippet": "points = pd.DataFrame(\n    [\n        (word, coords[0], coords[1])\n        for word, coords in [\n            (word, all_word_vectors_matrix_2d[thrones2vec.vocab[word].index])\n            for word in thrones2vec.vocab\n        ]\n    ],\n    columns=[\"word\", \"x\", \"y\"]\n)\n", "intent": "**Plot the big picture**\n"}
{"snippet": "df = pd.read_csv('house-votes-84.data', header=None)\n", "intent": "Load the data into a pandas dataframe. \n"}
{"snippet": "raw_data = pd.read_csv(\"../../data/1987.csv.bz2\")\n", "intent": "Load dataset. Change path if needed.\n"}
{"snippet": "heart = pd.read_csv(\"./processed.cleveland.data.clean\", header=None)\n", "intent": "Use `pd.read_csv` to read in the file (add header=None).\nUse `pd.shape` to check dimensions and `pd.head()` to take a look at it.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX_scaled = pd.DataFrame(ss.fit_transform(X))\nX_scaled.columns = X.columns\nX = X_scaled\n", "intent": "Use scikit-learn `StandardScaler()` to scale all of the features and use `seaborn` to produce a pairplot and a heatmap of the correlctions.\n"}
{"snippet": "components = pd.DataFrame(pca.components_, \n                        columns = X.columns,\n                        index   = X.columns)\n", "intent": "Check out the `components` of the PCA object - what do they mean?\n"}
{"snippet": "X_pca = pd.DataFrame(pca.transform(X))\nX_pca.columns = ['PC' + str(i+1) for i in range(0,13)]\nX_pca = X_pca.join(y)\nX_pca.head()\n", "intent": "Plot the first two principal components and overlay the disease state as the color to see if they cluster:\n"}
{"snippet": "loans = pd.read_csv(\"loan_data.csv\")\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "college_data = pd.read_csv('College_Data')\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "ad_data = pd.read_csv(\"advertising.csv\")\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "X = cv.fit_transform(X) \n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "df = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n", "intent": "We have a small dataset but a large number of features. This is a good candidate for PCA - want to\nreduce these features to a more reasonable number\n"}
{"snippet": "raw_data.fillna(0, inplace=True)\n", "intent": "All columns associated with cancelled flights have a MaN in the some columns, like a 'Carrier Delay', or other delays. Filing it by zero.\n"}
{"snippet": "from sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nX, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4, bias=100.0)\nmodel = LinearRegression()\nmodel.fit(X, y)\n", "intent": "Create a model to quantify\n"}
{"snippet": "from sklearn.datasets import make_regression\nn_features = 3\nX, y = make_regression(n_samples=30, n_features=n_features, \n                       n_informative=n_features, random_state=42, \n                       noise=0.5, bias=100.0)\nprint(X.shape)\n", "intent": "Generate a linear dataset with 3 features\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nX_train.head()\n", "intent": "The first step is to split your data into Training and Testing using `train_test_split`.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n", "intent": "Step 2) Split data into training and testing data\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_scaler = StandardScaler().fit(X_train)\ny_scaler = StandardScaler().fit(y_train)\nX_train_scaled = X_scaler.transform(X_train)\nX_test_scaled = X_scaler.transform(X_test)\ny_train_scaled = y_scaler.transform(y_train)\ny_test_scaled = y_scaler.transform(y_test)\n", "intent": "Step 3) Scale or Normalize your data. Use StandardScaler if you don't know anything about your data.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n", "intent": "Split our data into training and testing\n"}
{"snippet": "from sklearn.decomposition import RandomizedPCA\nn_components = 64\npca = RandomizedPCA(n_components).fit(X_train_processed)\nX_train_pca = pca.transform(X_train_processed)\nX_test_pca = pca.transform(X_test_processed)\n", "intent": "Now we are going to apply **PCA** to obtain a dictionary of codewords. \n**`RamdomizedPCA`** class is what we need.\n"}
{"snippet": "dfIBM = pd.read_csv('./data/WA_Fn-UseC_-HR-Employee-Attrition.csv')\n", "intent": "Let's get the data, look at the first lines, check types and omissions\n"}
{"snippet": "df_test = pd.read_csv('titanic/test.csv')\ndf_test.head()\n", "intent": "Read the test data:\n"}
{"snippet": "df_test['Survived'] = test_y\ndf_test[['PassengerId', 'Survived']] \\\n    .to_csv('titanic/results-rf.csv', index=False)\n", "intent": "Create a DataFrame by combining the index from the test data with the output of predictions, then write the results to the output:\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_feat,columns=df.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.3,random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "pca = PCA(n_components=2, whiten=False)\npca.fit(X)\nY = pca.transform(X)\n", "intent": "Now use the first 2 `loadings` of the PCA and plot them in a scatter plot and color the by the lables `y`.\n"}
{"snippet": "scaled_bnote = scaler.fit_transform(bnote.drop('Class',axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "bnote_feat = pd.DataFrame(data = scaled_bnote, columns=bnote.columns[:-1])\nbnote_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "** Use SciKit Learn to create training and testing sets of the data as we've done in previous lectures:**\n"}
{"snippet": "cdf = pd.read_csv('College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "pd.concat([pd.DataFrame({'Unique Values': dfIBM.nunique().sort_values()}),\n           pd.DataFrame({'Type': dfIBM.dtypes})], axis=1, sort=False).sort_values(by='Unique Values')\n", "intent": "We can check count of unique values for all features\n"}
{"snippet": "ad_data = pd.read_csv('advertising.csv')\nad_data.head()\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler() \nnumerical = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\nfeatures_log_minmax_transformed = pd.DataFrame(data = features_log_transformed)\nfeatures_log_minmax_transformed[numerical] = scaler.fit_transform(features_log_transformed[numerical])\ndisplay(features_log_minmax_transformed.head(n = 5))\n", "intent": "Data normalization ensures that each feature is treated equally when applying supervised learners. \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features_log_minmax_transformed, \n                                                    outcome, \n                                                    test_size = 0.2, \n                                                    random_state = 0)\nprint (\"Training set has {} samples.\".format(X_train.shape[0]))\nprint (\"Testing set has {} samples.\".format(X_test.shape[0]))\n", "intent": "Data is splitted in 8:2 proportion to created training and testing set\n"}
{"snippet": "import pandas as pd\nfrom IPython.display import display \ndata = pd.read_csv('student_data.csv')\ndisplay(data.head(10))\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "train, test = train_test_split(df, test_size = 0.2, random_state = 0)\ny_train, y_test = train['loss'], test['loss']\ndel train['loss'], test['loss']\n", "intent": "This is equivalent to a single iteration of 5-fold CV\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(documents,lowercase=True,stop_words='english')\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "from keras.datasets import mnist\n(train_images, train_labels), (val_images, val_labels) = mnist.load_data()\n", "intent": "Load the MNIST data.\n"}
{"snippet": "from keras.datasets import fashion_mnist\n(train_images, train_labels), (val_images, val_labels) = fashion_mnist.load_data()\n", "intent": "Load the fashion MNIST data.\n"}
{"snippet": "from keras.datasets import mnist\n(train_images, train_labels), (val_images, val_labels) = mnist.load_data()\n", "intent": "First we load the MNIST data.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_sc = pd.DataFrame(sc.fit_transform(X_train[Categorical_with_order+Numeric]),\n                          columns=Categorical_with_order+Numeric, index=X_train.index)\nX_holdout_sc = pd.DataFrame(sc.transform(X_holdout[Categorical_with_order+Numeric]),\n                            columns=Categorical_with_order+Numeric, index=X_holdout.index)\n", "intent": "Now we can to scale all numerical features for use Logistic Regression.\n"}
{"snippet": "train_data, test_data,train_target, test_target = train_test_split(data, target, test_size=0.33, random_state=54)\n", "intent": "Splitt inn i trening og testdata med `train_test_split`\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_data, test_data,train_target, test_target = train_test_split(df2[['0','1']], df2['2'], test_size=0.33, random_state=54)\n", "intent": "Splitt inn i trening og testdata med `train_test_split`\n"}
{"snippet": "import pandas as pd\ndftrain = pd.read_csv('myimagery/traindata.csv')\ndftest = pd.read_csv('myimagery/testdata.csv')\nprint dftrain.head(5)\n", "intent": "Read in the CSV file as Pandas dataframes\n"}
{"snippet": "colnames=['Age','Workclass','Sector','Education','Education-num','Marital-Status','Occupation','Relationship','Race','Sex','Capital-Gain','Capital-Loss','Hours-Per-Week','Native-Country','y']\ndf =pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',names=colnames)\n", "intent": "We begin by reading in the data with the column names specified\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\niris.keys()\n", "intent": "<h3>Collecting Data</h3>\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df\nY = iris.target\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 0)\n", "intent": "<h3>Splitting the dataset into training and testing datasets</h3>\n"}
{"snippet": "import pandas as pd\ntrain_data = pd.read_csv('train.csv')\ntrain_data.head(4)\n", "intent": "<h3>Collecting Training Data</h3>\n"}
{"snippet": "df = pd.DataFrame()\ndf['ship_type']  =  np.random.choice([\"romulan\", \"human\", \"klingon\", \"borg\", \"red_shirt\", \"ovid\"], size=500)\ndf['ship_value'] =  np.random.randint(200000, 10000000, size=500)\ndf['ship_speed'] =  np.random.randint(10, 60, size=500)\ndf['baths']      =  np.random.choice(np.arange(1, 4, 0.5), size=500)\n", "intent": "With 2-3 continious variables, and one categorical.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler      =  StandardScaler()\ncolumns     =  ['ship_value', 'ship_speed']\ndf[columns] =  scaler.fit_transform(df[columns])\nformula  =  \"baths ~ ship_value + ship_speed\"\ny, X     =  patsy.dmatrices(formula, data=df, return_type=\"dataframe\")\nlm = LinearRegression()\nmodel = lm.fit(X, y)\nscore = model.score(X, y)\nprint \"R^2: \", score\n", "intent": "This is a very standard implementation of Patsy with feature scaling.\n"}
{"snippet": "pd.DataFrame({'Name': X_train_RF.columns.values,\n              'Coefficient': rf.feature_importances_}).sort_values(by='Coefficient',\n                                                                   ascending=False)\n", "intent": "Let's see features importances\n"}
{"snippet": "df = pd.read_csv('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/datasets/breast_cancer_wisconsin/wdbc.data', \n                  header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "b_cancer = pd.read_csv('/Users/austinwhaley/Desktop/DSI-SF-4-austinmwhaley/datasets/breast_cancer_wisconsin/breast_cancer.csv')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "docs = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\ndocs.sum()\n", "intent": "What is being counted?\n_Warning: big ugly sparse matrix ahead._\n"}
{"snippet": "hep_pcs = pca.transform(Xn)\nhep_pcs = pd.DataFrame(pca.components_, columns=['PC'+str(i) for i in range(len(event_names))])\nhep_pcs['athlete'] = hep['Unnamed: 0']\nhep_pcs['score'] = hep['score']\nhep_pcs.head()\n", "intent": "---\nAdd back in the athelete and score columns from the original data.\n"}
{"snippet": "cvt      =  CountVectorizer(strip_accents='unicode', ngram_range=(2,4))\nX_all    =  cvt.fit_transform(insults_df['Comment'])\ncolumns  =  cvt.get_feature_names()\nx_df     =  pd.DataFrame(X_all.toarray(), columns=columns)\ntf_df    =  pd.DataFrame(x_df.sum(), columns=[\"freq\"])\ntf_df.sort_values(\"freq\", ascending=False).head(10)\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "cvt = CountVectorizer(stop_words=\"english\", ngram_range=(4,6))\nX_all = cvt.fit_transform(insults_df['Comment'])\nx_df     =  pd.DataFrame(X_all.toarray(), columns=cvt.get_feature_names())\ntf_df    =  pd.DataFrame(x_df.sum(), columns=[\"freq\"])\ntf_df.sort_values(\"freq\", ascending=False).head(10)\n", "intent": "Display the top 75 ngrams with frequencies.\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\nnewsgroups_train = fetch_20newsgroups(subset='train')\nnewsgroups_train.target_names\n", "intent": "http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n"}
{"snippet": "import pandas as pd\nvehicles = pd.read_csv('used_vehicles.csv')\nvehicles\n", "intent": "In this example we are going to predict price of vehicles\n"}
{"snippet": "oos = pd.read_csv('used_vehicles_oos.csv')\noos['type'] = oos.type.map({'car':0, 'truck':1})\noos\n", "intent": "To test the models we will use a pre-prepared out-of-sample data set, which will need to apply the same preparations to.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=24)\nprint('Train size:', X_train.size)\nprint('Test size:', X_test.size)\n", "intent": "Split data into train/test with proportional 0.7/0.3 which is common split for such amount of data.\n"}
{"snippet": "stocks = pd.read_csv(\"../Data/stock_px.csv\", parse_dates = True, index_col=0)\n", "intent": "Let's load in some [stock data](../Data/stock_px.csv) and take a look at how we can use `Pandas` to plot some familiar quantities from that data.\n"}
{"snippet": "from sklearn.decomposition import PCA\nRANDOM_STATE=1234\npca = PCA(n_components=2, random_state=RANDOM_STATE) \niris_2d = pca.fit_transform(iris) \n", "intent": "We'll use [scikit-learn's PCA](http://scikit-learn.org/stable/modules/decomposition.html\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2) \niris_2d = pca.fit_transform(iris) \n", "intent": "We'll use [scikit-learn's PCA](http://scikit-learn.org/0.17/modules/decomposition.html\n"}
{"snippet": "hsq = pd.read_csv('./hsq_data.csv')\n", "intent": "It is worth reading over the description of the data columns above for this.\n"}
{"snippet": "tp = np.sum((y_test == 1) & (y_pred_cv == 1))\nfp = np.sum((y_test == 0) & (y_pred_cv == 1))\ntn = np.sum((y_test == 0) & (y_pred_cv == 0))\nfn = np.sum((y_test == 1) & (y_pred_cv == 0))\nprint(tp, fp, tn, fn)\ncm = np.array(confusion_matrix(y_test, y_pred_cv, labels=[1,0]))\nconfusion = pd.DataFrame(cm, index=['is_male', 'is-female'],\n                         columns=['predicted_male', 'predicted_female'])\nconfusion.T\n", "intent": "**9.C Construct the confusion matrix for the Ridge LR.**\n"}
{"snippet": "train_df = pd.read_csv(os.path.join(PATH, 'train_sample.csv'), nrows=200000)\ntrain_df = train_df.join(pd.read_csv(os.path.join(PATH,'mlboot_train_answers.tsv'), delimiter='\\t').set_index('cuid'), on='cuid', how='inner')\n", "intent": "** DictVectorizer **\n"}
{"snippet": "train_df = pd.read_csv(os.path.join(PATH, 'train_sample.csv'), nrows=200000)\ntrain_df = train_df.join(pd.read_csv(os.path.join(PATH,'mlboot_train_answers.tsv'), delimiter='\\t').set_index('cuid'), on='cuid', how='inner')\ntrain_df['cnt1'] = train_df['cnt1'].apply(lambda x: x[1:-1]+',' if len(x)>2 else '')\ntrain_df['cnt2'] = train_df['cnt2'].apply(lambda x: x[1:-1]+',' if len(x)>2 else '')\ntrain_df['cnt3'] = train_df['cnt3'].apply(lambda x: x[1:-1]+',' if len(x)>2 else '')\n", "intent": "** DictVectorizer2 **\n"}
{"snippet": "train_df = pd.read_csv(os.path.join(PATH, 'train_sample.csv'), nrows=700000)\ntrain_df = train_df.join(pd.read_csv(os.path.join(PATH,'mlboot_train_answers.tsv'), delimiter='\\t').set_index('cuid'), on='cuid', how='inner')\n", "intent": "** DictVectorizer **\n"}
{"snippet": "train_df = pd.read_csv(os.path.join(PATH, 'train_sample.csv'), nrows=700000)\ntrain_df = train_df.join(pd.read_csv(os.path.join(PATH,'mlboot_train_answers.tsv'), delimiter='\\t').set_index('cuid'), on='cuid', how='inner')\ntrain_df['cnt1'] = train_df['cnt1'].apply(lambda x: x[1:-1]+',' if len(x)>2 else '')\n", "intent": "** DictVectorizer2 **\n"}
{"snippet": "for column in X_train.columns:\n    X_train_sq, sq_attr = apply_cat_op(X_train, [column], sq_operation, 'sq_')\n    data = pd.concat([X_train, X_train_sq], axis=1)\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data)\n    grig_search = GridSearchCV(model, model_parameters, n_jobs=-1, cv=cv, scoring='accuracy')\n    grig_search.fit(data_scaled, y_train);\n    print('Column:', column, ' ', \n          'Accuracy:', grig_search.best_score_, ' ',\n          'Best params:', grig_search.best_params_)\n", "intent": "Create squared feature for each columns and test in with model:\n"}
{"snippet": "from sklearn.datasets import fetch_olivetti_faces\nfaces = fetch_olivetti_faces(shuffle=False)\nprint(faces.DESCR)\n", "intent": "Import the olivetti faces dataset.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer()\nprint(X[:2])\nprint()\nprint(vec.fit_transform(X).toarray()[:2])\nprint()\nprint(vec.get_feature_names())\n", "intent": "Note that we create our validation folds from our complete dataset as contrasted to creating folds from the training set in the SVM notebook ...\n"}
{"snippet": "file_path = 'shakespeare.txt'\nwith open(file_path,'r') as f:\n    data = f.read()\n    print(\"Data length:\", len(data))\ndata = data.lower()\n", "intent": "In this notebook we'll use shakespeare data, but you can basically choose any text file you like!\n"}
{"snippet": "test_predictions = \ntest_pred_inversed = mlb.inverse_transform(test_predictions)\ntest_predictions_for_submission = '\\n'.join('%i\\t%s' % (i, ','.join(row)) for i, row in enumerate(test_pred_inversed))\ngrader.submit_tag('MultilabelClassification', test_predictions_for_submission)\n", "intent": "When you are happy with the quality, create predictions for *test* set, which you will submit to Coursera.\n"}
{"snippet": "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_log1p_recommends.csv'), \n                           index_col='id')\n", "intent": "Read targets from file.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn import cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error, confusion_matrix\nfrom sklearn.grid_search import GridSearchCV\nAuto = pd.read_csv('Auto.csv', na_values='?').drop('name',axis = 1).dropna()\nAuto.head(5)\n", "intent": "Following code will load and clean the dataset and load some useful functions\n"}
{"snippet": "california_housing_dataframe = pd.read_csv(\"https://dl.google.com/mlcc/mledu-datasets/california_housing_train.csv\", sep=\",\")\n", "intent": "Next, we'll load our data set.\n"}
{"snippet": "from sklearn import preprocessing\nstd_scale = preprocessing.StandardScaler().fit(df[['SQFT', 'BDRMS', 'AGE']])\ndf_std = std_scale.transform(df[['SQFT', 'BDRMS', 'AGE']])\nminmax_scale = preprocessing.MinMaxScaler().fit(df[['SQFT', 'BDRMS', 'AGE']])\ndf_minmax = minmax_scale.transform(df[['SQFT', 'BDRMS', 'AGE']])\n", "intent": "Refer to this if you are confused as to the formula: [Standardization](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n"}
{"snippet": "class sklearn.preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)\n", "intent": "[StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n"}
{"snippet": "trainLabels = pd.read_csv('trainLabels.csv')\ntrainLabels.head()\n", "intent": "Train test has 35126 marked images\n"}
{"snippet": "df_demo = pd.read_csv('iowa_demographics.csv')\ndf_demo.head()\n", "intent": "[Source](https://en.wikipedia.org/wiki/List_of_counties_in_Iowa)\n"}
{"snippet": "df = pd.DataFrame(bos)\ndf.corr()\n", "intent": "**Your turn**: What are some other numeric variables of interest? Plot scatter plots with these variables and *PRICE*.\n"}
{"snippet": "MeanAge = TitanicTrain.Age.mean()\nMedianAge = TitanicTrain.Age.median()\nTitanic.Age = Titanic.Age.fillna(value=MedianAge)\nTitanic[\"AgeGroup\"] = Titanic.apply(lambda row: AgeGroup(row[\"Age\"]), axis=1)\nTitanic[\"Major\"] = Titanic.apply(lambda row: IsMajor(row[\"Age\"]), axis=1)\n", "intent": "**3.1 Impute missing values**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscale = StandardScaler().fit(Titanic[['Age', 'Fare']])\nTitanic[['Age', 'Fare']] = scale.transform(Titanic[['Age', 'Fare']])\n", "intent": "**3.3 Scaling numerical features**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state = 0)\n", "intent": "Separate the points into a training and a test datasets with `random_state = 0`.\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\n", "intent": "Now let's use a decision tree on a real dataset.\n"}
{"snippet": "import numpy as np\nlines = open(\"data_clustering.csv\").read().split()[1:]\nx = np.array([line.split(',')[0] for line in lines], \n             dtype=np.float32)\ny = np.array([line.split(',')[1] for line in lines], \n             dtype=np.float32)\n", "intent": "It seems that we have 2 columns, named V1 and V2. Let's load it into 2 ndarrays : x and y.\n"}
{"snippet": "z = pandas.DataFrame(wine_tree.feature_importances_,\n                     index=X_train.columns,\n                     columns=[\"Importance\"])\nz.sort_values(by=\"Importance\", ascending=False).head(3)\n", "intent": "**Answer**: With optimized hyperparameters, the test accuracy can reach 0.845.\n"}
{"snippet": "train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\ntrain.describe(include=\"all\")\n", "intent": "It's time to read in our training and testing data using `pd.read_csv`, and take a first look at the training data using the `describe()` function.\n"}
{"snippet": "cols_to_fill = [\n    'transactions_count', 'sum_transactions_count', \n    'mean_transactions_count', 'std_transactions_count',\n    'min_transactions_count', 'max_transactions_count',    \n]\ntrain[cols_to_fill] = train[cols_to_fill].fillna(0)\ntest[cols_to_fill] = test[cols_to_fill].fillna(0)\n", "intent": "Fill in the data on `card_id` that do not have transactions over the past three months.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)\n", "intent": "We will use part of our training data (22% in this case) to test the accuracy of our different models.\n"}
{"snippet": "models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)\n", "intent": "Let's compare the accuracies of each model!\n"}
{"snippet": "from skimage import io\npic = io.imread('data/bird_small.png') / 255.\nio.imshow(pic)\n", "intent": "http://scikit-image.org/\n"}
{"snippet": "Xval, Xtest, yval, ytest = train_test_split(mat.get('Xval'),\n                                            mat.get('yval').ravel(),\n                                            test_size=0.5)\n", "intent": "divide original validation data into validation and test set\n"}
{"snippet": "from sklearn.datasets import fetch_olivetti_faces, fetch_lfw_people\nfrom ipywidgets import interact\nimage_shape = (64, 64)\ndataset = fetch_olivetti_faces()\nfaces = dataset.data\n", "intent": "Next, we will take a look at what happens if we project some dataset consisting of human faces onto some basis we call\nthe \"eigenfaces\".\n"}
{"snippet": "lenses_df = pd.read_csv('../data/lenses.csv', index_col=0, \n                        names=['Age', 'Spectacle Prescription', 'Astigmatic', 'Tear Production Rate','Prescription Class'])\nlenses_df\n", "intent": "<img src='assets/images/lenses_dataset.png' width=700px>\n"}
{"snippet": "feature_matrix_train, \\\nfeature_matrix_test, \\\ntarget_vector_train, \\\ntarget_vector_test = train_test_split(istanbul_stocks_feature, \n                                      istanbul_stocks_target, \n                                      test_size=0.1,\n                                      random_state=11)\n", "intent": "We will use forward selection to develop our models.\n"}
{"snippet": "from time import time\nfrom sklearn import datasets\nX, y = datasets.make_regression(int(1e5), \n                                n_features=1000, \n                                noise=75.0)\n", "intent": "We will use it to construct a regression problem with 1000000 instances and 1000 features.\n"}
{"snippet": "df = pd.read_csv('./breast_cancer_wisconsin/wdbc.data', header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer as DV\nencoder = DV(sparse = False)\ncat_hot_x = encoder.fit_transform(X[categ2].T.to_dict().values())\n", "intent": "Let's apply OneHotEncoding for categorical features and scale numeric features\n"}
{"snippet": "def general_classifier(X, y, model, random_state):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state)\n    model.fit(X_train, y_train)\n    train_score = model.score(X_train, y_train) \n    test_score = model.score(X_test, y_test)\n    return {'model' : model,\n            'train_score' : train_score,\n            'test_score' : test_score}\n", "intent": "Complete this wrapper function\n"}
{"snippet": "ndata.fillna(0).tail(5)\n", "intent": "Set the **inplace = True** flag\n"}
{"snippet": "def bar_chart(feature):\n    survived = train[train['Survived'] == 1][feature].value_counts()\n    dead = train[train['Survived'] == 0][feature].value_counts()\n    df = pd.DataFrame([survived, dead])\n    df.index = ['Survived', 'Dead']\n    df.plot(kind='bar', stacked=True, figsize=(10, 5))\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "df_genes = pd.read_csv('data/nci60_data.csv', index_col=0)\n", "intent": "No longer interested in prediction - looking to discover underlying similarities in the data\n"}
{"snippet": "weekly = pd.read_csv('data/weekly.csv')\n", "intent": "a) Fit Logistic Regression with Lag1, Lag2\n"}
{"snippet": "train_X, test_X, train_y, test_y = train_test_split(X, y, \n                                                    train_size=0.5,\n                                                    test_size=0.5,\n                                                    random_state=123,\n                                                    stratify=y)\nprint('All:', np.bincount(y) / float(len(y)) * 100.0)\nprint('Training:', np.bincount(train_y) / float(len(train_y)) * 100.0)\nprint('Test:', np.bincount(test_y) / float(len(test_y)) * 100.0)\n", "intent": "So, in order to stratify the split, we can pass the label array as an additional option to the `train_test_split` function:\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(centers=2, random_state=0)\nprint('X ~ n_samples x n_features:', X.shape)\nprint('y ~ n_samples:', y.shape)\nprint('\\nFirst 5 samples:\\n', X[:5, :])\nprint('\\nFirst 5 labels:', y[:5])\n", "intent": "First, we will look at a two class classification problem in two dimensions. We use the synthetic data generated by the ``make_blobs`` function.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n", "intent": "Again, we start by splitting our dataset into a training (75%) and a test set (25%):\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42)\nX.shape\n", "intent": "Let's start by creating a simple, 2-dimensional, synthetic dataset:\n"}
{"snippet": "X_train,X_val,y_train,y_val = train_test_split(X.drop(['shop', 'y'],axis=1),y, random_state=42)\nX_train.shape,X_val.shape,y_train.shape,y_val.shape\n", "intent": "Usual approach for data split supposed to look like this. We won't forget to drop target variable\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nprint('CountVectorizer defaults')\nCountVectorizer()\n", "intent": "Now, we use the CountVectorizer to parse the text data into a bag-of-words model.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\npipeline = make_pipeline(TfidfVectorizer(), LogisticRegression())\nparams = {'logisticregression__C': [.1, 1, 10, 100],\n          \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (2, 2)]}\ngrid = GridSearchCV(pipeline, param_grid=params, cv=5)\ngrid.fit(text_train, y_train)\nprint(grid.best_params_)\ngrid.score(text_test, y_test)\n", "intent": "Another benefit of using pipelines is that we can now also search over parameters of the feature extraction with ``GridSearchCV``:\n"}
{"snippet": "from sklearn.manifold import TSNE\ntsne = TSNE(random_state=42)\ndigits_tsne = tsne.fit_transform(digits.data)\n", "intent": "Using a more powerful, nonlinear techinque can provide much better visualizations, though.\nHere, we are using the t-SNE manifold learning method:\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\n", "intent": "We will now apply the IsolationForest algorithm to spot digits written in an unconventional way.\n"}
{"snippet": "h_vec = HashingVectorizer(encoding='latin-1')\n", "intent": "Now, let's compare the computational efficiency of the `HashingVectorizer` to the `CountVectorizer`:\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nh_pipeline = Pipeline([\n    ('vec', HashingVectorizer(encoding='latin-1')),\n    ('clf', LogisticRegression(random_state=1)),\n])\nh_pipeline.fit(docs_train, y_train)\n", "intent": "Finally, let us train a LogisticRegression classifier on the IMDb training subset:\n"}
{"snippet": "vec = HashingVectorizer(encoding='latin-1')\nsgd.score(vec.transform(docs_test), y_test)\n", "intent": "Eventually, let us evaluate its performance:\n"}
{"snippet": "WXtable = pd.DataFrame(WX, columns=['lag_{}'.format(name) for name in Xnames])\n", "intent": "Then, we could fit a model using the neighbourhood average synthetic features as well:\n"}
{"snippet": "listings = pd.read_csv('./data/berlin-listings.csv.gz')\nlistings['geometry'] = listings[['longitude','latitude']].apply(shp.Point, axis=1)\nlistings = gpd.GeoDataFrame(listings)\nlistings.crs = {'init':'epsg:4269'}\nlistings = listings.to_crs(epsg=3857)\n", "intent": "First, we'll work with the point data to find regions where airbnbs colocate. \n"}
{"snippet": "df = pd.read_csv('ks-projects-201801.csv')\n", "intent": "So, now let's upload the data and look at it's contents. \n"}
{"snippet": "train_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train_df['comment'], train_df['label'], random_state=17)\n", "intent": "We split data into training and validation parts.\n"}
{"snippet": "X_train_texts = tf_idf_texts.fit_transform(train_texts)\nX_valid_texts = tf_idf_texts.transform(valid_texts)\n", "intent": "Do transformations separately for comments and subreddits. \n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nlr = LogisticRegression(C=0.001, random_state=5, class_weight='balanced')\nscal = StandardScaler()\nlr.fit(scal.fit_transform(X), y)\npd.DataFrame({'feat': independent_columns_names,\n              'coef': lr.coef_.flatten().tolist()}).sort_values(by='coef', ascending=False)\n", "intent": "**Answer:** 2.\n**Solution:**\n"}
{"snippet": "lr = LogisticRegression(C=0.001, random_state=5, class_weight='balanced')\nlr.fit(X, y)\npd.DataFrame({'feat': independent_columns_names,\n              'coef': lr.coef_.flatten().tolist()}).sort_values(by='coef', ascending=False)\n", "intent": "**Answer:** 2.\n**Solution:**\n"}
{"snippet": "pd.DataFrame({'feat': independent_columns_names,\n              'coef': rf_grid_search.best_estimator_.feature_importances_}).sort_values(by='coef', ascending=False)\n", "intent": "Rating of the feature importance:\n"}
{"snippet": "scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n", "intent": "Scale the sample using `StandardScaler` with default parameters.\n"}
{"snippet": "time_df = pd.DataFrame(index=train_df.index)\ntime_df['target'] = train_df['target']\ntime_df['min'] = train_df[times].min(axis=1)\ntime_df['max'] = train_df[times].max(axis=1)\ntime_df['seconds'] = (time_df['max'] - time_df['min']) / np.timedelta64(1, 's')\ntime_df.head()\n", "intent": "Now let us look at the timestamps and try to characterize sessions as timeframes:\n"}
{"snippet": "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_log1p_recommends.csv'), \n                           index_col='id')\ny_train = train_target['log_recommends'].values\n", "intent": "**Read train target and split data for validation.**\n"}
{"snippet": "def impute_nan_with_median(table):\n    for col in table.columns:\n        table[col]= table[col].fillna(table[col].median())\n    return table   \n", "intent": "Let us implement a function that will replace the NaN values by the median in each column of the table.\n"}
{"snippet": "scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n", "intent": "As we have very different scales of our variables, we need to rescale them.\n"}
{"snippet": "import pandas as pd\nurl = '../../data/titanic.csv'\ntitanic = pd.read_csv(url, index_col='PassengerId')\ntitanic.shape\n", "intent": "scikit-learn models expect that all values are **numeric** and **hold meaning**. Thus, missing values are not allowed by scikit-learn.\n"}
{"snippet": "import pandas as pd\nurl = '../../../../2_dataset/titanic.csv'\ntitanic = pd.read_csv(url, index_col='PassengerId')\ntitanic.shape\n", "intent": "scikit-learn models expect that all values are **numeric** and **hold meaning**. Thus, missing values are not allowed by scikit-learn.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n                                features,binarized_labels, test_size=0.33, random_state=42)\n", "intent": "- http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html\n"}
{"snippet": "vect = CountVectorizer(lowercase=True)\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape\nvect.get_feature_names()[1000:1010]\nX_train_dtm.shape\n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "import pandas as pd\nyelp = pd.read_csv('homework_data/yelp.csv')\nyelp.head(1)\n", "intent": "Read `yelp.csv` into a DataFrame.\n"}
{"snippet": "Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train[train['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train[train['Pclass']==3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))\n", "intent": "filling missing values\n"}
{"snippet": "of_df = pd.read_csv(\"../../data/old-faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "vect = TfidfVectorizer(ngram_range=(1,2), min_df=.01, stop_words='english')\nfit_vect = vect.fit_transform(X)\nkm = KMeans(n_clusters=20)\nkm.fit(fit_vect)\ndf['cluster_num'] = km.labels_\nreview_clusters(df)\n", "intent": "1. For TfidfVectorizer - modify ngram_range, min_df, and stop_words\n2. For KMeans - modify n_clusters\n"}
{"snippet": "X['CRFA_C'].fillna(chr(0), inplace=True)\nX['CRFA_C'] = X['CRFA_C'].apply(lambda x: ord(x))\n", "intent": "<b>Fixing 'CRFA_C'</b>\n"}
{"snippet": "currency_df = pd.read_csv('./data/usd_orig.csv')\ncurrency_df.drop([\"open\",\"max\",\"min\"], inplace=True, axis=1)\ncurrency_df['date'] = pd.to_datetime(currency_df['date'])\ncurrency_df['price'] = currency_df['price'].apply(lambda x: x.replace(',', '.'))\ncurrency_df['change%'] = currency_df['change%'].apply(lambda x: x.replace(',', '.').replace('%', ''))\ncurrency_df.to_csv('./data/usd.csv', index=False)\n", "intent": "The exchange rate needs a little preprocessing\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n", "intent": "- Logistic Regression\n"}
{"snippet": "from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nX_scaled = pd.DataFrame(data = X)\nX_scaled[X_numcols] = scaler.fit_transform(X[X_numcols])\nX_scaled.head()\n", "intent": "[3]. Applying RobustScaler to the data. Reference post:\nhttp://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=0)\n", "intent": "- Logistic Regression\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1) \n", "intent": "<b>Splitting X and y into training and testing sets</b>\n"}
{"snippet": "import pandas as pd\nloc=r\"C:\\Users\\anshul\\PycharmProjects\\scikit-learn\\scikit-learn-application\\pima-indians-diabetes.csv\"\ncol_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigre', 'age', 'label']\npima = pd.read_csv(loc, header=None, names=col_names)\n", "intent": " - Pima Indian Diabetes Dataset\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\nonehot_encoder = DictVectorizer()\nX = [\n    {'city': 'New York'},\n    {'city': 'San Francisco'},\n    {'city': 'Chapel Hill'}\n]\nprint(onehot_encoder.fit_transform(X).toarray())\n", "intent": "<b>One-hot-encoding</b>\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"C:\\\\Users\\\\anshul\\\\winequality-red.csv\", sep=';')\ndf.describe()\n", "intent": "The Wine Dataset - https://archive.ics.uci.edu/ml/datasets/wine\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"C:\\\\Users\\\\anshul\\\\PycharmProjects\\\\scikit-learn\\\\scikit-learn-application\\\\Mastering-Machine-Learning-with-scikit-learn-Second-Edition-master\\\\chapter06\\\\train.tsv\", header=0, delimiter='\\t')\nprint(df.count())\n", "intent": "- one-versus-all\n- one-versus-the-rest\n"}
{"snippet": "df = pd.read_csv(\"C:\\\\Users\\\\anshul\\\\PycharmProjects\\\\scikit-learn\\\\scikit-learn-application\\\\Mastering-Machine-Learning-with-scikit-learn-Second-Edition-master\\\\chapter07\\\\pima-indians-diabetes.data\", header=None)\ny = df[8]\nX = df[[0, 1, 2, 3, 4, 5, 6, 7]]\n", "intent": "Comparing the performance of Logistic Regression and naive Bayes classifiers on the Pima Indians Diabetes Database\n"}
{"snippet": "currency_df = pd.read_csv('./data/usd.csv', parse_dates=['date', ])\n", "intent": "Load currency exchange data\n"}
{"snippet": "boston = load_boston()\n", "intent": "First, let's have a look at the dataset from the sklearn library about house prices\n"}
{"snippet": "lb = LabelBinarizer()\nlb.fit(list('CHIMSVAGLPTRFYWDNEQK'))\nX_binarized = pd.DataFrame()\nfor col in X.columns:\n    binarized_cols = lb.transform(X[col])\n    for i, c in enumerate(lb.classes_):\n        X_binarized[str(col) + '_' + str(c)] = binarized_cols[:,i]\n", "intent": "Binarize each column into 1s and 0s representing whether an amino acid is present in that position. Run the cell below.\n"}
{"snippet": "scaled_features = scaler.fit_transform(data.drop('Class', axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features, columns=data.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df = pd.read_csv('College_Data', index_col=0)\ndf.head()\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = iris.drop('species', axis=1)   \ny = iris['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "** Split your data into a training set and a testing set.**\n"}
{"snippet": "labels = pd.DataFrame(list(zip(distances.index.values.tolist(), cluster.labels_)), columns = [\"id\", \"cluster\"])\nlabels.head(10)\n", "intent": "We will now take the results of the clustering and associate each of the data points into a cluster.\n"}
{"snippet": "from sklearn.datasets import *\ndata_boston = load_boston()\nprint data_boston.keys()\n", "intent": "We chose Boston Housing dataset, which contains information about the housing values in suburbs of Boston.\n"}
{"snippet": "pd.DataFrame(zip(boston.columns[:-1], lr.coef_), columns = [\"features\", \"coefficients\"])\n", "intent": "So, it means, that for each feature we have found one coefficient. Let's see\n"}
{"snippet": "y, X = full_df['change%'], full_df.drop('change%', axis=1)\nX['topic'] = X['topic'].fillna('Empty')\nX['tags'] = X['tags'].fillna('Empty')\n", "intent": "Fill N/A values and drop target variable\n"}
{"snippet": "df = pd.read_csv(\"Iris.csv\")\n", "intent": "```\ndf = pd.read_csv(\"Iris.csv\")```\n"}
{"snippet": "scaler = StandardScaler()\nscaler.fit(df_data)\n", "intent": "```\nscaler = StandardScaler()\nscaler.fit(df_data)```\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ndata = fetch_20newsgroups()\ndata.target_names\n", "intent": "```\nfrom sklearn.datasets import fetch_20newsgroups\ndata = fetch_20newsgroups()\ndata.target_names```\n"}
{"snippet": "train = pd.read_csv(\"../datasets/titanic/train.csv\")\ntest = pd.read_csv(\"../datasets/titanic/test.csv\")\n", "intent": "```\ntrain = pd.read_csv(\"../datasets/titanic/train.csv\")\ntest = pd.read_csv(\"../datasets/titanic/test.csv\")```\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"C:/Users/zliu/Documents/GitHub/udemy_dsmlwp/datascience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "airports = pd.read_csv('./airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"data/cars_sample.csv\")\n", "intent": "Run the Ordinary Least Square using the package sklearn\n"}
{"snippet": "pop = pd.read_csv('data/cars_small.csv')\n", "intent": "Run PCA with 2 dimensions on the cars dataset\n"}
{"snippet": "data = pd.read_csv('AmesHousing.txt', delimiter='\\t')\nnumeric_values = np.where(\n    (data.dtypes == np.dtype('int64'))\n    | (data.dtypes == np.dtype('float64'))\n)[0]\nX = data[numeric_values[2:-1]].values\ny = data['SalePrice'].values\nfeature_names = data.columns[numeric_values[2:-1]]\n", "intent": "We will use the Ames housing dataset from http://ww2.amstat.org/publications/jse/v19n3/decock.pdf.\n"}
{"snippet": "one_hot_topics = OneHotEncoder().fit_transform(X[['topic']])\none_hot_tags = OneHotEncoder().fit_transform(X[['tags']])\n", "intent": "Transform categorical features to dummy encoding\n"}
{"snippet": "data = DataFrame(np.column_stack((y,X)))\n", "intent": "Why is this what we want? Let's look at the pairwise correlations.\n"}
{"snippet": "arsenic_all = pd.read_stata('ARM_Data/arsenic/all.dta')\narsenic_wells = pd.read_csv('ARM_Data/arsenic/wells.dat', delimiter=' ')\n", "intent": "Exercise: (a) clean up the NaN, (b) extract only those that voted for republican or democrat. (c) solve a 2-class logistic regression.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\ntrainA = pd.read_csv(data_path, delimiter = ',')\ndata_path = os.path.join(os.getcwd(), 'datasets', 'train_20news_partB.csv')\ntrainB = pd.read_csv(data_path, delimiter = ',')\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "data_path_test = os.path.join(os.getcwd(), 'datasets', 'spambase_test.csv')\nspambase_test = pd.read_csv(data_path_test, delimiter = ',')\nspambase_test\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure\n"}
{"snippet": "forest_vect = CountVectorizer(\n    vocabulary=best_features, \n    max_features=2000\n)\nX_train_forest = forest_vect.fit_transform(X_train_text)\nX_test_forest = forest_vect.transform(X_test_text)\n", "intent": "_9.1 &emsp; Forest v2_\n"}
{"snippet": "X, y, coef = make_regression(n_samples=50,\n                       n_features=1,\n                       n_informative=1,\n                       noise=10.0,\n                       shuffle=False,\n                       coef=True)\n", "intent": "[sklearn.linear_model.LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n"}
{"snippet": "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\nX_poly2 = poly.fit_transform(X)\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_poly2, y, test_size=0.3)\nX_poly2[0:2,:]\n", "intent": "[sklearn.preprocessing.PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n"}
{"snippet": "X, y = make_circles(n_samples=100, noise=.3, factor=0.2)\n", "intent": "[sklearn.datasets.make_circles](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html)\n"}
{"snippet": "df_features = pd.DataFrame(scaled_features, columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "titles_normalized = pd.read_csv('./data/titles_normalized.csv', index_col='idx')\n", "intent": "Perform preprocessing of text features, eliminating various word forms\n"}
{"snippet": "X = CV.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,  random_state=101 )\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('forest-cover-type.csv')\ndf.head()\n", "intent": "First, let's load the dataset:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)\n", "intent": "Before applying PCA it is important to get all of the features onto the same scale by removing the mean and scaling to unit variance.\n"}
{"snippet": "data_stat_PR = pd.DataFrame(columns=['Price Range','number_of_samples'])\np=data.groupby(by='Price indi')\ni = 0\nfor grp, temp in p:\n    temp_data = [grp,len(temp)]\n    data_stat_PR.loc[i] = temp_data\n    i = i+1\n", "intent": "Mean rental fee near <b>EW14 Raffles Place MRT station</b> is the highest.\n"}
{"snippet": "val_X = poly.fit_transform(val_data[:,0:2])\nval_y = val_data[:,2][:,None]\n", "intent": "<hr>\nDo you think the validation error will be (almost) zero?\n"}
{"snippet": "df_mms = p.MinMaxScaler().fit_transform(df) \n", "intent": "`3.` Now it's your turn.  Try fitting the **MinMaxScaler** transformation to this dataset. You should be able to use the previous example to assist.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2)\ndrug_data_normalized_pca2d = pca.fit_transform(drug_data_normalized)\n", "intent": "Next we use PCA to bring the dimensionality of the data from 12 down to 2.\n"}
{"snippet": "print(sum(pca.inverse_transform([0, 1]))\n", "intent": "We can similarly figure out how the second principal component is obtained from the original 12 features:\n"}
{"snippet": "data_train = pd.read_csv('../data/mobile/train.csv')\ndata_test = pd.read_csv('../data/mobile/test.csv')\ndata_test.drop(columns='id', inplace=True)\n", "intent": "Let`s look at data:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_Valid, Y_train, Y_Valid = train_test_split(pad_seq, labels, test_size=0.33, shuffle=True)\n", "intent": "4\\. Split the above data (the sequence and the label) into training (67%) and validation (33%) sets. [3 pts]\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True)\n", "intent": "Hold out 25% of the data for testing.\n"}
{"snippet": "df = pd.read_csv(\"KNN_Project_Data\")\n", "intent": "** Leia o arquivo csv 'KNN_Project_Data' em um DataFrame **\n"}
{"snippet": "X = scale(digits.data)\ny = digits.target\nk = 10\nXTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state = 1, test_size=0.8)\nclf = KNeighborsClassifier(n_neighbors=k)\nclf.fit(XTrain, yTrain)\n", "intent": "Repeat task 1.1 using k-nearest neighbors (k-NN). In part 1, use k=10. In part 3, find the best value of k. \n"}
{"snippet": "newsgroups = fetch_20newsgroups(subset='all')\n", "intent": "**3.1.** Load the dataset.\n1. Print the exact number of news articles in the corpus.\n2. Print all 20 categories the news articles belong to.\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nx = np.array([2, 3, 4]).reshape(3,1)\npoly = PolynomialFeatures(3)\npoly.fit_transform(x)\n", "intent": "This polynomial projection is useful enough that it is built into Scikit-Learn, using the ``PolynomialFeatures`` transformer:\n"}
{"snippet": "label_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(train_targets)\ny_valid = label_encoder.transform(valid_targets)\ny_test = label_encoder.transform(test_targets)\ny_train_1_hot = to_categorical(y_train, N_CLASSES)\ny_valid_1_hot = to_categorical(y_valid, N_CLASSES)\nwith open('label_encoder.p', 'wb') as f:\n    pickle.dump(label_encoder, f)\n", "intent": ">Also convert targets into 1-hot format required by using categorical_crossentropy loss \n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "from the datasets load the iris data into a variable called iris\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=20)\n", "intent": "Let's train the classifier\n"}
{"snippet": "scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\ntsne2 = TSNE(random_state=17)\ntsne_representation2 = tsne2.fit_transform(X_scaled)\n", "intent": "Let's look at another representation of the `scaled data` colored by binary features:\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "Load the sklearn `iris` dataset.  This is one of the built-in datasets included in scikit-learn (and one we've seen before).\n"}
{"snippet": "X, y = digits.data, digits.target\npca = PCA()\nX_r = pca.fit(X).transform(X)\nratios = pca.explained_variance_ratio_\nprint ratios\n", "intent": "Was 2 a good choice - i.e. can we capture enough of the variance with just 2 components?  Let's see what the rest would have looked like:\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "Load the iris dataset:\n"}
{"snippet": "df = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf['species'] = iris.target\ndf.head()\n", "intent": "Create a new pandas dataframe:\n"}
{"snippet": "df = pd.read_csv(\"data/heart_disease.csv\")\ndf.head()\n", "intent": "Import the dataset into a pandas dataframe:\n"}
{"snippet": "df['thal'] = df['thal'].fillna(value=1.0)\ndf['thal'].values\n", "intent": "Ah - turns out we do have to deal with the NaN values\n"}
{"snippet": "vectorizer = CountVectorizer(stop_words=\"english\")\n", "intent": "That's right -- words like \"is\" and \"the\" are so common that they are not really helpful features. Let's remove them in the next step:\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ntrain_set = (\"The sky is blue.\", \"The sun is bright.\")\ntest_set = (\"The sun in the sky is bright.\", \"We can see the shining sun, the bright sun.\")\ncount_vectorizer = CountVectorizer(stop_words=\"english\")\nvocab_train = count_vectorizer.fit_transform(train_set)\nprint \"Vocabulary:\", count_vectorizer.vocabulary_\n", "intent": "The first step is to create our training and testing document set and computing the term frequency matrix:\n"}
{"snippet": "from sklearn import cross_validation\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(\n        wine.values, grape.values, test_size=0.4, random_state=0)\n", "intent": "Implementing cross-validation on our wine SVC is straightforward:\n"}
{"snippet": "X_train_part, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, stratify=y, random_state=17)\n", "intent": "Let's make a split into a train sample and hold-out sample:\n"}
{"snippet": "svd.load_data(filename='./data/movielens/ratings.dat', sep='::', format={'col':0, 'row':1, 'value':2, 'ids': int})\n", "intent": "Populate it with the data from the ratings dataset, using the built in load_data method\n"}
{"snippet": "data.fillna(grouped_by_age_pclass, inplace=True)\n", "intent": "Let's Keep It Simple\n"}
{"snippet": "titanic = pd.read_csv('../data/kaggle-titanic/train.csv')\n", "intent": "Today, we are going to Predict Whether a Passenger will Survive or not, given other attributes\n"}
{"snippet": "x_train_text, y_train = imdb.load_data(train=True)\nx_test_text, y_test = imdb.load_data(train=False)\n", "intent": "Load the training- and test-sets.\n"}
{"snippet": "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n                           padding=pad, truncating=pad)\ntokens_pad.shape\n", "intent": "To input texts with different lengths into the model, we also need to pad and truncate them.\n"}
{"snippet": "data_src = europarl.load_data(english=False,\n                              language_code=language_code)\n", "intent": "Load the texts for the source-language, here we use Danish.\n"}
{"snippet": "data_dest = europarl.load_data(english=True,\n                               language_code=language_code,\n                               start=mark_start,\n                               end=mark_end)\n", "intent": "Load the texts for the destination-language, here we use English.\n"}
{"snippet": "x_train_scaled = x_scaler.fit_transform(x_train)\n", "intent": "We then detect the range of values from the training-data and scale the training-data.\n"}
{"snippet": "from sklearn.preprocessing import LabelBinarizer\nencoder = LabelBinarizer()\nhousing_cat_1hot = encoder.fit_transform(housing_cat)\nhousing_cat_1hot\n", "intent": "- A shortcut (text categories => integer categories => one-hot vectors)\n"}
{"snippet": "scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_train_part_scaled, X_valid_scaled, y_train, y_valid = train_test_split(X_scaled, y,\\\n                                                        test_size=0.3, stratify=y, random_state=17)\n", "intent": "Some models should not be scaled, but for others it is necessary:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX2D = pca.fit_transform(X)\nprint(pca.components_[0])\nprint(pca.components_.T[:,0])\n", "intent": "* Uses SVD decomposition as before.\n* You can access each PC using *components_* variable. (\n"}
{"snippet": "pca = PCA()\npca.fit(X)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nd = np.argmax(cumsum >= 0.95) + 1\nprint(d)\n", "intent": "* No need to choose arbitrary \n"}
{"snippet": "from sklearn.decomposition import IncrementalPCA\nn_batches = 100\ninc_pca = IncrementalPCA(n_components=154)\nfor X_batch in np.array_split(X_mnist, n_batches):\n    print(\".\", end=\"\")\n    inc_pca.partial_fit(X_batch)\nX_mnist_reduced_inc = inc_pca.transform(X_mnist)\n", "intent": "* PCA normally requires entire dataset in memory for SVD algorithm.\n* **Incremental PCA (IPCA)** splits dataset into batches.\n"}
{"snippet": "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\nt1 = time.time()\nX_reduced = rnd_pca.fit_transform(X_mnist)\nt2 = time.time()\nprint(t2-t1, \"seconds\")\n", "intent": "* Stochastic algorithm, quickly finds approximation of 1st d components. Dramatically faster.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_housing_data = scaler.fit_transform(\n    housing.data)\nscaled_housing_data_plus_bias = np.c_[\n    np.ones((m, 1)), \n    scaled_housing_data]\nimport pandas as pd\npd.DataFrame(scaled_housing_data_plus_bias).info()\n", "intent": "* Could use TF; let's use Scikit first.\n"}
{"snippet": "le = LabelEncoder()\nle.fit(dicti)\n", "intent": "One Hot Encoding (OHE)\n"}
{"snippet": "df.fillna('-', inplace=True)\ntest.fillna('-', inplace=True)\n", "intent": "Wanted more clever way to dell and fill nan, but dropna or fillna, will work good.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nvec = TfidfVectorizer(max_features=4000, stop_words='english')\ntfidf = vec.fit_transform(bbc_news.text)\ntfidf.shape\n", "intent": "** TFIDF Count Vector **\n"}
{"snippet": "df = pd.read_csv(\"HousingAll.csv\")\ndf['Date'] = pd.to_datetime(df['Date'])\ndf = df.set_index('Date')\ndf.head()\n", "intent": "unsure about how it downsamples\n"}
{"snippet": "X2 = data_train2.drop(['price_range','inch'], axis=1)\nscaler2 = StandardScaler()\nX_scaled2, y2 = scaler2.fit_transform(X2), data_train2['price_range']\nX_train_part_scaled2, X_valid_scaled2, y_train2, y_valid2 = train_test_split\\\n                            (X_scaled2, y2, test_size=.3, stratify=y2, random_state=17)\n", "intent": "For `OneVsOneClassifier` with `LogisticRegression` unscaled matrix features:\n"}
{"snippet": "from google.cloud import bigquery\nbq = bigquery.Client(project=PROJECT)\nfor phase in [\"TRAIN\", \"VALID\", \"TEST\"]:\n    query_string = create_query(phase, \"5000\")\n    df = bq.query(query_string).to_dataframe()\n    df.to_csv(\"taxi-{}.csv\".format(phase.lower()), index_label = False, index = False)\n    print(\"Wrote {} lines to {}\".format(len(df), \"taxi-{}.csv\".format(phase.lower())))\n", "intent": "Now let's execute a query for train/valid/test and write the results to disk in csv format. We use Pandas's `.to_csv()` method to do so.\n"}
{"snippet": "df_train = pd.read_csv(filepath_or_buffer = \"./taxi-train.csv\")\ndf_valid = pd.read_csv(filepath_or_buffer = \"./taxi-valid.csv\")\ndf_test = pd.read_csv(filepath_or_buffer = \"./taxi-test.csv\")\nCSV_COLUMN_NAMES = list(df_train)\nprint(CSV_COLUMN_NAMES)\nFEATURE_NAMES = CSV_COLUMN_NAMES[1:] \nLABEL_NAME = CSV_COLUMN_NAMES[0] \n", "intent": "Because the files are small we can load them into in-memory Pandas dataframes.\n"}
{"snippet": "import shutil\nshutil.rmtree(path = \"data/sines\", ignore_errors = True)\nos.makedirs(\"data/sines/\")\nnp.random.seed(1) \nfor i in range(0,10):\n    to_csv(\"data/sines/train-{}.csv\".format(i), 1000)  \n    to_csv(\"data/sines/valid-{}.csv\".format(i), 250)\n", "intent": "<h3> Cloud ML Engine </h3>\nNow to train on Cloud ML Engine with more data.\n"}
{"snippet": "import shutil\nshutil.rmtree(path = \"data/sines\", ignore_errors = True)\nos.makedirs(\"data/sines/\")\nfor i in range(0,10):\n    to_csv(\"data/sines/train-{}.csv\".format(i), 1000)  \n    to_csv(\"data/sines/valid-{}.csv\".format(i), 250)\n", "intent": "<h3> Cloud ML Engine </h3>\nNow to train on Cloud ML Engine with more data.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"../data/yelp.csv\")\n", "intent": "Read **`yelp.csv`** into a Pandas DataFrame and examine it.\n"}
{"snippet": "combined = data[\"sat_results\"]\ncombined = combined.merge(data[\"ap_2010\"], on=\"DBN\", how=\"left\")\ncombined = combined.merge(data[\"graduation\"], on=\"DBN\", how=\"left\")\nto_merge = [\"class_size\", \"demographics\", \"survey\", \"hs_directory\"]\nfor m in to_merge:\n    combined = combined.merge(data[m], on=\"DBN\", how=\"inner\")\ncombined = combined.fillna(combined.mean())\ncombined = combined.fillna(0)\n", "intent": "Now the datasets can be combined in one by DBN column.\n"}
{"snippet": "pca = PCA(whiten=False, n_components=2)\n", "intent": "Above we got all components and then projected down. Here we have to specify the number of components up front.\n"}
{"snippet": "Xp_orig = scl.inverse_transform(Xp)\n", "intent": "If you want to go back to the original, non-standardized space, you need to `scl` object:\n"}
{"snippet": "pca2D = PCA(whiten=False, n_components=2)\nXtrans = pca2D.fit_transform(Xn)\nXp = pca2D.inverse_transform(Xtrans)\npca2D_std = PCA(whiten=False, n_components=2)\nXtrans_std = pca2D.fit_transform(Xs)\nXp_std = pca2D.inverse_transform(Xtrans_std)\n", "intent": "Again I'll do this for both standardised and non-standardised data.\n"}
{"snippet": "pca = PCA(n_components=0.9, random_state=17).fit(X2)\nX_pca = pca.transform(X2)\n", "intent": "Reduce the dimension while preserving the variance:\n"}
{"snippet": "pca = PCA(whiten=False, n_components=3)\npca.fit(X.T)\n", "intent": "In this case we are interested in the least important eigenvector as that should give the perpendicular to the plane.\n"}
{"snippet": "df = pd.DataFrame(X[0:1000, :], columns=['u-g', 'g-r', 'r-i', 'i-z', 'T'])\n", "intent": "Seaborn works a lot better with Pandas data frames so we create one here and then print the first three rows in the table.\n"}
{"snippet": "with open('data/track1/driving_log.csv', newline='') as f:\n    track_data = list(csv.reader(f, skipinitialspace=True, delimiter=',', quoting=csv.QUOTE_NONE))\ntrack_df = pd.read_csv('data/track1/driving_log.csv', header=0)\nprint('total records: ', len(track_data))\nprint('\\nfirst record:\\n', track_data[0])\nprint('\\nDataframe header:\\n')\ntrack_df.head()\n", "intent": "This is data recorded using the driving simulator. \n"}
{"snippet": "vectorizer = TfidfVectorizer()\nvectors = vectorizer.fit_transform(newsgroups.data)\n", "intent": "Now we compute the TF-IDF statistic for the newspaper dataset. To do so we use the `TfidfVectorizer()` function from the `sklearn` library.\n"}
{"snippet": "ref_df= pd.DataFrame(X, columns=['CreditScore', 'Geography',\n       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n       'IsActiveMember', 'EstimatedSalary'])\nref_df.head(2)\n", "intent": "- Convert countries to dummy variable\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_2 = LabelEncoder()\nX[:,2] = labelencoder_X_2.fit_transform(X[:,2])\n", "intent": "- Convert gender to dummy variables\n"}
{"snippet": "onehotencoder = OneHotEncoder(categorical_features=[1])\nX = onehotencoder.fit_transform(X).toarray()\n", "intent": "- Avoid multiple dummy variable by using onehotencoder \n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n", "intent": "- **Feature Scaling** is key due to its expensive calculations\n"}
{"snippet": "import statsmodels.api as sm\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nX = df[['Cylinder', 'Liter', 'Cruise']]\ny = df['Price']\nX[['Cylinder', 'Liter', 'Cruise']] = scale.fit_transform(X[['Cylinder', 'Liter', 'Cruise']].as_matrix())\nest = sm.OLS(y, X).fit()\nest.summary()\n", "intent": "Let's put these columns into multivariate regression.\n"}
{"snippet": "y=df['y'] \nscaler=MinMaxScaler()\ndf.iloc[:,5:10]=scaler.fit_transform(df.iloc[:,5:10]) \n", "intent": "We have gone from 20 features to 52 featues.\n"}
{"snippet": "banknotes_scaled = scaler.fit_transform(banknotes.drop('Class',axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "df_feat = pd.DataFrame(data=banknotes_scaled,columns=banknotes.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "num_col_fill_MostCom = [\"LotFrontage\", \"GarageYrBlt\"]\nnum_df[num_col_fill_MostCom] = num_df[num_col_fill_MostCom].apply(lambda x:x.fillna(x.value_counts().index[0]))\n", "intent": "Let's fill \"LotFrontage\", \"GarageYrBlt\" features with most common values.\n"}
{"snippet": "num_df = num_df.apply(lambda x:x.fillna(0))\n", "intent": "We fill the rest just with 0.\n"}
{"snippet": "fake = pd.DataFrame(ans_df, columns = ['target'])\nfake.to_csv('ridge2.csv', index = False)\n", "intent": "RMSE: 4.359570467231344\n"}
{"snippet": "def pca(n_components, X_train, X_test, whiten=True):\n    pca = PCA(n_components=n_components, whiten=whiten).fit(X_train)\n    return pca.transform(X_train), pca.transform(X_test)\n", "intent": "But here we see that Confusion matrix diagonal value is relevant only for 3rd class, so that we should balance our target.\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features, columns = df.columns[:-1]) \n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "loans =  pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "col = pd.read_csv(\"College_Data\", index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df = pd.read_csv('data/winemag-data-130k-v2.csv', index_col=0)\ndf.info(memory_usage='deep')\n", "intent": "Let's download the data from Kaggle, extract them into ```data``` folder and check the main properties of the resulting DataFrame:\n"}
{"snippet": "df =  pd.read_csv(\"Classified Data\", index_col=0)\n", "intent": "Set index_col=0 to use the first column as the index.\n"}
{"snippet": "import pandas as pd\nurl = './data/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer.head()\n", "intent": "<a id=\"k-means-demo\"></a>\n---\n"}
{"snippet": "df = pd.read_csv('./assets/data/haystack.csv')\n", "intent": "**Load the haystack data**\n"}
{"snippet": "pd.DataFrame(lm.coef_.T, X.columns)\n", "intent": "**View LASSO coefficients and identify which feature is irrelevant**\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nprint digits.keys()\n", "intent": "This does not load in as a dataframe like we are used to, but instead is a native format to sklearn called a \"Bunch\"\n"}
{"snippet": "pd.DataFrame(digits.data).head()\n", "intent": "We have 64 features of the data, one for each pixel in the 8x8 pixel image.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit(X_train).transform(X_train)\nX_test_scaled = scaler.fit(X_test).transform(X_test)\nmlp = MLPClassifier(activation='logistic',solver='lbfgs',random_state=42, hidden_layer_sizes=(3))  \nmlp.fit(X_train_scaled, y_train)\nprint('Accuracy on the training subset: {:.3f}'.format(mlp.score(X_train_scaled, y_train)))\nprint('Accuracy on the test subset: {:.3f}'.format(mlp.score(X_test_scaled, y_test)))\n", "intent": "Let's also standardize our features since Multilayer Perceptron is sensitive to feature scaling\n"}
{"snippet": "url = 'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data'\ndf = pd.read_csv(url, sep='\\t', header=0)\ndf = df.drop('Unnamed: 0', axis=1)   \n", "intent": "Our analysis begins by getting the data from Tibshirani's website.   \n"}
{"snippet": "df = pd.read_csv(\"College.csv\")\ndata = df.values\ndata1 = data[:,2:19]\nX = data1[:,0:16]\nY = data1[:,16]\nprint(X.shape)\n", "intent": "Import the data from the file College.csv\n"}
{"snippet": "ss = StandardScaler()\ndf_train.price = ss.fit_transform(df_train[['price']])\ndf_test.price = ss.transform(df_test[['price']])\n", "intent": "Our model is sensitive to non-centered numeric features, so we need to scale them:\n"}
{"snippet": "df = pd.read_csv(\"College.csv\")\ndata = df.values\ndata1 = data[:,2:19]\nX = data1[0:16]\nY = data1[17]\n", "intent": "Import the data from the file College.csv\n"}
{"snippet": "x_scale = preprocessing.scale(cancer.data)\ny = cancer.target\nx_train, x_test, y_train, y_test = train_test_split(x_scale, cancer.target, stratify= cancer.target, random_state=0)\n", "intent": "Before splitting the data into train and test- scale the data since we will be using gradient ascent\n"}
{"snippet": "X_scale = StandardScaler()\nX = X_scale.fit_transform(digits.data)\nX[0,:] \n", "intent": "The training features range from 0 to 15.  To help the algorithm converge, we will scale the data to have a mean of 0 and unit variance\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.25,\n                                                    random_state=1234,\n                                                    stratify=y)\n", "intent": "<img src=\"sklearn/figures/train_test_split_matrix.svg\" width=\"100%\">\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n", "intent": "Again, we start by splitting our dataset into a training (75%) and a test set (25%):\n"}
{"snippet": "for column_name in ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\\\n                   'PoolQC', 'Fence', 'MiscFeature', 'FireplaceQu']:\n    comb_data[column_name].fillna('Not', inplace=True)\n", "intent": "*Features that can be dropped - Alley', 'Fence', 'MiscFeature', 'PoolQC' [>80% missing data]*\n"}
{"snippet": "for column_name in ['GarageYrBlt']:\n    comb_data[column_name].fillna(1900, inplace=True)\n", "intent": "It shows that null values in 'GarageYrBlt' mean houses without garage. I am filling with 1900 to imply less value than others with garage.\n"}
{"snippet": "stumble_upon['title'] = stumble_upon.boilerplate.map(lambda x: json.loads(x).get('title', ''))\nstumble_upon['body'] = stumble_upon.boilerplate.map(lambda x: json.loads(x).get('body', ''))\ntitles = stumble_upon['title'].fillna('')\nbody = stumble_upon['title'].fillna('')\ntitles[0:5]\n", "intent": "This is a bit of the NLP we covered in the pipeline lecture!\n---\n"}
{"snippet": "def movie_parser(m):\n    return [m['num_votes'], m['rating'], m['tconst'], m['title'], m['year']]\nparsed = np.array([movie_parser(m) for m in top250])\nmovies = pd.DataFrame(parsed, columns = ['num_votes', 'rating', 'tconst', 'title', 'year'])\n", "intent": "Keep the fields:\n    num_votes\n    rating\n    tconst\n    title\n    year\nAnd discard the rest\n"}
{"snippet": "X_train_logreg_,X_test_logreg_,y_train_logreg,y_test_logreg=train_test_split(feats_logreg,\n                                        target,test_size=0.3,random_state=17,stratify=target)\n", "intent": "Now we'll split our data.  *stratify* used due to imbalance in classes.\n"}
{"snippet": "X_train_quadratic = quadratic_featurizer.fit_transform(X_train)\nX_test_quadratic = quadratic_featurizer.transform(X_test) \nprint X_train\nprint X_train_quadratic\n", "intent": "Now we will transform the training and test features.\n"}
{"snippet": "X_train_quadratic = quadratic_featurizer.fit_transform(X_train)\nX_test_quadratic = quadratic_featurizer.transform(X_test) \n", "intent": "Now we will transform the training and test features.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nbinary_vectorizer = CountVectorizer(binary=True)\nX = binary_vectorizer.fit_transform(corpus)\nprint binary_vectorizer.vocabulary_\nX.todense()\n", "intent": "Now let's use scikit-learn to create our feature representations. `CountVectorizer` can be used to convert a corpus to a matrix of token counts.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "Now let's create a KNN classifier with sklearn.\n"}
{"snippet": "data = pd.read_csv(\"sample_train.csv\")\n", "intent": "Load all the data set\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n", "intent": "Split X and y into training and test data sets\n"}
{"snippet": "X_train_df = pd.DataFrame(X_train, columns=X.columns)\ny_train_df = pd.DataFrame(y_train, columns=['trade_price'])\nX_train_df.describe()\n", "intent": "Get statistics on explanatory variables for train data set\n"}
{"snippet": "embed_df = pd.read_table(\"embeddings-scaled.EMBEDDING_SIZE=50.txt\", header=None, sep=' ', names=['word'] + range(1,51))\nembeddings = {row[0]: list(row[1:]) for row in embed_df.as_matrix()}\n", "intent": "(http://metaoptimize.com/projects/wordreprs/)\n"}
{"snippet": "binary_vectorizer = CountVectorizer(binary=True) \nX_train_v = binary_vectorizer.fit_transform(X_train)\nprint X_train_v.shape\nprint len(binary_vectorizer.vocabulary_)\n", "intent": "Fit and transform the training data\n"}
{"snippet": "X_train_tb,X_test_tb,y_train_tb,y_test_tb=train_test_split(feats_tb,\n                                        target,test_size=0.3,random_state=17,stratify=target)\n", "intent": "Let's split our data. *stratify* used due to imbalance in classes.\n"}
{"snippet": "content_image = scipy.misc.imread(\"images/my_content.jpg\")\ncontent_image = reshape_and_normalize_image(content_image)\n", "intent": "Let's load, reshape, and normalize our \"content\" image (the Louvre museum picture):\n"}
{"snippet": "from __future__ import print_function, division\nimport pandas as pd\ndf = pd.read_csv(\"data/Daily_Demand_Forecasting_Orders.csv\", delimiter=';')\ndf\n", "intent": "https://archive.ics.uci.edu/ml/datasets.html\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndarbe = pd.read_csv('darbe.csv')\ndarbe.info()\n", "intent": " - [darbe](https://trends.google.com.tr/trends/explore?date=today%205-y&geo=TR&q=darbe)\n"}
{"snippet": "sc = StandardScaler()\n", "intent": "**Create a StandardScaler() object called scaler.**\n"}
{"snippet": "sdf = pd.DataFrame(ScledData,columns=imagedf.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "cdf = pd.read_csv('College_Data',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "kdf = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scalar = StandardScaler()\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.3,random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train datasets shape:\", train_df.shape)\nprint(\"Test datasets shape:\", test_df.shape)\ntrain_df.head()\n", "intent": "**2. Primary data analysis**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(yelp_class['text'],yelp_class['stars'], test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "Time to standardize the variables.\n** Import StandardScaler from Scikit learn.**\n"}
{"snippet": "dfnew =pd.DataFrame(df_new,columns=df.columns[:-1])\ndfnew.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X=dfnew\ny=df['TARGET CLASS']\nX_Train,X_Test,y_Train,y_Test = train_test_split(X,y,test_size=0.3,random_state=101)\n", "intent": "**Use train_test_split to split your data into a training set and a testing set.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_Train,X_Test,y_Train,y_Test = train_test_split(X,y,test_size=0.3)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "ad_data =pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "centers = [[1, 1], [-2, -1], [1, -1]]\nX, _ = make_blobs(n_samples=1000, centers=centers, cluster_std=0.6)\n", "intent": "Generate some data.  We use sklearn.datasets.samples_generator to get some data to play with\n"}
{"snippet": "df_features = pd.DataFrame(scaled_features, columns=df.columns[:-1])\ndf_features.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "applications['location_country'] = applications['location_country'].fillna('n\\a')\napplications['location_city'] = applications['location_city'].fillna('n\\a')\n", "intent": "Check **location_country** and **location_city**:\n"}
{"snippet": "full_data['Functional'] = full_data['Functional'].fillna('Typ')\n", "intent": "* **Functional**: Home functionality rating. `NaN` means typical, so replace the missing values with `Typ`.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nfeatures = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'ExterQual', 'ExterCond', 'HeatingQC',\n            'PoolQC', 'KitchenQual', 'BsmtFinType1', 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish',\n            'LandSlope', 'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', \n            'OverallCond', 'YrSold', 'MoSold')\nfor feature in features:\n    le = LabelEncoder()\n    le.fit(list(full_data[feature].values))\n    full_data[feature] = le.transform(list(full_data[feature].values))\nfull_data.head(10)\n", "intent": "**Use `LabelEncoder` to split values of each feature into different categories.**\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndf = pd.read_csv('winequality-red.csv', sep=';')\n", "intent": "We take examples from the book \"Mastering Machine Learning with scikit-learn\" written by Gavin Hackeling.\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "Try https://medium.com/@haydar_ai/learning-data-science-day-9-linear-regression-on-boston-housing-dataset-cd62a80775ef\n"}
{"snippet": "train = pd.read_csv(\"train.csv\")\nprint(train.shape)\ntrain.head()\n", "intent": "<a id=\"1\"></a>\n* In this part we load and visualize the data.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=2)\nprint(\"x_train shape\",X_train.shape)\nprint(\"x_test shape\",X_val.shape)\nprint(\"y_train shape\",Y_train.shape)\nprint(\"y_test shape\",Y_val.shape)\n", "intent": "<a id=\"3\"></a>\n* We split the data into train and test sets.\n* test size is 10%.\n* train size is 90%.\n"}
{"snippet": "train = pd.read_csv(\"../input/train.csv\")\nprint(train.shape)\ntrain.head()\n", "intent": "<a id=\"1\"></a>\n* In this part we load and visualize the data.\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nageAndFare = scaler.fit_transform(ageAndFare)\nageAndFare = pd.DataFrame(ageAndFare, columns = [\"age\", \"fare\"])\nageAndFare.plot.scatter(x = \"age\", y = \"fare\")\n", "intent": "Scaling is needed\n> We also see that both variables have different scales.\n"}
{"snippet": "loans = pd.read_csv('../../../../../Documents/data/lending-club/accepted_2007_to_2018Q2.csv.gz', compression='gzip', low_memory=True)\n", "intent": "Read the data into a pandas dataframe:\n"}
{"snippet": "applications['doc_date_i'] = ((pd.to_datetime(applications['app_dt'], format = '%Y-%m-%d')-pd.to_datetime(applications['doc_date'], format = '%Y-%m-%d')).dt.days/365.25).fillna(0)\napplications['client_date_i'] = (pd.to_datetime(applications['app_dt'], format = '%Y-%m-%d')-pd.to_datetime(applications['client_date'], format = '%Y-%m-%d')).dt.days/365.25\n", "intent": "Add new features: time from client registration and from issued of document (for both than more then better)\n"}
{"snippet": "from sklearn.model_selection import train_test_split\n_, itest = train_test_split(range(critics.shape[0]), train_size=0.7, test_size=0.3)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itest] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "data = []\nwith open('/Users/yuka/Desktop/DataScience/Yelp Project/yelp_training_set/yelp_training_set_business.json') as f:\n    for line in f:\n        data.append(json.loads(line))\nyelp_training_set_business=pd.DataFrame(data)\nyelp_training_set_business.head()\n", "intent": "Data loading: Load the business data\n"}
{"snippet": "data = []\nwith open('/Users/yuka/Desktop/DataScience/Yelp Project/yelp_training_set/yelp_training_set_checkin.json') as f:\n    for line in f:\n        data.append(json.loads(line))\nyelp_training_set_checkin=pd.DataFrame(data)\nyelp_training_set_checkin.head()\n", "intent": "Data Loading: Load the Checkin Data\n"}
{"snippet": "data = []\nwith open('/Users/yuka/Desktop/DataScience/Yelp Project/yelp_training_set/yelp_training_set_review.json') as f:\n    for line in f:\n        data.append(json.loads(line))\nyelp_training_set_review=pd.DataFrame(data)\nyelp_training_set_review.head()\n", "intent": "Data loading: Load the review data\n"}
{"snippet": "data = []\nwith open('/Users/yuka/Desktop/DataScience/Yelp Project/yelp_training_set/yelp_training_set_user.json') as f:\n    for line in f:\n        data.append(json.loads(line))\nyelp_training_set_user=pd.DataFrame(data)\nyelp_training_set_user.head()\n", "intent": "Data Loading: Load the user data\n"}
{"snippet": "yelp_final_training_data_review_votes=pd.DataFrame()\nfor i in range(200473):\n    test = pd.DataFrame(data=yelp_final_training_data_set[\"review votes\"][i], index=[i])\n    yelp_final_training_data_review_votes=yelp_final_training_data_review_votes.append(test)\n", "intent": "Feature transform: As the reviews votes column in the yelp_final_training_data_set is a dictonary, we need to convert it to the data frame structure:\n"}
{"snippet": "yelp_review_votes.to_csv(\"/Users/yuka/Desktop/DataScience/Yelp Project/yelp_review_votes_master_data.csv\")\nyelp_data_final.to_csv(\"/Users/yuka/Desktop/DataScience/Yelp Project/yelp_data_final_master_data.csv\")\n", "intent": "Feature transform: Export the data set into the project folder so that we only need to refer to the exported csv files for further analysis.\n"}
{"snippet": "yelp_training_set_user_votes=pd.DataFrame()\nfor i in range(len(yelp_training_set_user)):\n    test = pd.DataFrame(data=yelp_training_set_user[\"votes\"][i], index=[i])\n    yelp_training_set_user_votes=yelp_training_set_user_votes.append(test)\n", "intent": "Convert the data dictionary \"user review votes\" to the user review votes data frame\n"}
{"snippet": "yelp_training_set_checkin_info=pd.DataFrame()\nfor i in range(len(yelp_training_set_checkin)):\n    test = pd.DataFrame(data=yelp_training_set_checkin[\"checkin_info\"][i], index=[i])\n    yelp_training_set_checkin_info=yelp_training_set_checkin_info.append(test)\n", "intent": "Convert the data dictionary \"checkin info\" to checkin info data frame:\n"}
{"snippet": "df_all = pd.read_csv(\"input/weatherAUS.csv\")\n", "intent": "Reading data into memory and creating a Pandas DataFrame object.\n"}
{"snippet": "X_test_train.to_csv(\"/Users/yuka/Desktop/DataScience/Yelp Project/X_test_train.csv\")\n", "intent": "Export the cleaned X_test_train data into the csv file:\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(ngram_range =(1,3), stop_words='english')\nX_train_countVectorizer = cv.fit_transform(X_text_train)\nX_train_countVectorizer.shape \n", "intent": "Use the count vectorizer transformer to convert the review text to the vectors\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_transformer = TfidfVectorizer(ngram_range =(1,3), stop_words='english', lowercase=True, min_df=2 )\nX_text_train_tfidf = tfidf_transformer.fit_transform(X_text_train)\nX_text_train_tfidf.shape \n", "intent": "Use the tfidf transformer to convert the review text to the vectors\n"}
{"snippet": "yelp_data_final_update.to_csv(\"/Users/yuka/Desktop/DataScience/Yelp Project/yelp_data_final_master_data_formodeling.csv\")\n", "intent": "Explore the final data set for modeling into the csv so that we can use it later:\n"}
{"snippet": "yelp_data_final_update=pd.read_csv(\"/Users/yuka/Desktop/DataScience/Yelp Project/yelp_data_final_master_data_formodeling.csv\")\n", "intent": "Import the analytical dataset:\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(ngram_range =(1,3), stop_words='english', min_df=50)\nX_train_countVectorizer = cv.fit_transform(X_text_train)\nX_train_countVectorizer.shape \n", "intent": "Use the count vectorizer transformer to convert the review text to the vectors\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_transformer = TfidfVectorizer(ngram_range =(1,3), stop_words='english', lowercase=True, min_df=50)\nX_text_train_tfidf = tfidf_transformer.fit_transform(X_text_train)\nX_text_train_tfidf.shape \n", "intent": "Use the tfidf transformer to convert the review text to the vectors\n"}
{"snippet": "titanic_train['Age'] = titanic_train['Age'].fillna(titanic_train['Age'].mean())\n", "intent": "Above, when we said:\n"}
{"snippet": "trainCleaned.to_csv('DATA/house-prices/train_cleaned.csv',index=False)\ntestCleaned.to_csv('DATA/house-prices/test_cleaned.csv',index=False)\n", "intent": "To save the dataframe as comma-separated values (CSV) file, run this line of code.\n"}
{"snippet": "from sklearn import preprocessing\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(df)\ndf = pd.DataFrame(scaler.transform(df), index=df.index, columns=df.columns)\ndf.iloc[4:10]\n", "intent": "Next step is to standardize our data - using MinMaxScaler\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\n", "intent": "Let us start loading the data set.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=33)\n", "intent": "Separate train and test\n"}
{"snippet": "from sklearn.feature_selection import *\nfs=SelectKBest(score_func=f_regression,k=5)\nX_new=fs.fit_transform(X_train,y_train)\nprint X_train.shape\nprint X_new.shape\nprint zip(fs.get_support(),data.feature_names) \n", "intent": "We can select the most important features with sklearn\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscalerX = StandardScaler().fit(X_train)\nscalery = StandardScaler().fit(y_train)\nprint np.max(y_train), np.min(y_train), np.mean(y_train), \nX_train = scalerX.transform(X_train)\ny_train = scalery.transform(y_train)\nX_test = scalerX.transform(X_test)\ny_test = scalery.transform(y_test)\nprint np.max(y_train), np.min(y_train), np.mean(y_train)\n", "intent": "Data normalization:\n"}
{"snippet": "from sklearn.feature_selection import *\nfs=SelectKBest(score_func=f_regression,k=5)\nX_new=fs.fit_transform(X_train,y_train)\nprint X_train.shape\nprint X_new.shape\nprint zip(fs.get_support(),data.feature_names)\n", "intent": "We can select the most important features with sklearn\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscalerX = StandardScaler().fit(X_train)\nscalery = StandardScaler().fit(y_train)\nX_train = scalerX.transform(X_train)\ny_train = scalery.transform(y_train)\nX_test = scalerX.transform(X_test)\ny_test = scalery.transform(y_test)\nprint np.max(X_train), np.min(X_train), np.mean(X_train), np.max(y_train), np.min(y_train), np.mean(y_train)\n", "intent": "Data normalization:\n"}
{"snippet": "tf_idf = text.TfidfVectorizer()\nX = tf_idf.fit_transform(X_raw)\n", "intent": "[TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF)\n"}
{"snippet": "txt = [\"Hello there\", \"Anybody there\"]\ntemp = text.TfidfVectorizer()\nX_temp = temp.fit_transform(txt)\ntemp.get_feature_names()\n", "intent": "Just getting to know here\n"}
{"snippet": "ss = RobustScaler()\ndata_ss = pd.DataFrame(ss.fit_transform(data[cols]), index=data.index, columns=cols)\n", "intent": "After some testing, I found that the RobustScaler standardization technique performs better for this dataset due to its robust handling of outliers.\n"}
{"snippet": "data = pd.read_csv('carInsurance_train.csv',index_col='Id')\n", "intent": "Let's look at our dataset. You can download it here: https://www.kaggle.com/kondla/carinsurance\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA()\npca.fit(x_cols)\nprint(pca.explained_variance_ratio_.cumsum())\n", "intent": "---\nFirst let's look at all the PCA components to see how much variance is explained\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimp = Imputer()\ndata[['avg_rating_by_driver', 'avg_rating_of_driver']] = \\\n    imp.fit_transform(data[['avg_rating_by_driver', \n                            'avg_rating_of_driver']])\nprint('Missing data by column: \\n{}'.format(data.isnull().sum()))\n", "intent": "Next I'll fill in the numerical values of *avg_rating_by_driver* and *avg_rating_of_driver* by simply using the mean.\n"}
{"snippet": "lr = LinearRegression()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nlr.fit(X_train, y_train)\nlr.score(X_test, y_test)\n", "intent": "**Generalization is Key**\nSklearn provides some great tools to help with this.\n`train_test_split()`\n`cross_val_score()`\n"}
{"snippet": "from sklearn import model_selection,datasets\nfrom sklearn import linear_model\nboston = datasets.load_boston()\nprint(boston.DESCR)\nboston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\nboston_df\n", "intent": "Models are cheap. Lets make lots of them!\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(tfidf_vecs, \n                                                    ng.target, \n                                                    test_size=0.33)\nnb = MultinomialNB()\nnb.fit(X_train, y_train)\nnb.score(X_test, y_test)\n", "intent": "Let's try simple Naive Bayes classification on TFIDF vectors from above in `sklearn`:\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nimport sklearn.metrics.pairwise as smp\nX_train, X_test, y_train, y_test = train_test_split(ng_lsi, ng.target, \n                                                    test_size=0.33)\nknn = KNeighborsClassifier(n_neighbors=3, metric=smp.cosine_distances)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)\n", "intent": "- Try some simple classification on the result LSI vectors for the 20 NG set:\n"}
{"snippet": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\ntext = ['That is should come to this!', 'This above all: to thine own self be true.', 'Something is rotten in the state of Denmark.']\nvectorizer = CountVectorizer(ngram_range=(1,2))\nvectorizer.fit(text)\nx = vectorizer.transform(text)\nx_back = x.toarray()\npd.DataFrame(x_back, columns=vectorizer.get_feature_names())\n", "intent": "We have been working with a number of techniques and tools that help us navigate the world of NLP.       \n**For example, we have our Vectorizer:**\n"}
{"snippet": "categories = ['comp.graphics', 'rec.sport.baseball', 'rec.motorcycles', 'sci.space', 'alt.atheism']\nng_train = datasets.fetch_20newsgroups(subset='train', categories=categories, \n                                      remove=('headers', 'footers', 'quotes'))\n", "intent": "Let's retain only a subset of the 20 categories in the original 20 Newsgroups Dataset.\n"}
{"snippet": "ratings = pd.read_table('~/data/ml-20m/ratings.csv', sep=',')\n", "intent": "Load the ratings.dat data into a `ratings` variable with the same separator, and the column names UserID, MovieID, Rating, Timestamp.\n"}
{"snippet": "le = LabelEncoder()\nX_df.name = le.fit_transform(X_df['name'])\nX_df.animal_type = le.fit_transform(X_df['animal_type'])\nX_df.color = le.fit_transform(X_df['color'])\nX_df.breed = le.fit_transform(X_df['breed'])\n", "intent": "Let's try to apply LaberEncoder to features.\nFor faster calculations, reduce the sample size (no need to do that if you have enough power)\n"}
{"snippet": "pd.DataFrame(VT.T)\n", "intent": "If I transpose this, the rows are items, and the columns are the items in the \"hidden\" vector space created by the truncated SVD.\n"}
{"snippet": "pd.DataFrame(U)\n", "intent": "**U** is a matrix where each row is a user and each column shows the location in the hidden vector space created by the SVD.\n"}
{"snippet": "pd.DataFrame(Sigma)\n", "intent": "**Sigma** is just the singular values of the decomposition. In this case, we're not particularly interested in **Sigma**.\n"}
{"snippet": "max_features = 2000\n(X_train, y_train), (X_test, y_test) = reuters.load_data(\n    num_words=max_features)\nmaxlen = 10\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nnb_epoch = 20\n", "intent": "- Let's try an RNN for the same Reuters classification task:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=4444)\n", "intent": "Split the data into a test and training set. \n"}
{"snippet": "for column in house_votes_df.iloc[:,1:]:\n    col_mode = house_votes_df[column].mode()[0]\n    house_votes_df[column] = house_votes_df[column].fillna(col_mode)\n", "intent": "Convert each ? to the mode of the column (if a senator has not voted, make their vote 1 if most others voted 1, make it 0 if most others voted 0).\n"}
{"snippet": "df=pd.read_csv('house-votes-84.csv')\n", "intent": "<h1>Challenge 1</h1>\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:], df['republican'], test_size=.3, random_state=4444)\n", "intent": "<h1>Challenge 2</h1>\n"}
{"snippet": "df=pd.read_csv('house-votes-84.csv')\ndf.replace('y',1, inplace=True)\ndf.replace('n',0, inplace=True)\ncols=list(df.columns)\ncols.pop(0)\nfor i in cols:\n    df[i].replace('?',int(df[i].mode()), inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:-1], df['y.8'], test_size=.3, random_state=4444)\n", "intent": "<h1>Challenge 10</h1>\n"}
{"snippet": "X_df = df.drop('outcome_type', axis=1)[:10000]\nX_df = pd.get_dummies(X_df, columns=['animal_type','color','breed'])\nX_df.name = le.fit_transform(X_df['name'])\nX_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.3)\n", "intent": "Let's try to encode the data using pd.get_dummies, let's see how the quality of the models will change\n"}
{"snippet": "df= pd.read_csv('haberman.csv')\nprint(\"the average age of patients: {}\".format(df['30'].mean()))\nprint(\"the stdev age of patients: {}\".format(df['30'].std()))\nprint(\"the average age of patients: {}\".format(df.loc[df['1.1']==1]['30'].mean()))\nprint(\"the average age of patients: {}\".format(df.loc[df['1.1']==1]['30'].std()))\nprint(\"the average age of patients: {}\".format(df.loc[df['1.1']==2]['30'].mean()))\nprint(\"the average age of patients: {}\".format(df.loc[df['1.1']==2]['30'].std()))\n", "intent": "<h1>Challenge 12</h1>\n"}
{"snippet": "X = df2.loc[:, 'handicapped-infants':'export-administration-act-south-africa']\ny = df2['Class Name']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n", "intent": "Split the data into a test and training set. Use this function:\n"}
{"snippet": "movies_df = pd.read_csv('2013_movies.csv', header=0)\n", "intent": "For your movie classifiers, calculate the precision and recall for each class.\n"}
{"snippet": "haberman_df = pd.read_csv(\"haberman.data\", header=None)\nhaberman_df.rename(columns={0:'age_operation',1:'year_operation',2:'num_nodes',3:'survival'}, inplace=True)\n", "intent": "Draw the ROC curve (and calculate AUC) for the logistic regression classifier from challenge 12.\n"}
{"snippet": "movies = pd.read_csv('/home/kalgi/ds/metis/sf17_ds8/challenges/challenges_data/2013_movies.csv')\n", "intent": "For your movie classifiers, calculate the precision and recall for each class.\n"}
{"snippet": "column_names = ['Age of patient at time of operation', 'Patients year of operation','Number of positive axillary nodes detected',\n'Survival status']\nhaberman = pd.read_csv('/home/kalgi/Downloads/data.txt', names=column_names, header=None)\n", "intent": "Draw the ROC curve (and calculate AUC) for the logistic regression classifier from challenge 12.\n"}
{"snippet": "df=pd.read_csv('Dataframe519.csv', sep='|')\n", "intent": "<h1>Data Cleaning</h1>\n"}
{"snippet": "dfExpon=dfFinal.copy()\ndfExpon['EYear']=dfExpon['Year']**2\nX_trainB, X_testB, y_trainB, y_testB = train_test_split(dfExpon.dropna().iloc[:,1:], dfExpon.dropna().loc[:,'Price'], test_size=0.1,random_state=42)\n", "intent": "<h1>Create exponential features and complete linear regression analysis</h1>\n"}
{"snippet": "X_trainC, X_testC, y_trainC, y_testC = train_test_split(dfCoeff.dropna().iloc[:,1:], dfCoeff.dropna().loc[:,'Price'], test_size=0.1,random_state=42)\n", "intent": "<h1>RandomForest with coeff replaced dummies</h1>\n"}
{"snippet": "full_df.pivot_table(['ConvertedSalary'], ['CareerSatisfaction'], aggfunc='mean')\n", "intent": "Let's see average salaries by career satisfactions ranks\n"}
{"snippet": "files=os.listdir(\"/home/sufyan/Metis/Week4/Project3/newdata/\")\nall_dfs=[]\nfor file in files:\n    all_dfs.append(pd.read_csv(\"/home/sufyan/Metis/Week4/Project3/newdata/\"+file))\nfor i in range(len(all_dfs)):\n    all_dfs[i].set_index('Country or Area', inplace=True)\nCountryCodes=pd.read_csv(\"country names and ID.csv\")\n", "intent": "<h1>Build Working Panel</h1>\n"}
{"snippet": "mean_theatre = df3['Theaters'].mean()\nmean_runtime = df3['Runtime'].mean()\ndf3['Theaters'] = df3['Theaters'].fillna(value=mean_theatre)\ndf3['Runtime'] = df3['Runtime'].fillna(value=mean_runtime)\n", "intent": "**Multiple Regression 2**\n"}
{"snippet": "from keras.datasets import mnist\nimport numpy as np\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n", "intent": "Loading the MNIST dataset\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nx_train=x_train[0:20000]\n", "intent": "We will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784.\n"}
{"snippet": "prices_df=pd.DataFrame()\nprices_df[\"Price_Predictions\"]=m.fittedvalues.values\nprices_df[\"Real_Prices\"]=bos.PRICE.values\nsns.regplot(y=\"Price_Predictions\", x=\"Real_Prices\", data=prices_df, fit_reg = True)\n", "intent": "**Your turn:** Create a scatterpot between the predicted prices, available in `m.fittedvalues` and the original prices. How does the plot look?\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndv = DictVectorizer()\ntransformed_data = dv.fit_transform(census_data).toarray()\ntransformed_data\n", "intent": "Now, let's encode those features and instances.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntransformed_labels = le.fit_transform(new_df_labels)\ntransformed_labels\n", "intent": "Now that we've done that, let's encode the labels.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ncensus_train, census_test, labels_train, labels_test = train_test_split(transformed_data, transformed_labels)\n", "intent": "Now that we've done that, can you separate the `transformed_data` and `transformed_labels` into training and test sets?\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits_data = digits.data\ndigits_labels = digits.target\nd_train, d_test, dl_train, dl_test = train_test_split(digits_data, digits_labels)\n", "intent": "This model is a much better fit for our dataset, and is much more accurate than k-nearest neighbors.\n"}
{"snippet": "os_df = full_df[['CareerSatisfaction', 'OperatingSystem']]\nos_df['OperatingSystem'] = os_df['OperatingSystem'].map(\n    lambda os: category_encoders['OperatingSystem'].inverse_transform(os)\n)\nos_dummies_df = pd.get_dummies(os_df, columns=['OperatingSystem'], prefix=\"OS\")\n", "intent": "And also <b>OperatingSystem</b>.\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nboston_data = boston.data\nboston_target = boston.target\nb_train, b_test, bl_train, bl_test = train_test_split(boston_data, boston_target)\n", "intent": "So our score is rather low.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits_data = digits.data\ndigits_labels = digits.target\n", "intent": "This model is a much better fit for our dataset, and is much more accurate than k-nearest neighbors.\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nboston_data = boston.data\nboston_target = boston.target\n", "intent": "So our score is rather low.\n"}
{"snippet": "from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\ncal_house = fetch_california_housing()    \nX = pd.DataFrame(data=cal_house['data'], \n                 columns=cal_house['feature_names'])\\\n             .iloc[:,:-2]\ny = cal_house['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=1)\nprint(X_train.head(3))\n", "intent": "> **Ex. 12.2.0:** Load the california housing data with scikit-learn using the code below. Inspect the data set. \n"}
{"snippet": "degree_index = pd.Index(degrees,name='Polynomial degree ~ model complexity')\nax = pd.DataFrame({'Train set':train_mse, 'Test set':test_mse})\\\n    .set_index(degree_index)\\\n    .plot(figsize=(10,4))\nax.set_ylabel('Mean squared error')\n", "intent": "*So what happens to the model performance in- and out-of-sample?*\n"}
{"snippet": "order_idx = pd.Index(range(n_degrees+1),name='Polynomial order')\nax = pd.DataFrame(parameters,index=order_idx)\\\n.abs().mean(1)\\\n.plot(logy=True)\nax.set_ylabel('Mean parameter size')\n", "intent": "*What do you mean coefficient size increase?*\n"}
{"snippet": "print('\\n'.join(load_boston()['DESCR'].split('\\n')[13:28]))\n", "intent": "*Let's some load Boston house price data*\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_boston\nX = load_boston().data\ny = load_boston().target\nprint(load_boston().feature_names)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\npipe_preproc.fit(X_train) \nX_train_prep = pipe_preproc.transform(X_train) \nX_test_prep = pipe_preproc.transform(X_test) \n", "intent": "*And how do I apply the pipe on the data?*\n"}
{"snippet": "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=1/3, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=1/2, random_state=1)\n", "intent": "*What would this look like in Python?*\n"}
{"snippet": "preprocessed_df = preprocessed_df.fillna(0)\n", "intent": "Fill other NANs with zeros as the simple strategy. (To be honest I've tried other ones, but they didn't show significantly better results)\n"}
{"snippet": "df = pd.DataFrame({'Criminal':[1]*5+[0]*10,\n                   'From Jutland':np.random.randint(0,2,15),                   \n                   'Parents together':[0]*4+[1]*10+[0],\n                   'Parents unemployed':[1]*7+[0]*8})\nprint(df.sample(n=5))\n", "intent": "*Suppose we have data like below, we are interested in predicting criminal*\n"}
{"snippet": "my_split = df\\\n.groupby(['Parents together', \n          'Parents unemployed'])\\\n.Criminal\\\n.value_counts()\\\n.unstack(level=-1)\\\n.fillna(0)\nprint(my_split)\n", "intent": "*What if we also split by 'Parents unemployed'? And From Jutland?*\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv') \n", "intent": "Lets see how it does on a real dataset.\n"}
{"snippet": "from sklearn import datasets\nimport numpy as np\nfrom datetime import datetime as dt\niris = datasets.load_iris().data\nstart = dt.now()\npairwise_distances = []\nfor i in range(iris.shape[0]):\n    for j in range(i+1, iris.shape[0]):\n        pairwise_distances.append(np.sqrt(sum((iris[i] - iris[j])**2)))\nprint(\"Time:\", (dt.now() - start).total_seconds())\n", "intent": ">**Ex. 19.1.3**: Take the following bit of code and parallelize it. Report the speedup (if any)\n"}
{"snippet": "T = 1000\ndata = {v:np.cumsum(np.random.randn(T)) for v in ['A', 'B']}\ndata['time'] = pd.date_range(start='20150101', freq='D', periods=T)\nts_df = pd.DataFrame(data)\nts_df.set_index('time').plot(figsize=(10,5))\n", "intent": "We can easily make and plot time series.\n"}
{"snippet": "nan_data = [[1,np.nan,3],\n            [4,5,None],\n            [7,8,9]]\nnan_df = pd.DataFrame(nan_data, columns=['A','B','C'])\nprint(nan_df.isnull().sum())\n", "intent": "*What does a DataFrame with missing data look like?*\n"}
{"snippet": "print(nan_df.fillna(2))\nselection = nan_df.B.isnull()\nnan_df.loc[selection, 'B'] = -99\nprint(nan_df)\n", "intent": "*How do we fill observations with a constant?*\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndf = pd.read_csv(\n    'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv',\n     header=None,\n     usecols=[0,1,2]\n    )\ndf.columns=['Class label', 'Alcohol', 'Malic acid']\ndf.head()\n", "intent": "** Loading the wine data set **\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler, StandardScaler\nminmax_scale = MinMaxScaler().fit(df[['Alcohol', 'Malic acid']])\nmin_max_x = minmax_scale.transform(df[['Alcohol', 'Malic acid']])\n", "intent": "Mahematical formula is \n$x' = \\dfrac{(x-min)}{(max-min)}$\nAfter rescaling all values are in the range between 0 and 1\n"}
{"snippet": "test_df = pd.read_csv('test.csv', index_col='Respondent')\n", "intent": "Relatively high model improvement, as expected.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\ndf = pd.read_csv('adult.csv', na_values=['\n", "intent": "Task: Given attributes about a person, predict whether their income is <=50K or >50K\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('hw2data.csv')\ndf.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "X = df_final[['balance', 'income', 'student']]\ny = df_final['default']\nrandom_state = [10,20,40,80,160]\nfor s in random_state:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=s)\n    fpr,tpr,roc_auc, thresholds = generate_auc(X_train,y_train,LogisticRegression,C=100, penalty='l1')\n    generate_ROCplot(fpr,tpr,'LR',roc_auc)\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(documents)\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "import pandas as pd\nvehicles = pd.read_csv('used_vehicles.csv')\nvehicles.head(2)\n", "intent": "In this example we are going to predict price of vehicles\n"}
{"snippet": "train_features,test_features,train_labels,test_labels = train_test_split(main_features,labels)\n", "intent": "Let us split our dataset into training and test sets using the cross_validation module in SKLEARN\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3 , random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "data = pd.read_csv('yelp.csv')\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "import keras\nfrom keras.datasets import cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint(y_train[:10])\n", "intent": "---\nIn this notebook, we train a CNN on augmented images from the CIFAR-10 database.\n"}
{"snippet": "test_df = pd.read_csv('test.csv', index_col='Respondent')\n", "intent": "Anyway, the model performs better.\n"}
{"snippet": "man = train[train.Sex == 'male']['Survived'].value_counts()\nwoman = train[train.Sex == 'female']['Survived'].value_counts()\ndf = pd.DataFrame([man,woman])\ndf.index = ['Man','Woman']\ndf.plot(kind='bar',stacked=True, figsize=(8,4))\n", "intent": "<b> 1. Proportion of survived men and women </b>\n"}
{"snippet": "survived_class = train[train['Survived']==1]['Pclass'].value_counts()\ndead_class = train[train['Survived']==0]['Pclass'].value_counts()\ndf = pd.DataFrame([survived_class,dead_class])\ndf.index = ['Survived','Dead']\ndf.plot(kind='bar',stacked=True, figsize=(8,4))\n", "intent": "<b> 2. Proportion of survived and dead in PClass </b>\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('hw2data.csv')\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "size = len(df)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "with open('../sentiment-network/reviews.txt', 'r') as f:\n    reviews = f.read()\nwith open('../sentiment-network/labels.txt', 'r') as f:\n    labels = f.read()\n", "intent": "if you are looking for sentiment-network, here's the link: https://github.com/udacity/deep-learning/tree/master/sentiment-network\n"}
{"snippet": "fare_means = df.pivot_table('Fare', index='Pclass', aggfunc='mean')\nfare_means\n", "intent": "For the column Fare, however, it makes sense to fill in the NaN values with the mean by the column Pclass, or Passenger class.\n"}
{"snippet": "contentImage = io.BytesIO(urlopen(\"https://raw.githubusercontent.com/Baidaly/FORK_deep-learning-v2-pytorch/master/style-transfer/images/octopus.jpg\").read())\ncontent = load_image(contentImage).to(device)\nstyleImage = io.BytesIO(urlopen(\"https://raw.githubusercontent.com/Baidaly/FORK_deep-learning-v2-pytorch/master/style-transfer/images/hockney.jpg\").read())\nstyle = load_image(styleImage, shape=content.shape[-2:]).to(device)\n", "intent": "Next, I'm loading in images by file name and forcing the style image to be the same size as the content image.\n"}
{"snippet": "url = 'https://raw.githubusercontent.com/Baidaly/IPython-Notebooks/master/pytorch-udacity-challenge/Lesson%207/anna.txt'\nresponse = urllib.request.urlopen(url)\ndata = response.read()      \ntext = data.decode('utf-8') \n", "intent": "Then, we'll load the Anna Karenina text file and convert it into integers for our network to use. \n"}
{"snippet": "source = 'https://fb55.cartodb.com/api/v2/sql'\ndef queryCartoDB_py3(source, query, format='CSV'):\n    p = urllib.parse.urlencode({'q':query,\n                                'format':format\n                                })\n    with urllib.request.urlopen(source + '?' + p) as response:\n        the_page = response.read().decode('utf8')\n    return the_page\n", "intent": "Query to access Census 2000 and 2010 data from fb55's CartoDB acct.\n"}
{"snippet": "test_df = pd.read_csv('test.csv', index_col='Respondent')\n", "intent": "Load hold-out set from the disk\n"}
{"snippet": "data = pd.read_csv('D:/PjtData/4_PD/edX_DSE200x/wk7_ML/minute_weather.csv')\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nCreating a Pandas DataFrame from a CSV file<br><br></p>\n"}
{"snippet": "df = pd.read_csv('hw2data.csv')\ndf.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "regr.set_params(alpha=best_alpha)\ndf_coeffs = pd.DataFrame({'coeffs':regr.coef_, 'feature number':X.columns.values})\nsorted_coeffs=df_coeffs.sort_values('coeffs', ascending = False)\nsorted_coeffs[:100].plot(x='feature number',y='coeffs',kind='bar', figsize =(20,10))\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "df = pd.read_csv('https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv')\ndf.head()\n", "intent": "1) Load in the dataset `https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv` into a pandas dataframe\n"}
{"snippet": "PAULG_PATH = 'paulg/'\nPAULG_FILENAME = 'paulg/paulg.txt'\nwith open(PAULG_FILENAME) as f:\n    lines = f.read().split('\\n')\nlines[0]\n", "intent": "http://suriyadeepan.github.io/2017-02-13-unfolding-rnn-2/\n"}
{"snippet": "train = pd.read_csv( 'labeledTrainData.tsv', delimiter = '\\t' )\ntest  = pd.read_csv( 'testData.tsv', delimiter = '\\t' )\ntrain.head()\n", "intent": "Kaggle knowledge competition: movie review sentiment analysis. [homepage](https://www.kaggle.com/c/word2vec-nlp-tutorial)\n"}
{"snippet": "vect1 = CountVectorizer( stop_words = 'english' )\nX_train_dtm = vect1.fit_transform(X_train)\nX_test_dtm  = vect1.transform(X_test)\n", "intent": "Use the most simplest approach: remove english stopwords, bag of words + default logistic regression and naive bayes.\n"}
{"snippet": "tf_vect1 = TfidfVectorizer( \n    stop_words = 'english', \n    tokenizer = tokenizer_porter, \n    ngram_range = ( 1, 2 ),\n    min_df = 2\n)\nX_train_dtm = tf_vect1.fit_transform(X_train)\nX_test_dtm  = tf_vect1.transform(X_test)\n", "intent": "Use tf-idf instead of bag of words.\n"}
{"snippet": "token_text = [token.orth_ for token in parsed_review]\ntoken_pos = [token.pos_ for token in parsed_review]\npost = pd.DataFrame( list( zip(token_text, token_pos) ),\n                     columns = ['token_text', 'part_of_speech'] )\npost.head()\n", "intent": "What about part of speech tagging (POST)?\n"}
{"snippet": "pca = PCA(n_components=pca_comp.n_components).fit(X_train_scaled)\npca_features_train = pca.transform(X_train_scaled)\npca_features_test = pca.transform(X_test_scaled)\n", "intent": "PCA needs only 73 components to explain the variance. \nLets fit and transform train and test data with these components\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split( x, y, test_size = 0.3, random_state = 0 )\n", "intent": "`train_test_split` Split the dataset into 70 percent training and 30 percent testing \n"}
{"snippet": "sc = StandardScaler()\nsc.fit(x_train)\nx_train_sd = sc.transform(x_train)\nx_test_sd  = sc.transform(x_test)\n", "intent": "`StandardScaler` use .fit and .transform the perform the feature scaling\n"}
{"snippet": "class_le = LabelEncoder()\ndf[\"classlabel\"] = class_le.fit_transform( df[\"classlabel\"] )\ndf\n", "intent": "Same functionality using sklearn. Note that `fit_transform` is a shortcut for calling fit and transform separately.\n"}
{"snippet": "df = pd.read_csv(\n    filepath_or_buffer = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n    header = None,\n    sep = ','\n)\ndf.columns = ['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\ndf.dropna( how = \"all\", inplace = True ) \ndf.tail()\n", "intent": "Warming up using the iris dataset as an example. [Link](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html) to the warm up.\n"}
{"snippet": "x = df.iloc[ :, 2: ].values\ny = df.iloc[ :, 1 ].values\nle = LabelEncoder()\ny = le.fit_transform(y)\n", "intent": "Extract the input and output features and label encode the output feature's class.\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split( x, y, test_size = 0.20, random_state = 1 )\n", "intent": "Split the full dataset into 80 training / 20 testing.\n"}
{"snippet": "iris = datasets.load_iris()\nX, y = iris.data[ 50:, [1, 2] ], iris.target[50:]\nle = LabelEncoder()\ny  = le.fit_transform(y)\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.5, random_state = 1 )\n", "intent": "Load iris data. Use only two of the features and work on classifying two classes.\n"}
{"snippet": "from sklearn.base import clone\nlablenc_ = LabelEncoder()\nlablenc_.fit(y)\nclasses_ = lablenc_.classes_ \nclassifiers_ = []\nfor clf in classifiers:\n    fitted_clf = clone(clf).fit( X, lablenc_.transform(y) )\n    classifiers_.append(fitted_clf)\n", "intent": "Loop through all the classfiers and fit all the models.\n"}
{"snippet": "file_dir = 'lastfm-dataset-360K'\nfile_path = os.path.join(file_dir, 'usersha1-artmbid-artname-plays.tsv')\nif not os.path.isdir(file_dir):\n    subprocess.call(['curl', '-O', 'http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-360K.tar.gz'])\n    subprocess.call(['tar', '-xvzf', 'lastfm-dataset-360K.tar.gz'])\ncol_names = ['user', 'artist', 'plays']\ndata = pd.read_csv(file_path, sep = '\\t', usecols = [0, 2, 3], names = col_names)\ndata = data.dropna(axis = 0)\nprint(data.shape)\ndata.head()\n", "intent": "http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-360K.html\n"}
{"snippet": "def make_comparison_dataframe(historical, forecast):\n    return forecast.set_index('ds')[['yhat', 'yhat_lower', 'yhat_upper']].join(historical.set_index('ds'))\n", "intent": "Lets combine Historic and Forecast data together\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint('X_train shape:', X_train.shape)\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0] , 'test samples')\n", "intent": "Now that we have a basic understanding of the definition of a ResNet, we will build one a train it on the MNIST dataset.\n"}
{"snippet": "target_size = 224, 224\nimg = image.load_img(img_path, target_size=target_size)\nimg = image.img_to_array(img)\nimg = np.expand_dims(img, axis=0)\nprint(img[0, 0, 0])\nimg = img[..., ::-1]\nprint(img[0, 0, 0])\n", "intent": "In case your curious, the following two code cells shows what's happening underneath the hood when we call resnet50's `preprocess_input` function.\n"}
{"snippet": "train = pd.read_csv('Dataset/Train.csv')\ntest  = pd.read_csv('Dataset/Test.csv')\ntrain['source'] = 'train' \ntest['source']  = 'test'\ndata = pd.concat( [train, test], ignore_index = True )\ndata.shape\n", "intent": "Load the training and testing data and combine them together for preprocessing.\n"}
{"snippet": "digits = load_digits()\ndigits.data.shape\n", "intent": "Now we load the classic handwritten digits [datasets](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\n"}
{"snippet": "print(\"total variance:{}\".format(np.sum(np.var(X,0))))\npca = PCA(2) \npca.fit(X) \nprint(\"variance explained via the first and second components:{}, {}\\n\".format(pca.explained_variance_[0],pca.explained_variance_[1]))\nprint(\"Transformation matrix:\\n{}\".format(pca.components_)) \n", "intent": "(in this particular example ~97.6% of the variance is preserved if we project down to the leading principal component)\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nspacing = np.linspace(0, max(df['gross']), 100)\nlabels = []\nlabels = [\"low\", \"low-mid\", \"high-mid\", \"high\"]\ndf['gross_group'] = pd.qcut(df['gross'], 4, labels=labels)\nrating_group = df['gross_group'].values\nrating_encoder = LabelEncoder()\nrating_df = pd.DataFrame(rating_encoder.fit_transform(rating_group), columns=['encoded_gross']).astype(str)\ndf = pd.concat([df, rating_df], axis=1)\n", "intent": "- add another label if I want to! And talk about it!\n"}
{"snippet": "labels = ['age', 'cholesterol', 'sex', 'death']\ninfarkty = pd.read_csv('exponea-MLworkshop/lecture3/infarkty.txt', sep=\"\\t\\t\", names=labels).drop(0)\ninfarkty.index -=1\ninfarkty.head(10)\n", "intent": "- age: hodnoty voci veku 50 rokov\n- cholesterol: hodnoty oproti 5 mmol/liter (ideal pre dospelaka)\n- sex: 0 muzi, 1 zeny\n- death: True/False\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\n", "intent": "http://scikit-learn.org/stable/modules/preprocessing.html\n"}
{"snippet": "college = pd.read_csv('College_Data', index_col = 0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n", "intent": "Separate our data into the main and test sets in the ratio 70/30.\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features,columns=KNN_data.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101 )\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "coeff_ecom = pd.DataFrame(lm.coef_, X.columns, columns = ['Coef'])\ncoeff_ecom\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "df = pd.read_csv('kyphosis.csv')\n", "intent": "Read in the kyphosis csv and check out the head of the dataframe\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head()\n", "intent": "**Create the new dataframe with scaled features**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(scaled_features,df['TARGET CLASS'],\n                                                    test_size=0.30)\n", "intent": "**Notice that we can tie the TARGET CLASS back into our train_test_split by passing it as an additional argument to our function below**\n"}
{"snippet": "h = pd.read_csv('USA_housing.csv')\n", "intent": "Create a new dataframe, h, which will store the USA_housing.csv file\n"}
{"snippet": "coeff_df = pd.DataFrame(lm.coef_, X.columns, columns = ['Coefficients'])\ncoeff_df\n", "intent": "**Create a dataframe of coefficients**\n"}
{"snippet": "train = pd.read_csv('titanic_train.csv')\n", "intent": "Let's start by reading in the titanic_train.csv file into a pandas dataframe\n"}
{"snippet": "df = pd.read_json(PATH_TO_DATA + '/Dataset for Detection of Cyber-Trolls.json', lines= True)\ndf.head()\n", "intent": "**Read the data and have a look at it**\n"}
{"snippet": "data = datasets.load_boston()\nprint data.DESCR\n", "intent": "**Load the boston housing data with the `datasets.load_boston()` function.**\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\npredictors=['CRIM','AGE','TAX']\nY=boston.target\nX_train,X_test,Y_train,Y_test=train_test_split(X[predictors],Y,train_size=0.9)\nlr=LinearRegression()\nlr.fit(X_train,Y_train)\nlr.fit(X_test,Y_test)\n", "intent": "- Score and plot.  \n- How do your metrics change?\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X[predictors], y, train_size=0.7, random_state=8)\nfrom sklearn.linear_model import LinearRegression\nlr2 = LinearRegression()\nlr2.fit(X_train,y_train)\nlr2.score(X_test, y_test)\n", "intent": "- Score and plot.  \n- How do your metrics change?\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50)\nknn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(X_train, y_train)\n", "intent": "**Split the data into 66% training and 33% testing. Fit a KNN classifier with `n_neighbors=25` on the training data.**\n"}
{"snippet": "hsq = pd.read_csv('/Users/Mahendra/Desktop/GA/hw/5.5.2_evaluation-classifiers_confusion_matrix_roc-lab/datasets/hsq_data.csv')\nhsq.head()\n", "intent": "It is worth reading over the description of the data columns above for this.\n"}
{"snippet": "df = pd.read_csv('/Users/Mahendra/desktop/GA/hw/6.1.2_optimization-feature_selection-lab/datasets/titanic_train.csv')\n", "intent": "We'll be working with the titanic datasets - go ahead and import it from the dataset folder (or query for it as described above). \n"}
{"snippet": "ss = StandardScaler()\nXs = ss.fit_transform(X)\n", "intent": "**10.C Standardize the predictor matrix.**\n"}
{"snippet": "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.5,random_state=12)\n", "intent": "**Split data into training and testing with 50% in testing.**\n"}
{"snippet": "sd = pd.read_csv('/Users/Indraja/Documents/Dsi/7.4.2_pca-intro-lab/datasets/speed_dating.csv')\nsd.head()\n", "intent": "---\n- Remove columns with over 200 missing values.\n- Then, remove rows with missing values.\n- Verify no rows contain NaNs.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size = 0.3, stratify = target, random_state = 31)\n", "intent": "**Split dataset on train and test**\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "First we'll load the famous *iris* dataset, dealing with plant classification:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n                                                    train_size=0.75, test_size=0.25)\n", "intent": "Now we split the data into training and testing:\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "For demonstration, we will use a boston housing dataset, which comes with scikit-learn:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n                                                    train_size=0.75, test_size=0.25)\n", "intent": "Now that we're familiar with the input data, we need to split it up for training and testing:\n"}
{"snippet": "boston = load_boston()\nX, y = shuffle(boston.data, boston.target, random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "Let's load the data and split into training and testing portions\n"}
{"snippet": "X, y = shuffle(new_boston[\"data\"], new_boston[\"target\"], random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "Now we can reassign the new data to training and testing\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"booksummaries.txt\", sep=\"\\t\")\n", "intent": "First we'll read in the .tsv file into a pandas dataframe:\n"}
{"snippet": "import pandas as pd\nvocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\nprint (vocab_frame.shape[0])\nprint (vocab_frame)\n", "intent": "Our data frame will map tokenized words to stemmed words, recalling our work with pandas in Day 3 of the introductory series:\n"}
{"snippet": "from sklearn.manifold import MDS\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state = 10)\npos = mds.fit_transform(dist)  \nxs, ys = pos[:, 0], pos[:, 1] \n", "intent": "Two dimensional scaling must be applied for plotting:\n"}
{"snippet": "num_na = df_person.columns[df_person.isna().any()].tolist()\ndf_person = df_person.fillna(df_person.median())\nna_count = df_person.isna().sum()\nna_count[na_count > 0]\n", "intent": "As for the rest of columns with NAs, those are numeric, and we will fill NAs there with medians.\n"}
{"snippet": "sstrain = StandardScaler()\nsstest = StandardScaler()\nlr_sep = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\nXtrain_n = sstrain.fit_transform(Xtrain)\nXtest_n = sstest.fit_transform(Xtest)\n", "intent": "**For the sake of example, standardize the Xtrain and Xtest separately and show that their normalization parameters differ.**\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=10)\nskl_pca = pca.fit_transform(X_2)\nskl_pca\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "X = StandardScaler().fit_transform(x)\n", "intent": "Standardize the data and compare at least one of the scatterplots for the scaled data to unscaled above.\n"}
{"snippet": "cvt      =  CountVectorizer(strip_accents='unicode', ngram_range=(1,1))\nX_all    =  cvt.fit_transform(insults_df['Comment'])\ncolumns  =  np.array(cvt.get_feature_names())          \nX_all\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "cvt = CountVectorizer(stop_words=\"english\", ngram_range=(2,4))\nX_all = cvt.fit_transform(insults_df['Comment'])\ncolumns  =  np.array(cvt.get_feature_names())\nfreq_words = get_freq_words(X_all, columns)\nfreq_words\n", "intent": "Display the top 75 ngrams with frequencies.\n"}
{"snippet": "cvt      =  CountVectorizer(strip_accents='unicode')\nX_all    =  cvt.fit_transform(insults_df['Comment'])\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "df = pd.read_csv('./datasets/breast_cancer_wisconsin/breast_cancer.csv', na_values='?')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "bin_features_list = [\"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\", \"CODE_GENDER\"]\nlabel_encoder = LabelEncoder()\nfor feature in bin_features_list:\n    label_encoder.fit(train_data[feature])\n    train_data[feature] = label_encoder.transform(train_data[feature])\n    test_data[feature] = label_encoder.transform(test_data[feature])\n", "intent": "Some of the categorical features have only two values so it seems reasonable to transform them into binary format instead of one-hot encoding.\n"}
{"snippet": "for feature in np.concatenate((house_related_features, enquiry_related_features, social_surrounding_features)):\n    print(\"Filling missing data for feature {}\".format(feature))\n    train_data_encoded[feature].fillna(train_data_encoded[feature].mean(), inplace=True)\n    test_data_encoded[feature].fillna(test_data_encoded[feature].mean(), inplace=True)\n", "intent": "It was decided to fill missing values of house-related, enquiry-related and social surrounding-related features with mean value of each feature.\n"}
{"snippet": "df = pd.read_csv('../../data/telecom_churn.csv')\n", "intent": "In the first article, we looked at the data on customer churn for a telecom operator. We will reload the same dataset into a `DataFrame`:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2, whiten=True).fit(X)\nX_pca = pca.transform(X)\n", "intent": "To simplify the analysis, and aid visualization, we will again perform a PCA to isolate the majority of the variation into two principal components.\n"}
{"snippet": "from sklearn.decomposition import PCA\nfrom sklearn import datasets\niris = datasets.load_iris()\npca = PCA(n_components=2, whiten=True).fit(iris.data)\nX_pca = pca.transform(iris.data)\ny = iris.target\n", "intent": "To demonstrate, we will implement a DP to cluster the iris dataset.\n"}
{"snippet": "normalizer = preprocessing.Normalizer().fit(X)\n", "intent": "As with scaling, there is also a `Normalizer` class that can be used to establish normalization with respect to a training set.\n"}
{"snippet": "lb = preprocessing.LabelBinarizer()\nlb.fit([1, 2, 6, 4, 2])\n", "intent": "`LabelBinarizer` is a utility class to help create a label indicator matrix from a list of multi-class labels:\n"}
{"snippet": "lb = preprocessing.MultiLabelBinarizer()\nlb.fit_transform([(1, 2), (3,)])\n", "intent": "For multiple labels per instance, use MultiLabelBinarizer:\n"}
{"snippet": "le = preprocessing.LabelEncoder()\nle.fit([1,2,2,6])\n", "intent": "`LabelEncoder` is a utility class to help normalize labels such that they contain only consecutive values between 0 and `n_classes-1`.\n"}
{"snippet": "mode_imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n", "intent": "In our educational outcomes dataset, we are probably better served using mode imputation:\n"}
{"snippet": "y = impute_subset.pop('mother_hs').values\nX = preprocessing.StandardScaler().fit_transform(impute_subset.astype(float))\n", "intent": "Next, we scale the predictor variables to range from 0 to 1, to improve the performance of the regression model.\n"}
{"snippet": "from sklearn import model_selection\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n        wine.values, grape.values, test_size=0.4, random_state=0)\n", "intent": "Implementing cross-validation on our wine SVC is straightforward:\n"}
{"snippet": "tsne = TSNE(random_state=17)\ntsne_repr = tsne.fit_transform(X_scaled)\n", "intent": "Now, let's build a t-SNE representation:\n"}
{"snippet": "test_X, test_y = datasets.make_moons(50, noise=0.20)\ntest_X = test_X.astype(np.float32)\ntest_y = test_y.astype(np.int32)\n", "intent": "Since we used the scikit-learn interface, its easy to take advantage of the `metrics` module to evaluate the MLP's performance.\n"}
{"snippet": "train = pd.read_csv(\"train_data.csv\")\nvalid = pd.read_csv(\"validation_data.csv\")\ntrain_x = train[\"x\"].tolist()\ntrain_y = train[\"y\"].tolist()\nvalid_x = valid[\"x\"].tolist()\nvalid_y = valid[\"y\"].tolist()\n", "intent": "Next, we evaluate each of these models on our data.  Fill in the parts of the function marked \"TODO\" below.\n"}
{"snippet": "import pandas as pd\ntrain = pd.read_csv(\"train_data.csv\")\nvalid = pd.read_csv(\"validation_data.csv\")\ntrain_x = train[\"x\"].tolist()\ntrain_y = train[\"y\"].tolist()\nvalid_x = valid[\"x\"].tolist()\nvalid_y = valid[\"y\"].tolist()\n", "intent": "Now we evaluate each of these techniques on our data.  Fill in the parts of the function marked \"TODO\" below\n"}
{"snippet": "cwd = os.getcwd()\ndatadir = '/'.join(cwd.split('/')[0:-1]) + '/data/'\nf = datadir + 'ads_dataset_cut.txt'\ndata = pd.read_csv(f, sep = '\\t')\ndata.columns, data.shape\n", "intent": "First we'll load the dataset and take a quick peak at its columns and size\n"}
{"snippet": "df_auc = pd.DataFrame(pd.Series(feature_auc_dict), columns = ['auc'])\ndf_mi = pd.DataFrame(pd.Series(feature_mi_dict), columns = ['mi'])\nfeat_imp_df = df_auc.merge(df_mi, left_index = True, right_index = True)\nfeat_imp_df\n", "intent": "Next we want to add both of the dictionaries created above into a data frame.\n"}
{"snippet": "import numpy as np\nres = pd.DataFrame(xval_dict)\nres['low'] = res['mu'] - 1.96*res['sig']/np.sqrt(10)\nres['up'] = res['mu'] + 1.96*res['sig']/np.sqrt(10)\n", "intent": "<p>We can now load the results from above into a dataframe and begin to analyze\n</p>\n"}
{"snippet": "X = data['text']\nY = data['spam']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=.75)\n", "intent": "Since we are going to do some modeling, we should split our data into a training and test set.\n"}
{"snippet": "df = pd.read_csv('moviebuzz.csv')\n", "intent": "Let us go ahead and take a look at the dataset\n"}
{"snippet": "pca_orig = PCA(n_components = 2).fit(original_imgs)\n", "intent": "Run PCA with 2 components on the original images\n"}
{"snippet": "data2 = pd.DataFrame({'Age':  [17, 64, 18, 20, 38, 49, 55, 25, 29, 31, 33], \n                      'Salary': [25, 80, 22, 36, 37, 59, 74, 70, 33, 102, 88], \n             'Loan Default': [1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1]})\ndata2\n", "intent": "Let's consider a more complex example by adding the \"Salary\" variable (in the thousands of dollars per year).\n"}
{"snippet": "pca_orig = PCA\npca_orig = PCA(n_components = 2).fit(original_imgs)\n", "intent": "Run PCA with 2 components on the original images\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXs_new = ss.fit_transform(X_new)\nXs_new1 = ss.fit_transform(X_new1)\n", "intent": "Note : I have taken top 30 features of logistic and obseved the accuracy levels.They don't seem considerably good when compared with top 20 features\n"}
{"snippet": "pd.DataFrame([pca_results[\"Explained Variance\"], pca_results[\"Explained Variance\"].cumsum().rename(\"Cum. sum\")])                                                                                       \n", "intent": "To answer the question I'll print variances and their cummulated sum below:\n"}
{"snippet": "df_sample = pd.DataFrame(features).join(income_raw).sample(n=2500, random_state=0)\nincome_sampe = df_sample['income']\nfeatures_sample = df_sample.drop('income', axis = 1)\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, random_state=0, method='exact')\nX_2D_porjection = tsne.fit_transform(features_sample) \n", "intent": "As we can see we have significat correlation between several features. Let's also build 2D projection of data sample:\n"}
{"snippet": "df3 = pd.read_csv('/home/cneiderer/Metis/Neiderer_Metis/Challenges/challenges_data/2013_movies.csv',\n                 header='infer')\n", "intent": "For your movie classifiers, calculate the precision and recall for each class.\n"}
{"snippet": "cols = ['Age', 'Year', 'NumNodes', 'Survived']\ndf4 = pd.read_csv('/home/cneiderer/Metis/Neiderer_Metis/Challenges/challenges_data/haberman.data', \n                  header=0, names=cols)\n", "intent": "Draw the ROC curve (and calculate AUC) for the logistic regression classifier from challenge 12.\n"}
{"snippet": "trans_fitted = scaler.fit_transform(df.drop('Class', axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "df_fitted = pd.DataFrame(trans_fitted, columns=df.columns[:-1])\ndf_fitted.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "** Use SciKit Learn to create training and testing sets of the data as we've done in previous lectures:**\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nknn_pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\nknn_params = {'knn__n_neighbors': range(1, 10)}\nknn_grid = GridSearchCV(knn_pipe, knn_params,\n                        cv=5, n_jobs=-1, verbose=True)\nknn_grid.fit(X_train, y_train)\nknn_grid.best_params_, knn_grid.best_score_\n", "intent": "Now, let's tune the number of neighbors $k$ for k-NN:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nX = cv.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))\n    print(df)\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "pivoted2=pd.pivot_table(mpg, values=['cty', 'hwy'], index='year')\npivoted2\n", "intent": "3c. Has the average city mileage improved from 1999 to 2008?   Has the average highway mileage improved from 1999 to 2008?\n"}
{"snippet": "pd.pivot_table(iris, values=['petal_width'], index='species')\n", "intent": "4c. Compute the average petal width for each of the \"species\"-categories.\n"}
{"snippet": "sc = MinMaxScaler(feature_range=(0,1))\ntraining_set_scaled = sc.fit_transform(training_set)\n", "intent": "fit -> get min and max of training set, such that x_norm = (x - min(x))/(max(x) - min(x))\n"}
{"snippet": "residuals = pd.DataFrame(model_fit.resid)\nresiduals.plot();\n", "intent": "First, we get a line plot of the residual errors, suggesting that there may still be some trend information not captured by the model.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nprint '\\n The object \"iris\" here is of type:', type(iris)\nprint '\\n Keys of the bunch:', iris.keys()\n", "intent": "---\n* **Packaged Data:** small datasets packaged with `scikit-learn`, can be downloaded using \n    * ``sklearn.datasets.load_*``\n"}
{"snippet": "cv = CountVectorizer()\ncv.fit(text_train)\nlen(cv.vocabulary_)\n", "intent": "**First, we will create a dictionary of all the words using CountVectorizer**\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\nprint digits.keys()\nprint digits.images.shape\nprint digits.data.shape\n", "intent": "<img src='http://myselph.de/mnistExamples.png'>\n"}
{"snippet": "pd.concat([pd.DataFrame(df_1['parameters'].tolist()),\n           df_1['mean_validation_score']], axis=1).sort_values('mean_validation_score', ascending=False).iloc[:10, :]\n", "intent": "- Tradeoff Complexity vs. Generalizability\n"}
{"snippet": "from sklearn.datasets import make_classification\nX, y = make_classification(1000, n_features=20, n_informative=2, \n                           n_redundant=2, n_classes=2, random_state=0)\nfrom pandas import DataFrame\ndf = DataFrame(np.hstack((X, y[:, None])), \n               columns = range(20) + [\"class\"])\n", "intent": "We will generate some simple toy data using [sklearn](http://scikit-learn.org)'s `make_classification` function:\n"}
{"snippet": "from sklearn.datasets import make_circles\nX, y = make_circles(n_samples=1000, random_state=2)\n", "intent": "We generate another dataset for binary classification and apply a `LinearSVC` again.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits(n_class=6)\nX = digits.data\ny = digits.target\nn_samples, n_features = X.shape\nprint \"Dataset consist of %d samples with %d features each\" % (n_samples, n_features)\n", "intent": "Now for one of the classic datasets used in machine learning, which deals with optical digit recognition: \n"}
{"snippet": "from sklearn import (manifold, decomposition, random_projection)\nrp = random_projection.SparseRandomProjection(n_components=2, random_state=42)\nstime = time.time()\nX_projected = rp.fit_transform(X)\nplot_embedding(X_projected, \"Random Projection of the digits (time: %.3fs)\" % (time.time() - stime))\n", "intent": "Already a random projection of the data to two dimensions gives a not too bad impression:\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42, n_samples=1000, centers=3, cluster_std=1.5)\nX.shape\n", "intent": "Let's start of with a very simple and obvious example:\n"}
{"snippet": "sent = [\"The grass is green, and sometimes it feels nice to run barefoot.\",\n       \"The sky is blue but sometimes the fog doesn't let you see it.\"]\nvect = CountVectorizer()\nvect.fit(sent)\n", "intent": "[CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape\n", "intent": "- `**lowercase:**` boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "data = pd.read_csv('../../data/telecom_churn.csv').drop('State', axis=1)\ndata['International plan'] = data['International plan'].map({'Yes': 1, 'No': 0})\ndata['Voice mail plan'] = data['Voice mail plan'].map({'Yes': 1, 'No': 0})\ny = data['Churn'].astype('int').values\nX = data.drop('Churn', axis=1).values\n", "intent": "We will work our data on customer churn of telecom operator.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(newsgroup.data, \n                                                    newsgroup.target, \n                                                    train_size=0.8, \n                                                    random_state=42)\n", "intent": "In scikit-learn, a Train-Test split is simply done with the function train_test_split from the cross_validation package.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nlabel_dict = dict()\nfor col in disc_cols:\n    le = LabelEncoder()\n    df_data[col] = le.fit_transform(df_data[col])\n    label_dict[col] = dict()\n    for cls, label in zip(le.classes_, le.transform(le.classes_)):\n        label_dict[col][label] = cls\n", "intent": "(sklearn classifiers do not take string values as input)\n"}
{"snippet": "cat_encoder = OneHotEncoder(sparse=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot\n", "intent": "Alternatively, you can set `sparse=False` when creating the `OneHotEncoder`:\n"}
{"snippet": "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n                                        index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.most_frequent_)\n", "intent": "We will also need an imputer for the string categorical columns (the regular `Imputer` does not work on those):\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\nX_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\nX_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\ny_train = y_train.astype(np.int32)\ny_test = y_test.astype(np.int32)\nX_valid, X_train = X_train[:5000], X_train[5000:]\ny_valid, y_train = y_train[:5000], y_train[5000:]\n", "intent": "**Warning**: `tf.examples.tutorials.mnist` is deprecated. We will use `tf.keras.datasets.mnist` instead.\n"}
{"snippet": "california_housing_dataframe = pd.read_csv(\"https://storage.googleapis.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\n", "intent": "Next, we'll load our data set.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'])\n", "intent": "Here we're splitting the data into two sets, test and train. We'll create the model with train and then see how well it works with the test model.\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n", "intent": "Fashion MNIST data is being built into ``from keras.datasets`` module.\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n", "intent": "Fashion MNIST data is being built into ``from keras.datasets`` module.\n"}
{"snippet": "boston = load_boston()\nX, y = boston['data'], boston['target']\n", "intent": "**We will work with Boston house prices data (UCI repository).**\n**Download the data.**\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n", "intent": "MNIST data is being built into ``from keras.datasets`` module.\n"}
{"snippet": "df_combined[\"Cabin\"].fillna(\"U\", inplace=True)\nprint(\"Number of Nan values in Cabin column: {}\".format(df_combined[\"Cabin\"].isnull().sum()))\n", "intent": "We will leave first letter of Cabin as type and repleace all Nan values with \"U\" which means unknown.\n"}
{"snippet": "df_combined[\"Embarked\"].fillna(\"S\", inplace=True)\nprint(\"Number of Nan values in Embarked column: {}\".format(df_combined[\"Embarked\"].isnull().sum()))\n", "intent": "Fill embarked values by most recent appearing embarkment\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    X_data, y_data, test_size=0.2, random_state=RANDOM_SEED)\n", "intent": "Podzielmy dane na zbiory: treningowy oraz testowy.\n"}
{"snippet": "train_content = pd.read_table(os.path.join(PATH_TO_DATA,'train_content.txt'), sep='\\n', header=None)\ntest_content = pd.read_table(os.path.join(PATH_TO_DATA,'test_content.txt'), sep='\\n', header=None)\n", "intent": "***Tf-Idf with article content ***\n"}
{"snippet": "train_title = pd.read_table(os.path.join(PATH_TO_DATA,'train_title.txt'), sep='\\n', header=None)\ntest_title = pd.read_table(os.path.join(PATH_TO_DATA,'test_title.txt'), sep='\\n', header=None)\n", "intent": "***Tf-Idf with article titles***\n"}
{"snippet": "train_author = pd.read_table(os.path.join(PATH_TO_DATA,'train_author.txt'), sep='\\n', header=None).\\\n                applymap(lambda x: eval(x))\ntest_author = pd.read_table(os.path.join(PATH_TO_DATA,'test_author.txt'), sep='\\n', header=None).\\\n                applymap(lambda x: eval(x))\n", "intent": "***Bag of authors (i.e. One-Hot-Encoded author names)***\n"}
{"snippet": "train_published = pd.read_table(os.path.join(PATH_TO_DATA,'train_published.txt'), sep='\\n', header=None).\\\n                applymap(lambda x: eval(x))\ntest_published = pd.read_table(os.path.join(PATH_TO_DATA,'test_published.txt'), sep='\\n', header=None).\\\n                applymap(lambda x: eval(x))\n", "intent": "*** Time features***\n"}
{"snippet": "train_target = pd.read_csv(os.path.join(PATH_TO_DATA,'train_log1p_recommends.csv'), \n                           index_col='id')\ny_train = train_target['log_recommends'].values\ny_train = y_train[idx]\n", "intent": "**Read train target and split data for validation.**\n"}
{"snippet": "data = np.array([1, 1, 0, -1, 2, 1, 2, 3, -2, 4, 100]).reshape(-1, 1).astype(np.float64)\nStandardScaler().fit_transform(data)\n", "intent": "But, to some extent, it protects against outliers:\n"}
{"snippet": "train_df = pd.read_csv('../../data/train_sessions.csv',\n                       index_col='session_id')\ntest_df = pd.read_csv('../../data/test_sessions.csv',\n                      index_col='session_id')\ntimes = ['time%s' % i for i in range(1, 11)]\ntrain_df[times] = train_df[times].apply(pd.to_datetime).fillna(method='ffill', axis=1)\ntest_df[times] = test_df[times].apply(pd.to_datetime).fillna(method='ffill', axis=1)\ntrain_df = train_df.sort_values(by='time1')\ntrain_df.head()\n", "intent": "Reading original data\n"}
{"snippet": "new_feat_train = pd.DataFrame(index=train_df.index)\nnew_feat_test = pd.DataFrame(index=test_df.index) \nnew_feat_train['year_month'] = train_df['time1'].apply(lambda ts: ts.year * 100 + ts.month)\nnew_feat_test['year_month'] = test_df['time1'].apply(lambda ts: ts.year * 100 + ts.month)\n", "intent": "Add features based on the session start time: hour, whether it's morning, day or night and so on.\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimr = Imputer(missing_values='NaN', strategy='mean', axis=0)\nimr = imr.fit(x)\nx = imr.transform(x.values)\nx.shape\nx.mean()\n", "intent": "Read the page 102 of the book \"Python Machine Learning Unlock deeper insights ...\" (Raschka2015).\n"}
{"snippet": "import scipy\nfrom sklearn.preprocessing import scale\nx_s = scale(x,with_mean=True,with_std=True,axis=0)\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nx_s2 = mms.fit_transform(x)\nfrom sklearn.preprocessing import StandardScaler\nstdsc = StandardScaler()\nx_s3 = stdsc.fit_transform(x)\nprint(np.max(x_s2), np.max(x_s3), np.max(x_s2 - x_s3))\n", "intent": "Transforming all features to the same scale.\n"}
{"snippet": "seed_X = preprocessing.StandardScaler().fit_transform(seeds.iloc[:,:-1])\nseed_y = seeds.variety\nseed_pca = decomposition.PCA(n_components=2)\nseed_res = seed_pca.fit_transform(seed_X)\nseed_pca_rf = ensemble.RandomForestClassifier(random_state=42)\nsp_X_train, sp_X_test, sp_y_train, sp_y_test = model_selection.\\\n    train_test_split(seed_res, seed_y, test_size=.3, random_state=42)\nseed_pca_rf.fit(sp_X_train, sp_y_train)\nseed_pca_rf.score(sp_X_test, sp_y_test)\n", "intent": "* Run a classification on PCA'd data\n* How does it perform versus the raw data?\n"}
{"snippet": "data_scaled = pd.DataFrame(data=X, columns=data.drop('Class', axis=1).columns)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df_scaled = pd.DataFrame(data=X, columns=df.drop(['TARGET CLASS'], axis=1).columns)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "X = count_vectorizer.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "from sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nx_data_kbest = SelectKBest(f_classif, k=5).fit_transform(x_data_generated, y_data_generated)\nx_data_varth = VarianceThreshold(.9).fit_transform(x_data_generated)\n", "intent": "There are other ways that are also [based on classical statistics](http://scikit-learn.org/stable/modules/feature_selection.html\n"}
{"snippet": "in_words = \"alice's\"\nfor _ in range(maxlen):\n    in_sequence = sequence.pad_sequences(tokenizer.texts_to_sequences([in_words]), maxlen=maxlen)\n    wordid = model.predict_classes(in_sequence, verbose=0)[0]\n    for k, v in tokenizer.word_index.items():\n        if v == wordid:\n            in_words += \" \" + k\n            break\nprint(in_words)\n", "intent": "Text generation based on the maximum likelihood of the connected words.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"DIMSIM.csv\")\npd.options.display.max_columns = 999\n", "intent": "+ Duct Tape, or...\n+ WD-40\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = datasets.load_iris()\n", "intent": "+ Graph Decision Tree for Iris Dataset\n"}
{"snippet": "print(\"Extracting tf features for LDA...\")\ntf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n                                max_features=10000,\n                                stop_words='english')\nt0 = time()\ntf_vectorizer.fit(sentencelist)\nX = tf_vectorizer.transform(sentencelist)\nprint(\"done in %0.3fs.\" % (time() - t0))\n", "intent": "+ Here is the count vectorizer\n"}
{"snippet": "of_df = pd.read_csv(\"old-faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "import os\nimport pandas as pd\nimport matplotlib as mpl\nimport numpy as np\ndf = pd.read_csv('/Users/jamiew/GA-DataScience/GA-MyDSRepo/lesson-18/chipotle.tsv', sep='\\t')\n", "intent": "+ hint - examine how the values are separated \n+ What's the difference between a tsv and csv?\n"}
{"snippet": "import re\ndf['choice_description'] = df['choice_description'].fillna(\"[none]\").apply(lambda x: re.findall(r\"[\\w']+\",x))\n", "intent": "**Reid's Solution**\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"chipotle.tsv\", sep=\"\\t\")\n", "intent": "+ hint - examine how the values are separated \n+ What's the difference between a tsv and csv?\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"/Users/jamiew/GA-DataScience/GA-MyDSRepo/lesson-7/2008.csv\").fillna(0)\n", "intent": "http://stat-computing.org/dataexpo/2009/the-data.html\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nlfw_people = datasets.fetch_lfw_people(min_faces_per_person=50, \n                resize=0.4, data_home='../../data/faces')\nprint('%d objects, %d features, %d classes' % (lfw_people.data.shape[0],\n      lfw_people.data.shape[1], len(lfw_people.target_names)))\nprint('\\nPersons:')\nfor name in lfw_people.target_names:\n    print(name)\n", "intent": "PCA often serves as a preprocessing technique. Let's take a look at a face recognition task \n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nonehot = OneHotEncoder(categorical_features = \"all\", sparse=False) \nX =onehot.fit(df2[['UniqueCarrier', 'TailNum']])\nX =onehot.transform(df2[['UniqueCarrier', 'TailNum']])\n", "intent": "+ BUT YOU NEED TO SPECIFY WHICH VARIABLES ARE CATEGORICAL\n+ Could we use \"integer\" type as a proxy for that?\n"}
{"snippet": "from sklearn.preprocessing import  LabelEncoder\nle = LabelEncoder()\nle.fit(df['UniqueCarrier'].head(10))\nle.transform(df['UniqueCarrier'].head(10))\n", "intent": "+ THESE ARE NOT DUMMY VARIABLES\n+ Note the fit vs fit_transform function\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nonehot = OneHotEncoder(categorical_features = \"????\", sparse=True)\nX =onehot.fit_transform(df)\n", "intent": "+ BUT YOU NEED TO SPECIFY WHICH VARIABLES ARE CATEGORICAL\n+ Could we use \"integer\" type as a proxy for that?\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"/Users/jamiew/GA-DataScience/GA-MyDSRepo/lesson-7/2008.csv\").fillna(\"unk\")\n", "intent": "http://stat-computing.org/dataexpo/2009/the-data.html\n"}
{"snippet": "from sklearn.preprocessing import  LabelEncoder\nle = LabelEncoder()\nle.fit(df2['TailNum'])\nle.transform(df2['TailNum'])\n", "intent": "+ THESE ARE NOT DUMMY VARIABLES\n+ Note the fit vs fit_transform function\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nonehot = OneHotEncoder(categorical_features = is_cat_list, sparse=False)\nonehot.fit(df2)\nX =onehot.transform(df2)\n", "intent": "+ BUT YOU NEED TO SPECIFY WHICH VARIABLES ARE CATEGORICAL\n+ Could we use \"integer\" type as a proxy for that?\n"}
{"snippet": "from sklearn import datasets, neighbors, metrics\nimport pandas as pd\nimport seaborn as sns\niris = datasets.load_iris()\nirisdf = pd.DataFrame(iris.data, columns=iris.feature_names)\nirisdf['target'] = iris.target\nirisdf.head()\n", "intent": "+ Today we'll be working with the Iris dataset\n+ First we'll explore\n+ Then we'll build a terrible classifer\n+ Then we'll build a KNN classifer!\n"}
{"snippet": "data_100 = PCA(n_components=100).fit_transform(data.astype(float32) / 255)\nembeddings = TSNE(init=\"pca\", random_state=777, verbose=2).fit_transform(data_100)\nembeddings -= embeddings.min(axis=0)\nembeddings /= embeddings.max(axis=0)\n", "intent": "PCA them to 100 dimensions and run t-SNE. Normalize the results to [0, 1].\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\nX_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\ny_train = y_train.astype(np.int32)\n", "intent": "Let's try it on MNIST:\n"}
{"snippet": "print(\"Extracting features from the training dataset using a sparse vectorizer\")\nvectorizer = TfidfVectorizer(max_df=0.5, max_features=1000,\n                             min_df=2, stop_words='english')\nX = vectorizer.fit_transform(dataset.data)\nprint(\"n_samples: %d, n_features: %d\" % X.shape)\n", "intent": "**Build Tf-Idf features for texts**\n"}
{"snippet": "tracks.to_csv('../data/fma_tracks_with_genre.csv')\n", "intent": "To avoid having to collect the data everytime you restart the IPython kernel, save the DataFrame as a CSV file.\n"}
{"snippet": "tracks = pd.read_csv('../data/fma_tracks_with_genre.csv', index_col=0)\n", "intent": "You can now load it back with the following call instead of running the code in sections 1.1 to 1.3.\n"}
{"snippet": "str_dataName = './data/bdb_cleaned2.csv'\nbeers.to_csv(str_dataName)\n", "intent": "After replacing the NaN and discard the beers where the NaN can not be replaced, the data is saved in one csv file.\n"}
{"snippet": "NEIGHBORS = 80 \nRawCatweights = GetWeights(CatDistances,NEIGHBORS)\nfeatures = pd.DataFrame()\nfor df in newFrames:\n    features[df.columns] = df[df.columns]\n", "intent": "Here we use kNN method to sparsify the distance matrix. The our algortihm is extremly sensitive to the vlaue of this parameter\n"}
{"snippet": "all_movies_new =     pd.read_csv('350000-movies/AllMoviesDetailsCleaned.csv', sep=';', encoding='utf-8', low_memory=False,\n                                 error_bad_lines=False)\nall_movies_casting = pd.read_csv('350000-movies/AllMoviesCastingRaw.csv', sep=';', encoding='utf-8', low_memory=False,\n                                 error_bad_lines=False)\n", "intent": "Merge the information from two separate datasets\n"}
{"snippet": "metacritic_ratings = pd.read_csv('Saved_Datasets/metacritic_ratings.csv')\nlen(metacritic_ratings)\n", "intent": "Merge with csv created in Get_Metacritic_ratings\n"}
{"snippet": "df = pd.read_csv('Saved_Datasets/CleanDataset.csv')\n", "intent": "The final, merged dataset is then displayed below:\n"}
{"snippet": "df_ten = pd.read_csv('Saved_Datasets/NewFeaturesDataset.csv')\n", "intent": "These tenures were then added to the main dataset and can be seen in the column tenure of the following dataset:\n"}
{"snippet": "labels = preprocessing.LabelEncoder().fit_transform(df['success'])\nG_budg.set_coordinates(G_budg.U[:,1:3])\n", "intent": "From the graph of the eigenvalues, it can be seen that the first eigenvector explain 90% of the data.\n"}
{"snippet": "digits = datasets.load_digits()\nX = digits.data\ny = digits.target\n", "intent": "Let's look at the handwritten numbers dataset that we used before in the [3rd lesson](https://habrahabr.ru/company/ods/blog/322534/\n"}
{"snippet": "labels = preprocessing.LabelEncoder().fit_transform(df['success'])\nG_act_ten_spars.set_coordinates(G_act_ten_spars.U[:,1:3])\nG_act_ten_spars.plot_signal(labels, vertex_size=20)\n", "intent": "By looking at the eigenvalues, it can be seen than when the weight matrix is sparsed the first eigenvalue doesn't describe the data well. \n"}
{"snippet": "labels_reg = preprocessing.LabelEncoder().fit_transform(df['ROI']/2.64)\nG.plot_signal(labels_reg, vertex_size=20)\n", "intent": "In this second case, the graph is embedded on the first two eigenvectors with the their signals being the ROI.\n"}
{"snippet": "Genres = pd.DataFrame(genreArray, columns=Diffgenres)\nGenres.head(10)\n", "intent": "Observe the result in the dataframe\n"}
{"snippet": "NbGenre = pd.DataFrame(freqGenre, columns=Diffgenres)\nNbGenre\n", "intent": "Display of the number of times a genre appears in the dataframe\n"}
{"snippet": "ranking = np.linspace(1, len(Diffgenres)-1, num=len(Diffgenres)-1, endpoint=True, retstep=False, dtype=int)\nRankdf = pd.DataFrame(assosRank, index=ranking)\nRankdf\n", "intent": "Display of the ranking of the other genres with which each genre is most often associated\n"}
{"snippet": "street_names= {\"name\":df.name.unique()}\ndfp_features = pd.DataFrame(street_names)\ndfp_features.index.name='Progressive_index'\n", "intent": "To store all the features, we firstly initialize a \"classical\" Pandas Dataframe with the Street Names\n"}
{"snippet": "candidate_pca = decomposition.PCA().fit(can_answers)\ncandidate_features_pca = candidate_pca.transform(can_answers)\ncan_party_label = preprocessing.LabelEncoder().fit_transform(can_labels_true)\n", "intent": "Subsequently the actual PCA is performed on the candidate dataset\n"}
{"snippet": "exploitation = pd.read_csv('attacks.csv', index_col=0)\nexploitation.head()\n", "intent": "We will drop features that might not give insightful information for the next analysis.\n"}
{"snippet": "exploitation = pd.read_csv('attacks.csv', index_col=0)\n", "intent": "Again, we will drop features that might not give insightful information for the next analysis. \n"}
{"snippet": "df = pd.read_csv(os.path.join(PATH_TO_ALL_DATA, 'bank_train.csv'))\nlabels = pd.read_csv(os.path.join(PATH_TO_ALL_DATA,\n                                  'bank_train_target.csv'), \n                     header=None)\ndf.head()\n", "intent": "Let's explore the [UCI bank marketing dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing) where most of  features are categorial.\n"}
{"snippet": "boston = load_boston()\n", "intent": "Boston dataset is extremely common in machine learning experiments thus it is embedded in sklearn.\n"}
{"snippet": "X = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = boston.target\n", "intent": "Create pandas dataframe with objects in rows and features in columns\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "Let's split our data to train and test set in fraction of $\\frac{4}{1}$\n"}
{"snippet": "scaler = MinMaxScaler()\n", "intent": "Let's also do normalization to the range of $(0; 1)$ to make our data insensitive to the scale of features\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "Split into train and test set (with the same $\\text{random_state}$ which means we can compare results)\n"}
{"snippet": "boston = load_boston()\n", "intent": "Load Boston house prices dataset\n"}
{"snippet": "(X_train, y_train), (X_valid, y_valid) = mnist.load_data()\nprint(len(X_train), 'train samples')\nprint(len(X_valid), 'validation samples')\n", "intent": "Let's download MNIST dataset. There is a special function in Keras for that purpose (because MNIST is extremely popular)\n"}
{"snippet": "iris = load_iris()\n", "intent": "Iris dataset is extremely common in machine learning experiments thus it is embedded in sklearn.\n"}
{"snippet": "X = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n", "intent": "Create pandas dataframe with objects in rows and features in columns\n"}
{"snippet": "categorical_columns = df.columns[df.dtypes \n                                 == 'object'].union(['education'])\nfor column in categorical_columns:\n    df[column] = label_encoder.fit_transform(df[column])\ndf.head()\n", "intent": "Let's apply the transformation to other columns of type `object`.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X.values, label_binarize(y, classes=[0, 1, 2]), \n                                                    test_size=0.2, random_state=42)\n", "intent": "Split into train and test set\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint(len(X_train), \"train samples\")\nprint(len(X_test), \"test samples\")\n", "intent": "Let\"s download MNIST dataset. There is a special function in Keras for that purpose (because MNIST is extremely popular)\n"}
{"snippet": "import os\nprint(os.path.abspath(\".\"))\nmodel = model_from_json(open(\"mnist_cnn.json\").read())\n", "intent": "Load model architecture\n"}
{"snippet": "tsne = TSNE(perplexity=30, n_components=2, init=\"pca\", n_iter=3000)\nemb = tsne.fit_transform(reps)\n", "intent": "Fit our tSNE mapping to the `reps` data vectors\n"}
{"snippet": "tsne = TSNE(perplexity=30, n_components=2, init=\"pca\", n_iter=3000)\nemb = tsne.fit_transform(reps)\n", "intent": "Create a tSNE plot with a deep fully-connected layer. Fit our tSNE mapping to the `reps` data vectors\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(pima.iloc[:, :8], pima.iloc[:, 8], test_size=0.30, random_state=42)\n", "intent": "Create Training and testing split\n"}
{"snippet": "results = pd.DataFrame(columns=[\"Model\", \"Masking\", \"Embedding\", \"Accuracy train, %\", \"Accuracy test, %\", \"Train Time, s\"])\nresults\n", "intent": "Table of results for Reuters expetiments\n"}
{"snippet": "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nprint(len(X_train), 'train sequences')\nprint(len(X_valid), 'test sequences')\n", "intent": "Train/Test split in *stratified* (preserves the distribution across classes) manner in fraction of $4/1$.\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint(len(X_train), 'train samples')\nprint(len(X_test), 'test samples')\n", "intent": "Let's download MNIST dataset. There is a special function in Keras for that purpose (because MNIST is extremely popular)\n"}
{"snippet": "train_documents, test_documents, train_labels_mult, test_labels_mult = \\\n    train_test_split(all_documents, all_targets_mult, random_state=7)\nwith open(os.path.join(PATH_TO_ALL_DATA, \n                       '20news_train_mult.vw'), 'w') as vw_train_data:\n    for text, target in zip(train_documents, train_labels_mult):\n        vw_train_data.write(to_vw_format(text, target))\nwith open(os.path.join(PATH_TO_ALL_DATA, \n                       '20news_test_mult.vw'), 'w') as vw_test_data:\n    for text in test_documents:\n        vw_test_data.write(to_vw_format(text))\n", "intent": "**The data is the same, but we have changed the labels, train_labels_mult and test_labels_mult, into label vectors from 1 to 20.**\n"}
{"snippet": "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=21)\n", "intent": "Let's split our data into train and validation in fraction of $\\frac{1}{4}$.\n"}
{"snippet": "hist_full = pd.DataFrame(np.array([hist.history[\"loss\"], \n                                   hist.history[\"val_loss\"], \n                                   cb.ed_hist, \n                                   cb.norm_ed_hist]).T, \n                         columns=[\"Train CTC Loss\", \n                                  \"Test CTC Loss\", \n                                  \"Test Edit Distance\", \n                                  \"Test Normalized Edit Distance\"], \n                         index=np.arange(1, setting.epochs + 1))\n", "intent": "Let's also save the history of the training (both CTC loss and Edit Distance loss) for further analysis\n"}
{"snippet": "results = pd.DataFrame(columns=[\"RMSE\", \"MAE\", \"MAPE\"])\n", "intent": "We're going to use three types of models:\n* Recurrent (LSTM)\n* Multi-Layer Perceptron (Dense)\n* Convolutional\n"}
{"snippet": "data_train, data_test, ans_train, ans_test = train_test_split(data, ans, test_size=0.2)\n", "intent": "Splitting data into train and test in the fraction of $\\frac{4}{1}$\n"}
{"snippet": "boston = load_boston()\nX = boston.data\ny = boston.target\nX.shape, y.shape\n", "intent": "We will use the Boston house prices, available with scikit-learn, as an example.\n"}
{"snippet": "series = pd.read_csv('internet-traffic-data-in-bits-fr.csv', header = None, index_col=0, squeeze = True, \n                     parse_dates = True, dtype=\"float64\")\nprint(len(series))\nprint(series.head())\n", "intent": "We will analyze time series data downloaded from https://datamarket.com/data/set/232j. \nThese are internet traffic data collected on an hourly basis.\n"}
{"snippet": "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna('C')\n", "intent": "We can see that for 1st class median line is coming around fare $80 for embarked value 'C'. So we can replace NA values in Embarked column with 'C'\n"}
{"snippet": "cumul_sales = cumul_sales.set_index(\n    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(\n        level=-1).fillna(0)\ncumul_sales.columns = cumul_sales.columns.get_level_values(1)\ncumul_sales.shape\n", "intent": "Ah... they're creating a multi-task learning problem\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\ndf['Weight_mms'] = mms.fit_transform(df[['Weight']])\ndf['Height_mms'] = mms.fit_transform(df[['Height']])\ndf.describe().round(2)\n", "intent": "After scaling min = 0, max = 1\n$$x_{mms}(i) = \\frac{x(i) - min(X)}{max(X) - min(X)}$$\n"}
{"snippet": "ads = pd.read_csv('../../data/ads.csv', index_col=['Time'], parse_dates=['Time'])\ncurrency = pd.read_csv('../../data/currency.csv', index_col=['Time'], parse_dates=['Time'])\n", "intent": "As an example, let's look at real mobile game data. Specifically, we will look into ads watched per hour and in-game currency spend per day:\n"}
{"snippet": "train = pd.read_csv('r8-train-all-terms.txt', header=None, sep='\\t')\ntest = pd.read_csv('r8-test-all-terms.txt', header=None, sep='\\t')\ntrain.columns = ['label', 'content']\ntest.columns = ['label', 'content']\n", "intent": "https://www.cs.umb.edu/~smimarog/textmining/datasets/\n"}
{"snippet": "data = pd.read_csv('pima-indians-diabetes.csv')\ndisplay(data.info())\ndisplay(data.head())\ndisplay(data.describe())\ndisplay(data['Class'].value_counts())\n", "intent": "https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes\n"}
{"snippet": "from keras.preprocessing.sequence import pad_sequences\nmaxlen = 20\nX_train = pad_sequences(sentences_trained, padding='post', maxlen=maxlen)\ndisplay(X_train.shape)\ndisplay(sentences_trained[0])\ndisplay(X_train[0])\n", "intent": "text sequence has in most cases different length of words\n    -> pads the sequence of words with zeros\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=23)\nprint(\"Validation Set: {} samples\".format(len(X_validation)))\n", "intent": "validation_file = 'valid.p'\nwith open(validation_file, mode='rb') as f:\n    valid = pickle.load(f)\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(faces.data,\n        faces.target, random_state=0)\nprint(X_train.shape, X_test.shape)\n", "intent": "We'll perform a Support Vector classification of the images.  We'll\ndo a typical train-test split on the images to make this happen:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y_cls, random_state=0)\n", "intent": "<img src=\"./images/02-knns.png\" alt=\"Drawing\" style=\"width: 1500px;\"/>\n"}
{"snippet": "from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\nprint(\"Size of training set: {} size of test set: {}\".format(X_train.shape[0], X_test.shape[0]))\n", "intent": "Using *for* loops over the parameters of a model.\n"}
{"snippet": "import pandas as pd\nfrom IPython.display import display\nresults = pd.DataFrame(grid_search.cv_results_)\ndisplay(results.head())\n", "intent": "We can inspect the results of the cross-validated grid search.\n"}
{"snippet": "from sklearn.pipeline import Pipeline\npipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\npipe.fit(X_train, y_train)\nprint(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))\n", "intent": "You can create a *workflow* for training and classification after preprocessing.\n"}
{"snippet": "df = pd.read_csv('../../data/medium_posts.csv.zip', sep='\\t')\n", "intent": "We will predict the daily number of posts published on [Medium](https://medium.com/).\nFirst, we load our dataset.\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\nlabelencoder = LabelEncoder()\nX[:,3]\n", "intent": "hadndle categorical data: apply labelencoder and then onehotencoder\n"}
{"snippet": "cancer = datasets.load_breast_cancer()\nX = cancer.data\ny = cancer.target\nnp.random.seed(seed=133) \nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\n", "intent": "\"Model the probability that some class occurs\"\n"}
{"snippet": "pca = PCA(2)  \nXproj = pca.fit_transform(X)\nprint(X.shape)\nprint(Xproj.shape)\n", "intent": "Now, we can apply PCA to reduce the 64-dimensional data (8px by 8px) to a simple 2-dimensional form, and plot it visually:\n"}
{"snippet": "train_df = pd.read_csv('train.csv')\ntest_df = pd.read_csv('test.csv')\n", "intent": "Now we can read our data into pandas dataframes\n"}
{"snippet": "X_train, X_test, y_train, y_test = load_binarized_newsgroups_data(TfidfVectorizer(), \n                                                                  TruncatedSVD(n_components=100))\n", "intent": "__TfidfVectorizer + TruncatedSVD__\n"}
{"snippet": "X_train, X_test, y_train, y_test = load_binarized_newsgroups_data(TfidfVectorizer(),\n                                                                  SparseRandomProjection(n_components=100))\n", "intent": "__TfidfVectorizer + SparseRandomProjection__\n"}
{"snippet": "X_train, X_test, y_train, y_test = load_binarized_newsgroups_data(HashingVectorizer(n_features=100))\n", "intent": "__TfidfVectorizer + FeatureHasher__\n"}
{"snippet": "dv = DictVectorizer(dtype=np.uint8)\nrecords = df_train[['loc1', 'loc2', 'loc12', 'para1', 'dow']].to_dict(orient=\"records\")\ncategorical_train = dv.fit_transform(records)\ncategorical_train\n", "intent": "For categorical data, let's use One-Hot-Encoding\n"}
{"snippet": "scale = StandardScaler()\nnum_train = df_train[['para2', 'para3', 'para2', 'para4']].values\nnum_train = scale.fit_transform(num_train)\n", "intent": "We will also standardize the numerical variables to help the model converge and for easier coefficient interpretation\n"}
{"snippet": "cmp_df = make_comparison_dataframe(df, forecast)\ncmp_df.tail(n=3)\n", "intent": "Let's apply this function to our last forecast:\n"}
{"snippet": "categories = df_all.category_id.astype('str')\ncat_vectorizer = CountVectorizer(binary=1, dtype='uint8', min_df=10)\ncat_matrix = cat_vectorizer.fit_transform(categories)\n", "intent": "Also we can try using only category\n"}
{"snippet": "titles = df_all.title\ntitle_vectorizer = CountVectorizer(binary=1, dtype='uint8', min_df=10)\ntitle_matrix = title_vectorizer.fit_transform(titles)\n", "intent": "Using only this already gives 70% AUC.\nLet's try to use titles alone and then descriptions alone\n"}
{"snippet": "import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, Normalizer\ndf = pd.read_csv(\"https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv\")\ndf.head()\n", "intent": "1) Load in the dataset `https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv` into a pandas dataframe\n"}
{"snippet": "coef_df = pd.DataFrame({'coeffs':coef_optim, 'name':X.columns.values})\ncoef_df=coef_df.sort(['coeffs'])\ncoef_df.plot(x='name',y='coeffs',kind='bar')\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "rand_state = np.random.randint(0, 100)\nX_train, X_test, y_train, y_test = train_test_split(\n    scaled_X, y, test_size=0.2, random_state=rand_state)\nprint('Using:',orient,'orientations',pix_per_cell,\n    'pixels per cell and', cell_per_block,'cells per block')\nprint('Feature vector length:', len(X_train[0]))\n", "intent": "Then the nomalized data was randomly splitted into training and test sets as below.\n"}
{"snippet": "scaled_df = pd.DataFrame(scaled_features, columns=data.columns[:-1])\nscaled_df.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "fs = ExtraTreesClassifier() \nfs = fs.fit(train,target)\npd.DataFrame(train.columns, fs.feature_importances_).sort(ascending=False)\n", "intent": "test_new = fit.transform(test)\n"}
{"snippet": "x_train, x_test,y_train,y_test=train_test_split(\nx,y,test_size=0.2,random_state=43)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "cmp_df2 = make_comparison_dataframe(df, forecast2)\nfor err_name, err_value in calculate_forecast_errors(cmp_df2, prediction_size).items():\n    print(err_name, err_value)\n", "intent": "Here we will reuse our tools for making the comparison dataframe and calculating the errors:\n"}
{"snippet": "clean.to_csv('DATA/house-prices/train_cleaned.csv')\n", "intent": "To save the dataframe as csv, run this line of code.\n"}
{"snippet": "data1=pd.read_csv(\"https://serv.cusp.nyu.edu/~lw1474/ADS_Data/session07/data1.csv\")\ndata1.head()\n", "intent": "This is an artificial data set. It has five features and let's explore clustering models on this data set.\n"}
{"snippet": "print(\"total variance:{}\".format(np.sum(np.var(X,0))))\npca = PCA(2)\npca.fit(X)\nprint(\"variance explained via the first and second components:{}\\n\".format(pca.explained_variance_))\nprint(\"principal components:\\n{}\".format(pca.components_))\npca.explained_variance_ratio_\nnp.var(X,0)\n", "intent": "(in this particular example ~97.6% of the variance is preserved if we project down to the leading principal component)\n"}
{"snippet": "data = pd.read_csv('data/pokemon.csv')\ndata= data.set_index(\"\ndata.head()\n", "intent": "* Indexing using square brackets\n* Using column attribute and row label\n* Using loc accessor\n* Selecting only some columns\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"D:/Machine_Learning/MachineLearning-Python3/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "X_train, X_test, y_train, y_test = model_selection.train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\nclf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)   \n", "intent": "A single train/test split is made easy with the train_test_split function in the cross_validation library:\n"}
{"snippet": "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\ntfidf_train_x = tf_idf_vect.fit_transform(train_review_x['CleanedText'])\ntfidf_test_x = tf_idf_vect.transform(test_review_x['CleanedText'])\n", "intent": "<b>Naive Bayes with TF-IDF</b>\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = pd.read_csv(\"../data/hw2data.csv\")\ndata.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "from sklearn import tree\nimport graphviz \nX_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size = 0.2,random_state = 10)\nclf = tree.DecisionTreeClassifier(max_depth=5)\nclf.fit(X_train, y_train)\n", "intent": "7) Train a Decision Tree classifier with maximum depth 5 and plot the decision tree. How does performance compare?\n"}
{"snippet": "df_mean=pd.DataFrame(data=fancyimpute.SimpleFill().fit_transform(df.values), columns=df.columns, index=df.index)\ndf_iterative=pd.DataFrame(data=fancyimpute.IterativeImputer().fit_transform(df.values), columns=df.columns, index=df.index)\ndf_soft=pd.DataFrame(data=fancyimpute.SoftImpute().fit_transform(df.values), columns=df.columns, index=df.index)\n", "intent": "We will first construct the dataframe for the bottom three because for KNN we need to find the optimum value of the hyperparameter. \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nx_train_dataset1, x_test_dataset1, y_train_dataset1, y_test_dataset1 = train_test_split(x_1, y, test_size=0.3, random_state=42)\n", "intent": "Divide the training set and test set:\n"}
{"snippet": "from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nselect_feature = SelectKBest(chi2, k=5).fit(x_train_dataset1, y_train_dataset1)\n", "intent": "Next, the five best features are selected by Chi-squared stats to get the final training data and test data.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nraw_train_df, valid_df = train_test_split(mammo_df, \n                                   test_size = 0.25, \n                                   random_state = 2018,\n                                   stratify = mammo_df[['CLASS_ID']])\nprint('train', raw_train_df.shape[0], 'validation', valid_df.shape[0])\ntrain_df = raw_train_df.groupby(['CLASS']).apply(lambda x: x.sample(100, replace = True)\n                                                      ).reset_index(drop = True)\nprint('New Data Size:', train_df.shape[0], 'Old Size:', raw_train_df.shape[0])\ntrain_df[['CLASS_ID']].hist(figsize = (10, 5))\n", "intent": "We upsampling to balance the training data category.\n"}
{"snippet": "exog = pd.DataFrame(Exog)\nexog.columns = [\"f1\", \"f2\", \"f3\"]\nexog[\"ds\"] = pd.date_range(start='1/1/2018', periods=len(exog), freq='M')\n", "intent": "**Prepare dataframe of exogenous regressors with the same dates**\n"}
{"snippet": "with open('anna.txt', 'r') as f:\n    text=f.read()\nvocab = set(text)\nenum=enumerate(vocab)\nvocab_to_int = {c: i for i, c in enumerate(vocab)}\nint_to_vocab = dict(enumerate(vocab))\nchars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\nprint (enum)\n", "intent": "First we'll load the text file and convert it into integers for our network to use.\n"}
{"snippet": "data = df.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_data = scaler.fit_transform(data)\n", "intent": "Let's normalize our data for the machine learning algorithms to function better\nWe will use MinMaxScaler\n"}
{"snippet": "df = pd.read_csv('DJIA_table.csv', parse_dates=['Date'], index_col=['Date'] )\ndf = df.sort_index(ascending=True)\nprint(df.info())\n", "intent": "The downloaded data contain the daily price data between '2008-08-08' and '2016-07-01'\nfor ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n"}
{"snippet": "X, y = make_blobs(n_samples=10000, \n                  n_features=100, centers=100, random_state=0)\n", "intent": "X, y = make_blobs...\nfig = plt.figure(figsize=(10,8))\nax = fig.add_subplot(111)\nax.scatter...\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=30)\n", "intent": "- Create test/train splits of the original data set\nx_train, x_test, y_train, y_test = train_test_split(...)\n"}
{"snippet": "df = pd.read_csv('../../data/ads_hour.csv',index_col=['Date'], parse_dates=['Date'])\n", "intent": "scaler = MinMaxScaler(feature_range=(-1, 1))\nscaled = scaler.fit_transform(df.values)\n"}
{"snippet": "le = preprocessing.LabelEncoder()\nle.fit(authors)\nauthorship['Author_num'] = le.transform(authorship['Author']) \nprint(authorship['Author_num'])\n", "intent": "Use the LabelEncoder to encode Authors to integers...\n1. What does the LabelEncoder do for us? \n"}
{"snippet": "train_sc_df = pd.DataFrame(train_sc, columns=['Scaled'], index=train.index)\ntest_sc_df = pd.DataFrame(test_sc, columns=['Scaled'], index=test.index)\ntrain_sc_df.head()\n", "intent": "train_sc and test_sc are the scaled training and test dataset\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.85, random_state=1234)\n", "intent": "First of all, lets split our train data to train and validation sets:\n"}
{"snippet": "station_data = pd.read_csv(filepath_or_buffer='data/station_data.csv',\n                           sep=',', header=0,\n                           names=['Station', 'Name', 'Lat', 'Long', 'Dock_Count', 'City'])\nprint('dimensions of station data:',station_data.shape)\nprint('check if any data is missing:', \n      station_data.isnull().values.any())\nstation_data.head(5)\n", "intent": "The data set is ingested and feature names are relabeled to ease data manipulation.\n"}
{"snippet": "trip_data_agg['net_rate_previous_hour'] = \\\n        trip_data_agg.groupby(['Station', 'Date'])['net_rate']\\\n                .shift(1).fillna(0)\ntrip_data_agg['net_customers_previous_hour'] = \\\n        trip_data_agg.groupby(['Station', 'Date'])['net_customers']\\\n                .shift(1).fillna(0)\ntrip_data_agg['net_subscribers_previous_hour'] = \\\n        trip_data_agg.groupby(['Station', 'Date'])['net_subscribers']\\\n                .shift(1).fillna(0)\ntrip_data_agg.drop(['net_customers',  'net_subscribers'], inplace=True, axis=1)\n", "intent": "We now engineer time lagged features (i.e. by one hour).\n"}
{"snippet": "import reverse_geocoder as rg\nmta_data = pd.read_csv(filepath_or_buffer='data/MTA.pedvolumemodel_data.csv',\n                       sep=',')\nmta_data[['the_geom', 'MODEL6_VOL']].head(5)\n", "intent": "Having more time I would have matched these intersections with the bike stations.\n"}
{"snippet": "(trainX, trainy), (testX, testy) = mnist.load_data()\n", "intent": "Pixel scaling method is to calculate the mean pixel value across the entire training dataset, then subtract it from each image.\n"}
{"snippet": "ratings = pd.read_csv(PATH/'ratings.csv')\nratings.head()\n", "intent": "Table user/movie -> rating\n"}
{"snippet": "movies = pd.read_csv(PATH/'movies.csv')\nmovies.head()\n", "intent": "Table to get the titles of the movies.\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\n", "intent": "In this article we will use toy sklearn dataset **\"breast_cancer\"** (binary classification task). Lets load the dataset and take a look at the data.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ngender = le.fit_transform(dataset['Gender'])\ngeo = pd.get_dummies(dataset['Geography'],drop_first=True)\nX = X.drop(['Gender','Geography'],axis=1)\nX['Gender'] = gender\nX = pd.concat([X,geo],axis=1)\n", "intent": "Transformamos los features que son strings\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n", "intent": "Dividimos el dataset para entrenar y probar\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n", "intent": "Standarizamos los dataframes con los features\n"}
{"snippet": "data = pd.read_csv('College_Data.csv',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df = pd.read_csv('knn_project_data.csv')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "loan = pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nfor label in ['embarked','sex']:\n    titanic[label] = LabelEncoder().fit_transform(titanic[label])\n", "intent": "Three binomials, two categoricals, and four numerical features.\n"}
{"snippet": "pca = PCA(2) \npca.fit(X) \n", "intent": "Visualize the data using PCA (two dimensions). Color the points by the day of the week\n"}
{"snippet": "scaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df)\n", "intent": "We will use LogisticRegression as our base algorithm. Firstly, scaling the data. \n"}
{"snippet": "scaler_model = prep.MinMaxScaler()\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "import pandas as pd\ndata_frame = pd.read_csv('data/driving_log.csv', usecols=[0,1,2,3])\ndata_frame.describe(include='all')\n", "intent": "Now lets explore the Udacity's diriving_log.csv\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_feats,columns=df.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "coeff_df = pd.DataFrame(lm.coef_,X.columns,columns=['Coefficient'])\ncoeff_df\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": " X_train, X_test, y_train, y_test = train_test_split( yelp_class['text'], yelp_class['stars'], test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "PATH_TO_DATA = ('/Users/y.kashnitsky/Documents/Machine_learning/org_mlcourse_open/private/competitions/kaggle_alice/')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "train = train_df[['site%d' % i for i in range(1, 11)]].fillna(0).astype('int')\ntest = test_df[['site%d' % i for i in range(1, 11)]].fillna(0).astype('int')\ntrain_text = [' '.join([str(i) for i in j]) for j in train.values]\ntext_text = [' '.join([str(i) for i in j]) for j in test.values]\n", "intent": "Build Tf-Idf features based on sites. You can use `ngram_range`=(1, 3) and `max_features`=100000 or more\n"}
{"snippet": "scaler = StandardScaler()\ntrain_features_scaled = scaler.fit_transform(train_df[['year_month', 'morning', 'day', 'week_hour']])\ntest_features_scaled = scaler.transform(test_df[['year_month', 'morning', 'day', 'week_hour']])\n", "intent": "Scale this features and combine then with Tf-Idf based on sites (you'll need `scipy.sparse.hstack`)\n"}
{"snippet": "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), \n                                   max_features=100000)\n", "intent": "**Tf-Idf with article content.**\n"}
{"snippet": "X_sbs = sbs.fit_transform(df_scaled, y, custom_feature_names=columns)\n", "intent": "There is information about CV scoring on each iteration in log. The best quality we have with subset with 15 and from 17 to 24 features.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n", "intent": "- Now repeat the above after scaling your input data.\n"}
{"snippet": "X,Y = one_hot_data.iloc[:,1:].values, one_hot_data.admit.values\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\nprint(\"Number of training samples is\", len(X_train))\nprint(\"Number of testing samples is\", len(X_test))\nprint(X_train[:10])\nprint(\"\\n\")\nprint(X_test[:10])\n", "intent": "In order to test our algorithm, we'll split the data into a Training and a Testing set. The size of the testing set will be 10% of the total data.\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, Y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['over_200k', 'under_200k'],\n                        columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "std_scale = preprocessing.StandardScaler().fit(df[['SQFT', 'BDRMS', 'AGE']])\ndf_std = std_scale.transform(df[['SQFT', 'BDRMS', 'AGE']])\ndf_std\n", "intent": "[StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n"}
{"snippet": "df = pd.read_csv('march-machine-learning-mania-2016-v2/TourneySeeds.csv')\ndf\n", "intent": "Get season and seeds data\n"}
{"snippet": "df2 = pd.read_csv('march-machine-learning-mania-2016-v2/RegularSeasonDetailedResults.csv')\ndf2\n", "intent": "Get detailed season results (2003 onwards)\n"}
{"snippet": "seasons = pd.read_csv('march-machine-learning-mania-2016-v2/Seasons.csv')\nseasons = seasons[seasons['Season'] > 2002]\n", "intent": "Import season data (2003 and above)\n"}
{"snippet": "teams = pd.read_csv('march-machine-learning-mania-2016-v2/Teams.csv')\nteams.head()\n", "intent": "Import Teams data and replace team code's with team names\n"}
{"snippet": "games.to_csv('all_games.csv')\n", "intent": "Save games df to csv incase we lose the kernel\n"}
{"snippet": "pd.DataFrame(data = [pd.Series(dict_rfe),pd.Series(dict_pca), pd.Series(sbs_dict)], index = ['RFE', 'PCA', 'SBS']).T\n", "intent": "Comparing CV scores of all algorithms\n"}
{"snippet": "bin_NUMBER=3\ntemp2=pd.DataFrame(df[\"Salary\"].copy(),columns=[\"Salary\"])\nbins2,replacements2=create_bins(min(temp2[\"Salary\"]),max(temp2[\"Salary\"]),bin_NUMBER)\ntemp=pd.DataFrame(df[\"Salary\"].copy(),columns=[\"Salary\"])\nbins,replacements=create_bins(min(temp[\"Salary\"]),max(temp[\"Salary\"]),bin_NUMBER)\ntemp[\"Normalized Salary\"]=temp[\"Salary\"].apply(lambda x: salary_bin(bins,replacements,x))\ndf1=temp[\"Normalized Salary\"]\n", "intent": "The code below creates bin names for the salary data and classifies the correct output accordingly. This is used for the logistic regression.\n"}
{"snippet": "train_data = pd.read_csv(f'{PATH}train.csv')\ntest_data = pd.read_csv(f'{PATH}test.csv')\n", "intent": "For this one I am gonna use the title of the name; :) feature engineering.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"../DataScience-Python3/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\niris = load_iris()\nmodel = RandomForestClassifier(n_estimators=10)\nmodel = model.fit(iris['data'], iris['target'])\nmodel\n", "intent": "Let's train a simple random forest model and deploy it in the Predictive Service.\n<img src=\"images/left.png\"></img>\n"}
{"snippet": "X_train_pca=pd.DataFrame(pca.transform(X_train_stand),index=X_train.index)\n", "intent": "Transformation of data\n"}
{"snippet": "pd.DataFrame(\n    np.array([\n        sizes, \n        pca_rf_scores('scatt', model='rf'), \n        pca_rf_scores('resnet', model='rf'), \n        pca_rf_scores('vgg19', model='rf')\n    ]).transpose(),columns=['size of training data', 'scatt', 'resnet', 'vgg19'])\n", "intent": "The above table (1) shows the resulting test accuracies of different feature sets with the best number of components achieved above.\n"}
{"snippet": "pd.DataFrame(scores,\n             columns=['predicted day %s' % i for i in range(1,6)],\n             index=['with features of previous %s days' % i for i in range(6)])\n", "intent": "The AUC scores on each predicted day are as follows.\n"}
{"snippet": "pd.DataFrame(scores_chained,\n             columns=['predicted day %s' % i for i in range(1,6)],\n             index=['with features of previous %s days' % i for i in range(6)])\n", "intent": "The AUC scores on each predicted day are as follows.\n"}
{"snippet": "Enc = dfMCats.apply(lambda x: d[x.name].fit_transform(x))\nEnc.head()\n", "intent": "Label Encoder is a pretty quick solution\n"}
{"snippet": "dataset = load_iris()\nprint('Feature names:', dataset.feature_names)\nprint('Iris names:', dataset.target_names)\nprint('Number of instances:', dataset.data.shape[0])\n", "intent": "Now load the dataset and take a look at its properties:\n"}
{"snippet": "coef = model.coef_\nCoeff = pd.DataFrame(coef)\nCoeff\n", "intent": "See what is going on with the Coefficients\n"}
{"snippet": "A1 = b.loc[\"accepted\"]\nA0 = b.loc[\"rejected\"]\nO1 = A1/A0\nOdds['Odds'] = pd.DataFrame(O1)\nOdds\n", "intent": "Lets compare the rate of acceptance between prestige of 4 and of 1 using odds ratio. <br/>\nOdds Ratio is a simple statistic that tells a big story\n"}
{"snippet": "modelCVb = LogisticRegressionCV(cv=20)\nresults20 = modelCVb.fit(X ,y)\nCoefCVb = results20.coef_\nCoefCV20 = pd.DataFrame(CoefCVb)\nCoefCV20\n", "intent": "Cross Validation k-folds = 5 (above) <br/> <br/>\nCross Validatoin k-folds = 20 (below)\n"}
{"snippet": "boston_data = load_boston()\nx = boston_data['data']\ny = boston_data['target']\nmodel = LinearRegression()\nmodel.fit(x, y)\n", "intent": "Here we are importing the linear model from scikit-learn. We are also \n"}
{"snippet": "dataframe = pd.read_csv('housing_prices.csv') \ndataframe = dataframe.drop(['index', 'price', 'sq_price'], axis=1)\ndataframe = dataframe[0:10]\ndataframe\n", "intent": "Pandas to work with data as tables\nnumpy to use number matrices\nmatplotlib for creating and diplaying graphs\ntensorflow for machine learning\n"}
{"snippet": "import numpy as np\nfrom sklearn import preprocessing\nlabels = np.array([1,5,3,2,1,4,2,1,3])\nlb = preprocessing.LabelBinarizer()\nlb.fit(labels)\nlb.transform(labels)\n", "intent": "Transforming your labels into one-hot encoded vectors is pretty simple with scikit-learn using LabelBinarizer. Check it out below!\n"}
{"snippet": "rfe = RFE(LogisticRegression(), 2)\nrfe = rfe.fit(sensorReadings, lable)\n", "intent": "Create a classifier used to evaluate a subset of attributes. Also create the RFE model and select 2 attributes.\n"}
{"snippet": "def prepare_images(path, factor):\n    for file in os.listdir(path):\n        img = cv2.imread(path + '/' + file)\n        h, w, c = img.shape\n        new_height = h / factor\n        new_width = w / factor\n        img = cv2.resize(img, (new_width, new_height), interpolation = cv2.INTER_LINEAR)\n        img = cv2.resize(img, (w, h), interpolation = cv2.INTER_LINEAR)\n        print('Saving {}'.format(file))\n        cv2.imwrite('images/{}'.format(file), img)\n", "intent": "http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ntrain = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', subset='train', shuffle=True, random_state=23)\ntest = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', subset='test', shuffle=True, random_state=23)\n", "intent": "Before we begin, we must repopulate this Notebook with both the necessary\ndata and supporting functions.\n-----\n"}
{"snippet": "target = np.array([dataset.target_names[x] for x in dataset.target]).reshape(-1, 1)\ndf_full = pd.DataFrame(\n    np.concatenate([dataset.data, target], axis=1),\n    columns=feature_names + ['target'])\ndf_full[feature_names] = df_full[feature_names].astype(float)\ndf_full.sample(5, random_state=RANDOM_SEED)\n", "intent": "Load the dataset into DataFrame:\n"}
{"snippet": "X_dv, Y_dv = preproc.load_data(DEV_FILE)\n", "intent": "- Loading Dev data for english:\n"}
{"snippet": "reload(preproc);\nX_tr_nr, Y_tr_nr = preproc.load_data(NR_TRAIN_FILE)\n", "intent": "- Loading training data for norwegian:\n"}
{"snippet": "X_dv_nr, Y_dv_nr = preproc.load_data(NR_DEV_FILE)\n", "intent": "- Loading dev data for norwegian:\n"}
{"snippet": "X_te_nr, Y_te_nr = preproc.load_data(NR_TEST_FILE_HIDDEN)\n", "intent": "- Loading test data for norwegian:\n"}
{"snippet": "df_train = pd.read_csv('lyrics-train.csv')\n", "intent": "Read the data into a dataframe\n"}
{"snippet": "y = data['class']\nx = data.drop('class', axis=1)\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n", "intent": "Now I want to set my data set to train and test so that I can use some classification algorithms and test how good/bad I am doing.\n"}
{"snippet": "trainSet['ApplicantIncome'].fillna(trainSet['ApplicantIncome'].mean(), inplace = True)\ntrainSet['CoapplicantIncome'].fillna(trainSet['CoapplicantIncome'].mean(), inplace = True)\ntrainSet['LoanAmount'].fillna(trainSet['LoanAmount'].mean(), inplace = True)\ntrainSet['Loan_Amount_Term'].fillna(trainSet['Loan_Amount_Term'].mean(), inplace = True)\n", "intent": "Now I can replace the null values with the mean values.\n"}
{"snippet": "y = trainSet['Loan_Status']\ny=y.astype('int') \nX = trainSet.drop('Loan_Status', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n", "intent": "Now it is time to try the machine learning algorithm to train the model.\n"}
{"snippet": "y = trainSet['Loan_Status']\ny=y.astype('int') \nX = trainSet.drop('Loan_Status', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n", "intent": "OK finally! It sort of look ok now. Even though I am not happy with negative values but let's see what happens.\n"}
{"snippet": "df_train, df_test, y_train, y_test = train_test_split(df_full.drop('target', axis=1), df_full.target, test_size=0.4)\ndf_train.shape, y_train.shape, df_test.shape, y_test.shape\n", "intent": "Now, let's split our data to train and test, having the holdout set of 40%:\n"}
{"snippet": "products = pd.read_csv('data/amazon_baby.csv')\n", "intent": "We will use a dataset consisting of baby product reviews on Amazon.com.\n"}
{"snippet": "products['review'] = products['review'].fillna('')\n", "intent": "Replace nan with empty strings\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nmodel = LogisticRegression(penalty = 'l2', C=0.1)\nfrom patsy import dmatrix\nX = dmatrix(\"city + is_senior + is_manager + 0\", data=salary_data) \nX_scaled = scaler.fit_transform(X)\ny = salary_data['HighSalary']\nmodel.fit(X_scaled, y)\n", "intent": "- You can either create the dummy features manually or use the `dmatrix` function from `patsy`\n- Remember to scale the feature variables as well!\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\ndata.head()\n", "intent": "We are interested to find out whether there are linear relationships between advertising dollars spent and the sales of a product\n"}
{"snippet": "data = pd.read_csv('../../assets/datasets/train.tsv', sep='\\t', na_values='?')\ndata['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\ndata['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n", "intent": "- These are websites that always relevant like recipies or reviews (as opposed to current events)\n- Look at some examples\n"}
{"snippet": "sac = pd.read_csv('../../data/Sacramentorealestatetransactions.csv')\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "import numpy as np\nfrom sklearn import svm, grid_search, datasets\nfrom sklearn.neighbors import KNeighborsClassifier\niris_data = datasets.load_iris()\n", "intent": "While GridSearch can work with most sklearn models, we will try it out on KNN to start with iris dataset.\n"}
{"snippet": "union = make_union(age_pipe,\n                   one_hot_pipe,\n                   gender_pipe,\n                   fare_pipe)\nunion.fit_transform(df.head())\n", "intent": "Use the `make_union` function from the `sklearn.pipeline` modeule to combine all the pipes you have created.\n"}
{"snippet": "house = datasets.load_boston()\nbcancer = datasets.load_breast_cancer()\n", "intent": "We're going to be working with the Boston and breast cancer data sets, so let's go ahead and load those from sklearn's datasets library\n"}
{"snippet": "pd.DataFrame([{\n    'sepal_length': 4.0,\n    'sepal_width': 5.0,\n    'petal_length': 1.0,\n    'petal_width': 0.5\n}])\n", "intent": "Hmmm, it's not _setosa_, so what happened? Let's check how the DataFrame is constructed:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "Construct, fit, and predict with a linear model on both datasets\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(analytic_df, y)\nx_train.shape, x_test.shape\n", "intent": "Looks like there might be overfitting! Let's redo this but *first* make a training and a test set!\n"}
{"snippet": "data = pd.read_csv('~/Desktop/DSI-SF-2/datasets/401_k_abadie/401ksubs.csv') \ndata.head(2)\n", "intent": "1. Read the 401ksubs.csv data into Pandas.\n2. Explore the data by sorting, plotting, group_by, and any other ideas/techniques you have been using.\n"}
{"snippet": "wine_df = pd.read_json(api_response.text)\nwine_df.head(2)\n", "intent": "This sometimes works, but the data may need adjusting\n"}
{"snippet": "wine = pd.read_csv('../../assets/datasets/wine_v.csv')\n", "intent": "And read in our data:\n"}
{"snippet": "x_standard = StandardScaler().fit_transform(x)\n", "intent": "Since we don't know what units our data was measured in, let's standardize \"x\"\n"}
{"snippet": "df_new = pd.DataFrame(data=df, columns=['workclass_num', 'education-num', 'hours-per-week', 'income_num'])\ndf_new2 = pd.DataFrame(data=df, columns=['workclass_num', 'education-num'])\nprint df_new.head(n=5)\nprint len(df_new)\n", "intent": "Create a New Dataframe with just numerical data\n"}
{"snippet": "df = pd.DataFrame(data=adult, columns=['workclass', 'education-num', 'hours-per-week', 'income'])\ndf.head(n=5)\n", "intent": "Convert the data to a Pandas dataframe to work with the data:\n"}
{"snippet": "df_new = pd.DataFrame(data=df, columns=['workclass_num', 'education-num', 'hours-per-week', 'income_num'])\ndf_new2 = pd.DataFrame(data=df, columns=['workclass_num', 'education-num'])\ndf_new.head(n=5)\n", "intent": "Create a New Dataframe with just numerical data for the analysis\n"}
{"snippet": "scaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_train, X_val,y_train,y_val = train_test_split(X,y, test_size=0.2)\n", "intent": "Linear models like scaled input\n"}
{"snippet": "PCA_set = PCA(n_components=5)\nY = PCA_set.fit_transform(Xvote)\n", "intent": "Now, conduct a PCA using scikit learn\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "df = pd.read_csv(\"assets/datasets/iris.csv\")\ndf.head(5)\n", "intent": "Let's do some clustering with the iris dataset.\n"}
{"snippet": "votes = pd.read_csv('votes.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "PCA_set = PCA(n_components=5)\nY = PCA.fit_transform(xStand)\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "airports_pca = PCA(n_components=2)\nY = pcask2.fit_transform(xStand)\n", "intent": "Finally, conduct the PCA - use the results about to guide your selection of \"n\" componants\n"}
{"snippet": "Ydf = pd.DataFrame(Y)\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = mnist.load_data()\nn_train, height, width = X_train.shape\nn_test, _, _ = X_test.shape\n", "intent": "First, we can download the data with Keras. \n"}
{"snippet": "def make_image(xs):\n    a = np.fromstring(xs, sep=' ', dtype=np.float).reshape(96, 96)\n    return a / 256.0\ndf = pd.read_csv('../data/training.csv', converters={'Image': make_image})\ndf['Image'].head()\n", "intent": "Load data from csv files.\nImage is provided as space separated values in 'Image' column.\nTODO:\n  * Scala output labels into (-1, 1) range)\n"}
{"snippet": "def make_image(xs):\n    a = np.fromstring(xs, sep=' ', dtype=np.float).reshape(96, 96)\n    return a / 256.0\ndf = pd.read_csv('../data/training.csv', converters={'Image': make_image})\ndf['Image'].head()\n", "intent": "Load data from csv files.\nImage is provided as space separated values in 'Image' column.\n"}
{"snippet": "word_tsne = TSNE().fit_transform(word_vectors )\n", "intent": "Draw a graph as usual\n"}
{"snippet": "data = load_breast_cancer()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = pd.Series(data.target)\nX = X.drop(['worst concave points', 'mean concave points', 'worst perimeter', 'worst radius', 'worst area', 'mean concavity'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=.33,\n                                                    random_state=0)\n", "intent": "Let's investigate the accuracy of a random forests compared with a single decision tree using the boston dataset. \n"}
{"snippet": "insects = pd.read_csv('./data/insects.csv', sep='\\t')\ninsects.head()\n", "intent": "The data set we will use to demonstrate linear regression contains measurements on a single species of insect captured on two continents.\n"}
{"snippet": "sample_train = pd.read_csv(path + \"train_sample.csv\")\ndisplay(sample_train.head(n=5))\n", "intent": "----\nExplore the sample training data , which are 100,000 randomly-selected rows of training data.\n"}
{"snippet": "def make_some_noise(seed):\n    np.random.seed(seed)\n    return pd.DataFrame(np.random.randint(0, 7, size=(3, 1)))\nX_noise = make_some_noise(1)\nX_train_noisy = X_train + X_noise\nX_test_noisy = X_test + X_noise\n_ = lin_reg(X_train_noisy, y_train, X_test_noisy, y_test)\n", "intent": "Let's try another example that adds a little noise to our input values so we can see how this affects the model.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\niris_train, iris_test = train_test_split(iris)\nX_train = iris_train.drop(['sepal_length'], axis=1)\nX_test = iris_test.drop(['sepal_length'], axis=1)\ny_train = iris_train[['sepal_length']]\ny_test = iris_test[['sepal_length']]\ntry:\n    regr = lin_reg(X_train, y_train, X_test, y_test, graph=False, normalize=True)\nexcept ValueError as e:\n    print(\"ValueError :\", e)\n", "intent": "If we try to use linear regression to predict the the sepal length with our current features, we will get an error.\n"}
{"snippet": "ufo = pd.read_table('http://bit.ly/uforeports', sep=',')\n", "intent": "Let's read some data into a table and manipulate that data\n"}
{"snippet": "from pandas import DataFrame\ndata = DataFrame([[1., 6.5, 3.], [1., NA, NA], [NA, NA, NA], [NA, 6.5, 3.]])\ncleaned = data.dropna()\ndata\n", "intent": "In the next examples we will se the same application of filtering in DataFrame objects. By default _dropna_ erases all the rows that have NaNs.\n"}
{"snippet": "import numpy as np\ndf = DataFrame(np.random.rand(7,3))\ndf\n", "intent": "If we only want to keep a certain number of observations, remember that we can select them with the method iloc from the object DataFrame\n"}
{"snippet": "df.fillna({1:0.5, 2:0}) \n", "intent": "In the same way we can use a dictionary in order to define more specifically the data to replace in the NaNs\n"}
{"snippet": "from sklearn.datasets import load_diabetes\ndiabetes = load_diabetes()\nn = diabetes.data.shape[0]\ndata = diabetes.data\ntargets = diabetes.target\n", "intent": "We will demonstrate and compare different algorithms on diabetes dataset from sklearn.datasets. Let's load it.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = pd.DataFrame(np.random.randn(1000, 4))\ndata.describe() \n", "intent": "The next method allows us to understand how the data in a DataFrame is distributed.\n"}
{"snippet": "X = [[1., -1., 2.],\n    [2., 0., 0.],\n    [0., 1., -1.]]\nnormalizer = preprocessing.Normalizer().fit(X)\nnormalizer\n", "intent": "Remember the idea is to make a normalizer in order to use in other data that we didn't use during the fit.\n"}
{"snippet": "DataFrame(data, columns=['year','pop', 'state'])\n", "intent": "You can sort the during the creation process of the DataFrame\n"}
{"snippet": "data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],\n       'year': [2000, 2001, 2002, 2001, 2002, 2003],\n       'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\nframe2 =  DataFrame(data, columns=['year','state', 'pop',  'debt'],\n                   index=['one', 'two', 'three', 'four', 'five', 'six'])\nframe2\n", "intent": "If we didn't declare the values in a column we will have NaN registers.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n                 header=None, names=[\"sepal_length\",\"sepal_width\", \"petal_length\",\"petal_width\",\"class\"])\ndf_park = pd.read_csv('../datasets/parks.csv', index_col=['Park Code'], encoding='utf-8')\n", "intent": "The next code lines will download a dataset from the repository of the University of California and save it in a Dataframe\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n                 header=None, names=['sepal_length',\n                                     'sepal_width', \n                                     'petal_length',\n                                     'petal_width',\n                                    'class'])\ndf.tail()\n", "intent": "Pandas is a library to load the Iris dataset from a public repository into a DataFrame Object \n"}
{"snippet": "def build_q_table(n_states, actions):\n    table = pd.DataFrame(\n    np.zeros((n_states, len(actions))), \n    columns = np.arange(0,9), \n    )\n    return table\nbuild_q_table(N_STATES, ACTIONS)\n", "intent": "Second, we are going to define the Q table to this problem\n"}
{"snippet": "pd.DataFrame(zip(features.columns, np.transpose(model_lr.coef_)))\n", "intent": "**2) Which features are predictive for this logistic regression? Explain your thinking. Do not simply cite model statistics.**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncont_data = data.drop(['Channel','Region'], axis=1)\nX = scaler.fit_transform(cont_data)\nkm = KMeans(3)\nkm.fit(X)\n", "intent": "**3) Using ONLY the continuous features in the dataset, apply the K-means algorithm to find clusters in the data.**\n"}
{"snippet": "gs_results_df=pd.DataFrame(np.transpose([-gs.cv_results_['mean_test_score'],\n                                         gs.cv_results_['param_learning_rate'].data,\n                                         gs.cv_results_['param_max_depth'].data,\n                                         gs.cv_results_['param_n_estimators'].data]),\n                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\ngs_results_df.plot(subplots=True,figsize=(10, 10))\n", "intent": "We have managed to improve the results. But spent a lot of time on it. Let's look how our parameters have been changing from iteration to iteration:\n"}
{"snippet": "fitter = StandardScaler()\nfit_train = fitter.fit_transform(X_train)\n", "intent": "The data is rather normalized already but to review the process:\n"}
{"snippet": "masked_epi = masker.inverse_transform(samples)\nmax_zscores = image.math_img(\"np.abs(img).max(axis=3)\", img=masked_epi)\nplotting.plot_stat_map(max_zscores, bg_img=anat, dim=-.5)\n", "intent": "To recover the original data shape (giving us a masked and z-scored BOLD series), we simply use the masker's inverse transform:\n"}
{"snippet": "coef_vol = masker.inverse_transform(svc.coef_)\nplotting.plot_stat_map(coef_vol, bg_img=anat, dim=-.5)\n", "intent": "Since we have a value for each voxel, we can simply map this back to the volume using our `masker`, and visualize the weights.\n"}
{"snippet": "departments_df = pd.DataFrame(artworks['Department'].value_counts())\ndepartments_df.head(10)\n", "intent": "Let's see how art works split by department.\n"}
{"snippet": "titles = data['Title'].fillna('')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(max_features = 250000, ngram_range=(1,2), stop_words='english')\nvectorizer.fit(titles)\nX = vectorizer.transform(titles).toarray()\nX\n", "intent": "Not so great.  Let's try it instead with TfIDfVectorizer.  \n"}
{"snippet": "data.diameter_bin = data.diameter_bin.fillna(0)\ndata.height_bin = data.height_bin.fillna(0)\ndata.length_bin = data.length_bin.fillna(0)\ndata.width_bin = data.width_bin.fillna(0)\ndata.depth_bin = data.depth_bin.fillna(0)\ndata.circumference_bin = data.circumference_bin.fillna(0)\n", "intent": "Now we fill in the Nan values with 0 to indicate that they are distinct from the other data.\n"}
{"snippet": "pd.DataFrame(zip(X.columns, np.transpose(model.coef_)))\n", "intent": "Okay about 70 per cent.  Can we obtain more insights from examining the coefficients? Let's put them in a dataframe and look further.\n"}
{"snippet": "of_df = pd.read_csv(\"faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    contains_org = False\n    contains_person = False\n    for word in parsed:\n        contains_org |= word.ent_type_ == 'ORG'\n        contains_person |= word.ent_type_ == 'PERSON'\n    return contains_org, contains_person\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title']].head()\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "tpe_results=np.array([[x['result']['loss'],\n                      x['misc']['vals']['learning_rate'][0],\n                      x['misc']['vals']['max_depth'][0],\n                      x['misc']['vals']['n_estimators'][0]] for x in trials.trials])\ntpe_results_df=pd.DataFrame(tpe_results,\n                           columns=['score', 'learning_rate', 'max_depth', 'n_estimators'])\ntpe_results_df.plot(subplots=True,figsize=(10, 10))\n", "intent": "We have managed to find even better solution comparing to the RandomizedSearch.\nLet's look at the visualization of the process\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    contains_org = \n    contains_person = \n    return \ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title']].head()\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    contains_org |= any([word.ent_type_ == 'ORG' for word in parsed])\n    contains_person |= any([word.ent_type_ == 'PERSON' for word in parsed])\n    return contains_org and contains_person\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title']].head()\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "yelp_best_worst = yelp[(yelp['stars']==5)| (yelp['stars']==1)]\nX = yelp_best_worst.text\ny = yelp_best_worst.stars\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n", "intent": "- Select only 5-star and 1-star reviews.\n- The text will be the features, the stars will be the target.\n- Create a train-test split.\n"}
{"snippet": "X = rt.quote.values\ny = rt['is_fresh']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n", "intent": "---\nIt is up to you what ngram range you want to select. **Make sure that `binary=True`**\n"}
{"snippet": "s_df = pd.DataFrame(sanders)\ns_df.head()\n", "intent": "> *Hint: this is as easy as passing it to the DataFrame constructor!*\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',\n                     header=None)\ncolumns = ['Class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity',  'Magnesium', 'Total phenols', \n        'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n        'OD280/OD315 of diluted wines', 'Proline']\ndataset.columns = columns\nprint('Shape: ', dataset.shape, end='\\n\\n')\nprint(dataset.head(10))\n", "intent": "Abstract: Using chemical analysis determine the origin of wines   \nSource: https://archive.ics.uci.edu/ml/datasets/Wine\n"}
{"snippet": "class Preprocess:\n    def __init__(self, df, dim):\n        self.__df = df\n        self.__norm = Normalizer(self.__df.values)\n        self.__red = PCA(self.__norm(self.__df.values), dim)\n    def __call__(self, iid):\n        return self.__red(self.__norm(self.__df.loc[iid, :].values))\n", "intent": "Preprocess, PCA and Normalizer instances containts values that are related to the model, they have to be saved for future use in production !\n"}
{"snippet": "scaler = MinMaxScaler((-1,1))\nscaler.fit(x)\nscaled_x = scaler.transform(x)\nprint(scaled_x[0:10])\n", "intent": "Let's assume we want to rescale the values of this numpy array to the range from -1 to 1.\n"}
{"snippet": "import numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nvalues = ['Warm', 'Cold', 'Warm', 'Hot', 'Hot', 'Cold']\nlabenc = LabelEncoder()\nint_encoded = labenc.fit_transform(values)\nprint(int_encoded)\n", "intent": "LabelEncoder will take an input of labels and **encode these as sequential integers**. It uses the same syntax as the scalers.\n"}
{"snippet": "scores_df=pd.DataFrame(index=range(n_iter))\nscores_df['Grid Search']=gs_results_df['score'].cummin()\nscores_df['Random Search']=rs_results_df['score'].cummin()\nscores_df['TPE']=tpe_results_df['score'].cummin()\nscores_df['Annealing']=sa_results_df['score'].cummin()\nax = scores_df.plot()\nax.set_xlabel(\"number_of_iterations\")\nax.set_ylabel(\"best_cumulative_score\")\n", "intent": "Let's plot best_cumulative_score vs. number_of_iterations for all approaches:\n"}
{"snippet": "iris = datasets.load_iris()\n", "intent": "We load the dataset from the datasets module in sklearn.\n"}
{"snippet": "boston = datasets.load_boston()\n", "intent": "We load the dataset from the datasets module in sklearn.\n"}
{"snippet": "ads = pd.read_csv('data/ads.csv', index_col = 0)\n", "intent": "- Review Mutlivariate Linear Regression\n- Coding Qualitative Variables\n- Polynomial Regression\n"}
{"snippet": "std_scaled = StandardScaler()\ncol = X.columns\nX_scaled = std_scaled.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled,columns=cols)\n", "intent": "Now let's use a `StandardScaler` to scale the features.\n"}
{"snippet": "X_train,X_test,y_train,y_test = train_test_split(X,y)\n", "intent": "Oddly enough it seems to have made it worse. Let's revert back to our original non-scaled variables.\n"}
{"snippet": "vect = CountVectorizer(ngram_range=(2,2))\n", "intent": "handles phrases, not just words\n"}
{"snippet": "X = ks_sample[['duration','backers','usd_goal_real']].join(cat_var)\ny = ks_sample.state_simple\nX_train,X_test,y_train,y_test = train_test_split(X,y)\n", "intent": "Let's redefine the variables.\n"}
{"snippet": "data_path_test = os.path.join(os.getcwd(),'datasets','spambase_test.csv')\nspambase_test = pd.read_csv(data_path_test, delimiter=',')\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure\n"}
{"snippet": "file_A_path = os.path.join(os.getcwd(),'datasets','train_20news_partA.csv')\nfile_B_path = os.path.join(os.getcwd(),'datasets','train_20news_partB.csv')\nnews_A = pd.read_csv(file_A_path)\nnews_B = pd.read_csv(file_B_path)\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "def generate_leaderboards(values):\n    df = pd.DataFrame(values, columns=['target'])\n    public_lb, private_pb = train_test_split(df, test_size=0.3, shuffle=False)\n    return public_lb, private_pb\n", "intent": "Now, we are going to create some functions, which will help us in testing exploits.\n"}
{"snippet": "X,y = make_blobs(n_samples=200,centers=3,cluster_std=0.60,random_state=0)\n", "intent": "Let us generate and plot the dataset\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimputer = Imputer()\nX = imputer.fit_transform(X)\n", "intent": "We need to handle missing values.\n"}
{"snippet": "admission = pd.read_csv('https://serv.cusp.nyu.edu/~cq299/ADS2016/Data/admission.csv')  \nadmission.head()\n", "intent": "Example - learning the probability of school admission based on two exams\n"}
{"snippet": "data1=pd.read_csv(\"https://serv.cusp.nyu.edu/~lw1474/ADS_Data/session07/data1.csv\")\nprint(data1.shape)\ndata1.head()\n", "intent": "This is an artificial data set. It has five features and let's explore clustering models on this data set.\n"}
{"snippet": "ads = pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "messages = pandas.read_csv('./data/SMSSpamCollection', sep='\\t', quoting=csv.QUOTE_NONE,\n                           names=[\"label\", \"message\"])\nprint(messages)\n", "intent": "Instead of parsing TSV (or CSV, or Excel...) files by hand, we can use Python's `pandas` library to do the work for us:\n"}
{"snippet": "bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])\nprint len(bow_transformer.vocabulary_)\n", "intent": "Each vector has as many dimensions as there are unique words in the SMS corpus:\n"}
{"snippet": "loc = os.path.join(os.getcwd(),'datasets','spambase_test.csv')\nspambase_test = pd.read_csv(loc)\nspambase_test\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure\n"}
{"snippet": "locA = os.path.join(os.getcwd(),'datasets','train_20news_partA.csv')\nlocB = os.path.join(os.getcwd(),'datasets','train_20news_partB.csv')\nnews_A = pd.read_csv(locA)\nnews_B = pd.read_csv(locB)\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "train_target = pd.read_csv(os.path.join(PATH_TO_RAW_DATA, 'train_log1p_recommends.csv'), \n                           index_col='id')\n", "intent": "Read targets from file.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train[:, 3:5] = sc_X.fit_transform(X_train[:, 3:5])\nX_test[:, 3:5] = sc_X.transform(X_test[:, 3:5])\nprint('X_train: ')\nprint(X_train)\nprint()\nprint('X_test: ')\nprint(X_test)\n", "intent": "Normalization : $$x' = \\frac{x - mean(x)}{max(x) - min(x)}$$\n"}
{"snippet": "X = pd.DataFrame(scaled_features,columns=df.columns[:-1])\nX.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "dataset = pd.read_csv('cal_housing_clean.csv')\n", "intent": "** Import the cal_housing_clean.csv file with pandas. Separate it into a training (70%) and testing set(30%).**\n"}
{"snippet": "X, _ = datasets.make_circles(n_samples=200, factor=.1, noise=.05)\nscatter(X[:, 0], X[:, 1]);\n", "intent": "Test the different clustering approaches with a \"circles\" dataset.\n"}
{"snippet": "from sklearn import lda\nlda = decomposition.LatentDirichletAllocation(n_topics=6)\nlda.fit(tfidf)\nW = lda.transform(tfidf)\nH = lda.components_\n", "intent": "Compare the results of Nonnegative Matrix Factorization (NMF) with Latent Dirichlet Allocation (LDA).\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n", "intent": "Prepare the data with Features and transformations.\n"}
{"snippet": "(U, E, V) = principal_component_analysis(XX, 2)\npca = PCA(n_components=2)\npca.fit(XX)\nif np.transpose(V).shape == pca.components_.shape:\n    print(\"Try transposing your eigenvector output\")\nassert(np.allclose([1, 1], np.abs(np.diagonal(np.inner(V, pca.components_)))))\nprint(\"Passed\")\n", "intent": "Check that your function outputs a solution that agrees with the canonical python implementation of PCA\n"}
{"snippet": "text_clf = Pipeline([('vect', CountVectorizer()),\n                      ('tfidf', TfidfTransformer()),\n                     ('clf', MultinomialNB()),\n])\ntext_clf.fit(train.summary, train.overall)\n", "intent": "choosing summary, not reviewText as input may improve the model.\n"}
{"snippet": "df['v2'] = df['v2'] + df['Unnamed: 2'].fillna('') + df['Unnamed: 3'].fillna('') + df['Unnamed: 4'].fillna('')\n", "intent": "* spam - 1\n* ham - 0\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import classification_report\ndata=pd.read_csv(\"user_to_1st_time_investor.csv\")\n", "intent": "<h1>Prepare data</h1>\n"}
{"snippet": "from sklearn.ensemble import ExtraTreesClassifier\netree = ExtraTreesClassifier()\netree.fit(X, y)\nimportant_feature_df = pd.DataFrame()\nimportant_feature_df['score'] = etree.feature_importances_\nimportant_feature_df['indexes'] = X.columns\nimportant_feature_df.sort_values(by ='score',ascending=False).head(12)\n", "intent": "<h1> Feature Importance Seclection </h1>\n"}
{"snippet": "finalDF = pd.DataFrame({'vals':data.iloc[:,2]}) \nfinalDF =finalDF.join(weekday)\nfinalDF = finalDF.join(hour)\nfinalDF = finalDF.join(month)\nfinalDF.head()\n", "intent": "Then we reconstruct the dataframe\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LinearRegression\ny_data = finalDF['vals']\nx_data = finalDF.drop('vals', 1)\nxtrain, xtest, ytrain, ytest = train_test_split(x_data, y_data)\n", "intent": "Our first model is a simple linear regression. This is seen to be a poor choice, with the reasons why being explored below.\n"}
{"snippet": "x_ETtrain, x_ETtest, y_ETtrain, y_ETtest = train_test_split(pca_X, y_data)\nET.fit(x_ETtrain, y_ETtrain)\n", "intent": "Let's train another ExtraTressRegressor on PCA data.\n"}
{"snippet": "df = pd.read_csv('../shared-resources/heights_weights_genders.csv')\nmask = df.Gender == 'Female'\ndf[mask]\ndf.describe()\n", "intent": "The scatter plot shows a linear relationship between height and weight.\n"}
{"snippet": "new_sentence = [\"The quick brown fox jumps over the lazy dog.\"]\ntfm_new = vectorizer.transform(new_sentence)\ntfidf_new = transformer.transform(tfm_new)\nnew_sentence = pd.DataFrame({\"A. tfm\":tfm_new.toarray()[0],\"B. tfidf\":tfidf_new.toarray()[0]}, index = vectorizer.get_feature_names())\nnew_sentence[new_sentence.iloc[:,1] != 0]\n", "intent": "\"The quick brown fox jumps over the lazy dog.\"\n"}
{"snippet": "def make_X(data):\n    vectorizer = CountVectorizer()\n    term_doc_matrix = vectorizer.fit_transform(data)\n    term_doc_matrix = term_doc_matrix.todense()\n    term_doc_matrix = pd.DataFrame(term_doc_matrix)\n    transformer = TfidfTransformer()\n    tfidf = transformer.fit_transform(term_doc_matrix)\n    tfidf = pd.DataFrame(tfidf.toarray())\n    return tfidf, vectorizer, transformer\n", "intent": "that gives you:\n- New X Frame\n- Vectorizer to get TFM\n- Transformer that gives you TFIDF\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test,y_train, y_test = train_test_split(X2,y, test_size=0.15, random_state=100)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n", "intent": "Train Test Split\n- Set random seed to 100 for reproducibility\n- Split data to 85% train, 15% test.\n- Other methods include k-folds crossvalidation\n"}
{"snippet": "scaler = MinMaxScaler()\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "g = sns.clustermap(cdystonia.pivot_table(index=['patient'], columns=\"week\", values=\"twstrs\").dropna(), cmap=\"summer\")\n", "intent": "We can discover structures through clustering these values in a *clustermap*:\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree = 4, include_bias = False)\nx_new = poly.fit_transform(X)\n", "intent": "We can use sklearn to examine higher polynomial features.\n"}
{"snippet": "train = pd.read_csv('train.csv')\n", "intent": "In this subsection shows an analysis of the given data\n"}
{"snippet": "def process_embarked():\n    global data_all\n    data_all.Embarked.fillna('S',inplace=True)\n", "intent": "The categorical features (Dummys)  are created.\n"}
{"snippet": "features = pd.DataFrame()\nfeatures['feature'] = train.columns\nfeatures['importance'] = model.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n", "intent": "The importance of each feature for the trained model\n"}
{"snippet": "def data_look(car_list, notcar_list):\n    data_dict = {}\n    data_dict[\"n_cars\"] = len(car_list)\n    data_dict[\"n_notcars\"] = len(notcar_list)\n    example_img = mpimg.imread(car_list[0])\n    data_dict[\"image_shape\"] = example_img.shape\n    data_dict[\"data_type\"] = example_img.dtype\n    return data_dict\n", "intent": "Get some basic information of the data set such as the number of images, and the images size\n"}
{"snippet": "img1 = cv2.imread('test_images/test1.jpg')\nimg2 = cv2.imread('test_images/test2.jpg')\nimg3 = cv2.imread('test_images/test3.jpg')\nimg4 = cv2.imread('test_images/test4.jpg')\nimg5 = cv2.imread('test_images/test5.jpg')\nimg6 = cv2.imread('test_images/test6.jpg')\n", "intent": "Using the classifer on sliding windows to detect whether an image contain cars\n"}
{"snippet": "le = LabelEncoder()\n", "intent": "le = LabelEncoder()\nfor c in X.columns:\n    X[c] = le.fit_transform(X[c])\nenc = OneHotEncoder()\nX = enc.fit_transform(X)\nonehotlabels = X\n"}
{"snippet": "((trainX, trainY), (testX, testY)) = mnist.load_data()\nprint('trainX shape:', trainX.shape)\nprint('Number of training examples:', trainX.shape[0])\nprint('Number of test examples:', testX.shape[0])\n", "intent": "We start off by loading the dataset into memory. Fortunately, Keras has an in-built API to download and load the dataset we are going to use (MNIST).\n"}
{"snippet": "milk = pd.read_csv('monthly-milk-production.csv',index_col='Month')\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "((train_X, train_Y), (test_X, test_Y)) = cifar10.load_data()\nprint('train_X shape:', train_X.shape)\nprint('Number of training examples:', train_X.shape[0])\nprint('Number of test examples:', test_X.shape[0])\n", "intent": "We start off by loading the dataset into memory. Fortunately, Keras has an in-built API to automatically download and load the CIFAR-10 dataset.\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nR_train=X_train/255.0\nR_test=X_test/255.0\n", "intent": "* Load the MNIST data set using keras and plot a few numbers\n"}
{"snippet": "y = LabelEncoder().fit_transform(y)\n", "intent": "The ```StackingClassifier``` needs numeric class labels, so we encode the original ```str``` class names. \n"}
{"snippet": "iris_dataset = datasets.load_iris()\n", "intent": "Loading the iris dataset \n"}
{"snippet": "import tensorflow as tf\niris = datasets.load_iris()\nX = iris.data[:, :2]  \nY = iris.target\nprint(\"X:\",X)\nprint(\"Y:\",Y)\n", "intent": "Logistic Regression for classification. Simplest case for 2 classes. \nLogistic Neuron\n$y = \\frac{1}{1+exp(-z)}$\n$\\frac{dy}{dz}=y(1-y)$\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.keys()\n", "intent": "Get some data to play with\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(digits.data,\n                                                    digits.target)\n", "intent": "Split the data to get going\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "1) Instantiate the model\n"}
{"snippet": "pca = PCA(n_components=2)\n", "intent": "1) Instantiate the model\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n", "intent": "With Neural Network models, its important to scale the data, again we can do this easily with SciKit Learn (I promise we'll get to TensorFlow soon!)\n"}
{"snippet": "x = np.arange(100)\ny = np.ones(100)\nsynth_data = pd.DataFrame({\n    'x': x,\n    'y': y\n})\nregr = lm.LinearRegression()\nregr.fit(synth_data.x.values.reshape(100,1), synth_data.y)\nseaborn.regplot(x=\"x\", y=\"y\", data=synth_data)\nprint(\"Pearson correlation: %.3f, Regresion coefficient: %.3f\" % (synth_data.x.corr(synth_data.y), regr.coef_[0]))\n", "intent": "Sklon regresnej krivky je uplne iny ako velkost korelacie. Len znamienko indikujuce smer je rovnake.\n"}
{"snippet": "x = np.arange(100)\ny = x + stats.norm(0,1).rvs(100)\nsynth_data = pd.DataFrame({\n    'x': x,\n    'y': y\n})\nregr = lm.LinearRegression()\nregr.fit(synth_data.x.values.reshape(100,1), synth_data.y)\nseaborn.regplot(x=\"x\", y=\"y\", data=synth_data)\nprint(\"Pearson correlation: %.3f, Regresion coefficient: %.3f\" % (synth_data.x.corr(synth_data.y), regr.coef_[0]))\n", "intent": "Tu je perfektne linearny vztah, kde je jasne vidiet, ze ten sklon je uplne iny.\n"}
{"snippet": "x = np.arange(100)\ny = x + stats.norm(0,30).rvs(100)\nsynth_data = pd.DataFrame({\n    'x': x,\n    'y': y\n})\nregr = lm.LinearRegression()\nregr.fit(synth_data.x.values.reshape(100,1), synth_data.y)\nseaborn.regplot(x=\"x\", y=\"y\", data=synth_data)\nprint(\"Pearson correlation: %.3f, Regresion coefficient: %.3f\" % (synth_data.x.corr(synth_data.y), regr.coef_[0]))\n", "intent": "Aj ked pridame trochu sumu, tak je to velmi podobne\n"}
{"snippet": "titanic = pd.read_csv('data/titanic/train.csv')\ntitanic.head()\n", "intent": "* Two-way table\n* Heatmap\n* Stacked bar plot\n* Chi-kvadrat testy\n"}
{"snippet": "import pandas as pd\nyelp = pd.read_csv('data/yelp.csv')\nyelp.head()\n", "intent": "Read **`yelp.csv`** into a pandas DataFrame and examine it.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n", "intent": "Use CountVectorizer to create **document-term matrices** from X_train and X_test.\n"}
{"snippet": "final=pd.read_csv('test.csv',usecols=['PassengerId'])\nfinal['Survived']=results\nfinal=final.set_index('PassengerId')\nfinal.to_csv('final.csv') \n", "intent": "- then we set the PassengerId as the index (check the file gender_submission.csv (from the 3 files that kaggle provides) to see the format)\n"}
{"snippet": "import numpy as np\nimport pandas as pd\ntrain = pd.read_csv('labeledTrainData.tsv', \n                    header=0,\n                    delimiter=\"\\t\", \n                    quoting=3 )\n", "intent": "To import the tsv file, it is recommended to use the pandas package. The provided file can be imported as follows\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer = 'word',   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 5000) \ntrain_data_features = vectorizer.fit_transform(clean_train_reviews)\ntrain_data_features = train_data_features.toarray()\n", "intent": "For generating a bag of words model, we will use the scikit-learn package. Use the following code\n"}
{"snippet": "scaled_x_train = scaler.fit_transform(X_train)\nscaled_x_test = scaler.transform(X_test)\n", "intent": "Keep in mind we only fit the scaler to the training data, we don't want to assume we'll have knowledge of future test data. \n"}
{"snippet": "scaler = StandardScaler()\nscaler.fit(bank.drop('Class',axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n", "intent": "** Use SciKit Learn to create training and testing sets of the data as we've done in previous lectures:**\n"}
{"snippet": "data=pd.read_csv('College_Data.csv',index_col=0)\ndata.head()\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "dataframe=pd.read_csv('KNN_Project_Data.csv')\ndataframe.head()\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaled_features=scaler.transform(dataframe.drop('TARGET CLASS',axis=1))\ndf_feat=pd.DataFrame(scaled_features,columns=dataframe.columns[:-1])\ndf_feat.head()\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "yelp=pd.read_csv('yelp.csv')\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "X=CountVectorizer().fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "X=yelp_class['text']\ny=yelp_class['stars']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "from sklearn.cross_validation import train_test_split \nX=iris.drop('species',axis=1)\ny=iris['species']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=101)\n", "intent": "** Split your data into a training set and a testing set.**\n"}
{"snippet": "bos = pd.DataFrame(boston.data)\nbos.head()\n", "intent": "Now let's explore the data set itself. \n"}
{"snippet": "from sklearn import linear_model, metrics, model_selection\nadmissions_path = Path('.', 'data', 'admissions.csv')\nadmissions = pd.read_csv(admissions_path).dropna()\nadmissions.head()\n", "intent": "The true positive and false positive rates gives us a much clearer picture of where predictions begin to fall apart.\n"}
{"snippet": "feature_columns = [\"gre\"]\nX = admissions.loc[:, feature_columns]\ny = admissions.loc[:, \"admit\"]\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=46)\nlogit_simple = linear_model.LogisticRegression().fit(X_train, y_train)\n", "intent": "**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**\n"}
{"snippet": "from sklearn import linear_model, metrics, model_selection\nadmissions_path = Path('..', 'data', 'admissions.csv')\nadmissions = pd.read_csv(admissions_path).dropna()\nadmissions.head()\n", "intent": "The true positive and false positive rates gives us a much clearer picture of where predictions begin to fall apart.\n"}
{"snippet": "X = admissions.loc[:, ['gre']]\ny = admissions['admit']\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=46)\nlogit_simple = linear_model.LogisticRegression().fit(X_train, y_train)\n", "intent": "**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = pd.DataFrame(boston.target, columns=['MEDV'])\nboston = pd.concat([X, y], axis=1)\nboston.head()\n", "intent": "Use the Boston housing data for the exercises below.\n"}
{"snippet": "mammals = (\n    pd.read_csv(mammals_path, sep='\\t', names=cols, header=0)\n    .dropna()\n    .sort_values('body')\n    .reset_index(drop=True)\n)\nmammals = mammals.loc[mammals.loc[:, 'body'] < 200, :]\nmammals.hist();\nmammals.plot(kind='scatter', x='body', y='brain');\n", "intent": "When your data is very skewed, try a log transformation.\n"}
{"snippet": "path = Path('.', 'data', 'vehicles_test.csv')\ntest = pd.read_csv(path)\n", "intent": "<a id=\"testing-preds\"></a>\n"}
{"snippet": "path = Path('..', 'data', 'vehicles_train.csv')\ntrain = pd.read_csv(path)\n", "intent": "<a id=\"cutpoint-demo\"></a>\n"}
{"snippet": "path = Path('..', 'data', 'vehicles_test.csv')\ntest = pd.read_csv(path)\n", "intent": "<a id=\"testing-preds\"></a>\n"}
{"snippet": "from sklearn.model_selection import train_test_split\n_, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itest] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"/home/gautam/datascience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "The first step with any scikit-learn (sklearn) is to **instantiate** an object. Let's **instantiate** a StandardScaler object. \n"}
{"snippet": "one_hot = OneHotEncoder(sparse=False)\n", "intent": "Just like before, we need to **instantiate** our transformer, in this case a `OneHotEncoder` object. Let's do that now:\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ncustomer_one_hot_df = pd.DataFrame(ohe.fit_transform(categorical_customer_df), columns=['region_1', 'region_2', 'region_3'])\n", "intent": "1. save the result as a new dataframe\n1. Make sure to give the columns name\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\nohe.fit(categorical_customer_df)\ncustomer_one_hot_df = pd.DataFrame(ohe.transform(categorical_customer_df),\n                                   columns=['region_1', 'region_2', 'region_3'])\n", "intent": "1. save the result as a new dataframe\n1. Make sure to give the columns name\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ncategorical_ohe_df = pd.DataFrame(ohe.fit_transform(categorical_customer_df), columns = ['region_1', 'region_2', 'region_3'])\n", "intent": "1. save the result as a new dataframe\n1. Make sure to give the columns name\n"}
{"snippet": "pca_9 = PCA(n_components=9)\nfeature_pca_np = pca_9.fit_transform(feature_df)\nfeature_pca_df = pd.DataFrame(feature_pca_np, columns = feature_df.columns)\n", "intent": "It should create 9 components. \n1. fit the PCA model using `feature_df`\n1. tranform the dataframe and assign the output to `feature_pca_np`\n"}
{"snippet": "pca_3 = PCA()\n", "intent": "It should have 6 components.\n"}
{"snippet": "pca = PCA(n_components=9)\npca = pca.fit(feature_df)\nfeature_pca_np = pca.transform(feature_df)\n", "intent": "It should create 9 components. \n1. fit the PCA model using `feature_df`\n1. tranform the dataframe and assign the output to `feature_pca_np`\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nitrain, itest = train_test_split(xrange(critics.shape[0]), train_size=0.7)\nmask=np.ones(critics.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\n", "intent": "Lets set up the train and test masks first:\n"}
{"snippet": "pca_reg_3 = PCA(n_components=6)\npca_reg_3.fit(feature_region_3_df)\nfeature_pca_reg_3_df = pca_reg_3.transform(feature_region_3_df)\npca_variance(pca_reg_3, feature_region_3_df)\n", "intent": "It should have 6 components.\n"}
{"snippet": "pca_reg_3_reduced = PCA(n_components=4)\npca_reg_3_reduced.fit(feature_region_3_df)\nfeature_pca_reg_3_reduced_np = pca_reg_3_reduced.transform(feature_region_3_df)\nfeature_pca_reg_3_reduced_df = pd.DataFrame(feature_pca_reg_3_reduced_np,\n                                            columns=['Dimension 1', 'Dimension 2',\n                                                     'Dimension 3', 'Dimension 4'])\n", "intent": "1. It should have 2 components.\n2. Assign the result of the transform to `feature_pca_reg_3_reduced_np`\n3. Convert this to a dataframe.\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ncategorical_ohe_df = pd.DataFrame(ohe.fit_transform(categorical_customer_df))\n", "intent": "1. save the result as a new dataframe\n1. Make sure to give the columns name\n"}
{"snippet": "pca_9 = PCA(n_components=9)\nfeature_pca_np = pca_9.fit_transform(feature_df)\nfeature_pca_df = pd.DataFrame(feature_pca_np, columns=feature_df.columns)\n", "intent": "It should create 9 components. \n1. fit the PCA model using `feature_df`\n1. tranform the dataframe and assign the output to `feature_pca_np`\n"}
{"snippet": "boston_df_log = pd.DataFrame(np.log(1+boston_df), columns=boston_df.columns)\n", "intent": "From looking at the charts, we can see that we want to \nDESKEW: CRIM, ZN, NOX, KN, AGE, DIS, PTRATIO, B, LSTAT\nBIMODAL: INDUS, TAX?\nBOOLEAN: CHAS\n"}
{"snippet": "iris = load_iris()\niris_features_df = pd.DataFrame(iris['data'], columns=iris['feature_names'])\n", "intent": "Import and scale the data\n"}
{"snippet": "pca = PCA(n_components=2)\n", "intent": "Now, let's visualize our data to pick a good value of k. Perform PCA and create a scatter plot of PC1 and PC2\n"}
{"snippet": "pca = PCA()\niris_features_scaled_pca = pca.fit_transform(iris_features_df_scaled)\n", "intent": "Now, let's visualize our data to pick a good value of k.\n"}
{"snippet": "boston_scaler = StandardScaler()\nX_train_scaled = boston_scaler.fit_transform(X_train)\n", "intent": "Now, let's instantiate a scaler and fit it to the training data, then transform our training data.\n"}
{"snippet": "scaled_features = scaler.fit_transform(bank.drop('Class',axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "scaler = StandardScaler()\nscaler.fit(X_t)\nX_ts = scaler.transform(X_t)\n", "intent": "Now, let's instantiate a scaler and fit it to the training data, then transform our training data.\n"}
{"snippet": "pca = PCA()\npca.fit(X_ts)\nX_tsp = pca.transform(X_ts)\n", "intent": "Just so we can visualize our data, let's do a PCA and plot our components 1 and 2, and make the color the target for our train set. \n"}
{"snippet": "boston_scaler = StandardScaler()\n", "intent": "Now, let's instantiate a scaler and fit it to the training data, then transform our training data.\n"}
{"snippet": "pca = PCA()\nboston_train_pca = pca.fit_transform(X_train_scaled)\nboston_test_pca = pca.transform(X_test_scaled)\n", "intent": "Just so we can visualize our data, let's do a PCA and plot our components 1 and 2, and make the color the target for our train set. \n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ntarget_encoded = encoder.fit_transform(target)\ntarget_encoded[:5]\n", "intent": "Label encode your target so it's numeric\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(student_data_dummies, target, test_size = .3, random_state = 42)\n", "intent": "For reproducability, use:\n* test_size = .3\n* random_state = 42\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n", "intent": "Make sure to `fit` your scaler object on your train set and use it to transform your train and test sets.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ny = digits.target == 9\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, y, random_state=0)\n", "intent": "- Type-I Error - False Positive\n- Type-II Error - False Negative\n"}
{"snippet": "X_indices = np.arange(X.shape[-1])\nselector = SelectPercentile(f_classif, percentile=10)\nselector = SelectKBest(f_classif, k=3)\nselector.fit(X, y)\n", "intent": "    SelectKBest(f_classif)\n    SelectPercentile(f_classif)\n    SelectKBest(f_regression)\n    SelectPercentile(f_regression)\n"}
{"snippet": "college = pd.read_csv('College_Data',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "SVD = TruncatedSVD(20)\nlatent_semantic_analysis = SVD.fit_transform(document_term_matrix)\n", "intent": "Take your search query, encode it the way they encoded theyir text, and take top results\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\n", "intent": "1. Create a new `LabelEncoder`. \n1. Use it to label encode the `sex` column.\n1. Save the label encoded vector back to the `titanic_df`.\n"}
{"snippet": "this_label_encoder = LabelEncoder()\nthis_label_encoder.fit(titanic_df['sex'])\ntitanic_df['sex'] = this_label_encoder.transform(titanic_df['sex'])\n", "intent": "1. Create a new `LabelEncoder`. \n1. Use it to label encode the `sex` column.\n1. Save the label encoded vector back to the `titanic_df`.\n"}
{"snippet": "this_label_encoder = LabelEncoder()\nthis_label_encoder.fit(titanic_df['embarked'])\nlabel_encoded_embarked = this_label_encoder.transform(titanic_df['embarked'])\n", "intent": "1. Create a new `LabelEncoder`. \n1. Use it to label encode the `embarked` column.\n1. Save the label encoded vector back to the `titanic_df`.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA()\nfeat_pca = pca.fit_transform(feat_scaled)\n", "intent": "* Use your **target** as your color parameter.\n"}
{"snippet": "iris = load_iris()\nX = iris.data[:,:2]\ny = iris.target\ny = np.where( y==2 , 0,y)\ny\n", "intent": "we use the poly or linear kernel to clasify the last point correctly , poly seems more right\n"}
{"snippet": "datscienc = pd.read_csv(\"data_scientist.csv\")\ndatscienc\n", "intent": "The decision boundaries are not so right , they do not tend to seperate the groups correctly\n"}
{"snippet": "def annihilate_data(X,y,num=10):\n    y_0 = len(X[y == 0])\n    y_1 = len(X[y == 1])\n    smaller = 0 if y_0 < y_1 else 1\n    idx = np.random.choice(np.where(y == smaller)[0],size = num)\n    full_idx = np.append(np.where(y != smaller)[0],idx)\n    return X[full_idx],y[full_idx]\niris = load_iris()\nX = iris.data[:,:2]\ny = iris.target\n", "intent": "The svc has two distinct groups compared to logistic regression\n"}
{"snippet": "from pandas.tools.plotting import scatter_matrix\ndf = datasets.load_iris().data[:]\ndf = pd.DataFrame(df)\nscatter_matrix(df, alpha=0.2, figsize=(6, 6), diagonal='kde');\n", "intent": "Change to the full iris data set.  datasets.load_iris().data\n"}
{"snippet": "data = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaled_df = pd.DataFrame(scaled_features, columns=df.columns[:-1])\nscaled_df.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "coeff_df = pd.DataFrame(lm.coef_, X.columns, columns=['Coefficient'])\ncoeff_df\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "from sklearn.preprocessing import  LabelEncoder\nle = LabelEncoder()  \nle.fit(df['UniqueCarrier'].head(10))\nle.transform(df['UniqueCarrier'].head(10))\n", "intent": "+ THESE ARE NOT DUMMY VARIABLES\n+ Note the fit vs fit_transform function\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"../../DS-SF-32/2008.csv\").fillna(0)\n", "intent": "http://stat-computing.org/dataexpo/2009/the-data.html\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"/Users/Reid/OneDrive/GADataScience/sandbox/2008.csv\").fillna(\"unk\")\n", "intent": "http://stat-computing.org/dataexpo/2009/the-data.html\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"../../DS-SF-32/lessons/lesson-10/DIMSIM.csv\")\npd.options.display.max_columns = 999\n", "intent": "+ Duct Tape, or...\n+ WD-40\n"}
{"snippet": "of_df = pd.read_csv(\"../../DS-SF-32/lessons/lesson-14/old-faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "path = '../../DS-SF-32/lessons/lesson-18/chipotle.tsv'\ndf = pd.read_csv(path,sep='\\t')\n", "intent": "+ hint - examine how the values are separated \n+ What's the difference between a tsv and csv?\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(ngram_range=(1,2))\nd3 = df[df[cd].notnull()].copy()\nchoices = d3[cd].fillna('')\ncv.fit(choices)\nX = cv.transform(choices)\nprint cv.vocabulary_\nX\n", "intent": "+ Use a vectorizer of your choice!\n+ Consider a dimension reduction technique! \n    + PCA? SVD? LDA?\n"}
{"snippet": "(input_train, input_test, labels_train, labels_test, dec_input_train, dec_input_test\n    ) = train_test_split(input_, labels_, dec_input_, test_size=0.1)\n", "intent": "Sklearn's <tt>train_test_split</tt> is an easy way to split data into training and testing sets.\n"}
{"snippet": "raw_data = {'first_name': ['Jason', np.nan, 'Tina', 'Jake', 'Amy'], \n        'last_name': ['Miller', np.nan, 'Ali', 'Milner', 'Cooze'], \n        'age': [42, np.nan, 24, 24, np.nan], \n        'sex': ['m', np.nan, 'f', np.nan, 'f'], \n        'preTestScore': [4, np.nan, np.nan, 2, 3],\n        'postTestScore': [25, np.nan, np.nan, 62, 70]}\ndf = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'sex', 'preTestScore', 'postTestScore'])\ndf\n", "intent": "- Numerical features\n- Categorical features\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\niris = datasets.load_iris()\nX = iris.data\nY = iris.target\n", "intent": "                                            *** Using Iris Dataset ***\n"}
{"snippet": "import pandas as pd\ndf_Iris_Full = pd.read_csv(\".//Iris Dataset//Iris.csv\")\ndf_Iris_Batch1 = pd.read_csv(\".//Iris Dataset//Iris_Batch1.csv\")\ndf_Iris_Batch2 = pd.read_csv(\".//Iris Dataset//Iris_Batch2.csv\")\n", "intent": "*** Load Iris Data and Try **\n"}
{"snippet": "boston = load_boston()\nboston_df = pd.DataFrame(boston.data, columns= boston.feature_names)\nboston_df[\"target\"] = boston.target\n", "intent": "- load the Boston dataset using scikit-learn.\n"}
{"snippet": "auto_df = pd.read_csv(\"../data/Auto.csv\", na_values=\"?\")\nauto_df = auto_df.dropna() \nprint(len(auto_df.mpg),'rows')\nauto_df.head()\n", "intent": "<img src=\"../images/3.8.jpg\">\n"}
{"snippet": "carseats = pd.read_csv('../data/carseats.csv')\ncarseats = carseats.drop(\"Unnamed: 0\", axis=1)\ncarseats.head()\n", "intent": "<img src=\"../images/3.10.jpg\">\n"}
{"snippet": "df = pd.read_csv(\"../data/Auto.csv\", na_values=\"?\").dropna()\ndf.head(2)\n", "intent": "Dataset available on http://www-bcf.usc.edu/~gareth/ISL/data.html\n"}
{"snippet": "train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n", "intent": "In Kaggle, data that can be accessed by a Kernel is saved under ``../inputs/``\nFrom there we can load it with pandas:\n"}
{"snippet": "testfile = pd.read_csv('../input/test.csv')\n", "intent": "To create the ids required for the submission we need the original test file one more time\n"}
{"snippet": "features = scaler.fit_transform(df.drop(\"Class\", axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n", "intent": "Now, load the train and test data.\n"}
{"snippet": "medians = pd.DataFrame({'Median Sales' :df.iloc[:282451].groupby(by=['Type','Dept','Store','Month'])['Weekly_Sales'].median()}).reset_index()\n", "intent": "> \nWe will take the store median in the available data as one of its properties\n"}
{"snippet": "testfile['prediction']=final_y_prediction\ntestfile['DateType'] = [datetime.strptime(date, '%Y-%m-%d').date() for date in testfile['Date'].astype(str).values.tolist()]\ntestfile['Month'] = [date.month for date in testfile['DateType']]\ntestfile['Month'] = 'Month_' + testfile['Month'].map(str)\ntestfile=testfile.merge(medians, how = 'outer', on = ['Type','Dept','Store','Month'])\ntestfile['prediction'].fillna(testfile['prediction'].median(), inplace=True) \ntestfile['Median Sales'].fillna(testfile['Median Sales'].median(), inplace=True) \ntestfile['prediction']+=testfile['Median Sales']\ntestfile.describe()\n", "intent": "Let's add the means to our testfile and then subtract the expected difference.\n"}
{"snippet": "import numpy as np\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn import datasets\nfrom sklearn import svm\niris = datasets.load_iris()\n", "intent": "Let's revisit the Iris data set:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\nclf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)\n", "intent": "A single train/test split is made easy with the train_test_split function in the cross_validation library:\n"}
{"snippet": "data = pd.read_csv(\"data/imdb.csv\", quotechar='\"', escapechar='\\\\')\ndata[\"Text\"][1]\n", "intent": "1\\. Read in the data located in `data/imdb.csv`. Don't forget to set the `quotechar` to `\"` and `escapechar` to `\\`. **`[`*`5 points`*`]`**\n"}
{"snippet": "data = pd.read_csv(\"data/imdb.csv\", ...)\n", "intent": "1\\. Read in the data located in `data/imdb.csv`. Don't forget to set the `quotechar` to `\"` and `escapechar` to `\\`. **`[`*`5 points`*`]`**\n"}
{"snippet": "import pandas as pd\nusers = pd.read_csv(\"data/users.csv\")\n", "intent": "For structured data like we have here, we will use `pandas`.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"data/mailing.csv\")\nX = data.drop(['class'], 1)\nY = data['class']\n", "intent": "First read the data in and put the target variable in `Y` and the features in `X`.\n"}
{"snippet": "features_mat = pd.DataFrame(features, columns=df.columns[:-1])\nfeatures_mat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data = pd.read_csv(\"data/categorical.csv\")\n", "intent": "We have examined two ways of dealing with categorical data: binarizing/dummy variables and numerical scaling. We will practice these here.\n"}
{"snippet": "bike= pd.read_csv(('data/Bike-Sharing-Dataset/day.csv'))\nbike['dteday'] = pd.to_datetime(bike['dteday']).dt.strftime('%j')\nbike = bike.drop(0)\nbike.head(4)\n", "intent": "We'll be using the same bike sharing data as last week!\n"}
{"snippet": "bike = pd.read_csv(('data/Bike-Sharing-Dataset/day.csv'))\nbike['dteday'] = pd.to_datetime(bike['dteday']).dt.strftime('%j')\nbike = bike.drop(0)\nbike.head(4)\n", "intent": "We'll be using the same bike sharing data as last week!\n"}
{"snippet": "bike=Table().read_table(('data/Bike-Sharing-Dataset/day.csv'))\nbike['dteday'] = pd.to_datetime(bike['dteday']).strftime('%j')\nbike = bike.drop(0)\nbike.show(4)\n", "intent": "We'll be using the same bike sharing data as last week!\n"}
{"snippet": "components = pd.DataFrame({'component 1': Y_pca[:, 0],\n                          'component 2': Y_pca[:, 1],\n                          'Candidate': y})\ncolors = candidate_colors.values()\nsns.lmplot(x = 'component 1', y = 'component 2', data = components, hue='Candidate', legend=True, fit_reg=False, \n            hue_order=candidate_colors.keys(),palette=colors, scatter_kws={'s':5}, size=7);\n", "intent": "Finally, plot the data by running the cell below.\n"}
{"snippet": "lsa_components = pd.DataFrame({'component1': Y_lsa[:, 0],\n                          'component2': Y_lsa[:, 1],\n                          'Type': y})\nsns.lmplot(x = 'component1', y = 'component2', data = lsa_components, hue='Type', legend=True, fit_reg=False, \n           hue_order=['press release', 'speech', 'statement'], palette=colors, scatter_kws={'s':5}, size=7);\n", "intent": "Now, run the next cell to plot the transformed data.\n"}
{"snippet": "lsa_components = pd.DataFrame({'component1': Y_lsa[:, 0],\n                          'component2': Y_lsa[:, 1],\n                          'Type': campaign.Type})\nsns.lmplot(x = 'component1', y = 'component2', data = lsa_components, hue='Type', legend=True, fit_reg=False, \n            scatter_kws={'s':8}, size=12);\n", "intent": "And, take a look at the exact same data, but with the document type marked in colors rather than the candidates.\n"}
{"snippet": "lsa_components = pd.DataFrame({'component1': Y_lsa[:, 0],\n                          'component2': Y_lsa[:, 1],\n                          'Candidate': y})\nsns.lmplot(x = 'component1', y = 'component2', data = lsa_components, hue='Candidate', legend=True, fit_reg=False, \n           hue_order=candidate_colors.keys(), palette=candidate_colors.values(), scatter_kws={'s':8}, size=12);\n", "intent": "Now, run the next cell to plot the transformed data.\n"}
{"snippet": "old_bailey = pd.read_csv('data/obc_1822_1832.csv', index_col='trial_id')\nold_bailey = old_bailey.loc[:, ['year', 'transcript']]\nold_bailey.head()\n", "intent": "----\nLet's get working with the data.\n"}
{"snippet": "data = pd.read_csv(\"College_Data\", index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "from sklearn.datasets import fetch_olivetti_faces, fetch_lfw_people\nfrom ipywidgets import interact\nimage_shape = (64, 64)\ndataset = fetch_olivetti_faces('./')\nfaces = dataset.data\n", "intent": "Next, we will take a look at what happens if we project some dataset consisting of human faces onto some basis we call\nthe \"eigenfaces\".\n"}
{"snippet": "eth = pd.read_csv('./data/ethereum_dataset.csv',)\nprint(eth.shape)\neth.head()\n", "intent": "Load the data `./data/ethereum_dataset` and have a look. \n"}
{"snippet": "pipeline = Pipeline([\n    ('scaling', StandardScaler()),\n])\nX = pipeline.fit_transform(eth.values)\nprint(X.shape)\n", "intent": "You can clearly see the drop.\nApply a standard scaling to the data\n"}
{"snippet": "ccfd = pd.read_csv('./data/creditcard.csv')\nccfd.head()\n", "intent": "Load the `creditcard.csv` data into a dataframe called `ccfd`\n"}
{"snippet": "pipeline = Pipeline([\n    ('scaling', StandardScaler()),\n])\npreprocessor = pipeline.fit(XTrain)\nXTrain_s = preprocessor.transform(XTrain)\nXTest_s = preprocessor.transform(XTest)\n", "intent": "Apply the usual scaling preprocessing (on both the training and test set)\n"}
{"snippet": "new_gen_feature_arr = gen_ohe.transform(df_poke_new[['Gen_Label']]).toarray()\nnew_gen_features = pd.DataFrame(new_gen_feature_arr, columns=gen_feature_labels)\nnew_leg_feature_arr = leg_ohe.transform(df_poke_new[['Lgnd_Label']]).toarray()\nnew_leg_features = pd.DataFrame(new_leg_feature_arr, columns=leg_feature_labels)\nnew_poke_ohe = pd.concat([df_poke_new, new_gen_features, new_leg_features], axis=1)\ncolumns = sum([['Name', 'Generation', 'Gen_Label'], gen_feature_labels, ['Legendary', 'Lgnd_Label'], \n               leg_feature_labels], [])\nnew_poke_ohe[columns]\n", "intent": "We can now use our previously built LabelEncoder objects and perform one hot encoding on these new data observations using the following code.\n"}
{"snippet": "from sklearn.feature_selection import VarianceThreshold\nvt = VarianceThreshold(threshold=0.15)\nvt.fit(poke_gen)\n", "intent": "Next, we want to remove features from the one hot encoded features where the variance is less than 0.15.\n"}
{"snippet": "from dateutil.parser import parse\nimport os\nimport pandas as pd\ndf = pd.read_csv('meat.csv')\ndf.date = df.date.apply(parse)\ndf = df.sort(['date'])\ndf.index = df.date\ndf = df.fillna(0)\ndf.boxplot();\n", "intent": "**Note:** these two cells can't be executed because the dataset referenced isn't available anymore, and I don't know where it comes from.\n"}
{"snippet": "train_data, test_data = train_test_split(df, test_size=0.2, random_state=0)\nX_train=train_data.sqft_living\nX_train=X_train.reshape(-1, 1)\ny_train=train_data.price\ny_train=y_train.reshape(-1, 1)\nregr = linear_model.LinearRegression()\nregr.fit(X=X_train, y=y_train)\n", "intent": "Split data into training and testing.  \nWe use random_state=0 so that everyone running this notebook gets the same results.  \n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "data_path = 'c:/Users/E411208/Documents/Python Scripts/data/Bluetooth/'\ndata = pd.DataFrame()\nfor id,file in enumerate(os.listdir(data_path)):\n    if file.endswith(\".csv\"):\n        print('Loading file ... ' + file)\n        measurement = pd.read_csv(data_path + file)\n        measurement['measurement']=id \n        data = data.append(measurement,ignore_index = True)\n", "intent": "Here we load all CSV files, one CSV file equaels one measurement\n"}
{"snippet": "data_path = 'c:/Users/E411208/Documents/Python Scripts/data/Bluetooth/'\ndata = pd.DataFrame()\nfor id,file in enumerate(os.listdir(data_path)):\n    if file.endswith(\".csv\"):\n        print('Loading file ... ' + file)\n        measurement = pd.read_csv(data_path + file)\n        measurement['measurement']=id \n        data = data.append(measurement,ignore_index = True)\n", "intent": "Here we load all CSV files, merge them and normalize the data\n"}
{"snippet": "teens_labels = pd.DataFrame(kmeans.labels_ , columns = list(['labels']))\nteens_labels['labels'] = teens_labels['labels'].astype('category')\n", "intent": "kmeans.labels_\npd.DataFrame(kmeans.labels_, columns = \"label\")\n"}
{"snippet": "lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train.author.values)\n", "intent": "We use the LabelEncoder from scikit-learn to convert text labels to integers, 0, 1 2\n"}
{"snippet": "train = pd.read_csv(os.path.join(os.curdir, \"train.csv\"))\ntest    = pd.read_csv(os.path.join(os.curdir, \"test.csv\"))\nfull = train.append( test , ignore_index = True )\ntitanic = full[ :891 ]\ndel train , test\nprint ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)\n", "intent": "Now that our packages are loaded, let's read in and take a peek at the data.\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "Z = allmyfeatures.fit_transform(train)\nprint(\"Z has type \", type(Z))\nprint(\"Z has shape \", Z.shape)\n", "intent": "Now we'll call our mega-transformer on the original data, and hopefully get a sparse matrix out that encapsulates all of the features. \n"}
{"snippet": "degree, alpha = 9, 0\nstand_combo = [(\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n               (\"stand\", StandardScaler()),\n               (\"ridge\", Ridge(alpha=alpha))]\ndeg9regpipe = Pipeline(stand_combo)\ndeg9regpipe.fit(X_train, y_train)\n", "intent": "**Part C**: Next we'll fit a high-degree polynomial model with no regularization, which is very likely to overfit. \n"}
{"snippet": "pd.DataFrame({\"feature\": dfHitters.columns[:-1], \"weight\": lasso.coef_})\n", "intent": "We need to look at the resulting estimated regression parameters from the Lasso.  \n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_valid, y_valid) = mnist.load_data()\n", "intent": "**Part A**: First, we'll load the MNIST data directly from Keras. \n"}
{"snippet": "X = CountVectorizer().fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "with open('anna.txt', 'r') as f:\n    text=f.read()\nprint(text[100:200])\n", "intent": "Read the file as text.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(.95)\npca.fit(train_img)\ncomponents = pca.transform(train_img)\napproximation = pca.inverse_transform(components)\nprint('PCA used %s components' %pca.n_components_)\n", "intent": "Hint: For computing the Cumulative Explained Variance over PCA use:\n```\npca.explained_variance_ratio_.cumsum()\n```\n"}
{"snippet": "from sklearn.cluster import KMeans\nfrom sklearn.datasets.samples_generator import make_blobs\nX, y_true = make_blobs(n_samples=6000, n_features=60, centers=6,\n                       random_state=0)\n", "intent": "Hint: Use the sklearn.datasets.make_blobs to generate the data\n"}
{"snippet": "iris = pd.read_csv('iris.csv', encoding=\"latin1\")\niris.head()\n", "intent": "Read the iris CSV file\n"}
{"snippet": "iris_features_train, iris_features_test, iris_target_train, iris_target_test = \\\n    train_test_split(features,target,test_size=0.20, random_state=0)\n", "intent": "Split the iris data into training set and test set\n"}
{"snippet": "titanic_test = pd.read_csv('test.csv', encoding='latin1')\ntitanic_test.head()\n", "intent": "Create a DataFrame with the test.csv data\n"}
{"snippet": "titanic_test.Sex.replace(to_replace=['male','female'], value=[True,False], inplace=True)\navg_age = titanic_test.Age.mean()\ntitanic_test.Age = titanic_test.Age.fillna(avg_age)\ntitanic_test = titanic_test[pd.notnull(titanic_test['Fare'])]\nprint(titanic_test.info())\n", "intent": "Clean text and missing values\n"}
{"snippet": "titanic_test.Sex.replace(['male','female'],[True,False], inplace=True)\navg_age = titanic_test.Age.mean()\ntitanic_test.Age = titanic_test.Age.fillna(avg_age)\nprint(titanic_test.info())\n", "intent": "Clean text and missing values\n"}
{"snippet": "X_reduced = PCA(n_components=3).fit_transform(plotly_iris.data)\n", "intent": "<hr style=\"border: 1px solid\n"}
{"snippet": "X = yelp_class[\"text\"]\ny = yelp_class[\"stars\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = tf.keras. \\\n                            datasets.mnist.load_data()\n", "intent": "https://en.wikipedia.org/wiki/MNIST_database\n"}
{"snippet": "unique_list = []\nfor comedian in data.columns:\n    uniques = data[comedian].nonzero()[0].size\n    unique_list.append(uniques)\ndata_words = pd.DataFrame(list(zip(full_names, unique_list)), columns=['comedian', 'unique_words'])\ndata_unique_sort = data_words.sort_values(by='unique_words')\ndata_unique_sort\n", "intent": "Tools: *Numpy, and previous ones from above*\n"}
{"snippet": "from sklearn import decomposition\npca = decomposition.PCA(n_components=20)\npca.fit(imagesWithPixelsScaled)\nimage_pca_Representation = pca.transform(imagesWithPixelsScaled)\nprint(image_pca_Representation.shape)\n", "intent": "So now we can use this, with arbirtrary numbers of dimensions, to reduce the number of features we need to do our logistic regression\n"}
{"snippet": "import pandas as pd\nresults = pd.DataFrame(estimator.cv_results_)\nresults.head()\n", "intent": "The scores from all the different sets of parameters was recorded in the estimator object and can be converted to a dataframe. \n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X)\n", "intent": "But we can do better - let's do PCA on the data `X`\n"}
{"snippet": "data = loadmat('data/ex8_movies.mat')\ndf_r = pd.DataFrame(data['R'])\ndf_y = pd.DataFrame(data['Y'])\n", "intent": "**Recommender System - Collaborative Filtering**\n"}
{"snippet": "path = os.getcwd() + '/data/ex1data1.txt'\ndata = pd.read_csv(path, header=None, names=['Population', 'Profit'])\ndata.head()\n", "intent": "**2 Linear regression with one variable**\n"}
{"snippet": "path = os.getcwd() + '/data/ex1data2.txt'\ndata2 = pd.read_csv(path, header=None, names=['Size', 'Bedrooms', 'Price'])\ndata2.head()\n", "intent": "**3 Linear regression with multiple variables**\n"}
{"snippet": "path = os.getcwd() + '/data/ex2data2.txt'\ndata2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])\norig_data2 = data2.copy()\ndata2.head()\n", "intent": "Logistic Regression with Regularization\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "[https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)\n"}
{"snippet": "pca = sklearn.decomposition.PCA(n_components=1)\npca.fit(X_norm)\n", "intent": "**Sklearn Solution**\n"}
{"snippet": "faces = loadmat('data/ex7faces.mat')\nX = pd.DataFrame(faces['X'])\nX.shape\n", "intent": "**PCA and Eigenfaces**\n"}
{"snippet": "import statsmodels.api as sm\nfrom statsmodels.formula.api import logit, glm, ols\ndat = pd.DataFrame(data, columns = ['Temperature', 'Failure'])\nlogit_model = logit('Failure ~ Temperature',dat).fit()\nprint (logit_model.summary())\n", "intent": "Lets plot this data\n"}
{"snippet": "dfhw=pd.read_csv(\"https://dl.dropboxusercontent.com/u/75194/stats/data/01_heights_weights_genders.csv\")\nprint (dfhw.shape)\ndfhw.head()\n", "intent": "(I encountered this dataset in Conway, Drew, and John White. Machine learning for hackers. \" O'Reilly Media, Inc.\", 2012.)\n"}
{"snippet": "itrain, itest = train_test_split(range(df.shape[0]), train_size=0.6)\nmask=np.ones(df.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\nmask[:10]\n", "intent": "We split the data into training and test sets...\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500)\nX = cv.fit_transform(corpus).toarray()\n", "intent": "Through tokenization\n"}
{"snippet": "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", \n                 header=None)\ndf.head()\n", "intent": "Read in Wisconsin Breast Cancer Dataset\n"}
{"snippet": "df = pd.DataFrame(data=mtcars)\ndf.head()\n", "intent": "Convert to a Pandas Dataframe for our analysis\n"}
{"snippet": "df = pd.DataFrame(adult)\n", "intent": "Convert the data to a Pandas dataframe to work with the data:\n"}
{"snippet": "import pandas as pd\npd.DataFrame(X, columns=iris.feature_names).head()\n", "intent": "**\"Observations\"** are also known as samples, instances, or records.\n"}
{"snippet": "iris = pd.read_csv(\"../../assets/datasets/iris.csv\")\n", "intent": "And read in our data:\n"}
{"snippet": "iris = pd.read_csv(\"../../assets/datasets/iris.csv\")\niris.head()\n", "intent": "First, let's import our data. We'll be using *Iris* again for this exercise. \n"}
{"snippet": "iris = pd.read_csv(\"..\\..\\assets\\datasets\\iris.csv\")\n", "intent": "First, let's import our data. We'll be using *Iris* again for this exercise. \n"}
{"snippet": "db = pd.read_csv(\"../../assets/datasets/iris.csv\")\ndb.head()\n", "intent": "We're going to load the iris data from the scikit \"datasets\" package\n"}
{"snippet": "db = pd.read_csv(\"..\\assets\\datasets\\\")\n", "intent": "We're going to load the iris data from the scikit \"datasets\" package\n"}
{"snippet": "df3 = pd.DataFrame(sales2016[\"Store_Number\"])\ndf3[\"2016Model\"] = predictions2016\ndf3[\"2016Model1\"] = predictions22016\ndf3[\"DifferenceRate\"] = (df3[\"2016Model\"] - df3[\"2016Model1\"])/df3[\"2016Model\"]\ndf3.head()\n", "intent": "df3 obsevers the difference in sales values for 20156 between our two models (with cross validation and without).\n"}
{"snippet": "df3 = pd.DataFrame(sales2016[\"Store_Number\"])\ndf3[\"2016Model\"] = predictions2016\ndf3[\"2016Model1\"] = predictions22016\ndf3[\"Difference\"] = df3[\"2016Model\"] - df3[\"2016Model1\"]\ndf3.head()\n", "intent": "df3 obsevers the difference in sales values for 20156 between our two models (with cross validation and without).\n"}
{"snippet": "import pandas as pd\nX_train, Y_train = X[:-nb_train//10], Y_train[:-nb_train//10]\nX_val, y_val = X[-nb_train//10:], y[-nb_train//10:]\nhistory = model.fit(X_train, Y_train, callbacks=[ThreeAccuracyCallback(X_val, y_val)])\npd.DataFrame(history.history).plot();\n", "intent": "The following code fits a multi-label logistic regression model to the multi-digit MNIST images and uses an `AccuracyCallback` for evaluation.\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nvae.fit(x_train, x_train,\n        shuffle=True,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_data=(x_test, x_test))\n", "intent": "That was easy. Now we'll train the thing on the usual dataset\n"}
{"snippet": "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()\n", "intent": "We'll load up our raw data set exactly as before:\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df['Sex'])\ndf['Sex_label_encoded'] = le.transform(df['Sex'])\n", "intent": "[sklearn.preprocessing.LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n"}
{"snippet": "df = pd.read_csv('../datasets/raw-material-properties.csv')\n", "intent": "Quelle: [https://openmv.net/info/raw-material-properties](https://openmv.net/info/raw-material-properties)\n"}
{"snippet": "from sklearn.preprocessing import Imputer\ndf_tmp = df.drop('Sample', axis=1)\nimp = Imputer(missing_values='NaN', strategy='mean', axis=0)\nimp.fit(df_tmp.values)\nimputed_data = imp.transform(df_tmp.values)\ndf_tmp = pd.DataFrame(imputed_data, columns=df_tmp.columns)\nprint('Fehlende Werte enthalten: {}'.format(df_tmp.isnull().values.any()))\nprint()\nprint('Anzahl Zeilen: {}'.format(df_tmp.shape[0]))\nprint('Anzahl Spalten: {}'.format(df_tmp.shape[1]))\n", "intent": "[sklearn.preprocessing.Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.33, random_state=42)\n", "intent": "[sklearn.model_selection.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n"}
{"snippet": "df = pd.read_csv('../../datasets/pima-indians-diabetes.csv')\n", "intent": "Quelle: [https://www.kaggle.com/uciml/pima-indians-diabetes-database](https://www.kaggle.com/uciml/pima-indians-diabetes-database)\n"}
{"snippet": "df=pd.read_csv(\"./datasets/spambase_test.csv\")\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure\n"}
{"snippet": "data_path_train = os.path.join(os.getcwd(), 'datasets', 'splice_train.csv')\nsplice_train = pd.read_csv(data_path_train, delimiter = ',')\nprint('Number of instances: {}, number of attributes: {}'.format(splice_train.shape[0], splice_train.shape[1]))\nsplice_train.head(5)\n", "intent": "Load the `splice_train.csv` and `splice_test.csv` into two separate dataframes. Display the shape and first 10 instances for each dataframe.\n"}
{"snippet": "ss = StandardScaler()\n", "intent": "Standardize features by removing the mean and scaling to unit variance\n"}
{"snippet": "word_freq_df = pd.DataFrame(frequency_array.toarray(),\n                            columns = vectorizer.get_feature_names())\n", "intent": "As before, this array can be turned into a dataframe.\n"}
{"snippet": "import pandas as pd\nmasses_data = pd.read_csv('mammographic_masses.data.txt')\nmasses_data.head()\n", "intent": "Start by importing the mammographic_masses.data.txt file into a Pandas dataframe (hint: use read_csv) and take a look at it.\n"}
{"snippet": "stem_vectorizer = CountVectorizer(tokenizer = tokenize_and_stem)\n", "intent": "The `tokenize` function can be added as an option for a count vectorizer.\n"}
{"snippet": "bg_df = pd.read_csv('data/boardgames.csv')\n", "intent": "Let's do it again, but with a different data set\n"}
{"snippet": "for dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\nprint (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())\n", "intent": "the embarked feature has some missing value. and we try to fill those with the most occurred value ( 'S' ).\n"}
{"snippet": "for dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\nprint (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())\n", "intent": "Fare also has some missing value and we will replace it with the median. then we categorize it into 4 ranges.\n"}
{"snippet": "path = sorted(tf.gfile.Glob(os.path.join(data_path, '*.ndjson')))[0]\nprint(tf.gfile.Open(path).read()[:1000] + '...')\n", "intent": "Let's further explore what the `NDJSON` file format is.\n"}
{"snippet": "feature_cols = ['CG_SPEND','LEADS','CAR2','CAR6','GBR2','GBR6','JPC6','JTR2','JTR6','N652','N656','SA60','SD60','ST12','ST60','TKM2','TKM6','WMN2','WMN6','Cable_Air_Type','Local Cable_Air_Type','Network_Air_Type','Regional Cable_Air_Type','Satellite_Air_Type','Spot_Air_Type','Syndication_Air_Type']\nX = tv_dummies.loc[:,feature_cols]\ny = tv_dummies.loc[:, 'CPS']\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=1)\n", "intent": "Based on the testing of appropriate feature columns above we will use CREATIVE and AIR TYPE as our feature columns\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=99)\n", "intent": "Notice that we create the train/test split first. This is because we will reveal information about our testing data if we standardize right away.\n"}
{"snippet": "feature_cols = ['gre']\nX = admissions.loc[:, feature_cols]\ny = admissions.loc[:, 'admit']\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=46)\nlogit_simple = linear_model.LogisticRegression().fit(X_train, y_train)\n", "intent": "**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**\n"}
{"snippet": "path = Path('.', 'data', 'vehicles_train.csv')\ntrain = pd.read_csv(path)\n", "intent": "<a id=\"cutpoint-demo\"></a>\n"}
{"snippet": "from sklearn import preprocessing\nscaler = preprocessing.StandardScaler()\nall_features_scaled = scaler.fit_transform(all_features)\nall_features_scaled\n", "intent": "Some of our models require the input data to be normalized, so go ahead and normalize the attribute data. Hint: use preprocessing.StandardScaler().\n"}
{"snippet": "vect = CountVectorizer()\nvect.fit(X_train)\nX_train_dtm = vect.transform(X_train)\n", "intent": "<a id='countvectorizer-model'></a>\n"}
{"snippet": "vect = CountVectorizer()\nvect.fit(X_train)\nX_train_dtm = vect.transform(X_train)\nX_test_dtm = vect.transform(X_test)\n", "intent": "greater, because now all the words like Pizza and pizza are separate\n"}
{"snippet": "vect = TfidfVectorizer(stop_words='english')\n", "intent": "<a id='yelp_tfidf'></a>\n"}
{"snippet": "vect = CountVectorizer()\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\n", "intent": "<a id='countvectorizer-model'></a>\n"}
{"snippet": "path = Path('.', 'data', 'vehicles_train.csv')\ntrain = pd.read_csv(path)\ntrain\n", "intent": "<a id=\"manual-bagged\"></a>\n"}
{"snippet": "path = Path('.', 'data', 'titanic.csv')\ntitanic = pd.read_csv(path)\n", "intent": "- Use a random forest classifier to predict who survives on the Titanic. Use five-fold cross-validation to evaluate its accuracy.\n"}
{"snippet": "path = Path('..', 'data', 'vehicles_train.csv')\ntrain = pd.read_csv(path)\n", "intent": "<a id=\"manual-bagged\"></a>\n"}
{"snippet": "path = Path('..', 'data', 'titanic.csv')\ntitanic = pd.read_csv(path)\n", "intent": "- Use a random forest classifier to predict who survives on the Titanic. Use five-fold cross-validation to evaluate its accuracy.\n"}
{"snippet": "df1 = pd.read_csv('train.csv') \ndf2 = pd.read_csv('test.csv')\n", "intent": "Reading the testing and training data set.\n"}
{"snippet": "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())\n", "intent": "Keras returns *classes* as a single column, so we convert to one hot encoding\n"}
{"snippet": "norm=preprocessing.Normalizer().fit(x)\n", "intent": "each row value / norm(row)\n"}
{"snippet": "pd.DataFrame({'cnt':data_train.tags.value_counts(),'%':data_train.tags.value_counts()/data_train.tags.count()})\n", "intent": "The distribution of tag in the training set is not highly biased.\n"}
{"snippet": "PATH_TO_TRAINING = os.path.join(os.getcwd(), \"data\", \"train.csv\")\nassert os.path.isfile(PATH_TO_TRAINING), \"file does not exist\"\noriginal_df = pd.read_csv(PATH_TO_TRAINING)\n", "intent": "Get the data and do some initial cleaning\n"}
{"snippet": "df = pd.read_csv('data.csv')\nprint('Shape of dataframe: ', df.shape)\ndf.head(50)\n", "intent": "The first step is always to clean the data and prepare it for its further analysis. \n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\nprint(x_train.shape, x_test.shape)\n", "intent": "First we have to create our train and test set. \n"}
{"snippet": "d = { 'name' : ['PSY_GANGNAM', 'KATY_ROAR', 'LMFAO_PARTY', 'EMINEM_LOVELIE', 'SHAKIRA_WAKA'], 'views' : [2951311450,2269849627,1348417123,1393162453,1542731300],'release' : ['2012-07-15','2013-09-05','2011-03-08','2010-08-05','2010-06-04']}\nd = pd.DataFrame(d)\nd\n", "intent": "The data set comprises of comments from years 2013 to 2016, while the actual release dates of these videos is around 2010-2013.\n"}
{"snippet": "Comparison = pd.DataFrame([CountVectorizer_scores.CountVectorizer, TfidfVectorizer_scores.TfidfVectorizer]) \n", "intent": "The following function compares the performance of the the two models and the above classifiers\n"}
{"snippet": "def get_stock_data(stock_name, normalized=0):\n    url = 'http://chart.finance.yahoo.com/table.csv?s=%s&a=11&b=15&c=2011&d=29&e=10&f=2016&g=d&ignore=.csv' % stock_name\n    col_names = ['Date','Open','High','Low','Close','Volume','Adj Close']\n    stocks = pd.read_csv(url, header=0, names=col_names) \n    df = pd.DataFrame(stocks)\n    date_split = df['Date'].str.split('-').str\n    df['Year'], df['Month'], df['Day'] = date_split\n    df[\"Volume\"] = df[\"Volume\"] / 10000\n    df.drop(df.columns[[0,3,5,6, 7,8,9]], axis=1, inplace=True) \n    return df\n", "intent": "Read data and store it in a dataframe. Then, use df.head() to show the first rows of the dataset. \n"}
{"snippet": "pca = PCA(n_components=2)\nprint pca.fit_transform(X)[0:5,:]\n", "intent": "__fit_transform__ is also available (and is sometimes faster).\n"}
{"snippet": "ratings = pd.read_csv(path+'ratings.csv')\nratings.head()\n", "intent": "Let us know explore the data and see .\n"}
{"snippet": "num_tops = 22\nmodel = decomposition.NMF(init=\"nndsvd\", \n                          n_components=num_tops, \n                          max_iter=400, \n                          tol=0.0001,\n                          )\nW = model.fit_transform(A)\nH = model.components_\nprint (\"Generated factor W of size %s and factor H of size %s\" % ( str(W.shape), str(H.shape) ) )\n", "intent": "Do the factorization and produce the factors\n"}
{"snippet": "df_interactions = pd.DataFrame(interactions)\ndf_interactions.head(10)\n", "intent": "Now we will start to use the dataset of user interactions dataset. Again, we can easily convert our list to a DataFrame.\n"}
{"snippet": "iso = manifold.Isomap(n_neighbors=25, n_components=2)\nX_iso = iso.fit_transform(X)\nfig = bk.figure(title='Isomap - S-Curve', x_axis_label='c1', y_axis_label='c2',\n                plot_width=750, plot_height=400)\nfig.scatter(X_iso[:, 0], X_iso[:, 1], size=8, alpha=0.8, line_color='black',\n            fill_color=pal.linear_map(y, seqcolors))\nbk.show(fig)\n", "intent": "Manifold learning algorithms, however, available in the `sklearn.manifold` submodule, are able to recover the underlying 2-dimensional manifold:\n"}
{"snippet": "X_train, X_test, y_train, y_test = cross_validation.train_test_split(np.array(X[X.columns[2:3]]),\\\n                                                                     np.array(y),\\\n                                                                     test_size=0.25, random_state=0)\n", "intent": "Keep just the attributes in the column number 2 (3rd column) and use 25% of the data as a testing set:\n"}
{"snippet": "X_train, X_test, y_train, y_test = cross_validation.train_test_split(np.array(X),\\\n                                                                     np.array(y),\\\n                                                                     test_size=0.25, \n                                                                     random_state=0)\n", "intent": "Keep all the 10 attributes and split training and testing sets:\n"}
{"snippet": "idx_train, idx_valid = cross_validation.train_test_split(boston.index, test_size=0.20)\nboston_train, boston_valid = boston.ix[idx_train], boston.ix[idx_valid]\nbostony_train, bostony_valid = data.target[idx_train], data.target[idx_valid]\n", "intent": "Split the data in **Training and Validation Set**:\n"}
{"snippet": "hidden_size = 10\nactivation_function = 'tanh'\nstart_time = time.time()\nerrors = pd.DataFrame([a for a in multitest([activation_function], [hidden_size],\n                                            slider_source.data['batch'], \n                                            slider_source.data['rate'], \n                                            X_train2, y_train2, X_test2, y_test2)])\nerrors.columns = ['activation', 'hidden', 'batch', 'rate', 'error']\nprint(\"Execution time: %s seconds\" % (time.time() - start_time))\n", "intent": "Next we perform the test with all the combination of parameters selected. It can take some time (up to 25 minutes in cpu!), please be patient.\n"}
{"snippet": "X_train, X_test, y_train, y_test = cross_validation.train_test_split(np.array(X),\\\n                                                                     np.array(y),\\\n                                                                     test_size=0.25, random_state=0)\n", "intent": "Keep all the 10 attributes and split training and testing sets:\n"}
{"snippet": "sdf = pd.DataFrame(sd, columns = df.columns.drop('TARGET CLASS'))\nsdf.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "ratings = pd.read_csv(path+'ratings.csv')\nratings.head()\n", "intent": "We're working with the movielens data, which contains one rating per row, like this:\n"}
{"snippet": "data = pd.read_csv('aquastat.csv.gzip', compression='gzip')\ndata.region = data.region.apply(lambda x: simple_regions[x])\ndata = data.loc[~data.variable.str.contains('exploitable'),:]\ndata = data.loc[~(data.variable=='national_rainfall_index')]\n", "intent": "http://www.fao.org/nr/water/aquastat/data/query/index.html\n"}
{"snippet": "print(\"Computing Isomap embedding\")\niso = manifold.Isomap(n_neighbors, n_components).fit_transform(cats_and_dogs)\nprint(\"Done.\")\nplotting(iso[:n_cats], iso[n_cats:])\n", "intent": "Let's first try the  **Isomap embedding**\n"}
{"snippet": "print(\"Computing Spectral embedding\")\nemb = manifold.LocallyLinearEmbedding(n_neighbors, n_components, eigen_solver='auto', \n                                        method = 'standard').fit_transform(cats_and_dogs)\nprint(\"Done.\")\nplotting(emb[:n_cats], emb[n_cats:])\n", "intent": "Now let's do **Spectral embedding**. There are different method for this routine. However this 'standard' is the one giving less problems!\n"}
{"snippet": "iris = load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\nprint('Class labels:', np.unique(y))\n", "intent": "We reload the Iris data set:\n"}
{"snippet": "import os\npath = os.getcwd() + '\\data\\ex2data2.txt'\ndata2 = pd.read_csv(path, header=None, names=['Test 1', 'Test 2', 'Accepted'])\ndata2.head()\n", "intent": "Similar to part 1, let's start by visualizing the data.\n"}
{"snippet": "ratings = pd.read_csv(path + 'ratings.csv')\nratings.head()\n", "intent": "We're working with the movielens data, which contains one rating per row, like this:\n"}
{"snippet": "movies = pd.read_csv(path + 'movies.csv')\nmovies.head()\n", "intent": "Just for display purposes, let's read in the movie names too.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\npipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\n                     LogisticRegression())\nparam_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(text_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n", "intent": "\\begin{equation*}\n\\text{tfidf}(w, d) = \\text{tf} \\log\\big(\\frac{N + 1}{N_w + 1}\\big) + 1\n\\end{equation*}\n"}
{"snippet": "import pandas as pd\nimport os\ndataset = pd.read_csv('data/kc_house_data.csv')\nprint(dataset.columns)\nprint()\nprint(dataset.head())\n", "intent": "The File contains 19 house features plus the price and the id columns, along with 21613 observations.\n"}
{"snippet": "movie_names = pd.read_csv(path+'movies.csv').set_index('movieId')['title'].to_dict()\n", "intent": "Just for display purposes, let's read in the movie names too.\n"}
{"snippet": "df = pd.read_csv('../data/kaggle/train.csv')\n", "intent": "Agora vamos carregar o dataset e inspecionar algumas coisas:\n"}
{"snippet": "test = pd.read_csv('data/house_prices_test.csv')\n", "intent": "__MAKING A SUBMISSION__\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_df=0.8, max_features=15000, sublinear_tf=True, use_idf=True)\n", "intent": "Mas, primeiro, precisamos criar a matriz de atributos. Para isso, vamos utilizar o TF-IDF.\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\ny_train_wide = to_categorical(y_train, num_classes)\ny_test_wide = to_categorical(y_test, num_classes)\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n", "intent": "Load in the data again as we need to keep it in image shape\n"}
{"snippet": "from urllib2 import urlopen\npath = 'faithful.txt'\nremote = urlopen('https://raw.githubusercontent.com/aidiary/PRML/master/ch9/faithful.txt')\nwith open('faithful.txt', 'w') as f:\n    f.write(remote.read())\n", "intent": "**NOTE:** In order to help you out, I will get you started by downloading the data and plotting it\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"tips.csv\")\nlen(df)\n", "intent": "As a second example, we will examine a dataset of 244 meals, with details of total meal bill and tip amount.\n"}
{"snippet": "df = pd.read_csv(\"advertising.csv\", index_col=0)\nx = df.drop(\"Sales\",axis=1)\nx.head()\n", "intent": "We will use the previous advertising dataset, which had 3 independent features: TV, Radio, Newspaper.\n"}
{"snippet": "vectorizer = CountVectorizer(stop_words=\"english\")\nX = vectorizer.fit_transform(raw_documents)\n\"and\" in vectorizer.vocabulary_\n", "intent": "We can use the built-in list of stop-words for a given language by just specifying the name of the language (lower-case):\n"}
{"snippet": "custom_stop_words = [ \"and\", \"the\", \"game\" ] \nvectorizer = CountVectorizer(stop_words=custom_stop_words)\nX = vectorizer.fit_transform(raw_documents)\n\"game\" in vectorizer.vocabulary_\n", "intent": "Or we could use our own custom stop-word list, which might be more appropriate for specific applications:\n"}
{"snippet": "seq_len = 500\ntrn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\ntest = sequence.pad_sequences(test, maxlen=seq_len, value=0)\n", "intent": "Pad (with zero) or truncate each sentence to make consistent length.\n"}
{"snippet": "import nltk\ndef stem_tokenizer(text):\n    standard_tokenizer = CountVectorizer().build_tokenizer()\n    tokens = standard_tokenizer(text)\n    stemmer = PorterStemmer()\n    stems = []\n    for token in tokens:\n        stems.append( stemmer.stem(token) )\n    return stems\n", "intent": "To use NLTK stemming with Scikit-learn, we need to create a custom tokenisation function:\n"}
{"snippet": "vectorizer = CountVectorizer(tokenizer=stem_tokenizer)\nX = vectorizer.fit_transform(raw_documents)\nterms = vectorizer.get_feature_names()\nprint(terms[200:220])\n", "intent": "Now we can use our custom tokenizer with the standard CountVectorizer approach:\n"}
{"snippet": "def lemma_tokenizer(text):\n    standard_tokenizer = CountVectorizer().build_tokenizer()\n    tokens = standard_tokenizer(text)\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    lemma_tokens = []\n    for token in tokens:\n        lemma_tokens.append( lemmatizer.lemmatize(token) )\n    return lemma_tokens\n", "intent": "We can perform lemmatisation in the same way, using NLTK with Sckit-learn:\n"}
{"snippet": "vectorizer = CountVectorizer(stop_words=\"english\",min_df = 3,tokenizer=lemma_tokenizer)\nX = vectorizer.fit_transform(raw_documents)\nprint(X.shape)\n", "intent": "Let's put all of these steps together:\n"}
{"snippet": "final_res_train = pd.DataFrame()\nfinal_res_train[\"count\"]= train_cnt.iloc[:][0]\nfinal_res_test = pd.DataFrame()\nfinal_res_test[\"count\"]= test_cnt.iloc[:][0]\n", "intent": "Resultant final dataframe to be merged to train dataset\n"}
{"snippet": "final_train=pd.concat([train_wNA, final_res_train], axis=1)\nfinal_train = final_train[(final_train['count']<80)]\nfinal_train = final_train[final_train.columns.difference([\"count\"])]\nfinal_train.to_csv('row_filter_train.csv')\nfinal_test=pd.concat([train_wNA, final_res_test], axis=1)\nfinal_test = final_test[(final_test['count']<80)]\nfinal_test = final_test[final_test.columns.difference([\"count\"])]\nfinal_test.to_csv('row_filter_test.csv')\n", "intent": "Merging the frequency column to training and testing dataset for further filtering\n"}
{"snippet": "final_res_train = pd.DataFrame()\nfinal_res_train[\"count\"]= train_cnt.iloc[:][0]\nfinal_res_train.head()\nfinal_res_test = pd.DataFrame()\nfinal_res_test[\"count\"]= test_cnt.iloc[:][0]\nfinal_res_test.head()\n", "intent": "Resultant final dataframe to be merged to train dataset\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.30)\n", "intent": "Split the data into a **training set** and a **test set**\n"}
{"snippet": "def lemma_tokenizer(text):\n    standard_tokenizer = TfidfVectorizer().build_tokenizer()\n    tokens = standard_tokenizer(text)\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    lemma_tokens = []\n    for token in tokens:\n        lemma_tokens.append( lemmatizer.lemmatize(token) )\n    return lemma_tokens\n", "intent": "As we are using the TFIDF so we are crating the tokenizer from TfidfVectorizer instead of CountVectorizer\n"}
{"snippet": "electoral_votes = pd.read_csv(\"data/electoral_votes.csv\").set_index('State')\nelectoral_votes.index.name = None\nelectoral_votes.head()\n", "intent": "*As a matter of convention, we will index all our dataframes by the state name*\n"}
{"snippet": "sac = pd.read_csv('../../assets/datasets/Sacramentorealestatetransactions.csv')\nsac.describe()\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "conmat = np.array(confusion_matrix(y_test, y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['over_200k', 'under_200k'],\n                         columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ncategoricals = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n                'native-country', 'income']\nfor x in categoricals:\n    df[x] = (pd.Series(le.fit_transform(df[x])))\ndf.head()\n", "intent": "Convert the categorical Data to numeric for our analysis. **HINT:** Refer to lesson 1.1 for writing a function of this sort\n"}
{"snippet": "X_scaled = StandardScaler().fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled, columns=X.columns)\nX_scaled\n", "intent": "First, let's standardize the data\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder()\n", "intent": " - n_values\n - n_features\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nfile = \"datatraining.txt\"\ndataframe = pd.read_csv(file)\nsubframe = dataframe.values[:,1:7]\n", "intent": "- Reference the dataset and turn it into a dataframe with relevant column identities\n- Select the columns valid for Rescaling in a subframe\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nfile = \"datatraining.txt\"\ndataframe = pd.read_csv(file)\nsubframe = dataframe.values[:,1:7]\n", "intent": "- Reference the dataset and turn it into a dataframe with relevant column identities\n- Select the columns valid for Standardization in a subframe\n"}
{"snippet": "X = subframe\nscaler = StandardScaler().fit(X)\nrescaled = scaler.transform(X)\nnp.set_printoptions(precision=3)\nprint(rescaled[0:5,:])\n", "intent": "- Use sklearn to Standardize the subframe based on a mean of 0 and stdev of 1 (experiment)\n- Display the changes in a summary\n"}
{"snippet": "from sklearn.preprocessing import Normalizer\nfile = \"datatraining.txt\"\ndataframe = pd.read_csv(file)\nsubframe = dataframe.values[:,1:7]\n", "intent": "- Reference the dataset and turn it into a dataframe with relevant column identities\n- Select the columns valid for Normalization in a subframe\n"}
{"snippet": "polls04=pd.read_csv(\"data/p04.csv\")\npolls04.State=polls04.State.replace(states_abbrev)\npolls04.set_index(\"State\", inplace=True);\npolls04.head()\n", "intent": "Let us also load in data about the 2004 elections from `p04.csv` which gets the results in the above form for the 2004 election for each state.\n"}
{"snippet": "from sklearn.preprocessing import Binarizer\nfile = \"datatraining.txt\"\ndataframe = pd.read_csv(file)\nsubframe = dataframe.values[:,1:7]\n", "intent": "- Reference the dataset and turn it into a dataframe with relevant column identities\n- Select the columns valid for Binarization in a subframe\n"}
{"snippet": "le = preprocessing.LabelEncoder()\n", "intent": "<hr />\nFirst thing we need to do is initialize the label encoder.\nThen we need to use *fit_transform()* to assign each features data to a new label.\n"}
{"snippet": "features = pd.DataFrame()\nfeatures['feature'] = train.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(['importance'], ascending=False).head(12)\n", "intent": "<hr />\n>This step is enabled by the built in '**feature\\_importances\\_**' attribute in the *ExtraTreesClassifier()* model we initialized earlier\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = np.expand_dims(x_train / np.max(x_train), -1)\nx_test = np.expand_dims(x_test / np.max(x_test), -1)\ny_train = keras.utils.to_categorical(y_train, 10)\ny_test = keras.utils.to_categorical(y_test, 10)\n", "intent": "And now -- knowing about Convolution and Pooling, let's use these layers and do some machine learning.\nFirst -- the MNIST digits.\n"}
{"snippet": "pay_features = ['pay_'+str(i) for i in range(2,7)]\npay_features_pca = PCA().fit(default[pay_features])\npay_features_pca.explained_variance_ratio_\n", "intent": "Since we know that pay_1 is important we will keep it as is.\n"}
{"snippet": "img = tf_keras.preprocessing.image.load_img('../images/monkey.jpg')\nx = tf_keras.preprocessing.image.img_to_array(img) \nx = x.reshape((1,) + x.shape)\ni = 0\nfor batch in datagen.flow(x, batch_size=1,\n                          save_to_dir='../images',\n                          save_prefix='monkey', save_format='jpeg'):\n    i += 1\n    if i > 16:\n        break      \n", "intent": "<img src=\"../../images/monkey.jpg\" width=\"400\">\n"}
{"snippet": "from urllib2 import urlopen\npath = 'faithful.txt'\nremote = urlopen('https://raw.githubusercontent.com/aidiary/PRML/master/ch9/faithful.txt')\nwith open('faithful.txt', 'w') as f:\n    f.write(remote.read())\n", "intent": "Let's start with the old faithful data you used in your homework:\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntvec = TfidfVectorizer(stop_words=None)\ntvec.fit([uber_text])\ndf  = pd.DataFrame(tvec.transform([uber_text]).todense(),\n                   columns=tvec.get_feature_names(),\n                   index=['uber_text'])\ndf.transpose().sort_values('uber_text', ascending=False).head(10).transpose()\n", "intent": "<span style=\"font-size:1.2em; color:orange\">Do the same for TFIDF</span>\n"}
{"snippet": "def references_organization(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == 'ORG' for word in parsed])\ndata['references_organization'] = data['title'].fillna(u'').map(references_organization)\ndata[data.references_organization][['title']].head()\n", "intent": "Let's see if we can find organizations in our page titles.\n"}
{"snippet": "results2012 = pd.read_csv(\"data/2012results.csv\")\nresults2012.set_index(\"State\", inplace=True)\nresults2012 = results2012.sort_index()\nresults2012.head()\n", "intent": "We load in the actual 2012 results so that we can compare our results to the predictions.\n"}
{"snippet": "classfive = DataFrame({\n    'x' : random.random(50) * 50 + 100,\n    'y' : random.random(50) * 50 + 100,\n    'label' : ['orange' for i in range(50)]\n})\n", "intent": "What happens when we introduce a new length of cluster?\n"}
{"snippet": "pca_transformer = PCA(29).fit(x_train)\nx_train_2d = pca_transformer.transform(x_train)\nx_test_2d =  pca_transformer.transform(x_test)\nfitted_lr = LogisticRegression(C=100000).fit(x_train_2d, y_train)\nprint(\"Train set score: {0:4.4}%\".format(fitted_lr.score(x_train_2d, y_train)*100))\nprint(\"Test set score: {0:4.4}%\".format(fitted_lr.score(x_test_2d, y_test)*100))\n", "intent": "<HR>\nWe need 29 principal components to capture at least 90% of the variance. They capture 91.53% of the variance.\n<HR>\n"}
{"snippet": "np.random.seed(9001)\ndf = pd.read_csv('data/dataset_hw5_2.csv')\nmsk = np.random.rand(len(df)) < 0.5\ndata_train = df[msk]\ndata_test = df[~msk]\n", "intent": "**7.0:** First task: split the data using the code provided below. \n"}
{"snippet": "data = datasets.load_iris()\n", "intent": "First, we load the famous IRIS dataset\n"}
{"snippet": "def load_dataset(split):\n    dataset = datasets.load_iris()\n    X, y = dataset['data'], dataset['target']\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=123)\n    return X_train, X_test, y_train, y_test\n", "intent": "The iris data set (https://en.wikipedia.org/wiki/Iris_flower_data_set) is loaded and split into train and test parts by the function `load_dataset`.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\nprint(data)\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"data/cell2cell_data_80_percent.csv\")\n", "intent": "1\\. Load the data into a pandas `DataFrame()`.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"data/imdb.csv\")\n", "intent": "1\\. Load the data into a pandas `DataFrame()`.\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=.75)\n", "intent": "Let's start by dividing our data into training and test sets.\n"}
{"snippet": "electoral_votes = pd.read_csv(\"data/electoral_votes.csv\").set_index('State')\nelectoral_votes.head()\n", "intent": "*As a matter of convention, we will index all our dataframes by the state name*\n"}
{"snippet": "def whiskey_distance(name, distance_measures, n):\n    distances = pd.DataFrame()\n    whiskey_location = np.where(data.index == name)[0][0]\n    for distance_measure in distance_measures:\n        current_distances = distance.squareform(distance.pdist(data, distance_measure))\n        most_similar = np.argsort(current_distances[:, whiskey_location])[0:n]\n        distances[distance_measure] = zip(data.index[most_similar], current_distances[most_similar, whiskey_location])\n    return distances\n", "intent": "What other entries do we have that are similar?\n"}
{"snippet": "vect = CountVectorizer()\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\n", "intent": "Note: Whatever you train it on it only knows those words.\n"}
{"snippet": "vect = CountVectorizer(ngram_range=(1, 2))\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape\n", "intent": "number of word phrases\n"}
{"snippet": "import pandas as pd\nyelp = pd.read_csv('/Users/jennawhite/Documents/DS-SEA-4/data/yelp.csv')\nyelp.head(1)\n", "intent": "Read `yelp.csv` into a DataFrame.\n"}
{"snippet": "tfidf_transformer = TfidfTransformer().fit(bow)\n", "intent": "Fit the transformed bag of words into the tf-idf transformer\n"}
{"snippet": "coeffecients = pd.DataFrame(lm.coef_,X.columns)\ncoeffecients.columns = ['Coeffecient']\ncoeffecients\n", "intent": "<a id='coefficients'></a>\n"}
{"snippet": "scaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(ks[[\"usd_goal_real\"]])\nscaled_data\n", "intent": "Let scale the amount of money they asked for (usd_goal_real)\n"}
{"snippet": "norm_features = preprocessing.scale(features)\npd.DataFrame(norm_features).head()\n", "intent": "|label|1|2|3|4|5|6|7|8|9|10|\n|-|-|\n|genre|'Pop_Rock'|'Electronic'|'Rap'|'Jazz'|'Latin'|'RnB'|'International'|'Country'|'Reggae'|'Blues'|\n"}
{"snippet": "scaled_features = preprocessing.scale(train_data)\npd.DataFrame(scaled_features).head()\n", "intent": "Center to the mean and component wise scale to unit variance.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target)\nprint(Xtrain.shape, Xtest.shape)\n", "intent": "Let's try a classification task on the digits. The first thing we'll want to do is split the digits into a training and testing sample:\n"}
{"snippet": "pca = PCA(n_components=.9, whiten=True)\npca.fit(X_train)\n", "intent": "**90% variance is explained with 5 features**\n"}
{"snippet": "X, y = pima_predict_df.drop(['Class variable'], 1), pima_predict_df['Class variable']\nX_scale = scale(X)\nX_train, X_test, y_train, y_test = train_test_split(X_scale, y, \n                                                    test_size=0.2, random_state=2)\ncv = ShuffleSplit(X_train.shape[0], n_iter=5, test_size=0.2, random_state=6)\n", "intent": "Use kNN to get baseline accuracy\n==\n"}
{"snippet": "X, y = pima_predict_df.drop(['Class variable'], 1), pima_predict_df['Class variable']\nX_scale = scale(X)\nX_train, X_test, y_train, y_test = train_test_split(X_scale, y, \n                                                    test_size=0.2, random_state=2)\ncv = ShuffleSplit(X_train.shape[0], n_iter=5, test_size=0.2, random_state=6)\n", "intent": "Use GaussianNb to get baseline accuracy\n==\n"}
{"snippet": "X, y = pima_predict_df.drop(['Class variable'], 1), pima_predict_df['Class variable']\nX_scale = scale(X)\nX_train, X_test, y_train, y_test = train_test_split(X_scale, y, \n                                                    test_size=0.2, random_state=2)\ncv = ShuffleSplit(X_train.shape[0], n_iter=5, test_size=0.2, random_state=6)\npca = PCA(n_components=.9).fit(X_train)\nX_train = pca.transform(X_train)\nX_test = pca.transform(X_test)\n", "intent": "**Use GaussianNb**\n==\n"}
{"snippet": "from sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\njob_Trans = mlb.fit_transform([{str(val)} for val in Marketing['job'].values])\neducation_Trans = mlb.fit_transform([{str(val)} for val in Marketing['education'].values])\nmonth_Trans = mlb.fit_transform([{str(val)} for val in Marketing['month'].values])\nday_of_week_Trans = mlb.fit_transform([{str(val)} for val in Marketing['day_of_week'].values])\n", "intent": "For other categorical variables, we encode the levels as digits using Scikit-learn's MultiLabelBinarizer and treat them as new features.\n"}
{"snippet": "df_train = pd.read_csv('./data/' + label_csv_name + '.csv', header=None,nrows =19999)\ndf_train.columns = ['id','imageId', 'url', 'labelId']\ndf_train.head()\n", "intent": "image_demo = cv2.imread('./data/base/Images/coat_length_labels/fff3f9da02b33c0d2619a1dde0914737.jpg')\nimage_demo.shape\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n         X, y, test_size=0.3, random_state=0)\n", "intent": "Splitting data into 70% training and 30% test data:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\nX_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=0.3, random_state=0)\n", "intent": "Splitting the data into 70% training and 30% test subsets.\n"}
{"snippet": "import pandas as pd\ndf_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)\ndf_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash', \n'Alcalinity of ash', 'Magnesium', 'Total phenols', \n'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', \n'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\ndf_wine.head()\n", "intent": "Loading the *Wine* dataset from Chapter 4.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nsample_submission = pd.read_csv(\"sample_submission.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "X = boston.data\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X)\nX_reduced = pca.transform(X)\nprint(\"Reduced dataset shape:\", X_reduced.shape)\n", "intent": "So we have 506 examples and 13 points.\n"}
{"snippet": "from sklearn import linear_model\ny = boston.target\nX_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(X, y, random_state=0)\nX_train_reduced, X_test_reduced, y_train_reduced, y_test_reduced = sklearn.cross_validation.train_test_split(X_reduced, y, random_state=0)\nnp.testing.assert_array_equal(y_train_reduced, y_train)\n", "intent": "Now let's run our regression, both *in the reduced space* and in the original\n"}
{"snippet": "from sklearn.datasets import fetch_lfw_people\nfaces = fetch_lfw_people(min_faces_per_person = 60)\nprint(faces.target_names)\nprint(faces.images.shape)\n", "intent": "we can use cross validation to decide on the optimal C value, and it generally depends on the kind of results wished for ...\nSVMs in Action!!\n"}
{"snippet": "digits_new = pca.inverse_transform(data_new)\nplot_digits(digits_new)\n", "intent": "Finally, we can use the inverse transform of the PCA object to construct the new digits\n"}
{"snippet": "import numpy as np \nfrom sklearn import datasets\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n", "intent": "try to do the same thing with the Iris data.\nhttps://archive.ics.uci.edu/ml/datasets/Iris\n. you can also download it using the code below:\n"}
{"snippet": "window_size = 7\nX,y = window_transform_series(series = dataset,window_size = window_size)\ntest_window_transform(dataset,window_size)\n", "intent": "With this function in place apply it to the series in the Python cell below.  We use a window_size = 7 for these experiments.\n"}
{"snippet": "img1 = mpimg.imread(img_list_1[80])\nimg2 = mpimg.imread(img_list_2[80])\nplot_2_images(img1, img2, title1='Vehicle Image', title2='Non Vehicle Image')\n", "intent": "Let's have a look at one image from each class:\n"}
{"snippet": "vect2 = CountVectorizer(stop_words='english')\nX_vect2 = vect2.fit_transform(bar['COMMENTS']) \nlda = LatentDirichletAllocation(n_topics=12, learning_method=\"batch\")\nX_lda = lda.fit_transform(X_vect2)\nlda.fit(X_vect2)\n", "intent": "Much of the old categories are distinguishable from the topics as well as the clusters. \n"}
{"snippet": "best_feature_selector = SelectKBest(f_regression, k=10)\nX_train_best = best_feature_selector.fit_transform(X_train, y_train)\nX_train_best.shape\n", "intent": "Best features determined by looking at the correlation between each feature and the output.\n"}
{"snippet": "import pandas as pd\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nsample_submission = pd.read_csv(\"sample_submission.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "pipeline = Pipeline([\n  ('feature_selection', SelectFromModel(Lasso(alpha=100.0))),\n  ('regression', LinearRegression())\n])\npipeline.fit(X_train, y_train)\n", "intent": "Best features determined using the lasso (a linear model with L1 regularisation).\n"}
{"snippet": "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\ntwenty_train = fetch_20newsgroups(subset='train',\n                                  categories=categories,\n                                  shuffle=True,\n                                  random_state=0)\ntwenty_test = fetch_20newsgroups(subset='test',\n                                 categories=categories,\n                                 shuffle=True,\n                                 random_state=0)\ntwenty_train.target_names\n", "intent": "The following code will download training and test sets containing the documents. It might take a little bit of time to fetch the data!\n"}
{"snippet": "X_new = pd.DataFrame({'TV': [50000]})\nX_new.head()\n", "intent": "Thus, we would predict Sales of **2,383 widgets** in that market.\nOf course, we can also use Statsmodels to make the prediction:\n"}
{"snippet": "collage=pd.read_csv('College_Data',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df_coeff=pd.DataFrame(data=lm.coef_,index=X_train.columns,columns=['coeff'])\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "advdata = pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nselected = titanic.copy()\nselected = selected.drop(['embarked'], axis=1)\nselected['sex'] = selected['sex'] == 'female'\nselected_data_train, selected_data_test, selected_labels_train, selected_labels_test = \\\n  train_test_split(selected, labels, test_size=0.25, random_state=42)\nrf, score = rf_eval(selected_data_train, selected_labels_train, selected_data_test, selected_labels_test)\nscore\n", "intent": "Take these with a hint of salt.  Classification without \"age\" performs just as well!\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size = 0.3,random_state=123)\n", "intent": "Score and plot. How do your metrics change? What does this tell us about the size of training/testing splits?\n"}
{"snippet": "columns = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole Weight\",\n           \"Shucked weight\", \"Viscera weight\", \"Shell weight\", \"Rings\" ]\ndf = pd.read_csv(\"abalone.data\", names=columns)\ndf.head()\n", "intent": "We'll deal with Titanic dataset again, this time with XGBClassifier\n"}
{"snippet": "pd.read_csv(\"data/sample_submission.csv\").head()\n", "intent": "What answer should look like.\n"}
{"snippet": "data_dets['versaoDocumento'] = data_dets['versaoDocumento'].fillna(0)\ndata_dets['versaoDocumento'].value_counts()\n", "intent": "We can see that versaoDocumento only contains the value 1.0 and Nan. We can replace Nan for 0\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\ndet_prod_uCom_encoder = LabelEncoder()\ndet_prod_xProd_encoder = LabelEncoder()\ninfAdic_infCpl_encoder = LabelEncoder()\ndata_dets['det.prod.uCom_encoded'] = det_prod_uCom_encoder.fit_transform(data_dets['det.prod.uCom'])\ndata_dets['det.prod.xProd_encoded'] = det_prod_xProd_encoder.fit_transform(data_dets['det.prod.xProd'])\ndata_dets['infAdic.infCpl_encoded'] = infAdic_infCpl_encoder.fit_transform(data_dets['infAdic.infCpl'])\ndata_dets.loc[:3,['det.prod.uCom','det.prod.uCom_encoded','det.prod.xProd','det.prod.xProd_encoded', 'infAdic.infCpl', 'infAdic.infCpl_encoded']]\n", "intent": "Handeling categorical columns like det.prod.uCom, det.prod.xProd. We will add an encoded column for each one\n"}
{"snippet": "future_dates = my_model.make_future_dataframe(periods=7*24, freq='h')\nfuture_dates.tail(3)\n", "intent": "Let's create the dataframe with the future dates we want to forcast. This means 7 days * 24 hours extra datepoints.\n"}
{"snippet": "future_dates = my_model.make_future_dataframe(periods=7*24, freq='h')\nfuture_dates.tail(7*24)\n", "intent": "Let's create the dataframe with the future dates we want to forcast. This means 7 days * 24 hours extra datepoints.\n"}
{"snippet": "def factor_betas(pca, factor_beta_indices, factor_beta_columns):\n    assert len(factor_beta_indices.shape) == 1\n    assert len(factor_beta_columns.shape) == 1\n    fbetas = pd.DataFrame(pca.components_.T, factor_beta_indices, factor_beta_columns)\n    return fbetas\nproject_tests.test_factor_betas(factor_betas)\n", "intent": "Implement `factor_betas` to get the factor betas from the PCA model.\n"}
{"snippet": "def factor_returns(pca, returns, factor_return_indices, factor_return_columns):\n    assert len(factor_return_indices.shape) == 1\n    assert len(factor_return_columns.shape) == 1\n    freturns = pd.DataFrame(pca.transform(returns), factor_return_indices, factor_return_columns)\n    return freturns\nproject_tests.test_factor_returns(factor_returns)\n", "intent": "Implement `factor_returns` to get the factor returns from the PCA model using the returns data.\n"}
{"snippet": "def idiosyncratic_var_matrix(returns, factor_returns, factor_betas, ann_factor):\n    print(factor_betas)\n    common_returns_ = pd.DataFrame(np.dot(factor_returns, factor_betas.T), returns.index, returns.columns)\n    residuals_ = (returns - common_returns_)\n    return pd.DataFrame(np.diag(np.var(residuals_))*ann_factor, returns.columns, returns.columns)\nproject_tests.test_idiosyncratic_var_matrix(idiosyncratic_var_matrix)\n", "intent": "Implement `idiosyncratic_var_matrix` to get the idiosyncratic variance matrix.\n"}
{"snippet": "def idiosyncratic_var_vector(returns, idiosyncratic_var_matrix):\n    x = pd.DataFrame(np.diag(idiosyncratic_var_matrix),  returns.columns)\n    print(x)\n    return x\nproject_tests.test_idiosyncratic_var_vector(idiosyncratic_var_vector)\n", "intent": "Implement `idiosyncratic_var_vector` to get the idiosyncratic variance Vector.\n"}
{"snippet": "def sharpe_ratio(factor_returns, annualization_factor):\n    df_sharpe = pd.DataFrame(data=annualization_factor*factor_returns.mean()/factor_returns.std(),columns=['Sharpe Ratio'])\n    print(df_sharpe['Sharpe Ratio'])\n    return df_sharpe['Sharpe Ratio']\nproject_tests.test_sharpe_ratio(sharpe_ratio)\n", "intent": "The last analysis we'll do on the factors will be sharpe ratio. Implement `sharpe_ratio` to calculate the sharpe ratio of factor returns.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"adult.data.csv\", header=None, names=COLUMNS)\n", "intent": "We load the data into pandas because it is small enough to manage in memory, and look at some properties.\n"}
{"snippet": "car_sales = pd.read_csv('car_data.csv')\n", "intent": "* pandas to_datetime\n"}
{"snippet": "train = pd.read_csv('resources/Machine Learning Sections/Logistic-Regression/titanic_train.csv')\n", "intent": "Linear vs logistic slope\n"}
{"snippet": "recipes = pd.read_csv('recipes_muffins_cupcakes.csv')\nrecipes\n", "intent": "__Step 2:__ Import Data\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(centers=2, random_state=0, cluster_std=0.7)\nprint('X ~ n_samples x n_features:', X.shape)\nprint('y ~ n_samples:', y.shape)\nprint('\\nFirst 5 samples:\\n', X[:5, :])\nprint('\\nFirst 5 labels:', y[:5])\n", "intent": "First, we will look at a two class classification problem in two dimensions. We use the synthetic data generated by the ``make_blobs`` function.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nseed=8\nval_size=0.3\nD1_train, D1_val,= train_test_split(D1, test_size=val_size, random_state=seed)\nD2_train, D2_val,= train_test_split(D2, test_size=val_size, random_state=seed)\n", "intent": "* Generate a training and test partition (70%-30%)\n"}
{"snippet": "seed=8\nval_size=0.3\nnewdata_train, newdata_val,= train_test_split(newdata, test_size=val_size, random_state=seed)\n", "intent": "Separamos la data en training (70%) y test (30%)\n"}
{"snippet": "df = pd.read_csv(\"College_Data\",index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head(5)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.3, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"adult.test.csv\", header=None, names=COLUMNS)\n", "intent": "We load the data into pandas because it is small enough to manage in memory, and look at some properties.\n"}
{"snippet": "X= ad_data[[ 'Age', 'Area Income', 'Daily Time Spent on Site',\n       'Daily Internet Usage', 'Male']]\ny=ad_data[[\"Clicked on Ad\"]]\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.3, random_state=101)\n", "intent": "** Split the data into training set and testing set using train_test_split**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()  \n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "X = cv.fit_transform(X)   \n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "X = yelp_class['text']\ny= yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(\n  X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "f = 'alice.txt'\nwith open(f, 'r') as fp:\n    txt = fp.read()\n", "intent": "- setting up the data\n"}
{"snippet": "rgrs = Lasso(alpha=alpha_optim)\nrgrs.fit(x_tr, y_tr)\nCFS = pd.DataFrame({'coefficients':range(len(rgrs.coef_)), 'VAL':rgrs.coef_})\nCFS = CFS.sort_values(['VAL'])[::-1]\nCFS[0:50].plot(x='coefficients',y='VAL',kind='bar',figsize=(12,6), cmap='Greys')\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "from sklearn import preprocessing\ntotdata = float(len(x))\nscaler = preprocessing.StandardScaler()\nx = scaler.fit_transform(x)\nx_tr = x[0:int(totdata*0.8)]\nx_ts = x[int(totdata*0.8):]\ny_tr = y[0:int(totdata*0.8)]\ny_ts = y[int(totdata*0.8):]\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "df = pd.read_csv('data/hw2data.csv')\ndf.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "coeff = pd.DataFrame({'Coefficient Number': range(len(our_lasso.coef_)), 'Value': our_lasso.coef_})\ntop_sorted = coeff.sort_values('Value', ascending=False)[:100]\ntop_sorted.plot(x='Coefficient Number', y='Value', kind='bar', figsize=(20,10))\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntraining_inputs, testing_inputs, training_classes, testing_classes = train_test_split(all_inputs, all_classes, train_size=0.75, random_state=1)\n", "intent": "Now split the data.\n"}
{"snippet": "filename = './japanese_credit.data'\ndf = pd.read_csv(filename, index_col=None, header=None)\n", "intent": "Dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/\n"}
{"snippet": "labelencoder = LabelEncoder()\ntarget_label = labelencoder.fit_transform(data.LotShape) \ntarget_label[0:30]\n", "intent": "One-hot and label encoding\n"}
{"snippet": "train, test = train_test_split(data, test_size = 0.4)\n", "intent": "Splitting dataset into train and test\n"}
{"snippet": "df=pd.read_csv(\"train.csv\")\ndf.head()\n", "intent": "Dataset link: https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/\n"}
{"snippet": "data = pd.read_csv('train.csv')\n", "intent": "Load the \"train.csv\" dataset.\n"}
{"snippet": "le = LabelEncoder()\nX['Sex'] = le.fit_transform(X['Sex'])\n", "intent": "Convert Sex column into 1/0 using Label Encoding.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n", "intent": "Split dataset into train and test with test size as 20% of total dataset.\n"}
{"snippet": "df = pd.read_csv('creditcard.csv')\n", "intent": "Let's load the data in order to do some analysis\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=0)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "df = pd.read_csv('../../data/ads_hour.csv',index_col=['Date'], parse_dates=['Date'])\n", "intent": "We will take real time-series data of total ads watched by hour in one of our games.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = np.asmatrix(X).astype(np.float)\nX = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=0)\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "df = pd.read_csv('./wine/winequality-white.csv', header=0, sep=';')\n", "intent": "Data: https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n"}
{"snippet": "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2)\nX += np.random.random(X.shape)\ndatasets = [make_moons(noise=0.1), make_circles(noise=0.1, factor=0.5), (X, y)]\n", "intent": "<h1 align=\"center\">2D datasets</h1> \n"}
{"snippet": "import shutil\nshutil.rmtree('data/sines', ignore_errors=True)\nos.makedirs('data/sines/')\nnp.random.seed(1) \nfor i in xrange(0,10):\n  to_csv('data/sines/train-{}.csv'.format(i), 1000)  \n  to_csv('data/sines/valid-{}.csv'.format(i), 250)\n", "intent": "<h3> Cloud ML Engine </h3>\nNow to train on Cloud ML Engine with more data.\n"}
{"snippet": "df = pd.read_csv('weights.csv', sep=';', index_col=0)\ndf.head()\n", "intent": "Data -  [link](https://www.dropbox.com/s/8srfeh34lnj2cb3/weights.csv?dl=0)\n"}
{"snippet": "from sklearn.decomposition import PCA  \nmodel = PCA(n_components=2)            \nmodel.fit(X_iris)                      \nX_2D = model.transform(X_iris)         \n", "intent": "Following the sequence of steps outlined earlier, we have:\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nrnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\nrnd_clf.fit(iris[\"data\"], iris[\"target\"])\nfor name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n    print(name, score)\n", "intent": "Now, lets try using the Random Forest Classifier to tell us which of features from the Iris Setosa data set are the most useful.\n"}
{"snippet": "import numpy as np \nimport pandas as pd \nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris[\"data\"]\ny = iris[\"target\"]\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nprint('The accuracy of the Random Forest classifier on training data is {:.2f}'.format(random_forest.score(X_train, y_train)))\nprint('The accuracy of the Random Forest classifier on test data is {:.2f}'.format(random_forest.score(X_test, y_test)))\n", "intent": "Apply Gradient Boost, Random Forest and Ada Boost to the iris data set.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder = LabelEncoder()\nstate = dataset.State  \ndataset['State_Encoded'] = labelencoder.fit_transform(state.values)\n", "intent": "The `State` column contains categorical features. This needs to be converted into Dummy Variables\n"}
{"snippet": "ss = StandardScaler()\nX_train_scaled=ss.fit_transform(X_train)\nX_test_scaled= ss.transform(X_test)\n", "intent": "Now train a simple linear regression on scaled data:\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"higgs.csv\")\nX = data.drop(\"Label\",1)\ny = data.Label.values\n", "intent": "__Before you begin__ - extract (un-zip) __`./higgs.csv.zip`__ contents to this location. There should be just one file (`higgs.csv`).\n"}
{"snippet": "import pandas as pd\nfrom IPython.display import FileLink\ndef save_results(filename, y_ans):\n    answer_dataframe = pd.DataFrame(columns=[\"ID\", \"ans\"])\n    answer_dataframe['ID'] = range(0,len(y_ans))\n    answer_dataframe['ans'] = y_ans\n    answer_dataframe.to_csv('{}'.format(filename), index=False)\n    return FileLink('{}'.format(filename))\n", "intent": "Saving you results to file.\n"}
{"snippet": "parksInfo_sub = parksInfo_dummied[['ACRES','SUM_TOTPOPSVCA','total_amenities',]]\ny = np.array(parksInfo_dummied.tweet_class)\nX = parksInfo_sub\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42)\nparam_grid = {'n_estimators': range(1,50)}\nrf = RandomForestClassifier()\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid)\ngrid_search.fit(train_X, train_y)\ngrid_search.best_params_\n", "intent": "**This hyperparameterized-RF classification model with a reduced \nLet's look at the accuracy with the 3 initial variables of interest...\n"}
{"snippet": "url = \"https://www.dropbox.com/s/2qe1mai9eyd4mqa/abstracts1.zip?dl=1\"  \nimport urllib.request\nu = urllib.request.urlopen(url)\ndata = u.read()\nu.close()\nwith open(\"abstracts1.zip\", \"wb\") as f :\n    f.write(data)\n", "intent": "We downloaded ~50k abstracts on various Physics topics from arxiv.\nThe code below helps you to donload zipped abstracts from dropbox and unzip them.\n"}
{"snippet": "def import_data(filename):\n    data = pd.read_csv(filename)\n    return data\n", "intent": "Import training data:\n"}
{"snippet": "def preprocess_names(data):\n    data['Title'] = data.Name.apply(lambda x: re.search(\", (.*?)\\.\", x).group(1))\n    data['NameLength'] = data.Name.apply(len)\n    data = data.drop('Name', axis=1)\n    le = LabelEncoder()\n    data.Title = le.fit_transform(data.Title)\n    return data\n", "intent": "`Name` values include titles. Otherwise, they are unique, so let's drop `Name` and describe it with the title and the length of the name.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(scaled_X, scaled_y, test_size=0.2)\n", "intent": "... and repeat the training\n"}
{"snippet": "diabetes_4top = pd.DataFrame(data=feat, columns=['Glucose', 'Insulin', 'BMI', 'Age'])\n", "intent": "We can see the top 4 performing features are **Glucose**, **Insulin**, **BMI** and **Age**\n"}
{"snippet": "from keras.datasets import cifar10\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n", "intent": "- The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes\n<img src ='imgs/cifar.png'/>\n"}
{"snippet": "df = pd.read_csv('../../data/telecom_churn.csv')\n", "intent": "In the first article, we looked at the data on customer churn for a telecom operator. We will load again that dataset into a `DataFrame`:\n"}
{"snippet": "wine = pd.read_csv('./datasets/winequality_merged.csv')\nwine.head(2)\n", "intent": "**Load the wine dataset and pull out red vs. white as the true clusters.**\n"}
{"snippet": "boston = learn.datasets.load_dataset('boston')\nx, y = boston.data, boston.target\ny.resize( y.size, 1 ) \ntrain_x, test_x, train_y, test_y = cross_validation.train_test_split(\n                                    x, y, test_size=0.2, random_state=42)\nprint( \"Dimension of Boston test_x = \", test_x.shape )\nprint( \"Dimension of test_y = \", test_y.shape )\nprint( \"Dimension of Boston train_x = \", train_x.shape )\nprint( \"Dimension of train_y = \", train_y.shape )\n", "intent": "<h2>Import the Boston Data</h2>\n<br />\nWe don't worry about adding column names to the data.\n"}
{"snippet": "scaler = preprocessing.StandardScaler( )\ntrain_x = scaler.fit_transform( train_x )\ntest_x  = scaler.fit_transform( test_x )\n", "intent": "We scale the inputs to have mean 0 and standard variation 1.\n"}
{"snippet": "bostonDF = pd.DataFrame( boston.data )\nbostonDF.head()\n", "intent": "<h2>Convert the boston data into a panda data-frame</h2>\n"}
{"snippet": " X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n      X, y, test_size=0.2, random_state=42)\n", "intent": "Split the data into training and test data.\n"}
{"snippet": "scaler = preprocessing.StandardScaler( )\nX_train = scaler.fit_transform( X_train )\nX_train\n", "intent": "Scale the X data to 0 mean and unit standard deviation\n"}
{"snippet": "scaler = MinMaxScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n", "intent": "We use Sklearn's MinMaxScaler to normalize our data between 0 and 1\n"}
{"snippet": "y_all = LabelEncoder().fit_transform(y_all)\ny_all = np_utils.to_categorical(y_all)\nX_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, \n                                                    test_size=0.2, random_state=23, \n                                                    stratify=y_all)\n", "intent": "One-Hot_encode the labels, then create a stratified train/validattion split.\n"}
{"snippet": "from sklearn.datasets import load_iris, load_breast_cancer, load_wine\niris = load_iris()\ncancer = load_breast_cancer()\nwine = load_wine()\nprint(type(iris))\n", "intent": "load iris, breast cancer and wine standard datasets and assign them to iris, cancer and wine.\n"}
{"snippet": "df = pd.read_csv('../../data/medium_posts.csv', sep='\\t')\n", "intent": "We will predict the daily number of posts published on [Medium](https://medium.com/).\nFirst, we load our dataset:\n"}
{"snippet": "x = StandardScaler().fit_transform(df)\nX = pd.DataFrame(x, columns = wine.feature_names )\nX.head()\n", "intent": "* __Standardize the dataset__\n"}
{"snippet": "y = pd.DataFrame(wine.target, columns =['class'])\n", "intent": "* Perform using Singular Value Decomposition (**SVD**)\n* For simplicity chose two components\n* Implement first for wine dataset\n"}
{"snippet": "x = StandardScaler().fit_transform(df)\nX = pd.DataFrame(x, columns =wine.feature_names )\nX.head()\n", "intent": "* __Standardize the dataset__\n"}
{"snippet": "from sklearn import datasets, linear_model\ndiabetes = datasets.load_diabetes()\nprint(diabetes.DESCR)\n", "intent": "Let's now visit regression, which is a linear least squares problem, using the diabetes dataset that is part of Scikit-learn.  \n"}
{"snippet": "from sklearn.decomposition import PCA\nX = np.array( [ [1.0,0], [0,1.0], [1,1]])\npca = PCA(n_components=2) \npca.fit(X)\nprint(\"singular values\")\nprint(pca.singular_values_)  \nprint(\"explained variance ratio\")\nprint(pca.explained_variance_ratio_) \n", "intent": "Lets use sci-kit learn to find the PCA decomposition first\n"}
{"snippet": "m_cols = ['movie_id', 'title', 'release_date']\nmovie_info = pd.read_csv('data/ml-100k/movie-info.dat', sep='|', names=m_cols, usecols=range(3))\nmovie_info.head()\n", "intent": "Let's see which are the most highly rated movies that we can recommend.  First, let's load the movie information into another dataframe\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(min_df=1)\nposts = [post1, post2, post3, post4, post5]\nX_train = vectorizer.fit_transform(posts)\n", "intent": "Now we will train our vectorizer\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components = 2) \nX2D = pca.fit_transform(X)\nprint(pca.components_[0]) \nprint(pca.components_.T[:,0]) \n", "intent": "* Uses SVD decomposition as before.\n* You can access each PC using *components_* variable. (\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_mldata\nmnist = fetch_mldata('MNIST original')\nX, y = mnist[\"data\"], mnist[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nX = X_train\npca = PCA()\npca.fit(X)\nd = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1\nd\n", "intent": "* Example applying PCA to MNIST dataset with 95% preservation = results in ~150 features (original = 28x28 = 784)\n"}
{"snippet": "max_users = 70\nmax_movies = 50\nclustered = pd.concat([most_rated_movies_1k.reset_index(), pd.DataFrame({'group':predictions})], axis=1)\nhelper.draw_movie_clusters(clustered, max_users, max_movies)\n", "intent": "To visualize some of these clusters, we'll plot each cluster as a heat map:\n"}
{"snippet": "rnd_pca = PCA(\n    n_components=154, \n    random_state=42, \n    svd_solver=\"randomized\")\nX_reduced = rnd_pca.fit_transform(X_mnist)\n", "intent": "* Stochastic algorithm, quickly finds approximation of 1st d components. Dramatically faster.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfTransformer\ntf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\nX_train_tf = tf_transformer.transform(X_train_counts)\nX_train_tf.shape\n", "intent": "From occurrences to frequencies\n"}
{"snippet": "tfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nX_train_tfidf.shape\n", "intent": "Or we can call fit and transform using fit_transform\n"}
{"snippet": "scaler = StandardScaler()\nsvm_clf1 = LinearSVC(C=1, loss=\"hinge\")\nscaled_svm_clf1 = Pipeline([\n(\"scaler\", scaler),\n    (\"linear_svc\", svm_clf1)\n])\nscaled_svm_clf1.fit(X,y)\n", "intent": "Step 2: Do feature scaling of the features using StandardScaler() and model the SVM Linear classifier\n"}
{"snippet": "doc = nlp('I get a discount on newspapers')\ntags = {}\nfor word in doc:\n    tags[word.orth_] = {'lemma': word.lemma_, 'pos (coarse)': word.pos_, 'pos (fine)':word.tag_}\npd.DataFrame(tags).T\n", "intent": "<a name=\"section2\"></a>\nLets see how we can access parts of speech with spacy:\n"}
{"snippet": "pd.DataFrame(np.transpose([X.columns, lm.coef_]), columns=[\"Variables\", \"Coefficients\"])\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "backorder_test=pd.read_csv(\"Kaggle_Test_Dataset_v2.csv\")\ntest = (backorder_test\n              .replace(['Yes', 'No'], [1, 0]))               \n", "intent": "Apply ML model to the test set\n"}
{"snippet": "df_d= pd.get_dummies(df[['age','job','education','day_of_week','y']], drop_first = True)\nLR=LogisticRegression()\nX1 = df_d.drop('y', axis =1)\ny1 = df_d['y']\nx_train1, x_test1, y_train1, y_test1 = train_test_split(X1,y1, random_state =42)\nLR.fit(x_train1, y_train1)\n", "intent": "**Build a Model**  \n"}
{"snippet": "data_raw = pd.read_csv(\"datasets/titanic_train.csv\", index_col='PassengerId')\ndata_validate = pd.read_csv(\"datasets/titanic_test.csv\", index_col='PassengerId')\ndata_raw.sample(10)\n", "intent": "Let's take a view into the dataset itself.\n"}
{"snippet": "cluster.fillna('').head()\n", "intent": "And the actual ratings in the cluster look like this:\n"}
{"snippet": "features_train, features_test, labels_train, labels_test = train_test_split(data_features, data_labels,\n                                                                            test_size=0.2, random_state=42)\n", "intent": "Splitting up the labels and features into training and testing sets.\n"}
{"snippet": "import pandas as pd\nseeds_df = pd.read_csv('../datasets/seeds.csv')\nseeds_df\n", "intent": "**Step 1:** Load the dataset _(written for you)_.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../datasets/fish.csv')\ndf.head()\n", "intent": "**Step 1:** Load the dataset _(this bit is written for you)_.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('../datasets/fish.csv')\ndf.head()\n", "intent": "**Step 1:** Load the dataset, extracting the species of the fish as a list `species` _(done for you)_\n"}
{"snippet": "data = pd.read_csv(\"datasets/titanic_train.csv\", index_col='PassengerId')\ndata_validate = pd.read_csv(\"datasets/titanic_test.csv\", index_col='PassengerId')\n", "intent": "Let's take a view into the dataset itself.\n"}
{"snippet": "data.loc[:,'Embarked']=data.Embarked.map({'S':0,'C':1,'Q':2})\ndata.loc[:,'Embarked']=data.Embarked.fillna(0)\ndata.loc[:,'Embarked']=data.Embarked.astype('int')\n", "intent": "There's a couple missing values for Embarked. We'll use the most common value (S/0). \n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nimport sklearn.metrics.pairwise as smp\nimport numpy as np\nX_train, X_test, y_train, y_test = train_test_split(X, ng_train.target, test_size=0.3)\n", "intent": "Let's try some simple classification on the result LSI vectors for the 20 NG set and see how we do:\n"}
{"snippet": "for dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')    \nprint (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())\n", "intent": "the embarked feature has some missing value. and we try to fill those with the most occurred value ( 'S' ).\n"}
{"snippet": "for dataset in full_data: \n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())    \ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)    \nprint (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())\n", "intent": "Fare also has some missing value and we will replace it with the median. then we categorize it into 4 ranges.\n"}
{"snippet": "import pandas as pd\nurl = '../data/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer\n", "intent": "1. K-means clustering\n2. Clustering evaluation\n3. DBSCAN clustering\n"}
{"snippet": "coefs = pd.DataFrame(coef_CV_lambda,columns=range(1,10),index=np.arange(.0001,.01,.0001))\nprint(coefs.loc[lambda_opt])\n", "intent": "Parameters for optimal lambda\n"}
{"snippet": "token_text = [token.orth_ for token in parsed_review]\ntoken_pos = [token.pos_ for token in parsed_review]\npd.DataFrame(zip(token_text, token_pos),\n             columns=['token_text', 'part_of_speech'])\n", "intent": "What about part of speech tagging?\n"}
{"snippet": "token_lemma = [token.lemma_ for token in parsed_review]\ntoken_shape = [token.shape_ for token in parsed_review]\npd.DataFrame(zip(token_text, token_lemma, token_shape),\n             columns=['token_text', 'token_lemma', 'token_shape'])\n", "intent": "What about text normalization, like stemming/lemmatization and shape analysis?\n"}
{"snippet": "token_entity_type = [token.ent_type_ for token in parsed_review]\ntoken_entity_iob = [token.ent_iob_ for token in parsed_review]\npd.DataFrame(zip(token_text, token_entity_type, token_entity_iob),\n             columns=['token_text', 'entity_type', 'inside_outside_begin'])\n", "intent": "What about token-level entity analysis?\n"}
{"snippet": "token_text = [token.orth_ for token in parsed_review]\ntoken_pos = [token.pos_ for token in parsed_review]\npd.DataFrame({'token_text':token_text,'part_of_speech':token_pos})\n", "intent": "What about part of speech tagging?\n"}
{"snippet": "token_lemma = [token.lemma_ for token in parsed_review]\ntoken_shape = [token.shape_ for token in parsed_review]\npd.DataFrame({'token_text':token_text,'token_lemma':token_lemma,'token_shape':token_shape})\n", "intent": "What about text normalization, like stemming/lemmatization and shape analysis?\n"}
{"snippet": "token_entity_type = [token.ent_type_ for token in parsed_review]\ntoken_entity_iob = [token.ent_iob_ for token in parsed_review]\npd.DataFrame({'token_text':token_text,'entity_type':token_entity_type,'inside_outside_begin':token_entity_iob})\n", "intent": "What about token-level entity analysis?\n"}
{"snippet": "partA_loc = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\npartA = pd.read_csv(partA_loc)\npartA.head() \n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "pca = PCA(n_components=50)\nX_train_pca = pca.fit_transform(X_train_robust, y = y_train) \nX_val_pca = pca.transform(X_val_robust)\nsvc_radial_basis = SVC(kernel='rbf', random_state = 0)\nsvc_radial_basis.fit(X_train_pca, y_train)\nprint('RBF accuracy - validation set: {:.5f}'.format(svc_radial_basis.score(X_val_pca, y_val)))\n", "intent": "<h2>SVC attempt with PCA Dimensionality Reduction</h2>\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape\n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "labels.pivot_table(\n    index = 'cluster',\n    aggfunc = 'count'\n)\n", "intent": "Let's see how many stations in each cluster\n"}
{"snippet": "token_list=word_tokenize(book.read())\n", "intent": "and create the list of tokens\n"}
{"snippet": "corpus = \" \".join([book.read() for book in books])\n", "intent": "and we put them together in a large corpus\n"}
{"snippet": "train_sessions = train_dataset[site_cols].astype(str).apply(lambda s: ' '.join(s), axis=1)\ntest_sessions = test_dataset[site_cols].astype(str).apply(lambda s: ' '.join(s), axis=1)\nvec = TfidfVectorizer(ngram_range=(1, 6), max_features=200000, stop_words=['0'])\nvec = vec.fit(test_sessions.append(train_sessions))\ntrain_v = vec.transform(train_sessions)\ntest_v = vec.transform(test_sessions)\n", "intent": "Build Tf-Idf features based on sites. You can use `ngram_range`=(1, 3) and `max_features`=100000 or more\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\nlabel = LabelEncoder()\nX[:,0] = label.fit_transform(X[:,0])\nX\n", "intent": "<img src=\"images/ed.PNG\"/>\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ndummy_features = ohe.fit_transform(np.r_[y_train, y_test].reshape(-1, 1))\ny_train, y_test = dummy_features[:len(y_train)], dummy_features[len(y_train):]\n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "dat = pd.read_csv(\"datasets/regionalhappy.csv\")\n", "intent": "Load Data\n=========\nNext, let's load the data.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    customer_data.drop(['BikeBuyer'], axis=1),\n    customer_data['BikeBuyer'],\n    test_size=0.3)\n", "intent": "The first step is to split the data into training and testing divisions.\n"}
{"snippet": "nr.seed(1115)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 300)\nX_train = Features[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nX_test = Features[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])\n", "intent": "Now, execute the code in the cell below to create training and testing splits of the dataset. \n"}
{"snippet": "vect = TfidfVectorizer(stop_words='english')\ndtm = vect.fit_transform(yelp.text)\nfeatures = vect.get_feature_names()\ndtm.shape\n", "intent": "**More details:** [TF-IDF is about what matters](http://planspace.org/20150524-tfidf_is_about_what_matters/)\n"}
{"snippet": "nr.seed(1115)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 300)\nx_train = Features[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nx_test = Features[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])\n", "intent": "Now, execute the code in the cell below to creat training and testing splits of the dataset. \n"}
{"snippet": "pca_mod = skde.PCA()\npca_comps = pca_mod.fit(x_train)\npca_comps\n", "intent": "The code in the cell below computes the principle components for the training feature subset. Execute this code:\n"}
{"snippet": "pd.DataFrame(lm.coef_, X.columns, columns=['Coefficients'])\n", "intent": "That was almost too easy. Let's take a look at the coefficents of the model.\n"}
{"snippet": "pd.DataFrame(lm.coef_, X.columns, columns=['Coefficients'])\n", "intent": "The question was whether company resources should be biased towards the website or the app. Recall this table of the coefficients:\n"}
{"snippet": "X = data.drop(['Clicked on Ad', 'Timestamp'], axis=1)\ny = data['Clicked on Ad']\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n", "intent": "Begin by splitting the data into training and testing chunks.\n"}
{"snippet": "anon = pd.read_csv('classified.csv')\nanon.head()\n", "intent": "The file `classified.csv` contains the anonymized data that we will use. Import it as a pandas `DataFrame` and look at the head\n"}
{"snippet": "anon = pd.read_csv('classified.csv', index_col=0)\nanon.head()\n", "intent": "It looks like the first column is supposed to be the index. Let's try importing it again while setting that first column as the index.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(scaled_anon, anon['TARGET CLASS'], test_size=0.3)\n", "intent": "Developing a KNN model requires a train-test split, just like any other.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df.drop('Kyphosis', axis=1)\ny = df['Kyphosis']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "In this toy dataset we will not be doing any fancy resampling methods.\n"}
{"snippet": "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/titanic.csv'\ntitanic = pd.read_csv(url)\ntitanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\ntitanic.Age.fillna(titanic.Age.median(), inplace=True)\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\nembarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\ntitanic = pd.concat([titanic, embarked_dummies], axis=1)\ntitanic.head()\n", "intent": "We'll build a classification tree using the Titanic data:\n"}
{"snippet": "cancer = load_breast_cancer()\n", "intent": "The breast cancer that is included with Scikit-learn is ideal for this demonstration.\n"}
{"snippet": "df = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n", "intent": "We can work with the data more easily if we turn it into a pandas `DataFrame` object.\n"}
{"snippet": "pca = PCA(n_components=2)\npca.fit(scaled_data)\npc = pca.transform(scaled_data)\n", "intent": "PCA in Scikit-learn functions very similarly to the standard scaler and other machine learning models.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nsms_train, sms_test, label_train, label_test = \\\n    train_test_split(sms['message'], sms['label'], test_size=0.2)\nlengths = (len(x) for x in (sms_train, sms_test, label_train, label_test))\nprint(tuple(lengths))\n", "intent": "OK, you know that we should have done this step earlier, but whatever.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nbag_transformer = CountVectorizer(stop_words='english')\n", "intent": "In order to make use of the text data, we must vectorize it. Import a `CountVectorizer` and use it to transform `X` into a bag of words.\n"}
{"snippet": "import pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\ndf = pd.read_csv('iris.csv')\n", "intent": "We return the iris dataset for this exercise.\n"}
{"snippet": "def bar_chart(feature): \n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "df = pd.read_csv('ISL_Fig_2_9_data.csv', index_col=0)\n", "intent": "But now the concept is *classification*, not *regression*.\n"}
{"snippet": "df = pd.read_csv('RWA_DHS6_2010_2011_HH_ASSETS.CSV', index_col=0)\n__________________\n", "intent": "Load, clean, and prepare DHS asset ownership data:\n"}
{"snippet": "def probability_to_rank(prediction, scaler=1):\n    pred_df=pd.DataFrame(columns=['probability'])\n    pred_df['probability']=prediction\n    pred_df['rank']=pred_df['probability'].rank()/len(prediction)*scaler\n    return pred_df['rank'].values\n", "intent": "**5.1.2. Xgboost K-fold & OOF function**\nWork in progress\n"}
{"snippet": "df = pd.read_csv('IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv')\n", "intent": "Load and clean PHMRC VA data:\n"}
{"snippet": "df['Cause'] = df.gs_text34.map({'Stroke':'Stroke', 'Diabetes':'Diabetes'}).fillna('Other')\ny = np.array(df.Cause)\n", "intent": "For class purposes, we will simplify the prediction task: was the death due to stroke, diabetes, or something else?\n"}
{"snippet": "df = pd.read_csv('IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv', low_memory=False)\n", "intent": "What do you think we should do about that warning?\n"}
{"snippet": "df = pd.read_csv('../Week_4/IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv', low_memory=False)\nX = np.array(df.filter(like='word_'))\ndf['Cause'] = df.gs_text34.map({'Stroke':'Stroke', 'Diabetes':'Diabetes'}).fillna('Other')\ny = np.array(df.Cause)\nimport sklearn.tree\nweights = 1000. / df.Cause.value_counts()\nsample_weight = np.array(weights[y])\n", "intent": "We used gini; let's check if entropy is any different!\n"}
{"snippet": "def my_transform(X, phases=[0,.5], freqs=[1., 2.]):\n", "intent": "How about if you know that this is some sort of periodic function?\n"}
{"snippet": "df = pd.read_csv('weather-numeric.csv')\ndf\n", "intent": "Or, since H: drive is preventing me from loading that into Sage Cloud, let's look at the good, old weather data from Week 1 of class:\n"}
{"snippet": "pca = sklearn.decomposition.PCA()\nXt = pca.fit_transform(X)\n", "intent": "How long does this take?\n"}
{"snippet": "X = np.array(df.filter(like='X_'))\npca = sklearn.decomposition.PCA()\nXt = pca.fit_transform(X)\n", "intent": "What will happen now?\n"}
{"snippet": "pca = sklearn.decomposition.PCA()\nXt = pca.fit_transform(X)\n", "intent": "Have a look at plain, old PCA on this data:\n"}
{"snippet": "train = pd.read_csv(\"../input/train.csv\")\ntest    = pd.read_csv(\"../input/test.csv\")\nfull = train.append( test , ignore_index = True )\ntitanic = full[ :891 ]\ndel train , test\nprint ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)\n", "intent": "Now that our packages are loaded, let's read in and take a peek at the data.\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "rte = sklearn.ensemble.RandomTreesEmbedding()\nXt = rte.fit_transform(X)\nXt\n", "intent": "Read more, if interested: http://scikit-learn.org/stable/modules/ensemble.html\n"}
{"snippet": "pca = sklearn.decomposition.PCA()\nXt = pca.fit_transform(X)\n", "intent": "We can use PCA to look at it:\n"}
{"snippet": "df = pd.read_csv('../Week_4/IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv', low_memory=False)\n", "intent": "An example from Verbal Autopsy: symptom duration\n"}
{"snippet": "df = pd.read_csv('../Week_4/IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv', low_memory=False)\n", "intent": "A sketch of the ways with the VA data:\n"}
{"snippet": "df = pd.read_csv('/homes/abie/ML4HM/Week_4/IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv', low_memory=False)\n", "intent": "An example from Verbal Autopsy: symptom duration\n"}
{"snippet": "df = pd.read_csv('/homes/abie/ML4HM/Week_4/IHME_PHMRC_VA_DATA_ADULT_Y2013M09D11_0.csv', low_memory=False)\n", "intent": "A sketch of the ways with the VA data:\n"}
{"snippet": "(x_train, _), (x_test, _) = mnist.load_data()\nprint(\"The shape of x_train dataset is\", x_train.shape)\n", "intent": "Downloading the MNIST data\n"}
{"snippet": "from sklearn import datasets\nX, _ = datasets.make_circles(n_samples=150, factor=.5, noise=.05)\nplot(X[:,0], X[:,1], 'bx')\n", "intent": "Test the different clustering approaches with a \"circles\" dataset.\n"}
{"snippet": "from sklearn.feature_extraction import text\nvectorizer = text.CountVectorizer(max_df=0.8, max_features=10000, stop_words=text.ENGLISH_STOP_WORDS, ngram_range=(1,2))\ncounts = vectorizer.fit_transform(dataset.data)\ntfidf = text.TfidfTransformer().fit_transform(counts)\n", "intent": "Se diamo il numero di ngrams tra 1 e 2 abbiamo anche le coppie di parole\n"}
{"snippet": "cabin = pd.DataFrame()\ncabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\ncabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\ncabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\ncabin.head()\n", "intent": "*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntrain,test=train_test_split(df,test_size=0.2)\nX_train=train[['TV','radio','newspaper']]\ny_train=train['sales']\nX_test=test[['TV','radio','newspaper']]\ny_test=test['sales']\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "import pandas as pd\ndata=pd.read_csv(\"hw2data.csv\")\ndata.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "opt_regr=linear_model.Lasso(alpha=0.001)\nopt_regr.fit(X_train,y_train)\n[round(x,2) for x in range(len(opt_regr.coef_))]\ndata_coeffs=pd.DataFrame({'coeffs':opt_regr.coef_,'name':X_train.columns.values})\ndata_coeffs=data_coeffs.sort_values(['coeffs'])\ndata_coeffs[::-1][0:50].plot(x='name',y='coeffs',kind='bar',figsize=(15,15),color='gray')\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "X_predict = data_predict.drop(columns=['age', 'gender', 'income', 'per_id', 'gender_F',\\\n                                       'gender_M', 'gender_None', 'gender_O', 'gender_enc'])\nX_predict_scaled = pd.DataFrame(ss.transform(X_predict))\nX_predict_scaled.columns = X_predict.columns\nX_predict_scaled.index = X_predict.index\nX_predict_pca = pd.DataFrame(pca.transform(X_predict_scaled))\nX_predict_pca.columns = [\"pca_comp_\" + str(i) for i in range(n_components)]\nX_predict_pca.index = X_predict_scaled.index\n", "intent": "Apply scaling and dimensionality reduction to data with missing values\n"}
{"snippet": "docs_meta = pd.read_csv(\"../download/documents_meta.csv\", usecols=['document_id', 'source_id', 'publisher_id']) \ndocs_meta.count()\n", "intent": "Sources and publishers - Yes!\n"}
{"snippet": "docs_ent = pd.read_csv(\"../download/documents_entities.csv\", usecols=['document_id', 'entity_id'])\ndocs_ent.count()\n", "intent": "Entities - 1 out of 15389 is missing\n"}
{"snippet": "events_CTR_train = pd.read_csv(\"../generated/final/events_CTR_train.csv\")\nevents_CTR_train.count()\n", "intent": "Can be run after feature_base_3\n"}
{"snippet": "page_addoc_dist = pd.read_csv(\"../generated/final/page_addoc_topics_no_w_dist.csv\")\npage_addoc_dist.head()\n", "intent": "Can be run after feature_base_4_topics\n"}
{"snippet": "page_addoc_dist = pd.read_csv(\"../generated/final/page_addoc_cats_dist.csv\")\npage_addoc_dist.head()\n", "intent": "Can be run after feature_base_4_cats\n"}
{"snippet": "family = pd.DataFrame()\nfamily[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\nfamily[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\nfamily[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\nfamily[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\nfamily.head()\n", "intent": "The two variables *Parch* and *SibSp* are used to create the famiy size variable\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "page_addoc_dist = pd.read_csv(\"../generated/final/page_addoc_topics_no_w_dist_test.csv\")\npage_addoc_dist.head()\n", "intent": "Can be run after feature_base_4_topics\n"}
{"snippet": "page_addoc_dist = pd.read_csv(\"../generated/final/page_addoc_cats_dist_test.csv\")\npage_addoc_dist.head()\n", "intent": "Can be run after feature_base_4_cats\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = bikes[\"temp\"]\ny = bikes[\"total_rentals\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(len(X_train), len(X_test))\nprint(len(y_train), len(y_test))\n", "intent": "Step 2 - split your data into training and test sets\n"}
{"snippet": "import pandas as pd\nimport seaborn as sns\nbikes = pd.read_csv(\"assets/data/bikeshare.csv\")\nbikes.rename(columns={\"count\": \"total_rentals\"}, inplace=True)\nbikes.head()\n", "intent": "This is just the code snippets from the slides extracted into an easier-to-read format.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df[predictors]\ny = df[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(len(X_train), len(X_test))\nprint(len(y_train), len(y_test))\n", "intent": "Make sure to:\n- do a train/test split\n- fit your model\n- evaluate your model (with an appropriate metric)\n"}
{"snippet": "from sklearn.model_selection import KFold, train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nseven_fold_cv = KFold(n_splits=7, shuffle=True, random_state=42)\n", "intent": "We'll be using the training set for cross-validation\n"}
{"snippet": "import pandas as pd\nbikes = pd.read_csv(\"../assets/data/bikeshare.csv\")\nbikes.rename(columns={\"count\": \"total_rentals\"}, inplace=True)\n", "intent": "For this example, we will build on the bikes dataset we used last time, this time with more \"best practice\".\nRead in the bikes dataset.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n", "intent": "We'll be using the training set for cross-validation\n"}
{"snippet": "df = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf[\"target\"] = iris.target\ndf[\"target\"] = df[\"target\"].map({idx:name for idx, name in enumerate(iris.target_names)})\ndf.head()\n", "intent": "Making iris a DataFrame\n"}
{"snippet": "train_valid_X = full_X[ 0:891 ]\ntrain_valid_y = titanic.Survived\ntest_X = full_X[ 891: ]\ntrain_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\nprint (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)\n", "intent": "Below we will seperate the data into training and test datasets.\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nvalues = np.array([0, 4.7, 7, 9, 24.4, 58, 100]).reshape(-1, 1)\nmm = MinMaxScaler()\nvalues_normed = mm.fit_transform(values)\nvalues_normed\n", "intent": "- normalise to between 0 and 1 (how?)\n"}
{"snippet": "bikes = pd.read_csv(\"assets/data/bikeshare.csv\")\n", "intent": "- must remember to change interpretation\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.3,\n                                                    random_state=99)\nscaler = StandardScaler()\nX_train_transformed = scaler.fit_transform(X_train)\nX_test_transformed = scaler.transform(X_test)\n", "intent": "- to convert back to original units, you can do it with `StandardScaler`\n"}
{"snippet": "df2 = df_with_dummies\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(df2.drop(\"status_group\", axis=1),\n                                                            df2[\"status_group\"],\n                                                            test_size=0.3,\n                                                            random_state=1,\n                                                            stratify=y)\ndisplay(y_train_2.value_counts() / len(y_train_2))\ndisplay(y_test_2.value_counts() / len(y_test_2))\n", "intent": "Try:\n- standardisation: to make KNN consider features equally\n- stratification: to improve on the \"representativeness\" of your samples\n"}
{"snippet": "from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.3,\n                                                    random_state=44)\n", "intent": "Do the train-test split:\n"}
{"snippet": "df_iris_2 = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf_iris_2[\"target\"] = iris.target\ndf_iris_2[\"target\"] = df_iris_2[\"target\"].map({idx:name for idx, name in enumerate(iris.target_names)})\ndf_iris_2 = df_iris_2[df_iris_2[\"target\"] != \"setosa\"]\n", "intent": "Retrain logistic regression for two harder iris classes\n"}
{"snippet": "X = pd.get_dummies(bank[[\"job\", \"cons.conf.idx\", \"euribor3m\"]], columns=[\"job\"], drop_first=False)\nX.drop([\"job_admin.\"], axis=1, inplace=True)\ny = bank[\"y\"]\nX_train, X_test_2, y_train, y_test_2 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\ngrid = GridSearchCV(LogisticRegression(),\n                    param_grid={\"penalty\": [\"l1\", \"l2\"],\n                                \"C\": np.logspace(-4, 2, 7)},\n                    scoring=\"roc_auc\",\n                    cv=StratifiedKFold(10))\ngrid.fit(X_train, y_train);\n", "intent": "Try job, consumer confidence index, and euribor3m\n"}
{"snippet": "baseball = pd.read_csv(\"assets/data/hitters.csv\")\nbaseball = baseball[baseball[\"Salary\"].isnull() == False]\nbaseball.head()\n", "intent": "This avoids the same tree being constructed each time (which would happen if one feature is \"good\" by our measure of importance)\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df.loc[df[\"stars\"].isin([1, 5]), \"text_stemmed\"]\ny = df.loc[df[\"stars\"].isin([1, 5]), \"stars\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\ny.value_counts()\n", "intent": "Let's start with \"1 star\" vs. \"5 star\" reviews as a binary classification\n"}
{"snippet": "all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n", "intent": "- **MiscFeature** : data description says NA means \"no misc feature\"\n"}
{"snippet": "try_new_vectoriser(CountVectorizer(binary=False,\n                                   stop_words='english',\n                                   min_df=2,\n                                   max_features=1000\n                                  ),\n                   X_train,\n                   y_train)\n", "intent": "Limit to top 1000 most frequent words\n"}
{"snippet": "vectorizer_1000 = CountVectorizer(binary=False,\n                                   stop_words='english',\n                                   min_df=2,\n                                   max_features=1000)\nX_train_text = vectorizer_1000.fit_transform(X_train)\nrf = RandomForestClassifier()\nrf.fit(X_train_text, y_train);\n", "intent": "Fit another Random Forest on the latest model\n"}
{"snippet": "try_new_vectoriser(CountVectorizer(binary=False,\n                                   stop_words='english',\n                                   min_df=2,\n                                   ngram_range=(1, 2)\n                                  ),\n                   X_train,\n                   y_train)\n", "intent": "Let's try mixing **both** words and n-grams\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vec = TfidfVectorizer(stop_words=\"english\",\n                            min_df=2,\n                            max_features=1000)\ntry_new_vectoriser(tfidf_vec,\n                   X_train,\n                   y_train)\n", "intent": "- It's best to think of TF-IDF as a value that measure the importance of a word in a document\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df.loc[:, \"text\"]\ny = df.loc[:, \"airline_sentiment\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\ny.value_counts()\n", "intent": "Do a train-test split so we can test our best algorithm at the end\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df[\"text_cleaned\"],\n                                                    df[\"airline_sentiment\"],\n                                                    test_size=0.3,\n                                                    random_state=42,\n                                                    stratify=df[\"airline_sentiment\"])\n", "intent": "Do a train-test split so we can test our best algorithm at the end\n"}
{"snippet": "import pandas as pd\ncars = pd.read_csv(\"assets/data/cars.csv\")\n", "intent": "Car data (source: [Kaggle](https://www.kaggle.com/abineshkumark/carsdata/version/1))\n"}
{"snippet": "import pandas as pd\nloans = pd.read_csv(\"/Users/tyrone/programming/ga/datascience/18_group_project_2/data/loans.csv.gz\")\nloans.rename(columns={'title':'Loan Title', 'loan_amnt':'Loan Amount'}, inplace=True)\nloans.head(3)\n", "intent": "<b> 1.0 Load and examine the data\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df[['emahist', 'bblower_dist', 'macdhist', 'Volume_BTC_ma6', 'rsi_ma6']]\ny = df[\"target\"]\ny_binary = y.map({\"buy\": 1, \"sell\": 0})\nX_train, X_test, y_train, y_test = train_test_split(X, y_binary, stratify=y, test_size=0.3, random_state=42)\nprint(len(X_train), len(X_test))\n", "intent": "<B> 1.4.1 KNeighborsClassifier\n"}
{"snippet": "all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n", "intent": "- **Alley** : data description says NA means \"no alley access\"\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df[[\"alcohol\",\"density\", \"chlorides\"]]\ny = df[\"quality\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(len(X_train), len(X_test))\nprint(len(y_train), len(y_test))\n", "intent": "Remember: we want to avoid testing on the final test set until the end.\n"}
{"snippet": "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nX = df_white.drop(\"quality\", axis=1)\ny = df_white[\"quality\"]\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.3,\n                                                    random_state=42)\nprint(len(X_train), len(X_test))\n", "intent": "Remember: we want to avoid testing on the final test set until the end.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nX = df[[\"fixed acidity\", \"chlorides\", \"total sulfur dioxide\"]]\ny = df[\"colour\"]\ny_binary = y.map({\"white\": 0, \"red\": 1})\nX_train, X_test, y_train, y_test = train_test_split(X, y_binary, stratify=y, test_size=0.3, random_state=42)\n", "intent": "Using the make-up of classes investigated above as an indication - should you or shouldn't you stratify your samples in the train-test split?\n"}
{"snippet": "import cv2\nimg = cv2.imread('data/python-cv/empire.jpg')\ndetector = cv2.FeatureDetector_create('SIFT')\nextractor = cv2.DescriptorExtractor_create('SIFT')\nkeypoints = detector.detect(img)\ndescriptions = extractor.compute(img, keypoints)\n", "intent": "***SIFT Descriptor***\n"}
{"snippet": "import pandas as pd\nimport pandas.tools.rplot as rplot\ntips = pd.read_csv('data/tips.csv')\nprint tips.head()\nprint \"any missing values:\", any(pd.isnull(tips))\n", "intent": "- RPlot is a flexible API for producing Trellis plots. These plots allow you to arrange data in a rectangular grid by values of certain attributes.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nn_samples, n_features = X.shape\ntrain_index, test_index = train_test_split(range(n_samples), test_size = 0.2)\ndata_records = []\ndata_names = []\n", "intent": "***FEATURE and DATA ENGINEERING***\n"}
{"snippet": "sf = SparseFilter(n_features=50, n_iterations=1000)\nsf_X = sf.fit_transform(X)\npca = PCA(n_components=15)\npca_X = pca.fit_transform(X)\nsf_pca_X = np.c_[sf_X, pca_X]\nss = StandardScaler()\nnorm_sf_pca_X = ss.fit_transform(sf_pca_X)\nprint norm_sf_pca_X.shape\n", "intent": "- Use X, y, train_index, test_index\n"}
{"snippet": "X_harafull_train, X_harafull_test, y_train, y_test = train_test_split(X_hara_full, y, \n                                                                      random_state = 0)\nprint X_harafull_train.shape, X_harafull_test.shape\nprint y_train.shape, y_test.shape\n", "intent": "hara full performance\n"}
{"snippet": "ss = StandardScaler()\nX_harasurf_norm = ss.fit_transform(X_harasurf_full)\n", "intent": "normalization (not sparse anymore) + sgd\nIt runs much faster than SVC and achieves comparable result - good features go a long way\n"}
{"snippet": "all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n", "intent": "- **Fence** : data description says NA means \"no fence\"\n"}
{"snippet": "import pandas as pd\npath = 'material/yelp.csv'\nyelp = pd.read_csv(path)\n", "intent": "First, we read **`yelp.csv`** into a pandas DataFrame and examine it.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(150)\npca.fit(faces.data)\n", "intent": "Similarly to the lecture we will apply a PCA dimensionality reduction to 150 dimensions: \n"}
{"snippet": "vect = CountVectorizer(ngram_range=(1, 2))\n", "intent": "n-grams concatenate n words to form a token. The following accounts for 1- and 2-grams\n"}
{"snippet": "tfidf = TfidfTransformer(use_idf=True, smooth_idf=False, norm='l2')\ntfidf.fit_transform(tf).toarray()[-1][:3]\n", "intent": "For example, we would normalize our 3rd document `'The sun is shining and the weather is sweet'` as follows:\n"}
{"snippet": "tfidf = TfidfTransformer(use_idf=True, smooth_idf=True, norm='l2')\ntfidf.fit_transform(tf).toarray()[-1][:3]\n", "intent": "$$\\text{idf}(t) = log{\\frac{1 + n_d}{1+\\text{df}(d,t)}} + 1$$ \n"}
{"snippet": "token_text = [token.orth_ for token in parsed_review]\ntoken_pos = [token.pos_ for token in parsed_review]\npd.DataFrame(list(zip(token_text, token_pos)), columns=['token_text', 'part_of_speech'])\n", "intent": "What about part of speech tagging?\n"}
{"snippet": "ordered_vocab = [(term, voc.index, voc.count)\n                 for term, voc in food2vec.wv.vocab.items()]\nordered_vocab = sorted(ordered_vocab, key=lambda x: -x[2])\nordered_terms, term_indices, term_counts = zip(*ordered_vocab)\nword_vectors = pd.DataFrame(food2vec.wv.syn0norm[term_indices, :],\n                            index=ordered_terms)\nword_vectors\n", "intent": "Let's take a look at the word vectors our model has learned.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "Let's demonstrate the naive approach to validation using the Iris data, which we saw previously:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=42)\n", "intent": " We split the data into a training and testing set:\n"}
{"snippet": "all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n", "intent": "- **FireplaceQu** : data description says NA means \"no fireplace\"\n"}
{"snippet": "user_interaction_df = pd.read_csv(user_interaction_path)\nuser_interaction_results_df = pd.read_csv(user_interaction_results_path)\n", "intent": "Load data into dataframes:\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n", "intent": "* Cross Validation \n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\ndef one_hot_dataframe(data, cols, replace=False):\n    enc = OneHotEncoder()\n    e = enc.fit_transform(data[cols].values)\n    vecData = pd.DataFrame(e.toarray())\n    if replace is True:\n        data = data.drop(cols, axis=1)\n        data = data.join(vecData)\n    return (data, vecData)\n", "intent": "Now we can start playing with the features...\n"}
{"snippet": "df = pd.read_csv(\"yield.csv\",sep=\"\\t\")\ndf.head()\n", "intent": "data from - https://onlinecourses.science.psu.edu/stat501/node/325\n"}
{"snippet": "X_test = pd.DataFrame([[3, 5, 4, 2], [5, 4, 3, 2]], columns=iris.feature_names)\nX_test.head()\n", "intent": "**step3:** Test the Classifier on new data\n"}
{"snippet": "data=data[['Adj. Close','High_Low_Change','Change_Perc','Adj. Volume']]\ndata.fillna(data.mean(),inplace=True)\n", "intent": "** Now, let's make our new dataframe: **\n"}
{"snippet": "from sklearn import cross_validation\nX_train,X_test,y_train,y_test=cross_validation.train_test_split(X,y,test_size=0.3)\n", "intent": "**splitting our data as usual+cross-validation: **\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(features, target,train_size=0.6,random_state=0)\n", "intent": "**Having encoded our categorical values, we are now ready to build our model**\n"}
{"snippet": "nino = pd.read_csv('../data/tao-all2.dat.gz', sep=' ', names=names, na_values='.', \n                   parse_dates=[[1,2,3]])\nnino.columns = [x.replace('.', '_').replace(' ', '_') for x in nino.columns]\nnino['air_temp_F'] = nino.air_temp_ * 9/5 + 32\nwind_cols = [x for x in nino.columns if x.endswith('winds')]\nfor c in wind_cols:\n    nino['{}_mph'.format(c)] = nino[c] * 2.237\npd.to_datetime(nino.date, format='%y%m%d')\nnino = nino.drop('obs', axis=1)\nnino['year'] = nino.year_month_day.dt.year\n", "intent": "* Using the nino dataset, see if you can predict what the temperature (``air_temp_F``) will be for the next day \n"}
{"snippet": "for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n", "intent": "- **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\nfrom pprint import pprint\ncats = ['rec.sport.baseball', 'sci.electronics', 'misc.forsale']\ntrain_data = fetch_20newsgroups(subset='train', categories=cats)\n", "intent": "In this lab, we consider the 20 newsgroups text dataset from [scikit-learn](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html).\n"}
{"snippet": "year = '2015'\ncols = ['Country Name', year]\ngdp = pandas.read_csv('../datasets/API_NY.GDP.MKTP.CD_DS2_en_csv_v2.csv',\n                      index_col=0, skiprows=4, usecols=cols)\ngdp.rename(columns={year : 'GDP'}, inplace=True);\ngdp.head()\n", "intent": "Let us consider the following prediction problem :\n* $y$ : Gross national product (in dollars)\n* $x_1$ : Population size\n* $x_2$ : Literacy rate\n"}
{"snippet": "smarket = pd.read_csv('../datasets/Smarket.csv', index_col=0, parse_dates=True)\nsmarket.head()\n", "intent": "We consider the *Standard & Poor's 500* (S&P) stock index over a 5 year period :\n"}
{"snippet": "df3 = pd.read_csv('https://github.com/ApoorvP02121996/Sentiment-Analysis---Movie-Reviews/raw/master/Naive%20Bayes/training_set.csv')\ndf3.columns = ['target', 'text']\nprint (df3.shape)\ndf3.head()\n", "intent": "Dataset source: https://github.com/ApoorvP02121996/Sentiment-Analysis---Movie-Reviews/blob/master/Naive%20Bayes/training_set.csv\n"}
{"snippet": "X.fillna(0, inplace = True)\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n", "intent": "First, I will try to apply default random forest regressor to the training and testing dataframes.\n"}
{"snippet": "def imp_df(column_names, importances):\n    df = pd.DataFrame({'feature': column_names,\n                       'feature_importance': importances}) \\\n           .sort_values('feature_importance', ascending = False) \\\n           .reset_index(drop = True)\n    return df\ndef var_imp_plot(imp_df, title):\n    imp_df.columns = ['feature', 'feature_importance']\n    sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df, orient = 'h', color = 'royalblue') \\\n       .set_title(title, fontsize = 20)\n", "intent": "I define a few helper functions to make analysis more convenient and presentable.\n"}
{"snippet": "weights=[3, 1, 1, 3]\nprofessional_id = 'd7f9afe721af42b1a03a993909e0568c'\nrecommendation, score = recommend_questions_for_professional(professional_id, weights=weights)\npd.DataFrame(recommendation)\n", "intent": "Professional with already 60 answered questions.\n"}
{"snippet": "weights=[0, 5, 1/5, 3]\nprofessional_id = 'ea75c5fce38348e0a151c3c346929e6a'\nrecommendation, score = recommend_questions_for_professional(professional_id, weights=weights)\npd.DataFrame(recommendation)\n", "intent": "A new Professional without any answered questions. The recommendation is based on the hashtags.\n"}
{"snippet": "stats = tuner.analytics().dataframe()              \nstats = stats.nsmallest(2, 'FinalObjectiveValue')   \nstats.head()\n", "intent": "First, let's figure out what the top 2 jobs are.\n"}
{"snippet": "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n", "intent": "- **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n"}
{"snippet": "scaler = StandardScaler()\nX_tr_ = scaler.fit_transform(X_tr)\nX_va_ = scaler.transform(X_va)\neval_clf(X_tr_, y_tr, X_va_)\n", "intent": "Scaling improves significantly all model but Decision Tree has lower F1 score.\n"}
{"snippet": "X_ba, y_ba = balance(X_tr, y_tr)\nscaler = StandardScaler()\nX_ba_ = scaler.fit_transform(X_ba)\nX_va_ = scaler.transform(X_va)\neval_clf(X_ba_, y_ba, X_va_)\n", "intent": "Both preprocessing procedures (balancing and scaling) are good from Decision Tree, 3NN, Neural Net but not for Naive Bayes model.\n"}
{"snippet": "data = pandas.read_csv(data_file, index_col='Id')\ndata = data.drop(['Unnamed: 0'], axis=1)\ndata.info()\ndata.head()\n", "intent": "Drop 'Unnamed: 0' columns and set 'Id' as index.\n"}
{"snippet": "X = df_tr.drop(['SalePrice'], axis=1)\nX_te = df_te.drop(['SalePrice'], axis=1)\npca = PCA()\nX = pca.fit_transform(X)\nX_te = pca.transform(X_te)\n", "intent": "Use PCA to transform features. Hereafter the feature obtained from PCA will be used.\n"}
{"snippet": "df_tr, df_te = train_test_split(data, test_size=0.25, random_state=17)\nX = df_tr.drop(['SalePrice'], axis=1)\nX_te = df_te.drop(['SalePrice'], axis=1)\npca = PCA()\nX = pca.fit_transform(X)\nX_te = pca.transform(X_te)\n", "intent": "select as less as possible features\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndata = pd.read_csv(\"housing_price_univariate.csv\")\ndata.head()\n", "intent": "***\n- Now, let's have a look at the data \n - Every row displays the Price and Area of each house\n"}
{"snippet": "data = pd.read_csv('https://raw.githubusercontent.com/madmashup/targeted-marketing-predictive-engine/master/banking.csv', header=0)\ndata = data.dropna()\nprint(data.shape)\nprint(list(data.columns))\n", "intent": "This dataset provides the customer information. It includes 41188 records and 21 fields.\n"}
{"snippet": "my_data = pd.read_csv(\"drug200.csv\", delimiter=\",\")\nmy_data[0:5]\n", "intent": "now, read data using pandas dataframe:\n"}
{"snippet": "df = pd.read_csv('../Datasets/loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv**\n"}
{"snippet": "for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n", "intent": "- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\n", "intent": "Okay, we split our dataset into train and test set:\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target\n", "intent": "Example 1 - Simple Stacked Classification\n"}
{"snippet": "questions = pd.read_csv(\"socialmedia_relevant_cols_clean.csv\")\nquestions.columns=['text', 'choose_one', 'class_label']\nquestions.head()\n", "intent": "It looks solid, but we don't really need urls, and we would like to have our words all lowercase (Hello and HELLO are pretty similar for our task)\n"}
{"snippet": "data = pd.read_csv('../data/training.csv')\n", "intent": "Download the training & test data from the Practice Problem approach. We'll do a bit of quick investigation on the dataset:\n"}
{"snippet": "X_train['Dependents'] = X_train['Dependents'].fillna('0')\nX_train['Self_Employed'] = X_train['Self_Employed'].fillna('No')\nX_train['Loan_Amount_Term'] = X_train['Loan_Amount_Term'].fillna(X_train['Loan_Amount_Term'].mean())\n", "intent": "We'll compile a list of `pre-processing` steps that we do on to create a custom `estimator`.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(data[pred_var], data['Loan_Status'], \\\n                                                    test_size=0.25, random_state=42)\n", "intent": "To make sure that this works, let's do a test run for it:\n"}
{"snippet": "pred_var = ['Gender','Married','Dependents','Education','Self_Employed','ApplicantIncome','CoapplicantIncome',\\\n            'LoanAmount','Loan_Amount_Term','Credit_History','Property_Area']\nX_train, X_test, y_train, y_test = train_test_split(data[pred_var], data['Loan_Status'], \\\n                                                    test_size=0.25, random_state=42)\n", "intent": "- Next step is creating `training` and `testing` datasets:\n"}
{"snippet": "import pandas as pd\ntest_df = pd.read_csv('../data/test.csv', encoding=\"utf-8-sig\")\ntest_df = test_df.head()\n", "intent": "- Load the test set:\n"}
{"snippet": "credit_df = pd.read_csv('creditcard.csv')\ncredit_df.head()\n", "intent": "The dataset we use is publicly available at https://www.kaggle.com/mlg-ulb/creditcardfraud\n"}
{"snippet": "all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n", "intent": "- **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. \n"}
{"snippet": "df = pd.DataFrame({'coefficient-name':range(len(regr.coef_)), 'value':regr.coef_})\nsrt = df.sort_values(['value'])[: : -1]\nsrt[0:100].plot(x='coefficient-name',y='value',kind='bar',figsize=(15,8))\n", "intent": "The lasso regression decays quickly to reach zero. Once it reaches it, it tries to maintain the value 0 with a few overshoots\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "pd.DataFrame(zip(train_cols.columns, np.transpose(result.coef_)))\n", "intent": "hint 1: np.exp(X)\nhint 2: conf['OR'] = params\n           conf.columns = ['2.5%', '97.5%', 'OR']\n"}
{"snippet": "f = open('../../lectures/data/temperature.csv')\ntemperature = pd.read_csv(f,sep=',', header='infer', parse_dates=[0],index_col=0)\nf.close()\ntemperature.index\ndata = data.set_index(['Time'])\n", "intent": "Create a new DataFrame with the temperature data, and set the index to be the Timestamp.\n"}
{"snippet": "from sklearn.datasets import load_boston\nprint(load_boston().DESCR)\ndf = pd.DataFrame(\n        data=np.column_stack((load_boston().data,load_boston().target)),\n        columns=np.append(load_boston().feature_names,['Median_Value'])\n    )\ndf.describe()\n", "intent": "Now let's try a regression task. To begin, let's use an existing dataset.\n"}
{"snippet": "column_names = pd.read_excel('langevincodebook.xlsx',sheetname = 'Sheet2')\ndata_names = column_names['Description'].values\ndata = pd.read_csv('LANGEVIN_DATA.txt',sep=' ',names = data_names,index_col =False)\n", "intent": "Importing the data and the column names from the codebook:\n"}
{"snippet": "X1_train, X1_test, Y_train, Y_test = train_test_split(X1, Y, test_size=0.3)\nreg = tree.DecisionTreeRegressor()\nreg = reg.fit(X1_train,Y_train)\nr2_score_avg1 = np.average([reg.score(X1_test,Y_test) for i in range(5000)])\nprint('R^2 value: ', r2_score_avg1)\n", "intent": "The first regression tree will test our environmental and time variables.\n"}
{"snippet": "dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\nCSVdata=pd.read_csv('Building Electrical.csv', parse_dates=[0], date_parser=dateparse)\n", "intent": "First, read the csv data files, and convert the index 'Timestamp' to datetimeindex.\n"}
{"snippet": "f = open('C:/F16-12-752-master/projects/thongyi_weijian1/data/CBECS.csv')\ndata = pd.read_csv(f,sep=',', header='infer', parse_dates=[1])\ndata = data.set_index('PUBID')\ndata.tail()\n", "intent": "Please download the dataset and change the file path.\n"}
{"snippet": "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n", "intent": "- **MSZoning (The general zoning classification)** :  'RL' is by far  the most common value.  So we can fill in missing values with 'RL'\n"}
{"snippet": "print(\"Extracting tf features for LDA...\")\nn_features = 1000\nn_samples = 2000\ntf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n                                max_features=n_features,\n                                stop_words='english')\nt0 = time()\ntf = tf_vectorizer.fit_transform(data_samples)\nprint(\"done in %0.3fs.\" % (time() - t0))\nprint tf[0][0][0]\n", "intent": "Now we are ready to compute the token counts.\n"}
{"snippet": "t0 = time()\ncorpus_lda = lda.fit_transform(tf)\nprint corpus_lda[10]/np.sum(corpus_lda[10])\nprint(\"done in %0.3fs.\" % (time() - t0))\nprint corpus_titles[10]\n", "intent": "**Task**: Fit model `lda` with the token frequencies computed by `tf_vectorizer`.\n"}
{"snippet": "name = \"birds.jpg\"\nname = \"Seeds.jpg\"\nbirds = imread(\"Images/\" + name)\nbirdsG = np.sum(birds, axis=2)\n", "intent": "Select and visualize image `birds.jpg` from file and plot it in grayscale\n"}
{"snippet": "of_df = pd.read_csv(\"./datasets/old-faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "colleges = pd.read_csv('College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "wb = pd.read_csv('world_bank/API_19_DS2_en_csv_v2.csv', sep=',', header=0, skiprows=3) \nwb = wb.drop(wb.columns[[-1]], 1)\nprint(\"Dataframe shape:\", wb.shape)\nwb.head()\n", "intent": "Load the World Bank data as a dataframe.\n"}
{"snippet": "wb_meta_country = pd.read_csv('world_bank/Metadata_Country_API_19_DS2_en_csv_v2.csv', sep=',', header=0) \nwb_meta_country = wb_meta_country.drop(wb_meta_country.columns[[-1]], 1)\nwb_meta_country.head()\n", "intent": "Load the additional csv file which contains the metadata of the countries on the main World Bank data.\n"}
{"snippet": "wb_meta_indi = pd.read_csv('world_bank/Metadata_Indicator_API_19_DS2_en_csv_v2.csv', sep=',', header=0) \nwb_meta_indi = wb_meta_indi.drop(wb_meta_indi.columns[[-1]], 1)\nwb_meta_indi.head()\n", "intent": "We also have to include the another csv file containing the metadata of the indicator on the main World Bank data. \n"}
{"snippet": "wb_inter = pd.DataFrame()\nwb_inter = wb[wb.columns.values]\nwb_inter[list(wb.columns.values[range(column_start_of_year,column_start_of_year+interval_duration)])] = wb_interpolated[list(range(0,len(years_column)))]\n", "intent": "A new dataframe is created to accomodate the new interpolated values. \n"}
{"snippet": "all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n", "intent": "- **Functional** : data description says NA means typical\n"}
{"snippet": "df_ccpi_clean = pd.DataFrame()\narray_flag = np.ones((len(df_ccpi)), dtype=bool)\nseries_flag = pd.Series(data = array_flag, index=range(len(df_ccpi)))\nfor indi in sorted(set(list_deleted_indi)):\n    series_flag = (series_flag & (df_ccpi['Indicator Name'] != indi))\ndf_ccpi_clean_reset = df_ccpi[series_flag]\nprint('Number of row with missing value:', df_ccpi_clean_reset.isnull().any().sum())\n", "intent": "We will have to remove 24 indicators from the data. It leaves us with the remaining 56 indicators.\n"}
{"snippet": "df_ccpi_feature_processed = pd.DataFrame()\nfor indi in set_indi_average:\n    df_indi = df_ccpi_feature_label_removed[(df_ccpi_feature_label_removed['Indicator Name'] == indi)]\n    df_indi_ave = (df_indi.sum(axis = 0)) \n    df_ccpi_feature_processed[indi] = df_indi_ave[4:-1]/len(ccpi_country)\nprint('Dataframe shape:',df_ccpi_feature_processed.shape)\n", "intent": "New engineered dataframe is generated. First the dataframe with the averaged features is appended. \n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\ntarget_int = (dataset_tempe['annual mean']*100).astype(int)\nRFEmodel = LogisticRegression()\nrfe = RFE(RFEmodel, 7)\nrfe = rfe.fit(X_tempe_train, target_int)\n", "intent": "In order to know which indicators has big impact on global temperature, Recrusive Feature Elimination is implemented.\n"}
{"snippet": "wb_inter = pd.DataFrame()\nwb_inter = wb[wb.columns.values]\nwb_inter[list(wb.columns.values[range(4,61)])] = wb_interpolated[list(range(0,57))]\nwb_inter.head(8)\n", "intent": "A new dataframe is created to accomodate the new interpolated values. \n"}
{"snippet": "df_ccpi = pd.DataFrame()\nfor country in country_class_y['Country Name']:\n    df1 = wb_inter[wb_inter['Country Name'] == country]\n    df_ccpi = pd.concat( [df_ccpi,df1], ignore_index=True, axis=0)\ndf_ccpi[:8]\n", "intent": "New dataframe which contains only the data from countries being mentioned in CCPI document is created.\n"}
{"snippet": "df_ccpi_feature_processed = pd.DataFrame()\nfor indi in set_indi_average:\n    df_indi = df_ccpi_feature_label_removed[(df_ccpi_feature_label_removed['Feature Interaction'] == 'average') & (df_ccpi_feature_label_removed['Indicator Name'] == indi)]\n    df_indi_ave = (df_indi.sum(axis = 0)) \n    df_ccpi_feature_processed[indi] = df_indi_ave[4:-1]/len(country_class_y)\nprint('Dataframe shape:',df_ccpi_feature_processed.shape)    \ndf_ccpi_feature_processed.head()\n", "intent": "Generate new engineered dataframe. First we append the dataframe with the averaged features. \n"}
{"snippet": "df_ccpi = pd.DataFrame()\nfor country in ccpi_country['Country Name']:\n    df1 = wb_inter[wb_inter['Country Name'] == country]\n    df_ccpi = pd.concat( [df_ccpi,df1], ignore_index=True, axis=0)\ndf_ccpi[:8]\n", "intent": "New dataframe which contains only the data from countries being mentioned in CCPI document is created.\n"}
{"snippet": "df_ccpi_feature_processed = pd.DataFrame()\nfor indi in set_indi_average:\n    df_indi = df_ccpi_feature_label_removed[(df_ccpi_feature_label_removed['Feature Interaction'] == 'average') & (df_ccpi_feature_label_removed['Indicator Name'] == indi)]\n    df_indi_ave = (df_indi.sum(axis = 0)) \n    df_ccpi_feature_processed[indi] = df_indi_ave[4:-1]/len(ccpi_country)\nprint('Dataframe shape:',df_ccpi_feature_processed.shape)    \ndf_ccpi_feature_processed.head()\n", "intent": "Generate new engineered dataframe. First we append the dataframe with the averaged features. \n"}
{"snippet": "wb_inter = pd.DataFrame()\nwb_inter = wb[wb.columns.values]\nwb_inter[list(wb.columns.values[range(4,61)])] = wb_interpolated[list(range(0,57))]\n", "intent": "A new dataframe is created to accomodate the new interpolated values. \n"}
{"snippet": "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n", "intent": "- **Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\ntarget_int = (dataset_tempe['annual mean']*100).astype(int)\nRFEmodel = LogisticRegression()\nrfe = RFE(RFEmodel, 7)\nrfe = rfe.fit(X_tempe_train, target_int) \n", "intent": "In order to know which indicators has big impact on global temperature, Recrusive Feature Elimination is implemented.\n"}
{"snippet": "df_test_clean = pd.DataFrame()\nfor indi in set_saved_indi_filter:\n    df_temp = df_test[df_test['Indicator Name'] == indi]\n    df_test_clean = pd.concat([df_test_clean, df_temp], ignore_index = True, axis = 0)\nprint(df_test_clean.shape)\ndf_test_clean.head()\n", "intent": "We will be using only 49 features from the indicator, because the others 31 is not available (for all category on at least one class (bad:good))\n"}
{"snippet": "wb = pd.read_csv('world_bank/API_19_DS2_en_csv_v2.csv', sep=',', header=0, skiprows=3) \nwb = wb.drop(wb.columns[[-1]], 1)\nwb.head()\n", "intent": "Load the world bank data as a dataframe.\n"}
{"snippet": "wb_meta_country = pd.read_csv('world_bank/Metadata_Country_API_19_DS2_en_csv_v2.csv', sep=',', header=0) \nwb_meta_country = wb_meta_country.drop(wb_meta_country.columns[[-1]], 1)\nwb_meta_country.head()\n", "intent": "Read the additional csv file which contains the metadata of the counries on the main world bank data.\n"}
{"snippet": "wb_meta_indi = pd.read_csv('world_bank/Metadata_Indicator_API_19_DS2_en_csv_v2.csv', sep=',', header=0) \nwb_meta_indi = wb_meta_indi.drop(wb_meta_indi.columns[[-1]], 1)\nwb_meta_indi.head()\n", "intent": "We also have to include the another csv file containing the metadata of the indicator on the main world bank data. \n"}
{"snippet": "df_ccpi = pd.DataFrame()\nfor country in country_class_y['Country Name']:\n    df1 = wb_inter[wb_inter['Country Name'] == country]\n    df_ccpi = pd.concat( [df_ccpi,df1], ignore_index=True, axis=0)\ndf_ccpi[:8]\n", "intent": "New dataframe which contains only the data from countries mention in CCPI document.\n"}
{"snippet": "import operator\nlist_deleted_indi = []\nlist_tuple_country_del_indi = []\nfor c, i in enumerate(range(0,4480,80)):\n    list_tuple_country_del_indi.append((df_ccpi['Country Name'][i], df_ccpi[i:i+80].isnull().any(axis=1).sum())) \n    for i in df_ccpi[i:i+80]['Indicator Name'][df_ccpi[i:i+80].isnull().any(axis=1)]:\n        list_deleted_indi.append(i)    \ndf_tuple_cdi = pd.DataFrame(list_tuple_country_del_indi, columns = ['Country Name','Number of Missing Indicator'])\ndf_tuple_cdi = df_tuple_cdi.set_index('Country Name')\n", "intent": "Check the data for each country which the entire years of a indicator are missing.\n"}
{"snippet": "dict_df_temp = {}\nrange_tempe = range(1960, 2017)\nfor year in range_tempe:\n    year = str(year)\n    dict_df_temp[year] = pd.DataFrame()\n    dict_df_temp[year] = wb[['Country Name', year]]\n    dict_df_temp[year] = dict_df_temp[year].fillna(0)\ndict_df_temp[year].head()\n", "intent": "We could also use the entire countries instead too make our features matrix bigger (not necessarily better).\n"}
{"snippet": "df_ccpi = pd.DataFrame()\nfor country in country_class_y['Country Name']:\n    df1 = wb[wb['Country Name'] == country]\n    df1 = wb_inter[wb_inter['Country Name'] == country]\n    df_ccpi = pd.concat( [df_ccpi,df1], ignore_index=True, axis=0)\ndf_ccpi[:8]\n", "intent": "New dataframe which contains only the data from countries mention in CCPI document is created.\n"}
{"snippet": "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n", "intent": "- **KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent)  for the missing value in KitchenQual.\n"}
{"snippet": "dict_df_temp = {}\nrange_tempe = range(1960, 2017)\nfor year in range_tempe:\n    year = str(year)\n    dict_df_temp[year] = pd.DataFrame()\n    dict_df_temp[year] = wb[['Country Name', year]]\n    dict_df_temp[year] = dict_df_temp[year].fillna(0)\ndict_df_temp[year].head()\n", "intent": "We could also use the entire countries to make our features matrix bigger (not necessarily better).\n"}
{"snippet": "print(pd.DataFrame({'effect': params_tempe*10**20, 'error': err[0]*10**20}))\n", "intent": "List above shows which countries and their indicator contribute to decrease global temperature.\n"}
{"snippet": "wb_meta_country = pd.read_csv('world_bank/Metadata_Country_API_19_DS2_en_csv_v2.csv', sep=',', header=0) \nwb_meta_country = wb_meta_country.drop(wb_meta_country.columns[[-1]], 1)\nwb_meta_country.head()\n", "intent": "Read the additional csv file which contains the metadata of the countries on the main world bank data.\n"}
{"snippet": "df_ccpi = pd.DataFrame()\nfor country in country_class_y['Country Name']:\n    df1 = wb_inter[wb_inter['Country Name'] == country]\n    df_ccpi = pd.concat( [df_ccpi,df1], ignore_index=True, axis=0)\ndf_ccpi[:8]\n", "intent": "New dataframe which contains only the data from countries mention in CCPI document is created.\n"}
{"snippet": "df_ccpi = pd.DataFrame()\nfor country in country_class_y['Country Name']:\n    df1 = wb[wb['Country Name'] == country]\n    df1 = wb_inter[wb_inter['Country Name'] == country]\n    df_ccpi = pd.concat( [df_ccpi,df1], ignore_index=True, axis=0)\ndf_ccpi[:8]\n", "intent": "New dataframe which contains only the data from countries mention in CCPI document.\n"}
{"snippet": "df_ccpi_clean = pd.DataFrame()\narray_flag = np.ones((len(df_ccpi)), dtype=bool)\nseries_flag = pd.Series(data = array_flag, index=range(len(df_ccpi)))\nfor indi in sorted(set(list_indi)):\n    series_flag = (series_flag & (df_ccpi['Indicator Name'] != indi))\ndf_ccpi_clean = df_ccpi[series_flag]\ndf_ccpi_clean_reset = df_ccpi_clean.reset_index(drop=True)\ndf_ccpi_clean_reset = df_ccpi_clean_reset.fillna(0)\ndf_ccpi_clean_reset.head(8)\n", "intent": "Filter the data frame so it only consists of the indicators which are complete for the entire year (1960-2016).\n"}
{"snippet": "display(pd.DataFrame(np.round(pca_samples, 4), columns = pca_results.index.values))\n", "intent": "The code below will help us see how the log-transformed sample data has changed after having a PCA transformation applied to it in six dimensions.\n"}
{"snippet": "df = pd.read_csv('College_Data', index_col='Unnamed: 0')\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "scaler = StandardScaler()\nX = df.drop('TARGET CLASS', axis=1)\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n", "intent": "- **Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n"}
{"snippet": "df_err = pd.DataFrame(data=error_rate, columns=['Error'])\ndf_err.plot()\n", "intent": "**Now create the following plot using the information from your for loop.**\n"}
{"snippet": "df = pd.read_csv(\"Classified Data\", index_col=0)\n", "intent": "Set index_col=0 to use the first column as the index.\n"}
{"snippet": "df = pd.DataFrame({\n    \"name\": [\"Goldy\", \"Scooby\", \"Brian\", \"Francine\", \"Goldy\"],\n    \"kind\": [\"Fish\", \"Dog\", \"Dog\", \"Cat\", \"Dog\"],\n    \"age\": [0.5, 7., 3., 10., 1.]\n}, columns = [\"name\", \"kind\", \"age\"])\ndf\n", "intent": "Here we create a toy DataFrame of pets including their name and kind:\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\nvec_enc = DictVectorizer()\nvec_enc.fit(df.to_dict(orient='records'))\n", "intent": "Scikit-Learn also has several library for constructing one-hot-encodings.\n"}
{"snippet": "qsos = pd.read_csv(\"qso10000.csv\",index_col=0,\n                                  usecols=usecols)\nqsos = qsos[(qsos[\"dered_r\"] > -9999) & (qsos[\"g_r_color\"] > -10) & (qsos[\"g_r_color\"] < 10)]\nqso_features = copy.copy(qsos)\nqso_redshifts = qsos[\"spec_z\"]\ndel qso_features[\"spec_z\"]\n", "intent": "Looks like there are some missing values in the catalog which are set at -9999. Let's zoink those from the dataset for now.\n"}
{"snippet": "qsos.to_csv(\"qsos.clean.csv\")\n", "intent": "Ok. This looks pretty clean. Let's save this for future use.\n"}
{"snippet": "df = pd.read_csv(\"../data/coal_prod_cleaned.csv\")\n", "intent": "Using cleaned data from [Data Cleaning](Data%20Cleaning.ipynb) Notebook. See Notebook for details.\n"}
{"snippet": "app_train = pd.read_csv(path + \"application_train.csv\")\napp_train.head()\n", "intent": "Application data consists of static data for all applications and every row represents one loan.\n"}
{"snippet": "train_df = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\ntest_df = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\nmacro_df = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])\ntrain_df = pd.merge(train_df, macro_df, how='left', on='timestamp')\ntest_df = pd.merge(test_df, macro_df, how='left', on='timestamp')\nprint(train_df.shape, test_df.shape)\nulimit = np.percentile(train_df.price_doc.values, 99)\nllimit = np.percentile(train_df.price_doc.values, 1)\ntrain_df['price_doc'].ix[train_df['price_doc']>ulimit] = ulimit\ntrain_df['price_doc'].ix[train_df['price_doc']<llimit] = llimit\n", "intent": "Let us read the train, test and macro files and combine macro information with train and test.\n"}
{"snippet": "all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n", "intent": "- **SaleType** : Fill in again with most frequent which is \"WD\"\n"}
{"snippet": "train_df = pd.read_csv(\"../input/train.csv\")\ntrain_df.shape\n", "intent": "First let us import the train file and get some idea about the data.\n"}
{"snippet": "df = pd.read_csv(\"./dataset_Facebook.csv\", delimiter = \";\")\nfeatures = [\"Category\",\n            \"Page total likes\",\n            \"Type\",\n            \"Post Month\",\n            \"Post Hour\",\n            \"Post Weekday\",\n            \"Paid\"]\ndf[features].head()\n", "intent": "* We have loaded the necessary libraries above\n* Now let's load the data\n"}
{"snippet": "df = pd.read_csv(\"./dataset_Facebook.csv\", delimiter = \";\")\n", "intent": "* We have loaded the necessary libraries above\n* Now let's load the data\n"}
{"snippet": "df['age_medianimpute'] = df['age'].fillna(df['age'].median())\ndf['age_modeimpute'] = df['age'].fillna(df['age'].mode())\n", "intent": "   1) median (middle value) for that column\n   2) mode (most frequent value) for that column\n"}
{"snippet": "df['age'].fillna(df['age'].median(), inplace=True)\ndf['age'].fillna(df['age'].mode(), inplace=True)\n", "intent": "   1) median (middle value) for that column\n   2) mode (most frequent value) for that column\n"}
{"snippet": "path_train = os.path.join(os.getcwd(), 'datasets', 'landsat', 'landsat_train.csv')\nlandsat_train = pd.read_csv(path_train, delimiter = ',')\nprint(\"There are {} entries and {} columns in the landsat_train DataFrame\"\\\n      .format(landsat_train.shape[0], landsat_train.shape[1]))\n", "intent": "Load the `landsat_train.csv` dataset into a `pandas` DataFrame called  `landsat_train` and display the shape of the DataFrame.\n"}
{"snippet": "X = sales2015\ny = sales2015[\"Sale_Dollars_sum_2015\"]\nX_train, X_test, y_train, y_test = train_test_split(sales2015, y, test_size=0.4)\nprint X_train.shape, y_train.shape\nprint X_test.shape, y_test.shape\n", "intent": "Sales Decrease in 2016\n"}
{"snippet": "header = [\"Sample_Code_Num\",\"Clump_T\",\"Uni_Cell_Size\",\"Uni_Cell_Shape\",\"Marg_Adh\",\"SECS\",\"Bare_Nuc\",\"Bland_Chro\",\"Norm_Nuc\",\"Mitoses\",\"Class\"]    \ndfCancer = pd.read_csv(\"../../assets/datasets/breast-cancer-wisconsin.csv\", names = header)\ndfCancer.head()\n", "intent": "The column names are taken from the dataset info file. Create an array\nwith the column names and assign them as the header when loading the\ncsv.\n"}
{"snippet": "df = pd.DataFrame(\"../assets/datasets/breast-cancee)\n", "intent": "The column names are taken from the dataset info file. Create an array\nwith the column names and assign them as the header when loading the\ncsv.\n"}
{"snippet": "all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n", "intent": "- **MSSubClass** : Na most likely means No building class. We can replace missing values with None\n"}
{"snippet": "tfidfvectorizer = TfidfVectorizer(min_df=2, stop_words='english')\nX, y = make_xy(critics, tfidfvectorizer)\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.3, train_size = 0.7, random_state = 2018)\nclf = MultinomialNB(alpha = best_alpha)\nclf.fit(X_train, Y_train)\ntraining_accuracy = clf.score(X_train, Y_train)\ntest_accuracy = clf.score(X_test, Y_test)\nprint(\"Accuracy on training data: {:2f}\".format(training_accuracy))\nprint(\"Accuracy on test data:     {:2f}\".format(test_accuracy))\n", "intent": "5) Use TF-IDF weighting instead of word counts\n"}
{"snippet": "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\nfor f in categorical:\n        if train_df[f].dtype == 'object':\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))\n            features_to_use.append(f)\n", "intent": "Encode categorical values into numerical values between 0 and n_classes - 1 (different from get_dummies\n"}
{"snippet": "X_train = pd.read_json(\"train.json\")\nX_test = pd.read_json(\"test.json\")\n", "intent": "Here both \"listing_id\" and 'created' represents the order when the post was created.\n"}
{"snippet": "train = pd.read_csv('cleaned_train.csv', header = 0)\n", "intent": "This training data is exported data from previous statistical analysis.\n"}
{"snippet": "for i in range(len(train_stack_list)):\n    stat = pd.DataFrame(train_stack_list[i], columns = column_name_list[i])\n    stat['row_id'] = range(stat.shape[0])\n    train_df = pd.merge(train_df, stat)\nfor i in range(len(test_stack_list)):\n    stat = pd.DataFrame(test_stack_list[i], columns = column_name_list[i])\n    stat['row_id'] = range(stat.shape[0])\n    test_df = pd.merge(test_df, stat)\n", "intent": "Both train_stack_list and test_stack_list are of dimension (10, 49352, 6).\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nimport itertools\nvectorizer = CountVectorizer(min_df=1, stop_words = 'english')\nx = vectorizer.fit_transform(itertools.chain.from_iterable(train['features']))\nnames = vectorizer.get_feature_names()\ncounts = np.sum(x.toarray(), axis=0)\nn2c = list(zip(names, counts))\nn2c.sort(key=lambda x: x[1], reverse=True)\nprint(n2c)\n", "intent": "Let's first extract important features from the training dataset.\n"}
{"snippet": "a = pd.read_csv(\"FE6_rf.csv\", header = 0)\nb = pd.read_csv(\"FE6_xgb.csv\", header = 0)\nc = pd.read_csv(\"FE6_withnan_lgbm.csv\", header = 0)\nd = pd.read_csv(\"FE6_lgbm.csv\", header = 0)\navg = pd.DataFrame()\navg['listing_id'] = a['listing_id']\navg['high'] = (a['high'] + b['high'] + c['high'] + d['high'])/4.0\navg['medium'] = (a['medium'] + b['medium'] + c['medium'] + d['medium'])/4.0\navg['low'] = (a['low'] + b['low'] + c['low'] + d['low'])/4.0\navg.to_csv('FE6_4avg.csv', index = False)\n", "intent": "Decrease the prediction logloss slightly.\n"}
{"snippet": "validation_size = 0.30\nseed = 2018\nx = train_df.values\nX_train, X_validation, Y_train, Y_validation = train_test_split(x, y, test_size = validation_size, random_state = seed)\n", "intent": "Split the training data into train/validation data.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nimport itertools\nvectorizer = CountVectorizer(min_df=1)\nx = vectorizer.fit_transform(itertools.chain.from_iterable(train['features']))\nnames = vectorizer.get_feature_names()\ncounts = np.sum(x.toarray(), axis=0)\nn2c = list(zip(names, counts))\nn2c.sort(key=lambda x: x[1], reverse=True)\nprint(n2c[:30])\n", "intent": "Let's first extract important features from the training dataset.\n"}
{"snippet": "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()\n", "intent": "Is there any remaining missing value ? \n"}
{"snippet": "cls['homepage'] = cls['homepage'].fillna('').apply(lambda x: 0 if x == '' else 1)\ng = sns.PairGrid(data=cls, x_vars=['homepage'], y_vars='return', size=5)\ng.map(sns.pointplot, color=sns.xkcd_rgb[\"plum\"])\ng.set(ylim=(0, 1))\n", "intent": "It seems that movies that belong to a franchise have a higher probability of being a success. \n"}
{"snippet": "train_X, test_X, train_y, test_y = train_test_split(X, y, \n                                                    train_size=0.5, \n                                                    random_state=123,\n                                                    stratify=y)\nprint('All:', np.bincount(y) / float(len(y)) * 100.0)\nprint('Training:', np.bincount(train_y) / float(len(train_y)) * 100.0)\nprint('Test:', np.bincount(test_y) / float(len(test_y)) * 100.0)\n", "intent": "So, in order to stratify the split, we can pass the label array as an additional option to the `train_test_split` function:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n", "intent": "Again, we start by splitting our dataset into a training (75%) and a test set (25%):\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ntext_train, text_test, y_train, y_test = train_test_split(text, y, \n                                                          random_state=42,\n                                                          test_size=0.25,\n                                                          stratify=y)\n", "intent": "Next, we split our dataset into 2 parts, the test and training dataset:\n"}
{"snippet": "print('CountVectorizer defaults')\nCountVectorizer()\n", "intent": "Now, we use the CountVectorizer to parse the text data into a bag-of-words model.\n"}
{"snippet": "from sklearn.datasets import load_digits\nfrom sklearn.neighbors import KNeighborsClassifier\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, random_state=0)\nparam_grid = {'n_neighbors': [1, 3, 5, 10, 50]}\ngs = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=5, verbose=3)\ngs.fit(X_train, y_train)\nprint(\"Score on test set: %f\" % gs.score(X_test, y_test))\nprint(\"Best parameters: %s\" % gs.best_params_)\n", "intent": "Apply grid-search to find the best setting for the number of neighbors in ``KNeighborsClassifier``, and apply it to the digits dataset.\n"}
{"snippet": "from sklearn.manifold import TSNE\ntsne = TSNE(random_state=42)\ndigits_tsne = tsne.fit_transform(digits.data)\n", "intent": "Using a more powerful, nonlinear techinque can provide much better visualizations, though.\nHere, we are using the TSNE manifold learning method:\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nh_pipeline = Pipeline((\n    ('vec', HashingVectorizer(encoding='latin-1')),\n    ('clf', LogisticRegression(random_state=1)),\n))\nh_pipeline.fit(docs_train, y_train)\n", "intent": "Finally, let us train a LogisticRegression classifier on the IMDb training subset:\n"}
{"snippet": "df['one'].fillna(df['one'].mean())\n", "intent": "- Another common way to impute is by the mean of the column.\n"}
{"snippet": "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n", "intent": "**Skewed features**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nitrain, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itest] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "A, b = load_svmlight_file(\"../data/ionosphere.txt\")\n", "intent": "or use a real dataset in LibSVM format\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\n", "intent": "For this exercise, we'll use Fisher's Iris Data Set:\n"}
{"snippet": "df = pd.read_csv('./datasets/breast_cancer_wisconsin/wdbc.data', \n                 header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "mle = MultiColumnLabelEncoder()\nle = LabelEncoder()\nohe = OneHotEncoder()\n", "intent": "https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn\n"}
{"snippet": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten, \\\n                                            Convolution2D, MaxPooling2D,Conv2D\nfrom keras.utils import np_utils\nfrom keras import backend\nimport tensorflow\nfrom keras.datasets import mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n", "intent": "https://elitedatascience.com/keras-tutorial-deep-learning-in-python\n"}
{"snippet": "users = pd.read_csv('takehome_users.csv', encoding = \"ISO-8859-1\", parse_dates=['creation_time'])\nengagement = pd.read_csv('takehome_user_engagement.csv', encoding = \"ISO-8859-1\", parse_dates=['time_stamp'])\n", "intent": "Read CSV files and explore the dataset properties\n"}
{"snippet": "logins = pd.read_json('logins.json')\nlogins.info()\nlogins.head()\n", "intent": "**Import login data into pandas dataframe**\n"}
{"snippet": "riders_data = pd.get_dummies(riders, columns=['city'], drop_first=True)\nriders_data = pd.get_dummies(riders_data, columns=['phone'], dummy_na=True, drop_first=True)\nriders_data.ultimate_black_user = riders_data.ultimate_black_user.astype(np.int)\nriders_data.retained = riders_data.retained.astype(np.int)\nriders_data[['avg_rating_of_driver','avg_rating_by_driver']] = riders_data[['avg_rating_of_driver', 'avg_rating_by_driver']].apply(lambda x: x.fillna(x.mean()))\n", "intent": "**Prepare dataframe for modeling**\n"}
{"snippet": "columns=['rf','et','logit','nb','xgb','lgb']\ntrain_pred_df_list=[rf_train_pred_df, et_train_pred_df, logit_train_pred_df, nb_train_pred_df,\n                    xgb_train_pred_df, lgb_train_pred_df]\ntest_pred_df_list=[rf_test_pred_df, et_test_pred_df, logit_test_pred_df, nb_test_pred_df,\n                    xgb_test_pred_df, lgb_test_pred_df]\nlv1_train_df=pd.DataFrame(columns=columns)\nlv1_test_df=pd.DataFrame(columns=columns)\nfor i in range(0,len(columns)):\n    lv1_train_df[columns[i]]=train_pred_df_list[i]['prediction_probability']\n    lv1_test_df[columns[i]]=test_pred_df_list[i]['prediction_probability']\n", "intent": "Let's group ouf level 1 OOF predictions output together to genenerate the input for level 2 stacking\n"}
{"snippet": "predict_whole = predict_start + ['avg_rating_of_driver', 'avg_rating_by_driver', 'avg_surge', 'surge_pct', 'weekday_pct']\nX_whole = riders_data[predict_whole].values\nXw_train, Xw_test, yw_train, yw_test = train_test_split(X_whole, y, train_size=0.8)\n", "intent": "* **Logistic regression** grid search using all features (even those extending beyond the first 30 days)\n"}
{"snippet": "scaler = StandardScaler()\nto_scale = popular_prescribers[['LISINOPRIL', 'presc_drug_count']]\nscaler.fit(to_scale.values)\nscaled = scaler.transform(to_scale.values)\nlogd = np.log1p(scaled)\npopular_prescribers.loc[:,'LISINOPRIL.transf'] = logd[:,0].reshape(-1,1)\npopular_prescribers.loc[:,'presc_drug_count_transf'] = logd[:,1].reshape(-1,1)\n", "intent": "We will standard scale the variables, and also try seeing if taking a log might help. \n"}
{"snippet": "scaler = StandardScaler()\ntop_19_drugs = [drug for drug in top_20_drugs if drug != 'LISINOPRIL']\nto_scale = popular_prescribers[top_19_drugs]\nscaler.fit(to_scale.values)\nscaled = scaler.transform(to_scale.values)\nlogd = np.log1p(scaled)\ntransf_drugs_df = pd.DataFrame(logd, columns = ['{}.transf'.format(drug) for drug in top_19_drugs])\npopular_prescribers_transf = pd.concat([popular_prescribers, transf_drugs_df], axis=1 ).fillna(0)\npopular_prescribers_transf.loc[:, dummy_columns] = popular_prescribers_transf[dummy_columns].astype(np.int64)\n", "intent": "Now we'll see how this simple exponential transformation to the target variable affects performance for other drugs. \n"}
{"snippet": "tweets_with_originals = pd.read_csv('tweets_with_originals.csv')\n", "intent": "There are tweets without the original text.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words = stopwords, strip_accents = 'unicode', min_df = 10)\ntweet_input = tweets_with_originals.text_clean\ntweet_input=tweet_input.str.replace(r\" (\\d|\\W)+\",\"\") \nT = tfidf_vectorizer.fit_transform(tweet_input) \n", "intent": "Using TF-IDF, we'll look for additional stopwords \n"}
{"snippet": "from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\ny = tweets_with_originals.airline_sentiment.map({'negative': 0, 'neutral':1, 'positive':2})\ntfidf_learning_inputs = []\nfor M in tfidf_input_mats:\n    T_train, T_test, y_train, y_test = train_test_split(T, y, test_size=0.3)\n    sm = SMOTE() \n    T_train_sm, y_train_sm = sm.fit_sample(T_train, y_train.ravel())\n    tfidf_learning_inputs.append([T_train_sm, T_test, y_train_sm, y_test])\n", "intent": "There are a lot of strategies we could try to rebalance the dataset. The easiest would be oversampling the neutral/positive sentiment tweets. \n"}
{"snippet": "boston_df = pd.read_csv('Boston.csv')\nboston_df.info()\n", "intent": "see http://www-bcf.usc.edu/~gareth/ISL/\n"}
{"snippet": "PCA_set = PCA(n_components=4)\nY = PCA_set.fit_transform(Xvote)\n", "intent": "Now, conduct a PCA using scikit learn\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "X_scaled = pd.DataFrame(data=X_scaled_np, columns=data.drop(columns=['Class']).columns)\nX_scaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "lv2_columns=['rf_lf2', 'logit_lv2', 'xgb_lv2','lgb_lv2']\ntrain_lv2_pred_list=[rf_lv2_train_pred, logit_lv2_train_pred, xgb_lv2_train_pred, lgb_lv2_train_pred]\ntest_lv2_pred_list=[rf_lv2_test_pred, logit_lv2_test_pred, xgb_lv2_test_pred, lgb_lv2_test_pred]\nlv2_train=pd.DataFrame(columns=lv2_columns)\nlv2_test=pd.DataFrame(columns=lv2_columns)\nfor i in range(0,len(lv2_columns)):\n    lv2_train[lv2_columns[i]]=train_lv2_pred_list[i]\n    lv2_test[lv2_columns[i]]=test_lv2_pred_list[i]\n", "intent": "On level 3, we follow simlar workflow as level 2. First we put the OOF output from level 2 together, and then send them to our chosen algorithms.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(df_feat_scaled, df['TARGET CLASS'], test_size=0.3)\n", "intent": "**Use train_test_split to split your data into a training set and a testing set.**\n"}
{"snippet": "X_cv = count_vectorizer.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "X = yelp_class.text\ny = yelp_class.stars\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "train_data_10users.to_csv(os.path.join(PATH_TO_DATA, \n                                       'train_data_10users.csv'), \n                        index_label='session_id', float_format='%d')\ntrain_data_150users.to_csv(os.path.join(PATH_TO_DATA, \n                                        'train_data_150users.csv'), \n                         index_label='session_id', float_format='%d')\n", "intent": "**Write dataframes to csv files for further analysis.**\n"}
{"snippet": "img_path = '../data/convnet_vis/cat.jpg'\nfrom keras.preprocessing import image\nimport numpy as np\nimg = image.load_img(img_path, target_size=(150, 150))\nimg_tensor = image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\nimg_tensor /= 255.\nprint(img_tensor.shape)\n", "intent": "This will be the input image we will use -- a picture of a cat, not part of images that the network was trained on:\n"}
{"snippet": "import cv2\nimg = cv2.imread(img_path)\nheatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\nheatmap = np.uint8(255 * heatmap)\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\nsuperimposed_img = heatmap * 0.4 + img\n", "intent": "Finally, we will use OpenCV to generate an image that superimposes the original image with the heatmap we just obtained:\n"}
{"snippet": "X1 = PolynomialFeatures(1).fit_transform(df.age.reshape(-1,1))\nX2 = PolynomialFeatures(2).fit_transform(df.age.reshape(-1,1))\nX3 = PolynomialFeatures(3).fit_transform(df.age.reshape(-1,1))\nX4 = PolynomialFeatures(4).fit_transform(df.age.reshape(-1,1))\nX5 = PolynomialFeatures(5).fit_transform(df.age.reshape(-1,1))\ny = (df.wage > 250).map({False:0, True:1}).as_matrix()\nprint('X4:\\n', X4[:5])\nprint('y:\\n', y[:5])\n", "intent": "Create polynomials for 'age'. These correspond to those in R, when using raw=TRUE in poly() function.\n"}
{"snippet": "allData = pd.read_csv('data/data.csv')\ndata = allData[allData['shot_made_flag'].notnull()].reset_index()\ndata['game_date_DT'] = pd.to_datetime(data['game_date'])\ndata['dayOfWeek']    = data['game_date_DT'].dt.dayofweek\ndata['dayOfYear']    = data['game_date_DT'].dt.dayofyear\ndata['secondsFromPeriodEnd']   = 60*data['minutes_remaining']+data['seconds_remaining']\ndata['secondsFromPeriodStart'] = 60*(11-data['minutes_remaining'])+(60-data['seconds_remaining'])\ndata['secondsFromGameStart']   = (data['period'] <= 4).astype(int)*(data['period']-1)*12*60 + (data['period'] > 4).astype(int)*((data['period']-4)*5*60 + 3*12*60) + data['secondsFromPeriodStart']\ndata.loc[:10,['period','minutes_remaining','seconds_remaining','secondsFromGameStart']]\n", "intent": "show the newly created fields as a sanity check\n"}
{"snippet": "featureInds = mainLearner.feature_importances_.argsort()[::-1]\nfeatureImportance = pd.DataFrame(np.concatenate((featuresDB.columns[featureInds,None], mainLearner.feature_importances_[featureInds,None]), axis=1),\n                                  columns=['featureName', 'importanceET'])\nfeatureImportance.iloc[:30,:]\n", "intent": "look at the feature importances according to ET Classifier\n"}
{"snippet": "submission=sample_submission.copy()\nsubmission['target']=logit_lv3_test_pred*0.5+ xgb_lv3_test_pred*0.5\nfilename='stacking_demonstration.csv.gz'\nsubmission.to_csv(filename,compression='gzip', index=False)\n", "intent": "Well, for training score, we manage to arravie at 0.28443.\nWe can now try to apply the same weight distribution to generate our submission.\n"}
{"snippet": "from io import StringIO\nfrom sklearn.feature_extraction.text import CountVectorizer\nsent1 = \"The quick brown fox jumps over the lazy brown dog.\"\nsent2 = \"Mr brown jumps over the lazy fox.\"\nwith StringIO('\\n'.join([sent1, sent2])) as fin:\n    count_vect = CountVectorizer(analyzer=preprocess_text)\n    count_vect.fit_transform(fin)\ncount_vect.vocabulary_ \n", "intent": "Or just **override the analyzer** totally with our preprocess text:\n"}
{"snippet": "count_vect = CountVectorizer(analyzer=preprocess_text)\nfull_train_set = count_vect.fit_transform(df_train['request_text_edit_aware'])\nfull_tags = df_train['requester_received_pizza']\ntest_set = count_vect.transform(df_test['request_text_edit_aware'])\nclf = MultinomialNB() \nclf.fit(full_train_set, full_tags) \n", "intent": "More data == better model (in most cases)\n"}
{"snippet": "grailPOS = pd.Series(grail.count_by(spacy.attrs.POS))/len(grail)\npridePOS = pd.Series(pride.count_by(spacy.attrs.POS))/len(pride)\nrcParams['figure.figsize'] = 16, 8\ndf = pd.DataFrame([grailPOS, pridePOS], index=['Grail', 'Pride'])\ndf.columns = [tagDict[column] for column in df.columns]\ndf.T.plot(kind='bar')\n", "intent": "It's fun to compare the distribution of parts of speech in each text: \n"}
{"snippet": "def verbsToMatrix(verbCounts): \n    return pd.Series({t[0]: t[1] for t in verbCounts})\nverbsDF = pd.DataFrame({'Elizabeth': verbsToMatrix(elizabethVerbs), \n                        'Darcy': verbsToMatrix(darcyVerbs), \n                        'Jane': verbsToMatrix(janeVerbs)}).fillna(0)\nverbsDF.plot(kind='bar', figsize=(14,4))\n", "intent": "We can now merge these counts into a single table, and then we can visualize it with Pandas. \n"}
{"snippet": "inaugural = [nlp(open(doc, errors='ignore').read()) for doc in inauguralFilenames]\n", "intent": "Let's load the Inaugural Address documents into SpaCy to analyze things like average sentence length. SpaCy makes this really easy. \n"}
{"snippet": "df_fn = pd.DataFrame(fn_sorted, columns=['right_prob', 'test_idx', 'dataset_idx'])\ndf_fp = pd.DataFrame(fp_sorted, columns=['right_prob', 'test_idx', 'dataset_idx'])\n", "intent": "Each of fp_sorted and fn_sorted has list of tuples (probability of their right class, instance's index in the dataset)\n"}
{"snippet": "X.to_csv('data/training_features.csv', index = False)\nX_test.to_csv('data/testing_features.csv', index = False)\ny.to_csv('data/training_labels.csv', index = False)\ny_test.to_csv('data/testing_labels.csv', index = False)\n", "intent": "The naive method of guessing the median training value provides us a low baseline for our models to beat! \n"}
{"snippet": "df_fn = pd.DataFrame(fn_sorted, columns=['right_prob', 'test_idx'])\ndf_fp = pd.DataFrame(fp_sorted, columns=['right_prob', 'test_idx'])\n", "intent": "Each of fp_sorted and fn_sorted has list of tuples (probability of their right class, instance's index in the dataset)\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import ShuffleSplit\nimport visuals as vs\ndata = pd.read_csv('housing.csv')\nprices = data['MEDV']\nfeatures = data.drop('MEDV', axis = 1)\nprint \"Boston housing dataset has {} data points with {} variables each.\".format(*data.shape)\n", "intent": "[Dataset download link](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)\n"}
{"snippet": "train = pd.read_csv(\"../testdata/train.csv\")\ntest    = pd.read_csv(\"../testdata/test.csv\")\nfull = train.append( test , ignore_index = True )\ntitanic = full[ :891 ]\ndel train , test\nprint ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)\n", "intent": "Now that our packages are loaded, let's read in and take a peek at the data.\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "zip_pop_df = pd.read_csv('zip-pop',delimiter='\\t')\nzip_inc_df = pd.read_csv('zip-income',delimiter='\\t')\nzip_inc_df.drop(['rank','location','city','population'], axis=1, inplace = True)\nzip_inc_df.columns = ['zip_code','ave_income']\nzip_pop_df.columns = ['\nzip_pop_df.drop('\nzip_data_df = pd.merge(zip_pop_df,zip_inc_df,on='zip_code')\nzip_data_df['city'] = zip_data_df['city'].str[:-6]\n", "intent": "Add data from Zipatlas.com.  Median Income and population per zip code in Iowa.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(stop_words='english')\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "polyLonModel = svm.SVR()\nX_train, X_test, y_train, y_test = train_test_split(polyFeatures[['lat','lon','latlon']], dataFt['height'], test_size=0.2)\npolyLonModel.fit(X_train, y_train)\n", "intent": "Ok! let's try with our new feature!\n"}
{"snippet": "def load_data(filename1, filename2):\n    X_complete = pd.read_csv(filename1, usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n    Y_complete = pd.read_csv(filename1, usecols=[20])\n    X_holdout = pd.read_csv(filename2)\n    return X_complete, Y_complete,X_holdout\nX_complete,Y_complete, X_holdout = load_data('data.csv','holdout.csv')\n", "intent": "We get X_complete, Y_complete from data.csv, and get holdout_data from holdout.csv.  \n"}
{"snippet": "df = pd.DataFrame()\nentries = {}\nfor dfStock in dfs:\n    entries[dfStock['Ticker'][0]] = sum(dfStock.isnull().values.ravel())\ndf = df.append(entries,ignore_index=True)\nprint \"Missing value count for each stock\"\ndisplay(df)\ndf.plot(kind='bar',title=\"Missing Value Cells\",figsize=(18,9)).set(xlabel='Ticker', ylabel='Count')\n", "intent": "Let's try to find out whether there are missing values\n"}
{"snippet": "dates = dfs[0]['Date'].values\ndates_list = [dt.datetime.strptime(date, \"%Y-%m-%d\") for date in dates]\ndf = pd.DataFrame(index=dates_list)\ndfNormalized = pd.DataFrame(index=dates_list)\nfor dfStock in dfsProcessed:\n    dfTemp = Util().FilterDataFrameByDate(dfStock.copy(),dates_list[0],dates_list[-1])\n    df[dfStock['Ticker'][0]] = dfTemp['Adjusted_Close'].values\n    dfNormalized[dfStock['Ticker'][0]] = dfTemp['Norm_Adjusted_Close'].values\ndf.plot(title=\"Ajusted Close Prices\",figsize=(18,9)).set(xlabel='Date', ylabel='Price')\ndfNormalized.plot(title=\"Normalized Ajusted Close Prices\",figsize=(18,9)).set(xlabel='Date', ylabel='Price Variation')\n", "intent": "The below figure shows the prices behaviour after pre-processing data:\n"}
{"snippet": "table = pd.DataFrame({'probability':[0.25, 1/3, 2/3, .95]})\ntable['odds'] = table.probability / (1 - table.probability)\ntable\n", "intent": "**Exercise.**\nConvert the following probabilities to odds:\n1. .25\n1. 1/3\n1. 2/3\n1. .95\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfeature_cols = ['gre']\nX = admissions.loc[:, feature_cols]\ny = admissions.loc[:, 'admit']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 46)\nlogit_simple = LogisticRegression()\nlogit_simple.fit(X_train, y_train)\n", "intent": "**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**\n"}
{"snippet": "train = pd.read_csv(dataf,index_col=0)\nfeatures_names = list(train)[1:]\n", "intent": "Next, we give this dataset the `train` name.\n"}
{"snippet": "localtrain, localval=train_test_split(train, test_size=0.25, random_state=2017)\ndrop_cols=['id','target']\ny_localtrain=localtrain['target']\nx_localtrain=localtrain.drop(drop_cols, axis=1)\ny_localval=localval['target']\nx_localval=localval.drop(drop_cols, axis=1)\n", "intent": "Next step, let's create the local train and validation data for this little exercise\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\ndata.head(5)\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "num_trees = [100,500,1000,5000,10000]\noob_score_RF = []\nfor i in num_trees:\n    rf = RandomForestRegressor(n_estimators=i,max_features=6,min_samples_leaf=5,oob_score=True)\n    rf.fit(X,y)\n    oob_score_RF.append(rf.oob_score_)\noob_score_df = pd.DataFrame({'num_trees':num_trees,'oob_scores':oob_score_RF})\nprint oob_score_df\noob_score_df.plot(x='num_trees',y='oob_scores')\n", "intent": "Six features to use at random per node performs best\n"}
{"snippet": "num_samples = range(1,11)\noob_score_RF = []\nfor i in num_samples:\n    rf = RandomForestRegressor(n_estimators=1000,max_features=6,min_samples_leaf=i,oob_score=True)\n    rf.fit(X,y)\n    oob_score_RF.append(rf.oob_score_)\noob_score_df = pd.DataFrame({'num_samples':num_samples,'oob_scores':oob_score_RF})\nprint oob_score_df\noob_score_df.plot(x='num_samples',y='oob_scores')\n", "intent": "500 or 1000 estimators performs best - Varies based on run\n"}
{"snippet": "clf = PCA(4)\nX_trans = clf.fit_transform(demean(X))\nX_trans.shape\n", "intent": "Two principle components capture over 99% of variance\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import mean_squared_error\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)\ntreereg = DecisionTreeRegressor(max_depth=4)\ntreereg.fit(X_train,y_train)\n", "intent": "Depth of 4 appears to be ideal\n"}
{"snippet": "df = pd.read_csv('./AVGO.csv')\ndf1 = pd.read_csv('./AAPL.csv')\ndf2 = pd.read_csv('./SMH.csv')\n", "intent": "The spreadhseets were downloaded from Yahoo Finance. They are also part of the capstone project repository.\n"}
{"snippet": "df_scaled = pd.DataFrame(scaled, columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df = pd.read_csv('College_Data')\ndf.rename(columns={\"Unnamed: 0\":\"College Name\"}, inplace=True)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "X = df_feat\ny = df['TARGET CLASS']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Use train_test_split to split your data into a training set and a testing set.**\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nX,y = datasets.load_diabetes(return_X_y=True)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nreg = xgb.XGBRegressor(objective=squared_log_error,n_estimators=200)\nreg.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=10,eval_metric=rmsle_metric)\n", "intent": "Trying to fit diabetes dataset. Here we use our custom objective function log_error\n"}
{"snippet": "data = pd.read_csv('data/bank.csv', header=0, delimiter=';')\ndata = data.dropna()\nprint(data.shape)\nprint(list(data.columns))\n", "intent": "This dataset provides the customer information. It includes 41188 records and 21 fields.\n"}
{"snippet": "data = pd.read_csv('data/daily_weather.csv')\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nCreating a Pandas DataFrame from a CSV file<br></p>\n"}
{"snippet": "import pandas as pd\ntrain = pd.read_csv(\"../data/train.csv\")\n", "intent": "1. Drop missing values\n2. Fill missing values with test statistic\n3. Predict missing values with a machine learning algorithm\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values = 'NaN', strategy ='mean', axis = 0)\nimp.fit(train)\ntrain = imp.transform(train)\n", "intent": "Alternative way of filling missing value with test statistic is by using our Imputer method found in sklearn.preprocessing.\n"}
{"snippet": "data['Age']= data.fillna(data.Age.median())\ndata['Fare'] = data.fillna(data.Fare.median())\ndata['Embarked'] = data['Embarked'].fillna('S')\ndata.info()\n", "intent": "* Impute missing values:\n"}
{"snippet": "model_results = []\nfor _ in range(100):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=50)\n    vect = CountVectorizer(min_df=2)\n    X_trans = vect.fit_transform(X_train.text)\n    model = MultinomialNB().fit(X_trans, y_train)\n    acc = model.score(vect.transform(X_test.text), y_test)\n    model_results.append(acc)\nsum(model_results)/len(model_results)\n", "intent": "Because the model is perfect, the prediction will completely line up with what we had before.\n"}
{"snippet": "key_pts_frame = pd.read_csv('/data/training_frames_keypoints.csv')\nn = 0\nimage_name = key_pts_frame.iloc[n, 0]\nkey_pts = key_pts_frame.iloc[n, 1:].as_matrix()\nkey_pts = key_pts.astype('float').reshape(-1, 2)\nprint('Image name: ', image_name)\nprint('Landmarks shape: ', key_pts.shape)\nprint('First 4 key pts: {}'.format(key_pts[:4]))\n", "intent": "Then, let's load in our training data and display some stats about that dat ato make sure it's been loaded in correctly!\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'splice_train.csv')\nsplice_train = pd.read_csv(data_path, delimiter = ',')\nprint(\"No. of instances: \", splice_train.shape[0], \"No. of attributes: \", splice_train.shape[1])\nsplice_train.head(10)\n", "intent": "Load the `splice_train.csv` and `splice_test.csv` into two separate dataframes. Display the shape and first 10 instances for each dataframe.\n"}
{"snippet": "pd.DataFrame(np.exp(logr.coef_).T, index = X.columns.values)\n", "intent": "Look at the result of our logistic regression, taking into account how the odds change when compared to each of our control dummy variables:\n"}
{"snippet": "X,y = datasets.load_boston(return_X_y=True)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nreg = xgb.XGBRegressor(objective=squared_log_error,n_estimators=200)\nreg.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=10,eval_metric=rmsle_metric)\n", "intent": "The same for boston dataset boston. Using custom objective here\n"}
{"snippet": "from sklearn.feature_selection import RFE\nrfe = RFE(logr, 10)\nrfe = rfe.fit(X,y)\n", "intent": "Use Recursive Feature Elimination to see if we can hone in on some attributes to rebuild a logr model\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_boston\nboston = load_boston()\nX = pd.DataFrame(boston.data)\ny = pd.DataFrame(boston.target)\n", "intent": "Load the Boston housing data.  Fix any problems, if applicable.\n"}
{"snippet": "import patsy\nX = patsy.dmatrix('~ C(PdDistrict) + C(Resolution) + C(Weekday)', df)\nX = pd.DataFrame(X, columns=X.design_info.column_names)\n", "intent": "If you're having trouble with patsy formulas, read more at http://patsy.readthedocs.io/en/latest/formulas.html\n"}
{"snippet": "df_1k_H = pd.read_csv('../assets/_CSVs/df_1k_H.csv', index_col=0)\ndf_1k_W = pd.read_csv('../assets/_CSVs/df_1k_W.csv', index_col=0)\ndf_1k_S = pd.read_csv('../assets/_CSVs/df_1k_S.csv', index_col=0)\n", "intent": "1000-, 500-, and 100-record samples for each original writer - Twain, Wilde, Lincoln, and Modern\n"}
{"snippet": "df_1k_H = pd.read_csv('_CSVs/df_1k_H.csv', index_col=0)\ndf_1k_W = pd.read_csv('_CSVs/df_1k_W.csv', index_col=0)\ndf_1k_S = pd.read_csv('_CSVs/df_1k_S.csv', index_col=0)\ndf_500_H = pd.read_csv('_CSVs/df_500_H.csv', index_col=0)\ndf_500_W = pd.read_csv('_CSVs/df_500_W.csv', index_col=0)\ndf_500_S = pd.read_csv('_CSVs/df_500_S.csv', index_col=0)\ndf_100_H = pd.read_csv('_CSVs/df_100_H.csv', index_col=0)\ndf_100_W = pd.read_csv('_CSVs/df_100_W.csv', index_col=0)\ndf_100_S = pd.read_csv('_CSVs/df_100_S.csv', index_col=0)\n", "intent": "1000-, 500-, and 100-record samples for each original writer - Twain, Wilde, Lincoln, and Modern\n"}
{"snippet": "results_df = pd.DataFrame(columns=['pair', 'vec', 'features', 'model', 'm_acc', 'm_prec'])\n", "intent": "I would like to be able so store my results of the various tests.\n"}
{"snippet": "categories = [\n    'alt.atheism',\n    'talk.religion.misc',\n]\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\ndata = fetch_20newsgroups(subset='train', categories=categories)\nprint(\"%d documents\" % len(data.filenames))\nprint(\"%d categories\" % len(data.target_names))\nprint()\n", "intent": "Load some categories from the training set\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfeatures = [c for c in df.columns if c != 'acceptability'] \nfor c in df.columns:\n    df[c] = le.fit_transform(df[c]) \nX = df[features] \ny = df['acceptability'] \n", "intent": "Since most of the features are categorical text we will need to encode them as numbers using the LabelEncoder.\n"}
{"snippet": "import graphviz\nexport_graphviz(dt, out_file=\"mytree.dot\")\nwith open(\"mytree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)\n", "intent": "Next, let's visualize the tree:\n"}
{"snippet": "X,y = datasets.load_boston(return_X_y=True)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nreg = xgb.XGBRegressor(objective='reg:linear',n_estimators=200)\nreg.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=10,eval_metric=rmsle_metric)\n", "intent": "Now switch to objective='reg:linear'. The results are better. Why?\n"}
{"snippet": "X = StandardScaler().fit_transform(X)\n", "intent": "Who does a DBSCAN on unscaled data?! Savages. That's who.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=10)\nskl_pca = pca.fit_transform(X_2)\nskl_pca\n", "intent": "Now, repeat the process with sklearn.\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "air = pd.read_csv('datasets/airport.csv')\n", "intent": "In this case, we want to look at this dataset in an unsupervised manner. \n"}
{"snippet": "import numpy as np\nimport pandas as pd\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ntargets = pd.DataFrame(data.target, columns=['MEDV'])\ndata.feature_names\ndata.target\ntargets.head()\n", "intent": "Let's take a minute to see what the data looks like.\n"}
{"snippet": "X_norm = pd.DataFrame(X_norm,columns = features)\n", "intent": "Conduct additional exploratory data analysis. These must include both univariate and bivariate analyses.\n"}
{"snippet": "URL = 'https://raw.githubusercontent.com/josephofiowa/GA-DSI/master/NHL_Data_GA.csv'\nnhl = pd.read_table(URL, names=['Team','PTS','Rank','TOI','GF','GA','GF60','GA60','GF%','SF','SA','SF60','SA60','SF%','FF','FA','FF60','FA60','FF%','CF','CA','CF60','CA60','CF%','Sh%','Sv%','PDO','PIM'],sep=',')\nnhl = nhl[1:]\nnhl.head()\n", "intent": "Feel free to also do basic EDA. At least check the head()!\n"}
{"snippet": "votes = pd.read_csv(votes_file)\nairport = pd.read_csv(airport_file)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas and handle any preprocessing that it may need. \n"}
{"snippet": "cvt = CountVectorizer(max_df=50)\nX_all = cvt.fit_transform(insults_df[\"Comment\"])\nwords = cvt.vocabulary_\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "X = insults_df['Comment']\ny = insults_df['Insult']\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size=.33)\n", "intent": "Try 70/30 to start.\n"}
{"snippet": "from sklearn import preprocessing    \ndf = pd.DataFrame(['A', 'B', 'B', 'C'], columns=['Col'])    \ndf['Fact'], indexer = pd.factorize(df['Col'])\n", "intent": "[Ref](https://stackoverflow.com/questions/40336502/want-to-know-the-diff-among-pd-factorize-pd-get-dummies-sklearn-preprocessing)\n"}
{"snippet": "import tensorflow as tf\nimport numpy as np\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\n", "intent": "Let's try out using `tf.keras` and Cloud TPUs to train a model on the fashion MNIST dataset.\nFirst, let's grab our dataset using `tf.keras.datasets`.\n"}
{"snippet": "coeffs = pd.DataFrame(model.coef_, columns = iris.feature_names, index =iris.target_names)\ncoeffs\n", "intent": "some cause neg changes other positive changes\n"}
{"snippet": "from sklearn.feature_selection  import SelectKBest\nkbest=SelectKBest(k=5)\n", "intent": "other libraries  google feature selection\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer(\n    binary=True,  \n    stop_words='english', \n    max_features=50, \n)\nX = v.fit_transform(data.title).todense()     \nX = pd.DataFrame(X, columns=v.get_feature_names())\nX.head()\n", "intent": "- `CountVectorizer` builds a feature per word automatically as we did manually for `recipe`, `electronic` above.\n"}
{"snippet": "from sklearn import datasets \ndata = datasets.load_boston()\n", "intent": "from sklearn import datasets\ndata = dataset.load_boston()\n"}
{"snippet": "df = pd.read_csv(\"../../assets/datasets/iris.csv\")\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "df = pd.read_csv(\"../assets/datasets/votes.csv\", index_col=0)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "napolean = StandardScaler().fit_transform(x)\nprint napolean.shape\nprint napolean[0:5]\n", "intent": "Next, create the covariance matrix from the standardized x-values and decompose these values to find the eigenvalues and eigenvectors\n"}
{"snippet": "df = pd.read_csv(\"../../assets/datasets/airport_operations.csv\", index_col=0)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "WORK_DIRECTORY = '/tmp/pytorch-example/cifar-10-data'\ndata_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)\n", "intent": "We will use the tools provided by the SageMaker Python SDK to upload the data to a default bucket.\n"}
{"snippet": "path = Path('.', 'data', 'titanic.csv')\ntitanic = pd.read_csv(path)\ntitanic.head()\n", "intent": "We'll build a classification tree using the Titanic survival data set:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXs_train = ss.fit_transform(X_train)\nXs_test= ss.fit_transform(X_test)\n", "intent": "Make sure to...\n* instantiate a `StandardScaler` object\n* `fit` the scaler on your training data\n* `transform` both your training and test data.\n"}
{"snippet": "pd.DataFrame(example_results)\n", "intent": "Then we pass the results list to pass to a DataFrame.\n"}
{"snippet": "pd.DataFrame(results)\n", "intent": "Use a DataFrame to display your results.\n"}
{"snippet": "def plot_coef(model, top_n = 10):\n    cols = X_train.columns\n    coef = model.coef_\n    zipped = list(zip(cols, coef))\n    zipped.sort(key=lambda x: x[1], reverse = True)\n    top_10 = pd.DataFrame(zipped).head(top_n)\n    bottom_10 = pd.DataFrame(zipped).tail(top_n)\n    return pd.concat([top_10, bottom_10], axis=0).plot.barh(x = 0, y = 1)\n", "intent": "For your best model, \n* plot relevant coefficients using the `plot_coef` functoin.\n"}
{"snippet": "def plot_coef(model, top_n = 10):\n    cols = X_train.columns\n    coef = model.coef_\n    zipped = list(zip(cols, coef))\n    zipped.sort(key=lambda x: x[1], reverse = True)\n    top_10 = pd.DataFrame(zipped).head(top_n)\n    bottom_10 = pd.DataFrame(zipped).tail(top_n)\n    return pd.concat([top_10, bottom_10], axis=0).plot.barh(x = 0, y = 1)\n", "intent": "For your best model, \n* plot relevant coefficients using the `plot_coef` functoin.\n* Interpret coefficients in terms of \\$ value\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=42)\n", "intent": "But in reality our model has to form predictions on *unseen* data. Let's model this situation.\n"}
{"snippet": "from sklearn.datasets import load_iris\ndata = load_iris()\nX = data.data\ny = data.target\nprint(type(X))\nprint(X.shape)\nprint(\"First three rows of data\\n %s\" % X[:3])\nprint(\"First three labels: %s\" % (y[:3]))\n", "intent": "The data set is distributed with sci-kit learn, the only thing we have to do is to important a function and call it.\n"}
{"snippet": "from sklearn.datasets import load_iris\ndata = load_iris()\nX = data.data\ny = data.target\nprint(type(X))\nprint(X.shape)\nprint(\"First three rows of data\\n {}\".format(X[:3]))\nprint(\"First three labels: {}\".format(y[:3]))\n", "intent": "The data set is distributed with sci-kit learn, the only thing we have to do is to important a function and call it.\n"}
{"snippet": "WORK_DIRECTORY = '/tmp/cifar-10-data'\ndata_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)\n", "intent": "We will use the tools provided by the SageMaker Python SDK to upload the data to a default bucket.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"/home/ubuntu/workspace/src/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "data = load_digits()\nX = data.data\nprint(X.shape)\n", "intent": "* Laad de digits dataset.\n* Sla de data samples op onder de variabele 'X'.\n* Hoeveel samples zijn er? En hoeveel features heeft elk sample?\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n", "intent": "* Verdeel de data in een train set (80%) en test set (20%).\n"}
{"snippet": "X = load_digits()\nsamples,col=X.data.shape\nprint(samples)\nprint(col)\n", "intent": "* Laad de digits dataset.\n* Sla de data samples op onder de variabele 'X'.\n* Hoeveel samples zijn er? En hoeveel features heeft elk sample?\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X.data, y, test_size=0.2)\nprint(len(X_train))\nprint(len(X_test))\n", "intent": "* Verdeel de data in een train set (80%) en test set (20%).\n"}
{"snippet": "df_trace=pm.trace_to_dataframe(traces_13_2_2, varnames=['alpha_ed'])\npd.plotting.scatter_matrix(df_trace.iloc[-1000:,0:6], diagonal='kde')\ndf_trace.iloc[burnin::,0:6].corr()\n", "intent": "Below we will look at the parameter correlation plots of the alpha values and see that there are a lot of them!\n"}
{"snippet": "sac = pd.read_csv('Sacramentorealestatetransactions.csv')\nsac.head()\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, Y_pred))\nconfusion = pd.DataFrame(conmat, index=['under_200k', 'over_200k'],\n                         columns=['predicted_under_200k','predicted_over_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "import pandas\nurl = 'http://www.jdawiseman.com/papers/trivia/monopoly-rents.html'\ndfs = pandas.read_html(url, header=1, index_col=0)\n", "intent": "The set of properties is on an HTML table here:\nhttp://www.jdawiseman.com/papers/trivia/monopoly-rents.html\n"}
{"snippet": "response = s3_client.get_object(Bucket=data_bucket_name, Key=file_data)\nresponse_body = response[\"Body\"].read()\ncounties = pd.read_csv(io.BytesIO(response_body), header=0, delimiter=\",\", low_memory=False) \n", "intent": "Grab the data from the CSV file in the bucket.\n"}
{"snippet": "import pandas\nwheel = pandas.read_csv('wheel.csv')\nwheel\n", "intent": "The data is in wheel.csv.\nWe would like to understand the relationship between _seconds_ and _signal_\n"}
{"snippet": "with open('pg1679.txt') as f:\n    one_book = f.read().decode('utf-8',errors='ignore')\n", "intent": "total word count \n[121684, 596920, 18834, 7322, 34693, 92436, 6002, 3991, 38592, 7961, 67483, 69952, 13895, 41845, 136545, 24523, 10014, 88327]\n"}
{"snippet": "pd.read_csv(submission_file_name, index_col='id')\n", "intent": "You can download this file and submit on the Kaggle website or use the Kaggle command line tool's \"submit\" method.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\nimport pandas as pd\nfile = open(\"B1.txt\", 'rt')\ntext = file.read()\ntext=nltk.word_tokenize(text)\nprint(text[0:10])\n", "intent": "Text files for books was obtained from http://www.glozman.com/textpages.html.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\nimport pandas as pd\nfile = open(\"B1.txt\", 'rt')\ntext = file.read()\ntext=nltk.word_tokenize(text)\nprint(text[0:15])\n", "intent": "Text for books was obtained from http://www.glozman.com/textpages.html.\n"}
{"snippet": "feature_cols = ['gre']\nX = admissions.loc[:, feature_cols]\ny = admissions.loc[:, 'admit']\nX_train, X_test, y_train, y_test = (\n    model_selection.train_test_split(X, y, random_state=46)\n)\nlogit_simple = linear_model.LogisticRegression().fit(X_train, y_train)\n", "intent": "**We can predict the `admit` class from `gre` and use a train-test split to evaluate the performance of our model on a held-out test set.**\n"}
{"snippet": "vect = CountVectorizer()\nvect = vect.fit(X_train)\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\n", "intent": "<a id='countvectorizer-model'></a>\n"}
{"snippet": "sanders_df = pd.DataFrame(sanders)\nsanders_df.head()\n", "intent": "> *Hint: this is as easy as passing it to the DataFrame constructor!*\n"}
{"snippet": "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", header = None)\ndata_test = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", header = None, skiprows=1)\ndata.columns = ['age', 'workclass','fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', \n'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'IncomeGroup']\ndata_test.columns = ['age', 'workclass','fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', \n'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'IncomeGroup']\n", "intent": "Now lets read this into a Pandas data frame and take a look.\n"}
{"snippet": "scaler = StandardScaler()\nscaled_data = scaler.fit_transform(X)\n", "intent": " --------------------------------------------------------------------------------------\n"}
{"snippet": "scaled_data_df = pd.DataFrame(scaled_data)\nfor col in scaled_data_df.columns:\n    scaled_data_df[col].hist()\n", "intent": "** Distribution for all columns:**\n"}
{"snippet": "pca_1 = PCA(n_components = 5, random_state=0)\ndata_pca_1 = pca_1.fit_transform(scaled_data)\npca_1\n", "intent": "***The dataset appears to have a normal distribution but some skew to the right.***\n"}
{"snippet": "pd.read_csv('file_name.csv').to_sql('table_name',con=conn,if_exists='replace',index=False)\n", "intent": "Load our csv files into tables\n"}
{"snippet": "airport_df = pd.read_csv('airports.csv')\n", "intent": "Join airport_cancellations.csv and airports.csv into one table (try to do it with both pandas and SQLite)\n"}
{"snippet": "pca = PCA(n_components = 5, random_state=0)\ndata_pca = pca.fit_transform(scaled_data)\npca\n", "intent": "***The dataset appears to have a normal distribution but some skew to the right.***\n"}
{"snippet": "with open('anna.txt', 'r') as f:\n    text = f.read()\nvocab = set(text)\nvocab2int = {c: i for i, c in enumerate(vocab)}\nint2vocab = dict(enumerate(vocab))\nchars = np.array([vocab2int[c] for c in text], dtype=np.int32)\n", "intent": "First we load the text file and convert it into integers for network to use\n"}
{"snippet": "data_train['num_room'].fillna(data_train[\"num_room\"].mean(), inplace=True)\ndata_test['num_room'].fillna(data_train[\"num_room\"].mean(), inplace = True)\ndata_train.head()\ndata_test.head()\n", "intent": "Let's just build a model using the mean num of rooms for the missing values\n"}
{"snippet": "ids_valid = val_ids.iloc[:,0].values\ntemp_df = pd.DataFrame()\ntemp_df[\"images\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(ids_valid)]\ntemp_df[\"masks\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(ids_valid)]\nx_valid = np.array(temp_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\ny_valid = np.array(temp_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\ndel temp_df\ngc.collect()\n", "intent": "I have fixed the validation set for all my experiments. This allows me to easily compare different model performance.\n"}
{"snippet": "train_list = np.random.rand(len(data_bin)) < 0.8\ndata_train = data_bin[train_list]\ndata_val = data_bin[~train_list]\ndata_train.to_csv(\"formatted_train.csv\", sep=',', header=False, index=False) \ndata_val.to_csv(\"formatted_val.csv\", sep=',', header=False, index=False) \ndata_test_bin.to_csv(\"formatted_test.csv\", sep=',', header=False,  index=False) \n", "intent": "Split the data into 80% training and 20% validation and save it before calling XGboost\n"}
{"snippet": "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)\n", "intent": "Using the salt coverage as a stratification criterion. Also show an image to check for correct upsampling.\n"}
{"snippet": "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\ntags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n                     dtype=\"long\", truncating=\"post\")\n", "intent": "Next, we cut and pad the token and label sequences to our desired length.\n"}
{"snippet": "train = pd.read_csv(TRAIN_DATA_FILE)\ntest = pd.read_csv(TEST_DATA_FILE)\nlist_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values\n", "intent": "Read in our data and replace missing values:\n"}
{"snippet": "tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n", "intent": "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed).\n"}
{"snippet": "COMMENT = 'comment_text'\ntrain[COMMENT].fillna(\"unknown\", inplace=True)\ntest[COMMENT].fillna(\"unknown\", inplace=True)\n", "intent": "There are a few empty comments that we need to get rid of, otherwise sklearn will complain.\n"}
{"snippet": "submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\nsubmission.to_csv('submission.csv', index=False)\n", "intent": "And finally, create the submission file.\n"}
{"snippet": "df_train = pd.read_csv('titanic/train_preprocessed.csv')\n", "intent": "Read our preprocessed data from our last lesson, in the `titanic` data folder.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "We can use the `iris` dataset which is built into sklearn.  You can import the dataset directly as follows.\n"}
{"snippet": "pd.DataFrame({'lags':range(30), 'pvalue':sm.stats.diagnostic.acorr_ljungbox(arima_mod.resid.values, lags=30)[1], \n              'critial':np.array([0.05]*30)}).set_index('lags').plot(figsize=(15,5))\n", "intent": "Autocorrelation test\n"}
{"snippet": "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_header, strip_newsgroup_quoting, strip_newsgroup_footer\ndata = []\nfor f in file_list:\n    with open(f, 'rb') as fin:\n        content = fin.read().decode('latin1')        \n        content = strip_newsgroup_header(content)\n        content = strip_newsgroup_quoting(content)\n        content = strip_newsgroup_footer(content)        \n        data.append(content)\n", "intent": "Here we read in the content of all the files and remove the header, footer and quotes (of earlier messages in each email).\n"}
{"snippet": "count_vect_train_transformer = CountVectorizer()\ncount_vect_train = count_vect_train_transformer.fit_transform(train_data)\n", "intent": "*Hint : Use Count vectorizer*\n- .fit()\n- .transform()\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "X = pd.DataFrame(iris.data, columns=iris.feature_names)\nY = iris.target\n", "intent": "Define the \"x\" and \"y\" variables. *Remember*, y is our classifer, and x is our attributes.\n"}
{"snippet": "vote_df = pd.read_csv(votes_file)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "location_data = pd.read_csv('../data/ia_zip_city_county_sqm.csv')\n", "intent": "To ensure accuracy, we are importing a list of Iowa zip codes, cities, counties and county numbers.\n"}
{"snippet": "location_data = pd.read_csv('/Users/stel/joce/data_science/project-3-datasets/ia_zip_city_county_sqm.csv')\n", "intent": "To ensure accuracy, we are importing a list of Iowa zip codes, cities, counties and county numbers.\n"}
{"snippet": "from sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nboston = datasets.load_boston()\nX = boston.data\ny = boston.target\n", "intent": "Use the following code to import the Boston House Prices dataset and linear models in python.\n"}
{"snippet": "from sklearn import datasets\ndiabetes = datasets.load_diabetes()\nX = diabetes.data\ny = diabetes.target\n", "intent": "Use the following codes to import the diabetes dataset.\n"}
{"snippet": "from sklearn import datasets\ndiabetes = datasets.load_diabetes()\nX = diabetes.data\ny = diabetes.target\nfrom sklearn.preprocessing import PolynomialFeatures\nX = PolynomialFeatures(2, include_bias=False).fit_transform(X)\n", "intent": "Import the diabetes dataset as in Q1 and add the interaction variables.\nYou should have 65 variables and one target variable.\n"}
{"snippet": "import tarfile\nfrom sagemaker.session import Session\nwith tarfile.open('onnx_model.tar.gz', mode='w:gz') as archive:\n    archive.add('resnet152v1.onnx')\nmodel_data = Session().upload_data(path='onnx_model.tar.gz', key_prefix='model')\n", "intent": "Now that we have the model data locally, we will need to compress it and upload the tarball to S3 for the SageMaker Python SDK to create a Model\n"}
{"snippet": "images_new = pca.inverse_transform(images_pca)\nprint(\"transformed shape:\", images_new.shape)\n", "intent": "The transformed data has been reduced to a single dimension. Then we reproject the data back using the inverse transform\n"}
{"snippet": "examples = [] \nfor file in frames: \n    with fs.open(file, 'rb') as f: \n        examples.append(imresize(imread(f), .125)) \n", "intent": "Read in the images into an array, downsizing by an eigth\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(examples, target,\n                                                random_state=42, test_size=0.3)\n", "intent": "Now we'll use the utility function ```train_test_split``` provided by scikit-learn, to create a random split of test and train images\n"}
{"snippet": "pca  =  PCA(n_components =10).fit(Xtrain)\nimages_pca = pca.transform(Xtrain)\ntree = DecisionTreeClassifier().fit(images_pca, ytrain)\ntree.score(pca.transform(Xtest), ytest)\n", "intent": "About 10 components explains ~90% of the variance. Let's use those\n"}
{"snippet": "names = [f for f in fs.ls(root+'/imrecog_data/NWPU-RESISC45/train/airplane') if f.endswith('.jpg')]\npositive_patches = []\nfor name in names:\n    with fs.open(name, 'rb') as f:\n        positive_patches.append(color.rgb2gray(imread(f, 'jpg')))\n", "intent": "We need to build libraries of both positive (airplane) and negative (not airplane) examples\n"}
{"snippet": "infile = infile.replace('.segments.pkl', '.jpg')\ntraining_rgb = imread(infile)\n", "intent": "Let's display that matrix ontop of the image\n"}
{"snippet": "infile = 'data/1367407802.Wed.May.01_11_30_02.GMT.2013.jvspeijk.c2.snap.jpg' \ntest_rgb = imread(infile)\ntest_rgb = test_rgb[:nrows_mask,:,:]\ntest_rgb = preprocessing.scale(test_rgb[:,:,0]) \nM_tst, N_tst = test_rgb.shape\n", "intent": "Let's test at an image from the same place but at a different time\n"}
{"snippet": "from scipy.misc import imresize\ninfile = 'data/1367407802.Wed.May.01_11_30_02.GMT.2013.jvspeijk.c2.snap.jpg' \ntest_rgb = imread(infile)\ninfile = 'data/1369558802.Sun.May.26_09_00_02.GMT.2013.jvspeijk.c2.snap.jpg'\ntraining_rgb = imread(infile)\ntraining_rgb = imresize(training_rgb, .125)\ntest_rgb = imresize(test_rgb, .125)\n", "intent": "We're going to use downscaled versions of images to speed up the process\n"}
{"snippet": "with fs.open(root+'/imrecog_data/UCMerced_LandUse/Images/train/beach/beach99.jpg', 'rb') as fim:\n    image = imread(fim) \n", "intent": "Load in a test image and show it\n"}
{"snippet": "WORK_DIRECTORY = 'data'\ntrain_input = sagemaker_session.upload_data(WORK_DIRECTORY, key_prefix=\"{}/{}\".format(prefix, WORK_DIRECTORY) )\n", "intent": "Once we have the data locally, we can use use the tools provided by the SageMaker Python SDK to upload the data to a default bucket. \n"}
{"snippet": "imgr = imresize(img, fct)\nLcr = np.round(imresize(Lc, fct, interp='nearest')/255 * np.max(Lc))\n", "intent": "Step 7: We're going to resize to speed things up a little in the next step (CRF)\n"}
{"snippet": "print('CRF ... ')\nres = getCRF(imgr, Lcr.astype('int'), theta, n_iter, labels, compat_spat, compat_col, scale, prob)\ndel imgr\nresr = np.round(imresize(res, 1/fct, interp='nearest')/255 * np.max(res))\ncode1 = np.unique(res)\ncode2 = np.unique(resr)   \nresrr = np.zeros(np.shape(resr), dtype='int8')\nfor kk in range(len(code1)):\n   resrr[resr==code2[kk]] = code1[kk]   \ndel res, resr\n", "intent": "Step 8: Conditional Random Field post-processing. This takes a couple of minutes\n"}
{"snippet": "from scipy.misc import imresize\nfrom imageio import imread\nimages = []\nfor file in image_files:\n    with fs.open(file, 'rb') as f:\n        images.append(imresize(imread(f), imsize))\n", "intent": "Below we're reading and resizing each image one by one\n"}
{"snippet": "from scipy.io import loadmat\nclasses = []\nfor file in class_files:\n    with fs.open(file, 'rb') as f:\n        tmp = (loadmat(f)['class']!=2).astype('uint8')\n        classes.append(imresize(tmp, imsize).astype('int'))\n", "intent": "The classes are stored in .mat format. Water is class '2'\nWe load in each file, binarize it (water=0, not water=1), and resize it\n"}
{"snippet": "reviews = pd.read_csv('reviews.txt', header=None)\nlabels = pd.read_csv('labels.txt', header=None)\n", "intent": "- store multiple reviews in DataFrame dim(n,1)\n"}
{"snippet": "bank_data = pd.read_csv('./data/bank_campaign_small.csv')\nbank_data.head()\n", "intent": "We will be using the dataset available from [UCI data repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n"}
{"snippet": "import pandas as pd\nfrom io import StringIO\ncsv_data = unicode(csv_data)\ndf = pd.read_csv(StringIO(csv_data))\ndf\n", "intent": "Raschka, Sebastian. Python Machine Learning. Packt Publishing. Kindle Edition. \n"}
{"snippet": "import pandas as pd\nfrom sklearn import model_selection, metrics\nimport numpy as np\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nsample_submission = pd.read_csv(\"sample_submission.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "print(predict, y_val)\nprint(my_scaler.inverse_transform(predict))\nprint(my_scaler.inverse_transform(y_val))\n", "intent": "Predicting all values\n"}
{"snippet": "from sklearn import preprocessing\ninterval_data = df.as_matrix(columns=interval_attributes)\ninterval_imputer = preprocessing.Imputer(strategy='mean')\nimputed_interval_data = interval_imputer.fit_transform(interval_data)\nnominal_data = df.as_matrix(columns=nominal_attributes)\nbinary_data  = df.as_matrix(columns=binary_attributes)\ncat_imputer = preprocessing.Imputer(strategy='most_frequent')\nimputed_nominal_data = cat_imputer.fit_transform(nominal_data)\nimputed_binary_data  = cat_imputer.fit_transform(binary_data)\n", "intent": "- Finding the missing values.\n- Imputing the missing values.\n"}
{"snippet": "def make_xy(critics, vectorizer=None):\n    if vectorizer is None:\n        vectorizer = CountVectorizer(ngram_range=(1, 5))\n    X = vectorizer.fit_transform(critics.quote)\n    X = X.tocsc()  \n    y = (critics.fresh == 'fresh').values.astype(np.int)\n    return X, y\nX, y = make_xy(critics)\n", "intent": "For task 1, we can try 2 to 5 grams. and use the same to build a niave bayes model with the same alpha as before.\n"}
{"snippet": "vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\nX, y = make_xy(critics, vectorizer)\ntrain_X, test_X, train_y, test_y =  train_test_split(X,y, train_size = 0.7)\nclf = MultinomialNB(alpha=1).fit(xtrain, ytrain)\ntraining_accuracy = clf.score(xtrain, ytrain)\ntest_accuracy = clf.score(xtest, ytest)\nprint(\"Accuracy on training data: {:2f}\".format(training_accuracy))\nprint(\"Accuracy on test data:     {:2f}\".format(test_accuracy))\n", "intent": "For task 5, we will use tfidfvectorizer as the example above.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidfvectorizer = TfidfVectorizer(min_df=1, stop_words='english')\nXtfidf=tfidfvectorizer.fit_transform(critics.quote)\nX = Xtfidf.tocsc()  \ny = (critics.fresh == 'fresh').values.astype(np.int)\n", "intent": "For task 5, we will use tfidfvectorizer as the example above.\n"}
{"snippet": "milk = pd.read_csv('./monthly-milk-production.csv', index_col='Month')\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "electoral_votes = pd.read_csv(\"hw2_data/electoral_votes.csv\").set_index('State')\nelectoral_votes.head()\n", "intent": "*As a matter of convention, we will index all our dataframes by the state name*\n"}
{"snippet": "import statsmodels.api as sm\nfrom statsmodels.formula.api import logit, glm, ols\ndat = pd.DataFrame(data, columns = ['Temperature', 'Failure'])\nlogit_model = logit('Failure ~ Temperature',dat).fit()\n", "intent": "Lets plot this data\n"}
{"snippet": "excel = pd.ExcelFile('breatcancer_expr.xlsx').parse(sheet_name=0, dtype=object, engine='xlrd', verbose=True)\nexcel.to_csv(path_or_buf='expr.csv', sep=',', header=True, index=False, mode='w', encoding='CP949')\nexpr = pd.read_csv('expr.csv', engine='c', sep=',', encoding='CP949')\nexcel = pd.ExcelFile('breatcancer_clinical.xlsx').parse(sheet_name=0, dtype=object, engine='xlrd', verbose=True)\nexcel.to_csv(path_or_buf='clinic.csv', sep=',', header=True, index=False, mode='w', encoding='CP949')\nclinical = pd.read_csv('clinic.csv', engine='c', sep=',', encoding='CP949')\n", "intent": "- Load excel files by using Pandas\n- Change excel files to csv files\n- Csv files are faster than excel files when we read data\n"}
{"snippet": "sc = StandardScaler()\nsc.fit(x_train)\nx_train_std = sc.transform(x_train)\nx_test_std = sc.transform(x_test)\n", "intent": "- Use StandardScaler\n- Standardize train data & test data\n"}
{"snippet": "df = MinMaxScaler().fit_transform(df)\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "loan = pd.read_csv(\"loan_data.csv\")\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "kp = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaled_dp = pd.DataFrame(scaled_feature,columns= kp.columns[ :-1])\nscaled_dp.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data = pd.read_csv('https://github.com/Thinkful-Ed/data-201-resources/raw/master/hotel-reviews.csv')\n", "intent": "This notebook will guide you through the creation of a simple bag of words model for text matching.\n"}
{"snippet": "iris_dataframe = pd.DataFrame(X_train, columns = iris_dataset.feature_names)\npd.plotting.scatter_matrix(\n    iris_dataframe,\n    c = y_train,\n    figsize = (15, 15),\n    marker = 'o',\n    hist_kwds = {'bins':20},\n    s = 60,\n    alpha = .8\n)\n", "intent": "<h3>Create Pair Plots</h3>\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    iris_dataset['data'], iris_dataset['target'], random_state = 0\n)\nknn = KNeighborsClassifier(n_neighbors = 1)\nknn.fit(X_train, y_train)\nprint(\"Test set score : {:2f}\".format(knn.score(X_test, y_test)))\n", "intent": "<h3>Training and Evaluation in one line</h3>\n"}
{"snippet": "d = load_iris()\n", "intent": "https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1469-1809.1936.tb02137.x\n"}
{"snippet": "import spacy\nfile = open(\"trump.txt\", \"r\",encoding='utf-8') \ntrump = file.read() \nnlp = spacy.load(\"en\")\ndoc = nlp(trump)\nfor span in doc.sents:\n    print(\"> \", span)\n", "intent": "Let's take one of President Trump's speech and divide into words.\n"}
{"snippet": "from sklearn.manifold import TSNE\nimport pandas as pd\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X)\ndf = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\ndf\n", "intent": "To train we just use the TSNE to reduce the dimensionality:\n"}
{"snippet": "from rasa_nlu.training_data import load_data\nfrom rasa_nlu.model import Trainer, Interpreter\nfrom rasa_nlu.components import ComponentBuilder\nimport rasa_nlu.config\ncfg = 'config.json'\ntraining_data = load_data('anna.json')\ntrainer = Trainer(rasa_nlu.config.load(cfg))\ntrainer.train(training_data)\nmodel_directory = trainer.persist('.')\n", "intent": "The training is straight forward.\n"}
{"snippet": "df = pd.read_csv('College_Data',index_col=0)\n", "intent": "** Import the libraries you usually use for data analysis.**\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint(\"The MNIST database has a training set of %d examples.\" \n      % len(X_train))\nprint(\"The MNIST database has a test set of %d examples.\" % len(X_test))\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the MNIST database.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"/Users/user/Desktop/DataScience-Python3/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "scaler = StandardScaler()\ncustomer_sc = scaler.fit_transform(customer_features)\ncustomer_sc_df = pd.DataFrame(customer_sc, columns=customer_features.columns)\nsc_stats = customer_sc_df.describe().T\nsc_stats['skew'] = st.skew(customer_features)\nsc_stats['kurt'] = st.kurtosis(customer_features)\ndisplay(stats)\ndisplay(sc_stats)\n", "intent": "$$Z = \\frac{X-\\mu}{\\sigma}$$\n"}
{"snippet": "train, test = train_test_split(boston_df, test_size = 0.3)\ntrain.head(5)\n", "intent": "b) Divide the filtered data randomly into a train set (70% of the data) and test set (30% of the data).\n"}
{"snippet": "boston_df = pd.read_csv('Boston1.csv')\nboston_train, boston_test = train_test_split(boston_df, test_size = 0.3)\nboston_df.head(4)\n", "intent": "1) Create train and test sets from the Boston data set.\n"}
{"snippet": "train = pd.read_csv(\"train.csv\")\n", "intent": "- We need to open the file as Pandas DataFrame .. \n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, binarized_labels, test_size=0.3)\n", "intent": "- http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html\n"}
{"snippet": "fa = FactorAnalysis()\nprint 'components'\nprint fa.components_ \n", "intent": "For homoscedastic case, both fa and pca work well\n"}
{"snippet": "data.to_csv(os.path.join(output_dir,'cluster_data_w_label.csv'), index = False, sep = '|')\n", "intent": "** Save the data matrix with the cluster lables **\n"}
{"snippet": "adv = pd.read_csv(\"advertising.csv\")\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "scale = StandardScaler()\nXs = scale.fit_transform(X)\n", "intent": "We know that we need to scale our data for the KNN algorithm right?\n"}
{"snippet": "ss = StandardScaler()\ncities_ss = ss.fit_transform(cities)\ncities_ss = pd.DataFrame(cities_ss, index=cities.index, columns=cities.columns)\ncities_ss.head()\n", "intent": "This data definitely needs scaling\n"}
{"snippet": "data = make_classification(n_samples=400, n_features=2,n_informative=2,\n                           n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, random_state=1)[0]\n", "intent": "Clustering is usually never this easy. Let's look at some more difficult data.\n"}
{"snippet": "ss = StandardScaler()\nXs = ss.fit_transform(X)\nkm3_s = KMeans(n_clusters=3, random_state=10)\nkm3_s.fit(Xs)\nlabs3_s = km3_s.labels_\npd.value_counts(labs3_s)\n", "intent": "Let's go ahead and scale the data and refit a clustering algorithm\n"}
{"snippet": "country_labels = pd.DataFrame(index=df.index, data=labs3_s, columns=[\"cluster\"])\ncountry_labels.head()\n", "intent": "Let's go ahead and look at some of the countries in each cluster.\nWe're going randomly pick 10 countries from each cluster (0, 1, 2)\n"}
{"snippet": "X, y = make_moons(n_samples=3000,\n                    noise=0.12,\n                    random_state=0)\n", "intent": "We're going to use the keras library to the fake moons dataset from sklearn\n"}
{"snippet": "Xr, yr = make_regression(n_samples=2000, n_features=1, noise=2, random_state=4,bias = 0.9)\n", "intent": "Now let's train a neural net on a regression dataset\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.25, random_state=42)\n", "intent": "Try it out on a testing set\n"}
{"snippet": "sample = {\"income\":[30000, 55000, 36000], \n          \"white_pop\":[50, 85, 95], \n          \"college_deg\":[15, 40, 50], \n          \"class\":[\"A\",\"B\", \"X\"]}\nsample= pd.DataFrame(sample)\nsample\n", "intent": "Let's take a look at this sample data set.\n"}
{"snippet": "teams = pd.read_csv(zf.open(tablenames[tablenames.index('Teams.csv')]))\nplayers = pd.read_csv(zf.open(tablenames[tablenames.index('Batting.csv')]))\nsalaries = pd.read_csv(zf.open(tablenames[tablenames.index('Salaries.csv')]))\nfielding = pd.read_csv(zf.open(tablenames[tablenames.index('Fielding.csv')]))\nmaster = pd.read_csv(zf.open(tablenames[tablenames.index('Master.csv')]))\n", "intent": "Create pandas DataFrames for each of the five data sets. \n"}
{"snippet": "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\ndata.head()\nX = data.drop(\"sales\", axis = 1)\ny = data.sales\n", "intent": "Let's use train/test split with RMSE to decide whether Newspaper should be kept in the model:\n"}
{"snippet": "X = titanic.drop(\"survived\", axis =1)\ny = titanic.survived\nX_train, X_test, y_train, y_test = train_test_split(X ,y, test_size = .25, random_state = 4)\n", "intent": "We are going to split our titanic dataset into two sets: training and testing.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X ,y, \n                                                    test_size = .25,\n                                                   random_state = 42)\nmodel = DecisionTreeClassifier(max_depth = 6, random_state= 42)\nmodel.fit(X_train, y_train)\ntestscore = model.score(X_test, y_test)\nprint (\"The test score is {:.3f} percent\".format(testscore*100))\n", "intent": "Train a model with the best depth value and evaluate it on a test set\n"}
{"snippet": "from sklearn.tree import export_graphviz\nimport graphviz\nexport_graphviz(dt, out_file='titanic.dot', \n                    feature_names=X.columns, \n                    class_names=[\"dead\", \"alive\"])\nwith open(\"titanic.dot\") as f: \n        dot_graph = f.read()\ngraphviz.Source(dot_graph)\n", "intent": "Visualize the tree!\n"}
{"snippet": "from sklearn.tree import export_graphviz\nimport graphviz\nexport_graphviz(iris_model, out_file='iris.dot', \n                    feature_names=X.columns, \n                    class_names=y.unique())\nwith open(\"iris.dot\") as f: \n        dot_graph = f.read()\ngraphviz.Source(dot_graph)\n", "intent": "Visualize the decision trees using graphviz\n"}
{"snippet": "le = LabelEncoder()\nemb_encoded = le.fit_transform(X_train.Embarked)\nemb_encoded[:20]\n", "intent": "Now let's try this on the Embarked column\n"}
{"snippet": "onehot = OneHotEncoder()\nemb_onehot = onehot.fit_transform(emb_encoded.reshape(-1, 1))\nemb_onehot.toarray()\n", "intent": "How to use the OneHotEncoder object\n"}
{"snippet": "lb = LabelBinarizer()\nbinarized_data = lb.fit_transform(X_train.Embarked.fillna(\"unknown\"))\nbinarized_data[:30]\n", "intent": "We can also use the LabelBinarizer to do this as well\n"}
{"snippet": "cm_digits = pd.DataFrame(confusion_matrix(y_test, preds))\ncm_digits\n", "intent": "Confusion matrix time\n"}
{"snippet": "import sklearn\ndigits = sklearn.datasets.load_digits()\n", "intent": "Character recognition in images is a classic problem solved with neural networks.\n"}
{"snippet": "path = \"../../data/NLP_data/yelp.csv\"\nyelp = pd.read_csv(path, encoding='unicode-escape')\nyelp.head()\n", "intent": "Let's analyze the sentiment of yelp reviews\n"}
{"snippet": "vect = CountVectorizer()\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\n", "intent": "Use CountVectorizer to create document-term matrices from X_train and X_test\n"}
{"snippet": "pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names()).head()\n", "intent": "Let's put it in a dataframe\n"}
{"snippet": "vect = CountVectorizer(binary=True)\ndf = vect.fit_transform(simple_train).toarray().sum(axis=0)\npd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())\n", "intent": "Binary = True assigns a 1 if a word is present irregardless of count, and 0 for absent words.\n"}
{"snippet": "vect = TfidfVectorizer()\npd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n", "intent": "Let's check out the sklearn version\n"}
{"snippet": "vect = CountVectorizer(stop_words=\"english\")\nX_dtm = vect.fit_transform(X)\n", "intent": "Let's make our first model\n"}
{"snippet": "vect = TfidfVectorizer(stop_words=\"english\")\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\nnb = MultinomialNB()\nnb.fit(X_train_dtm, y_train)\nprint nb.score(X_train_dtm, y_train)\nprint nb.score(X_test_dtm, y_test)\n", "intent": "Let's do this again with the tfidf vectorizer\n"}
{"snippet": "df = pd.read_table(\"../../data/NLP_data/sms.tsv\",encoding=\"utf-8\", names= [\"label\", \"message\"])\ndf.head()\n", "intent": "This is a really helpful technique to find the words most associated with either class.\n"}
{"snippet": "path = \"../../data/NLP_data/yelp.csv\"\nyelp = pd.read_csv(path, encoding='unicode-escape')\nyelp.head()\n", "intent": "To wrap our text classification section, we're going to learn how to incorporate stemming and lemmatization in our vectorizers. \n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "This data set is available in the [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\n"}
{"snippet": "poly = PolynomialFeatures(2)\nXp = poly.fit_transform(X)\n", "intent": "Let's try this exercise again but with the polynomial transformed features.\n"}
{"snippet": "kc = pd.read_csv(\"../../data/kc_house_data.csv\")\n", "intent": "Same exercise that we did in the previous class but now try polynomial and regularized models. on the King County housing dataset.\n"}
{"snippet": "pca = PCA(2)  \ndata_pca = pca.fit_transform(data)\n", "intent": "Let's PCA the digits data using two components\n"}
{"snippet": "pca = PCA(n_components=.5).fit(data)\npca.n_components_\n", "intent": "2 components gets us about 28.5 of the way there, let's see how many components it takes to get 50, 70, and 90%.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=.25, \n                                                    random_state=42)\n", "intent": "Try it out on a testing set\n"}
{"snippet": "X = \ny = \nX_train, X_test, y_train, y_test = train_test_split()\n", "intent": "We are going to split our titanic dataset into two sets: training and testing.\n"}
{"snippet": "from sklearn.datasets import make_classification\ndata = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, \n                    class_sep=.20, random_state = 34)\n", "intent": "Let's bring back to the model plotting function for the purpose of visualizing an overfit model against a test set.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X ,y, \n                                                    test_size = .25,\n                                                   random_state = 42)\ntestscore = \nprint (\"The test score is {:.3f} percent\".format(testscore*100))\n", "intent": "Train a model with the best depth value and evaluate it on a test set\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X ,y, test_size = .25,\n                                                    random_state = 38)\ntrain_errors = []\ntest_errors = []\ndepths = range(1,21)\n", "intent": "Graph 1: Plot validation curve of model complexity versus error rates for training and test sets\n"}
{"snippet": "theurl = \"http://en.wikipedia.org/wiki/Election_Day_(United_States)\"\nwikitable = pd.read_html(theurl, match=\"Midterm\", header=0)[0]\nprint wikitable.head()\n", "intent": "When was the election?\n"}
{"snippet": "vect = \nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape \n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "vect = \ndf = vect.fit_transform(simple_train).toarray().sum(axis=0)\npd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())\n", "intent": "Binary = True assigns a 1 if a word is present irregardless of count, and 0 for absent words.\n"}
{"snippet": "vect = \npd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n", "intent": "Let's check out the sklearn version\n"}
{"snippet": "f=pd.DataFrame([[0.068966, 0.137931, 0.068966], [0.344828, 0.241379, 0.137931]], columns=['T=Hot', 'T=Mild', 'T=Cold'], index=['W=Sunny', \"W=Cloudy\"])\nprint(f)\n", "intent": "Look at the following table:\n"}
{"snippet": "vectorizer=CountVectorizer(stop_words='english', max_df=1.0, min_df=0.025) \n", "intent": "We need to create our Bag-of-Words representation (BoW). Here's how\n"}
{"snippet": "descriptions_bow=vectorizer.fit_transform(corpus)   \ndescription_vocabulary = vectorizer.get_feature_names()  \n", "intent": "As many other objects in Sklearn, CountVectorizer is applied with the function fit_transform\n"}
{"snippet": "lda=LatentDirichletAllocation(n_components=10, learning_method='batch')\nx=lda.fit_transform(descriptions_bow)\n", "intent": "It is finally time to run our LDA! We will try with sklearn first...\n"}
{"snippet": "index_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = []\nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n", "intent": "Since the test data is generated with combination of shops and items, we have to restructure train data to match the test data generation. \n"}
{"snippet": "for feat in sales_m.columns:\n    if 'item_cnt' in feat:\n        sales_m[feat]=sales_m[feat].fillna(0)\n    elif 'item_price' in feat:\n        sales_m[feat]=sales_m[feat].fillna(sales_m[feat].median())\n", "intent": "Fill missing values\n"}
{"snippet": "iris = sklearn.datasets.load_iris()\nX = iris.data\nY = iris.target\n", "intent": "Iris example\n====\nWe take all four features now and apply SVD:\n"}
{"snippet": "import pandas\nstations = pandas.read_csv(\"Divvy_Stations_2016_Q3.csv\")\nbikes = pandas.concat([pandas.read_csv(\"Divvy_Trips_2016_Q3.csv\"),\n                       pandas.read_csv(\"Divvy_Trips_2016_Q4.csv\")])\n", "intent": "We know the stations.\n"}
{"snippet": "features = df.pivot_table(index=[\"to_station_id\", \"to_station_name\", \"stopweekday\"],\n                          columns=\"stoptime10\", values=\"dist\").reset_index()\nfeatures.head()\n", "intent": "Let's build the features.\n"}
{"snippet": "piv = features.pivot_table(index=[\"to_station_id\",\"to_station_name\"], columns=\"stopweekday\", values=\"cluster\")\npiv.head()\n", "intent": "We first need to get 7 clusters for each stations, one per day.\n"}
{"snippet": "features = df.pivot_table(index=[\"station_id\", \"station_name\", \"weekday\"],\n                          columns=\"time10\", values=[\"startdist\", \"stopdist\"]).reset_index()\nfeatures.head()\n", "intent": "Let's build the features.\n"}
{"snippet": "piv = features.pivot_table(index=[\"station_id\",\"station_name\"], \n                           columns=\"weekday\", values=\"cluster\")\npiv.head()\n", "intent": "We first need to get 7 clusters for each stations, one per day.\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\nimport pandas\ndf = pandas.DataFrame(data.data, columns=data.feature_names)\ndf.to_csv(\"cancer.txt\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n", "intent": "Let's save some data first.\n"}
{"snippet": "import pyensae\npyensae.download_data(\"OnlineNewsPopularity.zip\", url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00332/\")\n", "intent": "[Online News Popularity Data Set](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity)\n"}
{"snippet": "df = pd.read_csv('songdata.csv')\ndf['text'] = 'trats ' + df['text'] + ' dne' \ndata = df['text'].str.cat(sep=' ').lower() \ndata = ' '.join(word.strip(string.punctuation) for word in data.split()) \n", "intent": "The next cell will import the data and do some parsing for you. In the end you will have a single string with all the data\n"}
{"snippet": "import pandas as pd\ndf = pd.DataFrame({'predictions':y_pred, 'actual label':y_test})\novo_acc = round((df[df.predictions == df['actual label']] .shape[0]/df.shape[0])*100, 2)\novo_acc\n", "intent": "<h1><a name=\"acc ovo\">One Vs One Accuracy</a></h1>\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom time import time\nfrom IPython.display import display \ndata = pd.read_csv(\"data/creditcard.csv\")\ndisplay(data.head(n=1))\n", "intent": "Load data and explore them\n"}
{"snippet": "survived = train[train['Survived']==1]['Sex'].value_counts()\ndead = train[train['Survived']==0]['Sex'].value_counts()\ndf = pd.DataFrame([survived,dead])\ndf.index = ['Survived','Dead']\ndf.head()\ndf.plot(kind='bar',stacked=True, figsize=(10,5))\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "content_image = scipy.misc.imread(\"images/louvre_small.jpg\")\nimshow(content_image)\ncontent_image = reshape_and_normalize_image(content_image)\n", "intent": "Let's load, reshape, and normalize our \"content\" image (the Louvre museum picture):\n"}
{"snippet": "style_image = scipy.misc.imread(\"images/monet.jpg\")\nimshow(style_image)\nstyle_image = reshape_and_normalize_image(style_image)\n", "intent": "Let's load, reshape and normalize our \"style\" image (Claude Monet's painting):\n"}
{"snippet": "party = house_data[\"party\"]\nvotes = house_data.loc[:, 1:]\ntrain_input, test_input, train_outcome, test_outcome = train_test_split(votes, party, test_size = 0.33, random_state = 42)\n", "intent": "Split the data into a test and training set. But this time, use this function:\n```python\nfrom sklearn.cross_validation import train_test_split\n```\n"}
{"snippet": "data.drop(['Item_Type', 'Outlet_Establishment_Year'], axis=1, inplace=True)\ntrain = data.loc[data['source']==\"train\"]\ntest = data.loc[data['source']==\"test\"]\ntest.drop(['Item_Outlet_Sales', 'source'], axis=1, inplace=True)\ntrain.drop(['source'], axis=1, inplace=True)\ntrain.to_csv(\"train_modified.csv\", index=False)\ntest.to_csv(\"test_modified.csv\", index=False)\n", "intent": "- Convert data back into train and test data sets.\n"}
{"snippet": "img = (io.imread('Images/Q2/colorful1.jpg',dtype='float64')/255.0)\ncompileResults(img,5)\n", "intent": "Results for Image 1: Colorful.jpg\n"}
{"snippet": "img2 = (io.imread('Images/Q2/colorful2.jpg',dtype='float64')/255.0)\n", "intent": "Results for Image 2: Colorful2.jpg\n"}
{"snippet": "img3 = (io.imread('Images/Q2/colorful3.jpg',dtype='float64')/255.0)\n", "intent": "Results for Image 3: Colorful3.jpg\n"}
{"snippet": "df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf = df.copy(deep=True)\ndf.head()\n", "intent": "    create copy of data to maintain original file integrity\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_, X_validation = train_test_split(features, random_state = 42, test_size = 0.1)\n", "intent": "Split 10% data for models final validation\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('train.csv')\ndf.head(10) \n", "intent": "<span style=\"font-size:2em\">View the data with Pandas</span>\n"}
{"snippet": "dataV2 = pd.read_csv('lalonde.csv', index_col=0)\nlogistic = linear_model.LogisticRegression()\nZ = dataV2['treat']\nX = dataV2.drop('treat', axis=1)\nlogistic.fit(X,Z)\n", "intent": "Let us train our data to estimate the propensity score for each datapoint, i.e the probability of being treated (or not).\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(ad_data.drop(['Clicked on Ad', 'Ad Topic Line', 'Country', 'City', 'Timestamp'],axis=1), \n                                                    ad_data['Clicked on Ad'], test_size=0.30, \n                                                    random_state=101)\n", "intent": "** Split the data into training set and testing set using train_test_split**\n"}
{"snippet": "df_scaled = pd.DataFrame(scaled_features, columns=df.columns[:-1])\ndf_scaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "import pandas as pd\ndf_train = pd.read_csv('data/titanic/train.csv')\ndf_test = pd.read_csv('data/titanic/test.csv')\n", "intent": "training data set and testing data set are given by Kaggle\nyou can download from kaggle directly [kaggle](https://www.kaggle.com/c/titanic/data)  \n"}
{"snippet": "def bar_chart(feature):\n    survived = df_train[df_train['Survived']==1][feature].value_counts()\n    dead = df_train[df_train['Survived']==0][feature].value_counts()\n    df_survived_dead = pd.DataFrame([survived,dead])\n    df_survived_dead.index = ['Survived','Dead']\n    df_survived_dead.plot(kind='bar',stacked=True, figsize=(15,5))\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "def bar_chart(feature):\n    survived = df_train[df_train['Survived']==1][feature].value_counts()\n    dead = df_train[df_train['Survived']==0][feature].value_counts()\n    df_survived_dead = pd.DataFrame([survived,dead])\n    df_survived_dead.index = ['Survived','Dead']\n    df_survived_dead.plot(kind='bar',stacked=True, figsize=(10,5))\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "def load_data():\n", "intent": "Load the data and generate a train and a test set.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)\n", "intent": "<h3>Standerdize the data</h3>\n"}
{"snippet": "X_train, X_test = train_test_split(X_, random_state = 42, test_size = 0.2)\n", "intent": "Split into train and test set\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nsplit_frac = 0.8\nval_x,train_x,val_y,train_y = train_test_split(features,labels,test_size=split_frac)\n", "intent": "With our data in nice shape, we'll split it into training, validation, and test sets.\n"}
{"snippet": "train = pd.read_csv('mnist_train_data.csv')\ntrain_lab = pd.read_csv('mnist_train_labels.csv')\ntest = pd.read_csv('mnist_test_data.csv')\ntest_lab = pd.read_csv('mnist_test_labels.csv')\n", "intent": "load data into python and check data structure\n"}
{"snippet": "wine_ori = pd.read_csv('wine_original.csv')\nlabels = wine_ori['class']\ndel wine_ori['class']\n", "intent": "Load the wine dataset\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\ndigits = datasets.load_digits()\n", "intent": "Loading a sample dataset\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = iris.data[:, :2]\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=4)\n", "intent": "Splitting data into validation, testing and training samples\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vec=CountVectorizer(tokenizer=tokenize)\nX_train=count_vec.fit_transform(X_train) \nX_test=count_vec.transform(X_test) \n", "intent": "* Construct a Document Term Matrix and Bag of Words Model for the input data.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nimport string\ndef tokenize(sentence):\n    sentence=sentence.translate(str.maketrans('','',string.punctuation))\n    tokens=nltk.word_tokenize(sentence)\n    return tokens\ncount_vec=CountVectorizer(tokenizer=tokenize)\nX_train=count_vec.fit_transform(X_train) \nX_test=count_vec.transform(X_test) \n", "intent": "> * Construct a Document Term Matrix\n> * Construct Bag of Words\n"}
{"snippet": "Trump2018 = open('C:/Users/ushai/Dropbox/Data Science/Datasets/trump_state_union_2018.txt')\nTrump2019 = open('C:/Users/ushai/Dropbox/Data Science/Datasets/trump_state_union_2019.txt')\nTrump2018_raw = Trump2018.read()\nTrump2019_raw = Trump2019.read()\nTrump_raw = Trump2018_raw + Trump2019_raw\n", "intent": "Donald trump (R) State of the Union 2018, 2019\n"}
{"snippet": "ypred_Dem = pd.DataFrame()\n", "intent": "- Here we divide the data in to four different samples and check consistency of clusters across samples for both Democratic and Republican documents.\n"}
{"snippet": "X_copy = X.copy()  \nX_scaled = scaler.transform(X_copy)\npd.DataFrame({\n    \"Mean\": X_scaled.mean(axis=0), \n    \"Std\": X_scaled.std(axis=0)\n})\n", "intent": "**Exercise**: It looks like some of the features are on a larger numerical scale.  Let's put all the data on the same scale.\n"}
{"snippet": "messages = pandas.read_csv(DATA_FILENAME, sep='\\t', quoting=csv.QUOTE_NONE,\n                           names=[\"label\", \"message\"])\nprint(messages)\n", "intent": "Instead of parsing TSV (or CSV, or Excel...) files by hand, we can use Python's `pandas` library to do the work for us:\n"}
{"snippet": "bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])\nprint(len(bow_transformer.vocabulary_))\n", "intent": "Each vector has as many dimensions as there are unique words in the SMS corpus:\n"}
{"snippet": "sub = pd.DataFrame()\nsub['ImageId'] = new_test_ids\nsub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\nsub.to_csv('sub-dsbowl2018-3rd.csv', index=False)\n", "intent": "... and then finally create our submission!\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nitrain, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itrain] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "audio_file_path = '../../Datasets'\nfilename = '/5e1b34a6_nohash_0.wav'\nsample_rate, samples = wavfile.read(str(audio_file_path) + filename)\n", "intent": "Let us read a sample audio file from this dataset: \n"}
{"snippet": "from keras.datasets import cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n", "intent": "CIFAR-10 is a dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.\n"}
{"snippet": "from keras.datasets import fashion_mnist\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n", "intent": "**TASK 1: Run the code below to download the dataset using Keras.**\n"}
{"snippet": "from bs4 import BeautifulSoup\nfname = 'time.html'\nfname = os.path.join(DATA_DIR, fname)\nwith open(fname) as f:\n    html = f.read()\n    soup = BeautifulSoup(html)\n", "intent": "The best way to read in `.html` files in Python is with the `BeautifulSoup` package.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncountvec = CountVectorizer()\nsparse_dtm = countvec.fit_transform(no_digits)\n", "intent": "Our next step is to turn the text into a document term matrix using the scikit-learn function called `CountVectorizer`.\n"}
{"snippet": "dataset = pd.read_csv('mushrooms.csv')\n", "intent": "To load the data from the file, we use the [Pandas](https://pandas.pydata.org/) library.\n"}
{"snippet": "from sklearn.datasets import make_regression\nX, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4, bias=100.0)\n", "intent": "What is linear data?\n"}
{"snippet": "try:\n    churn_data = pd.read_csv('telecom_churn_data.csv')\nexcept:\n    churn_data = pd.read_csv('data/telecom_churn_data.csv')\ntotal_records = churn_data.shape[0]\nchurn_data.shape\n", "intent": "Load the churn data for the telecom provider\n"}
{"snippet": "text = None\nwith open('anna.txt', 'r') as f:\n    text = f.read()\nch_set = set(text)\nch_size = len(ch_set)\nprint('our input has %d unique characters' % ch_size)\nch_to_int = {c:i for i,c in enumerate(ch_set)}\nint_to_ch = dict(enumerate(ch_set))\ndatas = np.array([ch_to_int[c] for c in text], dtype = np.int32)\n", "intent": "We will train our char-RNN on Anna Karenina (anna.txt), we need to load our data into memory to get the character-sizes ($D$ and $C$)\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values='NaN', strategy='mean', axis=1)\nX = imputer.fit_transform(df.iloc[:, :8].values)\ny = df.iloc[:, 8].values\nfit_and_score_rlr(X, y)\n", "intent": "Next, replace missing features through mean imputation. Run  a regression and measure the performance of the model.\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(categorical_features=range(5))\nX_encoded = enc.fit_transform(X)\nfit_and_score_rlr(X_encoded, y, normalize=False)\n", "intent": "Now, encode the categorical variables with a one-hot encoder. Again, run a classification and measure performance.\n"}
{"snippet": "class SBS(object):\n    def __init__(self):\n        pass\n    def fit(self):\n        pass\n    def transform(self):\n        pass\n    def fit_transform(self):\n        pass\n", "intent": "- Implement SBS below. Then, run the tests.\n"}
{"snippet": "df_raw = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "X_mat = scaler.transform(df_raw.drop('TARGET CLASS',axis=1))\nX_scale = pd.DataFrame(X_mat, columns=df_raw.columns[:-1])\nX_scale.head()\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "df_scale = pd.DataFrame(df_scale)\ndf_scale.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "y = encoder.fit_transform(dataset.iloc[:,0])\n", "intent": "Let's start by encoding the binary classification output.\n"}
{"snippet": "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n                                                  test_size=10000,\n                                                  random_state=42)\n", "intent": "To measure our Neural Networks performance we will need some validation data. The `train_test_split` helper from scikit-learn does this for us.\n"}
{"snippet": "url = './titanic.csv'\ntitanic = pd.read_csv(url)\ntitanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\ntitanic.Age.fillna(titanic.Age.median(), inplace=True)\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\nembarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\ntitanic = pd.concat([titanic, embarked_dummies], axis=1)\ntitanic.head()\n", "intent": "We'll build a classification tree using the Titanic data:\n"}
{"snippet": "train_df = pd.read_csv('data/train.csv', index_col=0, infer_datetime_format=True)\ntrain_df.index.name=None \ntrain_df.index = pd.to_datetime(train_df.index) \ntest_df = pd.read_csv('data/test.csv', index_col=0, infer_datetime_format=True)\ntest_df.index.name=None \ntest_df.index = pd.to_datetime(test_df.index) \n", "intent": "- What does each 'unit' (e.g. row) of data represent?\n"}
{"snippet": "pd.DataFrame(contributions, columns=predictors)\n", "intent": "prediction = bias + sum(contributions)\n"}
{"snippet": "X_train_new = pd.DataFrame(X_train_new, columns=X_train.columns)\nmeans = []\nstdevs = []\nfor k in X_train_new.columns:\n    means.append(X_train_new[k].mean())\n    stdevs.append(X_train_new[k].std())\n", "intent": "Extract Mean and Standard Dev from original train + validation dataset. \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n", "intent": "Splitting data into validation, testing and training samples\n"}
{"snippet": "import numpy as np\niris = datasets.load_iris()\nX = iris.data[:, :2]  \ny = iris.target\nh = .02  \n", "intent": "Plotting Decision Boundaries\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\ntheta,residuals,rank,s = numpy.linalg.lstsq(X_train, y_train)\n", "intent": "Least Square Regression\n"}
{"snippet": "for i in pd.DataFrame(zip(X.columns, lm.coef_), columns=['features','estimatedCoefficients']):\n    print i\n", "intent": "Outputting list of features that are deemed to be statistically significant by this process\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8)  \n", "intent": "- To avoid this problem, we split up the dataset in a **training set** for fitting the model, and a **test set** for testing the model.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled)\n", "intent": "Now actually running the regression on the reduced dataset\n"}
{"snippet": "from yellowbrick.features.manifold import Manifold\nvisualizer = Manifold(manifold='lle', target='continuous')\nvisualizer.fit_transform(X_scaled[random_features],y_scaled)\nvisualizer.poof()\n", "intent": "Now passing Locally Linear Embedding - does this help us for linaer regression?\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_normed,y_normed, test_size=0.3, random_state=42)\n", "intent": "Step1: Simple Model with Linear Regression\n"}
{"snippet": "poly = PolynomialFeatures(interaction_only=True)\nX_normed_wint = poly.fit_transform(X_normed)\n", "intent": "The above did not return any interactions! That means the interactions are probably not going to make large changes to our models above\n"}
{"snippet": "selector = RFECV(lin_reg, step=1, cv=5, scoring='neg_mean_squared_error')\nselector.fit(X_ratios, y_normed)\n", "intent": "Feature Selection - We use this promising model above to pick only the best features\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_new, y_normed, test_size=0.3, random_state=42)\n", "intent": "Now let's rerun our best model so far and evaluate changes to model metrics resulting from removing unneeded features\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_normed, y_normed, test_size=0.3, random_state=42)\n", "intent": "Finally, let's see how the models performs against our Test Set\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_ratios, y_normed, test_size=0.3, random_state=42)\n", "intent": "Best model After Adding Ratios and Doing Feature Selection\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_ratios, y_normed, test_size=0.3, random_state=42)\n", "intent": "Let's introduce some degree of regularization to see if we can decrease validation RMSE even further\n"}
{"snippet": "poll_data = pd.read_csv(path_to_repo + 'data/538/2012_poll_data_states.csv', sep='\\t')\ncensus_data = pd.read_csv(path_to_repo + 'data/538/census_demographics.csv')\nstates = pd.read_csv(path_to_repo + 'data/538/states.csv')\n", "intent": "Let's look at the 538 polling data and see if we can use the census data to predict who was ahead in the polls, using kNN.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_normed, y_normed)\n", "intent": "Now actually running the regression on the reduced dataset\n"}
{"snippet": "X_train, X_val, y_train, y_val = train_test_split(X_new,y_normed, test_size=0.3, random_state=42)\n", "intent": "Now let's try Linear Regression with CV again and let's see if we have any improvements with the transformed version of X\n"}
{"snippet": "estimator = linear_model.LinearRegression(fit_intercept=True, normalize=False)\nselector = RFECV(estimator, step=1, cv=5, scoring='neg_mean_squared_error')\nselector.fit(X_train_int, y_train)\nprint(\"Optimal number of features : %d\" % selector.n_features_)\n", "intent": "Let's pick only interaction features that minimize CV RMSE by feeding the new data into RCEFV\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_normed,y_normed, test_size=0.3, random_state=42)\n", "intent": "Let's introduce some degree of regularization to see if we can decrease validation RMSE even further\n"}
{"snippet": "from sklearn.feature_selection import RFECV\nestimator2 = linear_model.LinearRegression()\nselector2 = RFECV(estimator2, step=3, cv=5)\nselector2 = selector2.fit(data_scaled, y)\n", "intent": "Trying to select important features by doing a RFECV instead of looking at univariate linear regressions first\n"}
{"snippet": "cal = pd.read_csv('Datasources/inside_airbnb/calendar.csv')\n", "intent": "Preparing the dataset first: taking the avg of each listing price time series\n"}
{"snippet": "linear_regression_wke = linear_model.LinearRegression(normalize=False, fit_intercept=True)\nstandardization = StandardScaler()\nStand_coef_linear_reg_wke = make_pipeline(standardization, linear_regression_wke)\nlinear_regression_wke.fit(X_wke,target_wke)\nfor coef, var in sorted(zip(map(abs, linear_regression_wke.coef_), X_wke.columns[:-1]),reverse=True):\n    print (\"%6.3f %s\" % (coef,var))\n", "intent": "Feature Importance for Weekends\n"}
{"snippet": "linear_regression_wkd = linear_model.LinearRegression(normalize=False, fit_intercept=True)\nstandardization = StandardScaler()\nStand_coef_linear_reg_wkd = make_pipeline(standardization, linear_regression_hol)\nlinear_regression_wkd.fit(X_wkd,target_wkd)\nfor coef, var in sorted(zip(map(abs, linear_regression_wkd.coef_), X_wkd.columns[:-1]),reverse=True):\n    print (\"%6.3f %s\" % (coef,var))\n", "intent": "Feature Importance for Weekdays\n"}
{"snippet": "label_bin = LabelBinarizer()\n", "intent": "Binarizing Variables\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8)\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n", "intent": "- Now use a training and test set for fit and test your model.\n- How is its performance different?\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X[features_one], np.log(np.log(target)), test_size=.30, random_state=1)\nX_train_two, X_val, y_train_two, y_val = train_test_split(X_train, y_train, test_size=.30, random_state=1)\n", "intent": "In this iteration, we try predicting the log of the log of the target\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_ratios[best_features], y_normed, test_size=0.3, random_state=42)\n", "intent": "Best model After Adding Ratios and Doing Feature Selection\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_ratios, y_normed, test_size=0.3, random_state=42)\n", "intent": "Starting with Lasso\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n", "intent": "Now actually running the regression on the reduced dataset\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(data, y)\n", "intent": "Now actually running the regression on the reduced dataset\n"}
{"snippet": "from yellowbrick.features.manifold import Manifold\nvisualizer = Manifold(manifold='lle', target='continuous')\nvisualizer.fit_transform(data[random_features],y)\nvisualizer.poof()\n", "intent": "Now passing Locally Linear Embedding - does this help us for linaer regression?\n"}
{"snippet": "bitcoin = pd.read_csv('data/bitcoin_historical_prices.csv')\n", "intent": "Let's load the dataset as a pandas `DataFrame`. This will make it easy to compute basic properties from the dataset and to clean any irregularities. \n"}
{"snippet": "train = pd.read_csv('data/train_dataset.csv')\n", "intent": "Neural networks typically work with vectors and tensors, both mathematical objects that organize data in a number of dimensions. \n"}
{"snippet": "train = pd.read_csv('data/train_dataset.csv')\n", "intent": "We will load our same train and testing set from previous activitites. \n"}
{"snippet": "import numpy as np\nscores = []\nfor i in xrange(15):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8)\n    scores.append(model.fit(X_train, y_train).score(X_test, y_test))\nfor score in scores:\n    print np.round(score, 2),\nprint \"\\navg:\", np.mean(scores)\n", "intent": "Note that `train_test_split` makes random splits, so your accuracy can be different than the one above, as you can see below:\n"}
{"snippet": "scaler = MinMaxScaler()\n", "intent": "Scale features before creating Ridge model. Unscaled features would otherwise get penalized differently by the regularization term\n"}
{"snippet": "sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n", "intent": "Scaling Training and Test Sets:\n"}
{"snippet": "labels_path = os.path.join(os.getcwd(), 'datasets', 'landsat', 'landsat_classes.csv')\nlandsat_labels = pd.read_csv(labels_path, delimiter=',', index_col=0)\nlandsat_labels\nlandsat_labels_dict = landsat_labels.to_dict()[\"Class\"]\ntrain_path = os.path.join(os.getcwd(), 'datasets', 'landsat', 'landsat_train.csv')\ntest_path = os.path.join(os.getcwd(), 'datasets', 'landsat', 'landsat_test.csv')\nlandsat_train = pd.read_csv(train_path, delimiter=',')\nlandsat_test = pd.read_csv(test_path, delimiter=',')\n", "intent": "Load the `landsat_train.csv` dataset into a `pandas` DataFrame called  `landsat_train` and display the shape of the DataFrame.\n"}
{"snippet": "wages = pd.DataFrame(dict(\n        monthly_income_amount = data[u'monthly_income_amount'],\n        monthly_rent_amount = data[u'monthly_rent_amount'],\n        Credit_Line_approved = data[u'Credit_Line_approved'],\n        age=data[u'age']) )\nwages = pd.concat([wages, data['flgGood']], axis = 1)\ng = sns.pairplot(wages.sort('flgGood'), hue=\"flgGood\", palette=myPalette)       \n", "intent": "Is there any relations between wages and age? And total spend with default?\n"}
{"snippet": "cols = [u'raw_serasa_score', u'raw_unit4_score',\n        u'raw_lexisnexis_score',u'raw_TU_score', u'raw_FICO_money_score']\nraw_scores_bin = pd.DataFrame(dict(\n        raw_serasa_score = mu.binarize(data[u'raw_serasa_score']),\n        raw_unit4_score = mu.binarize(data[u'raw_unit4_score']),\n        raw_lexisnexis_score = mu.binarize(data[u'raw_lexisnexis_score']),\n        raw_TU_score = mu.binarize(data[u'raw_TU_score']),\n        raw_FICO_money_score = mu.binarize(data[u'raw_FICO_money_score'])))\nraw_scores = pd.concat([raw_scores_bin, data['flgGood']], axis = 1)\n", "intent": "Are the scores given by other entities good? Do they guarantee good payment?\n"}
{"snippet": "def rmsle(Y_test, Y_pred):\n    Y_pred[Y_pred <0] = 0\n    return np.sqrt( np.sum( (np.log(Y_pred+1)-np.log(Y_test+1))**2 )/len(Y_test))\nfrom datetime import datetime\ndef prepare_submission(X_test, Y_pred, file_name):\n    d = X_test.datetime.apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n    Y_pred[Y_pred<0]=0\n    ret = pd.DataFrame( { 'datetime':d , 'count':Y_pred})\n    ret.to_csv(file_name,sep=',', index=False)\n    return ret\n", "intent": "As stated by Kaggle, this competition will be analyzed using the Root Mean Squared Logarithmic Error (RMSLE), so lets get it from scikit\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfrom sklearn import linear_model\nfrom sklearn import ensemble\na_train, a_test, b_train, b_test = train_test_split(X_train, Y_train)\n", "intent": "Now that the vector are ready, its time to apply the machine learning algorithms\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\na_train, a_test, b_train, b_test = train_test_split(X_train, Y_train)\n", "intent": "Now that the vector are ready, its time to apply the machine learning algorithms\n"}
{"snippet": "features = pd.concat([data.get(['Fare', 'Age']),\n                      pd.get_dummies(data.Sex, prefix='Sex'),\n                      pd.get_dummies(data.Pclass, prefix='Pclass'),\n                      pd.get_dummies(data.Embarked, prefix='Embarked')],\n                     axis=1)\nfeatures = features.drop('Sex_male', 1)\nfeatures = features.fillna(features.dropna().median())\nfeatures.head(5)\n", "intent": "Let us now rebuild a new version of the data by treating the class as categorical variable:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfor k in xrange(10):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8)\n    print model.fit(X_train, y_train).score(X_test, y_test).round(3), \n", "intent": "Supervised problems, like linear regression, always require a training and a testset, to avoid having overfitted your model.\n"}
{"snippet": "from keras.datasets import mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n", "intent": "The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays.\nDocumentation : https://keras.io/datasets/\n"}
{"snippet": "from keras.datasets import mnist\nfrom keras.utils import to_categorical\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\ntrain_images = train_images.reshape((60000, 28, 28, 1))\ntrain_images = train_images.astype('float32') / 255\ntest_images = test_images.reshape((10000, 28, 28, 1))\ntest_images = test_images.astype('float32') / 255\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\n", "intent": "We will reuse a lot of the code we have already covered in the MNIST example from episode 1.\n"}
{"snippet": "img_path = '../../data/cats_and_dogs_small/test/cats/cat.1700.jpg'\nfrom keras.preprocessing import image\nimport numpy as np\nimg = image.load_img(img_path, target_size=(150, 150))\nimg_tensor = image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\nimg_tensor /= 255.\nprint(img_tensor.shape)\n", "intent": "This will be the input image we will use -- a picture of a cat, not part of images that the network was trained on:\n"}
{"snippet": "daily_user_views_diff = pd.DataFrame(df['views']\\\n                                     .resample('D')\\\n                                     .sum()\\\n                                     .diff(periods=1)\\\n                                     .astype(float))['2015-07-02':] \n", "intent": "I'll run the augmented dickey-fuller test on the differenced data to show that the differencing should make the data set more stationary:\n"}
{"snippet": "of_df = pd.read_csv(\"../../assets/dataset/old-faithful.csv\")\nof_df.head()\n", "intent": "Not so great on this dataset. Now let's try some real data.\n"}
{"snippet": "of_df = pd.read_csv(\"old-faithful.csv\")\nof_df.head()\n", "intent": "Not so great on this dataset. Now let's try some real data.\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    contains_org = any([word.ent_type_ == 'ORG' for word in parsed])\n    contains_person = any([word.ent_type_ == 'PERSON' for word in parsed])\n    return contains_org and contains_person\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title']].head()\n", "intent": "Lets write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n"}
{"snippet": "def references_organization(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == 'ORG' for word in parsed])\ndata['references_organization'] = data['title'].fillna(u'').map(references_organization)\ndata[data['references_organization']][['title','references_organization']].head() \n", "intent": "Let's see if we can find organizations in our page titles.\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    contains_org = any([word.ent_type_ == 'ORG' for word in parsed])\n    contains_person = any([word.ent_type_ == 'PERSON' for word in parsed])\n    return contains_org and contains_person\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title','references_org_person']].head()\n", "intent": "Lets write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nX, y = boston.data, boston.target\nfeatures = boston.feature_names\n", "intent": "In this exercise, we'll use one of sklearn's standard datasets to analyze Boston house prices.\n"}
{"snippet": "df = pd.read_csv('iris.csv',\n                 names=['SepalLength','SepalWidth','PetalLength','PetalWidth','Name'],\n                 header=None)\n", "intent": "Let's create a Pandas DataFrame with the data.\n"}
{"snippet": "df_additional_full = pd.read_csv('bank-additional-full.csv', sep = ';')\n", "intent": "Extend the analysis and cross-validation to bank-additional-full.csv. How does the performance change?\n"}
{"snippet": "dists = pd.DataFrame(dists, columns = df_wide.index, index=df_wide.index)\ndists.ix[0:10, 0:10]\n", "intent": "Convert dists to a Pandas DataFrame, use the index as column index as well (distances are a square matrix).\n"}
{"snippet": "movies = pd.read_table('movielens/movies.dat', sep='::', \n                       names= ['ITEMID', 'Title', 'Genres'], index_col= 'ITEMID')\n", "intent": "Import 'movies.dat' to a 'movies' pandas dataframe. Make sure you name the columns, use the correct separator and define the index.\n"}
{"snippet": "df = pd.read_csv(\"heart_disease.csv\",header=None)\ndf.columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']\ndf.head()\n", "intent": "Note: You'll have to manually add column labels\n"}
{"snippet": "labels=[\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \n        \"ca\", \"thal\", \"num\"]\ntarget=[\"num\"]\ndf = pd.read_csv(\"./heart_disease.csv\", names=labels)\ndf.head()\n", "intent": "Note: You'll have to manually add column labels\n"}
{"snippet": "data = pd.read_csv('houses.csv')\n", "intent": "Load a subset of the housing data\n"}
{"snippet": "Xpd = pd.DataFrame(digits.data) \nXpd.head()\n", "intent": "Here's a small routine to visually plot the first 400 rows (i.e. digits) in the dataset:\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\nX = iris['data']\nNames = iris['feature_names']\ntarget_names = iris['target_names']\ny = iris['target']\nprint target_names\nprint Names\nprint X[:1]\n", "intent": "Import the data from the iris dataset in sklearn, generate X and y arrays\n"}
{"snippet": "test = pd.read_csv('/Users/ruben/Downloads/test.csv')\n", "intent": "We're gonna concatenate the training and test set, so we're sure our feature matrices are aligned.\n"}
{"snippet": "ratdata = pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/ML_2017/Manhattan 311 Rodent Complaint Locations 2016.csv\").values\nratdata\n", "intent": "Find out which neighborhoods(see map below) that have more serious rat problem.\n<img src=\"Manhattan neighborhood map.png\" width=600 height=400>\n"}
{"snippet": "s = pd.Series([1,3,5,np.nan,6,8])\ndates = pd.date_range('20130101', periods=6)\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))\n", "intent": "10 Minutes to pandas: http://pandas.pydata.org/pandas-docs/stable/10min.html\n"}
{"snippet": "path = 'https://serv.cusp.nyu.edu/~cq299/ADS2016/Data/Bayesian/'\ndata=pd.read_csv(path+\"example3.csv\")\nX=np.matrix(data.iloc[:,:-1])\ny=np.asarray(data.Y)\n", "intent": "http://scikit-learn.org/stable/modules/grid_search.html\n"}
{"snippet": "data=pd.read_csv(\"session_3_stop.csv\")\n", "intent": "Please download the data set here or on NYU-Classes.\nhttps://serv.cusp.nyu.edu/classes/ML_2016_Spring/ML_2017/\nfile: \"session_3_stop.csv\"\n"}
{"snippet": "import pandas as pd\ndata = pd.DataFrame(data={'fruit': [\"banana\", \"apple\", \"banana\", \"apple\", \"banana\",\"apple\", \"banana\", \n                                    \"apple\", \"apple\", \"apple\", \"banana\", \"banana\", \"apple\", \"banana\",], \n                          'tasty': [\"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \n                                    \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\"], \n                          'size': [\"large\", \"large\", \"large\", \"small\", \"large\", \"large\", \"large\",\n                                    \"small\", \"large\", \"large\", \"large\", \"large\", \"small\", \"small\"]})\nprint(data)\n", "intent": "(1) Parameters Learning\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values='NaN', strategy='median', axis=0)\n", "intent": "Impute missing values with median\n"}
{"snippet": "data = datasets.load_iris()\nX = data.data[:100, :2]\ny = data.target[:100]\nX_full = data.data[:100, :]\n", "intent": "- Load the Iris dataset\n"}
{"snippet": "from keras.datasets import cifar10\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\ny_train = y_train.reshape(-1)\ny_test = y_test.reshape(-1)\n", "intent": "Dataset of 50,000 32x32 color training images, \nlabeled over 10 categories, \nand 10,000 test images.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\n_, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[_] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\"/Users/ruben/Downloads/train.csv\")\n", "intent": "Let's do some data exploration first\n"}
{"snippet": "coeffdf = pd.DataFrame(lm.coef_,index = X_train.columns,columns=['coeff'])\ncoeffdf\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "yelp_class = pd.DataFrame(data = yelp[(yelp['stars'] == 1) |(yelp['stars'] ==5)] )\nyelp_class.head()\n", "intent": "**Dataframe - yelp_class that contains the columns of yelp dataframe but for only the 1 or 5 star reviews.**\n"}
{"snippet": "import pandas as pd\nurl = 'https://raw.githubusercontent.com/JamesByers/GA-SEA-DAT2/master/data/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer\n", "intent": "1. K-means clustering\n2. Clustering evaluation\n3. DBSCAN clustering\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    train_size=800, \n                                                    random_state=42)\n", "intent": "We will build a decision tree on the TRAIN data. (800 rows)\nWe will TEST our PREDICTIONS on the TEST data set (remaining rows)\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df2, test_size=0.2, \n                                     random_state = 12)\n", "intent": "Use the ready-made function from sklearn.\nWe can cut into two ourselves.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nfor label in ['embarked','sex']:\n    titanic[label] = LabelEncoder().fit_transform(titanic[label])\nprint(titanic.head())\n", "intent": "Three binomials, two categoricals, and four numerical features.\n"}
{"snippet": "with hana_query(all_transcripts_sql) as cursor:\n    rows = cursor.fetchall()\nt_df = pd.DataFrame(rows, columns=COLUMNS['TRANSCRIPT']+[\"HeightF\", \"WeightF\", \"BMIF\", \"TemperatureF\"])\nt_df.describe()\n", "intent": "We want to evaluate the quality of the data - in particular with regards to null values - and determine boundaries to cut off outliers.\n"}
{"snippet": "missing_df = pd.read_csv('./dataset/dataset_1_missing.txt')\nfull_df = pd.read_csv('./dataset/dataset_1_full.txt')\nno_y_ind = missing_df[missing_df['y'].isnull()].index\nwith_y_ind = missing_df[missing_df['y'].notnull()].index\nk=3\npredicted_knn, r_knn = fill_knn(missing_df, full_df, no_y_ind, with_y_ind, k)\nprint 'R^2 value of KNN fit, for k=', k, ': ', r_knn\npredicted_lin, r_lin = fill_lin_reg(missing_df, full_df, no_y_ind, with_y_ind)\nprint 'R^2 value of linear regression fit:', r_lin\n", "intent": "**Solution:**\nEvaluate predicted values using linear regression vs KNN.\n"}
{"snippet": "data = pd.read_csv('green_tripdata_2015-01.csv', usecols=range(0, 21), index_col=False)\ndata.head(n=3)\n", "intent": "Let's now read the csv file correctly and extract the time stamp values.\n"}
{"snippet": "test = pd.read_csv(\"/Users/ruben/Downloads/test.csv\")\n", "intent": "Let's make our predictions on the test set.\n"}
{"snippet": "hd_data = pd.read_csv('datasets/dataset_3_train.txt')\ndisease = hd_data.iloc[:, -1:]\nindicators = hd_data.iloc[:, :-1]\ntest1_data = pd.read_csv('datasets/dataset_3_test_1.txt')\ndisease_t1 = test1_data.iloc[:, -1:]\nindicators_t1 = test1_data.iloc[:, :-1]\ntest2_data = pd.read_csv('datasets/dataset_3_test_2.txt')\ndisease_t2 = test2_data.iloc[:, -1:]\nindicators_t2 = test2_data.iloc[:, :-1]\n", "intent": "- Fit a logistic regression model to the training set, and report its accuracy on both the test sets. \n"}
{"snippet": "page = urllib.urlopen('https://cs109alabs.github.io/lab_files/').read()\nsoup = BeautifulSoup(page, \"lxml\")\nprint soup.prettify()[:1000]\n", "intent": "In this part we use the solution to the Challenge problem in Homework \n"}
{"snippet": "ensemble = pd.DataFrame({}) \ncost_weighting = []\npreds_alter = preds - 1    \ntypes = preds.value_counts()\nprint types\nprint cost(y, preds_alter)\ncost_weighting.append(cost(y, preds_alter))\nensemble['logreg'] = preds_alter\n", "intent": "We have our predictions... how does our model do.\n"}
{"snippet": "ensemble = pd.DataFrame({}) \ncost_weighting = []\npreds_alter = preds - 1\nprint cost(y, preds_alter)\ncost_weighting.append(cost(y, preds_alter))\nensemble['logreg'] = preds_alter\n", "intent": "We have our predictions... how does our model do.\n"}
{"snippet": "loans = pd.read_csv('loan_data.csv')\n", "intent": "**Nutze Pandas, um die Datei \"loan_data.csv\" als DataFrame namens \"loans\" zu laden.**\n"}
{"snippet": "df = pd.read_csv('College_Data',index_col=0)\n", "intent": "** Lies die CSV-Datei \"College_Data\" mit `read_csv` ein. Finde heraus, wie du die erste Spalte als Index definierst.**\n"}
{"snippet": "df = pd.read_csv('KNN_Project_Data')\n", "intent": "**Lese die CSV-Datei \"KNN_Project_Data\" in einen DataFrame ein.**\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "**Erstelle ein StandardScaler() Objekt namens \"scaler\".**\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head()\n", "intent": "**Konvertiere die skalierten Eigenschaften in einen DataFrame und sieh dir dessen Head an.**\n"}
{"snippet": "from patsy import dmatrix\nall_data = pd.concat([data, test])\nfeatures = \"SalaryNormalized ~ ContractType + Category + County + City + Town + Hood + \" + \" + \".join(top_keywords)\nY, X = dmatrices(features, data=all_data.fillna(0), return_type='dataframe')\ndescription_features = cv.fit_transform(all_data.FullDescription).todense()\nX = np.hstack((X, description_features))\n", "intent": "Design model on entire dataset, so that columns and coefficients will be aligned.\n"}
{"snippet": "ad_data = pd.read_csv('advertising.csv')\n", "intent": "**Lese die advertising.csv Datei ein und erstelle einen DataFrame namens ad_data.**\n"}
{"snippet": "train = pd.read_csv('titanic_train.csv')\n", "intent": "Beginnen wir damit die Datei \"titanic_train.csv\" in einen Pandas DataFrame zu laden.\n"}
{"snippet": "df_store = pd.read_csv('sc_data/Store_Transactions.csv')\ndf_store.shape\ndf_store.head()\n", "intent": "**1:** Utilize the Apriori Algorithm to uncover frequent itemsets\n"}
{"snippet": "df_movie_pivot = pd.pivot_table(df_ratings, index='movieId', columns='userId',values='rating').fillna(0)\ndf_movie_pivot.head()\n", "intent": "**3:** Pick 2 movies and find movies that are similar to the movies you have picked\n"}
{"snippet": "projects  = pd.read_csv('./data/projects.csv')\nstr_to_bool(projects)\nprint(projects.shape[0])\nprint(projects.dtypes)\noutcomes  = pd.read_csv('./data/outcomes.csv')\nstr_to_bool(outcomes)\nprint(outcomes.shape[0])\nprint(outcomes.dtypes)\n", "intent": "First, check the size of the datasets:\n"}
{"snippet": "from sklearn.datasets import make_regression\nX, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4)\n", "intent": "What is linear data?\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ndata = X.copy()\nlabel_encoder.fit(data[\"gender\"])\nlabel_encoder.classes_\n", "intent": "Label Encoding simply encodes each category as an integer value. Sklearn provides a preprocessing library to assist with this. \n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\nv = DictVectorizer(sparse = False)\nD = [{'A':1, 'B':2}, {'B':3, 'C':1}]\nX = v.fit_transform(D)\nX\n", "intent": "- https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=50, centers=2, cluster_std=0.5, random_state=4)\ny = 2 * y - 1\n", "intent": "- https://datascienceschool.net/view-notebook/6c6d450cb2ee49558856fd924b326e00/\n"}
{"snippet": "import pandas as pd\nwith open(\"/Users/ruben/repo/personal/ga/DAT-23-NYC/data/amazon/small-movies.txt\") as f:\n    data = pd.DataFrame([{line[:line.find(':')]: line[line.find(':')+2:] \n                          for line in review.split('\\nreview/')}\n                         for review in ('\\n' + f.read()).split('\\nproduct/') if len(review) > 1])\n    data['score'] = data.score.astype(float).astype(int)\n    data['helpfulness'] = data.helpfulness.str.split('/').map(\n        lambda frac: float(frac[0])/float(frac[1]) if frac[1] != '0' else None)\n", "intent": "This was an exercise in lesson \n"}
{"snippet": "data = pd.read_csv(\"data/mailing.csv\")\nX = data.drop(['class'], 1)\nY = data['class']\n", "intent": "Let's read our data in and put the target variable in `Y` and all the other features in `X`.\n"}
{"snippet": "data = pd.read_csv(\"data/imdb.csv\", quotechar=\"\\\"\", escapechar=\"\\\\\")\n", "intent": "1\\. Read in the data located in `data/imdb.csv`. Don't forget to set the `quotechar` to `\"` and `escapechar` to `\\`. **`[`*`5 points`*`]`**\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ntext_from_corpus = fetch_20newsgroups(subset='train')\n", "intent": "**Step \n*Reference*: http://scikit-learn.org/stable/datasets/index.html\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nimport sys, os\nimport datetime as dt\nfrom os import linesep as endl\npd.set_option('display.width', 170)\ntab = pd.read_csv('retailer.csv', sep = '|')\nprint tab.sample(10)\n", "intent": "The dataset contains numbers of items sold per week and store for each SKU.  \nSKUs belong to BRANDS, SEGMENTS, TYPES and COLLECTIONS\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, Y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['over_200k', 'under_200k'],\n                         columns=['predicted_over_200k','predicted_under_200k'])\nconfusion\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "df['SQFT_SC_test'] = MinMaxScaler().fit_transform(df['SQFT'])\ndf['BDRMS_SC_test'] = MinMaxScaler().fit_transform(df['BDRMS'])\ndf['AGE_SC_test'] = MinMaxScaler().fit_transform(df['AGE'])\ndf['PRICE_SC_test'] = MinMaxScaler().fit_transform(df['PRICE'])\ndf.head(3)\n", "intent": "Refer to this if you are confused as to the formula: [Standardization](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n"}
{"snippet": "df_SC['SQFT_SC_test'] = StandardScaler().fit_transform(df['SQFT'])\ndf_SC['BDRMS_SC_test'] = StandardScaler().fit_transform(df['BDRMS'])\ndf_SC['AGE_SC_test'] = StandardScaler().fit_transform(df['AGE'])\ndf_SC['PRICE_SC_test'] = StandardScaler().fit_transform(df['PRICE'])\n", "intent": "[StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n"}
{"snippet": "iris = load_iris()\nprint iris.keys()\nprint ''\niris_sublist = ['target_names', 'feature_names']\nfor i in iris_sublist:\n    for x in iris[i]:\n        print i + \": \" + x\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "pd.DataFrame(metrics.confusion_matrix(y, labels))\n", "intent": "Compute the Confusion Matrix to test the performance of the clustering analysis\n"}
{"snippet": "path_to_repo = '/Users/ruben/repo/personal/ga/DAT-23-NYC/'\ncolumn_headers = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\ndata = pd.read_csv(path_to_repo + 'data/iris/iris.csv', header=None, names=column_headers)\n", "intent": "Our model will predict if a flower is a versicolor or not, based only on the petal's width and length.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nx_train,x_test, y_train,  y_test =train_test_split(wine_df,y, test_size=0.25)\n", "intent": "Let's create a test that simulates fresh data that model might be predicting on when it is put into production.\n"}
{"snippet": "path = \"../../data/titanic.csv\"\ntitanic = pd.read_csv(path)\ntitanic.head()\n", "intent": "Before we go over how to make validation and learning curve plots in sklearn, let's import the titanic dataset and clean it.\n"}
{"snippet": "scale = StandardScaler()\nXs = scale.fit_transform(X)\nXs\n", "intent": "We know that we need to scale our data for the KNN algorithm right?\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\ndf = pd.DataFrame(boston[\"data\"])\ndf.columns = boston[\"feature_names\"]\ndf[\"MEDV\"] = boston[\"target\"]\ndf.head()\n", "intent": "Ridge is better for dealing with multicollinearity and Lasso is better for high number of features.\n"}
{"snippet": "pca = PCA()\nX_transformed = pd.DataFrame(pca.fit_transform(X_train))\nX_transformed.head()\n", "intent": "Now lets transform the data into principal component space.\n"}
{"snippet": "pca_dim_reducer = PCA(n_components = 20)\nX_transformed_reduced = pd.DataFrame(pca_dim_reducer.fit_transform(X_train))\nreduced_dim_covar = X_transformed_reduced.cov()\nnp.trace(reduced_dim_covar)\n", "intent": "As a general rule, want to retain 90% of the total original variance\n"}
{"snippet": "pca = PCA()\nX_transformed = pca.fit_transform(X_train)\nX_transformed_reduced = X_transformed[:,:20]\n", "intent": "Transform the data using the original PCA, but keep only the first num_comp columns\n"}
{"snippet": "train_df, test_df = train_test_split(df, random_state = 0)\n", "intent": "Redo the train/test split\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df[predictors], df[target], random_state=2)\n", "intent": "This time, let's separate X from y\n"}
{"snippet": "categories = ['alt.atheism', 'sci.space', 'talk.religion.misc', 'comp.graphics']\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\nprint newsgroups_train.target_names\nprint newsgroups_train.filenames.shape, newsgroups_train.target.shape\n", "intent": "It is possible to load only a sub-selection of the categories:\n"}
{"snippet": "from sklearn.feature_extraction import FeatureHasher\nfh = FeatureHasher(n_features=5)\nfeature_dict = X_train[categorical_predictors].to_dict(orient='records')\nfh.fit(feature_dict)\nout = pd.DataFrame(fh.transform(feature_dict).toarray())\n", "intent": "1: Feature Hashing\nCollapses levels of the variable to a specified number of features (hashing trick?)\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndv = DictVectorizer(sparse=False)\nfeature_dict = X_train[categorical_predictors].to_dict(orient='records')\ndv.fit(feature_dict)\nout = pd.DataFrame(\n    dv.transform(feature_dict),\n    columns = dv.feature_names_\n)\n", "intent": "2: One-hot encoding  \nCreates a dummy variable for each level of the categorical variable\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndv = DictVectorizer(sparse=False)\nfeature_dict = X_train[categorical_predictors].to_dict(orient='records')\ndv.fit(feature_dict)\nout = pd.DataFrame(\n    dv.transform(feature_dict),\n    columns = dv.feature_names_\n)\nfeature_dict\n", "intent": "2: One-hot encoding  \nCreates a dummy variable for each level of the categorical variable\n"}
{"snippet": "from sklearn.pipeline import FeatureUnion\nfu = FeatureUnion([\n        ('numerical_pipe', pipe),\n        ('categorical_pipe', categorical_pipe)\n    ] \n)\n", "intent": "**Exercise:** Use sklearn's FeatureUnion to combine both of your pipelines (one continuous and one categorical) into a single step.\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndv = DictVectorizer(sparse=False)\nfeature_dict = X_train[categorical_predictors].to_dict(orient='records')\ndv.fit(feature_dict)\nout = pd.DataFrame(\n    dv.transform(feature_dict),\n    columns = dv.feature_names_\n)\n", "intent": "2: One-hot encoding\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nclass MyImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, cols):\n        self.cols = cols\n    def fit(self, X, y=None):\n        self.imp = Imputer(strategy='mean')\n        self.imp.fit(X[self.cols])\n        return self\n    def transform(self, X):\n        return self.imp.transform(X[self.cols])\n", "intent": "**Exercise:** Write your own class, MyImputer that takes as an argument the columns you would like to impute missing values for.\n"}
{"snippet": "from sklearn.pipeline import FeatureUnion\nfu = FeatureUnion( \n)\n", "intent": "**Exercise:** Use sklearn's FeatureUnion to combine both of your pipelines (one continuous and one categorical) into a single step.\n"}
{"snippet": "pca = PCA()\nX_transformed = pca.fit_transform(X_train)\n", "intent": "Now lets transform the data into principal component space.\n"}
{"snippet": "new_covar = pd.DataFrame(X_transformed).cov()\n", "intent": "What does the covariance look like now?\n"}
{"snippet": "iris = datasets.load_iris()\nX = iris.data[:, :2]  \ny = iris.target\n", "intent": "Import some data to play with.\n"}
{"snippet": "X_transformed = PCA(n_components=20).fit_transform(X_train)\n", "intent": "Create a new PCA object that is explicity a dimension reducer\n"}
{"snippet": "new_covar = np.round(pd.DataFrame(X_transformed).cov(), 10)\n", "intent": "What does the covariance look like now?\n"}
{"snippet": "X_reduced = PCA(n_components=num_comp).fit_transform(X_train)\n", "intent": "Create a new PCA object that is explicity a dimension reducer\n"}
{"snippet": "train_df, test_df = train_test_split(df, random_state=0)\n", "intent": "Redo the train/test split\n"}
{"snippet": "from sklearn.pipeline import FeatureUnion\nfu = FeatureUnion([('categorical', categorical_pipe), ('numeric', pipe)])\n", "intent": "**Exercise:** Use sklearn's FeatureUnion to combine both of your pipelines (one continuous and one categorical) into a single step.\n"}
{"snippet": "for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    comb_data[col] = comb_data[col].fillna('None')\n", "intent": "All this columns Show or Describe same parameter that is condition of Garage. So we can replace their missing value in single loop\n"}
{"snippet": "for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    comb_data[col] = comb_data[col].fillna(0)\n", "intent": "This columns show or Describe only parameter that is Basement. Since the house with no Basement we can replace the missing value as '0'.\n"}
{"snippet": "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    comb_data[col] = comb_data[col].fillna('None')\n", "intent": "As this columns explain only same parameter about Basement.\n"}
{"snippet": "comb_data[\"Functional\"] = comb_data[\"Functional\"].fillna(\"Typ\")\n", "intent": "This feature explain the Home Functionality rating, and its column contain most high contribution as 'Typ'\nSo we will replace the Nan value but Typ\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ncategories = ['alt.atheism', 'sci.space', 'talk.religion.misc', 'comp.graphics']\nnewsgroups = fetch_20newsgroups(subset='train', categories=categories)\nprint newsgroups.target_names \nprint len(newsgroups.data), 'newsgroups articles in', len(newsgroups.target_names), 'groups'\n", "intent": "Let's import the dataset. We use the keyword argument `categories=[...]` to limit our dataset. Omit this argument to load all twenty newsgroups.\n"}
{"snippet": "comb_data['Exterior1st'] = comb_data['Exterior1st'].fillna(comb_data['Exterior1st'].mode()[0])\ncomb_data['Exterior2nd'] = comb_data['Exterior2nd'].fillna(comb_data['Exterior2nd'].mode()[0])\n", "intent": "Again like above column this column also contain only 1 missing value. We can replace missing value using mode method.\n"}
{"snippet": "comb_data['SaleType'] = comb_data['SaleType'].fillna(comb_data['SaleType'].mode()[0])\n", "intent": "Fill the missing value using Mode method since only one category is maximum.\n"}
{"snippet": "comb_data['MSSubClass'] = comb_data['MSSubClass'].fillna(\"None\")\n", "intent": "This columns gives description as building class. In this Na means that house has not given class, so we will replace the nan value as None\n"}
{"snippet": "comb_data[\"GarageCars\"].fillna(0, inplace=True)\n", "intent": "In this columns we will replace null value as 0.\n"}
{"snippet": "sessions = pd.read_csv('./data/sessions.csv')\ninput_train = pd.read_csv('./data/train_users.csv')\ndf_train, train_labels = input_train.iloc[:,:-1], input_train.iloc[:,-1:]\n", "intent": "Here, we will try to infer additional features from the sessions data and use it in conjunction with the user train data.\n"}
{"snippet": "sessions['action_type'].fillna('-unknown-', inplace=True)\nsessions['action_detail'].fillna('-unknown-', inplace=True)\n", "intent": "We will fill NaN with the -unknown- tag.\n"}
{"snippet": "sessions['secs_elapsed'].fillna(0, inplace=True)\n", "intent": "Let's impute the NaN values with the minimum. i.e. secs_elapsed = 0.\n"}
{"snippet": "scaled_df = pd.DataFrame(data=tranformed_scaler, columns=bank_data.drop('Class', axis=1).columns)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "college = pd.read_csv('College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "X, y = load_iris().data, load_iris().target\n", "intent": "Load the iris dataset.\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_feat, columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train1, X_test1, y_train1, y_test1 = train_test_split(yelp_class['text'], y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "df = dd.read_csv(taxi_fp, \n                 parse_dates=['tpep_pickup_datetime', \n                              'tpep_dropoff_datetime'])\n", "intent": "*Dask Profiler* will now be available at [`http://localhost:8787/status/`](http://localhost:8787/status/)\n"}
{"snippet": "train_data = pd.read_csv(\"E:\\\\INSOFE\\\\AI and Deep Learning\\\\Cute5\\\\train.csv\")\ntest_data = pd.read_csv(\"E:\\\\INSOFE\\\\AI and Deep Learning\\\\Cute5\\\\test.csv\")\n", "intent": "Read train and test data\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import cross_validation\nn_trees = np.arange(30, 70, 10)\nfor n in n_trees:\n    clf = RandomForestClassifier(n_estimators=n)\n    X_train, X_test, y_train, y_test = cross_validation.train_test_split(df_data, df_target, test_size=0.4)\n    clf.fit(X_train, y_train)    \n    print(n, clf.score(X_test, y_test))\n", "intent": "Selected values: learning_rate = 0.05, n_estimators = 80\n"}
{"snippet": "clf.feature_importances_\nfeat_imp = pd.DataFrame()\nfeat_imp['feature'] = X.columns\nfeat_imp['imp'] = clf.feature_importances_\nsns.barplot(x='imp', y='feature', data=feat_imp.sort_values('imp'), orient='h', palette='Blues')\n", "intent": "Select two features with highest importance just to visualize decision surface. CV score for the model with only two features selected is quite low.\n"}
{"snippet": "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()\nTrain_tfidf = tfidf.fit_transform(newsgroups.data)\n", "intent": "$${\\displaystyle \\mathrm {idf} (t,D)=\\log {\\frac {|D|}{|\\{\\,d_{i}\\in D\\mid t\\in d_{i}\\,\\}|}}} ,$$\n"}
{"snippet": "titanic_df = pd.read_csv(\"../data/titanic.csv\")\n", "intent": "Pandas can import and export variet of data files like:\n* csv, text\n* SQL databse\n* Excel\n* json\n* others (eg. HDF5, pickle,etc)\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimputer = Imputer(strategy=\"median\")\nimputer.fit(trainset_num)\nimputer.transform(trainset_num)\n", "intent": "Imputer transformation can be used to fill in blanks with e.g. median\n"}
{"snippet": "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, min_df=2, max_df=0.5)\nX = vectorizer.fit_transform(dataset.data)\nprint \"Dimensions feature matrix:\", X.shape\n", "intent": "Extract features from the training dataset using a sparse vectorizer.\n"}
{"snippet": "X = df3.iloc[:,:3]\ny = df3.iloc[:,3]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=4444)\n", "intent": "Use logistic regression to predict survival after 5 years. How well does your model do?\n"}
{"snippet": "X = df.iloc[:,1:]\ny = df[0]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4444)\n", "intent": "Split the data into a test and training set. Use this function:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(bcs_df.iloc[:,:3], bcs_df.iloc[:,3], test_size=0.3)\n", "intent": "The earliest year of surgery is 1958. The most recent year is 1969.\n"}
{"snippet": "df = pd.read_csv(\"../house_votes_84.csv\",header=None)\n", "intent": "For the house representatives data set, calculate the accuracy, precision, recall and f1 scores of each classifier you built (on the test set).\n"}
{"snippet": "df = pd.read_csv('house-votes-84.data', names=['Party']+['X'+str(i) for i in xrange(1,17)])\ndf.replace('y', 1, inplace=True)\ndf.replace('n', 0, inplace=True)\ndf.replace('?', np.nan, inplace=True)\ndf.fillna(df.mean(),inplace=True)\ndf.head()\n", "intent": "For the house representatives data set, calculate the accuracy, precision, recall and f1 scores of each classifier you built (on the test set).\n"}
{"snippet": "dfCon = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data',header=None)\n", "intent": "For the house representatives data set, calculate the accuracy, precision, recall and f1 scores of each classifier you built (on the test set).\n"}
{"snippet": "pca = PCA(n_components=2).fit(X_train)\nX_train = pca.transform(X_train)\nX_test = pca.transform(X_test)\npca_std = PCA(n_components=2).fit(X_train_std)\nX_train_std = pca_std.transform(X_train_std)\nX_test_std = pca_std.transform(X_test_std)\n", "intent": "Now, we perform a PCA on the standardized and the non-standardized datasets to transform the dataset onto a 2-dimensional feature subspace.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nimport sklearn.metrics.pairwise as smp\nX_train, X_test, y_train, y_test = train_test_split(ng_lsi, ng.target, test_size=0.33)\nknn = KNeighborsClassifier(n_neighbors=3, metric=smp.cosine_distances)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)\n", "intent": "- Try some simple classification on the result LSI vectors for the 20 NG set:\n"}
{"snippet": "ratings = pd.read_table('./movielens/ratings.dat', sep='::', names= ['UserID','MovieID','Rating','Timestamp'])\n", "intent": "Load the ratings.dat data into a `ratings` variable with the same separator, and the column names UserID, MovieID, Rating, Timestamp.\n"}
{"snippet": "n, r = 100, .1\nX = pd.DataFrame()\ncentroids = [[.2, .2], [.8, .2], [.5, .8]]\nfor centroid in centroids:\n    X = X.append(pd.DataFrame(dict(\n        x1=centroid[0] + r * np.random.randn(n),\n        x2=centroid[1] + r * np.random.randn(n))), ignore_index=True)\nX = pd.DataFrame(X)\n", "intent": "Create some test data.\n"}
{"snippet": "df = pd.read_csv('movie_data.csv')\ndf\n", "intent": "**2000 top grossing films based on domestic adjusted gross\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "Split the dataset, then set aside the test set for later...\n"}
{"snippet": "df_all = pd.concat([pd.to_numeric(df_num.CA),df_rsp,df_att],axis=1)\ndf_all = df_all[pd.notnull(df_all['CA'])]\ndf_all.fillna(value=0,inplace=True)\n", "intent": "Make a giant-ass matrix with y as column1 and features as rest of the columns\n"}
{"snippet": "import pandas as pd\ncombats = pd.read_csv('train.csv')\ncombats.head(3)\n", "intent": "http://rautaku.hatenablog.com/entry/2017/12/10/000000\n"}
{"snippet": "iris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "- Multi-class classification\n- Using accuracy as evaluation metric\n- Using cross-validation to select tuning parameters (aka \"hyperparameters\")\n"}
{"snippet": "data = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT5/master/data/default.csv')\nX = data[['balance']]\ny = data.default\n", "intent": "- Binary classification\n- Using AUC as evaluation metric\n- Using cross-validation to select between models\n"}
{"snippet": "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\n", "intent": "- Regression\n- Using RMSE as evaluation metric\n- Using cross-validation to select features\n"}
{"snippet": "from sklearn.datasets import fetch_olivetti_faces\noliv=fetch_olivetti_faces()\nprint oliv.keys()\nprint oliv.data.shape \n", "intent": "This notebook is based on Shankar Muthuswamy's example at https://shankarmsy.github.io/posts/pca-sklearn.html\n"}
{"snippet": "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \nsentences_train = vect.fit_transform(paragraphs_text)\nmodel = lda.LDA(n_topics=10, n_iter=500)\nmodel.fit(sentences_train) \nn_top_words = 10\ntopic_word = model.topic_word_\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n    print('Topic {}: {}'.format(i+1, ', '.join(topic_words)))\n", "intent": "We can try running LDA using the paragraphs as documents.\n"}
{"snippet": "votes = pd.read_csv('http://gadatascience.com/datasets/congress/congressional_votes.csv')\ncongress = votes[['display_name', 'id', 'party', 'state']].drop_duplicates().sort('display_name').reset_index(drop=True)\nprint \"We have %d votes on %d bills from %d members in congress.\" % \\\n    (len(votes), votes.question.nunique(), len(congress))\n", "intent": "Download voting data.\n"}
{"snippet": "titanic = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT5/master/data/titanic_train.csv')\ntitanic.head(10)\n", "intent": "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle.\n"}
{"snippet": "data, features, feature_names, target, target_names, labels = load_data()\nfeatures = features[~is_setosa]\nlabels = labels[~is_setosa]\nvirginica = (labels == \"virginica\")\n", "intent": "So, if the petal length is less than 2 the Iris is a setosa\n"}
{"snippet": "import os\nFILE_PATH = './BuildingMachineLearningSystemsWithPython/ch03/data/toy/'\nposts = [open(os.path.join(FILE_PATH, f)).read() for f in os.listdir(FILE_PATH)]\nvecorizer = CountVectorizer(min_df=1)\nX_train = vecorizer.fit_transform(posts)\nnum_samples, num_features = X_train.shape\nprint('\n", "intent": "So the first sentence contains all but one of the words in the feature set, the second all but three\n"}
{"snippet": "gbc_tuned_df = pd.DataFrame(data=gbc.feature_importances_[:15], index=X_train.columns.values[:15])\ngbc_tuned_plot = gbc_tuned_df.sort_values(by=0).plot.barh(figsize=(15, 10), fontsize=14)\ngbc_tuned_plot.axes.legend().set_visible(False)\ngbc_tuned_plot.set_title('Feature Importance Ranked', fontsize=14)\ngbc_tuned_plot.axes.set_xlabel('Relative ranking score that sums to 1.0', fontsize=14)\ngbc_tuned_plot.axes.set_ylabel('Features', fontsize=14)\n", "intent": "<!--\n-->\nHere is a horizontal barchart of the sorted feature importances from the tuned **`GradientBoostingClassifier`**:\n"}
{"snippet": "data_A = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\ndataframeA= pd.read_csv(data_A, delimiter=',' )\ndata_B = os.path.join(os.getcwd(), 'datasets',  'train_20news_partB.csv')\ndataframeB= pd.read_csv(data_B, delimiter= ',' )\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "data_A = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\ndataframeA= pd.read_csv(data_A, delimiter=',' )\ndata_B = os.path.join(os.getcwd(), 'datasets',  'train_20news_partB.csv')\ndataframeB= pd.read_csv(data_B, delimiter= ',' )\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n<span style=\"color:red\">OK</span>\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'spambase_test.csv')\nspambase_test = pd.read_csv(data_path, delimiter = ',')\nspambase_test\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure\n"}
{"snippet": "data_path_A = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\nnews_A = pd.read_csv(data_path_A, delimiter = ',')\ndata_path_B = os.path.join(os.getcwd(), 'datasets', 'train_20news_partB.csv')\nnews_B = pd.read_csv(data_path_B, delimiter = ',')\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'train_images_partB.csv')\nmc_train = pd.read_csv(data_path, delimiter = ',')\ndata_path = os.path.join(os.getcwd(), 'datasets', 'valid_images_partB.csv')\nmc_val = pd.read_csv(data_path, delimiter = ',')\ndata_path = os.path.join(os.getcwd(), 'datasets', 'test_images_partB.csv')\nmc_test = pd.read_csv(data_path, delimiter = ',')\n", "intent": "*Your answer goes here (max. 600 words)*\n"}
{"snippet": "vectorizer = CountVectorizer(stop_words='english', max_features=10000, min_df=2, max_df=0.5)\nX = vectorizer.fit_transform(dataset.data)\nn_samples, n_features = X.shape\nprint \"Dimensions feature matrix:\", X.shape\n", "intent": "Extract features from the training dataset using a sparse vectorizer.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nprint('train/test split: {}/{}={:.2f}'.format(len(X_train), len(X_valid), len(X_valid)/len(X_train)))\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=0)\nprint('train/test split: {}/{}={:.2f}'.format(len(X_train), len(X_valid), len(X_valid)/len(X_train)))\n", "intent": "The training (and test) dataset is more balanced now.\n"}
{"snippet": "pseudo_inputs = sparse\nnew_test_inputs = training_inputs[0:1000,:]\ntest_cov, normal_msense, pseudo_msense, normal_peroutput_msense, pseudo_peroutput_msense, normal_mu, pseudo_mu, K_normal, K_pseudo = dp4gp.get_noise_scale(ys,new_test_inputs,training_inputs,pseudo_inputs,lengthscales,sigma,calc_normal=use_normal)  \n", "intent": "Sample at each training point (not we don't leave one out, we are leaving in the training data)\n"}
{"snippet": "pseudo_inputs = sparse\ntest_inputs = training_inputs[0:1000,:]\ncalc_normal = False\ntest_cov, normal_msense, pseudo_msense, normal_peroutput_msense, pseudo_peroutput_msense, normal_mu, pseudo_mu, K_normal, K_pseudo = dp4gp.get_noise_scale(ys,test_inputs,training_inputs,pseudo_inputs,lengthscales,sigma,calc_normal=calc_normal)\n", "intent": "Sample at each training point (not we don't leave one out, we are leaving in the training data)\n"}
{"snippet": "for j in range(5):\n    print(\"zipcodes in cluster\", j)\n    print(cleannyczips[km.labels_ == j])\n    print(\"\\n\\n\\n\\n\")\nclustersdf = gp.GeoDataFrame()\nclustersdf['ZIPCODE'] = cleannyczips\nclustersdf['cluster'] = km.labels_\n", "intent": "the cluster centers for 5 k-means clusters of business patterns (number of businesses) at the zipcode level for NYC zipcodes\n"}
{"snippet": "for j in range(5):\n    print(\"zipcodes in cluster\", j)\n    print(cleannyczips[labelsag == j])\n    print(\"\\n\\n\\n\\n\")\nclustersdf = gp.GeoDataFrame()\nclustersdf['ZIPCODE'] = cleannyczips\nclustersdf['cluster'] = labelsag\n", "intent": "as Figure 2 but for hierarchical agglomerative clustering\n"}
{"snippet": "for j in range(nc):\n    print(\"zipcodes in cluster\", j)\n    print(cleannyczips[labelsag == j])\n    print(\"\\n\\n\\n\\n\")\nclustersdf = gp.GeoDataFrame()\nclustersdf['ZIPCODE'] = cleannyczips\nclustersdf['cluster'] = labelsag\n", "intent": "As figure 2 and 4: time series of business counts in NYC by zipcode, with hierarchical clustering in 7 clusters. \n"}
{"snippet": "for j in range(nc):\n    print(\"zipcodes in cluster\", j)\n    print(cleannyczips[labelsag == j])\n    print(\"\\n\\n\\n\\n\")\nclustersdf = gp.GeoDataFrame()\nclustersdf['ZIPCODE'] = cleannyczips\nclustersdf['cluster'] = labelsag\n", "intent": "As Figure  2, 4, 6 but for smoothed time series \n"}
{"snippet": "save_dir = \"/home/ucsd-train02/projects/single_cell_intestine/results/\"\ndf_tsne_pca.to_csv(save_dir+\"tsne_phenograph_k30_results.csv\")\n", "intent": "Save the cluster assignments to a file that we will use later for differential gene expression.\n"}
{"snippet": "dataset = pd.read_csv('data.csv')\ndataset['Timestamp'] = pd.to_datetime(dataset['Unnamed: 0'])\nlabel = dataset['Measured & upscaled [MW]']\ndataset = dataset.drop(columns=['Unnamed: 0', 'Measured & upscaled [MW]', 'Monitored Capacity [MW]'])\ndataset = dataset.set_index('Timestamp')\n", "intent": "Here I'm making some preprocessing to use data in machine learning algorithms.\n"}
{"snippet": "model = PCA(n_components=2)\nPCs = model.fit_transform(X)\n", "intent": "Let's find the principal components in this data. Note that we can't have more components than n_features, so we're looking for all two components.\n"}
{"snippet": "from sklearn import decomposition\nlda1 = decomposition.LatentDirichletAllocation(n_topics=6)\nlda1.fit(tfidf)\nW = lda1.transform(tfidf)\nH = lda1.components_\n", "intent": "Compare the results of Nonnegative Matrix Factorization (NMF) with Latent Dirichlet Allocation (LDA).\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n                                                    train_size=0.75, test_size=0.25)\n", "intent": "Here we'll use 75% of the data for training, and test on the remaining 25%.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n                                                    train_size=0.75, test_size=0.25)\n", "intent": "Now we can use the train_test_split feature:\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "First we'll load the Boston dataset.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n                                                    train_size=0.75, test_size=0.25)\n", "intent": "Then split our data into train and test sets.\n"}
{"snippet": "Loadings =  pd.DataFrame((pc_final.components_.T * np.sqrt(pc_final.explained_variance_)).T,columns=credit_new.columns).T\n", "intent": "Loadings=Eigenvectors * sqrt(Eigenvalues)\nloadings are the covariances/correlations between the original variables and the unit-scaled components.\n"}
{"snippet": "from sklearn import datasets,svm\ndigits = datasets.load_digits()\nx_digits= digits.data\ny_digits = digits.target\nsvc = svm.SVC(C=1,kernel = 'linear')\nsvc.fit(x_digits[:-100],y_digits[:-100]).score(x_digits[-100:],y_digits[-100:])\n", "intent": "**Model selection: choosing estimators and their parameters**\n"}
{"snippet": "from sklearn import cluster,datasets\niris= datasets.load_iris()\nx_iris = iris.data\ny_iris = iris.target\nk_means = cluster.KMeans(n_clusters =3)\nk_means.fit(x_iris)\n", "intent": "**Unsupervised learning: seeking representations of the data**\n"}
{"snippet": "x1= np.random.normal(size=100)\nx2= np.random.normal(size=100)\nx3=x1+x2\nX=np.c_[x1,x2,x3]\nfrom sklearn import decomposition\npca = decomposition.PCA()\npca.fit(X)\n", "intent": "**Principal component analysis: PCA**\n"}
{"snippet": "max_features = 5000\ncv = CountVectorizer(max_features=max_features)\nX_style = cv.fit_transform(data.beer_style)\n", "intent": "Let's use each word in the beer style as a feature as well (e.g., \"IPA\")\n"}
{"snippet": "from sklearn.svm import LinearSVC\nfrom sklearn.datasets import make_classification\nX,y = make_classification(n_features=4,random_state=0)\nclf = LinearSVC(random_state=0)\nclf.fit(X,y)\n", "intent": "[kernel functions](http://scikit-learn.org/stable/modules/svm.html\n"}
{"snippet": "v = VarianceThreshold(0.5)\nv.fit_transform(modelData.fillna(-999)).shape\n", "intent": "http://bluewhale.cc/2016-11-25/use-scikit-learn-for-feature-selection.html\n"}
{"snippet": "df = pd.read_csv(\"/Users/Tenorio/vermonster/exercises/DS_BOS_07/Data/Spam Classification/sms.csv\")\ndf.head()\n", "intent": "Now read in the data with `pandas` `read_csv()` and check it out with `head()`:\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer(decode_error = 'ignore')\nvect.fit(X_train)\nprint(vect.get_feature_names())\n", "intent": "Use `sklearn.feature_extraction.text.CountVectorizer` on the training set to create a vectorizer called `vect`.\n"}
{"snippet": "titanic = pd.read_csv('/Users/Tenorio/vermonster/exercises/DS_BOS_07/Data/Titanic/titanic.csv')\ntitanic.head()\n", "intent": "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle.\n"}
{"snippet": "iris = datasets.load_iris()\nprint iris.feature_names\nX = iris.data\nprint X[:, 1]\nprint iris.target_names\n", "intent": "Display the features names\n"}
{"snippet": "df = pd.read_csv(\"example data/Classified Data\",index_col=0)\n", "intent": "Set index_col=0 to use the first column as the index.\n"}
{"snippet": "from sklearn.datasets import load_iris\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier()\niris = load_iris()\nclf = clf.fit(iris.data, iris.target)\ntree.export_graphviz(clf, out_file='tree2.dot')  \n", "intent": "** note: need to install graphviz and run notebook from Anaconda launcher - otherwise may complain about path\n"}
{"snippet": "pred = pd.DataFrame(index=y_test.index)\npred[\"Actual\"] = y_test\n", "intent": "Create the prediction DataFrame:\n"}
{"snippet": "from sklearn import datasets\nboston = datasets.load_boston() \nX = boston['data']   \nY = boston['target'] \n", "intent": "Let's take a look at the famous Boston Housing data\n"}
{"snippet": "import pandas as pd\ndf = pd.read_table('../smsspamcollection/SMSSpamCollection',\n                    sep='\\t',\n                    header = None,\n                    names=['label','sms_message'])\ndf.head()\n", "intent": "https://github.com/udacity/machine-learning/blob/master/projects/practice_projects/naive_bayes_tutorial/Naive_Bayes_tutorial.ipynb\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=4)\nprint X_train[0]\nprint X_test[0]\nprint y_train[0]\nprint y_test[0]\n", "intent": "* refaire le meme avec random_state = 4\n"}
{"snippet": "import pandas\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nimport pickle\ndf = pandas.read_csv(\"train_loan.csv\")\n", "intent": "* Chargement du fichier sur les prets bancaires\n"}
{"snippet": "val = {}\nmodule = 'Adult'\nval[module] = pd.read_csv('../3-data/phmrc_cleaned.csv')\n", "intent": "An approach to really do the cross-validation *out of sample*:\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidfvec = TfidfVectorizer()\ndtm_tfidf_df = pandas.DataFrame(tfidfvec.fit_transform(df.body).toarray(), columns=tfidfvec.get_feature_names(), index = df.index)\ndtm_tfidf_df\n", "intent": "To do so, we simply do the same thing we did above with CountVectorizer, but instead we use the function TfidfVectorizer.\n"}
{"snippet": "import os\nreview_path = '../../A-Data/poems/reviewed/'\nrandom_path = '../../A-Data/poems/random/'\nreview_files = os.listdir(review_path)\nrandom_files = os.listdir(random_path)\nreview_texts = [open(review_path+file_name).read() for file_name in review_files]\nrandom_texts = [open(random_path+file_name).read() for file_name in random_files]\nreview_texts[0] \n", "intent": "First we will read the texts and turn them into lists.  \n"}
{"snippet": "df1 = pandas.DataFrame(review_texts, columns = ['body'])\ndf1['label'] = \"review\"\ndf2 = pandas.DataFrame(random_texts, columns = ['body'])\ndf2['label'] = \"random\"\ndf = pandas.concat([df1,df2])\ndf\n", "intent": "Notice the strange output here? These poems are saved in a bag of words format.\nNext we'll create a Pandas dataframe.\n"}
{"snippet": "import os\nreview_path = '../A-Data/poems/reviewed/'\nrandom_path = '../A-Data/poems/random/'\nreview_files = os.listdir(review_path)\nrandom_files = os.listdir(random_path)\nreview_texts = [open(review_path+file_name).read() for file_name in review_files]\nrandom_texts = [open(random_path+file_name).read() for file_name in random_files]\nreview_texts[0] \n", "intent": "First we will read the texts and turn them into lists.  \n"}
{"snippet": "population_index.to_csv(\"test_df_to_csv.csv\")\n", "intent": "> The data can be saved to new csv files with teh following command or to html using df.to_html()\n"}
{"snippet": "from sklearn import svm\nfrom sklearn import datasets\nclf = svm.SVC()\niris = datasets.load_iris()\nX, y = iris.data, iris.target\nclf.fit(X, y)\n", "intent": "we might sometimes build a model that take a long time to construct. We can easily save the model (on disk) for future use.\n"}
{"snippet": "rand = np.random.mtrand.RandomState(8675309)  \ncats = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\ntraindata = fetch_20newsgroups(subset='train',                  \n                          categories=cats,                      \n                          shuffle=True,                         \n                          remove=('headers', 'footers', 'quotes'), \n                          random_state=rand)\n", "intent": "_20newsgroups_ dataset\nSee http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n"}
{"snippet": "X = encode[:50]\ntsne_data = manifold.TSNE(n_components=2).fit_transform(X)\n", "intent": "We will take first 50 values to visualize the data.\n"}
{"snippet": "tsne_data = manifold.TSNE(n_components=2).fit_transform(X)\nplot_clustering(tsne_data[indices], X[indices], y_num, title='t-SNE')\n", "intent": "**Excerise 5:** Apply T-SNE on the data\n"}
{"snippet": "from sklearn import decomposition\nimport numpy as np\npca = decomposition.PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nplot_clustering(X_pca, X, y, title='PCA')\n", "intent": "**Excerise 1:** Apply PCA on the data\n"}
{"snippet": "tsne_data = manifold.TSNE(n_components=2).fit_transform(X)\nplot_clustering(tsne_data, X, y, title='t-SNE')\n", "intent": "**Excerise 4:** Apply t-SNE on the data\n"}
{"snippet": "from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split\nknr = KNeighborsRegressor(n_neighbors=3)\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=0)\nknr.fit(X_train, y_train)\nknr.score(X_test, y_test)\n", "intent": "Split the boston housing data into training and test set, apply the ``KNeighborsRegressor`` and compute the test set $R^2$.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression, Lasso, Ridge\nfrom sklearn.cross_validation import train_test_split\nimport patsy\nformula = 'label ~ spelling_errors_ratio + linkwordscore + non_markup_alphanum_characters + frameTagRatio -1'\ny_mat, x_mat = patsy.dmatrices(formula, data = su)\nx_train, x_test, y_train, y_test = train_test_split(x_mat,y_mat, test_size=0.3)\n", "intent": "And print out the results as shown in the example above.\n---\n"}
{"snippet": "formula = 'label ~ C(alchemy_category) + spelling_errors_ratio + linkwordscore + non_markup_alphanum_characters + frameTagRatio -1'\ny_mat, x_mat = patsy.dmatrices(formula, data = norm_su)\nx_train, x_test, y_train, y_test = train_test_split(x_mat,y_mat, test_size=0.3)\n", "intent": "And print out the results as shown in the example.\n---\n"}
{"snippet": "formula = 'label ~ C(alchemy_category) + spelling_errors_ratio + linkwordscore + non_markup_alphanum_characters + frameTagRatio -1'\ny_mat, x_mat = patsy.dmatrices(formula, data = norm_su)\nx_train, x_test, y_train, y_test = train_test_split(x_mat,y_mat, test_size=0.3)\n", "intent": "Normalize the numeric and categorical columns of the predictor matrix.\n---\n"}
{"snippet": "import pandas as pd\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/titanic.csv'\ntitanic = pd.read_csv(url, index_col='PassengerId')\ntitanic.shape\n", "intent": "scikit-learn models expect that all values are **numeric** and **hold meaning**. Thus, missing values are not allowed by scikit-learn.\n"}
{"snippet": "dfv = pd.read_csv(votes_file)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "Xs = StandardScaler().fit_transform(X)\n", "intent": "Next, create the covariance matrix from the standardized x-values and decompose these values to find the eigenvalues and eigenvectors\n"}
{"snippet": "PCA_set = PCA(n_components=5)\nX = PCA_set.fit_transform(Xs)\n", "intent": "Now, conduct a PCA using scikit learn\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "df = pd.read_csv('./assets/datasets/airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "Xs = StandardScaler().fit_transform(X)\n", "intent": "Then, standardize the features for analysis\n"}
{"snippet": "pc = pd.DataFrame(Xpc, columns = ['PC1', 'PC2'])\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "y = boston.target\nboston = pd.DataFrame(boston.data)\n", "intent": "Next, separate the data into the features and target using the following code:\n`y = boston.target`\n`boston = pd.DataFrame(boston.data)`\n"}
{"snippet": "churn_file = pd.read_csv(r'C:\\Users\\LinRi001\\Desktop\\churn project\\train.csv')\ntransactions = pd.read_csv(r'C:\\Users\\LinRi001\\Desktop\\churn project\\transactions.csv')\nmembers = pd.read_csv(r'C:\\Users\\LinRi001\\Desktop\\churn project\\members_v3.csv') \n", "intent": "This is a Kaggle competition data\nhttps://www.kaggle.com/c/kkbox-churn-prediction-challenge/data\n"}
{"snippet": "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\npcaSimple = PCA(n_components=2)\npcaSimple.fit(X)\nprint(pcaSimple.explained_variance_ratio_) \n", "intent": "Run a simple example\n"}
{"snippet": "of_df = pd.read_csv(\"../assets/datasets/old-faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\n", "intent": "The datasets are loaded into a dictionary.\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\nprint(digits.keys())\n", "intent": "The datasets are loaded into a dictionary\n"}
{"snippet": "project_data = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaled_df = pd.DataFrame(scaled_features, columns=project_data.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = scaled_df\ny = project_data['TARGET CLASS']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n", "intent": "**Use train_test_split to split your data into a training set and a testing set.**\n"}
{"snippet": "from sklearn import datasets            \ndiabetes = datasets.load_diabetes()     \nx_train = diabetes.data[:-20]           \ny_train = diabetes.target[:-20]          \nx_test = diabetes.data[-20:]            \ny_test = diabetes.target[-20:]           \n", "intent": "Start by breaking the 442 patients into a training set (composed of the first 422 patients) and a test set (the last 20 patients).\n"}
{"snippet": "import pandas as pd\npd.set_option('display.max_columns', 500)\nimport zipfile\nwith zipfile.ZipFile('KaggleCredit2.csv.zip', 'r') as z:\n    f = z.open('KaggleCredit2.csv')\n    data = pd.read_csv(f, index_col=0)\ndata.head()\n", "intent": "Read the data into Pandas \n"}
{"snippet": "s_data = scaler.fit_transform(csv.drop('Class',axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "df_feat = pd.DataFrame(data=s_data, columns=csv.columns.drop('Class'))\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "def references_organization(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == 'ORG' for word in parsed])\ndata['references_organization'] = data['title'].fillna(u'').map(references_organization)\ndata[data['references_organization']][['title']].head()\n", "intent": "Let's see if we can find organizations in our page titles.\n"}
{"snippet": "seed = 8 \nX_train, X_test, y_train, y_test = train_test_split(X, y1, test_size=0.2,\n                                                    random_state=seed)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n", "intent": "Split data into training and test datasets\n"}
{"snippet": "def create_ND_clusters(num_clusters, num_features):\n    X, y = sklearn.datasets.make_blobs(n_samples=50000, n_features=num_features, centers=num_clusters, \n                                       cluster_std=1.0, center_box=(0.0, 10.0), shuffle=True, \n                                       random_state=2)\n    return X, y\nnum_clusters = 5\nnum_dimensions = 100\nX_Nd, y_Nd = create_ND_clusters(num_clusters, num_dimensions)\n", "intent": "This section creates a generic dataset to use in an N-dimensional unit test using a single core.\n"}
{"snippet": "token_text = [token.orth_ for token in parsed_review]\ntoken_pos = [token.pos_ for token in parsed_review]\npd.DataFrame(data=list(zip(token_text, token_pos)),\n             columns=['token_text', 'part_of_speech'])\n", "intent": "<h2>Part of Speech Tagging</h2>\n"}
{"snippet": "token_lemma = [token.lemma_ for token in parsed_review]\ntoken_shape = [token.shape_ for token in parsed_review]\npd.DataFrame(list(zip(token_text, token_lemma, token_shape)),\n             columns=['token_text', 'token_lemma', 'token_shape'])\n", "intent": "<h2>Text normalization, like stemming/lemmatization and shape analysis?</h2>\n"}
{"snippet": "token_entity_type = [token.ent_type_ for token in parsed_review]\ntoken_entity_iob = [token.ent_iob_ for token in parsed_review]\npd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)),\n             columns=['token_text', 'entity_type', 'inside_outside_begin'])\n", "intent": "<h2>Token-level entity analysis</h2>\n"}
{"snippet": "warPeacePOS = pd.Series(war_peace.count_by(spacy.attrs.POS))/len(war_peace)\nrcParams['figure.figsize'] = 16, 8\ndf = pd.DataFrame([warPeacePOS], index=['war_peace'])\ndf.columns = [tagDict[column] for column in df.columns]\ndf.T.plot(kind='bar')\n", "intent": "<h2>It's fun to compare the distribution of parts of speech in each text:</h2>\n"}
{"snippet": "def verbsToMatrix(verbCounts): \n    return pd.Series({t[0]: t[1] for t in verbCounts})\nverbsDF = pd.DataFrame({'Napoleon': verbsToMatrix(NapoleonVerbs), \n                        'Pierre': verbsToMatrix(PierreVerbs), \n                        'Nicholas': verbsToMatrix(verbsForCharacters(war_peace, 'Nicholas'))}).fillna(0)\nverbsDF.plot(kind='bar', figsize=(14,4));\n", "intent": "<h2>Now merge these counts into a single table, and then visualize it with Pandas.</h2>\n"}
{"snippet": "grailPOS = pd.Series(grail.count_by(spacy.attrs.POS))/len(grail)\npridePOS = pd.Series(pride.count_by(spacy.attrs.POS))/len(pride)\nrcParams['figure.figsize'] = 16, 8\ndf = pd.DataFrame([grailPOS, pridePOS], index=['Grail', 'Pride'])\ndf.columns = [tagDict[column] for column in df.columns]\ndf.T.plot(kind='bar')\n", "intent": "<h2>It's fun to compare the distribution of parts of speech in each text:</h2>\n"}
{"snippet": "def verbsToMatrix(verbCounts): \n    return pd.Series({t[0]: t[1] for t in verbCounts})\nverbsDF = pd.DataFrame({'Elizabeth': verbsToMatrix(elizabethVerbs), \n                        'Darcy': verbsToMatrix(darcyVerbs), \n                        'Jane': verbsToMatrix(janeVerbs)}).fillna(0)\nverbsDF.plot(kind='bar', figsize=(14,4))\n", "intent": "<h2>Now merge these counts into a single table, and then visualize it with Pandas.</h2>\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    contains_org = any([word.ent_type_ == 'ORG' for word in parsed])\n    contains_person = any([word.ent_type_ == 'PERSON' for word in parsed])\n    return contains_org and contains_person\ndata['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\ndata[data['references_org_person']][['title']].head()\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "N = 200\nnp.random.seed(0)\nX, y = sklearn.datasets.make_circles(N, noise = 0.05, factor = 0.5)\n", "intent": "Create data for a simple binary classification problem.\n"}
{"snippet": "data = pd.read_csv(\"task/task_data.csv\").drop([\"sample index\"], axis=1)\ndata.head()\n", "intent": "Read data to `pandas` DataFrame object, discarding first column as it doesn't contain useful information.\n"}
{"snippet": "result_1.to_csv(\"result_1.csv\", header=[\"score\"], index_label=\"sensor\")\n", "intent": "Write it to the file `result_1.csv`:\n"}
{"snippet": "result_3.to_csv(\"result_3.csv\", index=False)\n", "intent": "Note: in this table, score means how much accuracy we lose permuting respective feature.\nSave the results in `result_3.csv` file:\n"}
{"snippet": "df = pd.read_csv('data/train.csv')\nX = df.loc[:, df.columns != 'label'].as_matrix().astype('uint8')\nY = df.label.as_matrix().tolist()\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.25)\n", "intent": "load and split kaggle training data\n"}
{"snippet": "clf2 = joblib.load('mnist.randomforestmodel.sav')\ndf = pd.read_csv('data/test.csv')\nX_sample = df.values\n", "intent": "load the trained model and the kaggle sample set then visualize the first entry\n"}
{"snippet": "index = [i for i in range(1, len(Y_sample)+1)]\nindex\ndf = pd.DataFrame(Y_sample, index=index, columns=[\"Label\"])\ndf.index.name = \"ImageId\"\ndf.to_csv('data/result.csv')\n", "intent": "structure kaggle submission according to requirements\n"}
{"snippet": "data2 = pd.read_csv(\"LogR2.csv\",header = None)\ndata2.columns = [\"Microchip1\",\"Microchip2\",\"Pass\"]\ndata2.head()\n", "intent": "Now imagine this situation, where you have two microchip voltage values. \n"}
{"snippet": "esquerda = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n                     'A': ['A0', 'A1', 'A2', 'A3'],\n                     'B': ['B0', 'B1', 'B2', 'B3']})\ndireita = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n                          'C': ['C0', 'C1', 'C2', 'C3'],\n                          'D': ['D0', 'D1', 'D2', 'D3']})    \n", "intent": "Parecido com o `JOIN` em SQL:\n"}
{"snippet": "import pandas as pd\nyelp = pd.read_csv('../data/yelp.csv')\nyelp.head(1)\n", "intent": "Read `yelp.csv` into a DataFrame.\n"}
{"snippet": "df= pd.DataFrame()\ndf['residuals'] = bos.PRICE - pred\ndf['predicted_vals'] = pred\n", "intent": "The mean is 22.53 and the standard deviation is 7.91\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('http://www.ats.ucla.edu/stat/data/binary.csv')\ndata\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\nrfModel = rf.fit(trainingData)\nrfFeatureImportance = pd.DataFrame([(name, rfModel.featureImportances[idx]) for idx, name in attrs],columns=['feature_name','feature_importance'])\nprint(rfFeatureImportance.sort_values(by=['feature_importance'],ascending =False))\n", "intent": "1. Build and train a RandomForestClassifier and print out a table of feature importances from it.\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.svm import LinearSVC\niris = datasets.load_iris()\niris = datasets.load_iris()\nX2 = iris.data[:, :2]  \ny2 = iris.target\nsvc = SVC(kernel='linear').fit(X2, y2)\nrbf_svc = SVC(kernel='rbf', gamma=0.7).fit(X2, y2)\npoly_svc = SVC(kernel='poly', degree=3).fit(X2, y2)\nlin_svc = LinearSVC().fit(X2, y2)\n", "intent": "Credits: Forked from http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html\n"}
{"snippet": "df_original=pd.read_csv(\"Fraud_Data.csv\",header=0)\n", "intent": "Reading csv file and storing as dataframe\n"}
{"snippet": "Pipe_Rf_roc=Pipeline([(\"Scaler\",StandardScaler()),(\"Rf\",RandomForestClassifier())])\nParam_grid_Rf_roc={\"Scaler__with_std\":[True],'Rf__n_estimators':[30],'Rf__min_samples_split':[650],\\\n                  'Rf__max_features':['auto',1,2],'Rf__class_weight':[{0:1,1:1},'balanced',{0:1,1:2}]}\nbest_Rf_roc=GridSearchCV(Pipe_Rf_roc,Param_grid_Rf_roc,cv=4,verbose=1,n_jobs=1,scoring='roc_auc')\nbest_Rf_roc.fit(Train_X,Train_Y)\nprint best_Rf_roc.best_params_,best_Rf_roc.best_score_,\n", "intent": "I also Tried to optimize for roc-auc score for random forests.  The results were not much different. \n"}
{"snippet": "to_keep_ml3=[\"device use frequency\",\"hours_to_purchase\",\"Country_mask\",\"purchase_value\",\"age\"]\nx_array3=df_ml[to_keep_ml3].values\ny_array3=df_ml[\"class\"]\nprint x_array3.shape,y_array3.shape\nTrain_X3,Test_X3,Train_Y3,Test_y3=train_test_split(x_array3,y_array3,test_size=0.25,stratify=y_array2,random_state=42)\n", "intent": "Now I tried adding some other values such as purchase value and age to our feature set.\n"}
{"snippet": "Scaler_vis=StandardScaler()\nx_scaled=Scaler_vis.fit_transform(x_array)\n", "intent": "Lets try to visualize how the fraud and not fraud classes look for the two most important variables after they have been standard scaled.\n"}
{"snippet": "Pipe_Rf=Pipeline([(\"Scaler\",StandardScaler()),(\"Rf\",RandomForestClassifier())])\nParam_grid_Rf={\"Scaler__with_std\":[True],'Rf__n_estimators':[20],'Rf__min_samples_split':[650,800],\\\n                  'Rf__max_features':['auto'],'Rf__class_weight':['balanced',{0:1,1:1}]}\nbest_Rf_recall=GridSearchCV(Pipe_Rf,Param_grid_Rf,cv=4,verbose=1,n_jobs=1,scoring='recall')\nbest_Rf_recall.fit(Train_X,Train_Y)\nprint best_Rf_recall.best_params_,best_Rf_recall.best_score_,\n", "intent": "We do the same thing for Random forests and spot very similar trends. \n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\n", "intent": "Use CountVectorizer to create document-term matrices from X_train and X_test.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nCount=CountVectorizer()\nX_train_dtm=Count.fit_transform(X_train)\nX_test_dtm=Count.transform(X_test)\n", "intent": "Use CountVectorizer to create **document-term matrices** from X_train and X_test.\n"}
{"snippet": "def references_organization(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == 'ORG' for word in parsed])\ndata['references_organization'] = data['title'].fillna(u'').map(references_organization)\ndata[data['references_organization']][['title']].head().values\n", "intent": "Let's see if we can find organizations in our page titles.\n"}
{"snippet": "def references_org_person(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == 'ORG' for word in parsed]) and any([word.ent_type_ == \"PERSON\" for word in parsed])\ndata['references_org_person'] = data['title'].fillna(u'').map(references_organization)\ndata[data['references_org_person']][['title']].head().values\n", "intent": "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "labels_ADMSSN = data_valid['ADMSSN']\ndata_modeling = data_valid[['APPLCN','SATNUM','ACTNUM','YEAR', \n                            'ACTCM25_NORM', 'ACTCM75_NORM', 'SATCM25_NORM', 'SATCM75_NORM']]\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data_modeling, \n                                                    labels_ADMSSN, \n                                                    test_size = 0.20, \n                                                    random_state = 11)\n", "intent": "Enough of pre-processing, let's put our data into a LinearRegression model and see how it performs.\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn.svm import LinearSVC\niris = datasets.load_iris()\niris = datasets.load_iris()\n", "intent": "Credits: Forked from http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html\n"}
{"snippet": "daily_rets = prices_df.pivot_table(values=['daily_ret'],\n                                   index='date', \n                                   columns='ticker')\nvol_horizons = [x*21 for x in [1, 3, 6, 12]]\nfor period in vol_horizons:\n    factor_data[str(period)+'d_vol'] = (daily_rets['daily_ret'].rolling(period, \n                                                                  min_periods=(int(period*.8)))\\\n                                      .std() * np.sqrt(252)).stack()\n", "intent": "Define stock price volatility over 1, 3, 6, and 12 months\n"}
{"snippet": "tech_factor_dictionaries = [bop, mfi, dx, stoch_osc, ATR]\ntech_factor_names = ['bop', 'mfi', 'dx', 'stoch_osc', 'atr']\ntech = []\nfor factor, col in zip(tech_factor_dictionaries, tech_factor_names):\n    df = pd.DataFrame(pd.concat(factor), columns=[col])\n    df = df.swaplevel()\n    df.sort_index(level=0, inplace=True)\n    tech.append(df)\ntech_factors = pd.concat(tech, axis=1)\ntech_factors.index.names = ['date', 'ticker']\n", "intent": "Combine dictionaries for technical factors\n"}
{"snippet": "all_factors.to_csv('data//all_factors.csv')\n", "intent": "Write final factor data to csv\n"}
{"snippet": "fwd_ret_horizons = [1, 5, 21, 42, 63]\nfor ret in fwd_ret_horizons:\n    all_factors['fwdret_' + str(ret) + 'd'] = all_factors.pivot_table(values='adj_close',\n                                                                      index='date', \n                                                                      columns='ticker').pct_change(ret).shift(-ret).stack()\n", "intent": "The 63 day future return horizon will be used for this analysis.\n"}
{"snippet": "vect = TfidfVectorizer(stop_words='english')\ndtm = vect.fit_transform(yelp.text)\nfeatures = vect.get_feature_names()\ndtm.shape\n", "intent": "Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF!\n"}
{"snippet": "from keras.datasets import imdb\n(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n                                                      num_words=None,\n                                                      skip_top=0,\n                                                      maxlen=None,\n                                                      seed=113,\n                                                      start_char=1,\n                                                      oov_char=2,\n                                                      index_from=3)\n", "intent": "Load data from keras\n"}
{"snippet": "indir = 'C:\\\\Users\\\\useradmin\\\\Desktop\\\\Learning\\\\Python-Data-Science-and-Machine-Learning-Bootcamp\\\\Machine Learning Sections\\\\Decision-Trees-and-Random-Forests\\\\'\nloans = pd.read_csv(indir +'loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "df = pd.read_csv('../data/housing-data.csv')\n", "intent": "This dataset contains multiple columns:\n- sqft\n- bdrms\n- age\n- price\n"}
{"snippet": "df = pd.read_csv('churn.csv')\n", "intent": "Load the csv file into memory using Pandas\n"}
{"snippet": "bnd_new = pd.DataFrame(bnd_trans,columns=bnd.columns[:-1])\nbnd_new.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "knn_data = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "knn_data_scl = pd.DataFrame(scaled_f,columns=knn_data.columns[:-1])\nknn_data_scl.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nbow_trans = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "X = bow_trans.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "titles = data['title'].fillna('')\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features = 1000, \n                             ngram_range=(1, 2), \n                             stop_words='english',\n                             binary=True)\nvectorizer.fit(titles)\nX = vectorizer.transform(titles)\n", "intent": " We previously used the Count Vectorizer to extract text features for this classification task\n"}
{"snippet": "sequences = []\ny = []\nfor i in xrange(5000):\n    y0 = gen_curve()\n    y1 = gen_curve() + gen_anomaly()\n    sequences.append(y0)\n    sequences.append(y1)\n    y.extend([False, True])\nsequences = pd.DataFrame(sequences)\ny = np.array(y)\n", "intent": "- 10000 curves\n- all with the same length\n- 50% with anomaly\n- 50% without anomaly\n"}
{"snippet": "from scipy.signal import welch\nfff, Pxx = welch(sequences_scaled, nperseg=64, axis=1)\nX = pd.DataFrame(Pxx)\nX.head()\n", "intent": "In this example, the anomaly has a higher frequency than the base signal. Could we isolate it based on the frequency spectrum?\n"}
{"snippet": "features_missing_values = ['Total_Images','Total_Links']\ndata_missing_values = data4[features_missing_values]\nimputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\ndata_missing_values_array = imputer.fit_transform(data_missing_values)\ndata_missing_values = pd.DataFrame(data_missing_values_array, index=data_missing_values.index, columns=data_missing_values.columns)\ndata_missing_values\n", "intent": "- 'Total_Images'\n- 'Total_Links'\n"}
{"snippet": "features_missing_values = ['Total_Images','Total_Links', 'Total_Past_Communications']\ndata_missing_values = data3[features_missing_values]\nimputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\ndata_missing_values_array = imputer.fit_transform(data_missing_values)\ndata_missing_values = pd.DataFrame(data_missing_values_array, index=data_missing_values.index, columns=data_missing_values.columns)\ndata_missing_values\n", "intent": "- 'Total_Images'\n- 'Total_Links'\n- 'Total_Past_Communications'\n"}
{"snippet": "sales = pd.read_csv('data/kc_housing_sales_data.csv')\nsales.head()\n", "intent": "Dataset is from house sales in King County, the region where the city of Seattle, WA is located.\nRead data from csv file and put in dataframe.\n"}
{"snippet": "train_data,test_data = train_test_split(sales, test_size = 0.2)\n", "intent": "Split the data in train data and test data (80:20) ratio\n"}
{"snippet": "train_data,test_data = train_test_split(sales, test_size = 0.2)\n", "intent": "First let's split the data into training and test data.\n"}
{"snippet": "train_data,test_data = train_test_split(sales, test_size = 0.2, train_size = 0.8, random_state = 1)\n", "intent": "Let us split the dataset into training set and test set. Make sure to use `seed=0`:\n"}
{"snippet": "train_data,test_data = cross_validation.train_test_split(sales, test_size=0.2, train_size=0.8, random_state=1)\n", "intent": "Let us split the sales dataset into training and test sets.\n"}
{"snippet": "X_train, X_test = train_test_split(dataframe, test_size=1, random_state=5) \ny_train =X_train[\"y_auto_cura\"]\ny_test = X_test[\"y_auto_cura\"]\n", "intent": "Lo primero es dividir los datos de entrada en entrenamiento y test.\n"}
{"snippet": "count_month = df.groupby('month')['count'].sum()\nnew_index = ['Jan', 'Feb', 'Mar', 'Apr']\ncount_month = count_month.reindex(new_index)\ncount_month = pd.DataFrame(count_month).reset_index()\n", "intent": "**Weekends are when most of the rides are happening**\n"}
{"snippet": "df = pd.read_csv('../input/train.csv')\ndf\n", "intent": "<a id=\"24\"></a> <br>\n"}
{"snippet": "df = pd.read_csv('../input/train.csv')\ndf\n", "intent": "<a id=\"26\"></a> <br>\n"}
{"snippet": "df = pd.read_csv('../input/train.csv')\ndf = df[df['Age']==50]\ndf\n", "intent": "<a id=\"27\"></a> <br>\n"}
{"snippet": "df = pd.DataFrame(['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D'],\n                  index=['excellent', 'excellent', 'excellent', 'good', 'good', 'good', 'ok', 'ok', 'ok', 'poor', 'poor'])\ndf.rename(columns={0: 'Grades'}, inplace=True)\ndf\n", "intent": "<a id=\"28\"></a> <br>\n"}
{"snippet": "normal_sample = np.random.normal(loc=0.0, scale=1.0, size=10000)\nrandom_sample = np.random.random(size=10000)\ngamma_sample = np.random.gamma(2, size=10000)\ndf = pd.DataFrame({'normal': normal_sample, \n                   'random': random_sample, \n                   'gamma': gamma_sample})\n", "intent": "<a id=\"38\"></a> <br>\n"}
{"snippet": "np.random.seed(123)\ndf = pd.DataFrame({'A': np.random.randn(365).cumsum(0), \n                   'B': np.random.randn(365).cumsum(0) + 20,\n                   'C': np.random.randn(365).cumsum(0) - 20}, \n                  index=pd.date_range('1/1/2017', periods=365))\ndf.head()\n", "intent": "<a id=\"42\"></a> <br>\n"}
{"snippet": "concrete_data = pd.read_csv('https://ibm.box.com/shared/static/svl8tu7cmod6tizo6rk0ke4sbuhtpdfx.csv')\nconcrete_data.head()\n", "intent": "Let's download the data and read it into a <em>pandas</em> dataframe.\n"}
{"snippet": "df = pd.DataFrame({'A':[11, 33, 22],'B':[3, 3, 2]})\n", "intent": "Try to convert the following Pandas Dataframe  to a tensor\n"}
{"snippet": "df = pd.read_csv('iris.csv', skipinitialspace=True)\ndf.head()\n", "intent": "**Prepping the Model**\n"}
{"snippet": "data = pd.read_csv('data/heart.csv')\ndata.head()\n", "intent": "Read in the heart disease dataset using `pandas`.\n"}
{"snippet": "dummy_e = DummyEncoder()\ncp_dummy = dummy_e.fit_transform(cp_imp)\ncp_dummy.shape\n", "intent": "Now we can use this estimator just as the `OneHotEncoder` estimator:\n"}
{"snippet": "pipeline_cont = Pipeline([('impute', SimpleImputer(missing_values=np.nan, \n                                     strategy='mean', \n                                     copy=True)), \n                          ('norm', StandardScaler())])\noldpeak_out = pipeline_cont.fit_transform(data[['oldpeak']])\noldpeak_out.mean(), oldpeak_out.std()\n", "intent": "Now let's create a second pipeline for the continuous variables using the imputation and normalization estimators.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X, y, test_size=0.25)\nprint('XTrain shape:', X_train_raw.shape, 'YTrain shape:', y_train_raw.shape, '\\n')\nprint('XTest shape:', X_test_raw.shape, 'YTest shape:', y_test_raw.shape)\n", "intent": "Now we can use the train_test_split function to split the entire dataset into 75% `train` data and 25% `test` data:\n"}
{"snippet": "age_scaler = StandardScaler()\n", "intent": "We'll also need to create a `StandardScaler` to scale the y (age) back and forth:\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\n", "intent": "Import the MNIST handwritten digits dataset from sklearn.datasets and perform a multinomial logistic regression classifier like the one above.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris=load_iris()\ndf=pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n                     columns=iris['feature_names'] + ['target'])\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "X=pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\ny= iris['target']\n", "intent": "Define the \"x\" and \"y\" variables. *Remember*, y is our classifer, and x is our attributes.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris=load_iris()\ndf-pd.DataFrame(iris)\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "df = pd.read_csv('census_data.csv')\ndf.head()\n", "intent": "**Prepping the Data/Model**\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['over_200k', 'under_200k'],columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "df.pivot_table(index = \"Group\", aggfunc = (lambda x: len([i for i in x if i > 100])))\n", "intent": "1. Reference a column which has a certain value, then change the value of the corresponding row in a separate column\n"}
{"snippet": "af = pd.DataFrame(features)\n", "intent": "CONVERT & MERGE TO ONE DATAFRAME\n"}
{"snippet": "X = pd.DataFrame(iris.data)\ny = pd.DataFrame(iris.target)\nX.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\ny.columns = ['Targets']\n", "intent": "Define the \"x\" and \"y\" variables. *Remember*, y is our classifer, and x is our attributes.\n"}
{"snippet": "votes = pd.read_csv(votes_file, index_col=0)\nvotes.head()\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "pca1 = PCA(n_components=5)\npca1.fit(X_standard)\npca1.components_\n", "intent": "Now, conduct a PCA using scikit learn\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "x_standard = StandardScaler().fit_transform(X)\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "pca = PCA(n_components=5)\npca.fit(x_standard)\nprint pca.components_\n", "intent": "Finally, conduct the PCA - use the results above to guide your selection of n components\n"}
{"snippet": "votes = pd.read_csv(votes_file)\nvotes.head()\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "from sklearn.datasets import make_regression\nimport numpy as np\nnp.random.seed(99)\nX, y, coef = make_regression(30,10, 10, bias=1, noise=2, coef=True)\ncoef\n", "intent": "We'll create a synthetic data set using the code below. (Read the documentation for `make_regression` to see what is going on).\n"}
{"snippet": "onehot_enc = OneHotEncoder()\nX_1hot = onehot_enc.fit_transform(X_le).toarray()\nX_1hot.shape\n", "intent": "**OneHotEncoder() accepts multidimensional array, but it returns sparse matrix.  Use .toarray() to obtain just the array**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(stop_words=updated_stop_words)\n", "intent": "With updated stop words list, pass the new list to the CountVectorizer constructor:\n"}
{"snippet": "encoder = LabelBinarizer(sparse_output=True)\n", "intent": "This returns a regular/dense matrix.  To return a sparse matrix, just pass ```sparse_output=True``` to the constructor:\n"}
{"snippet": "df['minmax'] = preproc.minmax_scale(df['n_tokens_content'])\n", "intent": "Min-Max scaling squeezes (or stretches) values to be within a range of values between 0 and 1\n"}
{"snippet": "X_partno_labelencoded = enc_label.fit_transform(df_balanced.Fail_Short_PartNo.values)\n", "intent": "Label encode it first:\n"}
{"snippet": "X_partno_onehot = enc_onehot.fit_transform(X_partno_labelencoded.reshape(-1,1))\n", "intent": "Then onehot encode it:\n"}
{"snippet": "X_complaint_counts = count_vect.fit_transform(df_balanced.Customer_Contention_Text.values)\nX_complaint_counts\n", "intent": "First, CountVectorize() it:\n"}
{"snippet": "X_complaint_tfidf = tfidf_transformer.fit_transform(X_complaint_counts)\nX_complaint_tfidf.shape\n", "intent": "Then, tfidf tranform it:\n"}
{"snippet": "X_partno_labelencoded = enc_label.fit_transform(data['Part5'])\n", "intent": "Label encode it first:\n"}
{"snippet": "fig = sns.jointplot(x=0, y=1, data=pd.DataFrame(g.samples2), marker='.')\nfig.ax_joint.collections[0].set_alpha(0.1);\nfig.set_axis_labels('$X_1$', '$X_2$');\n", "intent": "As we can see, we have managed to get 10000 non-autocorrelated samples.\nWe can check the distribution of samples as follows.\n"}
{"snippet": "X_partno_onehot_categorical = enc_labelbinarizer.fit_transform(part5)\n", "intent": "Alternatively, you can use LabelBinarizer to label encode and one hot encode all in one step.\n"}
{"snippet": "X_complaint_counts = count_vect.fit_transform(data['Customer_Complaint'])\nX_complaint_counts\n", "intent": "First, CountVectorize() it:\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\n", "intent": "We will now apply the IsolationForest algorithm to spot digits written in an unconventional way.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(X)\nX = sc.transform(X)\n", "intent": "Normalize data: the unit of measurement might differ so lets normalize the data before building the model\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n", "intent": "Split data into train and test. When ever we are using radom function its advised to use a seed to ensure the reproducibility of the results.\n"}
{"snippet": "import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import datasets\nimport numpy as np\niris = datasets.load_iris()\n", "intent": "The Iris dataset comes as part of the scikit-learn dataset package which contains some of the populare datasets of machine learning literature.\n"}
{"snippet": "df = pd.read_csv('Data/Grade_Set_2.csv')\nprint df\ndf.plot(kind='scatter', x='Hours_Studied', y='Test_Grade', title='Grade vs Hours Studied')\nprint(\"Correlation Matrix: \")\ndf.corr()\n", "intent": "It is a form higher order linear regression modeled between dependent and independent variables as an nth degree polynomial.\n"}
{"snippet": "from sklearn.datasets import make_classification\nX, y = make_classification(100, 2, 2, 0, weights=[.5, .5], random_state=0) \nclf = SVC(kernel='linear', random_state=0)\nclf.fit(X, y)\n", "intent": "Let's consider a two class example to keep things simple\n"}
{"snippet": "df = pd.read_csv('Data/TS.csv')\nts = pd.Series(list(df['Sales']), index=pd.to_datetime(df['Month'],format='%Y-%m'))\n", "intent": "Let's predict sales data using ARIMA\n"}
{"snippet": "survey = pd.read_csv(\"survey.csv\")\nages = np.array(survey['age'])\nknow_lgbtq_rate = survey['knowlgbtq'] / survey['numr']\nknow_lgbtq_rate = np.array(know_lgbtq_rate)\nresp = survey['numr']\n", "intent": "**Solution**\nWe'll start by importing our count data into pandas and doing some basic EDA.\n"}
{"snippet": "def Find_Optimal_Cutoff(target, predicted):\n    fpr, tpr, threshold = metrics.roc_curve(target, predicted)\n    i = np.arange(len(tpr)) \n    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n    roc_t = roc.ix[(roc.tf-0).abs().argsort()[:1]]\n    return list(roc_t['threshold']) \n", "intent": "To simplify finding optimal probability threshold and bring in re-usability, I have made a function to find the optimal probability cutoff point.\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=3, include_bias=False)\nX2 = poly.fit_transform(X)\nX2 \n", "intent": "Let's add some synthetic features that are higher-order powers of X:\n"}
{"snippet": "hs_inv = ohe.inverse_transform(hs_train_transformed)\nhs_inv\n", "intent": "We can verify all values by inverting the entire transformed array.\n"}
{"snippet": "hs_train_transformed = ohe.fit_transform(hs_train_imputed)\nhs_train_transformed\n", "intent": "From here we can then encode as we did previously.\n"}
{"snippet": "si_step = ('si', SimpleImputer(strategy='constant', fill_value='MISSING'))\nohe_step = ('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))\nsteps = [si_step, ohe_step]\npipe = Pipeline(steps)\nhs_train = train[['HouseStyle']].copy()\nhs_train.iloc[0, 0] = np.nan\nhs_transformed = pipe.fit_transform(hs_train)\nhs_transformed.shape\n", "intent": "Each step is a two-item tuple consisting of a string that labels the step and the instantiated estimator.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nnum_si_step = ('si', SimpleImputer(strategy='median'))\nnum_ss_step = ('ss', StandardScaler())\nnum_steps = [num_si_step, num_ss_step]\nnum_pipe = Pipeline(num_steps)\nnum_transformers = [('num', num_pipe, num_cols)]\nct = ColumnTransformer(transformers=num_transformers)\nX_num_transformed = ct.fit_transform(train)\nX_num_transformed.shape\n", "intent": "Once we have our numeric column names, we can use the `ColumnTransformer` again.\n"}
{"snippet": "cols = ['density', 'residual sugar', 'total sulfur dioxide', 'fixed acidity']\nsubset_df = wines[cols]\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nscaled_df = ss.fit_transform(subset_df)\nscaled_df = pd.DataFrame(scaled_df, columns=cols)\nfinal_df = pd.concat([scaled_df, wines['wine_type']], axis=1)\nfinal_df.head()\n", "intent": "Another way of visualizing multivariate data for multiple attributes together is to use ***parallel coordinates***.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('data/bikeshare.csv')\n", "intent": "- from Kaggle ([Description](https://www.kaggle.com/c/bike-sharing-demand/data))\n"}
{"snippet": "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\n", "intent": "- Advertising dataset from **\"An Introduction to Statistical Learning\"**\n- http://www-bcf.usc.edu/~gareth/ISL/\n"}
{"snippet": "df = pd.read_csv(os.path.join('..', '..', 'dataset', 'dataset-ucla-admissions.csv'))\ndf.dropna(inplace = True)\ndef question_3(df):\n    print '%.1f (%.1f)' % (df['gpa'].mean(), df['gpa'].std())\n    print '%.1f (%.1f)' % (df['gre'].mean(), df['gre'].std())\n    for i in range(1, 5):\n        print '%s' % (df[df.prestige == i].shape[0])\nquestion_3(df[df.admit == 0])\nprint\nquestion_3(df[df.admit == 1])\n", "intent": "> \n>\n> Provide a table that explains the data by admission status.\n"}
{"snippet": "import pandas as pd\nurl = 'data/titanic.csv'\ntitanic = pd.read_csv(url)\ntitanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\ntitanic.Age.fillna(titanic.Age.median(), inplace=True)\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\nembarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\ntitanic = pd.concat([titanic, embarked_dummies], axis=1)\ntitanic.head()\n", "intent": "- Kaggle knowledge competition: https://www.kaggle.com/c/titanic\n"}
{"snippet": "from keras.datasets import mnist\nimport h5py as h5py\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint(\"The MNIST database has a training set of %d examples.\" % len(X_train))\nprint(\"The MNIST database has a test set of %d examples.\" % len(X_test))\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the MNIST database.\n"}
{"snippet": "df.to_csv('processed_bank.csv')\n", "intent": "Our data looks good, so we will now save our processed data so we have it ready for building our model in the next chapter.\n"}
{"snippet": "from sklearn import preprocessing \nle = preprocessing.LabelEncoder()\n", "intent": "Data Label Encoding\n"}
{"snippet": "file_name='https://ibm.box.com/shared/static/worxgoibhvr53yccwmwx71iuly491h6u.csv'\ndf=pd.read_csv(file_name)\ndf.head()\n", "intent": " Let's load the dataset using Pandas:\n"}
{"snippet": "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=0.33,random_state=0)\n", "intent": "We split the data into a training and testing set: \n"}
{"snippet": "Z_test=pr.fit_transform(X_test)\nZ_train=pr.fit_transform(X_train)\n", "intent": "We transform the data:\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"data.csv\")\n", "intent": "duplicate in dataset\n"}
{"snippet": "test_img = mpimg.imread('./test_images/test6.jpg')\nprint (test_img.shape[1])\n", "intent": "Applying classifer to detect cars in image\n"}
{"snippet": "columns_X = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\nX = df[columns_X]\nscaler = preprocessing.MinMaxScaler().fit(X)\nX = scaler.transform(X)\n", "intent": "- (http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n"}
{"snippet": "filtered_train_data = train_df[importance_df1.feature]\nfiltered_train_data = train_df[importance_df1.feature]\nX_train, X_dev, y_train, y_dev = train_test_split(train_df, target, test_size=0.30, random_state=42)\nX_train_filtered, X_dev_filtered, y_train_filtered, y_dev_filtered = train_test_split(filtered_train_data, target, test_size=0.30, random_state=42)\n", "intent": "Looking at the decision tree, we also do not see any of the variables that Lasso and Ridge eliminated in the important variables.\n"}
{"snippet": "new_covar = pd.DataFrame(X_transformed).cov()\nnew_covar.head().round(3)\n", "intent": "What does the covariance look like now?\n"}
{"snippet": "pca = PCA().fit(X_train)\n", "intent": "Fit PCA to the training data\n"}
{"snippet": "X_reduced = PCA(n_components=20).fit_transform(X_train)\n", "intent": "Create a new PCA object that is explicity a dimension reducer\n"}
{"snippet": "X_reduced = PCA().fit_transform(X_train)[:, :20]\n", "intent": "Transform the data using the original PCA, but keep only the first num_comp columns\n"}
{"snippet": "X = pd.DataFrame(np.random.rand(100, 3))\n", "intent": "Generate a 100x3 data frame of random numbers\n"}
{"snippet": "X = pd.DataFrame(np.random.rand(100,3))\n", "intent": "Generate a 100x3 data frame of random numbers\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(df[predictors], df[target], random_state=2)\n", "intent": "This time, let's separate X from y\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndv = DictVectorizer(sparse=False) \nX_train = pd.DataFrame(\n    dv.fit_transform(X_train.to_dict(orient='records')),\n    columns = dv.feature_names_\n    )\nX_test = pd.DataFrame(\n    dv.transform(X_test.to_dict(orient='records')),\n    columns = dv.feature_names_\n)\n", "intent": "fetaure transformation \n"}
{"snippet": "x_df = pd.DataFrame(index = df.index)\nnp.random.seed(seed = 0)\nfor i in range(100):\n    x = 'X{}'.format(i)\n    x_df[x] = np.random.random(df.shape[0])\nformula = 'SalePrice ~ 0 + IsAStudio + Beds + Baths + Size + LotSize + BuiltInYear + '\nformula += ' + '.join(x_df.columns.values)\n", "intent": "> Let's now add some artificial noise:\n"}
{"snippet": "of_df = pd.read_csv(\"../datasets/old-faithful.csv\")\nof_df.head()\n", "intent": "Ouch! No so great on this dataset. Now let's try some real data.\n"}
{"snippet": "bike = pd.read_csv(('data/Bike-Sharing-Dataset/day.csv'))\nbike['dteday'] = pd.to_datetime(bike['dteday'])\nbike = bike.drop(0)\nbike.head(4)\n", "intent": "We'll be using the same bike sharing data as last week!\n"}
{"snippet": "un_cv = CountVectorizer(max_df=.95, min_df=2, stop_words='english')\n", "intent": "**Question 3.3:** Now, implement a model using `scikit-learn`. Again, follow similar steps from the [second section](\n"}
{"snippet": "tfidf_vectorizer_1832 = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\ntfidf_1832 = tfidf_vectorizer_1832.fit_transform(transcripts_1832)\ntfidf_feature_names_1832 = tfidf_vectorizer_1832.get_feature_names()\n", "intent": "**EXERCISE:** Create the TfidfVectorizer, fit_transform the data, and get the feature names for 1832.\n"}
{"snippet": "nmf_1832 = NMF(n_components=num_topics, random_state=1, init='nndsvd').fit(tfidf_1832)\n", "intent": "**EXERCISE:** Run NMF using `num_topics` for the number of components on the data from 1832.\n"}
{"snippet": "import numpy as np\nnp.random.seed(23) \nimport keras\nfrom keras.datasets import cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n", "intent": "---\nIn this notebook, we train a CNN to classify images from the CIFAR-10 database.\n"}
{"snippet": "target_sleep = 8 * 60 \nlabels = pd.read_csv(\"output.csv\") \nlabels = labels.drop([\"Date\", \"Minutes Awake\", \"Number of Awakenings\", \"Time in Bed\"], axis=1) \nlabels[\"WELL\"] = (labels['Minutes Asleep'] >= target_sleep).astype(int)\nlabels.loc[:, (\"BAD\")] = labels[\"WELL\"] == 0           \nlabels.loc[:, (\"BAD\")] = labels[\"BAD\"].astype(int)    \nlabels = labels.drop([\"Minutes Asleep\"], axis=1) \nlabels \n", "intent": "The dataframe now only has the features. Let's introduce the labels.\n"}
{"snippet": "data = pd.read_csv('monthly-milk-production.csv',index_col = 'Month')\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "data = pd.read_csv('titanic_train.csv')\n", "intent": "Pour charger le fichier csv dans un DataFrame pandas:\n"}
{"snippet": "np.random.seed(0)\ndf = pd.DataFrame(index = range(100))\ndf['x'] = np.random.uniform(0, 1, size = df.shape[0])\ndf['Noise'] = np.random.normal(size = df.shape[0])\ndf['y'] = df.x.apply(f) * (1 + .5 * df.Noise)\n", "intent": "- (http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n"}
{"snippet": "with open('enronemail_1h.txt','rb') as f:\n    data=pd.read_csv(f, sep='\\t', header=None, na_filter=True)\ndf_columns = [\"id\", \"spamflag\", \"subject\", \"body\"]\ndata.columns = df_columns\ndata['subject_body'] = data[\"subject\"] + data[\"body\"]\ndataClean = data.dropna()\ndataClean\n", "intent": "There are some lines with \"NA\" as the body and these should be removed to properly train the model\n"}
{"snippet": "data_feat = pd.DataFrame(scaled_features, columns=data.columns[0:-1])\ndata_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "x = data_feat\ny = data['TARGET CLASS']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n", "intent": "**Use train_test_split to split your data into a training set and a testing set.**\n"}
{"snippet": "diabetes = datasets.load_diabetes()\nX = diabetes.data\ny = diabetes.target.reshape(-1,1)\n", "intent": "We will be using the well known **diabetes** dataset taken from Sklearn's built in datasets\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n", "intent": "- Encode text into frequencies of vocabulary terms\n"}
{"snippet": "style_image = scipy.misc.imread(\"images/vango.jpg\")\nimshow(style_image)\n", "intent": "For our running example, we will use the following style image: \n"}
{"snippet": "content_image = scipy.misc.imread(\"images/chunge.jpg\")\ncontent_image = reshape_and_normalize_image(content_image)\n", "intent": "Let's load, reshape, and normalize our \"content\" image (the Louvre museum picture):\n"}
{"snippet": "style_image = scipy.misc.imread(\"images/vango.jpg\")\nstyle_image = reshape_and_normalize_image(style_image)\n", "intent": "Let's load, reshape and normalize our \"style\" image (Claude Monet's painting):\n"}
{"snippet": "data = pd.read_csv('./monthly-milk-production.csv').set_index('Month')\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape \n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "df_loan = read_csv('data/loan.csv')\n", "intent": "Let's start by loading the raw data.\n"}
{"snippet": "df_loan.describe(include='all').T.join(DataFrame(df_loan.dtypes, columns=['dtype']))\n", "intent": "Now let's profile the dataset. \n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\ndf['sub_grade_enc'] = enc.fit_transform(df['sub_grade'])\nplot_scatter(df_loan, 'int_rate', 'sub_grade_enc', sample=df_loan.shape[0] / 50)\n", "intent": "Now let's encode **sub_grade** and look at its correlation with **int_rate**.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\ndf = pd.read_csv(data_path, delimiter = ',')\ndata_B = os.path.join(os.getcwd(), 'datasets', 'train_20news_partB.csv')\ndf_2 = pd.read_csv(data_B, delimiter = ',')\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "np.random.seed(186)\nn = 20\nx1 = np.random.uniform(0,10,n)\nx2 = np.random.uniform(0,10,n)\ny = 7 + 1.3 * x1 + 2.5 * x2 + np.random.normal(0,4,n)\ndata = pd.DataFrame({'x1':x1,'x2':x2, 'y':y})\ndata.head()\n", "intent": "We will now generate a dataset to illustrate how multiple regression works. \n"}
{"snippet": "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.2, random_state=1)\nX_val, X_test, y_val, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=1)\nprint(y_train.shape)\nprint(y_val.shape)\nprint(y_test.shape)\n", "intent": "We split the dataset into subsets for using in training, validation, and testing. We will use an 80/10/10 split.\n"}
{"snippet": "sX_train, sX_holdout, sy_train, sy_holdout = train_test_split(X_scaled, y, test_size=0.2, random_state=1)\nsX_val, sX_test, sy_val, sy_test = train_test_split(sX_holdout, sy_holdout, test_size=0.5, random_state=1)\nprint(sy_train.shape)\nprint(sy_val.shape)\nprint(sy_test.shape)\n", "intent": "We need to create training, validation, and testing sets for our new scaled dataset. \n"}
{"snippet": "tfidf = TfidfVectorizer()\ndtm = tfidf.fit_transform(yelp.text)\nfeatures = tfidf.get_feature_names()\n", "intent": "Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF!\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\nprint(y_train.shape)\nprint(y_test.shape)\n", "intent": "You will be asked to create a train/test split. The size of the test set will be provided. \n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'spambase_test.csv')\nspambase_test = pd.read_csv(data_path, delimiter = ',')\nspambase_test.head()\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure called spambase_test\n"}
{"snippet": "def loadDataSet(name):\n    data_path = os.path.join(os.getcwd(), 'datasets', name + '.csv')\n    return pd.read_csv(data_path, delimiter = ',')\npartA = loadDataSet('train_20news_partA')\npartB = loadDataSet('train_20news_partB')\nprint(partA.head())\npartB.head()\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "def labelEncodeColumn(name):\n    le = LabelEncoder()\n    le.fit(auto_full[name]) \n    return le.transform(auto_full_edit[name])\n", "intent": "http://scikit-learn.org/0.17/modules/generated/sklearn.preprocessing.LabelEncoder.html\n"}
{"snippet": "print \"shape of auto_full_edit before one hot encoding\"\nprint auto_full_edit.shape\nencoder = OneHotEncoder(categorical_features=booleanMask)\ntransformation = encoder.fit_transform(auto_full_edit)\nprint encoder.n_values_  \nprint encoder.feature_indices_ \nprint encoder.active_features_ \nautoFullPreprocessed = transformation.toarray()\nautoFullPreprocessed.shape\n", "intent": "http://scikit-learn.org/0.17/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n"}
{"snippet": "def loadDataSet(name):\n    data_path = os.path.join(os.getcwd(), 'datasets', name + '.csv')\n    return pd.read_csv(data_path, delimiter = ',')\npartA = loadDataSet('train_20news_partA')\npartB = loadDataSet('train_20news_partB')\nprint(partA.head())\npartB.head()\n", "intent": "<span style=\"color:red\">OK\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size = 0.8, test_size=0.2, random_state=0\n)\nmodel = LinearRegression(normalize=True)\nmodel.fit(X_train, y_train)\n", "intent": "<font color=\"red\"> Good comparion</font>\n"}
{"snippet": "import pandas as pd\nfrom copy import deepcopy\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nsample_submission = pd.read_csv(\"sample_submission.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "encoder = OneHotEncoder()\n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "vect = CountVectorizer()\n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "df = pd.read_csv(\".data/2007.csv\")\ndf\n", "intent": "First, let's read the raw data for 2007 into a Pandas dataframe.\n"}
{"snippet": "ss = StandardScaler()\n", "intent": "**Create a StandardScaler() object called scaler.**\n"}
{"snippet": "X = pd.DataFrame(X, columns = bank_data.drop('Class', axis = 1).columns)\nX.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data = pd.read_csv('College_Data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "scaled_df = pd.DataFrame(scaler.transform(df[features]), columns=features)\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "train_x, test_x, train_y, test_y = train_test_split(yelp_class['text'], yelp_class['stars'],\n                                                   test_size = 0.3, random_state = 101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "Ad_data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\nAd_data.head()\n", "intent": "An Introduction to Statistical Learning:  \nhttp://www-bcf.usc.edu/~gareth/ISL\n"}
{"snippet": "iris = load_iris()\nX= iris.data\ny=iris.target\nX_train, X_test,y_train,y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n", "intent": "b- Split\t the\t dataset\t into\t testing\t and\t training\t sets\t with\t the\t following\t parameters:\t\ntest_size=0.4,\trandom_state=1\t\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\n", "intent": "c- Split\t the\t dataset\t into\t testing\t and\t training\t sets\t with\t the\t following\t parameters:\t\ntest_size=0.2,\trandom_state=3.\t\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n", "intent": "[Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) of Decision Tree Regressor\n"}
{"snippet": "weather_df = pd.read_csv('weather.csv')\ndel weather_df[\"Unnamed: 0\"]\nweather_df.head()\n", "intent": "What weather conditions are associated with flight delays leaving NYC? Use graphics to explore.\n"}
{"snippet": "from keras.datasets import mnist\n((X_train, y_train), (X_test, y_test)) = mnist.load_data()\n", "intent": "  * see [this link](http://yann.lecun.com/exdb/mnist/)\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "  * see [this link](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "  * see [this link](https://archive.ics.uci.edu/ml/datasets/iris)\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "- Boston Housing Dataset\n"}
{"snippet": "scaled_feature = scaler.fit_transform(data.drop('Class',axis=1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "heart_data = pd.read_csv(\"heart.csv\",sep=\" \")\n", "intent": "Load the data set \"heart.csv\" [1] in a variable called `heart_data`.\n"}
{"snippet": "data_features_int_encoding = pd.DataFrame(data_features_int_encoding.tolist())\ndata_features_int_encoding.head()\n", "intent": "Next we put these feature vectors back in a Pandas DataFrame as follows:\n"}
{"snippet": "import numpy as np\nfrom pandas import read_csv\nurl = 'https://raw.githubusercontent.com/szorowi1/qmss2017/master/module3/random/random.csv'\ndata = read_csv(url)\ndata.head(3)\n", "intent": "Pandas can read tabulated data directly from URLs. \n"}
{"snippet": "diabetes_df = pd.DataFrame(diabetes.data)\n", "intent": "Sometimes it is convenient to convert a Numpy array to a Pandas DataFrame:\n"}
{"snippet": "data_norm_poly_norm = pd.DataFrame(MinMaxScaler().fit_transform(data_norm_poly),columns=data_norm_poly.columns.values)\n", "intent": "Perform the appropriate action!\n"}
{"snippet": "cols = data_features_bin.columns.values\ntmp = pd.DataFrame()\ntmp['feature'] = cols\ntmp['coefficients'] = model.coef_[0]\n", "intent": "Can you plot the model paramters of the fitted logisti regression model?\n"}
{"snippet": "X_pca_stand = pca.fit_transform(X_stand)\n", "intent": "Let's now run another PCA on our standardized data this time:\n"}
{"snippet": "Y_mds = mds.fit_transform(X)\n", "intent": "Now we run the MDS: this is a big dataset, it will take some time...\n"}
{"snippet": "yeast = pd.read_table('yeast.csv', sep = ' ', header=0)\n", "intent": "Let's see if the MDS does a better job on Yeast gene expression data.\n"}
{"snippet": "cols = data_b.columns.values\nscaler = StandardScaler()\nscaler.fit(data_b)\ndata_b_norm = pd.DataFrame(scaler.transform(data_b),columns=cols)\n", "intent": "- Standardize the data set and re-run the code.\n"}
{"snippet": "iris = datasets.load_iris()\nirisdf = pd.DataFrame(iris.data, columns=iris.feature_names)\nirisdf['target'] = iris.target\n", "intent": "* What are the features?\n* What is the target variable?\n* How many labels does the target variable have? Is it binary?\n"}
{"snippet": "pd.DataFrame(irisdf.corr()).sort_values('target', ascending=False)\n", "intent": "*  Which features have the highest correlation with the target variable? Should you start with using the highest correlated feature as a predictor? \n"}
{"snippet": "import numpy as np\nfrom pandas import read_csv\nurl = 'https://raw.githubusercontent.com/szorowi1/qmss2017/master/module3/random/random.csv'\ndata = read_csv(url)\ndata = data.set_index('y')\ndata.describe().round(3)\n", "intent": "Data coding describes processes related to preprocessing of the data, such as rescaling and recoding data for use in fitting models.\n"}
{"snippet": "json.loads('{\"title\":\"IBM Sees Holographic Calls Air Breath\"}').get('title', '')\ndata =  pd.read_csv(\"../../assets/dataset/stumbleupon.tsv\", sep=\"\\t\",encoding=\"utf-8\")\ndata['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\ndata['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\ndata.head()\n", "intent": "* Located at `\"../../assets/dataset/stumbleupon.tsv\"`\n* Encodinge is `\"utf-8\"`\n"}
{"snippet": "titles = data['title'].fillna('')\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features = 1000, \n                             ngram_range=(1, 2), \n                             stop_words='english',\n                             binary=True)\nvectorizer.fit(titles)\nX = vectorizer.transform(titles)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MaxAbsScaler\npipeline = Pipeline([\n        ('features', vectorizer),\n        ('scaling', MaxAbsScaler()),\n        ('model', model)   \n    ])\npipeline.fit(X_train, y_train)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=15)\nparameters = {'C': [0.001, 0.1, 1, 10, 100]}\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg, param_grid=parameters)\nlogreg_cv.fit(X_train, y_train)\nprint(logreg_cv.best_params_)\n", "intent": "Lasso minimized all of the coefficents down to zero except for last_trip_date making this an important predictive factor for retained users.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",header = None)\ndf.head(10)\n", "intent": "** Remarks: Collecting data phase **\n"}
{"snippet": "data_path = \"~/dsi-sf-7-materials/datasets/churn.csv\"\ndf = pd.read_csv(data_path)\n", "intent": "Perform the same cleaning and scaling in preperation for modeling buidling.\n"}
{"snippet": "x_ss = StandardScaler().fit_transform(xTrain)\nx_mas = MaxAbsScaler().fit_transform(x_ss)\n", "intent": "Bengio paper states that the data's mean should be centered aroud zero and scaled between [-1,1]\n"}
{"snippet": "pca = PCA(n_components=30)\n", "intent": "We are going to take the first 30 principal component vectors\n"}
{"snippet": "count_vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\ntermFreq = count_vectorizer.fit_transform(US_corpus)\n", "intent": "Due to the way in which LDA works, we have to vectorize our corpus into a [Bag-of-Words model](https://en.wikipedia.org/wiki/Bag-of-words_model).\n"}
{"snippet": "fwe_cols = SelectFwe(alpha=0.05).fit(data,data.index).get_support()\nfwe_cols\n", "intent": "Scikit-Learn attributes can be **chained.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nX = df.as_matrix(columns=[\"gre\", \"gpa\", \"rank\"])\ny = df.as_matrix(columns=[\"admit\"])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n", "intent": "<h1>Build the model</h1>\n<p>Create train test splits for your data.  Use the training data to .</p>\n"}
{"snippet": "datafile = 'Rpolar'\ndt = pd.read_csv('working_data{}{}.csv'.format(os.sep,datafile))\ndt = dt.iloc[:,1:] \ny = dt.iloc[:,0]\nX = dt.iloc[:,1:]\nprint(X[0:5])\nprint(y[0:5])\n", "intent": "We use pandas to read the data. Don't worry, the use of pandas here is very basic, which you can easily grasp.\n"}
{"snippet": "os.chdir(os.pardir)\ndatafile = 'Rpolar'\ndt = pd.read_csv('working_data{}{}.csv'.format(os.sep,datafile))\ndt = dt.iloc[:,1:] \ny = dt.iloc[:,0]\nX = dt.iloc[:,1:]\nprint(X[0:5])\nprint(y[0:5])\n", "intent": "We use pandas to read the data. Don't worry, the use of pandas here is very basic, which you can easily grasp.\n"}
{"snippet": "def references_organization(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == 'ORG' for word in parsed])\ndata['references_organization'] = data['title'].fillna(u'').map(references_organization)\n", "intent": "Let's see if we can find organizations in our page titles.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=25)\n", "intent": "Out of curiosity, I decided to run a train_test_split model and see the results.  They were, not surprisingly, immensely overfit and unreliable.\n"}
{"snippet": "df3 = joined_tables.dropna()\ndf3.to_csv('df3.csv')\n", "intent": "Query the database for our intial data\n"}
{"snippet": "xStand = preprocessing.StandardScaler().fit_transform(x)\n", "intent": "I then want to standardize my x variable (I chose to do this using StandardScalar):\n"}
{"snippet": "PCA_A = PCA(n_components=5)\nY = PCA_A.fit_transform(xStand)\n", "intent": "Now we can conduct our PCA analysis:\n"}
{"snippet": "Ydf = pd.DataFrame(Y, columns=[\"PC1\", \"PC2\", \"PCA3\", \"PCA4\", \"PCA5\"])\n", "intent": "Creating a dataframe from the PCA analysis will allow us to visualize our results:\n"}
{"snippet": "import os\nfrom pandas import read_table, concat\nred = read_table(os.path.join('wine','winequality-red.csv'), sep=';')\nred['color'] = 'red'\nwhite = read_table(os.path.join('wine','winequality-white.csv'), sep=';')\nwhite['color'] = 'white'\nwines = concat([red,white])\nwines.head(5)\n", "intent": "**Hint:** Remember to add a new column denoating the red and white wines. Also, the datasets are semi-colon-separated.\n"}
{"snippet": "QuasarCandidates = pd.read_csv('QuasarCandidatesData.csv')\n", "intent": "We will merge these probabilities into the QuasarCandidate.csv file.\n"}
{"snippet": "map_characters = {0: 'Interphase', 1: 'Mitosis'}\ndict_characters=map_characters\ndf = pd.DataFrame()\ndf[\"labels\"]=y_train2\nlab = df['labels']\ndist = lab.value_counts()\nsns.countplot(lab)\nprint(dict_characters)\n", "intent": "Note class size imbalance\n"}
{"snippet": "map_characters = {0: 'G1/G2/S', 1: 'Anaphase', 2: 'Metaphase',3:'Prophase',4:'Telophase'}\ndict_characters=map_characters\ndf = pd.DataFrame()\ndf[\"labels\"]=y_train\nlab = df['labels']\ndist = lab.value_counts()\nsns.countplot(lab)\nprint(dict_characters)\n", "intent": "Note class size imbalance\n"}
{"snippet": "dataset = read_csv('../input/train.csv')\ndataset=dataset[['Id','Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age','Outcome']]\ndataset.head(10)\n", "intent": "*Step 3: Inspect and Clean Data*\n"}
{"snippet": "model = DecisionTreeClassifier(max_depth=2,min_samples_leaf=2)\nX_train, X_test, y_train, y_test = train_test_split(trainingFeatures2, trainingLabels, test_size=0.2, random_state=1)\nmodel.fit(X_train, y_train)\ncolumns = trainingFeatures2.columns\nfeature_names = trainingFeatures2.columns.values\ncoefficients = model.feature_importances_.reshape(trainingFeatures2.columns.shape[0], 1)\nabsCoefficients = abs(coefficients)\nfullList = pd.concat((pd.DataFrame(columns, columns = ['Feature']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\nprint('\\nFeature Importance:\\n\\n',fullList,'\\n')\nplot_decision_tree(model,feature_names)\n", "intent": "*Step 5: Evaluate Model*\n"}
{"snippet": "list_1=[]\nfor i in range(28000):\n    i=i+1\n    list_1.append(i)\nlist_2 = [7]*28000\nkerasmnist = os.path.join('.', 'working/kerasmnist')\nos.makedirs(kerasmnist, exist_ok = True)\ndf = pd.DataFrame(data={\"ImageId\": list_1, \"Label\": list_2})\ndf = df.to_csv(\"./working/kerasmnist/mnist_dummy_submission.csv\", sep=',',index=False)\n", "intent": "Submit to a competition on Kaggle\n"}
{"snippet": "from keras.datasets import mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nkerasmnist = os.path.join('.', 'working/kerasmnist')\nos.makedirs(kerasmnist, exist_ok = True)\nnp.savez(\"working/kerasmnist/MNIST_X_train\", x_train)\nnp.savez(\"working/kerasmnist/MNIST_Y_train\", y_train)\nnp.savez(\"working/kerasmnist/MNIST_X_test\", x_test)\nnp.savez(\"working/kerasmnist/MNIST_Y_test\", y_test)\n", "intent": "Add a new file to the dataset and create a new dataset version\n"}
{"snippet": "import seaborn as sns\ndf = pd.DataFrame()\ndf[\"labels\"]=y_train\nlab = df['labels']\ndist = lab.value_counts()\nsns.countplot(lab)\nprint(dict_characters)\n", "intent": "*Step Five: Describe Augmented Dataset*\n"}
{"snippet": "train_df = pd.read_json('../input/train.json')\ntest_df = pd.read_json('../input/test.json')\ntrain=train_df\ntrain.head(15)\n", "intent": "*Step 2: Exploratory Data Analysis*\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncvec = CountVectorizer(lowercase=False)\ncfit = cvec.fit(documents)\nprint(cfit.get_feature_names())\n", "intent": "We will use **CountVectorizer** to convert these documents into their numeric representation.\n"}
{"snippet": "cols = ['Label','Latin Name', 'Common Name','Train Images', 'Validation Images']\nlabels = pd.read_csv(\"../input/10-monkey-species/monkey_labels.txt\", names=cols, skiprows=1)\nlabels\n", "intent": "*Step 2: Load Data*\n"}
{"snippet": "data = pd.read_csv('../input/train.csv')\ntestingData = pd.read_csv('../input/test.csv')\nX = data.drop(\"label\",axis=1).values\ny = data.label.values\ndef describeDataset(features,labels):\n    print(\"\\n'X' shape: %s.\"%(features.shape,))\n    print(\"\\n'y' shape: %s.\"%(labels.shape,))\n    print(\"\\nUnique elements in y: %s\"%(np.unique(y)))\ndescribeDataset(X,y)\n", "intent": "*Step 1.2: Load Data*\n"}
{"snippet": "artist_file = '../input/notorious-big.txt'\nwith open(artist_file) as f: \n    print (f.read(1000))\n", "intent": "Here are the first 1000 characters from the collection of poems by Notorious B.I.G.\n"}
{"snippet": "artist_file = '../input/Lil_Wayne.txt'\nwith open(artist_file) as f: \n    print (f.read(1000))\n", "intent": "Here are the first 1000 characters from the collection of poems by Lil Wayne\n"}
{"snippet": "def markov(text_file):\n\tread = open(text_file, \"r\", encoding='utf-8').read()\n\ttext_model = markovify.NewlineText(read)\n\treturn text_model\n", "intent": "Markov Chain (https://github.com/jsvine/markovify)\n"}
{"snippet": "def split_lyrics_file(text_file):\n\ttext = open(text_file, encoding='utf-8').read()\n\ttext = text.split(\"\\n\")\n\twhile \"\" in text:\n\t\ttext.remove(\"\")\n\treturn text\n", "intent": "Separate each line of the input txt\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\ndata = np.c_[cancer.data, cancer.target]\ncolumns = np.append(cancer.feature_names, [\"target\"])\nsizeMeasurements = pd.DataFrame(data, columns=columns)\nX = sizeMeasurements[sizeMeasurements.columns[:-1]]\ny = sizeMeasurements.target\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)\nprint('\\n Feature Names: \\n\\n', X.columns.values, \"\\n\")\n", "intent": "*Step 2: Load and Describe Data*\n"}
{"snippet": "X_trainScaled, X_testScaled, Y_trainScaled, Y_testScaled = train_test_split(xValuesScaled, yValues, test_size=0.2)\nX_trainScaledPCA, X_testScaledPCA, Y_trainScaledPCA, Y_testScaledPCA = train_test_split(xValuesScaledPCA, yValues, test_size=0.2)\n", "intent": "*Step 10: Evaluate Additional Regressors*\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\ndata = np.c_[cancer.data, cancer.target]\ncolumns = np.append(cancer.feature_names, [\"target\"])\nsizeMeasurements = pd.DataFrame(data, columns=columns)\nX = sizeMeasurements[sizeMeasurements.columns[:-1]]\ny = sizeMeasurements.target\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)\nprint('\\n Column Values: \\n\\n', sizeMeasurements.columns.values, \"\\n\")\n", "intent": "*Step 2: Load Data*\n"}
{"snippet": "from pandas import read_csv\nmetadata = read_csv(os.path.join('nsf','abstracts_metadata.csv'))\nmetadata.Directorate.value_counts()\n", "intent": "Next we will load in the metadata associated with the abstracts.\n"}
{"snippet": "hall_of_fame = pd.read_csv(\"core/HallOfFame.csv\")\nhall_of_fame = hall_of_fame[hall_of_fame.category == \"Player\"]\nprint(hall_of_fame.info())\nhall_of_fame.head()\n", "intent": "From the data, we can see that the create of the hall of fame is from **1939** year.  \n"}
{"snippet": "salary = pd.read_csv(\"./core/Salaries.csv\")\nplayer = player.join(salary.groupby([\"playerID\"])[[\"playerID\",\"salary\"]].mean(),on=\"playerID\")\nprint(salary.info())\nsalary.head()\n", "intent": "---  \nFrom the input data, we see only less than 1/4 that the salary is not null to the player.\n"}
{"snippet": "from keras.datasets import mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n", "intent": "When Determinant is less than zero we get a reflection about y axis but the deformation is same (except the reflection)\n"}
{"snippet": "import numpy as np\ndataPort = pd.read_csv('../../data/Portfolio.csv', index_col = 0)\n", "intent": "First we will read in the `Portfolio` data and import the `numpy` package as we will need some of its functions below.\n"}
{"snippet": "import numpy as np\ndataPort = pd.read_csv('../data/Portfolio.csv', index_col = 0)\n", "intent": "First we will read in the `Portfolio` data and import the `numpy` package as we will need some of its functions below.\n"}
{"snippet": "dataCarseats = pd.read_csv('../../data/Carseats.csv', index_col = 0)\ndataCarseats.dtypes\n", "intent": "First import the data and take a look at the data types.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\ndataCarseats.ShelveLoc = enc.fit_transform(dataCarseats.ShelveLoc)\ndataCarseats.Urban = enc.fit_transform(dataCarseats.Urban)\ndataCarseats.US = enc.fit_transform(dataCarseats.US)\ntrain, test = train_test_split(dataCarseats, test_size = 0.5, random_state = 1)\ntrain_x = train.drop('Sales', axis = 1)\ntest_x = test.drop('Sales', axis = 1)\ntrain_y = train['Sales']\ntest_y = test['Sales']\n", "intent": "The `object` types will need to be converted to dummy variables before splitting into test and training sets.\n"}
{"snippet": "pd.DataFrame({'feature': train_x.columns,\n              'score': rf_carseats.feature_importances_\n             }).sort_values('score', ascending = False)\n", "intent": "In this case, with $m = \\sqrt{p}$, we have a Test MSE of 3.18\n"}
{"snippet": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndataOJ = pd.read_csv('../../data/OJ.csv', index_col = 0)\ndataOJ.dtypes\n", "intent": "a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.\n"}
{"snippet": "X = tfidf_mat\ny = metadata.Directorate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\nmnb = MultinomialNB(alpha=1)\nmnb_fit = mnb.fit(X_train, y_train)\nprint('MultinomialNB train: score = %0.3f' %mnb_fit.score(X_train,y_train))\nprint('MultinomialNB test: score = %0.3f' %mnb_fit.score(X_test,y_test))\n", "intent": "Let's show the most representative tokens now.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(dataHitters, test_size = 63, random_state = 1)\ntrain_x = train.drop('Salary', axis = 1)\ntest_x = test.drop('Salary', axis = 1)\ntrain_y = train['Salary']\ntest_y = test['Salary']\n", "intent": "b) Create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations.\n"}
{"snippet": "gbr_hitters = GradientBoostingRegressor(n_estimators = 1000,\n                                        learning_rate = 0.32,\n                                        random_state = 1)\ngbr_hitters.fit(train_x, train_y)\nhitters_imp = pd.DataFrame({'feature':  train_x.columns\n                            , 'score': gbr_hitters.feature_importances_\n                            }).sort_values('score')\nhitters_imp.sort_values('score', ascending = False)\n", "intent": "f) Which variables appear to be the most important predictors in the boosted model?\n"}
{"snippet": "for col in house.columns[house.isnull().sum() > 0]:\n    if house[col].dtype == object:\n        print '--------------Before Fill--------------'\n        print house[col].value_counts()\n        house[col] = house[col].fillna('None')\n        print '--------------After Fill--------------'\n        print house[col].value_counts()\n", "intent": "Filling the missing value with 'None'\n"}
{"snippet": "house.LotFrontage = house.LotFrontage.fillna(np.mean(house.LotFrontage))\n", "intent": "With 'LotFrontage' we can fill with either the mean or the median value. \n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor i in str_col:\n    fixed[i] = le.fit_transform(fixed[i])\n", "intent": "Using LabelEncoder to convert categorical to numerical\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXs_train = ss.fit_transform(Xtr)\nXs_test = ss.fit_transform(Xte)\ny_train = house_pre2010.SalePrice\ny_test = house_2010.SalePrice\n", "intent": "Standardising with StandardScaler\n"}
{"snippet": "for i in str_cols:\n    renovate[i] = le.fit_transform(renovate[i])\nrenovate.dtypes\n", "intent": "Using LabelEncoder to convert categorical to numerical\n"}
{"snippet": "sal_X = df.iloc[:,0:len(df.columns)-1]\nsal_y = pd.DataFrame(df[\"above_med_sal\"])\n", "intent": "Creating the train test split with the salary data.\n"}
{"snippet": "from sklearn.datasets import load_iris, load_breast_cancer\nml.plots.plot_tree(load_breast_cancer(), class_names=[\"malignant\", \"benign\"])\n", "intent": "Analyzing Decision Trees manually\n- Visualize and find the path that most data takes\n"}
{"snippet": "from pandas import DataFrame\ncolnames = ['Topic%0.2d' %n for n in range(n_topics)]\nloadings = DataFrame(document_loadings, columns=colnames)\ninfo = metadata.merge(loadings, left_index=True, right_index=True)\ninfo = info.melt(id_vars=['Year','Funds'], value_vars=colnames, \n                 var_name='Topic', value_name='Loading')\ngb = info.groupby(['Year','Topic'])\nwa = gb.apply(lambda x: np.average(x.Funds, weights=x.Loading))\nwa = wa.unstack().reset_index().melt(id_vars='Year', value_name='WA')\nwa.head(5)\n", "intent": "If desired, we can use **groupby** to further separate this weighted average for each year between 2006 and 2016.\n"}
{"snippet": "d = {'Sent': all_sents, 'Contains_claims': labels}\nsentDF = pd.DataFrame(d)\n", "intent": "Major problem: when there suppose to be 2294 claims, I'm only able to find 1702 sentences that contain claims using this method.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npivot['x'] = pca.fit_transform(pivot[x_cols])[:,0]\npivot['y'] = pca.fit_transform(pivot[x_cols])[:,1]\npca_df = pivot[['customer_name','cluster','x','y']]\npca_df.head(3)\n", "intent": "There is seperation between clusters. It can be better. Each cluster has its own grouping.\nThe best value for K seems to be 3.\n"}
{"snippet": "users_df = pd.read_csv('takehome_users.csv')\nprint(users_df.shape)\nusers_df.head()\n", "intent": "**Note:** 'takehome_users.csv' was re-encoded to utf-8 to allow proper opening.\n"}
{"snippet": "df = pd.read_csv('adult.csv')\n", "intent": "Let's start by reading in the adult.csv file into a pandas dataframe.\n"}
{"snippet": "df = pd.read_csv('breast-cancer-wisconsin.data.txt', \n                 names=['id','clump_thickness','uni_cellsize','uni_cellshape',\n                        'adhesion','epi_cellsize','nuclei','chromatin','nucleoli','mitoses',\n                       'class'])\n", "intent": "Let's start by reading in the adult.csv file into a pandas dataframe.\n"}
{"snippet": "data = pd.DataFrame(scaled_feat,columns= df.columns[:-1] )\n", "intent": "Converting the 'scaled_feat' to a dataframe\n"}
{"snippet": "df = pd.read_csv('iris-dataset.csv', names=['col1','col2','col3','col4','type'])\n", "intent": "Let's start by reading in the adult.csv file into a pandas dataframe.\n"}
{"snippet": "df = pd.read_csv('mammographic_masses.data', names=['bi-rads', 'age', 'shape', 'margin', 'density', 'severity'])\n", "intent": "Let's start by reading in the adult.csv file into a pandas dataframe.\n"}
{"snippet": "df = pd.read_csv('Prostate_Cancer.csv')\n", "intent": "Let's start by reading in the adult.csv file into a pandas dataframe. Cleaning the data, if required.\n"}
{"snippet": "from sklearn import preprocessing as prep\ngre_norm=pd.DataFrame(prep.normalize(dat_trans[\"gre\"]))\ngpa_norm=pd.DataFrame(prep.normalize(dat_trans[\"gpa\"]))\n", "intent": "But that doesn't look much better. Maybe that's because I'm doing something wrong. But he, let's try something from the SciKit!\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_glass, df['Type'], test_size=0.33, random_state=101)\n", "intent": "Creating the Training & Test Data from the Data Set\n"}
{"snippet": "coefficients = pd.DataFrame(lm.coef_, index=df.columns[:-1], columns=['Coefficients'])\n", "intent": "** Coefficients of PE **\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(df.drop(['Yearly Amount Spent','Email','Address','Avatar'], axis=1), df['Yearly Amount Spent'], \n                                                   test_size=0.3, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "df2012_13 = pd.read_csv(os.getenv('FDS')+'LoanStats_2012_to_2013.csv',low_memory=False,skiprows=1)\ndf2014 = pd.read_csv(os.getenv('FDS')+'LoanStats_2014.csv',low_memory=False,skiprows=1)\n", "intent": "<p>\n<span style=\"color:blue\">\n> Importing data of Lending club for the years 2012-14\n</span>\n</p>\n"}
{"snippet": "y_pred = pd.DataFrame(predictions)\n", "intent": "** Each item in your list will look like this: **\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"/Users/sofia/Projects/workspace/courses/udemy/data-science-and-machine-learning-with-python/data/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "columns_nunique = pd.DataFrame(raw_data_allyears_noEPA.nunique(),columns = ['n'])\n", "intent": "* **Finding the number of unique values in every columns:**\n"}
{"snippet": "columns_nunique_n = pd.DataFrame(raw_data_allyears_noEPA.nunique(),columns = ['n'])\n", "intent": "* ** Removing constant valued columns (no. of unique values = 1):**\n"}
{"snippet": "df_sparsity = pd.DataFrame(raw_data_allyears_noEPA.isnull().sum()/raw_data_allyears_noEPA.shape[0]\n                           ,columns=['sparsity'])\n", "intent": "* **Checking feature data sparsity:**\n"}
{"snippet": "admitraw = pd.read_csv(\"assets/admissions.csv\")\naddata1 = admitraw.dropna() \nprint addata1.head()\n", "intent": "<em>but this time I'm going to include the intercept in my cartesian graph</em></font>\n"}
{"snippet": "param_grid_1 = {'logisticregression__C': np.logspace(-3, 3, 10),\n                'logisticregression__class_weight': ['balanced',None]}\ngrid_logit_1 = GridSearchCV(make_pipeline(CountVectorizer(),\n                                          LogisticRegression(),\n                                          memory = \"cache_folder\"),\n                          param_grid= param_grid_1,\n                            scoring=ROC_AUC_score, cv=5)\ngrid_logit_1.fit(X_train_title,y_train_1)\n", "intent": "**Using Gridsearch**:\n"}
{"snippet": "feature_names_12 = review_vect.get_feature_names()\ncoef_12 = logit_12.coef_[0]\ntop_20_feat = (np.absolute(coef_12).argsort())[-20:]\ntop20_feat_names = np.array(feature_names_12)[top_20_feat]\ntop20_coef = coef_12[top_20_feat]\ntop20_feat_df = pd.DataFrame(list(zip(top20_feat_names,top20_coef)),\n                             columns = ['Feature','Coefficient'])\ntop20_feat_df = top20_feat_df.sort_values(by='Coefficient',\n                                          ascending = False)\nprint(top20_feat_df)\n", "intent": "**Visualizing coefficients**\n"}
{"snippet": "param_grid_12 = {'logisticregression__C': np.logspace(-3, 3, 10),\n                 'logisticregression__class_weight': ['balanced',None]}\ngrid_logit_12 = GridSearchCV(make_pipeline(CountVectorizer(),\n                                           LogisticRegression(),\n                                           memory = \"cache_folder\"),\n                          param_grid= param_grid_12,\n                             scoring=ROC_AUC_score, cv=5)\ngrid_logit_12.fit(non_review_train['Review'].values,y_train_12)\n", "intent": "**Tuning the parameters**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ndef get_bag_of_words(docs):\n    vectorizer = CountVectorizer()\n    bag_of_words = vectorizer.fit_transform(docs)\n    return bag_of_words.toarray()\nproject_tests.test_get_bag_of_words(get_bag_of_words)\n", "intent": "Let's extract some features from the text data using bag of words. Implement the `get_bag_of_words` using sklearn's `CountVectorizer` function.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ndef word_freq(docs):\n    return TfidfVectorizer().fit_transform(docs).toarray()\nword_freq_ten_ks = {}\nfor ticker, ten_ks in ten_ks_by_ticker.items():\n    word_freq_ten_ks[ticker] = {\n        'item1a': word_freq([' '.join(ten_k['lemma_item1a']) for ten_k in ten_ks]),\n        'item7': word_freq([' '.join(ten_k['lemma_item7']) for ten_k in ten_ks]),\n        'item7a': word_freq([' '.join(ten_k['lemma_item7a']) for ten_k in ten_ks])}\nproject_helper.print_ten_k_data([word_freq_ten_ks[example_ticker]], ['item1a', 'item7', 'item7a'])\n", "intent": "Just like we did with the bag of words, lets create TFIDF values.\n"}
{"snippet": "import pandas as pd\nteam_df = pd.read_csv(\"../Data/teaminfo.csv\")\nteam_df.tail(10)\n", "intent": "<u>Load the team data.</u>\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n", "intent": "- Produce training and testing sets. The framework comes with a simple utility: ```train_test_split.```\n"}
{"snippet": "errs = pd.DataFrame()\nerrs['e']=cmp['w']-cmp['pred_w']\nerrs['e']=errs['e'].abs()\navg=errs['e'].mean()\nstd=errs['e'].std()\nprint(\"Avg error = \", avg)\nprint(\"Std Deviation = \", std)\n", "intent": "- That is rather odd looking. How accurate were we?\n"}
{"snippet": "X, y, attribute_names = dataset.get_data(\n    target=dataset.default_target_attribute, \n    return_attribute_names=True)\neeg = pd.DataFrame(X, columns=attribute_names)\neeg['class'] = y\nprint(eeg[:10])\n", "intent": "Get the actual data.  \nReturned as numpy array, with meta-info (e.g. target feature, feature names,...)\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33)\n", "intent": "[sklearn train_test_split documentation](http://scikit-learn.org/stable/modules/cross_validation.html\n"}
{"snippet": "from sklearn.datasets import load_iris\niris_dataset = load_iris()\nprint(\"Keys of iris_dataset: {}\".format(iris_dataset.keys()))\nprint(iris_dataset['DESCR'][:193] + \"\\n...\")\n", "intent": "Iris is included in scikitlearn, we can just load it.  \nThis will return a `Bunch` object (similar to a `dict`)\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    iris_dataset['data'], iris_dataset['target'], \n    random_state=0)\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"y_train shape: {}\".format(y_train.shape))\nprint(\"X_test shape: {}\".format(X_test.shape))\nprint(\"y_test shape: {}\".format(y_test.shape))\n", "intent": "To evaluate our classifier, we need to test it on unseen data.  \n`train_test_split`: splits data randomly in 75% training and 25% test data.\n"}
{"snippet": "iris_df = pd.DataFrame(X_train, \n                       columns=iris_dataset.feature_names)\nsm = pd.scatter_matrix(iris_df, c=y_train, figsize=(10, 10), \n                  marker='o', hist_kwds={'bins': 20}, s=60, \n                  alpha=.8, cmap=mglearn.cm3)\n", "intent": "We can use a library called `pandas` to easily visualize our data. Note how several features allow to cleanly split the classes.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nX, y = mglearn.datasets.make_forge()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nclf = KNeighborsClassifier(n_neighbors=3)\nclf.fit(X_train, y_train)\n", "intent": "Let's build a kNN model for this dataset (called 'Forge')\n"}
{"snippet": "from sklearn.neighbors import KNeighborsRegressor\nX, y = mglearn.datasets.make_wave(n_samples=40)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nreg = KNeighborsRegressor(n_neighbors=3)\nreg.fit(X_train, y_train)\n", "intent": "To do regression, simply use `KNeighborsRegressor` instead\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nX, y = mglearn.datasets.load_extended_boston()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nlr = Ridge().fit(X_train, y_train)\n", "intent": "Linear regression can be found in `sklearn.linear_model`. We'll evaluate it on the Boston Housing dataset.\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\nlogreg = LogisticRegression().fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n", "intent": "Model selection: Logistic regression\n"}
{"snippet": "import pandas as pd\nresults = pd.DataFrame(grid_search.cv_results_)\ndisplay(results.head())\n", "intent": "We can retrieve and visualize the cross-validation resulst to better understand the impact of hyperparameters\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n", "intent": "Effect of pre-pruning: default tree overfits, setting `max_depth=4` is better\n"}
{"snippet": "sac = pd.read_csv('../assets/datasets/Sacramentorealestatetransactions.csv')\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n                                                    random_state=1)\n", "intent": "- Lets apply a data transformation _manually_, then use it to train a learning algorithm\n- First, split the data in training and test set\n"}
{"snippet": "from sklearn.svm import SVC\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n                                                    random_state=0)\nsvm = SVC()\nsvm.fit(X_train, y_train)\nprint(\"Test set accuracy: {:.2f}\".format(svm.score(X_test, y_test)))\n", "intent": "* First, we train the SVM without scaling\n"}
{"snippet": "scaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nsvm.fit(X_train_scaled, y_train)\nprint(\"Scaled test set accuracy: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\n", "intent": "* With scaling, we get a much better model\n"}
{"snippet": "from sklearn.feature_selection import SelectPercentile, f_regression\nselect = SelectPercentile(score_func=f_regression, percentile=5).fit(X, y)\nX_selected = select.transform(X)\nprint(\"X_selected.shape: {}\".format(X_selected.shape))\n", "intent": "* First, we select the 5% most informative features with `SelectPercentile`, and then evaluate a `Ridge` regressor\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best params:\\n{}\\n\".format(grid.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\n", "intent": "The Scaling+SVM pipeline wins!\n"}
{"snippet": "import os\ndata = pd.read_csv(\n    os.path.join(mglearn.datasets.DATA_FOLDER, \"adult.data\"), header=None, index_col=False,\n    names=['age', 'workclass', 'fnlwgt', 'education',  'education-num',\n           'marital-status', 'occupation', 'relationship', 'race', 'gender',\n           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n           'income'])\ndata = data[['age', 'workclass', 'education', 'gender', 'hours-per-week',\n             'occupation', 'income']]\ndisplay(data.head())\n", "intent": "Convert a feature with c categories to $c$ dummy variables:  \n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nprint(\"Test score: {:.2f}\".format(logreg.score(X_test, y_test)))\n", "intent": "Now the data is represented in a way that scikit-learn can work with, and we can\nproceed as usual:\n"}
{"snippet": "from pandas import read_csv\nfrom datetime import datetime\ndataset = read_csv('PRSA_data_2010.1.1-2014.12.31.csv')\ndataset\n", "intent": "The data is not ready to use. We must prepare it first.\nBelow are the first few rows of the raw dataset.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nvar_mod = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']\nle = LabelEncoder()\nfor i in var_mod:\n    df[i] = le.fit_transform(df[i])\ndf.dtypes \n", "intent": "In python, use Skicit-Learn to create a predictive model \nFirst, we have to convert all categorical models into numeric variables\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, Y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['over_200k', 'under_200k'],\n                         columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "with urllib.request.urlopen(\"http://goo.gl/j0Rvxq\") as url:\n    s = url.read()\n", "intent": "Object that get data from an URL\n"}
{"snippet": "dataset = datasets.load_iris()\n", "intent": "We'll import Iris dataset and put inside the object called dataset \n"}
{"snippet": "credit = pd.read_csv('https://raw.githubusercontent.com/fclesio/learning-space/master/Datasets/02%20-%20Classification/default_credit_card.csv')\n", "intent": "Now we'll import a structured dataset that all columns are numeric.\n"}
{"snippet": "algorithms = {'Algorithm': ['Random Forests', 'Gradient Boosting', 'Extra Trees', 'Ada Boosting', 'SVM', 'KNN', 'Decision Trees', 'Perceptron', 'Logistic Regression'],\n        'MSE': [round(mse_rfc,4), round(mse_gbc,4), round(mse_etc,4), round(mse_abc,4), round(mse_svc,4), round(mse_knc,4), round(mse_dtc,4), round(mse_ptc,4), round(mse_lrc,4)]}\nalgos = pd.DataFrame(algorithms)\nalgos.sort_values(by='MSE', ascending=1)\n", "intent": "Ok, let's ranking our algorithms to see the best one to start our analysis. \n"}
{"snippet": "from sklearn import cross_validation\nX_1, X_2, Y_1, Y_2 = cross_validation.train_test_split(\n    target_features, outcome_feature, test_size=0.5, random_state=0)\n", "intent": "Generate training and test set\n"}
{"snippet": "data = pd.read_csv('./data/MNIST/train.csv')\ndata = np.array(data)\n", "intent": "* http://www.fast.ai/, Fast.ai ML, Fast.ai DL\n* http://ruder.io/optimizing-gradient-descent/\n* http://cs231n.github.io/neural-networks-3/\n"}
{"snippet": "df_submission = pd.DataFrame({'id': id_test, 'price_doc': y_pred.astype('int64')})\n", "intent": "You need to create your submission when you want to test your model using the test data. These next cells creates a file with the submission format.\n"}
{"snippet": "from linear_models import load_data\ndataframe = load_data()\nprint dataframe.iloc[0]\n", "intent": "The dataset is about ridership of NYC subways. Here is a typical record in the dataset.\n"}
{"snippet": "train = pd.read_csv(\"train.csv\")\ntest  = pd.read_csv(\"test.csv\")\nfull = train.append( test , ignore_index = True )\ntitanic = full[ :891 ]\ndel train , test\nprint ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)\n", "intent": "Now that our packages are loaded, let's read in and take a peek at the data.\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, Y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['has_cancer', 'is_healthy'],\n                         columns=['predicted_cancer','predicted_healthy'])\nprint(confusion)\n", "intent": "Let's say again that we are predicting cancer based on some kind of detection measure, as before.\n"}
{"snippet": "X = imputer.transform(housing_num)\ntype(X)\nhousing_tr = pd.DataFrame(X,columns=housing_num.columns)\nhousing_tr.head()\n", "intent": "Transforming Training Set\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\nlabelEnc=LabelEncoder()\ncat_vars=['Embarked','Sex',\"Title\",\"FsizeD\",\"NlengthD\",'Deck']\nfor col in cat_vars:\n    titanic[col]=labelEnc.fit_transform(titanic[col])\n    titanic_test[col]=labelEnc.fit_transform(titanic_test[col])\n", "intent": "    Using Label Encoder one at a time as Categorical Encoder is not available\n"}
{"snippet": "from sklearn import preprocessing\nstd_scale = preprocessing.StandardScaler().fit(titanic[['Age', 'Fare']])\ntitanic[['Age', 'Fare']] = std_scale.transform(titanic[['Age', 'Fare']])\nstd_scale = preprocessing.StandardScaler().fit(titanic_test[['Age', 'Fare']])\ntitanic_test[['Age', 'Fare']] = std_scale.transform(titanic_test[['Age', 'Fare']])\n", "intent": "    Scaling Age & Fare\n"}
{"snippet": "dataset =  load_data(path=TRAINING_DATA)\ntest_set = load_data(path=TEST_DATA)\n", "intent": "Since we already have different Training & Test sets, we don't neeed to create any splits. Hence, directly loading the data\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\ndata = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  \nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n", "intent": "You have already seen the code to load the soccer/football data:\n"}
{"snippet": "X_train, y_train, X_test, y_test = load_data('../input/sp500.csv', 50, True)\n", "intent": "The code cell below loads your training and test data. Do a quick overview of the data to make sure you understand it.\n"}
{"snippet": "movies_path = os.path.join(input_dir, 'movie.csv')\nmovies_df = pd.read_csv(movies_path, index_col=0)\nmovies_df.head()\n", "intent": "What movie is this the embedding of? Let's load up our dataframe of movie metadata.\n"}
{"snippet": "import pandas as pd\nmelbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'\nmelbourne_data = pd.read_csv(melbourne_file_path) \nfiltered_melbourne_data = melbourne_data.dropna(axis=0)\ny = filtered_melbourne_data.Price\nmelbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n                        'YearBuilt', 'Lattitude', 'Longtitude']\nX = filtered_melbourne_data[melbourne_features]\nfrom sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\n", "intent": "The data is loaded into **train_X**, **val_X**, **train_y** and **val_y** using the code you've already seen (and which you've already written).\n"}
{"snippet": "X_new = pd.DataFrame({'TV': [50]})\nX_new.head()\nX_new2 = pd.DataFrame({'Radio': [50]})\nX_new2.head()\n", "intent": "Thus, we would predict Sales of **9,409 widgets** in that market.\nOf course, we can also use Statsmodels to make the prediction:\n"}
{"snippet": "conmat_10 = np.array(confusion_matrix(Y_test, Y_pp.pred_class_thresh10.values, labels=[1,0]))\nconfusion_10 = pd.DataFrame(conmat_10, index=['has_cancer', 'is_healthy'],\n                            columns=['predicted_cancer','predicted_healthy'])\nprint(confusion_10)\n", "intent": "This will reduce our false negative rate to 0, but at the expense of our false positive rate.\n"}
{"snippet": "train_whole = pd.read_csv(\"train_clean_csv.csv\", index_col=0)\ntarget_whole = train_whole['SalePrice']\ntrain = train_whole.iloc[:1000,:]\ntest = train_whole.iloc[1000:,:]\ntrain_target = train[\"SalePrice\"]\ntest_target = test[\"SalePrice\"]\n", "intent": "Load the data and split it into a train and test set.\n"}
{"snippet": "features = cont_feat + ord_feat + dum_feat\nfeature_importances = R_2.feature_importances_\nfeatures_df = pd.DataFrame({'Features': features, 'Importance Score': feature_importances})\nfeatures_df.sort('Importance Score', inplace=True, ascending=False)\nfeatures_df.head(), features_df.tail()\n", "intent": "A random forest with all features performs well.\n"}
{"snippet": "Final = pd.DataFrame()\nfor f in os.listdir(datapath):\n    filepath = os.path.join(datapath,f)\n    if filepath.endswith('.csv'):\n        Res = make_inputs(filepath)\n        Final = Final.append(Res)\n", "intent": "I'll iterate over each file, run the above and concat to a final df. Then we'll pivot\n"}
{"snippet": "result_last.to_csv('12-10-3.csv',index=False)\n", "intent": "df2 = pd.read_csv('test_modified_12-10.csv')\naa=solo(X, y,df2)\nresult_last = pd.DataFrame({'Id':df2['Id'],'SalePrice':aa})\n"}
{"snippet": "from keras.models import model_from_json\nwith open(\"./models/c13/simple_nn.json\", 'r') as json_file:\n    loaded_model_json = json_file.read()\nloaded_model = model_from_json(loaded_model_json)\nloaded_model.load_weights(\"./models/c13/simple_nn.h5\")\nloaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n", "intent": "Load model structure from simple_nn.json, and load weights from simple_nn.h5\n"}
{"snippet": "from keras.preprocessing import sequence\nmax_words = 500\nX_train = sequence.pad_sequences(X_train, maxlen=max_words)\nX_val = sequence.pad_sequences(X_val, maxlen=max_words)\nprint len(X_train[0])\n", "intent": "Bound the length of word sequence to 500, truncating longer reviews and zero-padding shorter reviews.\n"}
{"snippet": "movie_names = pd.read_csv(path+'movies.csv').set_index('movieId')['title'].to_dict\n", "intent": "Just for display purposes, let's read in the movie names too.\n"}
{"snippet": "train = pd.DataFrame(utils2.load_array(data_path+'train/train_features.bc'),columns=['TRIP_ID', 'CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID',\n       'TIMESTAMP', 'DAY_TYPE', 'MISSING_DATA', 'POLYLINE', 'LATITUDE', 'LONGITUDE', 'TARGET', 'COORD_FEATURES', 'DAY_OF_WEEK',\n                            'QUARTER_HOUR', 'WEEK_OF_YEAR'])\n", "intent": "Meanshift clustering as performed in the paper\n"}
{"snippet": "train = pd.DataFrame(utils2.load_array(data_path+'train/train_features.bc'),columns=['TRIP_ID', 'CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID',\n       'TIMESTAMP', 'DAY_TYPE', 'MISSING_DATA', 'POLYLINE', 'LATITUDE', 'LONGITUDE', 'TARGET',\n                            'COORD_FEATURES', \"DAY_OF_WEEK\", \"QUARTER_HOUR\", \"WEEK_OF_YEAR\"])\n", "intent": "Load training data and cluster centers\n"}
{"snippet": "metrics_pct = np.array(bcw.metrics_pct.values)\nmetrics_pct = metrics_pct[:, np.newaxis]\nX_train, X_test, Y_train, Y_test = train_test_split(metrics_pct, bcw[['class']].values, \n                                                    test_size=0.33, stratify=bcw[['class']].values,\n                                                    random_state=77)  \n", "intent": "Split into 66% training set and 33% testing set\n>```\n>X = metrics_pct (predictor)\n>Y = class (non-cancer:0 vs cancer:1)\n>```\n"}
{"snippet": "import pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"bank_note_data.csv\")\n", "intent": "** Use pandas to read in the bank_note_data.csv file **\n"}
{"snippet": "scaledDf = pd.DataFrame(X_train, columns=df.columns[:-1])\nscaledDf.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "** Use SciKit Learn to create training and testing sets of the data as we've done in previous lectures:**\n"}
{"snippet": "import cufflinks as cf\ndv = pd.DataFrame(error_rate, index=range(1, 100), columns=['Error rate'])\ndv.head()\ndv.iplot()\n", "intent": "**Now create the following plot using the information from your for loop.**\n"}
{"snippet": "X = CountVectorizer().fit_transform(X)\nprint(X.shape)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = features\ny = new_df.Danger\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=0.2,random_state=42)\n", "intent": "Created different models to test for performance and started with a Logistic Regression model.\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\none_hot_y = enc.fit_transform(y.reshape(-1, 1)).todense()\n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "with open(\"classes.txt\", \"r\") as f:\n    classes = f.read().splitlines()\nprint(classes[9])\n", "intent": "Now we should put the weights into their places:\n"}
{"snippet": "from Bio import Phylo\ntree = Phylo.read('../data/tree.nwk', 'newick')\ndistances = []\nfor node in tree.get_terminals():\n    distances.append(tree.distance(tree.root, node))\nsum(distances)/float(len(distances))\n", "intent": "Can you compute the average root-to-tip distance in the `data/tree.nwk` file?\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconmat = np.array(confusion_matrix(Y_test, Y_pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['has_cancer', 'is_healthy'],\n                         columns=['predicted_cancer','predicted_healthy'])\nprint(confusion)\n", "intent": "Look at the confusion matrix\n"}
{"snippet": "df2 = pd.DataFrame(scaled_features, columns=df.columns[:-1])\ndf2.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "college = pd.read_csv(\"College_Data\",index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "yelp = pd.read_csv(\"yelp.csv\")\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nx = np.array([1, 2, 3, 4])\npoly = PolynomialFeatures(3, include_bias=False)\npoly.fit_transform(x[:, None])\n", "intent": "**Polynomial basis functions**\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "**Model validation the wrong way**\n"}
{"snippet": "X = iris.data\npca = PCA( n_components = 2, whiten = True ).fit( X )\nX_pca = pca.transform( X )\nX_pca\n", "intent": "Our data has 3 different species, 150 samples and 4 features(dimenesions)\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer(lowercase = True, token_pattern ='(?u)\\\\b\\\\w\\\\w+\\\\b')\n", "intent": ">>**Instructions:**\nImport the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. \n"}
{"snippet": "df[\"avg_rating_by_driver\"]=df[\"avg_rating_by_driver\"].fillna(-1) \ndf[\"avg_rating_of_driver\"]=df[\"avg_rating_of_driver\"].fillna(-1) \nbins = [-2, 0, 1.1, 2.1,  3.1, 4.1 ,5.1]\ngroup_names = ['None', '0-1', '1-2', '2-3', '3-4', '4-5']\ndf['avg_rating_by_driver_grouped'] = pd.cut(df[\"avg_rating_by_driver\"], bins, labels=group_names)\ndf['avg_rating_of_driver_grouped'] = pd.cut(df[\"avg_rating_of_driver\"], bins, labels=group_names)\n", "intent": "For both the ratings we will bin them into categories None ,0-1, 1-2, 2-3, 3-4, 4-5, where 0 would be the category with missing ratings.\n"}
{"snippet": "df['phone']=df['phone'].fillna('Missing')\n", "intent": "Create a category \"Missing\" for those users with missing entries for \"phone\".\n"}
{"snippet": "lr_cm = confusion_matrix(y_test, lr_ypred, labels=lr.classes_)\nlr_cm = pd.DataFrame(lr_cm, columns=lr.classes_, index=lr.classes_)\nlr_cm\n", "intent": "[Confusion Matrix](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nencoder.fit(y.reshape(len(y),-1))\ny_encoded = encoder.transform(y.reshape(len(y),-1))\nX_mnist = X\ny_mnist = y_encoded\n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "sc = StandardScaler()\npc = PCA(n_components=2)\nclf = CLF\n", "intent": "There is no deskew class in `sklearn` so we can not put this in a `Pipeline`.\n"}
{"snippet": "c = list(Counter([tuple(set(x)) for x in battles.dropna(subset = [\"attacker_king\", \"defender_king\"])[[\"attacker_king\", \"defender_king\"]].values if len(set(x)) > 1]).items())\np = pd.DataFrame(c).plot.barh(figsize = (10, 6))\n_ = p.set(yticklabels = [\"%s vs. %s\" % (x[0], x[1]) for x in list(zip(*c))[0]], xlabel = \"No. of Battles\"), p.legend(\"\")\n", "intent": "Which pairs fought the most battles?\n"}
{"snippet": "pca = PCA(2)  \nprojected = pca.fit_transform(digits.data)\nprint(digits.data.shape)\nprint(projected.shape)\n", "intent": "Reduce the dimensionality.\n"}
{"snippet": "pca_1 = PCA(n_components=3)\n", "intent": "Basically almost all of our data is captured by the first principal component. \n"}
{"snippet": "pca_1 = PCA(n_components=1)\n", "intent": "Basically almost all of our data is captured by the first principal component. \n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "Instantiate the scaler.\n"}
{"snippet": "customer_sc_df = pd.DataFrame(customer_sc, columns=customer_features.columns)\n", "intent": "Load it into a dataframe.\n"}
{"snippet": "stats = pd.DataFrame()\nstats['mean'] = categorical_encoded_df[ms_sub_class_encoded_cols].mean()\nstats['std'] = categorical_encoded_df[ms_sub_class_encoded_cols].std()\nstats['var'] = categorical_encoded_df[ms_sub_class_encoded_cols].var()\nstats.sort_values('std', ascending=False)\n", "intent": "Next, let's look at mean and standard deviation of the filtered dataframe.\n"}
{"snippet": "data = pd.read_csv('../../assets/datasets/train.tsv', sep='\\t', na_values='?')\ndata['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', '')).fillna('')\ndata['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', '')).fillna('')\n", "intent": "- These are websites that always relevant like recipies or reviews (as opposed to current events)\n- Look at some examples\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "- Iris: Classify flower species based on measurements\n- Handwritten Digits: Is the digit a 0, 1, 2, 3, ... ?\n"}
{"snippet": "import pandas as pd\nsongs = pd.read_csv('data/songs.csv')\n", "intent": "Use kmeans to cluster the following song data.  Discuss the meaning of the clusters. \n"}
{"snippet": "titanic_train = pd.read_csv('data/titanic_train.csv', index_col=0)\n", "intent": "- Describe Titanic Task\n"}
{"snippet": "nhl = pd.read_csv('data/NHL_Data_GA.csv')\n", "intent": "**Load the Data**\nLocated in data folder, named as follows:\n```\ndata/NHL_Data_GA.csv\n```\n"}
{"snippet": "PATH_TO_DATA = ('../../data')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "tfidf_merged_obj = TfidfVectorizer(max_features=80000, ngram_range=(1,5), sublinear_tf=True)\ntfidf_merged_obj.fit(full_sites_merged['sites_visited'])\n", "intent": "Build Tf-Idf features based on sites. You can use `ngram_range`=(1, 3) and `max_features`=100000 or more\n"}
{"snippet": "full_new_feat = pd.DataFrame(index=full_df.index)\nfull_new_feat['start_month'] = full_df['time1'].apply(lambda ts: 100 * ts.year + ts.month)\n", "intent": "Add features based on the session start time: hour, whether it's morning, day or night and so on.\n"}
{"snippet": "strt_mnth_sc = StandardScaler().fit_transform(full_new_feat[['start_month']])\nX_train_strt_mnth_s = csr_matrix(hstack([X_train, strt_mnth_sc[:idx_split,:]]))\nprint(get_auc_lr_valid(X_train_strt_mnth_s, y_train))\n", "intent": "Scale these features and combine them with Tf-Idf based on sites (you'll need `scipy.sparse.hstack`)\n"}
{"snippet": "strt_mnth_hr_isday_s = StandardScaler().fit_transform(full_new_feat[['isdaytime']])\nX_train_strt_hour_isday_morn_s = csr_matrix(hstack([X_train, strt_mnth_hr1[:idx_split,:],strt_mnth_hr_isday_s[:idx_split,:], strt_mnth_mrng[:idx_split,:]]))\nprint(get_auc_lr_valid(X_train_strt_hour_isday_morn_s, y_train))\n", "intent": "Perform cross-validation with logistic regression.\n"}
{"snippet": "def examine_coefficients(model, df):\n    df = pd.DataFrame(\n        { 'Coefficient' : model.coef_[0] , 'Feature' : df.columns}\n    ).sort_values(by='Coefficient')\n    return df[df.Coefficient !=0 ]\n", "intent": "- Examine the coefficients using the `examine_coefficients` function provided\n"}
{"snippet": "scale_obj = StandardScaler()\nX_scaled = scale_obj.fit_transform(X)\n", "intent": "- 30 degrees\n- 45 degrees\n- 60 degrees\n- 75 degrees\n"}
{"snippet": "from PIL import Image\nlfw_people = datasets.fetch_lfw_people(min_faces_per_person=50, \n                resize=0.4, data_home='../../data/faces')\nprint('%d objects, %d features, %d classes' % (lfw_people.data.shape[0],\n      lfw_people.data.shape[1], len(lfw_people.target_names)))\nprint('\\nPersons:')\nfor name in lfw_people.target_names:\n    print(name)\n", "intent": "Let's load a dataset of peoples' faces and output their names. (This step requires stable, fast internet connection.)\n"}
{"snippet": "liqdata = pd.read_csv('/Users/sebozek/GA/Iowa_Liquor_Sales_reduced.csv')\n", "intent": "Load your data from project 3\n"}
{"snippet": "import numpy as np\nfrom sklearn import svm, grid_search, datasets\nfrom sklearn.neighbors import KNeighborsClassifier\niris = datasets.load_iris()\n", "intent": "While GridSearch can work with most sklearn models, we will try it out on KNN to start with iris dataset.\n"}
{"snippet": "pca = PCA(n_components=3)\nX_r = pca.fit(Xn).transform(Xn)\n", "intent": "Now, conduct a PCA using scikit learn\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "pca = PCA(n_components=2)\nX_r = pca.fit(Xn).transform(Xn)\n", "intent": "Finally, conduct the PCA - use the results about to guide your selection of \"n\" componants\n"}
{"snippet": "cvt = CountVectorizer(stop_words='english', \n                      ngram_range = (2, 4))\ncvt.fit_transform(insults_df['Comment'])\nsummaries = \"\".join(insults_df[\"Comment\"])\nngrams_summaries = cvt.build_analyzer()(summaries)\nngrams_2_4 = Counter(ngrams_summaries).most_common(75)\npd.DataFrame(ngrams_2_4, columns = ['ngram', 'Counts'])\n", "intent": "Display the top 75 ngrams with frequencies.\n"}
{"snippet": "pipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('cls', BernoulliNB())\n]) \npipeline.fit(X_train, Y_train)\npipeline.score(x_test, y_test)\n", "intent": "How do they compare?\n"}
{"snippet": "pipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('rf', RandomForestClassifier())\n]) \npipeline.fit(X_train, Y_train)\npipeline.score(x_test, y_test)\n", "intent": "How do they compare?\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer(\n    binary=True,  \n    stop_words='english', \n    max_features=50, \n)\nX = v.fit_transform(data.title).todense()\nX = pd.DataFrame(X, columns=v.get_feature_names())\nX.head()\n", "intent": "- `CountVectorizer` builds a feature per word automatically as we did manually for `recipe`, `electronic` above.\n"}
{"snippet": "def bar_chart(feature):\n    survived = train[train[\"Survived\"] == 1][feature].value_counts()\n    dead = train[train[\"Survived\"] == 0][feature].value_counts()\n    df = pd.DataFrame([survived, dead])\n    df.index = ['Survived', 'Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))\n", "intent": "- Pclass\n- Sex\n- SibSp ( \n- Parch ( \n- Embarked\n- Cabin\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits() \nprint(digits.keys())\nprint(\"\\nuse data-X target-y to train and test\")\n", "intent": "Classfication of digit with hand written image (gray scale data matrix)\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nboston.keys()\n", "intent": "(1) import data and explore\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target)\n", "intent": "(2) split into train/test with boston dataset:\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nboston.keys()\n", "intent": "Unsupervised transformations for preprocessing\n--------------------------------------------------\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits(n_class=5)\nX, y = digits.data, digits.target\nprint(X.shape)\n", "intent": "PCA for dimensionality Reduction\n---------------------------------\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=16)\nX.shape\n", "intent": "Clustering (KMeans)\n=============\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nX, y = digits.data, digits.target\n", "intent": "A less trivial example (bad adjusted_rand_score)\n-------------------------\n"}
{"snippet": "from sklearn.datasets import load_iris\nfrom sklearn.neighbors import KNeighborsClassifier\niris = load_iris()\nX, y = iris.data, iris.target\nn_samples = X.shape[0]\nprint(X.shape)\nprint(y.shape)\nnp.unique(y)\n", "intent": "Cross-Validation (more robost compared with  training / test split)\n=====================================\n"}
{"snippet": "from sklearn import preprocessing\nstd_scale = preprocessing.StandardScaler().fit(df[['SQFT', 'BDRMS', 'AGE']])\ndf_std = std_scale.transform(df[['SQFT', 'BDRMS', 'AGE']])\n", "intent": "Refer to this if you are confused as to the formula: [Standardization](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n", "intent": "Avoid Overfitting with Hyper-Parameters\n----------------------------------------\n"}
{"snippet": "from sklearn.datasets import make_regression\nfrom sklearn.cross_validation import train_test_split\nX, y, true_coefficient = make_regression(n_samples=80, n_features=30, n_informative=10, noise=100, coef=True, random_state=5)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)\nprint(X_train.shape)\nprint(X_test.shape)\ntrue_coefficient\n", "intent": "```\ny_pred = x_test[0] * coef_[0] + ... + x_test[n_features-1] * coef_[n_features-1] + intercept_\n```\n"}
{"snippet": "d = {'col1': \"ts1\", 'col2': \"ts2\"}\ndf =pd.DataFrame(data=d,index = [0,1])\ndf\n", "intent": "How many rows are in the dataset?\n"}
{"snippet": "sac = pd.read_csv('../../assets/datasets/Sacramentorealestatetransactions.csv')\nsac.head(10)\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "subset = ['ASSAULT','VANDALISM']\nsf_crime_sub = sf_crime[sf_crime['Category'].str.contains('|'.join(subset))]\nX = patsy.dmatrix('~ C(hour) + C(DayOfWeek) + C(PdDistrict) + X + Y', sf_crime_sub)  \nY = sf_crime_sub.Category.values\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, stratify=Y, random_state=5)\n", "intent": "setting threshhold for binary class\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nstudent = pd.read_csv('data/student-mat.csv')\nstudent.head()\n", "intent": "Pandas allows for easy reading and provides a dataframe of the dataset.\n"}
{"snippet": "try:\n    from sklearn.model_selection import train_test_split    \nexcept:\n    from sklearn.cross_validation import train_test_split   \nXTrain, XTest, yTrain, yTest = train_test_split(preproData,target_classes,test_size = 0.25) \n", "intent": "<div class=\"exo\"> <b>Question:</b> Use Scikit-Learn to split the dataset into learning and training sets with <tt>train_test_split</tt>.\n</div>\n"}
{"snippet": "try:\n    from sklearn.model_selection import train_test_split    \nexcept:\n    from sklearn.cross_validation import train_test_split   \nXTrain, XTest, yTrain, yTest = train_test_split(num_features,dfy.values.ravel(),test_size = 0.25) \n", "intent": "<h1> Testing samples </h1>\n"}
{"snippet": "df = df_raw\ndf.pivot_table(\n    index = 'label',\n    aggfunc=[np.min, np.max],\n    values = 'prob').apply(\n    lambda x: np.round(x, decimals = 8))\n", "intent": "Because we have our data set labeled, we can identify an epsilon (threshold, aka \"outlier fraction\") using descriptive stats.\n"}
{"snippet": "from sklearn import preprocessing\nstd_scale = preprocessing.StandardScaler().fit(df[['SQFT', 'BDRMS', 'AGE']])\ndf_std = std_scale.transform(df[['SQFT', 'BDRMS', 'AGE']])\n", "intent": "[StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n"}
{"snippet": "with_zip=pd.DataFrame()\nwith_zip['zip']=zips\n", "intent": "Further the clustering of 5 will be used.\n"}
{"snippet": "conmat = np.array(confusion_matrix(y_test, pred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['over_200k', 'under_200k'],\n                         columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "df_cities = pd.read_csv('ds_cities.csv')\ndf_cities.head()\n", "intent": "Source\n[Best_cities_for_data_scientists]('https://infogr.am/30f00c18-997e-4a9d-b903-191899a53890')\n"}
{"snippet": "adult = pd.DataFrame(adult)\nadult.head()\n", "intent": "Convert the data to a Pandas dataframe to work with the data:\n"}
{"snippet": "vote = pd.read_csv(votes_file)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "pc = PCA(n_components=5)\npc.fit(X_s)\n", "intent": "Now, conduct a PCA using scikit learn\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "ip = pd.read_csv('IP_address.txt', sep = '\\t')\nip.head()\n", "intent": "source: http://proxylist.hidemyass.com/\n"}
{"snippet": "df.to_csv('iowa_clean_10.csv')\n", "intent": "Perform some exploratory statistical analysis and make some plots, such as histograms of transaction totals, bottles sold, etc.\n"}
{"snippet": "adult = pd.read_csv(\n    './data/adult.data', \n    names=[\n        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n        \"Hours per week\", \"Country\", \"Target\"], \n    header=None, na_values=\"?\")\nadult = pd.get_dummies(adult)\nadult[\"Target\"] = adult[\"Target_ >50K\"]\n", "intent": "<h1 align=\"center\">Adult test</h1> \n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('https://www.dropbox.com/s/1k9cgsd7bzce0yk/housing-data.csv?dl=1')\n", "intent": "https://www.dropbox.com/s/1k9cgsd7bzce0yk/housing-data.csv?dl=1\n"}
{"snippet": "coefficients = pd.DataFrame({\"Feature\":X_train.columns,\"Coefficients\":np.transpose(rcv.coef_)})\n", "intent": "RidgeCV fit the best of all the Regularized Cross Validated Fits in terms of r squared.\n"}
{"snippet": "train = []\nfor category_id, category in enumerate(CATEGORIES):\n    for file in os.listdir(os.path.join(train_dir, category)):\n        train.append(['train/{}/{}'.format(category, file), category_id, category])\ntrain = pd.DataFrame(train, columns=['file', 'category_id', 'category'])\nprint('train df shape = ', train.shape)\ntrain.head(2)\n", "intent": "There are at least 221 images for each species of seedling in the dataset.\n"}
{"snippet": "messages = pandas.read_csv('data/smsspamcollection/SMSSpamCollection', sep='\\t',\n                           names=[\"label\", \"message\"])\nmessages.head()\n", "intent": "We'll use **read_csv** and make note of the **sep** argument, we can also specify the desired column names by passing in a list of *names*.\n"}
{"snippet": "training_file = 'train.p'\ntesting_file = 'test.p'\nwith open(training_file, mode='rb') as f_train:\n    train = pickle.load(f_train)\nwith open(testing_file, mode='rb') as f_test:\n    test = pickle.load(f_test)\nX_train, X_val, y_train, y_val = train_test_split(train[\"features\"], train[\"labels\"], random_state=0, test_size=0.33)\nX_test, y_test = test[\"features\"], test[\"labels\"]\n", "intent": "Start by importing the data from the pickle file.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.15, random_state = 0)\n", "intent": "Ok..we are ready to start building the model now , let's start by splitting our data into training and test sets\n"}
{"snippet": "titanic_test.fillna(-999,inplace = True)\n", "intent": "and finally replace the null values of the dataframe with -999\n"}
{"snippet": "Submission = pd.DataFrame({\"PassengerID\":titanic_submission[:,0].astype(int),\"Survived\":submission})\n", "intent": "Let's go ahead and create the submission file , (My first ever! )\n"}
{"snippet": "Submission.to_csv(\"Submit_file_TPOT.csv\")\n", "intent": "Save the result as a csv format in my drive!\n"}
{"snippet": "df = pd.read_csv(\"train.csv\")\ndf.isnull().sum()\n", "intent": "Reading the files and investigating the null values\n"}
{"snippet": "pd.DataFrame({'feature':feature_cols,\n              'importance':treeclf.feature_importances_}).sort_values('importance',\n                                                                      ascending=False).head()\n", "intent": "Notice the split in the bottom right, which was made only to increase node purity.\n"}
{"snippet": "submission.to_csv(\"Titanic_CatBoost.csv\")\n", "intent": "And finally saving the file in a csv format in my drive.\n"}
{"snippet": "def pd_centers(featuresUsed, centers):\n    colNames = list(featuresUsed)\n\tcolNames.append('prediction')\n\tZ = [np.append(A, index) for index, A in enumerate(centers)]\n\tP = pd.DataFrame(Z, columns=colNames)\n\tP['prediction'] = P['prediction'].astype(int)\n\treturn P\n", "intent": "Let us first create some utility functions which will help us in plotting graphs:\n"}
{"snippet": "df_predict = pd.read_csv(\"./data/pima-data-trunc.csv\")\nprint(df_predict.shape)\n", "intent": "Once the model is loaded we can use it to predict on some data.  In this case the data file contains a few rows from the original Pima CSV file.\n"}
{"snippet": "fill_0 = Imputer(missing_values=0, strategy=\"mean\", axis=0)\nX_predict = fill_0.fit_transform(X_predict)\n", "intent": "Data has 0 in places it should not.  \nJust like test or test datasets we will use imputation to fix this.\n"}
{"snippet": "cleaned_stock_data = pd.read_csv('output/stocks_merged_ml_data-03-19-18.csv')\ncleaned_stock_data.head()\n", "intent": "**Read in the advertising.csv file and set it to a data frame called cleaned_stock_data.**\n"}
{"snippet": "import pandas as pd\ndf = pd.read_table('C:\\Users\\MLUSER\\Documents\\GitHub\\Udacity\\Naive Bayes Tutorial/SMSSpamCollection',\n                  sep='\\t',\n                  header=None,\n                  names=['label','sms_message'])\ndf.head()\n", "intent": "Step 1.1: Understanding our dataset\n--\n- Parse the data\n- Dataset from - https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n"}
{"snippet": "df = pd.read_csv(\"College.csv\")\ndata = None\ndata1 = data[:,2:19]\nX = None\nY = None\n", "intent": "Import the data from the file College.csv\n"}
{"snippet": "train_df = pd.read_csv(\"../data/train.csv\")\n", "intent": "** Read in the Customers Bike Rental csv train and test files as a Pandas DataFrame.**\n"}
{"snippet": "train_no_dummy_year_piv = all_no_dummy_interpolate[all_no_dummy_interpolate['train'] == 1].pivot_table(values='count',index='month',columns='year').rename(index=monthDict)\ntrain_no_dummy_year_piv\n", "intent": "** Let's have a close look on ditribution across different years. **\n"}
{"snippet": "wine_df = pd.read_json(api_response.text)\nwine_df.tail(2)\n", "intent": "This sometimes works, but the data may need adjusting\n"}
{"snippet": "train_no_dummy_year_temp_piv = all_no_dummy_interpolate[all_no_dummy_interpolate['train'] == 1].round().pivot_table(values='count',index='temp',columns=['workingday']).dropna()\n", "intent": "** Let's cluster temp and workingday impact on rentals. **\n"}
{"snippet": "spark_train_dummy = sqlContext.createDataFrame(trainingData_dummy)\nspark_test_dummy = sqlContext.createDataFrame(testData_dummy)\nspark_train_no_dummy = sqlContext.createDataFrame(trainingData_no_dummy)\nspark_test_no_dummy = sqlContext.createDataFrame(testData_no_dummy)\n", "intent": "** Create Apache Spark Data Frames ** \n"}
{"snippet": "spark_Kaggle_test_dummy = sqlContext.createDataFrame(all_dummy_interpolate[all_dummy_interpolate['train'] != 1].reset_index())\nspark_Kaggle_test_no_dummy = sqlContext.createDataFrame(all_no_dummy_interpolate[all_no_dummy_interpolate['train'] != 1].reset_index())\n", "intent": "** Prepare Kaggle test data. **\n"}
{"snippet": "DUMMY = True\ntrainingData_dummy = sqlContext.createDataFrame(spark_train_dummy.rdd.map(transformToLabeledPoint), [\"hour_cat\", \"label_count\", \"label_registered\", \"label_casual\", \"features_to_filter\"])\ntestData_dummy = sqlContext.createDataFrame(spark_test_dummy.rdd.map(transformToLabeledPoint), [\"hour_cat\", \"label_count\", \"label_registered\", \"label_casual\", \"features_to_filter\"])\ntestKaggle_dummy = sqlContext.createDataFrame(spark_Kaggle_test_dummy.rdd.map(transformToLabeledPointKaggle), [\"hour_cat\", \"label_count\", \"label_registered\", \"label_casual\", \"features_to_filter\"])\nchiSq_dummy = sqlContext.createDataFrame(spark_train_dummy.unionAll(spark_test_dummy)\\\n                                         .rdd.map(transformToLabeledPointChiSqSelector), [\"hour_cat\", \"label_count_cat\", \"label_registered_cat\", \"label_casual_cat\", \"features_to_filter\"])\n", "intent": "** Preparation Apache Spark data frame for linear regression **\n"}
{"snippet": "DUMMY = False\ntrainingData_no_dummy = sqlContext.createDataFrame(spark_train_no_dummy.rdd.map(transformToLabeledPoint), [\"hour_cat\", \"label_count\", \"label_registered\", \"label_casual\", \"features_to_filter\"])\ntestData_no_dummy = sqlContext.createDataFrame(spark_test_no_dummy.rdd.map(transformToLabeledPoint), [\"hour_cat\", \"label_count\", \"label_registered\", \"label_casual\", \"features_to_filter\"])\ntestKaggle_no_dummy = sqlContext.createDataFrame(spark_Kaggle_test_no_dummy.rdd.map(transformToLabeledPointKaggle), [\"hour_cat\", \"label_count\", \"label_registered\", \"label_casual\",\"features_to_filter\"])\nchiSq_no_dummy = sqlContext.createDataFrame(spark_train_no_dummy.unionAll(spark_test_no_dummy)\\\n                                         .rdd.map(transformToLabeledPointChiSqSelector), [\"hour_cat\", \"label_count_cat\", \"label_registered_cat\", \"label_casual_cat\", \"features_to_filter\"])\n", "intent": "** Preparation Apache Spark data frame for random forest and gradient boosted regression. No need for dummy variables. **\n"}
{"snippet": "kaggleSubmission_sum.to_csv('../data/kaggleSubmission_sum.csv', index=False)\nkaggleSubmission_count.to_csv('../data/kaggleSubmission_count.csv', index=False)\n", "intent": "** Kaggle result: 0.42860 **\n"}
{"snippet": "c = pd.DataFrame({'VarA':['aa','bb'], 'VarB':[22.2,33.3]}, \\\n                 index = ['Case1','Case2'])\nc\n", "intent": "DataFrames are designed to store heterogeneous multivariate data.\n"}
{"snippet": "import pandas as pd\nimport seaborn as sn\nd = pd.read_json('http://api.citybik.es/bicing.json')   \nd.info()\n", "intent": "We can read online data from public API's such as http://www.citybik.es/\n"}
{"snippet": "kids = pd.read_csv(\"http://www.mosaic-web.org/go/datasets/kidsfeet.csv\")\nkids.shape\n", "intent": "The <code>ix[]</code> can also be used to get a subset of the dataframe:\n"}
{"snippet": "df = pd.read_csv(\"assets/datasets/iris.csv\")\nprint df.Name.value_counts()\ndf.head(5)\n", "intent": "Let's do some clustering with the iris dataset.\n"}
{"snippet": "c = pd.DataFrame({'VarA':['aa', np.nan, 'cc'], 'VarB':[20,30,55], 'VarC':[1234, 3456, 6789]}, \\\n                 index = ['Case1','Case2','Case3'])\nc = c.fillna(\"\")\nc\n", "intent": "Another option od to use the ``fillna`` function to fillthem with empty strings:\n"}
{"snippet": "import pandas as pd\nunames = ['user_id', 'gender', 'age', 'occupation', 'zip']\nusers = pd.read_table('files/ml-1m/users.dat', sep='::', header=None, names=unames, engine='python')\nrnames = ['user_id', 'movie_id', 'rating', 'timestamp']\nratings = pd.read_table('files/ml-1m/ratings.dat', sep='::', header=None, names=rnames, engine='python')\nmnames = ['movie_id', 'title', 'genres']\nmovies = pd.read_table('files/ml-1m/movies.dat', sep='::', header=None, names=mnames, engine='python')\n", "intent": "We can read the database:\n"}
{"snippet": "data = pd.read_csv('files/AirPassengers.csv')\nprint data.head()\nprint '\\n Data Types:'\nprint data.dtypes\n", "intent": "Now, we can load the data set and look at some initial rows and data types of the columns:\n"}
{"snippet": "df2 = pd.read_csv('data/BattingAverage.csv', usecols=[0,1,2,3], dtype={'PriPos':'category'})\ndf2['BatAv'] = df2.Hits.divide(df2.AtBats)\ndf2.info()\n", "intent": "See also section 9.5.1\n"}
{"snippet": "ftransform = lambda x: 1./ (x + 0.001)\nfor var in range(13):\n    X_train1, X_test1, y_train1, y_test1 = train_test_split(bostonhp[:,[var]], target, test_size=0.2, random_state=42)\n    X_train2, X_test2, y_train2, y_test2 = train_test_split(ftransform(bostonhp[:,[var]]), target, test_size=0.2, random_state=42)\n    lin_mod1 = LinearRegression(fit_intercept=True)\n    lin_mod1.fit(X_train1,y_train1)\n    lin_mod2 = LinearRegression(fit_intercept=True)\n    lin_mod2.fit(X_train2,y_train2)\n    print(var,\"R^2 = {0}\".format(lin_mod1.score(X_test1,y_test1)), \" transf: R^2 = {0}\".format(lin_mod2.score(X_test2,y_test2)))\n", "intent": "We will now see that applying a simple transformation to some variables, their correlation with the target (measured with $R^2$) increases.\n"}
{"snippet": "ftransform = lambda x: 1./ (x + 0.001)\nvariable_list = [bostonhp[:,5], bostonhp[:,12], ftransform(bostonhp[:,12]), bostonhp[:,10]]\nmulti_variable = np.zeros((bostonhp.shape[0], len(variable_list)))\nfor i in range(len(variable_list)):\n    multi_variable[:,i] = variable_list[i] \n    X_train, X_test, y_train, y_test = train_test_split(multi_variable, target, test_size=0.2, random_state=42)\n    multi_lin_mod = LinearRegression(fit_intercept=True)\n    multi_lin_mod.fit(X_train, y_train)\n    print \"num_var %d, my score %f\" %(0 + i ,multi_lin_mod.score(X_test, y_test))\n", "intent": "In this first multiple varible regression we select 4 variable which are considered the most correlated with our target\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(reb_text, reb_target, test_size=0.2, random_state=42)\n", "intent": "Now we slpit our balanced dataset in Training and Test parts.\n"}
{"snippet": "df = pd.DataFrame()\ndf['ship_type'] = np.random.choice([\"romulan\", \"human\", \"klingon\", \"borg\",'red_shirt','ovid'], size=500)\ndf['baths'] = np.random.choice(np.arange(1, 4, 0.5), size=500)\ndf['ship_value'] = np.random.randint(200000, 10000000, size=500)\ndf['speed'] = np.random.randint(10,60, size=500)\n", "intent": "With 2-3 continious variables, and one categorical.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncolumns = ['ship_value','speed']\ndf[columns] = scaler.fit_transform(df[columns])\nformulas = 'baths ~ ship_value + speed ' \ny, x = patsy.dmatrices(formula, data=df, return_type='dataframe')\nlm = LinearRegression()\nmodel = lm.fit(x,y)\nscore = model.score(x,y)\nprint 'R^2', score\n", "intent": "More code will be written...\n"}
{"snippet": "X_scaled = preprocessing.MinMaxScaler().fit_transform(df[cols])\n", "intent": "Next, since each of our features have different units and ranges, let's do some preprocessing:\n"}
{"snippet": "scaler = StandardScaler()\nsvm_clf1 = LinearSVC(C=1, loss=\"hinge\")\nscaled_svm_clf1 = Pipeline((\n(\"scaler\", scaler),\n    (\"linear_svc\", svm_clf1)\n))\nscaled_svm_clf1.fit(X,y)\n", "intent": "Step 2: Do feature scaling of the features using StandardScaler() and model the SVM Linear classifier\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidVectorizer = TfidfVectorizer()\ntext_vectorized = tfidVectorizer.fit_transform(df.text)\ntext_vectorized_array = text_vectorized.toarray()\n", "intent": "see http://www.gadatascience.com/modeling/text_processing.html\n"}
{"snippet": "feature_importances = pd.DataFrame(dt.feature_importances_,\n                                   index = X.columns,\n                                    columns=['importance'])\nfeature_importances.sort_values(by='importance', ascending=False).head()\n", "intent": "We can calculate this in sklearn\n"}
{"snippet": "dt = DecisionTreeClassifier()\ndt.fit(X, y)\nimportances = pd.DataFrame(zip(dt.feature_importances_,rf.feature_importances_,),\n                           index=X.columns, columns=['dt_importance','rf_importance']).sort_values('rf_importance',ascending=False)               \nimportances.head()\n", "intent": "Let's compare the 2 models (re-init Decision Tree with no max depth constraint)\n"}
{"snippet": "black_holes = parser.parse(\"../data/ScienceDirect/black_holes.dat\")\nall_abstracts = abstracts + black_holes\ndata = pd.DataFrame()\ndata['abstracts'] = all_abstracts\n", "intent": "The TfdifVectorizer didn't seem to change anything. I am going to download more data and see what happens with different abstracts involved \n"}
{"snippet": "conf = pd.DataFrame()\nconf = res.conf_int()\nconf['OR'] = res.params\nprint(conf)\nprint(np.exp(conf))\n", "intent": "hint 1: np.exp(X)\nhint 2: conf['OR'] = params\n           conf.columns = ['2.5%', '97.5%', 'OR']\n"}
{"snippet": "from sklearn.datasets import load_iris\ndata = load_iris()\nX = data.data\ny = data.target\nprint(type(X))\nprint(X.shape)\nprint(\"First three rows of data\\n {}\".format(X[:3]))\nprint(\"First three labels: {}\".format(y[:3]))\nprint(data.DESCR)\n", "intent": "The data set is distributed with sci-kit learn, the only thing we have to do is to important a function and call it.\n"}
{"snippet": "DATA_ROOT = r\"~/Dropbox/DATA/Pawal Model Pipeline Data/data\"\ndf = pd.read_csv(os.path.join(DATA_ROOT,\"ebaytitles.csv\"))\ndf.head()\n", "intent": "Read csv file into a dataframe\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nsmall_X_tr = X_tr[1:10]\nX_train_counts = count_vect.fit_transform(small_X_tr)\nprint(X_train_counts.shape)\n", "intent": "Now fit it to a Vectorizer\n"}
{"snippet": "data = {'f1': [0.1, 2.1, 0.1], 'f2': [1.5, 5.7, 2.1], 'f3': [6, 33, 41], 'f4': ['red', 'blue', 'green']}\ndf = pd.DataFrame(data)\ndf\n", "intent": "```python\npd.get_dummies()\n```\n"}
{"snippet": "gbt_sort = pd.DataFrame({'pred':train_scores_gbt[:,1], 'y':y_train}).reset_index(drop=1)\ngbt_sort = gbt_sort.sort_values(by='pred')\ngbt_sort.head()\n", "intent": "Create sorted pred and y df\n"}
{"snippet": "from sklearn.datasets import make_hastie_10_2\nfrom sklearn.ensemble import GradientBoostingClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nclf_1 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n    max_depth=1, random_state=0).fit(X_train, y_train)\n", "intent": "Gradient Boosting Classifier\n"}
{"snippet": "df_loandata.to_csv('LoanData_Cleansed.csv')\n", "intent": "- includes target and feature variables\n"}
{"snippet": "df = pd.read_csv('knn-project-data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ntrain_Y = label_encoder.fit_transform([y for _,y in train_filtered])\ndev_Y = label_encoder.transform([y for _,y in dev_filtered])\n", "intent": "Next we convert the labels using the `LabelEncoder`.\n"}
{"snippet": "import pandas as pd\npd.DataFrame(data, columns = [\"x\",\"y\"])\n", "intent": "Consider loss on this data:\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\nvectorizer = DictVectorizer()\ntrain_X = vectorizer.fit_transform([feats(x) for x,_ in train_filtered])\ndev_X = vectorizer.transform([feats(x) for x,_ in dev_filtered])\ndev_X[0]\n", "intent": "Apply to training and test instances:\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ntrain_Y = label_encoder.fit_transform([y for _,y in train_filtered])\ndev_Y = label_encoder.transform([y for _,y in dev_filtered])\ndev_Y[:10]\n", "intent": "scikit-learn wants prefers numbers as classes:\n* $\\text{positive}\\rightarrow 0$\n* $\\text{negative}\\rightarrow 1$\n"}
{"snippet": "boston = load_boston()\nX = boston.data \ny = boston.target \ncolNames = boston.feature_names \n", "intent": "Let's try it on the Boston data set\n"}
{"snippet": "from sklearn.datasets import load_iris\ndata = load_iris()\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "ridge_models = []\nfor alpha in ridge_alphas:\n    scaler = XyScaler()\n    scaler.fit(X_train.values, y_train.values)\n    X_train_std, y_train_std = scaler.transform(X_train.values, y_train.values)\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(X_train_std, y_train_std)\n    ridge_models.append(ridge)\n", "intent": "First, let's build up an array of fit models.\n"}
{"snippet": "X, y = make_classification(\n    n_samples=100,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_classes=2,\n    random_state=0)\n", "intent": "**Q:** Generate a dataset using sklearn's make_classification module.\n"}
{"snippet": "data = np.genfromtxt('data/part3_nonseparable.csv', delimiter=',')\nX3 = data[:,1:3]\ny3 = data[:,3]\nX3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.3, random_state=300)\nplot_data_basic(X3,y3)\n", "intent": "1\\. Load the file `data/part3_nonseparable.csv` into a dataframe.\n"}
{"snippet": "npts = 10000\ny = scs.bernoulli(0.2).rvs(npts)\nX = pd.DataFrame({'a':scs.norm(0, 1).rvs(npts) + 0.3*y,\n                  'b':scs.poisson(2*y + 2).rvs(npts)})\n", "intent": "Let's start with some (fake) data, with two features (one continuous, one integer) and a binary label.\n"}
{"snippet": "Ybdf = pd.DataFrame(Yb)\n", "intent": "Let's compare SVM to logistic regression.\nFirst make the data \"shittier\" since its doing so well.\n"}
{"snippet": "df113 = pd.read_csv(path % 113)\ndf114 = pd.read_csv(path % 114)\ndf115 = pd.read_csv(path % 115)\n", "intent": "Once the above batches have been run and fitted, we can further test it by computing the AUC score for the remaining batches: 113, 114, and 115.\n"}
{"snippet": "testdf.fillna(0, inplace=True)\nY = testdf.player_won.values\nX = testdf[[c for c in testdf.columns if not c == 'player_won']].values\n", "intent": "Here we simply fill the concatenated dataframe's `NaN` values with 0 and instantiate the target variable, Y, and the predictor variable, X. \n"}
{"snippet": "import json\nsu['title'] = su.boilerplate.map(lambda x: json.loads(x).get('title', ''))\nsu['title'].fillna('', inplace=True)\n", "intent": "You will need to parse the json from the boilerplate field.\n---\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features = 5000,\n                             ngram_range=(1, 2),\n                             stop_words='english',\n                             binary=True)\n", "intent": "It is up to you what range of ngrams and features, and whether or not you want the columns binary or counts.\n---\n"}
{"snippet": "votes = pd.read_csv('../assets/datasets/votes.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "su['title'] = su.boilerplate.map(lambda x: json.loads(x).get('title', ''))\nsu['body'] = su.boilerplate.map(lambda x: json.loads(x).get('body', ''))\ntitles = su['title'].fillna('')\nbody = su['body'].fillna('')\ntitles[0:5]\n", "intent": "You will need to parse the json from the boilerplate field.\n---\n"}
{"snippet": "title_vectorizer = CountVectorizer(max_features = 1000, ngram_range = (1, 2), stop_words = 'english', binary = True)\ntitle_vectorizer.fit(titles)\n", "intent": "It is up to you what range of ngrams and features, and whether or not you want the columns binary or counts.\n---\n"}
{"snippet": "movies = pd.DataFrame(imdb.top_250(), columns = ['num_votes', 'rating', 'tconst', 'title', 'year'])\n", "intent": "Keep the fields:\n    num_votes\n    rating\n    tconst\n    title\n    year\nAnd discard the rest\n"}
{"snippet": "from sklearn.feature_extraction.text import HashingVectorizer\nhvec = HashingVectorizer()\nhvec.fit([spam])\n", "intent": "> \n>\n    from sklearn.feature_extraction.text import HashingVectorizer\n    hvec = HashingVectorizer()\n    hvec.fit([spam])\n"}
{"snippet": "df = pd.DataFrame(data=mtcars)\ncar = pd.DataFrame(data=mtcars, columns=['Car'])\n", "intent": "Convert to a Pandas Dataframe\n"}
{"snippet": "pca = pd.DataFrame(pca)\npca.head()\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_all, all_tweets['text'], test_size = 0.3)\n", "intent": "Double check that you are getting random data before moving forward.  What would happen if you over sample Trump more than Sanders?\n"}
{"snippet": "cvt = CountVectorizer()\nX_all = cvt.fit_transform(insults_df[\"Comment\"]).toarray()\nsummaries = \"\".join(insults_df['Comment'])\ncount_insults = cvt = cvt.build_analyzer()(summaries)\ncomment_count = []\nfor i in Counter(count_insults).most_common():\n     if i[1] > 50:\n        comment_count.append(i)\ncomment_count\n", "intent": "Plot a histogram of some kind might be helpful.\n"}
{"snippet": "cvt = CountVectorizer(ngram_range = (2, 4), stop_words=\"english\")\nX_all = cvt.fit_transform(insults_df[\"Comment\"])\nsummaries = \"\".join(insults_df['Comment'])\ncount_insults = cvt = cvt.build_analyzer()(summaries)\ncomment_count = []\nfor i in Counter(count_insults).most_common():\n     if i[1] > 50:\n        comment_count.append(i)\ncomment_count\n", "intent": "Display the top 75 ngrams with frequencies.\n"}
{"snippet": "PCA_set = PCA(n_components=5)\nX = PCA_set.fit_transform(xStand)\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "degree = 300\nsimple_model = make_pipeline(MinMaxScaler((0, 0.5)), PolynomialFeatures(degree), LinearRegression())\nsimple_model.fit(X_train.reshape((X_train.shape[0], 1)), Y_train)\n", "intent": "Let's fit a degree 300 polynomial to our data, as you can already imagine, the high degree is over-kill and will lead to overfitting.\n"}
{"snippet": "df_mms = p.MinMaxScaler().fit_transform(df)\n", "intent": "`3.` Now it's your turn.  Try fitting the **MinMaxScaler** transformation to this dataset. You should be able to use the previous example to assist.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n", "intent": "Now let's do the train/test split.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"C:/Users/kohleggermichael/OneDrive/Teaching/[WCIS] CRM und Information Mining II/Block-1/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "path = Path('..', 'data', 'vehicles_train.csv')\ntrain = pd.read_csv(path)\ntrain['vtype'] = train.vtype.map({'car':0, 'truck':1})\ntrain\n", "intent": "<a id=\"manual-bagged\"></a>\n"}
{"snippet": "df = pd.read_csv('titanic-train.csv')\n", "intent": "Load the csv file into memory using Pandas\n"}
{"snippet": "df['Age'].fillna(median_age, inplace = True)\ndf.info()\n", "intent": "impute the missing values for Age using the median Age\n"}
{"snippet": "survival_by_gender = df[['Sex','Survived']].pivot_table(columns =\n                        ['Survived'], index = ['Sex'], aggfunc=len)\nsurvival_by_gender\n", "intent": "Check the influence of Sex on Survival\n"}
{"snippet": "survival_by_Pclass = df[['Pclass','Survived']].pivot_table(columns =\n                        ['Survived'], index = ['Pclass'], aggfunc=len)\nsurvival_by_Pclass\n", "intent": "Check the influence of Pclass on Survival\n"}
{"snippet": "xStand = StandardScaler().fit_transform(x2)\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "average_age = data.Age.mean()\ndata.Age = data.Age.fillna(average_age)\nprint data.info()\n", "intent": "Fill missing values for age with average\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nf_train, f_test, t_train, t_test = train_test_split(features, target, test_size = .2, random_state = 0)\n", "intent": "Split the data into training and test sets\n"}
{"snippet": "test_data = pd.read_csv('test.csv')\nprint test_data.head()\nprint test_data.info()\nprint test_data.describe()\n", "intent": "Create a DataFrame with the test.csv data\n"}
{"snippet": "test_data.Sex = test_data.Sex.replace(['male','female'],[True,False])\ntest_data.Age = test_data.Age.fillna(average_age)\ntest_data.info()\n", "intent": "Clean the test data\n"}
{"snippet": "kaggle = test_data[['PassengerId','Survived']]\nkaggle.to_csv('kaggle_titanic_submission.csv', index=False)\n", "intent": "Save predictions in Kaggle submissions format - PassengerId and Survived (Hint - remember to set index=False in the to_csv function)\n"}
{"snippet": "seed = 7\nnp.random.seed(seed)\ntrain_path = \"C:/deep_learning/kaggle/amazon/train/\"\ntest_path = \"C:/deep_learning/kaggle/amazon/test/\"\nfeat_output_path = \"\"\ntrain = pd.read_csv(\"C:/deep_learning/kaggle/amazon/train_v2.csv\")\ntest =  pd.read_csv(\"C:/deep_learning/kaggle/amazon/sample_submission_v2.csv\")\ntrain.head()\n", "intent": "Set path variables:\n"}
{"snippet": "seed = 7\nnp.random.seed(seed)\ntrain_path = \"C:/deep_learning/kaggle/amazon/train/\"\ntest_path = \"C:/deep_learning/kaggle/amazon/test/\"\nfeat_output_path = \"C:/deep_learning/kaggle/amazon/feat/\"\ntrain = pd.read_csv(\"C:/deep_learning/kaggle/amazon/train_v2.csv\")\ntest =  pd.read_csv(\"C:/deep_learning/kaggle/amazon/sample_submission_v2.csv\")\ntrain.head()\n", "intent": "Set path variables:\n"}
{"snippet": "random_seed = 0\nrandom.seed(random_seed)\nnp.random.seed(random_seed)\ntrain_path = \"/media/alex/B44254FE4254C730/Users/Alex/Downloads/Kaggle Data/train-jpg/train-jpg/\"\ntest_path = \"/media/alex/B44254FE4254C730/Users/Alex/Downloads/Kaggle Data/test-jpg/\"\ntrain = pd.read_csv(\"/home/alex/Desktop/Rainforest/Data/train_v2.csv\")\ntest =  pd.read_csv(\"/home/alex/Desktop/Rainforest/Data/sample_submission_v2.csv\")\nobj_save_path = \"/home/alex/Desktop/Rainforest/Models/XGB/Objects/\"\nsubm_output_path = \"/home/alex/Desktop/Rainforest/Submissions/\"\n", "intent": "Set path variables:\n"}
{"snippet": "bunchobject = datasets.load_iris()\nfeature_index = [0,2]\nmy_data = bunchobject.data[:,feature_index]\n", "intent": "I chose the Iris dataset to illustrate this because the groupings are really obvious.\n"}
{"snippet": "airports_pca = PCA(n_components=3)\nairports_pca.fit(xStand)\nX = airports_pca.transform(xStand)\nX\n", "intent": "Finally, conduct the PCA - use the results above to guide your selection of n components\n"}
{"snippet": "def bar_chart(feature):\n  survived = train[train['Survived']==1][feature].value_counts()\n  dead = train[train['Survived']==0][feature].value_counts()\n  df = pd.DataFrame([survived, dead])\n  df.index = ['Survuved','Dead']\n  df.plot(kind='bar', stacked=True, figsize=(10,5))\n", "intent": "- Pclass\n- Sex\n- SibSp (\n- Parch (\n- Embarked\n- Cabin\n"}
{"snippet": "for dataset in train_test_data:\n  dataset['Embarked'] = dataset['Embarked'].fillna('S')\n", "intent": "more than 50% of 1st class are from S embark\nmore than 50% of 2nd class are from S embark\nmore than 50% of 3rd class are from S embark\n"}
{"snippet": "X_temp, X_test, y_temp, y_test = train_test_split(ingredients_weighted, data.cuisine, test_size=0.2, random_state=42)\nX_train, X_CV, y_train, y_CV = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\nprint('Number of training entries: {0} -> {1:.0f}% of data'.format(X_train.shape[0], 100*X_train.shape[0]/data.shape[0]))\nprint('Number of CV entries: {0} -> {1:.0f}% of data'.format(X_CV.shape[0], 100*X_CV.shape[0]/data.shape[0]))\nprint('Number of test entries: {0} -> {1:.0f}% of data'.format(X_test.shape[0], 100*X_test.shape[0]/data.shape[0]))\n", "intent": "Now that the sparse matrix is created, we will select splits for training, CV and test sets for our models.\n"}
{"snippet": "print(train.shape)\ncate = [f for f in train.columns if train.dtypes[f] =='O']\nfor c in cate:\n    train[c] = train[c].astype('category')\n    if train[c].isnull().any():\n        train[c] = train[c].cat.add_categories(['MISSING'])\n        train[c] = train[c].fillna('MISSING')\n", "intent": "look at the distribution of all categorical features\n"}
{"snippet": "features_missing = missing_data.index[missing_data['Total']>0].tolist()\nfor col in features_missing:\n    if full_dataset[col].dtypes=='O':\n        full_dataset[col] = full_dataset[col].fillna(full_dataset[col].mode()[0])\n    else:\n        full_dataset[col] = full_dataset[col].fillna(full_dataset[col].mean())\n", "intent": "We use MODE to fill catigorical features and MEAN to fill numerical features.\n"}
{"snippet": "train = full_dataset.loc['train']\ntest = full_dataset.loc['test']\ndummy_full = pd.get_dummies(full_dataset)\ntrain_dum = dummy_full.loc['train']\ntest_dum = dummy_full.loc['test']\nscaler = StandardScaler().fit(train_dum) \ntrain_scaled = scaler.transform(train_dum)\ntest_scaled = scaler.transform(test_dum)\nprint('The shape of dummy full dataset', dummy_full.shape)\n", "intent": "1) feature engineering\n    Transforming numerical features:\n    1. get dummy variables\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nfor c in catigorical_features:\n    lbl = LabelEncoder() \n    lbl.fit(list(full_data[c].values)) \n    full_data[c] = lbl.transform(list(full_data[c].values))\nprint('Shape all_data: {}'.format(full_data.shape))\n", "intent": "    4.lable encoder: Label Encoding some categorical variables that may contain information in their ordering set\n"}
{"snippet": "def pca_choose_feature(N_components, train, test):\n    Data_pca = PCA(n_components=N_components).fit(train)\n    train_pca = Data_pca.transform(train) \n    test_pca = Data_pca.transform(test)\n    return train_pca, test_pca\ntrain_pca_20, test_pca_20 = pca_choose_feature(20, train_scaled, test_scaled)\ntrain_pca_40, test_pca_40 = pca_choose_feature(40, train_scaled, test_scaled)\ntrain_pca_60, test_pca_60 = pca_choose_feature(60, train_scaled, test_scaled)\ntrain_pca_80, test_pca_80 = pca_choose_feature(80, train_scaled, test_scaled)\ntrain_pca_100, test_pca_100 = pca_choose_feature(100, train_scaled, test_scaled)\n", "intent": "2) feature selection: <BR>\n    1. PCA: select 20, 40, 60, 80, 100 features using Principle Component Analysis.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom patsy import dmatrix\nfeatures_df = dfrttf[['CA','MDW','NE','PNW','SE','SW','TX','Junior','Senior']]\ntarget_df = dfrttf['high_salary']\nX_train, X_test, Y_train, Y_test = train_test_split(features_df, target_df, \n                                                    test_size=0.33, random_state=5)\nscaler = StandardScaler() \nX_train_s = scaler.fit_transform(X_train)\nX_test_s = scaler.fit_transform(X_test)\n", "intent": "- You can either create the dummy features manually or use the `dmatrix` function from `patsy`\n- Remember to scale the feature variables as well!\n"}
{"snippet": "dfair = pd.read_csv('/Users/ajbentley/GA-DSI/curriculum/week-07/3.2-pca-lab-1/assets/datasets/Airport_operations.csv')\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "def dummyEncode(df):\n        names_of_columns_to_transform = list(df.select_dtypes(include=['category','object']))\n        le = LabelEncoder()\n        for feature in names_of_columns_to_transform:\n            try:\n                df[feature] = le.fit_transform(df[feature])\n            except:\n                print('Error encoding '+feature)\n        return df\n", "intent": "source: http://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python\n"}
{"snippet": "bike = pd.read_csv('bike_rentals.csv')\nbike.head()\n", "intent": "We can also use categorical variables as predictors in logistic regressions, we just have to turn them into dummy features.\n"}
{"snippet": "bike = pd.read_csv('bike_rentals.csv')\nbike.head()\n", "intent": "Let's recap using a straight line to predict rentals using multiple input variables\n"}
{"snippet": "import pandas as pd\nhr = pd.read_csv('HR.csv')\nhr.head()\n", "intent": "Use either SVM or Naive Bayes to correctly classify at risk employees.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "Now, let's split the data into train and test.\n"}
{"snippet": "df_usa_today = pd.read_csv(\"data/Trump_AND_China_6742.csv\",sep=\"\\t\",header=None,parse_dates=[0]).sample(150)\ndf_usa_today[1] = \"USA Today\" \ndf_ny_times = pd.read_csv(\"data/Trump_AND_China_8213.csv\",sep=\"\\t\",header=None,parse_dates=[0]).sample(150)\ndf_ny_times[1] = \"New York Times\"\ndf_ny_post = pd.read_csv(\"data/Trump_AND_China_164207.csv\",sep=\"\\t\",header=None,parse_dates=[0]).sample(150)\ndf_ny_post[1] = \"New York Post\"\ndf_ny_post.head()\n", "intent": "**Read the data we just downloaded**\n"}
{"snippet": "test_size = 0.30 \nseed = 2  \nxTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=test_size, random_state=seed)\n", "intent": "We would get a different result if we split the dataset randomly in ratio 70:30.\n"}
{"snippet": "advertising = pd.read_csv('../data/Advertising.csv', usecols=[1, 2, 3, 4])\nadvertising.info()\n", "intent": "Datasets available on http://www-bcf.usc.edu/~gareth/ISL/data.html\n"}
{"snippet": "PATH_TO_DATA = ('')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "airports_pca = PCA(n_components=3)\nairports_pca.fit(xStand)\nX = airports_pca.transform(xStand)\n", "intent": "Finally, conduct the PCA - use the results above to guide your selection of n components\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "Comparing machine learning models in scikit-learn\n"}
{"snippet": "col_names = (\"Sample code number\", \"Clump Thickness\", \"Uniformity of Cell Size\",\"Uniformity of Cell Shape\",\n\"Marginal Adhesion\",\n\"Single Epithelial Cell Size\",\n\"Bare Nuclei\",\n\"Bland Chromatin\",\n\"Normal Nucleoli\",\n\"Mitoses\",\n\"Class\")\ndf=pd.read_csv(\"../../assets/datasets/breast-cancer-wisconsin.csv\",names=col_names)\ndf\n", "intent": "The column names are taken from the dataset info file. Create an array\nwith the column names and assign them as the header when loading the\ncsv.\n"}
{"snippet": "sac = pd.read_csv('../../assets/datasets/Sacramentorealestatetransactions.csv')\nsac.head()\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "X_scaled=StandardScaler().fit_transform(X)\nX_scaled\n", "intent": "First, let's standardize the data\n"}
{"snippet": "coeff_df = pd.DataFrame(lm.coef_,X.columns,columns=['Coeffiecients'])\ncoeff_df['Coeffiecients'].values.tolist()\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "nychouses = pd.read_csv(\"allzips.csv\")\n", "intent": "read in zillow data\n"}
{"snippet": "from sklearn import metrics, cross_validation\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(titanic_short.values, \n                                                            titanic[\"Survived\"], test_size=0.33, random_state=42)\n", "intent": "cross validation will give me a better sense of the score. if the score on test and train is very different im overfitting\n"}
{"snippet": "path = 'https://serv.cusp.nyu.edu/~cq299/ADS2016/Data/Bayesian/'\ndata=pd.read_csv(path+\"example3.csv\")\ndata\nX=np.matrix(data.iloc[:,:-1])\ny=np.asarray(data.Y)\n", "intent": "http://scikit-learn.org/stable/modules/grid_search.html\n"}
{"snippet": "from sklearn.decomposition import PCA, NMF\nnmf = NMF(n_components=10)\nX2 = nmf.fit(x_scaled).transform(x_scaled)\nX2 = nmf.inverse_transform(X2)\n", "intent": "We can use non-negative matrix factorization for unsupervised extraction of consistent patterns in the data.\n"}
{"snippet": "iris = pd.read_csv('/Users/ajbentley/GA-DSI/curriculum/week-07/2.4-dimensionality-reduction/assets/datasets/iris.csv')\niris.head()\n", "intent": "Use an SVM, GridSearchCV and the iris dataset to find an optimal model.\nMake sure to search for optimal paramaters for kernel, C, \n"}
{"snippet": "scaler_model = MinMaxScaler()\n", "intent": "scale_model is an instance of MinMaxScaler()\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "train : test = 7 : 3, random_state is just random seed\n"}
{"snippet": "scale_model = MinMaxScaler()\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "adjacencies = pd.read_csv('/Users/bramvandesande/Projects/lcb/resources/rscenic/1.5_GENIE3_linkList.txt',\n                          usecols=['TF', 'Target', 'weight'])\nadjacencies.columns = [\"TF\", \"target\", \"importance\"]\n", "intent": "------\nStart from the GENIE3 output and check if derived co-expression modules are similar.\n"}
{"snippet": "r_df = pd.read_csv(os.path.join(RESOURCES_FOLDER, \"rscenic/2.4_motifEnrichment_selfMotifs.txt\"))\nlen(r_df)\n", "intent": "Load R table of enriched motifs and convert it to a comparable format.\n"}
{"snippet": "py_df = pd.read_csv(os.path.join(RESOURCES_FOLDER, \"regulomes_zeisel_2015.csv\"),\n                 index_col=[0,1], header=[0,1], skipinitialspace=True)\nlen(py_df)\n", "intent": "Load python table of enriched motifs and convert it to a comparable format.\n"}
{"snippet": "r_incidences = pd.read_csv(os.path.join(RESOURCES_FOLDER, \"rscenic/2.6_regulons_asIncidMat.txt\"), index_col=0)\n", "intent": "Create regulomes for R pipeline.\n"}
{"snippet": "df = pd.read_csv('KNN_Project_Data', index_col=0)\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "df.pivot_table(index=['country_name'], values=['status_num']).sort_values(by=['status_num'], ascending=False)\n", "intent": "<h3> This tells me to expect a significant difference in funding rates across sectors </h3>\n"}
{"snippet": "dfn_r = pd.DataFrame(dfn)\ndfn_r.isnull().sum()\n", "intent": "Now the data is all set and we're ready to model.\nThere's definitely going to be a ton of colinearity among these features.\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\ndef label_encode_col(col):\n    le = LabelEncoder()\n    return le.fit_transform(train_[col])    \n", "intent": "* normalize labels using `LabelEncoder`\n"}
{"snippet": "from sklearn.cluster import KMeans\nkmeans_model = KMeans(n_clusters=2, random_state=1)\nsenator_distances = kmeans_model.fit_transform(votes.iloc[:, 3:])\nsenator_distances[:5, :]\n", "intent": "Use the [fit_transform()](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n* Assign the result to senator_distances.\n"}
{"snippet": "scaled_features = scaler.fit_transform(df.drop('TARGET CLASS',axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\nprint('Train size: {}'.format(len(X_train)))\nprint('Test size: {}'.format(len(X_test)))\n", "intent": "Shuffle the data and split it to train and test parts.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(tokenizer=stem_tokenizer)\nvectorizer.fit(X_train)\nprint (vectorizer.transform([sms]))\n", "intent": "Fit a vectorizer which converts texts to count vectors.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit(vectorizer.transform(X_train))\nprint(tfidf_transformer.transform(vectorizer.transform([sms])))\n", "intent": "Convert count vectors to TFIDF\n"}
{"snippet": "import pandas as pd\nreviews = pd.read_csv('../data/en_reviews.csv', sep='\\t', header=None, names =['rating', 'text'])\nreviews[35:45]\n", "intent": "Implement and evaluate a classifier of user reviews using methods described in the NLP tutorial.\n"}
{"snippet": "import pandas as pd\nreviews = pd.read_csv('../data/en_reviews.csv', sep='\\t', header=None, names =['rating', 'text'])\nreviews[35:45]\n", "intent": "Implement and evaluate a classifier of user reviews using Support Vector Machines with RBF kernel. Use the word2vec vectors as features.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(vectors, target, test_size=0.2)\nprint('Train size: {}'.format(len(X_train)))\nprint('Test size: {}'.format(len(X_test)))\n", "intent": "Shuffle the data and split it to train and test parts.\n"}
{"snippet": "dfn_r = pd.DataFrame(dfn)\n", "intent": "Now the data is all set and we're ready to model.\nThere's definitely going to be a ton of colinearity among these features.\n"}
{"snippet": "array = dataset.values\nX=array[:,0:4]\nY=array[:,4]\nvalidation_size=0.20\nseed=7\nX_train,X_validation,Y_train,Y_validation=train_test_split(X,Y,test_size=validation_size,random_state=seed)\nprint (X_train[0:5])\nprint (Y_train[0:5])\n", "intent": "Note the diagonal grouping of some pairs of attributes. This suggests a high correlation and\na predictable relationship.\n"}
{"snippet": "PATH_TO_DATA = ('../../data')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'websites_train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'websites_test_sessions.csv'), index_col='session_id')\nwith open(r\"../../data/site_dic.pkl\", \"rb\") as input_file:\n    site_dict = pickle.load(input_file)\n", "intent": "Reading original data\n"}
{"snippet": "df_scaled = pd.DataFrame(bn_scaled, columns=bn.columns[1:])\ndf_scaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(yelp_class['text'], yelp_class['stars'])\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "sms = pd.read_table('data/sms.tsv', header=None, names=['label', 'message'])\n", "intent": "In this example we are going to train two models to classify SMS as \"Spam\" or \"Ham\".\n"}
{"snippet": "trdf = pd.read_csv('/data/arpit.goel/18_InventoryPrediction/02.ExtractedData/train.csv',header=0)\ntrdf_stores = trdf.merge(vf.drop_duplicates(subset=['Cliente_ID']), how=\"left\")\n", "intent": "Finally, we can apply these new tags on the actual Training and Test data sets that have been provided!\n"}
{"snippet": "trdf.to_csv('/data/arpit.goel/18_InventoryPrediction/04.FeatureEngineering/train_with_cnames.csv')\ntsdf.to_csv('/data/arpit.goel/18_InventoryPrediction/04.FeatureEngineering/test_with_cnames.csv')\n", "intent": "Write the data to file to save it for a new session....\n"}
{"snippet": "from sklearn import model_selection\nX_train, X_test, y_train, y_test = \\\n    model_selection.train_test_split(X, y, test_size=0.3, random_state=17)\n", "intent": "We randomly split the observations into a training set (70% of all data) and a test set (30%).\n"}
{"snippet": "df = pd.read_csv('data/phenotypes.pheno', sep=' ')\ndf_2W = df['2W']\ndf_4W = df['4W']\nsamples_with_phenotype_temp = np.where(pd.isnull(df_2W)!=True)[0].tolist() + np.where(pd.isnull(df_4W)!=True)[0].tolist()\nsamples_with_phenotype = []\nfor s in samples_with_phenotype_temp:\n    if samples_with_phenotype_temp.count(s) > 1 and s not in samples_with_phenotype:\n        samples_with_phenotype.append(s)\n", "intent": "**Note to authors:** we may let a empty cells for the student who will reach this point \n"}
{"snippet": "cancel = pd.read_csv(\"../../projects-weekly/project-07_faa/assets/airport_cancellations.csv\")\nops = pd.read_csv(\"../../projects-weekly/project-07_faa/assets/Airport_operations.csv\")\nap = pd.read_csv(\"../../projects-weekly/project-07_faa/assets/airports.csv\")\n", "intent": "PostgreSQL created in Navicat.\n"}
{"snippet": "pad_sents = keras.preprocessing.sequence.pad_sequences(sents, maxlen=max_len)\nlabel = np.asarray(label, dtype='float32')\n", "intent": "Using max length for each sentense is 200\n"}
{"snippet": "path = '../../datasets/climate'\nfilename = os.path.join(path, 'jena_climate_2009_2016.csv')\nwith open(filename) as fh:\n    lines = fh.read().split('\\n')\n    header = lines[:1][0].split(\",\")\n    lines = lines[1:]\n", "intent": "**Reading the Data**\n"}
{"snippet": "plot_points = np.hstack((X[:, :2], y.reshape(-1, 1)))\nplot_points = pd.DataFrame(plot_points)\n", "intent": "    Adding one more column in numpy array and converting it to pandas dataframe\n"}
{"snippet": "df = pd.read_csv('./data/all_stocks_5yr.csv.zip', compression='zip')\ndf.head()\n", "intent": "The data was downloaded from https://www.kaggle.com/camnugent/sandp500\n"}
{"snippet": "def answer_zero():\n    columns = np.append(cancer.feature_names, 'target');   \n    print(\"Features Column Size: \" + str(np.size(columns)))\n    index = pd.RangeIndex(start=0, stop=569, step=1);\n    data = np.column_stack((cancer.data, cancer.target))\n    print(\"Data Column Size: \" + str(np.size(data) / 569))\n    df = pd.DataFrame(data=data, index=index, columns=columns)\n    return len(cancer['feature_names'])\nanswer_zero() \n", "intent": "How many features does the breast cancer dataset have?\n*This function should return an integer.*\n"}
{"snippet": "sns.regplot('Latency', 'Throughput',\n           data=pd.DataFrame(X, columns=['Latency', 'Throughput']), \n           fit_reg=False,\n           scatter_kws={\"s\":20,\n                        \"alpha\":0.5})\n", "intent": "Visualize training data\n"}
{"snippet": "import pandas as pd\nhouses = pd.read_csv('cal_housing_clean.csv')\n", "intent": "** Import the cal_housing_clean.csv file with pandas. Separate it into a training (70%) and testing set(30%).**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_housing_data = scaler.fit_transform(housing.data)\nscaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n", "intent": "First we will use the standardscaler of sklearn to get a normalized dataset for the gradient descent. \n"}
{"snippet": "housing_prepared = full_pipeline.fit_transform(housing)\nhousing_prepared\n", "intent": "And you can run the whole pipeline simply:\n"}
{"snippet": "PCAdf3 = pd.DataFrame(X3, columns=['PC1','PC2','PC3'])\nj = PCAdf3.PC1\nk = PCAdf3.PC2\nl = PCAdf3.PC3\nPCAdf3.head()\n", "intent": "So it's mostly noise with a little signal in there somewhere. Maybe we need to look at the data a little differently...\n"}
{"snippet": "tf_idf_vectorizer = TfidfVectorizer(vocabulary = corpus_set)\nbow_tfidf = tf_idf_vectorizer.fit_transform(df.pureTextTweet)\nbow_tfidf.shape\n", "intent": "Bag-of-words where tf-idf count:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(train_files,train_targets ,test_size=0.3,random_state=42)\n", "intent": "Split Data into train and test\n"}
{"snippet": "def create_submission(predictions, test_id, loss):\n    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n    now = datetime.datetime.now()\n    if not os.path.isdir('subm'):\n        os.mkdir('subm')\n    suffix = str(round(loss, 6)) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n    sub_file = os.path.join('subm', 'submission_' + suffix + '.csv')\n    result1.to_csv(sub_file, index=False)\n", "intent": "Create submission format\n"}
{"snippet": "from sklearn.datasets import load_boston\nX,Y = load_boston(return_X_y = True)\n", "intent": "To make your life easier you can use the following to load your dataset (you need internet)\n"}
{"snippet": "path='./pycon-2016-tutorial/data/sms.tsv'\nsms = pd.read_table(path, header=None, names=['label', 'message'])\n", "intent": "* vect.fit(train)\n* vect.transform(train)\n* vect.transform(test)\n"}
{"snippet": "with open('anna.txt', 'r') as f:\n    text = f.read()\nvocab = set(text)\n", "intent": "Parse the text into a integer dictionary\n"}
{"snippet": "dfTrain = pd.read_csv(\"train.csv\") \ndfTest = pd.read_csv(\"test.csv\") \n", "intent": "Let's import train and test datasets:\n"}
{"snippet": "dfFull['CabinCat'] = dfFull.Cabin.str[0].fillna('Z')\ndfFull.loc[dfFull.CabinCat=='G','CabinCat']= 'Z'\ndfFull.loc[dfFull.CabinCat=='T','CabinCat']= 'Z'\ndfFull['CabinCat'] = dfFull['CabinCat'].map({'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'Z': 6}).astype(int)\ndfTrain = dfFull.loc[1:891,:]\ndfTest = dfFull.loc[892:,:]\ndfTrain.groupby('CabinCat').Survived.mean().plot(kind='bar')\n", "intent": "Now it is better. We have to move last two items to N/A and make 'Z' category:\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\ndocs = [{\"Mayur\": 1, \"is\": 1, \"awesome\": 2}, {\"No\": 1, \"I\": 1, \"dont\": 2, \"wanna\": 3, \"fall\": 1, \"in\": 2, \"love\": 3}]\ndv = DictVectorizer()\nX = dv.fit_transform(docs)\nprint(X.todense())\n", "intent": "DictVectorizer will convert mappings to vectors. \n"}
{"snippet": "cancel = pd.read_csv(\"../../projects-weekly/project-07/assets/airport_cancellations.csv\")\nops = pd.read_csv(\"../../projects-weekly/project-07/assets/Airport_operations.csv\")\nap = pd.read_csv(\"../../projects-weekly/project-07/assets/airports.csv\")\n", "intent": "PostgreSQL created in Navicat.\n"}
{"snippet": "bureau   = pd.read_csv(DATA_PATH + \"bureau.csv\")\nprint(\"Shape of bureau:\",    bureau.shape)\n", "intent": "---\n<a id=\"bureau\"></a>\n"}
{"snippet": "bureau_balance = pd.read_csv(DATA_PATH + \"bureau_balance.csv\")\nprint(\"Shape of bureau_balance:\",  bureau_balance.shape)\nprint(\"\\nColumns of bureau_balance:\")\nprint(\" --- \".join(bureau_balance.columns.values))\n", "intent": "<a id=\"bureau_bal\"></a>\n"}
{"snippet": "prev_app = pd.read_csv(DATA_PATH + \"previous_application.csv\")\nprint(\"Shape of prev_app:\",  prev_app.shape)\n", "intent": "---\n<a id=\"prev_app\"></a>\n"}
{"snippet": "pcb = pd.read_csv(DATA_PATH + \"POS_CASH_balance.csv\")\nprint(\"Shape of pcb:\",  pcb.shape)\nprint(\"\\nColumns of pcb:\")\nprint(\" --- \".join(pcb.columns.values))\n", "intent": "---\n<a id=\"pos_cash\"></a>\n"}
{"snippet": "for col in (\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\"):\n    pcb[col] = pcb[col].transform(lambda x: x.fillna(x.median()))\n", "intent": "<a id=\"pos_nan\"></a>\n"}
{"snippet": "install_pay = pd.read_csv(DATA_PATH + \"installments_payments.csv\")\nprint(\"Shape of install_pay:\",  install_pay.shape)\nprint(\"\\nColumns of install_pay:\")\nprint(\" --- \".join(install_pay.columns.values))\n", "intent": "---\n<a id=\"install_pay\"></a>\n"}
{"snippet": "for col in (\"DAYS_ENTRY_PAYMENT\", \"AMT_PAYMENT\"):\n    install_pay[col + \"_nan\"] = install_pay[col].map(lambda x: 1 if np.isnan(x) else 0)\n    install_pay[col] = install_pay[col].fillna(0)\n", "intent": "<a id=\"install_nan\"></a>\n"}
{"snippet": "credit_card = pd.read_csv(DATA_PATH + \"credit_card_balance.csv\")\nprint(\"Shape of credit_card:\",  credit_card.shape)\nprint(\"\\nColumns of credit_card:\")\nprint(\" --- \".join(credit_card.columns.values))\n", "intent": "---\n<a id=\"credit\"></a>\n"}
{"snippet": "train = pd.read_csv(DATA_PATH + \"train.csv\")\ntest  = pd.read_csv(DATA_PATH + \"test.csv\")\nprint(\"Shape of train:\", train.shape)\nprint(\"Shape of test:\",  test.shape)\n", "intent": "---\n<a id=\"final_merge\"></a>\n"}
{"snippet": "dfc_corr.to_csv('../../projects-weekly/project-07/assets/dfc_corr.csv')\n", "intent": "With the condensing of airports there are far more highly correlated pairs. I'm going to take this out into Excel to decide where to start pruning.\n"}
{"snippet": "cols = [\"DAYS_CREDIT_ENDDATE\", \"DAYS_ENDDATE_FACT\", \"AMT_CREDIT_MAX_OVERDUE\",\n        \"AMT_CREDIT_SUM\", \"AMT_CREDIT_SUM_DEBT\", \"AMT_CREDIT_SUM_LIMIT\",\n        \"AMT_ANNUITY\"]\nfor col in tqdm(cols):\n    bureau[col + \"_nan\"] = bureau[col].map(lambda x: 1 if np.isnan(x) else 0)\n    mode                 = bureau[bureau[col].notnull()][col].mode().iloc[0]\n    bureau[col]          = bureau[col].fillna(mode)\nsum(bureau.isnull().sum())\n", "intent": "<a id=\"bureau_nan\"></a>\n"}
{"snippet": "pd.DataFrame({\n    \"SK_ID_CURR\": test_id,\n    \"TARGET\": predictions\n}).to_csv(\"../submissions/old_method_mapped_max_days.csv\", index=False)\n", "intent": "---\n<a id=\"save\"></a>\n"}
{"snippet": "DATA_PATH = \"../../data/home_default/\"\nbureau   = pd.read_csv(DATA_PATH + \"bureau.csv\")\nprev_app = pd.read_csv(DATA_PATH + \"previous_application.csv\")\nprint(\"Shape of bureau:\",    bureau.shape)\nprint(\"Shape of prev_app:\",  prev_app.shape)\n", "intent": "---\n<a id=\"load\"></a>\n"}
{"snippet": "bureau   = pd.read_csv(DATA_PATH + \"bureau.csv\")\nprint(\"Shape of bureau:\", bureau.shape)\nprint(\"\\nColumns of bureau:\")\nprint(\" --- \".join(bureau.columns.values))\n", "intent": "---\n<a id=\"bureau\"></a>\n"}
{"snippet": "prev_app = pd.read_csv(DATA_PATH + \"previous_application.csv\")\nprint(\"Shape of prev_app:\",  prev_app.shape)\nprint(\"\\nColumns of prev_app:\")\nprint(\" --- \".join(prev_app.columns.values))\n", "intent": "---\n<a id=\"prev_app\"></a>\n"}
{"snippet": "for col in (\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\"):\n    pcb[col] = pcb[col].transform(lambda x: x.fillna(x.median()))\npcb = pcb.drop(pcb[pcb.NAME_CONTRACT_STATUS.isin([\"XNA\", \"Canceled\"])].index)\n", "intent": "<a id=\"pos_nan\"></a>\n"}
{"snippet": "pcb[\"nan\"] = np.zeros(len(pcb)).astype(int)\nfor col in (\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\"):\n    pcb[\"nan\"] += pcb[col].map(lambda x: 1 if np.isnan(x) else 0)\n    pcb[col]    = pcb[col].transform(lambda x: x.fillna(x.median()))\npcb = pcb.drop(pcb[pcb.NAME_CONTRACT_STATUS.isin([\"XNA\", \"Canceled\"])].index)\n", "intent": "<a id=\"pos_nan\"></a>\n"}
{"snippet": "install_pay[\"nan\"] = np.zeros(len(install_pay)).astype(int)\nfor col in (\"DAYS_ENTRY_PAYMENT\", \"AMT_PAYMENT\"):\n    install_pay[\"nan\"] = install_pay[col].map(lambda x: 1 if np.isnan(x) else 0)\n    install_pay[col]   = install_pay[col].fillna(0)\n", "intent": "<a id=\"install_nan\"></a>\n"}
{"snippet": "train = pd.read_csv(DATA_PATH + \"train.csv\")\ntest  = pd.read_csv(DATA_PATH + \"test.csv\")\nprint(\"Shape of train:\", train.shape)\nprint(\"Shape of test: \", test.shape)\n", "intent": "---\n<a id=\"final_merge\"></a>\n"}
{"snippet": "kml = pd.DataFrame(kmean.labels_)\n", "intent": "Block Delay is the gate-to-gate delay, so the biggest problem. Let's see what airports have the biggest and least problems with this metric.\n"}
{"snippet": "for col in (\"CNT_INSTALMENT\", \"CNT_INSTALMENT_FUTURE\"):\n    pcb[col] = pcb[col].transform(lambda x: x.fillna(x.median()))\npcb = pcb.drop(pcb[pcb.NAME_CONTRACT_STATUS.isin([\"XNA\", \"Canceled\"])].index)\n", "intent": "<a id=\"pos_process\"></a>\n"}
{"snippet": "for col in (\"DAYS_ENTRY_PAYMENT\", \"AMT_PAYMENT\"):\n    install_pay[col + \"_nan\"] = install_pay[col].map(lambda x: 1 if np.isnan(x) else 0)\n    install_pay[col] = install_pay[col].fillna(0)\n", "intent": "<a id=\"install_process\"></a>\n"}
{"snippet": "training_x, val_x, training_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=17)\nlgb_train = lgb.Dataset(data=training_x, label=training_y)\nlgb_eval  = lgb.Dataset(data=val_x, label=val_y)\nparams = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n          'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,\n          'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, \n          'min_split_gain':.01, 'min_child_weight':1}\nstart = time.time()\nmodel = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=200)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"feat_reduction\"></a>\n"}
{"snippet": "pd.DataFrame({\n    \"SK_ID_CURR\": test_id,\n    \"TARGET\": sub_preds\n}).to_csv(\"../submissions/lambda_40.csv\", index=False)\n", "intent": "<a id=\"final_pred\"></a>\n"}
{"snippet": "pd.DataFrame({\n    \"SK_ID_CURR\": test_id,\n    \"TARGET\": predictions\n}).to_csv(\"../submissions/not_classifier.csv\", index=False)\n", "intent": "<a id=\"final_pred\"></a>\n"}
{"snippet": "pd.DataFrame({\n    \"SK_ID_CURR\": test_id,\n    \"TARGET\": predictions\n}).to_csv(\"../submissions/more_tuning.csv\", index=False)\n", "intent": "<a id=\"final_pred\"></a>\n"}
{"snippet": "pd.DataFrame({\n    \"SK_ID_CURR\": test_id,\n    \"TARGET\": predictions_2\n}).to_csv(\"../submissions/dart.csv\", index=False)\n", "intent": "<a id=\"final_pred\"></a>\n"}
{"snippet": "pd.DataFrame({\n    \"SK_ID_CURR\": test_id,\n    \"TARGET\": predictions\n}).to_csv(\"../submissions/olivier_params.csv\", index=False)\n", "intent": "<a id=\"final_pred\"></a>\n"}
{"snippet": "bureau[[\"STATUS_0\", \"STATUS_1\", \"STATUS_2\", \"STATUS_3\", \"STATUS_4\", \"STATUS_5\", \"STATUS_C\", \"STATUS_X\"]] = (\n    bureau[[\"STATUS_0\", \"STATUS_1\", \"STATUS_2\", \"STATUS_3\", \"STATUS_4\", \"STATUS_5\", \"STATUS_C\", \"STATUS_X\"]]\n        .fillna(-1))\nbureau.isnull().sum()\n", "intent": "There won't be any effect if we replace NaN with 0\n"}
{"snippet": "memdf1 = pd.read_csv(\"../../projects-capstone/PSP/raw_data/PSP_MembershipData_1.csv\")\nmemdf2 = pd.read_csv(\"../../projects-capstone/PSP/raw_data/PSP_MembershipData_2.csv\")\n", "intent": "Step 2. Import Data\n"}
{"snippet": "bureau[[\"STATUS_0\", \"STATUS_1\", \"STATUS_2\", \"STATUS_3\", \"STATUS_4\", \"STATUS_5\", \"STATUS_C\", \"STATUS_X\"]] = (\n    bureau[[\"STATUS_0\", \"STATUS_1\", \"STATUS_2\", \"STATUS_3\", \"STATUS_4\", \"STATUS_5\", \"STATUS_C\", \"STATUS_X\"]]\n        .fillna(-1))\n", "intent": "There won't be any effect if we replace NaN with -1\nFIXME: make this simpler code\n"}
{"snippet": "full[\"no_prev_app\"] = full[merged_cols[0]].map(lambda x: 1 if np.isnan(x) else 0)\nfor col in merged_cols:\n    not_null  = full[col].notnull()\n    median    = full[not_null][col].median()\n    full[col] = full[col].fillna(median)    \n", "intent": "A load bar would be nice here\n"}
{"snippet": "credit_card = pd.read_csv(DATA_PATH + \"credit_card_balance.csv\")\nprint(\"Shape of credit_card:\",  credit_card.shape)\nprint(\"\\nColumns of credit_card:\")\nprint(\" --- \".join(credit_card.columns.values))\n", "intent": "---\n<a id=\"credit\"></a>\nFIXME: select (1) max months of credit history\n"}
{"snippet": "from scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nnumeric_feats = full.dtypes[full.dtypes != \"object\"].index\nskewed_feats = full[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewed_features = skewness[abs(skewness) > 0.75].index\nlam = 0.15\nfor feat in skewed_features:\n    full[feat] = boxcox1p(full[feat], lam)\n", "intent": "All of this skew analysis is thanks to Serginne\n"}
{"snippet": "ensemble = meta_model_pred*(1/10) + final_predictions['XGBoost']*(1.5/10) + final_predictions['Gradient Boosting']*(2/10) + final_predictions['Bayesian Ridge']*(1/10) + final_predictions['Lasso']*(1/10) + final_predictions['KernelRidge']*(1/10) + final_predictions['Lasso Lars IC']*(1/10) + final_predictions['Random Forest']*(1.5/10)\nsubmission = pd.DataFrame()\nsubmission['Id'] = test_ID\nsubmission['SalePrice'] = ensemble\nprint(\"Submission file, created!\")\n", "intent": "<a id='submission'></a>\n"}
{"snippet": "ensemble = meta_model_pred*(1/10) + final_predictions['XGBoost']*(1.5/10) + final_predictions['Gradient Boosting']*(2/10) + final_predictions['Bayesian Ridge']*(1/10) + final_predictions['Lasso']*(1/10) + final_predictions['KernelRidge']*(1/10) + final_predictions['Lasso Lars IC']*(1/10) + final_predictions['Random Forest']*(1.5/10)\nsubmission = pd.DataFrame()\nsubmission['Id'] = test_ID\nsubmission['SalePrice'] = ensemble\nsubmission.to_csv('../../submissions/patel_submission_tuned_params.csv',index=False)\nprint(\"Submission file, created!\")\n", "intent": "<a id='submission'></a>\n"}
{"snippet": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import Lasso\nfrom xgboost import XGBRegressor\nimport pandas as pd\nmaster_training_data = pd.read_csv('../data/house_prices/train.csv')\nmaster_testing_data = pd.read_csv('../data/house_prices/test.csv')\n", "intent": "^ Huge thank you to Steve DeLano for recommending Lasso!  Good to know what not to use\nhttps://www.youtube.com/watch?v=BHGNvgBATN0\n"}
{"snippet": "train.OutcomeSubtype = train.OutcomeSubtype.fillna(\"NaN\")\ntrain[\"Outcome\"] = train.OutcomeType + \" - \" + train.OutcomeSubtype\nprint(\"Number of unique OutcomeSubtypes: \", len(train.OutcomeSubtype.unique()))\nprint(\"Number of unique Outcomes: \", len(train.Outcome.unique()))\n", "intent": "After the missing values are filled in, let's recreate the Column 'Outcome' again\n"}
{"snippet": "training_data[\"Age\"].fillna(training_data.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\ntesting_data[\"Age\"].fillna(testing_data.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\n", "intent": "- It might help to add a column to the table saying which data I affected\n"}
{"snippet": "pca = decomposition.PCA(n_components=3)\ndelay = pca.fit_transform(X[[i for i in X.columns if \"delay\" in i]])\n", "intent": "- With 3 components we can explain 85% of the variance in the variables\n"}
{"snippet": "train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\ntest[\"Age\"].fillna(test.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\n", "intent": "- It might help to add a column to the table saying which data I affected\n"}
{"snippet": "for dataset in full:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n", "intent": "- This rubs me the wrong way though, I think another column should definitely be added\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVC\ntesting_data = pd.read_csv('../data/talking_data/test_reduced.csv')\n", "intent": "- Removing the device column\n- Using more data\n- Using SVC instead of XGBoost\n- Running the model on the reduced data (excluding month and year)\n"}
{"snippet": "start = dt.datetime.now()\nINPUT_SIZE = 224\nNUM_CLASSES = 16\nSEED = 1987\ndata_dir = '../../data/doge/'\nlabels = pd.read_csv(join(data_dir, 'labels.csv'))\nsample_submission = pd.read_csv(join(data_dir, 'sample_submission.csv'))\nprint(len(listdir(join(data_dir, 'train'))), len(labels))\nprint(len(listdir(join(data_dir, 'test'))), len(sample_submission))\n", "intent": "Using all the images would take more than the 1 hour kernel limit. Let's focus on the most frequent 16 breeds.\n"}
{"snippet": "drop_cols = list()\nfor col in train_x.columns:\n    unique_vals = train_x[col].unique()\n    if len(unique_vals) == 1:\n        drop_cols.append(col)\ntrain.drop(drop_cols, axis=1, inplace=True)\ntest.drop(drop_cols, axis=1, inplace=True)\ntrain.to_csv(\"../../data/santander/train_r.csv\")\ntest.to_csv(\"../../data/santander/test_r.csv\")\n", "intent": "<a id=\"boring\"></a>\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\ntraining_set = sc.fit_transform(training_set)\ntraining_set\n", "intent": "Scaling our features using normalization\n"}
{"snippet": "X_train = training_set[0:1257]\ny_train = training_set[1:1258]\ntoday = pd.DataFrame(X_train[0:5])\ntomorrow = pd.DataFrame(y_train[0:5])\nex = pd.concat([today, tomorrow], axis=1)\nex.columns = (['today', 'tomorrow'])\nex\n", "intent": "Now we split our stock prices by shifting the cells one block. That way, the input would be one day and the output would be the very next day.\n"}
{"snippet": "columns = ['item_id', 'movie title', 'release date', 'video release date', 'IMDb URL', 'unknown', 'Action', 'Adventure',\n          'Animation', 'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',\n          'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies = pd.read_csv('ml-100k/u.item', sep='|', names=columns, encoding='latin-1')\nmovie_names = movies[['item_id', 'movie title']]\nmovie_names.head()\n", "intent": "Note count\t100000.00000 from 1.7M movies is very inefficient\n"}
{"snippet": "scaler = preprocessing.StandardScaler()\ndf3 = pd.DataFrame(df2['Average'], index = df2.index)\ndf3 = df3.to_period(freq='M')\ndf3['Average'] = scaler.fit_transform(df3['Average'])\ndf4 = pd.DataFrame(df['Nacional'], index = df.index)\ndf4 = df4.to_period(freq='M')\ndf4['Nacional'] = scaler.fit_transform(df4['Nacional'])\ndf5 = df4.join(df3,rsuffix='_y')\ndf5.plot()\n", "intent": ">JB Pork shot up in 2013 and stayed high in 2014. However, looks like corn dropped during that time! What's your hypothesis in light of this?\n"}
{"snippet": "pca2 = decomposition.PCA(n_components=1)\ncandd = pca2.fit_transform(X[[i for i in X.columns if \"cancel\" in i or \"diver\" in i]])\n", "intent": "- With just 1 component we can explain more than 80% of the variance in the variables\n"}
{"snippet": "X, Y = datasets.make_blobs(centers=4, cluster_std=0.5, random_state=1)\n", "intent": "For our first example, let's create some synthetic easy-to-cluster data:\n"}
{"snippet": "pipeline = Pipeline((\n    ('vec', TfidfVectorizer(max_df = 0.8, ngram_range = (1, 2), use_idf=True)),\n    ('clf', MultinomialNB(alpha = 0.001)),\n))\n_ = pipeline.fit(twenty_train_small.data, twenty_train_small.target)\n", "intent": "Let's fit a model on the small dataset and collect info on the fitted components:\n"}
{"snippet": "pca_dg = PCA(2)\nX_proj = pca_dg.fit_transform(X)\nprint np.sum(pca_dg.explained_variance_ratio_)\n", "intent": "Therefore, need 29 components from 64 original features to explain 95% of original data\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(x, y, test_size=0.166666, random_state=1)\nprint 'The shape of the x train data set is: ', X_train.shape\nprint 'The shape of the y train data set is: ', y_train.shape\nprint 'The shape of the x validation data set is: ', X_validation.shape\nprint 'The shape of the y validation data set is: ', y_validation.shape\n", "intent": "Split the training set into two pieces - a training set of size 50000, and a separate validation set of size 10000. Also load in the test data.\n"}
{"snippet": "wine = pd.read_csv('wine_original.csv')\nprint(wine.shape)\nwine.head()\n", "intent": "For the questions in this section, load the wine dataset (wine original.csv).\n"}
{"snippet": "from sklearn.decomposition import PCA\ndf_phi_w = pd.DataFrame(phi_w, index=V, columns=C)\npca = PCA(n_components=100)\npca_phi_w = pca.fit_transform(phi_w)\npca_phi_w = pd.DataFrame(pca_phi_w, index=V)\nprint('For PCA(100), ', np.sum(pca.explained_variance_ratio_)* 100.0, '% of the variance is explained.')\n", "intent": "5.Suppose we want a 100-dimensional representation. How would you achieve this?\n"}
{"snippet": "df_test = pd.read_csv(\"../datasets/titanic_test.csv\")\ndf_test.head()\n", "intent": "<i>Read test data, for which we will make predictions</i>\n"}
{"snippet": "d = {'Survived': y_pred}\ndf_to_submit = pd.DataFrame(data=d, index=df_test[\"PassengerId\"])\ndf_to_submit.head()\n", "intent": "*Form a dataset and write it to a file*\n"}
{"snippet": "logins = pd.read_json('logins.json')\nlogins.info()\nlogins.head(10)\n", "intent": "\"logins.json\" file contains (simulated) timestamps of user logins in a particular geographic location\n"}
{"snippet": "pca3 = decomposition.PCA(n_components=3)\nX_decomp = pca3.fit_transform(X)\n", "intent": "- With 3 components we can explain more than 80% of the variance in the variables\n"}
{"snippet": "means_player = pd.DataFrame(playerstats.groupby('yearID')['1B_PA','2B_PA','3B_PA','HR_PA','BB_PA'].mean().reset_index())\nplayerstats_norm = pd.merge(left=playerstats, right=means_player,how='left',left_on='yearID',right_on='yearID')\nplayerstats_norm['1B_norm'] = playerstats_norm['1B_PA_x']-playerstats_norm['1B_PA_y']\nplayerstats_norm['2B_norm'] = playerstats_norm['2B_PA_x']-playerstats_norm['2B_PA_y']\nplayerstats_norm['3B_norm'] = playerstats_norm['3B_PA_x']-playerstats_norm['3B_PA_y']\nplayerstats_norm['HR_norm'] = playerstats_norm['HR_PA_x']-playerstats_norm['HR_PA_y']\nplayerstats_norm['BB_norm'] = playerstats_norm['BB_PA_x']-playerstats_norm['BB_PA_y']\nmeans_player.head()\nplayerstats_norm = playerstats_norm[['teamID','yearID','W','L','playerID','1B_norm','2B_norm','3B_norm','HR_norm','BB_norm']]\nplayerstats_norm.head()\n", "intent": "Show the head of the `playerstats` DataFrame. \n"}
{"snippet": "from scipy.stats import mode\npos_mode = fielding[fielding['yearID'] > 1946].groupby('playerID')['POS'].apply(mode).apply(lambda x: x[0][0])\npositions = pd.DataFrame(pos_mode)\npositions.columns = ['POS']\npos_sal = medianSalaries.merge(positions.reset_index(), on='playerID')\npos_sal.head()\nplayerLS_SAL_POS = playerLS.merge(pos_sal, on='playerID')\nplayerLS_SAL_POS.head() \n", "intent": "Show the head of the `playerLS` DataFrame. \n"}
{"snippet": "print F10.grid_scores_[1:10]\nk_tested = pd.DataFrame(F10.grid_scores_)\nk_tested.head()\n", "intent": "Visualize the result by plotting the score results verus values for $k$. \n"}
{"snippet": "green = pd.read_csv(\"green_taxi_cleaned.csv\")\n", "intent": "Report mean and median trip distance by hour of day\n"}
{"snippet": "green = pd.read_csv(\"green_taxi_cleaned.csv\")\ngreen[\"lpep_pickup_datetime\"] = pd.to_datetime(green[\"lpep_pickup_datetime\"])\ngreen[\"Lpep_dropoff_datetime\"] = pd.to_datetime(green[\"Lpep_dropoff_datetime\"])\n", "intent": "We derive the `tip_percentage` as a percentage of `Total_amount`\n"}
{"snippet": "sample.to_csv(\"sample.csv\", index=False)\n", "intent": "In order to maintain consistency of analysis we save the sample to disk.\n"}
{"snippet": "green = pd.read_csv(\"green_taxi_cleaned.csv\")\ngreen[\"lpep_pickup_datetime\"] = pd.to_datetime(green[\"lpep_pickup_datetime\"])\ngreen[\"Lpep_dropoff_datetime\"] = pd.to_datetime(green[\"Lpep_dropoff_datetime\"])\n", "intent": "We'll start by compressing the data to the average `Trip_distance` and average `Fare_amount` per hour over the course of the month.\n"}
{"snippet": "trend_in_dollars = trip_forecast[\"trend\"] * (1/24.8) * 2.27\ncost_revenue = pd.DataFrame()\ncost_revenue[\"cost\"] = trend_in_dollars\ncost_revenue[\"revenue\"] = fare_forecast[\"trend\"]\ncost_revenue.index = fare_forecast[\"ds\"]\n", "intent": "Fortunately our fare trend is already in dollars.  So we can simply plot the fare trend line against our other line.\n"}
{"snippet": "green = pd.read_csv(\"green_with_hour.csv\")\ngreen[\"lpep_pickup_datetime\"] = pd.to_datetime(green[\"lpep_pickup_datetime\"])\ngreen[\"Lpep_dropoff_datetime\"] = pd.to_datetime(green[\"Lpep_dropoff_datetime\"])\n", "intent": "We'll start by compressing the data to the average `Trip_distance` and average `Fare_amount` per hour over the course of the month.\n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = cifar10.load_data() \n", "intent": "Next, we can load the CIFAR-10 data set.\n"}
{"snippet": "with open('input.txt', 'r') as f:\n    read_data = f.read()\n    print read_data[0:100]\nf.closed\n", "intent": "We download the input file, and print a part of it:\n"}
{"snippet": "ratings_df = pd.read_csv('/resources/data/ml-1m/ratings.dat', sep='::', header=None)\nratings_df.head()\n", "intent": "We can do the same for the ratings.dat file:\n"}
{"snippet": "car_images = []\nnot_images = []\ncars = [\"25.png\", \"31.png\", \"53.png\"]\nnot_cars = [\"2.png\", \"3.png\", \"8.png\"]\nfor car in cars:\n    car_images.append(mpimg.imread('test_images/' + car))\nfor not_car in not_cars:\n    not_images.append(mpimg.imread('test_images/' + not_car))\n", "intent": "For each color space, the top three images are cars, and the bottom three are non-cars.  The car and non-car images are shown below:\n"}
{"snippet": "coeff_df = pd.DataFrame(new_train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)\n", "intent": "We can use the coefficients of the logistic regression to verify that our engineered features are good artificial features\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33)\nknn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(X_train, y_train)\n", "intent": "**Split the data into 66% training and 33% testing. Fit a KNN classifier with `n_neighbors=25` on the training data.**\n"}
{"snippet": "conmat = confusion_matrix(y_test, y_pred)\nconfusion = pd.DataFrame(conmat, \n                         index=['is_healthy', 'is_cancer'],\n                         columns=['predicted_healthy', 'predicted_cancer'])\nconfusion\n", "intent": "**Create the confusion matrix for your classfier's performance on the test set.**\n"}
{"snippet": "ss= StandardScaler()\nX = ss.fit_transform(df)\n", "intent": "We'll begin by standardizing the data:\n"}
{"snippet": "cv = CountVectorizer(preprocessor=cleaner)\ncustom_preprocess = pd.DataFrame(cv.fit_transform(space_messages).todense(),\n                                columns=cv.get_feature_names())\nprint(custom_preprocess.sum().sort_values(ascending=False).head(10))\n", "intent": "We can pass this function into `CountVectorizer` as a way to preprocess the text as a part of the fitting.\n"}
{"snippet": "demo_noage_ss = pd.DataFrame(demo_noage_ss, columns=['health', 'income', 'stress'])\n", "intent": "<a id=\"corr\"></a>\nWe will be using the correlation matrix to calculate the eigenvectors and eigenvalues.\n---\n"}
{"snippet": "num_samples = 500 * 1000\nnum_features = 40\nX, y = make_classification(n_samples=num_samples, n_features=num_features)\n", "intent": "First we create a training set of size num_samples and num_features.\n"}
{"snippet": "strategy = 'median' \nage_imputer = Imputer(strategy=strategy)\nage_imputer.fit(df['age'].values.reshape(-1, 1))\nages = age_imputer.transform(\n    df['age'].values.reshape(-1, 1))\nprint(ages[0:5], ages.mean())\n", "intent": "Our results are as follows:\n- Mean: 16.65\n- Median: 17\n- Mode: 16.0\nWhat is most appropriate? Check in on Slack\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n", "intent": "Use `train_test_split()` to create a training set and a test set, split 50/50\n"}
{"snippet": "pca = PCA(n_components= 2)\npca.fit(X_train)\nx_train_pca = pca.transform(X_train)\n", "intent": "ALL: 0  \nAML: 1 higher 'D49818_at', 'M23161_at', 'hum_alu_at', 'AFFX-PheX-5_at'\n"}
{"snippet": "df2_mean = df2.copy()\nfor column in df2.columns:\n    df2_mean[column] = df2[column].fillna(df2_drop[column].mean())\n", "intent": "So tpr is also 1 in since overall accuracy is 1\n"}
{"snippet": "df_s = pd.read_csv('data/security.csv')\ndf_s.head()\n", "intent": "2. Adding one more features\nsecurity\nsource: http://data.ap.org/projects/2016/airport-security-breaches/\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\ndata.sample(n=6)\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nitrain, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask=np.ones(critics.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\n", "intent": "Lets set up the train and test masks first:\n"}
{"snippet": "import pandas as pd\ntracks = pd.read_csv('Data/fma-rock-vs-hiphop.csv')\nechonest_metrics = pd.read_json('Data/echonest-metrics.json')\n", "intent": "First import `pandas` and use `read_csv` and `read_json` to read in the files `fma-rock-vs-hiphop.csv` and `echonest-metrics.json`.\n"}
{"snippet": "data = pd.read_csv('../datasets/10-houses.csv')\n", "intent": "Load a subset of the housing data\n"}
{"snippet": "n_samples = 200000\nn_features = 20\nX, y = make_classification(n_samples=n_samples, n_features=n_features)\n", "intent": "First we create a training set of size n_samples containing n_features each.\n"}
{"snippet": "df_predsFiltered.to_csv(\"solutions.csv\",index =False)\n", "intent": "We can finally save the list of predictions in the format requested by the test\n"}
{"snippet": "df = pd.read_csv('HR_comma_sep.csv')\n", "intent": "To import the dataset we will use Pandas library.It is the best Python library to play with the dataset and has a lot of functionalities. \n"}
{"snippet": "from sklearn.decomposition import PCA \nsklearn_pca = PCA(n_components=6)\nY_sklearn = sklearn_pca.fit_transform(X_std)\n", "intent": "The above plot shows almost 90% variance by the first 6 components. Therfore we can drop 7th component.\n"}
{"snippet": "np.random.seed(9001)\ndf = pd.read_csv('dataset_hw5.csv')\nmsk = np.random.rand(len(df)) < 0.5\ndata_train = df[msk]\ndata_test = df[~msk]\n", "intent": "1. First step is to  split  the observations into an approximate 50-50 train-test split.  Below is some code to do this for \n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"e:/datascience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n", "intent": "We can convert the categorical titles to ordinal.\n"}
{"snippet": "import pandas as pd\nmelbourne_file_path = '../../input/melbourne-housing-snapshot/melb_data.csv'\nmelbourne_data = pd.read_csv(melbourne_file_path) \nfiltered_melbourne_data = melbourne_data.dropna(axis=0)\ny = filtered_melbourne_data.Price\nmelbourne_predictors = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n                        'YearBuilt', 'Lattitude', 'Longtitude']\nX = filtered_melbourne_data[melbourne_predictors]\nfrom sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\n", "intent": "The data is loaded into **train_X**, **val_X**, **train_y** and **val_y** using the code you've already seen (and which you've already written).\n"}
{"snippet": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = pd.read_csv('../../input/melbourne-housing-snapshot/melb_data.csv')\ncols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = data[cols_to_use]\ny = data.Price\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\n", "intent": "We won't focus on the data loading. For now, you can imagine you are at a point where you already have train_X, test_X, train_y and test_y. \n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nencoder = OneHotEncoder()\ny = encoder.fit_transform(y).toarray()\nprint np.max(X)\nX = 1. * X / np.max(X)  \n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\nnames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\ndataset = pandas.read_csv(url, names=names)\n", "intent": "First we load the iris data from task 1 and split it into training and validation set.\n"}
{"snippet": "ultimate_df['phone'].fillna('iPhone', inplace=True)\n", "intent": "Seeing as iPhone makes up nearly 70% of the phones and only 0.8% of the data is missing, it may be a good assumption to fill these values as iPhones.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(ultimate_pred_df,\n                                                    ultimate_tar_df,\n                                                   test_size = 0.3,\n                                                   random_state=24,\n                                                   stratify = ultimate_tar_df)\n", "intent": "Now that we have an idea of what model to use, we can split the data into training and test sets.\n"}
{"snippet": "dataset = pd.read_csv('letter-recognition.data',delimiter=',',header=None)\ndataset.head()\n", "intent": "<h4> Import and pre-process the dataset</h4>\n"}
{"snippet": "dataset = pd.read_csv('fashion-mnist_train.csv')\ndataset = dataset.sample(frac=data_sampling_rate) \nnum_classes = 10\nclasses = {0: \"T-shirt/top\", 1:\"Trouser\", 2: \"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle boot\"}\ndisplay(dataset.head())\n", "intent": "Load the dataset and explore it.\n"}
{"snippet": "X_train_plus_valid, X_test, y_train_plus_valid, y_test \\\n    = train_test_split(X, Y, random_state=0, \\\n                                    train_size = 0.7)\nX_train, X_valid, y_train, y_valid \\\n    = train_test_split(X_train_plus_valid, \\\n                                        y_train_plus_valid, \\\n                                        random_state=0, \\\n                                        train_size = 0.5/0.7)\n", "intent": "Split the data into a **training set**, a **vaidation set**, and a **test set**\n"}
{"snippet": "from sklearn.datasets.samples_generator import make_blobs\nX, y = make_blobs(n_samples=200, centers=4, random_state=0, cluster_std=0.60)\nprint(X)\n", "intent": "First, let's try creating an artificial dataset with 200 items which we will use to test out partitional clustering.\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\ndata = iris.data\ntarget = iris.target\n", "intent": "In our second example, we will apply k-means to the Iris dataset for *k=3* clusters.\n"}
{"snippet": "from sklearn.datasets.samples_generator import make_blobs\nX, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.80)\n", "intent": "First, create a small artificial dataset to test with...\n"}
{"snippet": "import numpy as np\nfrom sklearn import cross_validation\nfrom sklearn import datasets\nfrom sklearn import svm\niris = datasets.load_iris()\n", "intent": "Let's revisit the Iris data set:\n"}
{"snippet": "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\ndataset = pandas.read_csv(url)\n", "intent": "a) We load the breast cancer data set. \n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"Preprocessing_Linear_Regression_Lab.csv\")\n", "intent": "Import \"Preprocessing and Linear Regression Dataset.csv\"\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"Logistic_Regression_Lab.csv\",index_col=False)\n", "intent": "Import \"Logistic Regression Lab.csv\". \n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"Decision_Tree_Lab.csv\",index_col=False)\n", "intent": "Import \"Decision Tree Lab.csv\".\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('Feature Selection Lab.csv')\n", "intent": "Import \"Feature Selection Lab.csv\". \n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "After feature elimination, try fitting linear regression again to see if you have improved or hurt your model performance.\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ncategories = ['comp.graphics', 'rec.motorcycles', 'sci.space', 'talk.politics.mideast', 'talk.religion.misc']\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\ndataset = fetch_20newsgroups(subset='all', categories=categories,\n                             shuffle=True, random_state=42)\n", "intent": "First let's load 20newsgroup data which contain newsletter articles + news categorical labels. We picked 5 news group to to this exercise.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)\nprint(x_train.shape)\nprint(x_test.shape)\n", "intent": "Let's split data into train and test sets.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('twitter-hate-speech.csv', index_col=0)\n", "intent": "We start by loading the dataset, for this we use [Pandas](https://pandas.pydata.org/)\n"}
{"snippet": "vectorizer = CountVectorizer(min_df=10, strip_accents='unicode', analyzer='word',\n                             tokenizer=preprocess_string, stop_words='english')\nX_preprocessed = vectorizer.fit_transform(df['tweet'].values)\n", "intent": "In the examples above we haven't yet used our preprocessing logic, we had just split up words as default. Let's do this now:\n"}
{"snippet": "array = dataset.values\nX = array[:,[0] + list(range(2,32))]\nle = LabelEncoder()\nle.fit([\"M\", \"B\"])\ny = le.transform(array[:,1]) \n", "intent": "b) We split the data into features X and labels y. After that we transform the binary labels to numerical values.\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\noneHot = OneHotEncoder()\noneHot.fit(y.reshape(len(y), -1))\ny = oneHot.transform(y.reshape(len(y), -1))\n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nsample_X, sample_y = shuffle(X, y)\nX_train, X_test, y_train, y_test = train_test_split(sample_X, sample_y.toarray(), \n                                                    test_size = 0.3, stratify = sample_y.toarray())\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nprint boston.data.shape\nprint boston.feature_names\nprint np.max(boston.target), np.min(boston.target), np.mean(boston.target)\nprint boston.DESCR\n", "intent": "Import the Boston House Pricing Dataset (http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html), and show their features.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.25, random_state=33)\n", "intent": "Build, as usual, training and testing sets:\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscalerX = StandardScaler().fit(X_train)\nscalery = StandardScaler().fit(y_train)\nX_train = scalerX.transform(X_train)\ny_train = scalery.transform(y_train)\nX_test = scalerX.transform(X_test)\ny_test = scalery.transform(y_test)\nprint np.max(X_train), np.min(X_train), np.mean(X_train), np.max(y_train), np.min(y_train), np.mean(y_train)\n", "intent": "In regression tasks, is very important to normalize data (to avoid that large-valued features weight too much in the final result)\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\nboston = load_boston()\nX = boston.data\ny = boston.target\n", "intent": "Let's peek into the `scikit-learn` API.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"../Documents/DSI-SF-3/datasets/sacramento_real_estate/Sacramentorealestatetransactions.csv\")\n", "intent": "We did this in our previous lab.\n"}
{"snippet": "bcw = pd.read_csv('../../../datasets/breast_cancer_wisconsin/wdbc.data', \n                  header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "ss = StandardScaler()\nXn = ss.fit_transform(X_prop)\nprint Xn.shape\n", "intent": "---\nAlways a necessary step when performing regularization.\n"}
{"snippet": "random_state = 1 \ntest_size = 0.20\ntrain_size = 0.80\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n                                                    train_size=train_size, random_state=random_state)\n", "intent": "c) Next we split the data into a training and a validation set.\n"}
{"snippet": "messages = pandas.read_csv('/Users/Javier/Desktop/DSI-2-TEACH/DSI-SF/datasets/smsspamcollection/SMSSpamCollection', sep='\\t', \n                           quoting=csv.QUOTE_NONE,\n                           names=[\"label\", \"message\"])\nmessages.head(3)\n", "intent": "Instead of parsing TSV (or CSV, or Excel...) files by hand, we can use Python's `pandas` library to do the work for us:\n"}
{"snippet": "listings = pd.read_csv(\"../input/listings.csv\")\n", "intent": "First, let's boot up and examine our data. Since our data comes in a simple CSV file, we load it into a `pandas` `DataFrame`.\n"}
{"snippet": "boston['BNBDensity_Houses'] = boston['NAMELSAD10'].map(houses).fillna(0)\nboston['BNBDensity_Apartments'] = boston['NAMELSAD10'].map(apartments).fillna(0)\n", "intent": "Let's create numerical variables counting how many of each we have per census tract.\n"}
{"snippet": "with open(\"synset_words.txt\") as fp:\n    synset_contents = fp.read()\nclass_labels_data = [{\n    \"line\": index, \n    \"id\": line[0:9], \n    \"label\": line[10:]\n} for index, line in enumerate(synset_contents.split(\"\\n\"))\n]\nvgg_labels = pd.DataFrame(class_labels_data)\n", "intent": "> _This was actually kind of hard to find but these labels should match up to the class prediction as ordinals._\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\npu_df_work_scaled = scaler.fit_transform(pu_df_work)\npu_df_work_scaled = pd.DataFrame(pu_df_work_scaled, columns = (\"Pu239\", \"Pu240\", \"label\"))\npu_df_work_scaled\n", "intent": "<a id='scaling'></a>\n"}
{"snippet": "train_users = pd.read_csv('../cache/train_users.csv')\n", "intent": "Load only the users with known destination\n"}
{"snippet": "train_users.fillna(-1, inplace=True)\n", "intent": "Replace NaN values with -1. \n"}
{"snippet": "y_train = train_users['country_destination']\ntrain_users.drop(['country_destination', 'id'], axis=1, inplace=True)\nx_train = train_users.values\nlabel_encoder = LabelEncoder()\nencoded_y_train = label_encoder.fit_transform(y_train)\n", "intent": "Select proper X and y. The labels should be encoded into integers to be usable by `XGBoost`:\n"}
{"snippet": "train_X, valid_X, train_label, valid_label = train_test_split(train_X, train_Y_one_hot, \n                                                           test_size=0.2, random_state=13)\n", "intent": "For the train further split to <code>train</code> and <code>validate</code>\n"}
{"snippet": "scaler = StandardScaler()\npca = PCA(n_components=2)\nlogistic = LogisticRegression(random_state=1)\npipeline = Pipeline(steps=[('StandardScaler', scaler), ('PCA', pca),\n                           ('LogisticRegression', logistic)])\npipeline.fit(X_train, y_train)\n", "intent": "d) Now we set up and train a pipeline which contains a scaler, dimensionality reduction and a classificator. \n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\ndf['weight_mms'] = mms.fit_transform(df[['Weight']])\ndf['Height_mms'] = mms.fit_transform(df[['Height']])\ndf.describe().round(2)\n", "intent": "2) MinMax normalization :\nscales exactly b/e 0 & 1\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\ndf['weight_ss'] = ss.fit_transform(df[['Weight']])\ndf['Height_ss'] = ss.fit_transform(df[['Height']])\ndf.describe().round(2)\n", "intent": "3) standard normalization :\nscales as mean = 0 & standard deviation = 1\n"}
{"snippet": "df = pd.read_csv(\"HR_comma_sep.csv\")\n", "intent": "1.load the dataset at HR_comma_sep.csv, inspect it with `.head()`, `.info()` and `.describe()`.\n"}
{"snippet": "cluster_2_rows_customers = np.where(customers_prediction == 2)[0]\ncluster_2_rows_general = np.where(predict_general == 2)[0]\ncustomers_undo_pca = pca.inverse_transform(customers_pca[cluster_2_rows_customers])\ncustomers_unscaled = scaler.inverse_transform(customers_undo_pca).round()\ndf = pd.DataFrame(customers_unscaled, columns = customer_columns)\n", "intent": "Note that cluster 2 is vastly overrepresented in the customer data compared to the general population. \n"}
{"snippet": "cluster_5_rows_customers = np.where(customers_prediction == 5)[0]\ncluster_5_rows_general = np.where(predict_general == 5)[0]\ncustomers_undo_pca_2 = pca.inverse_transform(customers_pca[cluster_5_rows_customers])\ncustomers_unscaled_2 = scaler.inverse_transform(customers_undo_pca_2).round()\ndf_2 = pd.DataFrame(customers_unscaled_2, columns = customer_columns)\n", "intent": "Note that cluster 5 is vastly underrepresented in the customer data compared to the general population. \n"}
{"snippet": "data = pd.read_csv('https://serv.cusp.nyu.edu/~cq299/ADS2016/Data/ExtraCredit1.csv')\n", "intent": "- $y = a_1x_1 + b_1$  \n- $y = a_2x_2 + b_2$  \n- $y = a_3x_3 + b_3$\n"}
{"snippet": "path = 'https://serv.cusp.nyu.edu/~cq299/ADS2016/Data/Bayesian/'\ndata4=pd.read_csv(path + \"example4.csv\", low_memory=False)\nlist_311=list(data4.loc[:,\"Adopt A Basket\":].columns)\ndepend_variable=['mean_log','gross_sq_feet_log']+list_311\ndata4['sale_price_log']=np.log(data4['sale_price']).round(decimals=3)\ndata4['gross_sq_feet_log']=np.log(data4['gross_sq_feet']).round(decimals=3)\ndata4['mean_log']=np.log(data4['mean']).round(decimals=3)\nX=data4[depend_variable]\ny=data4['sale_price_log']\n", "intent": "(For simplifing this question, let us ignore the over fitting problem now.)\n"}
{"snippet": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata2 = pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/ML_2017/Q2.csv\")\ntrain, test = train_test_split(data2, random_state=9, test_size=0.4)\n", "intent": "In this question, we use data: \"Q2.csv\" for Bayesian Network Learning.\n"}
{"snippet": "test = \"?,4,93,weighty\\n?,8,70,light\\n?,6,113,medium\\n?,6,95,weighty\\n?,4,115,medium\"\ntest = pd.read_csv(StringIO(unicode(\"MPG,cylinders,HP,weight\\n\" + test)))\n", "intent": "?,4,93,weighty ?,8,70,light ?,6,113,medium ?,6,95,weighty ?,4,115,medium\n"}
{"snippet": "scaler = StandardScaler()\nlogistic = LogisticRegression(random_state=1)\nrfe = RFECV(logistic, scoring='accuracy')\npipeline = Pipeline(steps=[('StandardScaler', scaler), ('rfe', rfe),\n                           ('LogisticRegression', logistic)])\npipeline.fit(X_train, y_train)\n", "intent": "f) Now we use RFE instead of PCA for feature selection. \n"}
{"snippet": "nd.imread(\"esb.jpg\")\n", "intent": "each pixel in the image has a value associated to it. 0 is black, 254 is white if it is an 8-bit image.\n"}
{"snippet": "iris = datasets.load_iris()\ni_data = iris[\"data\"][:,:2]\ntarget = iris[\"target\"]\ntarget = [0 if t == 0 else 1 for t in target]\ndata = MinMaxScaler().fit_transform(i_data)\ntrain_X, test_X, train_y, test_y = train_test_split(data, target, test_size = 0.25, random_state=33)\nN, M = train_X.shape\n", "intent": "<h2>Data Preprocessing </h2>\n"}
{"snippet": "iris = load_iris()\ndata = iris[\"data\"]\nlabels = iris[\"target\"]\nlabels = [0 if target == 0 else 1 for target in  iris.target]\nnormalised_data = MinMaxScaler().fit_transform(data) \ntrain_X, test_X, train_y, test_y = train_test_split(normalised_data, labels, test_size = 0.25, random_state=33)\n", "intent": "<h2>Data Preprocessing </h2>\n"}
{"snippet": "celebrity = pd.read_csv('twitterusers_top_100_celebrity.csv')\ncelebrity['field'] = 'celebrity'\ndata = pd.read_csv('twitterusers_top_100_data.csv')\ndata['field'] = 'data'\ntech = pd.read_csv('twitterusers_top_100_tech.csv')\ntech['field'] = 'tech'\nsports = pd.read_csv('twitterusers_top_100_sports.csv')\nsports['field'] = 'sports'\n", "intent": "Pull the datasets for celebrity, tech, data, and sports top 100 users into pandas dataframes\n"}
{"snippet": "bigdf1 = pd.read_csv('twitter_bigdf_12072013.csv')\n", "intent": "Now that we've got the followers saved and compiled to a csv, we can append that to the dataset.\n"}
{"snippet": "newpd = pd.read_csv('twitter_bigdf_appended_cleanedtweets_averageperuser.csv')\ndef cleanup(x):\n    exclude = set(string.punctuation)\n    s = str(x)\n    s = ''.join(ch for ch in s if ch not in exclude)\n    return s\nnewpd['Tweet'] = newpd['Tweet'].map(lambda x: cleanup(x))\n", "intent": "Create a dataframe and a \"was_retweeted\" column to analyze.  Will try a few different splits.\n"}
{"snippet": "shared = pd.DataFrame(sharedfollowers, columns={'common', 'combo'})\nshared.head()\n", "intent": "Now we have a dataframe with each user combo and a list of their shared followers. Let's take a look.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"D:/DataScience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0) \n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "display(pd.DataFrame({'number_of_clusters':[2, 3, 4, 5, 10, 20, 50], 'silhouette_score': [0.4262, 0.3968, 0.3320, 0.3509, 0.3510, 0.3501, 0.3577    ]}))\n", "intent": "**Answer:**\nSilhouette scores for different number of clusters are displayed below\n"}
{"snippet": "train_data = pd.read_csv(\"../../data/raw/train.csv\")\ntrain_data['Dates'] = pd.to_datetime(train_data['Dates'])\ntest_data = pd.read_csv(\"../../data/raw/test.csv\")\ntest_data['Dates'] = pd.to_datetime(test_data['Dates'])\n", "intent": "First we load and explore the dataset a little.\n"}
{"snippet": "area_df = pd.read_csv(path, names=['State','Area'])\narea_df\n", "intent": "If we wanted to give the columns specific names, we would have to pass another parameter called `names`. We can also omit the header parameter.\n"}
{"snippet": "dates = pd.date_range('20160101', periods=6) \ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))\nprint(dates)\ndf\n", "intent": "We can select data both by their labels and by their position. \n"}
{"snippet": "df[\"preTestScore\"].fillna(df[\"preTestScore\"].mean(), inplace=True)\ndf\n", "intent": "`inplace=True` means that the changes are saved to `df` right away\n"}
{"snippet": "dataset = CreateDataSet(4)\ndf = pd.DataFrame(data=dataset, columns=['location','Status','CustomerCount','StatusDate'])\ndf.head(5)\n", "intent": "Are easy! and are akin to Excel.\n"}
{"snippet": "dataset = CreateDataSet(4)\ndf = pd.DataFrame(data=dataset, columns=['location','Status','CustomerCount','StatusDate'])\ndf.info()\n", "intent": "Are easy! and are akin to Excel.\n"}
{"snippet": "df4 = pd.DataFrame({'B': ['B2', 'B3', 'B6', 'B7'],\n                    'D': ['D2', 'D3', 'D6', 'D7'],\n                    'F': ['F2', 'F3', 'F6', 'F7']},)\ndf4\n", "intent": "What if the columns for both dataframes are not exactly the same (but with some overlaps) ?\n"}
{"snippet": "data = {'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'],\n        'year': [2012, 2012, 2013, 2014, 2014],\n        'reports': [4, 24, 31, 2, 3],\n        'coverage': [25, 94, 57, 62, 70]}\ndf = pd.DataFrame(data, index = ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'])\ndf\n", "intent": "apply() can apply a function along any axis of the dataframe\n"}
{"snippet": "spam_data = np.asarray(pd.read_csv(\"datasets/spambase_data.csv\", header = None))\n", "intent": "Lets load in the dataset and take a look at what we have.\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nmedical_data = pd.read_csv(\"datasets/balanced_cleaned_diabetes_data.csv\")\n", "intent": "Lets load in the dataset and take a look at what we have.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "Always execute all the rows here\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nsample_submission = pd.read_csv(\"sample_submission.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nsample_submission = pd.read_csv(\"sample_submission.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\ntrain = pd.read_csv(\"train.tsv\")\ntest = pd.read_csv(\"test.tsv\")\nsample_submission = pd.read_csv(\"sample_submission.tsv\")\n", "intent": "Victor Kantor, xead.wl@gmail.com\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\noh_enc = OneHotEncoder(sparse=False)\nall_labels = np.concatenate([train_labels, test_labels])\nprint len(all_labels)\nY = oh_enc.fit_transform(all_labels.reshape(-1, 1))\ntrain_labels = Y[:len(train_labels)]\ntest_labels = Y[len(train_labels):]\nprint len(train_labels), len(test_labels)\n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nprint X_train.shape,y_train.shape, X_test.shape, y_test.shape\nprint(\"The MNIST database has a training set of %d examples.\" % len(X_train))\nprint(\"The MNIST database has a test set of %d examples.\" % len(X_test))\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the MNIST database.\n"}
{"snippet": "train_df = pd.read_csv(\"feature_engineering/training_sources.csv\")\nX_train = np.array(train_df[[\"mean\", \"nobs\", \"duration\"]])\ny_train = np.array(train_df[\"Class\"])\n", "intent": "**Problem 2a**\nRead in the training set file, and create a feature vector `X` and label array `y`.\n"}
{"snippet": "from astropy.table import Table\nTable.read(\"irsa_catalog_WISE_iPTF14jg_search_results.tbl\", format=\"ipac\")\n", "intent": "**Problem 1b**\nUse [`Table.read()`](http://docs.astropy.org/en/stable/api/astropy.table.Table.html\n"}
{"snippet": "data = pd.read_csv('advertising.csv')\n", "intent": "**Read in the advertising.csv file and set it to a data frame called ad_data.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "def create_submission(probabilities):\n    submission = pd.DataFrame(probabilities, columns=list(le.classes_))\n    submission.insert(0, 'Id', range(0, len(submission)))\n    submission.to_csv(\"submission.csv\", index=False)\n", "intent": "Training MLP with sknn and grid search\n"}
{"snippet": "df = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n", "intent": "Using principal component analysis, we are trying to figure out what components explain the most variance of the dataset.\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "Scale our data so each feature has a single unit variance.\n"}
{"snippet": "df_feat = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n", "intent": "**We're trying to predict whether the tumor is malignant or benign.**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_validation = sc.transform(x_validation)\nx_test = sc.transform(x_test)\n", "intent": "To improve the performance of our model, we scale the dataset.\n"}
{"snippet": "source= '/Users/jamalbacchus/IdeaProjects/Capstone/Mini Projects/Cancer Treatment/data_files'\ndf_variants = pd.read_csv(source+'/training_variants').set_index('ID').reset_index()\ntest_variants_df = pd.read_csv(source+\"/test_variants\")\ndf_text = pd.read_csv(source+\"/training_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\ntest_text_df = pd.read_csv(source+\"/test_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\nprint(\"Train Variant\".ljust(15), df_variants.shape)\nprint(\"Train Text\".ljust(15), df_text.shape)\nprint(\"Test Variant\".ljust(15), test_variants_df.shape)\nprint(\"Test Text\".ljust(15), test_text_df.shape)\ndf_variants.head()\n", "intent": "Let's take a casual look at the *variants* data.\n"}
{"snippet": "source= 'C:\\\\Users\\\\Jameel shaik\\\\Documents\\\\Projects\\\\Personalized Medicine Redefining Cancer Treatment'\ndf_variants = pd.read_csv(source+'/training_variants').set_index('ID').reset_index()\ntest_variants_df = pd.read_csv(source+\"/test_variants\")\ndf_text = pd.read_csv(source+\"/training_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\ntest_text_df = pd.read_csv(source+\"/test_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\nprint(\"Train Variant\".ljust(15), df_variants.shape)\nprint(\"Train Text\".ljust(15), df_text.shape)\nprint(\"Test Variant\".ljust(15), test_variants_df.shape)\nprint(\"Test Text\".ljust(15), test_text_df.shape)\ndf_variants.head()\n", "intent": "Let's take a casual look at the *variants* data.\n"}
{"snippet": "data = pd.read_csv(\"College_Data\")\ndata.set_index('Unnamed: 0', inplace=True)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vec = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "X = count_vec.fit_transform(X)\nX\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "from io import StringIO  \nmovie_txt = requests.get('https://raw.github.com/cs109/cs109_data/master/movies.dat').text\nmovie_file = StringIO(movie_txt) \nmovies = pd.read_csv(movie_file, delimiter='\\t')\nmovies[['id', 'title', 'imdbID', 'year']].irow(0)\n", "intent": "Here's a chunk of the MovieLens Dataset:\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=101)\n", "intent": "**Split the data into testing and training datasets**\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nnumerical = ['budget', 'popularity', 'revenue', 'runtime', 'vote_count', 'release_date']\nfeatures_log_minmax_transform = pd.DataFrame(data = features_log_transformed[numerical])\nfeatures_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])\nfeatures_log_minmax_transform.describe()\n", "intent": "After applying feature scaling to data, we apply **normalization** to ensure that each feature is treated equally when applying supervised learners. \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfeatures_final = features_preprocessed.copy()\nX_train, X_test, y_train, y_test = train_test_split(features_final, \n                                                    target, \n                                                    test_size = 0.30, \n                                                    random_state = 0)\nprint \"Training set has {} samples.\".format(X_train.shape[0])\nprint \"Testing set has {} samples.\".format(X_test.shape[0])\n", "intent": "The 3227 data after preprocessing is split into training sets(2258) and testing sets(969) for model training.\n"}
{"snippet": "cvscore = {}\ncvscore['DT'] = results['Decision Tree']['cv_score']\ncvscore['Ada'] = results['Ada Boost']['cv_score'] \ncvscore['RF'] = results['Random Forest']['cv_score'] \ncvscore['XGB'] = results['XGBoost']['cv_score'] \ncvscore['GB'] = results['Gradient Boosting']['cv_score'] \ncvscore['XGBest'] = grid_results['XGB_Best']['cv_score']\ncvscore['GBest'] = grid_results['GB_Best']['cv_score'] \ndf = pd.DataFrame(cvscore.items(), columns=['model', 'cv_score'])\nsns.barplot(x='model', y='cv_score', data=df, palette=\"cubehelix\", errcolor=\".2\", edgecolor=\".2\");\n", "intent": "- **CV Score:** **XGBoost Best** > XGBoost > Gradient Boosting Best > Gradient Boosting > Ada Boost > Random Forest > Decision Tree\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfeatures_final = features_preprocessed.copy()\nX_train, X_test, y_train, y_test = train_test_split(features_final, \n                                                    target, \n                                                    test_size = 0.30, \n                                                    random_state = 0)\nprint (\"Training set has {} samples.\".format(X_train.shape[0]))\nprint (\"Testing set has {} samples.\".format(X_test.shape[0]))\n", "intent": "The 3227 data after preprocessing is split into training sets(2258) and testing sets(969) for model training.\n"}
{"snippet": "dataframe = pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "df = pd.read_csv(\"{}/lalonde.csv\".format(DATA_PATH))\ndf.set_index('id', drop=True, inplace=True)\n", "intent": "We start by loading the data in a dataframe.\n"}
{"snippet": "nino = pd.read_csv('../data/tao-all2.dat.gz', sep=' ', names=names, na_values='.', \n                   parse_dates=[[1,2,3]])\nnino.columns = [x.replace('.', '_').replace(' ', '_') for x in nino.columns]\nnino['air_temp_F'] = nino.air_temp_ * 9/5 + 32\nwind_cols = [x for x in nino.columns if x.endswith('winds')]\nfor c in wind_cols:\n    nino['{}_mph'.format(c)] = nino[c] * 2.237\npd.to_datetime(nino.date, format='%y%m%d')\nnino = nino.drop('obs', axis=1)\n", "intent": "* Using the nino dataset, see if you can predict what the temperature (``air_temp_F``) will be for the next day \n"}
{"snippet": "y_df = pd.DataFrame(y)\ny_df['pred'] = pred\ny_df['err'] = y_df.pred - y_df.close\n(y_df\n .plot(figsize=(14,10))\n)\n", "intent": "You can plot the actuals and the predicted values. It looks like our model does a pretty poor job\n"}
{"snippet": "selector = SelectKBest(f_regression, k=2).fit(x, y)\nbest_features = np.where(selector.get_support())[0]\nprint(best_features)\n", "intent": "Find the 2 attributes in X that best correlate with y\n"}
{"snippet": "X_ff_train, X_ff_test, y_ff_train, y_ff_test = model_selection.\\\n    train_test_split(X_ff_scaled, y_ff_scaled, test_size=.3,\n                     random_state=42)\n", "intent": "Split the data into test and training data. What is the score on the test data?\n"}
{"snippet": "mush_X = mush_df.iloc[:,1:]\nmush_y = mush_df.class_p\nmush_X_train, mush_X_test, mush_y_train, mush_y_test = model_selection.\\\n    train_test_split(mush_X, mush_y, test_size=.3, random_state=42)\nmush_dt = tree.DecisionTreeClassifier()\nmush_dt.fit(mush_X_train, mush_y_train)\nmush_dt.score(mush_X_test, mush_y_test)\n", "intent": "* Create a testing and training set \n* Check if the model generalizes to the testing set\n* Visualize the tree (if you have graphviz)\n"}
{"snippet": "ss_ff = preprocessing.StandardScaler()\nss_ff.fit(Xff)\nX_ff_scaled = ss_ff.transform(Xff)\nX_ff_scaled = pd.DataFrame(Xff, columns=Xff.columns)\ny_ff_scaled = log(ff.area)\n", "intent": "* Try scaling the input and using the log of the area and see if you get a better score.\n* Examine the coefficients\n"}
{"snippet": "scaled = pd.DataFrame(scaled_feat , columns= df.columns[:-1])\nscaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n", "intent": "Convert Title words to values from 0 to 5\n"}
{"snippet": "test_data['Fare'].fillna(test_data['Fare'].dropna().mode()[0], inplace=True)\ntest_data.head()\n", "intent": "Replacing the sigle Fare missing value in test_data with the mode (most common Fare value)\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n", "intent": "Time to standardize the variables.\n** Import StandardScaler from Scikit learn.**\n"}
{"snippet": "scaled_df = pd.DataFrame(scaler_trans,columns = df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "census_data = pd.read_csv(\"./data/census_demographics.csv\")\ncensus_data.head()\n", "intent": "We'll see an example of the concepts mentioned above by considering a linear regression problem. Let us load the census data set.\n"}
{"snippet": "popdata.to_csv(\"Popular_Datapoints.csv\")\npopdata.head()\n", "intent": "For each of these dataframes, store the data in a .csv for easy future use (running all this code takes a long time!), and display the head of each:\n"}
{"snippet": "popdata = pd.read_csv(\"Popular_Datapoints.csv\")\nunpopdata = pd.DataFrame.from_csv(\"Unpopular_Datapoints.csv\")\n", "intent": "Now, if we want to jump straight to the models without running all of the above, we can just upload the csvs! \n"}
{"snippet": "popdata = pd.read_csv(\"Popular_Datapoints.csv\")\nqualitative = pd.read_csv(\"Qualitative.csv\")\ntop100 = pd.read_csv(\"Top_100.csv\")\nmerged201314 = pd.merge(qualitative, top100)\n", "intent": "We want to now merge our popular dataframe with a previous one that we had created that incorporates various qualitative information.\n"}
{"snippet": "y = df['y']\nX = df.drop('y', 1)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.1, random_state=42)\n", "intent": "b) Set to be the y variable in the dataframe from a and X to be the remaining features.\n"}
{"snippet": "import pandas as pd\npd.set_option('display.max_columns',600)\ndf = pd.read_csv('../data/hw2data.csv')\ndf.head()\n", "intent": "a) Load in hw2data.csv from ../data into a pandas dataframe.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)  \n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "predictions_df.to_csv('house-prices-pred.csv', index=False, float_format='%.0f')\npredictions_int_df.to_csv('house-prices-pred-int.csv', index=False, float_format='%.0f')\n", "intent": "The max is a bit off.  Would be worth investigating in another project. \n"}
{"snippet": "scores_df = pd.DataFrame(test_scores)\nscores_df.sort_values(by='accuracy', ascending=False).head()\n", "intent": "Had time to have a short bath in the Aare and even to make myself an espresso. \n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=48)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "pca1 = PCA(n_components=1) \nX_E = pca1.fit_transform(X_HDn)\nX_reconstructed = pca1.inverse_transform(X_E)\n", "intent": "Since the first component is so latge, lets only keep it, and then reconstruct the original data from only this component, setting the others to 0.\n"}
{"snippet": "df = pd.read_csv('https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv', index_col=0)\ndf.head(10)\n", "intent": "1) Load in the dataset `https://d1pqsl2386xqi9.cloudfront.net/notebooks/Default.csv` into a pandas dataframe\n"}
{"snippet": "X= df[['balance', 'income', 'student']]\ny=df['default']\nfor i in range(1,16):\n    print('random state =',i)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n    fpr,tpr,roc_auc,thresholds = generate_auc(X_train,y_train,LogisticRegression, penalty='l1')\n    generate_ROCplot(fpr,tpr,'LR',roc_auc)\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "import pandas as pd\nfrom  sklearn.tree import DecisionTreeClassifier\ntrain=pd.read_csv('train.csv')\ntest=pd.read_csv('test.csv')\n", "intent": "[click here](https://www.kaggle.com/c/titanic) to go to problem description\n"}
{"snippet": "import pandas\nimport numpy as np\ndata = pandas.read_csv(\"./train.csv\")\nseed = 1234\n", "intent": "some description that should be desribed soon\n"}
{"snippet": "digits = datasets.load_digits(n_class=6)\nX = digits.data\ny = digits.target\nn = len(X)\n", "intent": "t-SNE is a technique to visualize high-dimensional data by giving each datapoint a location in a two or three-dimensional map.\n"}
{"snippet": "index_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = []\nfor block_num in full_df['date_block_num'].unique():\n    cur_shops = full_df.loc[full_df['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = full_df.loc[full_df['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])), dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns=index_cols, dtype=np.int32)\nfull_df = grid.merge(full_df, how='left', on=index_cols).fillna(0)\n", "intent": "(shop_id, item_id) for each date_block_num\n"}
{"snippet": "def get_aggragation_feature(df, groupby_cols, agg_col):\n    gb = df[df['ds_type'] == 'trn'].groupby(groupby_cols)[agg_col]\n    fname_fmt = '-'.join(groupby_cols+[agg_col]) + ':%s'\n    agg_df = pd.DataFrame({\n            fname_fmt%'mean': gb.mean(),\n        })\n    new_df = df.join(agg_df, on=groupby_cols).fillna(0)\n    return new_df, agg_df.columns.tolist()\n", "intent": "**aggragation data**\n"}
{"snippet": "prevapp = pd.read_csv('./data/input/previous_application.csv')\nprevapp = prevapp.drop('SK_ID_PREV', axis=1)\nprevapp = summary_extra_data(prevapp, 'prevapp_')\nfull_df = full_df.join(prevapp, on='SK_ID_CURR')\nadd_features(prevapp.columns.tolist())\n", "intent": "**previous_application.csv**\n"}
{"snippet": "pcblc = pd.read_csv('./data/input/POS_CASH_balance.csv')\npcblc = pcblc.drop(['SK_ID_PREV', 'MONTHS_BALANCE'], axis=1)\npcblc = summary_extra_data(pcblc, 'pcblc_')\nfull_df = full_df.join(pcblc, on='SK_ID_CURR')\nadd_features(pcblc.columns.tolist())\n", "intent": "**POS_CASH_balance.csv**\n"}
{"snippet": "def do_pca(d,n):\n    pca = PCA(n_components=n)\n    X = pca.fit_transform(d)\n    print pca.explained_variance_ratio_\n    return X, pca\n", "intent": "We now carryout a 20D PCA, which captures 73% of the variance.\n"}
{"snippet": "installpay = pd.read_csv('./data/input/installments_payments.csv')\ninstallpay = installpay.drop(['SK_ID_PREV'], axis=1)\ninstallpay = summary_extra_data(installpay, 'installpay_')\nfull_df = full_df.join(installpay, on='SK_ID_CURR')\nadd_features(installpay.columns.tolist())\n", "intent": "**installments_payments.csv**\n"}
{"snippet": "usecols = ['id', 'total_distance', 'total_travel_time', 'number_of_steps']\nfr_df = pd.concat([    \n    pd.read_csv('./data/input/fastest_routes_train_part_1.csv', usecols=usecols),\n    pd.read_csv('./data/input/fastest_routes_train_part_2.csv', usecols=usecols),\n    pd.read_csv('./data/input/fastest_routes_test.csv', usecols=usecols),\n])\n", "intent": "* Addition Data\nhttps://www.kaggle.com/oscarleo/new-york-city-taxi-with-osrm\n"}
{"snippet": "import pandas as pd\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n                'TAX', 'PTRATIO', 'B', 'LSTAT']\ndf = pd.DataFrame(train_data, columns=column_names)\ndf.head()\n", "intent": "Use the [pandas](https://pandas.pydata.org) library to display the first few rows of the dataset in a nicely formatted table:\n"}
{"snippet": "distance_df = pd.DataFrame(distance_matrix, index=invocation_matrix.index, columns=invocation_matrix.index)\ndistance_df.iloc[81:85,60:62]\n", "intent": "From this result, we create a `DataFrame` to get a better visual representation of the data.\n"}
{"snippet": "import pandas as pd\ndissimilarity_df = pd.DataFrame(\n    dissimilarity_matrix,\n    index=commit_matrix.index,\n    columns=commit_matrix.index)\ndissimilarity_df.iloc[:5,:2]\n", "intent": "To be able to better understand the result, we add the file names from the `commit_matrix` as index and column index  to the `dissimilarity_matrix`.\n"}
{"snippet": "from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(plot_data['index'])\nplot_data['normalized_index_name'] = le.transform(plot_data['index']) * 10\nle.fit(plot_data['index_from_emails'])\nplot_data['normalized_index_email'] = le.transform(plot_data['index_from_emails']) * 10\nplot_data.head()\n", "intent": "I just add some nicely normalized indexes for plotting (note: there might be a method that's easier)\n"}
{"snippet": "distance_df = pd.DataFrame(distance_matrix, index=invocation_matrix.index, columns=invocation_matrix.index)\ndistance_df.iloc[81:85,60:62]\n", "intent": "From this data, we create a `DataFrame` to get a better representation. You can find the complete `DataFrame` here as excel file as well.\n"}
{"snippet": "n_components=300\nsvd = TruncatedSVD(n_components=n_components)\nX_train_svd = svd.fit_transform(X_train_vectorized)\nX_test_svd = svd.transform(X_test_vectorized)\n", "intent": "Now we have scaled vectorized vector using Tfidf, we can reduce the number of columns using a PCA\n"}
{"snippet": "X_all_reviews = vectorizer.fit_transform(X).todense()\nX_all_reviews_TFIDF_scaled =scaler.fit_transform(X_all_reviews)\n", "intent": "Finally, we can vectorize the full set with Tfidf, scale it, and use PCA to summarize with only 300 vectors :\n"}
{"snippet": "X2, pca2=do_pca(data,2)\n", "intent": "Justfor kicks, because we can plot it, we'll do the 2D PCA\n"}
{"snippet": "df = pd.read_csv('train.csv') \n", "intent": "- Visualize\n- Find Missing Data\n- Look For Correlations\n"}
{"snippet": "data = pd.read_csv('./minute_weather.csv')\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nCreating a Pandas DataFrame from a CSV file<br><br></p>\n"}
{"snippet": "labels_list = pd.read_csv('signnames.csv')\nlabels_name = labels_list['SignName'].tolist()\nlabels_list\n", "intent": "Visualize the German Traffic Signs Dataset using the pickled file(s).\n"}
{"snippet": "digits = datasets.load_digits()\n", "intent": "The data that we are interested in is a set of 1797 images of digits, each one made of $8\\times8$ pixels.\n"}
{"snippet": "fileTrain = 'train-tweets.txt'\ndfTrain = utils.read_textfile(fileTrain,',')\ntweets = dfTrain.tweet.values\nlabels = pd.factorize(dfTrain.sentiment)[0]\nsentiments = pd.factorize(dfTrain.sentiment)[1]\ntweetsTrain, tweetsTest, labelsTrain, labelsTest = cross_validation.train_test_split(tweets, labels, \n                                                                      train_size=0.85, random_state=1234)\n", "intent": "(comment this block when real testing data is given)\n"}
{"snippet": "ss = StandardScaler()\nXs = ss.fit_transform(X)\n", "intent": "Standardize the data and compare at least one of the scatterplots for the scaled data to unscaled above.\n"}
{"snippet": "df['bias'].unique()\nX = df['text'].values\ny = df['bias'].map(lambda x: 1 if x == 'partisan' else 0)\nX_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.33)\n", "intent": "Please split the dataset into a training and test set and convert the `bias` feature into 0s and 1s.\n"}
{"snippet": "sd = pd.read_csv('./datasets/speed_dating.csv')\nsd.head()\nsd.isnull().sum()[sd.isnull().sum() > 200]\n", "intent": "---\n- Remove columns with over 200 missing values.\n- Then, remove rows with missing values.\n- Verify no rows contain NaNs.\n"}
{"snippet": "subjective_pca_transformed_df = subj_pca.transform(subjective_sdt.values)\nsns.pairplot(pd.DataFrame(subjective_pca_transformed_df), kind='reg')\n", "intent": "---\nThe transform function in the PCA will create you new component variable matrix.\n"}
{"snippet": "pca = PCA(n_components=2)\ntrain_x = pca.fit_transform(train_x)\ntest_x = pca.transform(test_x)\n", "intent": "We *fit (find PC's) and transform* the training data, and then use the PC's to transform the test data.\n"}
{"snippet": "from sklearn import cross_validation\nX_dummy = pd.DataFrame([1,2,3,4],columns=['Dummy Data'])\nk_folds = cross_validation.KFold(n=X_dummy.shape[0],n_folds=4,shuffle=False)\nfor train_index,test_index in k_folds:\n    print train_index, test_index\n", "intent": "Example of creating folds manually\n"}
{"snippet": "df = pd.DataFrame()\ndf['ship_type']  =  np.random.choice([\"romulan\", \"human\", \"klingon\", \"borg\", \"red_shirt\", \"jyotsna\"], size=500)\ndf['ship_value'] =  np.random.randint(200000, 10000000, size=500)\ndf['ship_speed'] =  np.random.randint(10, 60, size=500)\ndf['baths']      =  np.random.choice(np.arange(1, 4, 0.5), size=500)\n", "intent": "With 2-3 continious variables, and one categorical.\n"}
{"snippet": "df = pd.read_csv('../../../datasets/breast_cancer_wisconsin/wdbc.data', \n                  header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "bcw = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-2/datasets/breast_cance_wisconsin/wdbc.data', \n                  header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "cancer = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-4/datasets/breast_cancer_wisconsin/breast_cancer.csv')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "def bar_chart(feature) :\n    survived = train[train['Survived'] == 1][feature].value_counts()\n    dead = train[train['Survived'] == 0][feature].value_counts()\n    df = pd.DataFrame([survived, dead])\n    df.index = ['Survived', 'Dead']\n    df.plot(kind = 'bar', stacked = True, figsize = (10, 5))\n", "intent": " - Pclass\n - Sex\n - SibSp (\n - Parch (\n - Embarked\n - Cabin\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_data, test_data = train_test_split(agg_data, test_size=0.2, random_state=1)\n", "intent": "We will split now the dataset into two parts, the training data and the testing data. The test data should be 0.2 of the original data size. \n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nsamples = x_train[:2]\nx_train_counts = count_vect.fit_transform(samples)\nprint(pd.DataFrame(x_train_counts.A, columns=count_vect.get_feature_names()).to_string())\n", "intent": "Now let's see how we can transform our tweets into vectors\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain_data, test_data = train_test_split(agg_data, test_size=0.2, random_state= 42)\n", "intent": "We will split now the dataset into two parts, the training data and the testing data. The test data should be 0.2 of the original data size. \n"}
{"snippet": "X5, pca5=do_pca(data, 5)\n", "intent": ">YOUR TURN NOW\nDo a 5 dimensional PCA, get the variance explanation, and display the components.\n"}
{"snippet": "spotify = pd.read_csv(\"../data/spotify_data.csv\", index_col=[0])\nspotify.head()\n", "intent": "Let's use AdaBoost on Spotify data\n"}
{"snippet": "Xr, yr = make_regression(n_samples=1000, n_features=1, noise=7, random_state=4,bias = 0.7)\n", "intent": "Now let's train a neural net on a regression dataset\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split( X, y, \n                                                    test_size = .25,\n                                                   random_state = 3)\n", "intent": "Let's use train/test split with RMSE to decide whether Newspaper should be kept in the model:\n"}
{"snippet": "pd.set_option(\"max.columns\", 30)\nkc = pd.read_csv(\"../data/kc_house_data.csv\")\nkc.head()\n", "intent": "We're going to work together to model housing prices using the king county home sales \n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33,\n                                                    random_state=4)\n", "intent": "Create confusion matrix for the Spotify data and calculate recall and precision scores\n"}
{"snippet": "imp = Imputer(axis=1)\n", "intent": "**Imputation** Replacing nan or null values with the mean or median values of a specific column in a dataset.\n"}
{"snippet": "imp_med = Imputer(axis=1, strategy=\"median\")\nimp_med.fit(X_train.Age)\nX_train.Age.median()\n", "intent": "We can also use the median instead of age\n"}
{"snippet": "path = \"../data/fraud.csv\"\nfraud = pd.read_csv(path, index_col=[0])\nfraud.drop(\"Time\", axis = 1)\nfraud.head()\n", "intent": "Let's move onto the real thing by modeling credit card fraud data\nhttps://www.kaggle.com/dalpozz/creditcardfraud\n"}
{"snippet": "from sklearn import datasets\ndigits_dict = datasets.load_digits()\ndigits_dict[\"DESCR\"].split(\"\\n\")\n", "intent": "The hand written digits of the mnist dataset.\n"}
{"snippet": "titanic.Age.fillna(titanic.Age.mean(), inplace=True)\n", "intent": "Sometimes a better strategy is to **impute missing values**:\n"}
{"snippet": "df = pd.read_table(\"../data/NLP_data/sms.tsv\",encoding=\"utf-8\", names= [\"label\", \"message\"])\ndf.head()\n", "intent": "This is a really helpful technique to find the words most associated with either class.\n"}
{"snippet": "path = \"../data/NLP_data/yelp.csv\"\nyelp = pd.read_csv(path, encoding='unicode-escape')\nyelp.head()\n", "intent": "To wrap our text classification section, we're going to learn how to incorporate stemming and lemmatization in our vectorizers. \n"}
{"snippet": "path = \"../data/NLP_data/ds_articles.csv\"\narticles = pd.read_csv(path, usecols=[\"text\", \"title\"], encoding=\"utf-8\")\narticles.dropna(inplace=True)\narticles.reset_index(inplace=True, drop=True)\narticles.head()\n", "intent": "We're going to build a very simple summarizer that uses tfidf scores on a corpura of data science and artificial intelligence articles\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "Ridge is better for dealing with multicollinearity and Lasso is better for high number of features.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncountvec = CountVectorizer()\nsparse_dtm = countvec.fit_transform(reviews['body_without_digits'])\n", "intent": "Our next step is to turn the text into a document term matrix using the scikit-learn function called `CountVectorizer`.\n"}
{"snippet": "import glob\nDAY2_DATA_DIR = '../../day-2/data'\nAUSTEN_DIR = os.path.join(DAY2_DATA_DIR, 'austen', '*.txt')\nfnames = glob.glob(AUSTEN_DIR)\nbooks = []\nfor fname in fnames:\n    with open(fname) as f:\n        text = f.read()\n    books.append(text)\n", "intent": "Read in all the Jane Austen books from day 2 and turn them into a DTM. What will be the rows and columns?\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\ntfidfvec = TfidfVectorizer()\nsparse_tfidf = tfidfvec.fit_transform(reviews['body_without_digits'])\nsparse_tfidf\n", "intent": "To do so, we simply do the same thing we did above with CountVectorizer, but instead we use the function TfidfVectorizer.\n"}
{"snippet": "tfidf_vectorizer = TfidfVectorizer(max_df=0.80, min_df=50,\n                                   max_features=5000,\n                                   stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(df_lit['text'])\n", "intent": "In sklearn, the input to LDA is a DTM (with either counts or TF-IDF scores).\n"}
{"snippet": "import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nlda = LatentDirichletAllocation(n_components=n_topics, max_iter=20, random_state=0)\nlda = lda.fit(tf)\n", "intent": "This is where we fit the model.\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\ntrain_dtm = vect.fit_transform(X_train)\ntrain_dtm.shape\n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(features, response, test_size=0.2)\n", "intent": "We don't want to train our classifier on the same dataset that we test it on, so let's split it into training and test sets.\n"}
{"snippet": "vocab = [(v,k) for k,v in countvectorizer.vocabulary_.items()]\nvocab = sorted(vocab, key=lambda x: x[0])\nvocab = [word for num,word in vocab]\ncoef = list(zip(vocab, lr.coef_[0]))\nimportant = pd.DataFrame(lr.coef_).T\nimportant.columns = lr.classes_\nimportant['word'] = vocab\nimportant.head()\n", "intent": "Now we can interpret the classifier by the features that it found important.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_dev, Y_train, Y_dev = train_test_split(X, Y, random_state = 101, test_size = 0.3)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "X, y = make_regression(n_samples=10000, n_features=100, random_state=2017)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "Wygenerujmy dane, np. 10000 wierszy i 100 kolumn (cech).\n"}
{"snippet": "X, y = make_classification(n_samples=10000, n_features=100, random_state=2017)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n", "intent": "Podobnie, wygenerujmy dane, np. 10000 wierszy i 100 kolumn (cech).\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nfeature_col_names = ['num_preg', 'glucose_conc', 'diastolic_bp', 'thickness', 'insulin', 'bmi', 'diab_pred', 'age']\npredicted_class_name = ['diabetes']\nX = df[feature_col_names].values    \ny = df[predicted_class_name].values \nsplit_test_size = 0.30\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_test_size, random_state=42)\n", "intent": "70% for training, 30% for testing\n"}
{"snippet": "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n", "intent": "<center>![alt text](mnist_plot.png)</center>\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_2 = LabelEncoder()\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()\nX = X[:, 1:]\n", "intent": "As we can see, we have to deal with a classification problem.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_table('smsspamcollection/SMSSpamCollection',\n                   sep='\\t', \n                   header=None, \n                   names=['label', 'sms_message'])\ndf.head()\n", "intent": "Para Problemas de Naives Bayes\n"}
{"snippet": "url = 'https://raw.githubusercontent.com/justmarkham/DAT7/master/data/titanic.csv'\ntitanic = pd.read_csv(url)\ntitanic.head(10)\n", "intent": "We'll build a classification tree using the Titanic data:\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn import manifold\nimport matplotlib.patheffects as PathEffects\ndigits = datasets.load_digits()\ndigits.data.shape\nfrom sklearn.utils.extmath import _ravel\nRS = 20150101\n", "intent": "OK - lets try something more exciting than the Iris dataset (this is taken from https://github.com/oreillymedia/t-SNE-tutorial) \n"}
{"snippet": "X = np.vstack([digits.data[digits.target==i]\n               for i in range(10)])\ny = np.hstack([digits.target[digits.target==i]\n               for i in range(10)])\ndigits_proj = manifold.TSNE(random_state=RS).fit_transform(X)\n", "intent": "Now let's run the t-SNE algorithm on the dataset. It just takes one line with scikit-learn.\n"}
{"snippet": "content_image = scipy.misc.imread(\"images/rsz_content.jpg\")\ncontent_image = reshape_and_normalize_image(content_image)\n", "intent": "Let's load, reshape, and normalize our \"content\" image (the Louvre museum picture):\n"}
{"snippet": "style_image = scipy.misc.imread(\"images/starry_night2.jpg\")\nstyle_image = reshape_and_normalize_image(style_image)\n", "intent": "Let's load, reshape and normalize our \"style\" image (Claude Monet's painting):\n"}
{"snippet": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combined_df:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_df.head()\n", "intent": "We can convert the categorical titles to ordinal\n"}
{"snippet": "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()\n", "intent": "Complete the ``Fare`` feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature\n"}
{"snippet": "azdias_1 = pd.read_csv('Udacity_AZDIAS_Subset.csv', delimiter=';')\nazdias_2 = pd.read_csv('Udacity_AZDIAS_Subset.csv', delimiter=';')\nazdias = clean_data(azdias_1, azdias_2)\nazdias.shape\n", "intent": "- Validate the clean function\n"}
{"snippet": "cs = cosine_similarity(qm, nm)\ncdf = pd.DataFrame(data=cs, index=tst_labels, columns=nms)\ncdf\n", "intent": "Build the cosine similarity matrix between the test data (qm) and the training reference vectors (nm)\n"}
{"snippet": "data2 = pd.DataFrame({'Age':  [17,64,18,20,38,49,55,25,29,31,33], \n                      'Salary': [25,80,22,36,37,59,74,70,33,102,88], \n             'Loan Default': [1,0,1,0,1,0,0,1,1,0,1]})\ndata2\n", "intent": "Let's consider a more complex example by adding a \"Salary\" variable (in the thousands of dollars per year).\n"}
{"snippet": "data_test = pd.read_csv(data_path+'/data_test.csv.gz')\nimages_test = create_images(data_test, \n                            n_theta_bins=10, \n                            n_phi_bins=20, \n                            n_time_bins=6)\nX_test = images_test / 10.\n", "intent": "Make predictions for test data and prepare a submission file.\n"}
{"snippet": "X = np.vstack((car_features, notcar_features)).astype(np.float64)\nX_scaler = StandardScaler().fit(X)\nscaled_X = X_scaler.transform(X)\ny = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n", "intent": "A standard scaler fit was applied to the feature vector to normalize the features.\n"}
{"snippet": "import pandas as pd\nloans_2007 = pd.read_csv(\"loans_2007.csv\")\nprint(\"DF SHAPE pre cleanse: \",loans_2007.shape, '\\n')\nloans_2007.drop_duplicates()\nhalf_count = len(loans_2007) / 2\nloans_2007 = loans_2007.dropna(thresh=half_count, axis=1)\nprint(\"DF SHAPE post cleanse: \",loans_2007.shape, '\\n')\n", "intent": "Read in the data and perform initial data exploration to determine the size and shape of the data set. \n"}
{"snippet": "from sklearn.manifold import MDS\nmds = MDS(n_components=2, random_state=42)\nt0 = time.time()\nX_reduced_mds = mds.fit_transform(X)\nt1 = time.time()\ntime_mds = round(t1-t0,2)\nprint(\"Complete MDS: \",time_mds)\n", "intent": "**Multidimensional Scaling (MDS):** reduces dimensionality while trying to <font color=red> preserve the distances between the instances. </font>\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_tr)\nX_tr = scaler.transform(X_tr)\nX_te = scaler.transform(X_te)\n", "intent": "Similarly, use StandardScaler from sklearn.preprocessing to normalize the training and testing data, using the training data\n"}
{"snippet": "from keras.utils.data_utils import get_file\nimport io\npath = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\nwith io.open(path, encoding='utf-8') as f:\n    text = f.read().lower()\ntext = text.replace(\"\\n\",\" \")\nprint('corpus length:', len(text))\ntext[0:100]\n", "intent": "Should have a character length of ~600k\n"}
{"snippet": "encoding = 'ISO-8859-1'\ndf = pd.read_csv('spam.csv', encoding=encoding)\nprint(df.columns)\ndf.head()\n", "intent": "We are going to use classifiers to predict SMS spam.\n1. load in data\n2. apply ML algo\n3. Assess accuracy\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ntwenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\nprint type(twenty_train)\ntwenty_train.data[0]\n", "intent": "We can now load the list of files matching those categories as follows:\n"}
{"snippet": "sc = StandardScaler()\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=100)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\nrfModel = rf.fit(trainingData)\nrfFeatureImportance = pd.DataFrame([(name, rfModel.featureImportances[idx]) for idx, name in attrs],columns=['feature_name','feature_importance'])\nprint(gbCvFeatureImportance.sort_values(by=['feature_importance'],ascending =False))\n", "intent": "1. Build and train a RandomForestClassifier and print out a table of feature importances from it.\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"C:/Users/1asch/udemy-datascience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "def make_submission_df(all_prediction):\n    df = test.merge(all_prediction, on=[\"shop_id\", \"item_id\"], how=\"left\")[[\"ID\", \"item_cnt_month\"]]\n    df[\"item_cnt_month\"] = df[\"item_cnt_month\"].fillna(0).clip(0, 20)    \n    return df\ndef make_submission_file(df, filename):\n    df.to_csv(\"./submission/%s.csv\" % filename, index=False)\ndef make_submission(all_prediction, filename=\"no_name\"):\n    make_submission_file(make_submission_df(all_prediction), filename)\n", "intent": "Utility function makes codes simple, so it's good to make these functions\n"}
{"snippet": "sales = pd.read_csv('../competitive-data-science-final-project/dataset/sales_train.csv.gz')\nshops = pd.read_csv('../competitive-data-science-final-project/dataset/shops.csv')\nitems = pd.read_csv('../competitive-data-science-final-project/dataset/items.csv')\nitem_cats = pd.read_csv('../competitive-data-science-final-project/dataset/item_categories.csv')\n", "intent": "Let's load the data from the hard drive first.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(sales_2015[[\"2015 Sales Q1\", \"Price per Liter mean\", \"2015 Volume Sold (Liters) mean\"]], sales_2015[\"2015 Sales\"], test_size=0.4)\nprint X_train.shape, y_train.shape\nprint X_test.shape, y_test.shape\n", "intent": "Create Model using train test split\n"}
{"snippet": "state_location = pd.read_csv(\"data/State_Location.csv\")\nstate_location.head()\n", "intent": "Impact of de-regulation\n"}
{"snippet": "data = pd.read_csv(\"http://www.ats.ucla.edu/stat/data/binary.csv\")\ndata.head()\n", "intent": "Example - learning the probability of school admission based on marks and school type\n"}
{"snippet": "seq_len = 500\nfrom keras.preprocessing.sequence import pad_sequences\ntrain = pad_sequences(train, maxlen=seq_len, value=0)\ntest = pad_sequences(test, maxlen=seq_len, value=0)\n", "intent": "Keras padding 0 at the beginning.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True)\n", "intent": "Using this, we can conveniently partition the data into training and testing data and report model accuracy.\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston=load_boston()\ndf=pd.DataFrame(boston.data,columns=boston.feature_names)\ndf['Price']=boston.target\ndf.head().T\n", "intent": " - Ridge `MSE + g * sum(a**2)`\n - Lasso `MSE + g * sum(abs(a))`\n - Wehre `g` is the regularization parameter we choose freely\n"}
{"snippet": "df['avg_rating_by_driver'].fillna(df['avg_rating_by_driver'].mean(), inplace=True)\ndf['avg_rating_of_driver'].fillna(df['avg_rating_of_driver'].mean(), inplace=True)\n", "intent": "Although there are few outliers I believe it is safe to impute the missing values with the mean\n"}
{"snippet": "from keras.datasets import mnist\n(X_train0, y_train0), (X_test0, y_test0) = mnist.load_data()\n", "intent": "<img src=\"https://datascienceschool.net/upfiles/90f2752671424cef846839b89ddcf6aa.png\">\n"}
{"snippet": "import pandas as pd\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data'\ncol_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age','label']\npima = pd.read_csv(url, header=None, names=col_names)\n", "intent": "[Pima Indian Diabetes dataset](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) from the UCI Machine Learning Repository\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\n", "intent": "**4 Steps for Vectorization**\n1. Import\n2. Instantiate\n3. Fit\n4. Transform\nThe difference from modelling is that a vectorizer does not predict\n"}
{"snippet": "from sklearn import datasets\ndigits = datasets.load_digits()\n", "intent": "Load the digits dataset from sklearn:\n"}
{"snippet": "conmat = np.array(confusion_matrix(Y_test, Y_pred, labels=[1,0]))\nprint conmat\nconfusion = pd.DataFrame(conmat, index=['has_cancer', 'is_healthy'],\n                         columns=['predicted_cancer','predicted_healthy'])\nprint(confusion)\n", "intent": "Let's say again that we are predicting cancer based on some kind of detection measure, as before.\n"}
{"snippet": "df = pd.read_csv('../../assets/datasets/cars.csv')\n", "intent": "Visualize the last tree. Can you make sense of it? What does this teach you about decision tree interpretability?\n"}
{"snippet": "df = pd.DataFrame(data = adult, columns=['workclass', 'education-num', 'hours-per-week', 'income'])\ndf.head()\n", "intent": "Convert the data to a Pandas dataframe to work with the data:\n"}
{"snippet": "df = pd.DataFrame(data = adult)\ndf\n", "intent": "Convert the data to a Pandas dataframe to work with the data:\n"}
{"snippet": "X_scale = StandardScaler().fit_transform(X)  \nX_scale = pd.DataFrame(X_scale, columns = X.columns)\nX_scale\n", "intent": "First, let's standardize the data\n"}
{"snippet": "import pandas as pd\npath = '../data/'\nurl = path + 'titanic.csv'\ntitanic = pd.read_csv(url, index_col='PassengerId')\ntitanic.shape\n", "intent": "scikit-learn models expect that all values are **numeric** and **hold meaning**. Thus, missing values are not allowed by scikit-learn.\n"}
{"snippet": "skewed = ['Avg_Basket_Size','Reward_Points_Earned']\nfeatures_log_transformed_purchage = pd.DataFrame(data = customer_purchase)\nfeatures_log_transformed_purchage[skewed] = features_log_transformed_purchage[skewed].apply(lambda x: np.log(x + 1))\n", "intent": "Aa we can see the data is very skewed. Lets remove the outlier and transform the data using log scale\n"}
{"snippet": "from sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import validation_curve\ndigits = load_digits()\nx = digits.data\ny = digits.target\n", "intent": "- validation_curve\n- GridSearchCV\n- ParameterGrid\n"}
{"snippet": "import pandas as pd\ntrain = pd.read_csv('input/train.csv')\ntest = pd.read_csv('input/test.csv')\n", "intent": "download from kaggle directly [kaggle](https://www.kaggle.com/c/titanic/data)  \n"}
{"snippet": "submission_example = pd.read_csv(Path_submission_example)\nsubmission_example.head()\n", "intent": "Example of a submission file for kaggle\n"}
{"snippet": "from sklearn.decomposition import PCA\ndef get_PCs(X,p):\n    pca = PCA(whiten=True)\n    pca.fit(X)\n    ix=np.where(np.cumsum(pca.explained_variance_ratio_)>p)[0][0]\n    pca = PCA(n_components=ix,whiten=True)\n    return pca.fit_transform(X)\n", "intent": "**PCA_BinF, PCA_BinaryF, PCA_RawCF**: Use projected components which keep more than 99% variations as the features\n"}
{"snippet": "f = open('HomeB/2015/HomeB_meter1_2015.csv')\ndata = pd.read_csv(f,sep=',', header='infer', parse_dates=[1])\nprint(data.head())\n", "intent": "Data Home B for predicition\n"}
{"snippet": "df_users['invited_by_user_id'] = df_users['invited_by_user_id'].fillna(0)\n", "intent": "8823 users have non-null last_session_creation_time, i.e. they have logged into the product.    \n6417 users are invited by other users.\n"}
{"snippet": "rdf = rdf.fillna(0)\n", "intent": "So, we have 9788 records from 1970-01-01 20:15:00 to 1970-04-13 19:00:00.\nThere are null values in count column.\n"}
{"snippet": "df['avg_rating_by_driver'].fillna(df['avg_rating_by_driver'].mean(), inplace=True)\ndf['avg_rating_of_driver'].fillna(df['avg_rating_of_driver'].mean(), inplace=True)\ndf.dropna(axis=0, inplace=True)\n", "intent": "There are missing values in columns 'avg_rating_by_driver', 'avg_rating_of_driver', and 'phone'.\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape\n", "intent": "- Parameter **lowercase:** boolean, True by default\n    - If True, Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "w=84\ncols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\ny_pred_test_mix=(w/100)*y_pred_test_nw2+y_pred_test_nw*((100-w)/100)\npd.DataFrame(y_pred_test_mix, index=dataTesting.index, columns=cols).to_csv('Final_pred_NW_MIX.csv', index_label='ID')\n", "intent": "Export results Neural Network model for mixed model Images Transfer Learning VGG16+Text. Text model weight=0.84\n"}
{"snippet": "image = io.imread(os.path.join(path, 'images_resize_gray', str(dataTraining.index[0]) + '_resize_gray.jpeg'))\n", "intent": "We are going to use the gray images to make the predictions.\n"}
{"snippet": "image = io.imread(os.path.join(path, 'images_resize_gray', str(dataTraining.index[0]) + '_resize_gray.jpeg'))\n", "intent": "Load an image from file.\n"}
{"snippet": "pca = PCA(n_components=24)\nimages_training_pca = pca.fit_transform(images_training)\n", "intent": "To reduce the dimensionality, we start appling PCA previus to make a model with Machine learning\n"}
{"snippet": "w=95\ncols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\ny_pred_test_mix=(w/100)*y_pred_test_genres2+y_pred_test_genres*((100-w)/100)\npd.DataFrame(y_pred_test_mix, index=dataTesting.index, columns=cols).to_csv('Final_pred_RF_MIX.csv', index_label='ID')\n", "intent": "Export results Random Forest model for mixed model Images+Text. Text model weight=0.95\n"}
{"snippet": "dataset = pd.read_csv('Titanic_train.csv', header = 0, dtype={'Age': np.float64})\n", "intent": "<a id=\"dataset_import\"></a>\n"}
{"snippet": "train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=0)\n", "intent": "<a id=\"splitting_dataset\"></a>\n"}
{"snippet": "for set_ in (train_set, test_set):\n    set_['Fare'] = set_['Fare'].fillna(set_['Fare'].median())\n", "intent": "<b>NOTE:</b> Fare feature also has some missing value. <b>We replace those values with the median</b>.\n"}
{"snippet": "dataset = datasets.load_iris()\nX = dataset['data']\ny = dataset['target']\nX.shape, y.shape\n", "intent": "First, we'll load the data and create one-hot vectors\n"}
{"snippet": "from sklearn.datasets import fetch_olivetti_faces\noliv=fetch_olivetti_faces()\nprint oliv.data.shape \n", "intent": "This notebook is based on Shankar Muthuswamy's example at https://shankarmsy.github.io/posts/pca-sklearn.html\n"}
{"snippet": "X, y = load_breast_cancer(return_X_y=True)\n", "intent": "First, we'll split our data int train, test, and validation datasets\n"}
{"snippet": "iris = datasets.load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris[\"data\"], iris[\"target\"], test_size=0.2)\niris[\"feature_names\"], iris[\"target_names\"]\n", "intent": "As in PyTorch, we'll start with the Iris dataset.\n"}
{"snippet": "iris = datasets.load_iris()\nX_train = iris['data']\ny_train = iris['target']\n", "intent": "As in PyTorch, we'll start with the Iris dataset.\n"}
{"snippet": "df_stat = pd.DataFrame(columns=['year','month','nrows'])\nquery = 'SELECT distinct year, month, start_station_id FROM rides WHERE year=? AND month=? '\nfor year in [2014,2015,2016]:\n    for month in range(1,13):\n        df = pd.read_sql_query(query, con, params=[year,month])\n        newdf = pd.DataFrame({'year':[year],'month':[int(month)],'nrows':[int(len(df))]})\n        df_stat = df_stat.append(newdf,ignore_index=True)\ndf_stat.head()\n", "intent": "- See when number of stations changed; this could effect number of rides\n- Need to account for changing \n"}
{"snippet": "bank_full_data = pd.read_csv('bank-full.csv', delimiter=';')\nprint(\"Bank dataset was loaded successfully!\")\n", "intent": "Loading the dataset from the CSV file.\n"}
{"snippet": "def preprocess_features(X):\n    output = pd.DataFrame(index=X.index)\n    for col, col_data in X.iteritems():\n        if col_data.dtype == object:\n            col_data = col_data.replace(['yes', 'no'], [1, 0])\n        if col_data.dtype == object:\n            col_data = pd.get_dummies(col_data, prefix=col)  \n        output = output.join(col_data)\n    return output\n", "intent": "Applying pandas_get_dummies to convert categorical features into binary variables. Also, we'll replace 'yes' -> 1, 'no' -> 0.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.3, random_state=10)\nprint(\"Training set has {} samples with {:.2f}% of 'yes' (subscribed) and {:.2f}% of 'no' (not subscribed).\"\n      .format(X_train.shape[0], \n        100 * len(y_train[y_train == 1])/len(y_train), \n        100 * len(y_train[y_train == 0])/len(y_train)))\nprint(\"Testing set has {} samples with {:.2f}% of 'yes' (subscribed) and {:.2f}% of 'no' (not subscribed).\"\n      .format(X_test.shape[0], \n        100 * len(y_test[y_test == 1])/len(y_test), \n        100 * len(y_test[y_test == 0])/len(y_test)))\n", "intent": "Splitting data into training and testing datasets.\n"}
{"snippet": "sc = StandardScaler()\nX_train = pd.DataFrame(sc.fit_transform(X_train), columns=X_all.columns)\nX_test = pd.DataFrame(sc.transform(X_test), columns=X_all.columns)\n", "intent": "Rescaling the features for them to have standard normal distribution with mean 0 and a standard deviation 1.\n"}
{"snippet": "features = X.columns\nfeature_importances = model.feature_importances_\nfeatures_df = pd.DataFrame({'Features': features, 'Importance Score': feature_importances})\nfeatures_df.sort_values('Importance Score', inplace=True, ascending=False)\nfeatures_df.head()\n", "intent": "Why are random forest models more accurate than decision trees? \n"}
{"snippet": "url = '../data/titanic.csv'\ntitanic = pd.read_csv(url)\ntitanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\ntitanic.Age.fillna(titanic.Age.median(), inplace=True)\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\nembarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\ntitanic = pd.concat([titanic, embarked_dummies], axis=1)\ntitanic.head()\n", "intent": "We'll build a classification tree using the Titanic data:\n"}
{"snippet": "jobs_w_salary.number_reviews.fillna(0, inplace=True)\n", "intent": "It also appears not to be statistically significant, although interestingly the pseudo-r2 did go up significantly when I added this to the model...\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "housing_data = pd.read_csv('USA_Housing.csv')\n", "intent": "Now, I will get idea from the dataset as to what is in it and how can it be fed to the model.\n"}
{"snippet": "b_new_order_item.to_csv('../2_data/explored/order_item.csv', index =False)\n", "intent": "Export file order_item\n"}
{"snippet": "from sklearn.utils import shuffle\nX_train, y_train = shuffle(X_train, y_train)\nfrom sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)    \n", "intent": "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project.\n"}
{"snippet": "df = pd.read_csv(\"file2.csv\", lineterminator='\\n')\ntweets = list(df[\"tweets\"])\ntimes = list(df[\"times\"])\n", "intent": "Create dataframes, then store them to csv files.\n"}
{"snippet": "cv = CountVectorizer()\nX = cv.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "X = StandardScaler().fit_transform(select_df)\nX[0]\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nScale the Features using StandardScaler\n<br><br></p>\n"}
{"snippet": "coeff_customers = pd.DataFrame(lm.coef_, X.columns, columns=['Coefficient'])\ncoeff_customers\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "df = pd.read_csv('data/multiTimeline.csv', skiprows = 1)\ndf.head()\n", "intent": "* Import data that you downloaded and check out first several rows:\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncn_vt = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "X = cn_vt.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"/Users/Carter/Projects/DataScience/DS-SF-32/lessons/lesson-7/2008.csv\").fillna(\"unk\")\n", "intent": "http://stat-computing.org/dataexpo/2009/the-data.html\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv(\"/Users/Carter/Projects/DataScience/DS-SF-32/lessons/lesson-7/2008.csv\").fillna(0)\n", "intent": "http://stat-computing.org/dataexpo/2009/the-data.html\n"}
{"snippet": "from sklearn.preprocessing import  LabelEncoder\nle = LabelEncoder()\nle.fit(df['UniqueCarrier'].unique().tolist()[0:10])\nle.transform(df['UniqueCarrier'].unique().tolist()[0:10])\n", "intent": "+ THESE ARE NOT DUMMY VARIABLES\n+ Note the fit vs fit_transform function\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nonehot = OneHotEncoder(categorical_features = is_cat_list, sparse=False)\nX =onehot.fit_transform(df2)\n", "intent": "+ BUT YOU NEED TO SPECIFY WHICH VARIABLES ARE CATEGORICAL\n+ Could we use \"integer\" type as a proxy for that?\n"}
{"snippet": "dataset = pd.read_csv('pima-indians-diabetes.data.txt', header=None)\ndataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, np.NaN)\ndataset.fillna(dataset.mean(), inplace = True)\nprint(dataset.isnull().sum())\n", "intent": "Removing rows with missing values can be too limiting on some predictive modeling problems, an alternative is to impute missing values.\n"}
{"snippet": "data['Loan_Amount_Applied'].fillna(data['Loan_Amount_Applied'].median(),inplace = True)\ndata['Loan_Tenure_Applied'].fillna(data['Loan_Tenure_Applied'].median(),inplace=True)\n", "intent": "Loan Amount and Tenure applied:\n"}
{"snippet": "data = pd.read_csv('bloodpressure_males.csv')\nage=data['age'].values\nweight=data['weight'].values\nheight=data['height'].values\nblood=data['bloodpressure'].values\n", "intent": "Run three independent single linear regression models \n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\ndf_iris = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                     columns= iris['feature_names'] + ['target'])\ndf_iris.head()\n", "intent": "* Import the iris dataset from scikit-learn, turn it into a DataFrame and view the head:\n"}
{"snippet": "train, test, train_labels, test_labels = train_test_split(data, labels, random_state=0, test_size = 0.2, train_size = 0.8)\ntrain_labels_wide = keras.utils.to_categorical(train_labels, num_classes)\ntest_labels_wide = keras.utils.to_categorical(test_labels, num_classes)\n", "intent": "Split the data into a training and test partition so we can evaluate at the end\n"}
{"snippet": "dataset = pd.read_csv('fashion-mnist_train.csv')\ndataset = dataset.sample(frac=data_sampling_rate) \nnum_classes = 10\nclasses = {0: \"T-shirt/top\", 1:\"Trouser\", 2: \"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle boot\"}\n", "intent": "Load the dataset and explore it.\n"}
{"snippet": "scaledFeatures = scaler.fit_transform(bank.drop('Class', axis = 1))\n", "intent": "**Use the .transform() method to transform the features to a scaled version.**\n"}
{"snippet": "dfScaled = pd.DataFrame(scaledFeatures, columns = bank.columns[:-1])\ndfScaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "df = pd.read_csv('College_Data', index_col = 0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=42)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "iowa_data = pd.read_csv('../../../../../../iowa_data/Iowa_Liquor_sales_sample_10pct.csv')\n", "intent": "Load your data from project 3\n"}
{"snippet": "import pandas as pd\nfrom sklearn.feature_extraction.text import HashingVectorizer\nhvec = HashingVectorizer()\nhvec.fit([spam])\n", "intent": "Lookup how to do this and then try it.\n"}
{"snippet": "votes = pd.read_csv(votes_file)\nairport_file = pd.read_csv(airport_file)\n", "intent": "After you've downloaded the data from the repository, go ahead and load it with Pandas\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, x_test, Y_train, Y_test = train_test_split(df['Reviews'], df['Positively Rated'], random_state=0)\n", "intent": "This shows we have imbalanced classes\n"}
{"snippet": "df=pd.read_csv('../Iris.csv')\n", "intent": "Lataa iris-dataset dataframeen totuttuun tapaan:\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimputer = Imputer(strategy=\"median\")\nX_train_num_np = imputer.??(X_train_num_df)\n", "intent": "<span style=\"color:red\"> **Imputer Class for pipeline** </span>\n"}
{"snippet": "median_total_bedrooms = train_set[\"total_bedrooms\"].median()\nmedian_bedrooms_per_room = train_set[\"bedrooms_per_room\"].median()\nX_train_num_df_null[\"total_bedrooms\"].fillna(median_total_bedrooms, inplace=True)\nX_train_num_df_null[\"bedrooms_per_room\"].fillna(median_bedrooms_per_room, inplace=True)\nX_train_num_df_null.head()\n", "intent": "*Option 3: Set the missing values to some values(median)*\n"}
{"snippet": "from sklearn.preprocessing import Imputer\nimputer = Imputer(strategy=\"median\")\nX_train_num_np = imputer.fit_transform(X_train_num_df)\n", "intent": "<span style=\"color:red\"> **Imputer Class for pipeline** </span>\n"}
{"snippet": "for i in range(data.shape[1]-1):\n    if int(data.iloc[:,i:i+1].isnull().sum()) > 0:\n        imp = Imputer(missing_values='NaN', strategy='mean')\n        imp.fit(data.iloc[:,i:i+1])\n        data.iloc[:,i:i+1] = imp.transform(data.iloc[:,i:i+1])\ndata.isnull().sum()\n", "intent": "Next. plug missing value by mean.\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(stop_words='english')\nfit_vect = vect.fit_transform(X)\n", "intent": "Test out differnt parameters of the vect (e.g. stop_words='english' or None) to observe how it changes the clusters\n"}
{"snippet": "airline_acronyms = pd.read_csv(AIRLINE_ACRONYMS_FILEPATH)\nairline_acronyms.head()\n", "intent": "SOURCE: https://www.faa.gov/airports/resources/acronyms/\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nvect.fit(text)\nprint(vect.get_feature_names())\n", "intent": "- boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "vect = CountVectorizer(max_features=5)\nvect.fit(text)\nprint(vect.get_feature_names())\n", "intent": "- int or None, default=None\n- If not None, build a vocabulary that only consider the top  max_features ordered by term frequency across the corpus.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer().fit(X_train)\n", "intent": "converting the text data into numerical data so that we can use it in sklearn\n"}
{"snippet": "class CountVectorizer:\n    def __init__(self, lowercase=True):\n        self.lowercase = lowercase\n    def fit(self, raw_documents):\n        self.vocabulary_ = raw_documents\n        return self\n    def __repr__(self):\n        return \"CountVectorizer(lowercase={})\".format(self.lowercase)\ncv = CountVectorizer()\ncv.fit(text)\n", "intent": "When we run the cv.fit(text) no output displays. This is because the method does not include a return value or print statements\n"}
{"snippet": "data = pd.read_csv(\"life_satisfaction_vs_gdp_per_capita_all.csv\", thousands=',')\n", "intent": "load \"datasets/lifesat/life_satisfaction_vs_gdp_per_capita_all.csv\"\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=20, include_bias=False)\nX_poly = poly_features.fit_transform(X)\n", "intent": "feature extension with PolynomialFeatures\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nstd_scaler = StandardScaler()\nX_poly_scaled = std_scaler.fit_transform(X_poly)\n", "intent": "Apply StandardScaler\n"}
{"snippet": "import pandas as pd\nurl = '../../assets/dataset/beer.txt'\nbeer = pd.read_csv(url, sep=' ')\nbeer\n", "intent": "1. K-means clustering\n2. Clustering evaluation\n3. DBSCAN clustering\n"}
{"snippet": "users = pd.read_csv('takehome_users.csv')\nusers.info()\n", "intent": "We can read in the users file. \n"}
{"snippet": "engagement = pd.read_csv('takehome_user_engagement.csv')\nengagement.info()\n", "intent": "Next, we load in the engagement data. \n"}
{"snippet": "users.last_session_day.fillna(0, inplace=True)\nusers.last_session_month.fillna(0, inplace=True)\nusers.last_session_year.fillna(0, inplace=True)\nusers = pd.get_dummies(users, drop_first=True) \n", "intent": "Next we ensure null values are filled and one-hot encoding the one string/categorical feature. \n"}
{"snippet": "logins = pd.read_json('logins.json')\nlogins.info()\n", "intent": "OK, so we will start by reading in the file and doing some quick exploration. \n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(min_df=5).fit(X_train)\n", "intent": "And features with high Tfidf are vice-versa\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=1221, stratify=y)\nprint('The train data has %.0f rows which is %.2f%% of the total. ' % (len(x_train), len(x_train)*100./len(ultimate)))\nprint('The  test data has %.0f rows which is %.2f%% of the total. ' % (len(x_test),  len(x_test) *100./len(ultimate)))\n", "intent": "Now, we can split the data into training and testing data. Note that we also stratify the split, because there is an imbalance in the data (3:1). \n"}
{"snippet": "from keras.datasets import mnist\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n", "intent": "---\nIn this notebook, we train an MLP to classify images from the MNIST database.\n"}
{"snippet": "evictions = pd.read_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Projects/Project 4/san_francisco/evictions/sf_eviction_notices.csv')\nbudget = pd.read_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Projects/Project 4/san_francisco/city_budget/budget.csv')\nspending_rev = pd.read_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Projects/Project 4/san_francisco/city_spending_revenue/spending_revenue.csv')\n", "intent": "My hypothesis with this data is that number of evictions can impact the spending budget of San Francisco.\n"}
{"snippet": "park_info = pd.read_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Projects/Project 4/san_francisco/parks/park_info.csv')\npark_scores = pd.read_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Projects/Project 4/san_francisco/parks/park_scores.csv')\n", "intent": "My hypothesis is that the size of a park helps predict the quality score of a park.\n"}
{"snippet": "prices_df.to_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Projects/Project 6/rv_prices.csv') \n", "intent": "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n"}
{"snippet": "bcw = pd.read_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Week 4 Notes & Code/Datasets/wdbc.data', \n                  header=None, index_col=None)\n", "intent": "---\nMy path, for example, below: is provided.\n(The file as suffix '.data' but is actually formatted as a .csv)\n"}
{"snippet": "bc = pd.read_csv('../Datasets/breast_cancer.csv')\n", "intent": "- Are there any missing values? Impute or clean if so.\n- Select a classification target and predictors.\n"}
{"snippet": "data = pd.read_csv('../Datasets/401ksubs.csv') \ndata.head(2)\n", "intent": "1. Read the 401ksubs.csv data into Pandas.\n2. Explore the data by sorting, plotting, group_by, and any other ideas/techniques you have been using.\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "1. Standarize the data\n"}
{"snippet": "data['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\ndata['Embarked'] = data['Embarked'].fillna('S')\ndata.info()\n", "intent": "* Impute missing values:\n"}
{"snippet": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Other\": 5}\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n", "intent": "After that, we convert the categorical Title values into numeric form.\n"}
{"snippet": "for dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n", "intent": "We find that category \"S\" has maximum passengers. Hence, we replace \"nan\" values with \"S\".\n"}
{"snippet": "for dataset in train_test_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n", "intent": "Replace missing Fare values with the median of Fare.\n"}
{"snippet": "models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines', 'Linear SVC', \n              'KNN', 'Decision Tree', 'Random Forest', 'Naive Bayes', \n              'Perceptron', 'Stochastic Gradient Decent'],\n    'Score': [acc_log_reg, acc_svc, acc_linear_svc, \n              acc_knn,  acc_decision_tree, acc_random_forest, acc_gnb, \n              acc_perceptron, acc_sgd]\n    })\nmodels.sort_values(by='Score', ascending=False)\n", "intent": "Let's compare the accuracy score of all the classifier models used above.\n"}
{"snippet": "with open('trump.txt', 'r') as fp:\n    txt = fp.read()\n", "intent": "<a name=\"getting-the-trump-data\"></a>\nNow let's load the text.  This is included in the repo or can be downloaded from:\n"}
{"snippet": "df.columns = [c.lower() for c in df.columns] \nfrom sqlalchemy import create_engine\nengine = create_engine('postgresql://:@localhost:5432/mydb')\ndf1 = pd.read_csv('../assets/airports.csv')\ndf1.columns = [c.lower() for c in df1.columns] \ndf2 = pd.read_csv('../assets/Airport_operations.csv')\n", "intent": "Load our csv files into tables\n"}
{"snippet": "stuff = pd.DataFrame(sklearn_pca.components_,columns=X.columns,index=['PCA-1','PCA-2'])\nstuff.head()\n", "intent": "Create a writeup on the interpretation of findings including an executive summary with conclusions and next steps\n"}
{"snippet": "df = pd.DataFrame(mtcars)\ndf.head()\n", "intent": "Convert to a Pandas Dataframe for our analysis\n"}
{"snippet": "from sklearn.datasets import load_iris\ndata = load_iris()\ndata.keys\n", "intent": "Sklearn already has the Iris dataset built in, so all we have to do is import it!\n"}
{"snippet": "import pandas as pd\ndftrain = pd.read_csv('imagery/traindata.csv')\ndftest = pd.read_csv('imagery/testdata.csv')\nprint dftrain.head(5)\n", "intent": "Read in the CSV file as Pandas dataframes\n"}
{"snippet": "PATH_TO_DATA = ('../data')\ntrain_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\ntest_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'), index_col='session_id')\n", "intent": "Reading original data\n"}
{"snippet": "train_target = pd.read_csv('../data/medium/train_log1p_recommends.csv', index_col='id')\ny_train = train_target['log_recommends'].values\ny_train = y_train[:X_train_sparse.shape[0]]\n", "intent": "**Read train target and split data for validation.**\n"}
{"snippet": "text = open(path_to_file).read()\nprint ('Length of text: {} characters'.format(len(text)))\n", "intent": "We can take a look and listen to get a better sense of the dataset:\n"}
{"snippet": "mnist = tf.keras.datasets.mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\ntrain_images = np.expand_dims(train_images, axis=-1)/255.\ntrain_labels = np.int64(train_labels)\ntest_images = np.expand_dims(test_images, axis=-1)/255.\ntest_labels = np.int64(test_labels)\n", "intent": "Let's download and load the dataset and display a few random samples from it:\n"}
{"snippet": "from IPython.display import HTML\nimport io, base64\nvideo = io.open('./pong_agent.mp4', 'r+b').read()\nencoded = base64.b64encode(video)\n", "intent": "And display the result:\n"}
{"snippet": "df = pd.read_csv('Classified Data', index_col=0)\n", "intent": "Set index_col=0 to use the first column as the index.\n"}
{"snippet": "from sklearn.decomposition import  TruncatedSVD\nlsa = TruncatedSVD(n_components=5)\nlsaDocTopic = lsa.fit_transform(X)\nprint(\"Document topic shape\", lsaDocTopic.shape)\nprint (\"Topics and word shape\", lsa.components_.shape)\n", "intent": "PLSA topic modeling in scikit-learn is implemented in the same way as LDA but uses a TruncatedSVD class.\n"}
{"snippet": "sbp_data = pd.read_csv(\"Ex03_SystolicBP_Regreesion.csv\")\n", "intent": "Reading the data file\n"}
{"snippet": "df_model=df_ORG\nstringIndexer2 = StringIndexer(inputCol=\"Dest\", outputCol=\"destIndex\")\nmodel_stringIndexer = stringIndexer2.fit(df_model)\nindexedDest = model_stringIndexer.transform(df_model)\nencoder2 = OneHotEncoder(dropLast=False, inputCol=\"destIndex\", outputCol=\"destVec\")\ndf_model = encoder2.transform(indexedDest)\n", "intent": "In the next two cell we select the features that we need to create the model.\n"}
{"snippet": "from sklearn import datasets\nfrom sklearn import tree\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport random as rnd\niris = datasets.load_iris()\ndf = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n                  columns=iris['feature_names'] + ['target'])\ndf.head()\n", "intent": "This is a companion notebook for the new [Data Science Solutions](https://strtupsci.com) book. The code is explained in the book.\n"}
{"snippet": "def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind = 'bar', stacked = True,figsize=(10,5))\n", "intent": "bar chart for categorical features\n-> Pclass ->Sex -> SbSp(\n"}
{"snippet": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain.head()\n", "intent": "We can convert the categorical titles to ordinal.\n"}
{"snippet": "url = 'https://raw.githubusercontent.com/cs109/2014_data/master/diamonds.csv'\ndiamonds = pd.read_csv(url, index_col=0) \ndiamonds.head()\n", "intent": "http://pandas.pydata.org/pandas-docs/stable/indexing.html\n"}
{"snippet": "X, X_test, y, y_test = train_test_split(stat.iloc[:,2:-1], stat.iloc[:,16], \n                                                    test_size=0.33, random_state=42)\nX.reset_index(inplace=True)\ny = y.reset_index()\nX_test.reset_index(inplace=True)\ny_test = y_test.reset_index()\nprint(\"Training set::{}{}\".format(X.shape,y.shape))\nprint(\"Testing set::{}\".format(X_test.shape))\n", "intent": "<h1> Train-Test Split\n"}
{"snippet": "from sklearn.tree import DecisionTreeRegressor\nX, X_test, y, y_test = train_test_split(stats.iloc[:,2:-3], stats.iloc[:,16], \n                                                    test_size=0.33, random_state=42)\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(X, y)\n", "intent": "<h1> Decision Tree Regression on All the Features\n"}
{"snippet": "X, X_test, y, y_test = train_test_split(stats.iloc[:,2:-1], stats.iloc[:,-1], \n                                                    test_size=0.33, random_state=42)\nX.reset_index(inplace=True)\ny = y.reset_index()\nX_test.reset_index(inplace=True)\ny_test = y_test.reset_index()\nprint(\"Training set::{}{}\".format(X.shape,y.shape))\nprint(\"Testing set::{}\".format(X_test.shape))\n", "intent": "<h1> Train Test Split\n"}
{"snippet": "X, X_test, y, y_test = train_test_split(stat.iloc[:,0:-1], stat.iloc[:,-1], \n                                                    test_size=0.33, random_state=42)\nX.reset_index(inplace=True)\ny = y.reset_index()\nX_test.reset_index(inplace=True)\ny_test = y_test.reset_index()\nprint(\"Training set::{}{}\".format(X.shape,y.shape))\nprint(\"Testing set::{}\".format(X_test.shape))\n", "intent": "<h1> Train-Test Split\n"}
{"snippet": "submit.to_csv('log_reg_baseline.csv', index = False)\n", "intent": "The predictions as given in Target variable should be assessed by the Client and may decide to classify applicant whether can repay loan or can not.\n"}
{"snippet": "from sklearn.datasets import make_classification\nX, y = make_classification(\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n    n_informative=3, n_redundant=1, flip_y=0,\n    n_features=20, n_clusters_per_class=1,\n    n_samples=100, random_state=10\n)\ndf = pd.DataFrame(X)\ndf['TARGET'] = y\ndf.TARGET.value_counts().plot(kind='bar', title='Count (TARGET)');\n", "intent": "For ease of visualization, let's create a small unbalanced sample dataset using the make_classification method:\n"}
{"snippet": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_df.head()\n", "intent": "We can convert the categorical titles to ordinal.\n"}
{"snippet": "engagement = pd.read_csv('takehome_user_engagement.csv')\nengagement.info()\n", "intent": "Next, we load in the engagement data.\n"}
{"snippet": "users.last_session_day.fillna(0, inplace=True)\nusers.last_session_month.fillna(0, inplace=True)\nusers.last_session_year.fillna(0, inplace=True)\nusers = pd.get_dummies(users, drop_first=True) \n", "intent": "Next we ensure null values are filled and one-hot encoding the one string/categorical feature.\n"}
{"snippet": "with open('ultimate_data_challenge.json') as f:\n    ultimate = json.load(f)\nultimate = pd.DataFrame(ultimate)\nultimate.signup_date = pd.to_datetime(ultimate.signup_date)\nultimate.last_trip_date = pd.to_datetime(ultimate.last_trip_date)\nultimate.head()\n", "intent": "We start of course by loading and then briefly exploring the data.\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=1221, stratify=y)\nprint('The train data has %.0f rows which is %.2f%% of the total. ' % (len(x_train), len(x_train)*100./len(ultimate)))\nprint('The  test data has %.0f rows which is %.2f%% of the total. ' % (len(x_test),  len(x_test) *100./len(ultimate)))\n", "intent": "Now, we can split the data into training and testing data. Note that we also stratify the split, because there is an imbalance in the data (3:1).\n"}
{"snippet": "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2)\nX += np.random.random(X.shape)\ndatasets = [make_moons(noise=0.1), make_circles(noise=0.1, factor=0.5), (X, y)]\n", "intent": "<h1 align=\"center\">Warm Up: 3 datasets</h1> \n"}
{"snippet": "D = pd.DataFrame({'area': area, 'length': length})\nD.head()\n", "intent": "As is shown above, the average explanation length for 99% is ~428.\n"}
{"snippet": "table.to_csv('table',header=None)\n", "intent": "Finally, we have a table for all informations include stations, weather and area-id.\n"}
{"snippet": "header=['station','count']\ndata=pd.read_csv(\"data/counts_year\", header=None,names=header,\n                 delim_whitespace=True,\n                 skipinitialspace=True)\n", "intent": "**Then table of measurements are joined with latitude/longitude information below.**\n"}
{"snippet": "header=['station','year'] + range(1,731)\ndata = pd.read_csv(\"data/concenate_data\", header=None,names=header,delim_whitespace=True)\n", "intent": "**Due to the fact that the file is too big, it cannot load into memory in one time.**\n"}
{"snippet": "train_df = pd.read_csv('data/titanic-kaggle/train.csv')\ntest_df = pd.read_csv('data/titanic-kaggle/test.csv')\n", "intent": "The Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames.\n"}
{"snippet": "f = open('stations_weight.txt', 'rb')\nd = []\nfor line in f.readlines():\n    data = re.compile(r\"[\\.\\w-]+\").findall(line)\n    d.append(data)\nheader = ['station', 'weight']  \ndf = pd.DataFrame(d,columns=header)\ndf['weight'] = df['weight'].astype(float)\nDjoined = df.join(station_info, on='station')\n", "intent": "Import the weight information calculated from the above process (stations_weight.txt) and perform a join on station table\n"}
{"snippet": "dfMeasurementsYear = pd.read_csv('TMINcounts_year',delimiter='\\t',header=None,names=['year','N_TMIN'])\nmeasurements = ['TMAXcounts_year','TMIN365counts_year','TMAX365counts_year']\nnames = ['N_TMAX','N_TMIN365','N_TMAX365']\nfor j in range(len(measurements)):\n    df = pd.read_csv(measurements[j],delimiter='\\t',index_col=0,header=None,names=[names[j]])\n    dfMeasurementsYear = dfMeasurementsYear.join(df,on='year')\n", "intent": "Let us first look at the recording statistics by year. For this purpose, we read all required tables into a single dataframe.\n"}
{"snippet": "dfMean = pd.read_csv('TMINmean',delimiter=r\"[\\[,\\]\\t]+\",header=None,names=['prefix','partitionID','day','TMINmean'])\ndfMean = dfMean[dfMean.ix[:,'prefix'] != '\"header\"']\ndfMean = dfMean[dfMean.ix[:,'prefix'] != '\"other\"']\ndfMean = dfMean[dfMean.ix[:,'prefix'] != '\"incomplete\"'] \ndfMean = pd.pivot_table(dfMean,values='TMINmean',rows='partitionID',cols='day')\ndfMean.head()\n", "intent": "After successfully running the job, we can load the results into a dataframe\n"}
{"snippet": "dfCov = pd.read_csv('TMINcov',delimiter=r\"[\\[,\\]\\t]+\",header=None,names=['prefix','partitionID','i','j','cov'])\ndfCov = dfCov[2:].reset_index(drop=True).drop('prefix',1) \ndfCov = pd.pivot_table(dfCov,values='cov',rows=['partitionID','i'],cols='j')\ndfCov = dfCov.reset_index('i')\ndfCov.to_pickle('covTable.pkl')\ndfCov.head(10)\n", "intent": "We load the covariance matrix elements for each partition into a dataframe.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\n", "intent": "First we'll load the iris data as we did before:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X.transpose())\nV=pca.components_ \nprint 'eigenvectors=\\n',V\nprint 'rotation matrix=\\n',rotation\nprint 'Product=\\n',V*rotation\n", "intent": "For more documentation on sklearn.decomposition.PCA see http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"}
{"snippet": "def get_files(f):\n    df = pd.read_csv(f,header=None)\n    df.columns = ['file']\n    df.head(2)\n    return df\n", "intent": "process chipseq signals\n"}
{"snippet": "def map_features_to_colors(df_track, map_dict1, clrs):\n    map_dict2 = {}\n    for n, k in enumerate(map_dict1.keys()):\n        map_dict2[k] = clrs[n]\n    assert list(map_dict2.values()) == clrs\n    ndf_track = df_track.apply(lambda x: x.map(map_dict1)).fillna(0).T\n    return (ndf_track, map_dict2)\n", "intent": "plot hiv and histology track\n"}
{"snippet": "nt = '{}numeric_track.txt'.format(wkdir)\nntdf = pd.read_csv(nt)\nntdf.head(2)\n", "intent": "get meta data about library ids, hiv status, histology, total reads sequenced etc\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n        reshaped_segments, labels, test_size=0.2, random_state=RANDOM_SEED)\n", "intent": "Finally, let's split the data into training and test (20%) set:\n"}
{"snippet": "f1 = '/home/szong/projects/resource/chrominfo.txt'\nchrominfo = pd.read_csv(f1, sep='\\t', index_col='\nchrominfo.columns = ['size', 'file_name']\nchrominfo.head(2)\n", "intent": "look at GISTIC amp and del peak length distribution\n"}
{"snippet": "f = '/projects/trans_scratch/validations/workspace/szong/Cervical/cnv/bin_segs/amps_bins_1kb_patients.txt'\nrecur_amps.to_csv(f, index=False, sep='\\t')\nrecur_amps = pd.read_csv(f, sep='\\t')\n", "intent": "find systematic cnv noise\n"}
{"snippet": "f = '/projects/trans_scratch/validations/workspace/szong/POG/pog_skin_normals_and_ffpe.tsv'\ndf = pd.read_csv(f, sep='\\t')\ndf.head(2)\n", "intent": "look at skin normal and ffpe samples\n"}
{"snippet": "x, y = 3, 10\ndf = pd.DataFrame(np.random.randn(x, y),\n                  index=['sample_{}'.format(i) for i in range(1, x + 1)],\n                  columns=['gene_{}'.format(i) for i in range(1, y + 1)])\n", "intent": "We should create some simple example data for the purpose of illustration.\n"}
{"snippet": "f1 = '/projects/trans_scratch/validations/workspace/szong/EXPANDS_500/pog500_annotated_all_biopsies.vcf'\ndf1 = pd.read_csv(f, sep='\\t')\ndf1.head(2)\n", "intent": "determine number of variants to sample\n"}
{"snippet": "f = '/projects/trans_scratch/validations/workspace/szong/EXPANDS_500/cnv_files.txt'\ndf21 = pd.read_csv(f, sep='\\t', header=None)\ndf21.columns = ['lib', 'cnv']\ndf21.head(2)\n", "intent": "make cnv input file\n"}
{"snippet": "f = '/projects/trans_scratch/validations/workspace/szong/EXPANDS_500/all_cnv_files.txt'\ndf = pd.read_csv(f, sep='\\t', header=None)\ndf.head()\n", "intent": "processing ploidy/tc corrected cnv, get rid of negative values, replace with 0\n"}
{"snippet": "f = '/projects/trans_scratch/validations/workspace/szong/EXPANDS_500/sub_populations.txt'\ndf = pd.read_csv(f, sep='\\t', header=None)\ndf.head()\n", "intent": "visualize subpopulations\n"}
{"snippet": "f1 = '/projects/trans_scratch/validations/workspace/szong/Cervical/mutsig2cv/118_patients.txt'\npatients = pd.read_csv(f1, header=None)[0].values.tolist()\nassert len(patients) == 118\npatients[:3]\n'HTMCP-03-06-02007' in patients\n'HTMCP-03-06-02026' in patients\n", "intent": " 53 samples, need to remove 2 samples: 'HTMCP-03-06-02007', and 'HTMCP-03-06-02026'\n"}
{"snippet": "df_dropped['price'].fillna(value=np.round(df.price.mean(),decimals=2),\n                                inplace=True)\n", "intent": "Fill Missing Price values with mean price\n"}
{"snippet": "df1 = pd.DataFrame(np.random.randint(1,10,size=(10,5)), columns=list('abcde'))\ndf2 = pd.DataFrame(100+np.random.randint(1,10,size=(10,5)), columns=list('abcde'))\ndf = pd.concat([df1,df2])\ndf = df.reset_index(drop=True)\ndf3 = df+100\ndf4 = pd.merge(df, df3, left_index=True, right_index=True)\ndf4.head()\n", "intent": "https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/\n"}
{"snippet": "count_vectorizer = CountVectorizer(\n    analyzer=\"word\", tokenizer=nltk.word_tokenize,\n    preprocessor=None, stop_words='english', max_features=None)    \n", "intent": "Start with a simple baseline. Bag of words\n"}
{"snippet": "f1 = '/projects/mwarren_prj/mwarren_prj_results/VDB-241/all_annotated.tsv'\ndf1 = pd.read_csv(f1, sep='\\t',dtype={'chrom': str}) \ndf1 = df1[df1.chrom != 'MT']\nnew_chr = df1.chrom.str.replace('X', '23').replace('Y', '24')\ndf1.chrom = new_chr\ndf1.tail()\n", "intent": "159 tert mutations in a region of 51kb, this can be used to sanity check\n"}
{"snippet": "f3 = '/home/szong/projects/resource/centromere.pos.txt'\ncentromeres = pd.read_csv(f3, sep='\\t')\ncentromeres.head(2)\n", "intent": "exclude centromeres\n"}
{"snippet": "f1 = '/projects/mwarren_prj/mwarren_prj_results/VDB-241/all_annotated.tsv'\ndf1 = pd.read_csv(f1, sep='\\t',dtype={'chrom': str}) \ndf1 = df1[df1.chrom != 'MT']\nnew_chr = df1.chrom.str.replace('X', '23').replace('Y', '24')\ndf1.chrom = new_chr\ndf1.tail()\n", "intent": "pro-processing mutations: pick only modifier mutations, exclude mutations at centromere. convert X>23, Y>24, remove other unplaced contigs and MT\n"}
{"snippet": "f = '/projects/trans_scratch/validations/workspace/szong/Cervical/ml/melb_data.csv'\nmelbourne_data = pd.read_csv(f)\nmelbourne_data.head()\n", "intent": "data can be download from here: https://www.kaggle.com/dansbecker/starting-your-ml-project/data\n"}
{"snippet": "f1 = '/projects/trans_scratch/validations/workspace/szong/Cervical/cnv/cleaned_cnv_files.txt'\ndf1 = pd.read_csv(f1, sep='\\t', header=None, comment='\ndf1.columns = ['patient', 'seg_path']\nassert df1.shape[0] == 124\ndf1.head(2)\n", "intent": "visualize all cnvs in one plot\n"}
{"snippet": "f1 = '/projects/trans_scratch/validations/workspace/szong/Cervical/cnv/Cervical_124_patients_adjusted/cnv_files_for_acen_removal.txt'\ndf1 = pd.read_csv(f1, sep='\\t', header=None, comment='\ndf1.columns = ['patient', 'seg_path']\nassert df1.shape[0] == 123\ndf1.head(2)\n", "intent": "visualize all cnvs in one plot\n"}
{"snippet": "tf = '/projects/trans_scratch/validations/workspace/szong/Cervical/tcga_cn/gdac.broadinstitute.org_CESC.Merge_snp__genome_wide_snp_6__broad_mit_edu__Level_3__segmented_scna_minus_germline_cnv_hg19__seg.Level_3.2016012800.0.0/CESC.snp__genome_wide_snp_6__broad_mit_edu__Level_3__segmented_scna_minus_germline_cnv_hg19__seg.seg.txt'\ntdf = pd.read_csv(tf, sep='\\t')\ntdf.head(2)\n", "intent": "hiv status, need to run for hiv pos and neg seprately and then plot together\n"}
{"snippet": "df_dropped['user_type'].fillna(method='ffill',inplace=True)\n", "intent": "Fill Missing user_type values with value from previous row (forward fill) \n"}
{"snippet": "f = '/projects/trans_scratch/validations/workspace/szong/Cervical/cnv/Cervical_124_patients_new/del_genes.conf_99.txt'\ndf = pd.read_csv(f, sep='\\t', header=None)\ndf.head(5)\n", "intent": "look at GISTIC results\n"}
{"snippet": "itrain, itest = train_test_split(range(df.shape[0]), train_size=0.6)\nmask=np.ones(df.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\n", "intent": "Here we'll consider classification but Decision trees can be use for regression as we know.\n"}
{"snippet": "data = pd.read_csv( 'nba_2013.csv' )\ndata.head().T\n", "intent": "pos:\n- SF small forward\n- C center\n- PF power forward\n- SG shooting guard\n- PG point guard\n- G guard\n- F forward\n"}
{"snippet": "df2 = pd.read_csv('haberman.data',names=['age','yearoperation','posauxnodes','survival'])\ndf2_train, df2_test = train_test_split(df2,test_size=0.25)\ndf2.head()\n", "intent": "Draw the ROC curve (and calculate AUC) for the logistic regression classifier from challenge 12\n"}
{"snippet": "titanicdf = pd.read_csv('train.csv',header=0)\nprint titanicdf[titanicdf.PassengerId==38]\n", "intent": "Tackle the Titanic Survivors kaggle competition4 with decision trees. Look at your splits, how does your tree decide?\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nbindf_train, bindf_test = train_test_split(bindf,test_size=0.25)\n", "intent": "Split the data into a test and training set. But this time, use this function: from sklearn.cross_validation import train_test_split\n"}
{"snippet": "iris = datasets.load_iris()\nirisdf = pd.DataFrame(iris.data, columns=iris.feature_names)\nirisdf['target'] = iris.target\nirisdf.head()\n", "intent": "We'll be using the Iris dataset. Read more about the Iris dataset [here](https://en.wikipedia.org/wiki/Iris_flower_data_set)\n"}
{"snippet": "iris = datasets.load_iris()\nirisdf = pd.DataFrame(iris.data, columns=iris.feature_names)\nirisdf['target'] = iris.target\nirisdf.head()\n", "intent": "API Docs for [sklearn.neighbors.KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier)\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n", "intent": "Use `CountVectorizer` to generate vectorized text features.\n"}
{"snippet": "df_dropped['user_type'].fillna(method='bfill',inplace=True)\n", "intent": "Fill Missing user_type values with value from next row (backward fill)\n"}
{"snippet": "vectorizer = CountVectorizer(max_features = 1000,\n                             ngram_range=(1, 2),\n                             stop_words='english',\n                             binary=True)\nvectorizer.fit(data['body'])\nX_new_text_features = vectorizer.transform(data['body'])\n", "intent": "Use `CountVectorizer` to generate vectorized text feature for `data['body']`\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(max_features = 1000, \n                             ngram_range=(1, 2), \n                             stop_words='english')\nvectorizer.fit(data['body'])\nX_new_text_features = vectorizer.transform(data['body'])\n", "intent": "Use `TfidfVectorizer` to generate vectorized text features.\nDoes it perform better than `CountVectorizer`?\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ntitles = data['title'].fillna('')\nvectorizer = CountVectorizer(max_features = 1000, \n                             ngram_range=(1, 2), \n                             stop_words='english',\n                             binary=True)\n", "intent": "We previously used the Count Vectorizer to extract text features for this classification task\n"}
{"snippet": "from sklearn.preprocessing import MaxAbsScaler\nscaler = MaxAbsScaler()\n", "intent": "[Docs](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html) for `MaxAbsScaler`\n"}
{"snippet": "from sklearn import datasets, cross_validation, metrics\niris = datasets.load_iris()\n", "intent": "We will build a 3 layer network with 10, 20 and 10 hidden units respectively for the iris dataset\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nimport re\nspam_data = pd.read_csv('spam.csv')\nspam_data['target'] = np.where(spam_data['target']=='spam',1,0)\nspam_data.head(10)\n", "intent": "In this assignment you will explore text message data and create models to predict if a message is spam or not. \n"}
{"snippet": "import face_recognition\ndef face_detector2(img_path):\n    img = cv2.imread(img_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    try:\n        result = len(face_recognition.face_locations(gray, model='cnn')) > 0 \n    except RuntimeError:\n        result = len(face_recognition.face_locations(gray)) > 0 \n    return result\n", "intent": "I'll be using the face recognition package provided here: https://github.com/ageitgey/face_recognition\n"}
{"snippet": "def best_reg_method(X, best_regulari):\n    method_coefs = pd.DataFrame({'variable':X.columns, \n                                 'coef':best_regulari.coef_, \n                                 'abs_coef':np.abs(best_regulari.coef_)})\n    method_coefs.sort_values('abs_coef', inplace=True, ascending=False)\n    return method_coefs.head(27)\n", "intent": "    What are the features with coefficients greater than 0\n---\n"}
{"snippet": "from sklearn.model_selection import train_test_split \nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=1734)\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train);\n", "intent": "**Part A**: First, let's set a baseline by performing a train-validation split on the data and then fitting a logistic regression model. \n"}
{"snippet": "df_normalized = df.dropna().copy()\nmin_max_scaler = preprocessing.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(df_normalized['price'].values.reshape(-1,1))\ndf_normalized['price'] = np_scaled.reshape(-1,1)\n", "intent": "Normalize price values using  **Min-Max Scaler**\n"}
{"snippet": "import pandas as pd\nyelp = pd.read_csv('../data/yelp.csv')\nyelp.head(10)\n", "intent": "Read `yelp.csv` into a DataFrame.\n"}
{"snippet": "vectorized_count_train_data = vectorizer_count.fit_transform(train_data)\ntype(vectorized_count_train_data)\n", "intent": "Fit the instance to the training data\n"}
{"snippet": "from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_tf_idf = TfidfVectorizer()\n", "intent": "Let's try it again with tf-idf\n"}
{"snippet": "vectorized_train_data = vectorizer_tf_idf.fit_transform(train_data)\nclf_tf_idf = MultinomialNB()\nclf_tf_idf.fit(vectorized_train_data, train_target)\n", "intent": "Now fit a new NB classifer instance with the tf-idf vectorized data\n"}
{"snippet": "vectorized = CountVectorizer(max_features=1000, max_df=0.95, min_df=2, stop_words='english')\na = vectorized.fit_transform(df.content)\na.shape\n", "intent": "What is the size of the document-term matrix?\n"}
{"snippet": "W = model.fit_transform(a)\nH = model.components_\n", "intent": "Get the factors $\\text{W}$ and $\\text{H}$ from the resulting model.\n"}
{"snippet": "lda = LatentDirichletAllocation(n_topics=10,\n                                max_iter=5,\n                                learning_method='online',\n                                learning_offset=50.,\n                                random_state=42)\nlda.fit(vectorized)\nprint(\"Topics in LDA model:\")\nvectorizer = tfidf_vectorizer\ntf_feature_names = vectorizer.get_feature_names()  \nprint_top_words(lda, tf_feature_names)\n", "intent": "Experiment with the number of topics. What patterns emerge?\nWhat is the best number of topics?\n"}
{"snippet": "train_A = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\ntrainA = pd.read_csv(train_A)\ntrain_B = os.path.join(os.getcwd(), 'datasets', 'train_20news_partB.csv')\ntrainB = pd.read_csv(train_B)\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'train_auto_numeric.csv')\nauto_numeric = pd.read_csv(data_path, delimiter = ',')\nprint(auto_numeric.shape[0])\nprint(auto_numeric.shape[1])\n", "intent": "<font color='red'>TASK MARK: 1</font>\n<br>\n<font color='green'>COMMENT:  - </font>\n"}
{"snippet": "df_normalized = df.dropna().copy()\nrobust_scaler = preprocessing.RobustScaler()\nrs_scaled = robust_scaler.fit_transform(df_normalized['quantity_purchased'].values.reshape(-1,1))\ndf_normalized['quantity_purchased'] = rs_scaled.reshape(-1,1)\n", "intent": "Normalize quantity purchased values using  **Robust Scaler**\n"}
{"snippet": "auto_full_edit = auto_full.copy(deep = True)\ncols = [\"make\", \"fuel-type\", \"aspiration\" , \"num-of-doors\", \"body-style\" , \"drive-wheels\" , \"engine-location\" , \"engine-type\" , \"num-of-cylinders\", \"fuel-system\", \"symboling\"]\nfor i in cols:\n    le = LabelEncoder()\n    auto_full_edit[i] = le.fit_transform(auto_full_edit[i]) \nenc = OneHotEncoder(categorical_features = [1,2,3,4,5,6,7,12,13,15,23]) \nX_enc = enc.fit_transform(auto_full_edit.drop('price',axis=1))\nX_enc.shape\n", "intent": "<font color='red'>TASK MARK: 5</font>\n<br>\n<font color='green'>COMMENT:   Interestingly, you do one-hot-encode *all* the categories.</font>\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nseed = 11\nclean_dataset = 'data/galaxies-clean.csv'\ndf = pd.read_csv(clean_dataset)\nfeatures = ['mag_u', 'mag_g', 'mag_r', 'mag_i', 'mag_z']\nX = df[features]\ny = df['redshift']\n", "intent": "Load and prepare the dataset for using the scikit-learn library.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=seed)\n", "intent": "Create a training and test dataset using the `train_test_split` function.\n"}
{"snippet": "from sklearn.model_selection import train_test_split \nX_tr, X_te, Y_tr, Y_te = train_test_split(X, Y, test_size=0.25, random_state=42)\ntrain_points = go.Scatter(name=\"Train Data\", \n                          x=X_tr, y=Y_tr, mode='markers',  marker=dict(color=\"blue\", symbol=\"o\"))\ntest_points = go.Scatter(name=\"Test Data\",\n                         x=X_te, y=Y_te, mode='markers', marker=dict(color=\"red\", symbol=\"x\"))\npy.iplot([train_points, test_points])\n", "intent": "It is always a good habbit to split data into training and test sets.\n"}
{"snippet": "df = pd.DataFrame({\n    \"name\": [\"Goldy\", \"Scooby\", \"Brian\", \"Francine\", \"Goldy\"],\n    \"kind\": [\"Fish\", \"Dog\", \"Dog\", \"Cat\", \"Dog\"],\n    \"age\": [0.5, 7., 3., 10., 1.]\n}, columns = [\"name\", \"kind\", \"age\"])\ndf\n", "intent": "Here we create a toy dataframe of pets including their name and kind:\n"}
{"snippet": "from sklearn.feature_extraction import DictVectorizer\nvec_enc = DictVectorizer()\nvec_enc.fit(df.to_dict(orient='records'))\n", "intent": "The `DictVectorizer` encodes dictionaries by taking keys that map to strings and applying a one-hot encoding.\n"}
{"snippet": "flavor_enc = DictVectorizer()\nflavor_enc.fit(icecream[[\"flavor\"]].to_dict(orient='records'))\nonehot_flavor = flavor_enc.transform(icecream[[\"flavor\"]].to_dict(orient='records'))\n", "intent": "Here we will construct one hot encodings for the flavor and toppings in seperate calls so we know which columns correspond to each:\n"}
{"snippet": "train_data = pd.read_csv(\"toy_training_data.csv\")\nprint(train_data.describe())\ntrain_data.head()\n", "intent": "To illustrate the potential for feature transformations consider the following *synthetic dataset*:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ndata_tr, data_te = train_test_split(data, test_size=0.25, random_state=42)\nprint(\"Training Data Size: \", len(data_tr))\nprint(\"Test Data Size: \", len(data_te))\n", "intent": "Always split your data into training and test groups.  \n"}
{"snippet": "import pandas as pd \ndf = pd.read_csv(\"adult.csv\").drop('Unnamed: 0',axis=1)\n", "intent": "read_csv has all kinds of important tuning parameters which can make the reading and the saving of the data more efficient\n"}
{"snippet": "dfRenew  = pd.read_csv('./properatti.csv')  \ndfRenew.loc[acomprar]\n", "intent": "PROPIEDADES A COMPRAR SEGUN MI PRESUPUESTO\n"}
{"snippet": "df = pd.read_csv('../data/all_stocks_5yr.csv.zip', compression='zip')\ndf.head()\n", "intent": "The data was downloaded from https://www.kaggle.com/camnugent/sandp500\n"}
{"snippet": "result = pd.DataFrame({\n    'datetime':X_test_datetime.values,\n    'count':np.ceil(np.abs(pred))\n})\nresult.to_csv('sol.csv',index=False)\n", "intent": "1. hour proved to be working great than time\n2. using months instead of seasons\n3. using atemp and temp both\n4. using workingday and holiday both\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\ntrain_y_onehot = np.float32( enc.fit_transform(train_y.reshape(-1,1)) \\\n                   .toarray() )\nval_y_onehot = np.float32( enc.fit_transform(val_y.reshape(-1,1)) \\\n                 .toarray() )\ntest_y_onehot = np.float32( enc.fit_transform(test_y.reshape(-1,1)) \\\n                  .toarray() )\n", "intent": "Onehot-encoding the labels:\n"}
{"snippet": "import numpy as np\nfrom sklearn import preprocessing\nlabels = np.array([1,5,3,2,1,4,2,1,3])\nlb = preprocessing.LabelBinarizer()\nlb.fit(labels)\none_hot = lb.transform(labels)\nprint(one_hot)\n", "intent": "* Transforming your labels into one-hot encoded vectors is pretty simple with scikit-learn using <b style='color: red'>LabelBinarizer</b>. \n"}
{"snippet": "cluster.fillna('').head(20)\n", "intent": "And the actual ratings in the cluster look like this:\n"}
{"snippet": "content_image = scipy.misc.imread(\"images/Shawnee.jpg\")\nimshow(content_image)\n", "intent": "Here is my input image:\n"}
{"snippet": "from sklearn.datasets import fetch_20newsgroups\ncategories = ['alt.atheism', 'soc.religion.christian']\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\nnewsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\nclass_names = ['atheism', 'christian']\n", "intent": "For this tutorial, we'll be using the [20 newsgroups dataset](http://scikit-learn.org/stable/datasets/\nIn [2]:\n"}
{"snippet": "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\ntrain_vectors = vectorizer.fit_transform(newsgroups_train.data)\ntest_vectors = vectorizer.transform(newsgroups_test.data)\n", "intent": "Let's use the tfidf vectorizer, commonly used for text.\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=train.shape[1])\ntrain_pca = pca.fit_transform(train, )\ntest_pca = pca.transform(test)\n", "intent": "Data is projected into a new space where the new transformed features has no mutual correlation.\n"}
{"snippet": "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n", "intent": "We are going to use the MNIST dataset to train the generator and the discriminator. The generator will then generate handwritten digits.\n"}
{"snippet": "import numpy as np\nlabels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\nimagenet_labels = np.array(open(labels_path).read().splitlines())\nlabels_batch = imagenet_labels[np.argmax(result_batch, axis=-1)]\nlabels_batch\n", "intent": "Fetch the `ImageNet` labels, and decode the predictions\n"}
{"snippet": "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n                'Acceleration', 'Model Year', 'Origin'] \nraw_dataset = pd.read_csv(dataset_path, names=column_names,\n                      na_values = \"?\", comment='\\t',\n                      sep=\" \", skipinitialspace=True)\ndataset = raw_dataset.copy()\ndataset.tail()\n", "intent": "Import it using pandas\n"}
{"snippet": "hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()\n", "intent": "Visualize the model's training progress using the stats stored in the `history` object.\n"}
{"snippet": "text = open(path_to_file).read()\nprint ('Length of text: {} characters'.format(len(text)))\n", "intent": "First, look in the text.\n"}
{"snippet": "from sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42)\nX.shape\n", "intent": "Clustering\n=============\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nX, y = digits.data, digits.target\n", "intent": "A less trivial example\n-------------------------\n"}
{"snippet": "from sklearn.datasets import load_iris\nfrom sklearn.neighbors import KNeighborsClassifier\niris = load_iris()\nX, y = iris.data, iris.target\nn_samples = X.shape[0]\nprint(X.shape)\nprint(y.shape)\nprint(y)\n", "intent": "Cross-Validation\n=====================================\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\nboston.keys()\n", "intent": "Load the boston dataset:\n"}
{"snippet": "tr_data = pd.read_csv('../input/train.csv')\nte_data = pd.read_csv('../input/test.csv')\nprint 'train shape is: {} \\r\\n\\\ntest shape is: {}'.format(tr_data.shape,te_data.shape)\n", "intent": "now lets load our data set for this tutorial:\nthe Otto dataset \n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\ncv.fit(text_train)\nlen(cv.vocabulary_)\n", "intent": "<img src=\"bag_of_words.svg\" width=80%>\n"}
{"snippet": "from helpers import Timer\nfrom sklearn.datasets import load_digits\nfrom sklearn.cross_validation import train_test_split\ndigits = load_digits()\nX, y = digits.data / 16., digits.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n", "intent": "$$\\hat{y} \\approx w^T \\phi(\\mathbf{x})> 0$$\n"}
{"snippet": "cars, noncars = carSVM.load_dataset()\nprint(\"Cars' images:           {}\".format(len(cars)))\nprint(\"Non-cars' images:       {}\\n\".format(len(noncars)))\nindex = np.random.randint(0,len(cars))\ncar_img = mpimg.imread(cars[index])\nnoncar_img = mpimg.imread(noncars[index])\nprint(\"Car image size:         {}x{}x{}\".format(car_img.shape[0], car_img.shape[1], car_img.shape[2]))\nprint(\"Non-car image size:     {}x{}x{}\\n\".format(noncar_img.shape[0], noncar_img.shape[1], noncar_img.shape[2]))\nprint(\"Car image data type:    {}\".format(car_img.dtype))\nprint(\"Noncar image data type: {}\".format(noncar_img.dtype))\n", "intent": "Loading the data to show a brief summary of the data sets\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\nprint X_train_dtm.shape\nprint X_test_dtm.shape\n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('student_data.csv')\ndisplay(data.head())\ndata[data.columns[1:]].describe()\n", "intent": "To load the data, we will use a very useful data package called Pandas. You can read on Pandas documentation here:\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(X_features, Y, test_size=.3, random_state=42,\n                                                   stratify = ultimate_ml['retained'])\n", "intent": "Let's start by splitting the dataset into train and test data, and then we can SMOTE technique on training dataset for resampling.\n"}
{"snippet": "import pandas as pd\ndata_df = pd.read_csv('data/hourly_wages.csv')\n", "intent": "My version of this thing\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n", "intent": "Splitting the data:\n"}
{"snippet": "from sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nX, y, true_coefficient = make_regression(n_samples=80, n_features=30, n_informative=10, noise=100, coef=True, random_state=5)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)\nprint(X_train.shape)\nprint(y_train.shape)\n", "intent": "```\ny_pred = x_test[0] * coef_[0] + ... + x_test[n_features-1] * coef_[n_features-1] + intercept_\n```\n"}
{"snippet": "tr_data = pd.read_csv('../input/train.csv')\nte_data = pd.read_csv('../input/test.csv')\nprint('train shape is: {} \\r\\n\\ test shape is: {}'.format(tr_data.shape, te_data.shape))\n", "intent": "now lets load our data set for this tutorial:\nthe Otto dataset \n"}
{"snippet": "import numpy as np\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.keys()\n", "intent": "Get some data to play with\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(digits.data,\n                                                    digits.target)\n", "intent": "Split the data to get going\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv(\n    \"adult.data\", header=None, index_col=False,\n    names=['age', 'workclass', 'fnlwgt', 'education',  'education-num',\n           'marital-status', 'occupation', 'relationship', 'race', 'gender',\n           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n           'income'])\ndata = data.drop(\"fnlwgt\", axis=1)\ndata.head()\n", "intent": "Apply dummy encoding and scaling to the \"adult\" dataset consisting of income data from the census.\nBonus: visualize the data.\n"}
{"snippet": "from sklearn.datasets import load_diabetes\ndiabetes = load_diabetes()\ndata = diabetes.data\ntarget = diabetes.target\n", "intent": "Load the diabetes dataset using ``sklearn.datasets.load_diabetes``. Apply ``LinearRegression`` and ``Ridge`` and visualize the coefficients.\n"}
{"snippet": "y = data['income']\nX = data.drop('income', axis=1)\nX = pd.get_dummies(X)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n", "intent": "Do it again keeping a test apart\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\n", "intent": "Let's try with Digits\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "Let's try with Boston data (a RandomForestRegressor with different n_estimators and max_depth)\n"}
{"snippet": "df_train = pd.read_csv('../train.csv')\ndf_store = pd.read_csv('../store.csv')\n", "intent": "also the procedure of how I went thorugh of doing all the load and transform jobs\n"}
{"snippet": "func = lambda x: 2 + 0.5 * x + 3 * x**2 + 5 * stats.norm.rvs(0, 10)\ndf = pd.DataFrame()\ndf[\"x\"] = list(range(0, 30))\ndf[\"y\"] = list(map(func, df[\"x\"])) \ndf.plot(x='x', y='y', kind='scatter')\n", "intent": "Next we look at a data set that needs a quadratic fit. Let's do both a linear and quadratic fit and compare.\n"}
{"snippet": "TARGET_VAR= 'target'\nTOURNAMENT_DATA_CSV = 'numerai_tournament_data.csv'\nTRAINING_DATA_CSV = 'numerai_training_data.csv'\nBASE_FOLDER = 'numerai/'\ndf_train = pd.read_csv(BASE_FOLDER + TRAINING_DATA_CSV)\ndf_train.head(5)\n", "intent": "- Numerai provides a data set that is allready split into train, validation and test sets. \n"}
{"snippet": "url = '../assets/dataset/bikeshare.csv'\nbikes = pd.read_csv(url, parse_dates=True)\nbikes['dteday'] = pd.to_datetime(bikes['dteday'])\nbikes.set_index('dteday',inplace=True)\n", "intent": "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X[feature_cols], y, train_size = 0.7)\nlr.fit(X_train, Y_train)\nprint('R^2 for training data')\nprint(lr.score(X_train, Y_train))\nprint('R^2 for testing data')\nlr.score(X_test, Y_test)\n", "intent": "Load the Boston housing data.  Fix any problems, if applicable.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X[feature_cols], y, train_size = 0.7)\nlr.fit(X_train, Y_train)\nprint('R^2 for training data')\nprint(lr.score(X_train, Y_train))\nprint('R^2 for testing data')\nlr.score(X_test, Y_test)\n", "intent": "- Score and plot.  \n- How do your metrics change?\n"}
{"snippet": "df = pd.read_csv(\"../assets/datasets/iris.csv\")\nprint(df['Name'].value_counts())\ndf.head(5)\n", "intent": "Let's do some clustering with the iris dataset.\n"}
{"snippet": "X_scaled = preprocessing.MinMaxScaler().fit_transform(df[cols]) \n", "intent": "Next, since each of our features have different units and ranges, let's do some preprocessing:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\npca.fit(df.drop('inflation', axis=1))\npca.explained_variance_ratio_\n", "intent": "The below code shows that with two \"principal components\" you can capture > 97% of the variation!\n"}
{"snippet": "pca = PCA(n_components = None)\nX_train_pca = pca.fit_transform(X_train)\nexplained_variance = pca.explained_variance_ratio_\n", "intent": "implementing pca to reduce to an lower dimension to increase the readability\n"}
{"snippet": "data_path_partA = os.path.join(os.getcwd(), 'datasets', 'train_20news_partA.csv')\ndata_path_partB = os.path.join(os.getcwd(), 'datasets', 'train_20news_partB.csv')\nnews_A = pd.read_csv(data_path_partA, delimiter = ',')\nnews_B = pd.read_csv(data_path_partB, delimiter = ',')\n", "intent": "Load the datasets `train_20news_partA.csv` and `train_20news_partB.csv` into two separate pandas DataFrames.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'splice_train.csv')\nsplice_train = pd.read_csv(data_path, delimiter = ',')\ndata_path = os.path.join(os.getcwd(), 'datasets', 'splice_test.csv')\nsplice_test = pd.read_csv(data_path, delimiter = ',')\nprint('Number of instances: {}, number of attributes: {}'.format(splice_train.shape[0], splice_train.shape[1]))\nsplice_train.head(5)\nprint('Number of instances: {}, number of attributes: {}'.format(splice_test.shape[0], splice_test.shape[1]))\nsplice_test.head(5)\n", "intent": "Load the `splice_train.csv` and `splice_test.csv` into two separate dataframes. Display the shape and first 10 instances for each dataframe.\n"}
{"snippet": "from kaggle_data import load_data, preprocess_data, preprocess_labels\nX_train, labels = load_data('data/kaggle_ottogroup/train.csv', train=True)\nX_train, scaler = preprocess_data(X_train)\nY_train, encoder = preprocess_labels(labels)\nX_test, ids = load_data('data/kaggle_ottogroup/test.csv', train=False)\nX_test, _ = preprocess_data(X_test, scaler)\nnb_classes = Y_train.shape[1]\nprint(nb_classes, 'classes')\ndims = X_train.shape[1]\nprint(dims, 'dims')\n", "intent": "See: [Data Description](1.2 Introduction - Tensorflow.ipynb\n"}
{"snippet": "everything.to_csv(\"../assets/everything.csv\",encoding=\"utf-8\")\n", "intent": "save \"everything\" as a csv for use in Tableau later\n"}
{"snippet": "dummyColumns = dataWithDummies.columns\npca = PCA()\npipe = Pipeline([('pca', pca)])\ntmp = np.array(dataWithDummies)\ndataOriginal = pipe.fit_transform(tmp)\ndataOriginal = pd.DataFrame(dataOriginal,columns=dummyColumns)\ndataOriginal.head()\n", "intent": "This set of data contains all the columns from the operations table + the airports as dummies\n"}
{"snippet": "df = pd.DataFrame(pca.components_,columns=dummyColumns)\nsns.heatmap(df)\n", "intent": "PC0 and PC2 is conal.\n"}
{"snippet": "corr = df.corr()\nyXCorr = corr.iloc[1,2:]\nyXCorr = abs(yXCorr)\nyXCorr = pd.DataFrame(yXCorr)\nyXCorr.sort_values(by=yXCorr.columns[0],inplace=True)\nyXCorr.index[0:3]\nX = df.iloc[:,yXCorr.index[-3:]]\nX.head()\n", "intent": "What sort of strategy might one take to drop features?\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df.Car)\nle.transform(df.Car)\ndf[\"Car\"] = le.transform(df.Car)\ndf.head()\n", "intent": "Loop through Cars List and Convert to Numeric. **HINT:** Reference the lesson for help with this!\n"}
{"snippet": "df = pd.DataFrame(adult)\ndf.head()\n", "intent": "Convert the data to a Pandas dataframe to work with the data:\n"}
{"snippet": "df[\"workclass\"] = preprocessing.LabelEncoder().fit_transform(df[\"workclass\"])\ndf[\"marital-status\"] = preprocessing.LabelEncoder().fit_transform(df[\"marital-status\"])\ndf[\"occupation\"] = preprocessing.LabelEncoder().fit_transform(df[\"occupation\"])\ndf[\"relationship\"] = preprocessing.LabelEncoder().fit_transform(df[\"relationship\"])\ndf[\"race\"] = preprocessing.LabelEncoder().fit_transform(df[\"race\"])\ndf[\"sex\"] = preprocessing.LabelEncoder().fit_transform(df[\"sex\"])\ndf[\"income\"] = preprocessing.LabelEncoder().fit_transform(df[\"income\"])\ndf[\"native-country\"] = preprocessing.LabelEncoder().fit_transform(df[\"native-country\"])\n", "intent": "Convert the categorical Data to numeric for our analysis. **HINT:** Refer to lesson 1.1 for writing a function of this sort\n"}
{"snippet": "X = pd.DataFrame(iris.data,columns=iris.feature_names)\ny = pd.DataFrame(iris.target,columns=[\"Class\"])\nX.head()\n", "intent": "Define the \"x\" and \"y\" variables. *Remember*, y is our classifer, and x is our attributes.\n"}
{"snippet": "df = pd.read_csv(\"../../assets/datasets/iris.csv\")\n", "intent": "We're going to load the iris data from the scikit \"datasets\" package\n"}
{"snippet": "fpred=[]\nfor i in xrange(len(predictions)):\n img=imresize((predictions[i][0]), (420,580))/255\n fpred.append((img,predictions[i][1]))\n", "intent": "But the original size of the images was 420x580, so let's resize our predictions \n"}
{"snippet": "def movie_parser(m):\n    return [int(m['num_votes']), float(m['rating']),m['tconst'],m['title'],int(m['year'])]\nparsed = [movie_parser(m) for m in top250]\nmovies = pd.DataFrame(parsed, columns=['num_votes','rating','tconst','title','year'])\n", "intent": "Keep the fields:\n    num_votes\n    rating\n    tconst\n    title\n    year\nAnd discard the rest\n"}
{"snippet": "path = '../rsc/_.txt'\ntext = open(path).read().lower()\nprint('corpus length:', len(text))\n", "intent": "Pick text corpora you would like to work on.\nTo chose: pantadeusz, potop, linux, nietzsche\n"}
{"snippet": "diabetic_data.to_csv('diabetic_data_clean.csv',index=False)\ndiabetes3 = pd.read_csv('diabetic_data_clean.csv')\nreadmitted_dummies = pd.get_dummies(diabetes3.readmitted, prefix='readmitted').iloc[:, 1:]\ndiabetes3 = pd.concat([diabetes3, readmitted_dummies], axis=1)\nfeature_cols = ['admission_type_id','readmitted', 'gender', 'age','diabetesMed', 'race', 'num_medications', 'number_diagnoses', 'time_in_hospital']\ndiabetes3.head(10)\n", "intent": "* Increases model accuracy but decreases the interpretability of the model\n"}
{"snippet": "import pandas as pd\ntitanic = pd.read_csv('titanic.csv', index_col='PassengerId')\ntitanic.head(5)\n", "intent": "For a description of the Titanic dataset see this Kaggle page: https://www.kaggle.com/c/titanic/data\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nXsubset, _, ysubset, _ = train_test_split(X, y, train_size=0.1)\n", "intent": "Let's start by working with just a subset of the data, because these things can be pretty slow:\n"}
{"snippet": "traindf = pd.read_csv('../input/train.csv')\ntestdf = pd.read_csv('../input/test.csv')\ntraindf.head()\n", "intent": "As a first start we will read in the training and testing data.\n"}
{"snippet": "weatherdf = pd.read_csv('../input/weather.csv').drop(['Station','CodeSum'],axis = 1)\nweatherdf = cleanse_data(weatherdf)\nweatherdf = weatherdf.set_index('Date')\nweatherdf.head()\n", "intent": "We are also provided weather data, which is believed to have an important impact on WNV occurrence. Let's take a look.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2017)\n", "intent": "Split the data into a test (20%) and train set.\n"}
{"snippet": "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.999)\nprint ('\\nExamples in training set: ' , str(len(x_train)))\nprint ('Examples in testing set: ' , str(len(x_test)))\nprint ('Dimensions of training set: ' , str(np.asarray(x_train).shape))\nprint ('Dimensions of testing set: ' , str(np.asarray(x_test).shape))    \n", "intent": "Now let's create a test/train data set split.\n"}
{"snippet": "tic = time()\ncnt_vec = CountVectorizer(tokenizer=tokenizer.tokenize, analyzer='word', ngram_range=(1,1), max_df=0.8, min_df=2, max_features=1000, stop_words=stop_words)\ncnt_vec.fit(X_train)\ntoc = time()\nprint \"elapsed time: %.2f sec\" %(toc - tic)\nvocab = cnt_vec.vocabulary_\nidx2word = {val: key for (key, val) in vocab.items()}\nprint \"vocab size: \", len(vocab)\nX_train_vec = cnt_vec.transform(X_train) \nX_test_vec = cnt_vec.transform(X_test)\n", "intent": "We'll use a count vectorizer to produce a vector of word counts for each document while filtering stop and low-frequency words.\n"}
{"snippet": "data = pd.read_csv(\"adult.csv\", index_col=0)\n", "intent": "Apply dummy encoding and scaling to the \"adult\" dataset consisting of income data from the census.\nBonus: visualize the data.\n"}
{"snippet": "df2015a.to_csv('project3.csv')\n", "intent": "Present your conclusions and results, including a Tableau Storyboard. If you have more than one interesting model feel free to include.\n"}
{"snippet": "sf_crime = pd.read_csv('../../assets/datasets/sf_crime_train.csv')\nsf_crime = sf_crime.dropna(inplace=True)\n", "intent": "Multinomial Logit\nBasically we have more than two(binomial) outcomes or classes. Let's look at recent San Francisco data set\n"}
{"snippet": "subset = ['ASSAULT','VANDALISM'] \nsf_crime_sub = sf_crime[sf_crime['Category'].str.contains('|'.join(subset))]\nX = patsy.dmatrix('~ C(hour) + C(DayOfWeek) + C(PdDistrict) + X + Y', sf_crime_sub) \nY = sf_crime_sub.Category.values\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, stratify=Y, random_state=5)\n", "intent": "Setting Thresholds for binary Classes Let's rebuild our target to have two classes, and review how to optimize thresholds\n"}
{"snippet": "subset = ['ASSAULT','VANDALISM'] \nsf_crime_sub = sf_crime[sf_crime['Category'].str.contains('|'.join(subset))]\nX = patsy.dmatrix('~ C(hour) + C(DayOfWeek) + C(PdDistrict) + X + Y', sf_crime_sub) \nY = sf_crime_sub.Category.values\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, stratify=Y, random_state=5)\n", "intent": "Setting Thresholds for binary Classes\nLet's rebuild our target to have two classes, and review how to optimize thresholds\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_norm =  StandardScaler().fit_transform(X)\nmodel.fit(X_norm, y)\ncoeffs = pd.DataFrame(model.coef_, columns = iris.feature_names, index =iris.target_names)\ncoeffs\n", "intent": "Let's normalize the data and repeat the exercise:\n"}
{"snippet": "lb = preprocessing.LabelBinarizer()\nlb.fit_transform(['yes', 'no', 'no', 'yes'])\n", "intent": "Binary targets transform to a column vector\n"}
{"snippet": "cdf = pd.read_csv('../../assets/datasets/cars.csv')\n", "intent": "Visualize the last tree. Can you make sense of it? What does this teach you about decision tree interpretability?\n"}
{"snippet": "pd.DataFrame(cm, index=[le.classes_, columns=['Predicted_'+ X for i in le.classes_]])\n", "intent": "now to build a dataframe...\n"}
{"snippet": "DATA_PATH = '/data/vision/fisher/data1/vsmolyakov/time_series/ECG5000/'\ntrain_df = pd.read_csv(DATA_PATH + \"/ECG5000_TRAIN\")  \ntest_df = pd.read_csv(DATA_PATH + \"/ECG5000_TEST\")\nX_train, y_train = train_df.iloc[:,1:].values, train_df.iloc[:,0].values\nX_test, y_test = test_df.iloc[:,1:].values, test_df.iloc[:,0].values\n", "intent": "The ECG5000 dataset can be downloaded from http://www.cs.ucr.edu/~eamonn/time_series_data/\n"}
{"snippet": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfeatures = [c for c in df.columns if c != 'acceptability']\nfor c in df.columns:\n    df[c] = le.fit_transform(df[c])\nX = df[features]\ny = df['acceptability']\n", "intent": "Since most of the features are categorical text we will need to encode them as numbers using the LabelEncoder:\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\nprint y_train[:3]\ny_train, y_test = ohe.fit_transform(y_train), ohe.fit_transform(y_test)\nprint y_train[:3]\n", "intent": "One-hot encode the labels first.\n"}
{"snippet": "X = data[0]\ny = data[1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nY_train = np_utils.to_categorical(y_train, 2) \nY_test = np_utils.to_categorical(y_test, 2)\nprint(\"y_train labels: \",y_train.shape)\nprint(\"Y_train one-hot labels: \\n\",Y_train.shape)\n", "intent": "Convert to one-hot labels.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)\n", "intent": "Split your data in to train and test sets.\n"}
{"snippet": "import keras\nfrom keras.datasets import cifar10\nfrom tflearn.datasets import cifar10\n(x_train, y_train), (x_test, y_test) =cifar10.load_data(dirname=\"/tmp/data\")\n", "intent": "---\nIn this notebook, we train a CNN on augmented images from the CIFAR-10 database.\n"}
{"snippet": "merged_routes['Codeshare'] = merged_routes.Codeshare.fillna('N')\n", "intent": "Fill in Nan values:\n"}
{"snippet": "pd.DataFrame(stats, columns=['Average Degree', \n                             'Standard Dev Degree', \n                              \"Maxium degree\",\n                             'Density', \n                             'Diameter', \n                             'Average path lenth']\n            ).describe()\n", "intent": "And here we calculate statistical measures of different graph properties:\n"}
{"snippet": "network_stats = np.array(Stats)\nx = StandardScaler().fit_transform(network_stats)\n", "intent": "Put everything into a matrix, do PCA for dimensionality reduction & cluster:\n"}
{"snippet": "scatter_data = pd.DataFrame(principalComponents)\nscatter_data['name'] = airlines\nscatter_data['labels'] = cluster.labels_\nscatter_data['country'] = scatter_data.name.map(airline_to_countryid)\nscatter_data['ontime'] = scatter_data.name.map(Delays_data.set_index('On-time')['On-time (A14)'].to_dict())\nscatter_data['delay'] = scatter_data.name.map(Delays_data.set_index('On-time')['Avg. Delay'].to_dict())\nscatter = scatter_data[scatter_data.delay.notna()].copy()\n", "intent": "**Analyzing inference from eignevalues to Delays**\n"}
{"snippet": "np.random.seed(0)\ndata, target = make_classification(\n    n_samples=1000,\n    n_features=45,\n    n_informative=12,\n    n_redundant=7\n)\ntarget = target.ravel()\n", "intent": "Load dataset and target values:\n"}
{"snippet": "proj_main = pd.read_csv('project_main.csv', delimiter =';').set_index('project_id') \nproj_main.head(3)\n", "intent": "Now let's move to the next data, **project_main.csv**\nStarting with importing the data into a dataframe with pandas.\n"}
{"snippet": "proj_main.to_csv('cleaned_project_main.csv')\n", "intent": "Lastly, let's save it to new csv file, **cleaned_project_main.csv**.\n"}
{"snippet": "project_unit = pd.read_csv('cleaned_project_unit.csv')\nproject_main = pd.read_csv('cleaned_project_main.csv')\nproject_unit_main = project_main.merge(project_unit, on = ['project_id'])\nproject_unit_main.head(3)\n", "intent": "So, let's get our **cleaned_project_unit** and **cleaned_project_main** to be ready to use.\n"}
{"snippet": "item_feature.to_csv('item_feature.csv', index = False)\n", "intent": "Finally, let's save it to new csv file, **item_feature.csv**.\n"}
{"snippet": "userLog = pd.read_csv('userLog_201801_201802_for_participants.csv', delimiter = ';')\n", "intent": "Let's apply the same process to **userLog_201801_201802_for_participants.csv**.\n"}
{"snippet": "userLog.to_csv('user_feature.csv', index = False)\n", "intent": "Lastly, let's save it to new csv file, **user_feature.csv**.\n"}
{"snippet": "imp = Imputer(strategy='mean')\nX_replace_with_mean = imp.fit_transform(X_new)\n", "intent": "Then we replace the missing values with mean of the corresponding columns.\n"}
{"snippet": "col = ['rss','intercept'] + ['coef_X_%d'%i for i in range(1,30)]\nind = ['model_pow_%d'%i for i in range(1,30)]\ncoef_matrix_simple = pd.DataFrame(index=ind, columns=col)\nmodels_to_plot = {1:231,3:232,6:233,12:234,18:235,29:236}\nfor i in range(1,30):\n    coef_matrix_simple.iloc[i-1,0:i+2] = linear_regression(data, power=i, models_to_plot=models_to_plot)\n", "intent": "In the cell below we show the fit for a number of polynomials of varying degrees. \nWhich do you think show bias? And which over-fitting?\n"}
{"snippet": "scaler = StandardScaler()\nX = scaler.fit_transform(X)\n", "intent": "Scale the training data X using the standard scalar. Store in X\n"}
{"snippet": "scaler = MinMaxScaler(feature_range=(0,1))\ndataset = scaler.fit_transform(dataset)\n", "intent": "We scale the closing price to 0 to 1 range:\n"}
{"snippet": "loans = pd.read_csv('loan_data.csv')\nloans.head()\n", "intent": "Read loan_data.csv as a dataframe called loans\n"}
{"snippet": "df = pd.read_csv('KNN_Project_Data')\ndf.head(3)\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "** Create a StandardScaler() object**\n"}
{"snippet": "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head(3)\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)\n", "intent": "**Creating and Training and Test data **\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=77)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "train = pd.read_csv('titanic_train.csv')\ntrain.head(2)\n", "intent": "We will be working with the [Titanic Data Set from Kaggle](https://www.kaggle.com/c/titanic) downloaded as titanic_train.csv file\n"}
{"snippet": "data = pd.read_csv('data/data.csv')\ndata.info()\n", "intent": "   - Download [spotify dataset from Kaggle](https://www.kaggle.com/geomack/spotifyclassification)\n"}
{"snippet": "node_model = manifold.LocallyLinearEmbedding(n_components=2, n_neighbors=6, eigen_solver='dense')\nembedding = node_model.fit_transform(X.T).T\n", "intent": "Compute a 2D embedding for visualization:\n"}
{"snippet": "messages = pd.read_csv('smsspamcollection/SMSSpamCollection',\n                       sep='\\t',names=['label','message'])\nmessages[:5]\n", "intent": "We'll use **read_csv** and make note of the **sep** argument, we can also specify the desired column names by passing in a list of *names*.\n"}
{"snippet": "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n", "intent": "We'll go ahead and check what is the IDF (inverse document frequency) of the word **`months`**\n"}
{"snippet": "X0 = df.values.astype(float)\nscaler = StandardScaler()\nX = scaler.fit_transform(X0)\n", "intent": "First, let us consider the case of scaled data via `StandardScaler`.\n"}
{"snippet": "pca = PCA()\npca.fit(X)\nfeatures = range(1,pca.n_components_+1)\n", "intent": "We fit use PCA and fit to the data.\n"}
{"snippet": "pca = PCA(n_components=3)\npca_transformed = pca.fit_transform(df.values.astype(float))\n", "intent": "Now we proceed to perform PCA reduction to $D=3$.\n"}
{"snippet": "X0 = df.values.astype(float)\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X0)\n", "intent": "Finally, we attempt to perform a clustering of the complete data set without performing a dimensional reduction. We start with a scaling:\n"}
{"snippet": "df = pd.read_csv('processed_data/df.csv')\ndf_type = pd.read_csv('processed_data/df_type.csv')\n", "intent": "`df` contains the complete data set we used in clustering and `df_type` contains the identification for 911 fire calls `Type`:\n"}
{"snippet": "matrix = df_transactions.pivot_table(index=['customer_name'], columns=['offer_id'], values='n').rename_axis(None, axis=1)\nmatrix = matrix.fillna(0).reset_index().rename(columns={'customer_name':'C_Name'})\nmatrix.head()\n", "intent": "Let us obtain a dataframe with such characteristics. We may use `pivot_table`:\n"}
{"snippet": "data2 = pd.DataFrame({'Age':  [17,64,18,20,38,49,55,25,29,31,33], 'Salary': [25,80,22,36,37,59,74,70,33,102,88], 'Loan Default': [1,0,1,0,1,0,0,1,1,0,1]})\ndata2\n", "intent": "Let's consider a more complex example by adding the \"Salary\" variable (in the thousands of dollars per year).\n"}
{"snippet": "calendar = pd.read_csv('calendar.csv.gz')\nprint('We have', calendar.date.nunique(), 'days and', calendar.listing_id.nunique(), 'unique listings in the calendar data.')\n", "intent": "How busy is it for Airbnb hosts in Toronto?\n"}
{"snippet": "train_raw = pd.read_csv('datasets/dataset_5_train.txt' , header = None)\ntest_raw = pd.read_csv('datasets/dataset_5_test.txt', header = None)\nex_table = pd.read_csv('datasets/dataset_5_description.txt', skiprows = 12 , delimiter = '\\t')\nex_text = pd.read_csv('datasets/dataset_5_description.txt', delimiter = ':', skiprows = range(11,26))\ntrain_raw.head()\nex_census = pd.read_csv('datasets/dataset_5_description.txt', delimiter = '\\t', skiprows = 12)\ntrain_raw.head()\n", "intent": "<font color = 'blue'>\n</font>\n<br>\n<br>\n"}
{"snippet": "results_array = np.array([[scorea_test , prun_score , dfeat_score] ,\n                          [gina_test , gin1_test , gin2_test] , \n                          [enta_test , ent1_test , ent2_test]])\nresults_df = pd.DataFrame(results_array , columns = ['Simple' , 'Prune' , 'Double_Feat'] ,\n                          index = ['score' , 'Gini' , 'Entropy'])\nresults_df\n", "intent": "<br>\n** c. Aggregating Results:**\n<br>\n"}
{"snippet": "data_raw = pd.read_csv('datasets\\dataset_1.txt')\ndata_raw.head(10)\n", "intent": "<br>\n**1. Load Data set and inspect it:**\n"}
{"snippet": "data_2 = pd.read_csv('datasets\\dataset_2.txt')\ndata_2.head(10)\n", "intent": "<br>\n**1. Read dataset and examine columns:**\n"}
{"snippet": "data_2_expanded = pd.DataFrame({}) \nfor column in data_2_new.columns:\n    if(data_2_new[column].dtype != np.dtype('object')):\n        data_2_expanded = pd.concat([data_2_expanded, data_2_new[column]], axis=1)\n    else:\n        encoding = pd.get_dummies(data_2_new[column])\n        data_2_expanded = pd.concat([data_2_expanded, encoding], axis=1)\nprint '\\n The number of predictors after the one-hot binary encoding is : ', data_2_expanded.iloc[:,:-1].shape[1]\nprint ' Which is the required number as stated\\n'\n", "intent": "**3. Apply get dummies for the rest of the categorical variables and expand the dataframe:**\n"}
{"snippet": "itrain, itest = train_test_split(xrange(df.shape[0]), train_size=0.6)\nmask=np.ones(df.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\n", "intent": "Here we'll consider classification but Decision trees can be use for regression as we know.\n"}
{"snippet": "def references_organisation(title):\n    parsed = nlp_toolkit(title)\n    return any([word.ent_type_ == \"ORG\" for word in parsed])\ndata[\"references_organisation\"] = data[\"title\"].fillna(u\"\").map(references_organisation)\ndata[data[\"references_organisation\"]][[\"title\"]].head()\n", "intent": "Let's see if we can find organisations in our page titles\n"}
{"snippet": "titles = data[\"title\"].fillna(\"\")\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectoriser = CountVectorizer(max_features = 1000,\n                             ngram_range = (1, 2),\n                             stop_words = \"english\",\n                             binary = True)\nvectoriser.fit(titles)\nX = vectoriser.transform(titles)\n", "intent": "We previously used the `CountVectorizer` to extract text features for this classification task\n"}
{"snippet": "import numpy as np\nfrom sklearn import preprocessing\nlabels = np.array([1,5,3,2,1,4,2,1,3])\nlb = preprocessing.LabelBinarizer()\nlb.fit(labels)\nlb.transform(labels)\n", "intent": "Transform labels into binary vectors using scikit learn LabelBinarizer.\n"}
{"snippet": "listings['security_deposit'] = listings['security_deposit'].fillna(value=0)\nlistings['security_deposit'] = listings['security_deposit'].replace( '[\\$,)]','', regex=True ).astype(float)\nlistings['cleaning_fee'] = listings['cleaning_fee'].fillna(value=0)\nlistings['cleaning_fee'] = listings['cleaning_fee'].replace( '[\\$,)]','', regex=True ).astype(float)\n", "intent": "Same way to clean up the other money value columns.\n"}
{"snippet": "for col in df.columns:\n    try:\n        mean = df[col].mean()\n        df[col] = df[col].fillna(value=mean)\n    except:\n        pass\ndf.head()\n", "intent": "Old: Fill all the null values with zero.\n"}
{"snippet": "scaled_features = pd.DataFrame(scaled_features, columns=bank_notes.drop('Class',axis=1).columns)\nscaled_features.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "colleges = pd.read_csv('College_Data', index_col = 0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "classified_data = pd.read_csv('KNN_Project_Data')\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "data_features = pd.DataFrame(scaled_features, columns=classified_data.columns[:-1])\ndata_features.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X = yelp_class['text']\ny = yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "import json\nimport urllib.request\nurl = \"http://api.fixer.io/2014-01-03\"\nresponse_bytes = urllib.request.urlopen(url).read()\nprint(response_bytes)\n", "intent": "The following snippet demonstrates how to fetch exchange rate EUR to X for a given date using a so called \"web service\".\n"}
{"snippet": "import pandas as pd\ndataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\ndata = pd.read_csv(dataset_url, delimiter=\";\")\nprint(\"shape (rows, cols) is\", data.shape)\nprint(\"column names are\", data.columns)\nprint(data.head())  \nprint(data.tail())  \n", "intent": "pandas allows reading different file formats from different sources:\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconmat = np.array(confusion_matrix(Y_test, Y_pred))\nconfusion = pd.DataFrame(conmat, index=['is_healthy', 'has_cancer'],\n                         columns=['predicted_healthy', 'predicted_cancer'])\nconfusion\n", "intent": "Look at the confusion matrix\n"}
{"snippet": "df_with_year = df['age'] > 1000\ndf.loc[df_with_year, 'age'] = 2015 - df.loc[df_with_year, 'age']\ndf.loc[df.age > 95, 'age'] = np.nan\ndf.loc[df.age < 16, 'age'] = np.nan\ndf['age'].fillna(-1, inplace=True)\n", "intent": "Convert year to age, set limits to age, and fill NaNs with -1.\n"}
{"snippet": "lasso_coefs = pd.DataFrame({'variable':X.columns, 'coef':lasso.coef_,\n                            'abs_coef':np.abs(lasso.coef_)})\nlasso_coefs.sort_values('abs_coef', inplace=True, ascending=False)\nlasso_coefs.head(30)\n", "intent": "Many indicators have been zeroed out, leaving only 86 out of over a thousand remaining to have an effect.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXn = ss.fit_transform(X)\n", "intent": "Lasso regularization selects for only a subset of the provided variables. It is very useful for removing the effects of redundant variables\n"}
{"snippet": "from sklearn.linear_model import ElasticNetCV\ntarget = 'SalePrice'\nnon_target = [col for col in res_house.columns.tolist() if col != target]\nformula = target + ' ~ ' + ' + '.join(non_target) + ' -1'\ny, X = patsy.dmatrices(formula, data=res_house, return_type=\"dataframe\")\nXn = ss.fit_transform(X)\noptimal_enet = ElasticNetCV(l1_ratio=np.linspace(0.01, 1.0, 50), n_alphas=500, cv=5, verbose=1)\noptimal_enet.fit(Xn, y)\nprint optimal_enet.alpha_\nprint optimal_enet.l1_ratio_\n", "intent": "The Elastic Net is a combination of Ridge and Lasso regression.\n"}
{"snippet": "candidates = \"realDonaldTrump BernieSanders\".split(\" \")\ncandidatesRDD = sc.parallelize(candidates)\\\n    .flatMap(lambda s: [(t.user.screen_name, t.text.encode('ascii', 'ignore')) for t in getTweets(s)])\\\n    .map(lambda s: (s[0], s[1]))\\\n    .reduceByKey(lambda s,t: s + '\\n' + t)\\\n    .filter(lambda s: len(re.findall(r'\\w+', s[1])) > 100 )\\\n    .map(lambda s: [s[0]] + getPersonalityInsight(s[1]))\ncandidatesPIDF = sqlContext.createDataFrame(\n   candidatesRDD, schema\n)\n", "intent": "For this project, the users are Trump and Bernie\n"}
{"snippet": "def select_item(x):\n    x = x.iloc[0]\n    if len(x) == 0:\n        return np.nan\n    else:\n        return x\nbusw = pd.pivot_table(bus, index=['business_id', 'name', 'review_count', 'city', 'stars', 'categories'],\n                      columns='variable', values='value', aggfunc=select_item)\nbusw.reset_index(inplace=True)\nbusw.head(3)\n", "intent": "Obviously, multiple rows refer to the same business. The data is in 'long' format to identify the variables.\nConverting to 'wide' format:\n"}
{"snippet": "busw.fillna(0, inplace=True)\nbusw.replace('None', 0, inplace=True)\nbusw.replace(True, 1, inplace=True)\nbusw.replace(False, 0, inplace=True)\nbusw.info()\n", "intent": "True, None, and False values need to be broken up to binary values\n"}
{"snippet": "use.fillna(0, inplace=True)\nuse.drop('Unnamed: 0', axis=1, inplace=True)\nuse.head(3)\n", "intent": "Empty values and elite identification need to be converted to binary values.\n"}
{"snippet": "df = pd.read_csv('data/WA_Fn-UseC_-HR-Employee-Attrition.csv')\n", "intent": "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset\n"}
{"snippet": "scaler = sklearn.preprocessing.MinMaxScaler()\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "sessions.loc[sessions.action == 'lookup', 'action_type'] = 'lookup'\nsessions.loc[sessions.action == 'lookup', 'action_detail'] = 'lookup'\nsessions.loc[sessions.action == 'track_page_view', 'action_type'] = 'track_page_view'\nsessions.loc[sessions.action == 'track_page_view', 'action_detail'] = 'track_page_view'\nsessions.action_type = sessions.action_type.fillna('missing')\nsessions.action_detail = sessions.action_detail.fillna('missing')\n", "intent": "The rest are easy, and lastly, we fill \"missing\" to the ones we can't find them a home.\n"}
{"snippet": "umportant_features = pd.DataFrame(final_clf.feature_importances_).T\numportant_features.columns = list(train.columns)\numportant_features = umportant_features.T\numportant_features.columns = ['feature_importance']\numportant_features.sort_values('feature_importance', ascending=False)\n", "intent": "79% Accuracy on test set\n"}
{"snippet": "df['avg_rating_by_driver'].fillna(df['avg_rating_by_driver'].mean(), inplace=True)\ndf['avg_rating_of_driver'].fillna(df['avg_rating_of_driver'].mean(), inplace=True)\ndf['phone'].fillna('iPhone', inplace=True)\ndf.info()\n", "intent": "There are few missing values. We can impute these columns with mean of the column\n"}
{"snippet": "ex_user = pd.DataFrame([0.77, 5., 4.3, 1., 0., 3., 100.,\n          1., 0., 0., 0., 1., 1., 0.])\nex_user = ex_user.T\nex_user.columns = df_encoded.columns\nex_user\n", "intent": "Here we test our neural network predicting feature with an example user.\n"}
{"snippet": "embed_tsne = tsne.fit_transform(np.random.random((63000,200)))\n", "intent": "embed_tsne = tsne.fit_transform(random.random(63000,200))\n"}
{"snippet": "g=d[d['type'] == 'EMS' ]\np=pd.pivot_table(g, values='e', index=['timeStamp'], columns=['title'], aggfunc=np.sum)\npp=p.resample('W', how=[np.sum]).reset_index()\npp.head()\n", "intent": "Pivot Table\n-----------\n"}
{"snippet": "teams_df = pd.DataFrame(Teams)\nprint(teams_df.head())\n", "intent": "Using pandas, convert the results to a DataFrame and print the first 5 rows using the head() method to ensure it looks right.\n"}
{"snippet": "house = pd.read_csv(\"C:/ProgramData/Anaconda3/SeattleHousing/kc_house_data.csv\")\nhouse.head()\n", "intent": "Let's first read in the house data as a dataframe \"house\" and inspect the first 5 rows\n"}
{"snippet": "X = scaled_features \ny = df['TARGET CLASS']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state =101)\n", "intent": "do \"shift-tab\" so that you can scroll-down and find example\n"}
{"snippet": "X = yelp_class['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "sessions[\"secs_elapsed\"] = sessions.groupby(\"action\").transform(lambda x: x.fillna(x.median()))\n", "intent": "Fill the missing secs_elapsed with the median secs_elapsed for each action.\n"}
{"snippet": "scaled_df_bank = pd.DataFrame(data=scaled_bank,columns=bank.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "soccer_data = pd.read_csv('CrowdstormingDataJuly1st.csv', sep=',', parse_dates=['birthday'])\nsoccer_data.head()\n", "intent": "In this part, we review obtained data and clean them before doing any manipulation with (un)supervised classifiers.\n"}
{"snippet": "soccer_data_clean[['height', 'weight']] = soccer_data_clean[['height', 'weight']].fillna(soccer_data_clean[['height', 'weight']].mean())\n", "intent": "For height and weight, we decide to use mean of values to replace null values.\n"}
{"snippet": "y = preprocessing.LabelEncoder().fit_transform(soccer_data_all_features[classes_column])\n", "intent": "Let's encode the classes again, for further comparisons.\n"}
{"snippet": "df_aliases = pd.read_csv('hillary-clinton-emails/Aliases.csv', index_col=0)\n", "intent": "First, we import all data as DataFrames.\n"}
{"snippet": "vect = CountVectorizer()\nvect.fit(X_train)\nX_train_dtm = vect.transform(X_train)\nX_test_dtm = vect.transform(X_test)\nX_test_dtm\n", "intent": "Use CountVectorizer to create **document-term matrices** from X_train and X_test.\n"}
{"snippet": "scaler = StandardScaler()\n", "intent": "Scale the data so high values won't impact on the distance between the data points.\n"}
{"snippet": "X = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\ny = iris['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "** Split your data into a training set and a testing set.**\n"}
{"snippet": "from sklearn import datasets, svm, metrics\ndigits = datasets.load_digits()\nprint(digits.data.shape)\ndigits.data \n", "intent": "The [digits dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\n"}
{"snippet": "df.first_affiliate_tracked = df.first_affiliate_tracked.fillna(\"untracked\")\n", "intent": "Set the missing values for \"first_affiliate_tracked\" to \"untracked\".\n"}
{"snippet": "train, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000, valid_portion=0.1)\ntrainX, trainY = train\ntestX, testY = test\n", "intent": "Siraj's [code](https://github.com/llSourcell/How_to_do_Sentiment_Analysis)\n"}
{"snippet": "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n", "intent": "Firstly, we have to create two data sets: training set and test set using built-in method from scilearn.\n"}
{"snippet": "df = pd.DataFrame(tp250)\n", "intent": "Keep the fields:\n    num_votes\n    rating\n    tconst\n    title\n    year\nAnd discard the rest\n"}
{"snippet": "M = len(df[male_mask].Sex.values)\nF = len(df[female_mask].Sex.values)\ncolumn_gender = [\"Male\",\"Female\"]\nrows_gender = [M,F]\nindex_gender = [0]\nMale_Female_Hist = dict(zip(column_gender,rows_gender))\nMale_Female_Hist = pd.DataFrame(Male_Female_Hist, index = index_gender)\nMale_Female_Hist.head()\nMale_Female_Hist_plot = sns.barplot(data = Male_Female_Hist )\nsns.axlabel(\"Gender\", \"Count\")\n", "intent": "** Not representative of total travelers in the dataset**\n"}
{"snippet": "name = ('id number','thickness','uniformity size','uniformity shape','adhesion','cell size','nuclei','chromatin','nucleoli','mitoses','class')\ndf = pd.read_csv('../../assets/datasets/breast-cancer-wisconsin.csv', names = name, na_values='?')\ndf.head()\ndf.dropna(inplace = True)\ndf.head()\n", "intent": "The column names are taken from the dataset info file. Create an array\nwith the column names and assign them as the header when loading the\ncsv.\n"}
{"snippet": "conmat = np.array(confusion_matrix(Ytest, Ypred, labels=[1,0]))\nconfusion = pd.DataFrame(conmat, index=['over_200k', 'under_200k'],\n                        columns=['predicted_over_200k','predicted_under_200k'])\nprint(confusion)\n", "intent": "Print out the confusion matrix\n"}
{"snippet": "df = pd.read_csv('../../assets/datasets/cars.csv')\ndf.head()\n", "intent": "Visualize the last tree. Can you make sense of it? What does this teach you about decision tree interpretability?\n"}
{"snippet": "wine = pd.read_csv('../../assets/datasets/wine_v.csv')\n", "intent": "First, let's import our data. We'll be using *Iris* again for this exercise. \n"}
{"snippet": "for c in pure_geo_columns:\n    dat.loc[:, c] = dat[c].cat.add_categories('N/A').fillna('N/A')\n", "intent": "---------------------------------------\n"}
{"snippet": "min_max_scaler = preprocessing.MinMaxScaler()\nfor feature in cont_features:\n    df.loc[:,feature] = min_max_scaler.fit_transform(df[[feature]])\n", "intent": "Preprocessing continuous variables\n"}
{"snippet": "jlt = Cats.Catalog()\nlogm, logsfr, w = jlt.Read('tinkergroup')\n", "intent": "First lets import a catalog\n"}
{"snippet": "nsa = Cats.Catalog()\nlogm, logsfr, w = nsa.Read('nsa_dickey')\n", "intent": "This seems promising for high masses. Lets try the NSA\n"}
{"snippet": "df_married = pd.DataFrame()\ndf_married['not_treated'] = [0, 0]\ndf_married['treated'] = [0, 0]\ndf_married.index = ['not_married', 'married']\ndf_married.loc['not_married', 'not_treated'] = len(df_not_treated[df_not_treated['married'] == 0])\ndf_married.loc['married', 'not_treated'] = len(df_not_treated[df_not_treated['married'] == 1])\ndf_married.loc['not_married', 'treated'] = len(df_treated[df_treated['married'] == 0])\ndf_married.loc['married', 'treated'] = len(df_treated[df_treated['married'] == 1])\ndf_married\n", "intent": "In order to present this data, we need to calculate number of married and not married subjet's for both groups.\n"}
{"snippet": "df_no_degree = pd.DataFrame()\ndf_no_degree['not_treated'] = [0, 0]\ndf_no_degree['treated'] = [0, 0]\ndf_no_degree.index = ['no_degree', 'have_degree']\ndf_no_degree.loc['no_degree', 'not_treated'] = len(df_not_treated[df_not_treated['nodegree'] == 0])\ndf_no_degree.loc['have_degree', 'not_treated'] = len(df_not_treated[df_not_treated['nodegree'] == 1])\ndf_no_degree.loc['no_degree', 'treated'] = len(df_treated[df_treated['nodegree'] == 0])\ndf_no_degree.loc['have_degree', 'treated'] = len(df_treated[df_treated['nodegree'] == 1])\ndf_no_degree\n", "intent": "In order to present this data, we need to calculate number of subjet's with and without degree for both groups.\n"}
{"snippet": "conf_matrix = confusion_matrix(y, y_predictions)\ndf_conf_matrix = pd.DataFrame(conf_matrix)\ndf_conf_matrix.index = ['true_not_treated','true_treated']\ndf_conf_matrix.columns = ['predicted_not_treated', 'predicted_treated']\nsn.set(font_scale=1.3)\nsn.heatmap(df_conf_matrix, annot=True, cmap=\"Greens\", fmt='g')\n", "intent": "To see how good our model is we have decided to present it's confusion matrix.\n"}
{"snippet": "df_married = pd.DataFrame()\ndf_married['not_treated'] = [0, 0]\ndf_married['treated'] = [0, 0]\ndf_married.index = ['not_married', 'married']\ndf_married.loc['not_married', 'not_treated'] = len(df_match[df_match['married_not_treated'] == 0])\ndf_married.loc['married', 'not_treated'] = len(df_match[df_match['married_not_treated'] == 1])\ndf_married.loc['not_married', 'treated'] = len(df_match[df_match['married_treated'] == 0])\ndf_married.loc['married', 'treated'] = len(df_match[df_match['married_treated'] == 1])\ndf_married\n", "intent": "Same as in Task 2 we calculate the number of married and not married subjet's for both groups.\n"}
{"snippet": "df_no_degree = pd.DataFrame()\ndf_no_degree['not_treated'] = [0, 0]\ndf_no_degree['treated'] = [0, 0]\ndf_no_degree.index = ['no_degree', 'have_degree']\ndf_no_degree.loc['no_degree', 'not_treated'] = len(df_match[df_match['nodegree_not_treated'] == 0])\ndf_no_degree.loc['have_degree', 'not_treated'] = len(df_match[df_match['nodegree_not_treated'] == 1])\ndf_no_degree.loc['no_degree', 'treated'] = len(df_match[df_match['nodegree_treated'] == 0])\ndf_no_degree.loc['have_degree', 'treated'] = len(df_match[df_match['nodegree_treated'] == 1])\ndf_no_degree\n", "intent": "And finally, we need to calculate number of subjet's with and without degree for both groups.\n"}
{"snippet": "df = pd.DataFrame(importances[np.where(importances>0)])\nplot = df.plot.hist(figsize=(10, 5), bins=50)\nplot.set(xlabel=\"Feature Importance\", ylabel=\"Count\")\nplot\n", "intent": "We can even verify this asusmption by using a histogram to present the values of importance of features who have a non-zero importance.\n"}
{"snippet": "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', header=0, sep=';')\n", "intent": "Data: https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n"}
{"snippet": "us_canada_user_rating_pivot2 = us_canada_user_rating.pivot(index = 'userID', columns = 'bookTitle', values = 'bookRating').fillna(0)\n", "intent": "Perfect! \"Green Mile Series\" books are definitely should be recommended one after another.   \n"}
{"snippet": "noise = [0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.25,1.5,1.75,2.0]\nn_clusters = []\nfor i in noise:\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=200, centers=centers, cluster_std=i,random_state=101)\n    af_model=AffinityPropagation(preference=-50,max_iter=500,convergence_iter=15,damping=0.5).fit(X)\n    n_clusters.append(len(af_model.cluster_centers_indices_))  \n", "intent": "Create data sets with varying degree of noise std. dev and run the model to detect clusters. Also, play with damping parameter to see the effect.\n"}
{"snippet": "df = pd.read_csv('Datasets/loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv**\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n", "intent": "**Instantiate a scaler standardizing estimator**\n"}
{"snippet": "noise = [0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.25,1.5,1.75,2.0]\nn_clusters = []\nfor i in noise:\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=200, centers=centers, cluster_std=i,random_state=101)\n    ms_model=MeanShift().fit(X)\n    n_clusters.append(len(ms_model.cluster_centers_))\n", "intent": "Create data sets with varying degree of noise std. dev and run the model to detect clusters.\n"}
{"snippet": "df['Cancer'] = pd.DataFrame(cancer['target'])\ndf.head()\n", "intent": "** Adding the target data to the DataFrame**\n"}
{"snippet": "import pandas as pd\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names).assign(species=iris.target_names[iris.target])\n", "intent": "We probably want to convert the data into a more convenient structure, namely, a `DataFrame`.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(iris.data)\nX_std[:5]\n", "intent": "Let's apply a standardization transformation from scikit-learn:\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components=3, whiten=True).fit(iris.data)\nX_pca = pca.transform(iris.data)\n", "intent": "`scikit-learn` provides a PCA transformation in its `decomposition` module. \n"}
{"snippet": "wine = pd.read_table('../data/wine.dat', sep='\\s+')\nwine.head()\n", "intent": "Import the wine dataset and perform PCA on the predictor variables, and decide how many principal components would you select.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\ny = indicator.Ease_Bus\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n", "intent": "Split the data into training and test\n"}
{"snippet": "X, y = datasets.make_moons(50, noise=0.20)\nX = X.astype(np.float32)\ny = y.astype(np.int32)\n", "intent": "Since we used the scikit-learn interface, its easy to take advantage of the `metrics` module to evaluate the MLP's performance.\n"}
{"snippet": "from sklearn import linear_model\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\nY = iris.target\nprint( iris.DESCR)\n", "intent": "Load the data set from Scikit Learn\n"}
{"snippet": "coeff_poly = pd.DataFrame(model_poly.coef_,index=df_poly.drop('y',axis=1).columns, \n                          columns=['Coefficients polynomial model'])\ncoeff_poly\n", "intent": "** Recall that the originating  function  is: ** \n$ y= 5x_1^2+13x_2+0.1x_1x_3^2+2x_4x_5+0.1x_5^3+0.8x_1x_4x_5+noise $\n"}
{"snippet": "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n", "intent": "- As with the Iris data: split the data into training and testing sets, then fit a Gaussian naive Bayes model against the test data.\n"}
{"snippet": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n", "intent": "- Let's demonstrate the naive approach to validation using the Iris data.\n"}
{"snippet": "import pandas as pd\npd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n", "intent": "- It is easier to inspect if we convert this to a ``DataFrame`` with labeled columns:\n"}
{"snippet": "categories = ['talk.religion.misc', 'soc.religion.christian',\n              'sci.space', 'comp.graphics']\ntrain = fetch_20newsgroups(subset='train', categories=categories)\ntest = fetch_20newsgroups(subset='test', categories=categories)\n", "intent": "- For simplicity we select a few of these categories and download the training and testing set:\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nx = np.array([2, 3, 4])\npoly = PolynomialFeatures(3, include_bias=False)\npoly.fit_transform(x[:, None])\n", "intent": "- Polynomial projection is built into Scikit-Learn using the ``PolynomialFeatures`` transformer:\n"}
{"snippet": "from pandas.tseries.holiday import USFederalHolidayCalendar\ncal = USFederalHolidayCalendar()\nholidays = cal.holidays('2012', '2016')\ndaily = daily.join(pd.Series(1, index=holidays, name='holiday'))\ndaily['holiday'].fillna(0, inplace=True)\n", "intent": "- We expect riders to behave differently on holidays. let's add an indicator of this as well:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=324)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nSplit the Dataset into Training and Test Datasets\n<br><br></p>\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=42)\n", "intent": "- Split the data into training and testing sets\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.keys()\n", "intent": "- Let's reuse the hand-written digits dataset here to see how the random forest classifier can be used.\n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.data.shape\n", "intent": "- The benefit of dimensionality reduction becomes much more apparent when looking at high-dimensional data. Let's apply PCA to the digits dataset.\n"}
{"snippet": "pca = PCA(2)  \nprojected = pca.fit_transform(digits.data)\nprint(digits.data.shape)\nprint(projected.shape)\n", "intent": "- Use PCA to project the 8x8 (64-dimensional) images to a more manageable number.\n"}
{"snippet": "pca = PCA(0.50).fit(noisy)\npca.n_components_\n", "intent": "- It's clear that the images are noisy. Let's train a PCA on the noisy data, requesting that the projection preserve 50% of the variance.\n"}
{"snippet": "from sklearn.datasets.samples_generator import make_blobs\nX, y_true = make_blobs(n_samples=400, centers=4,\n                       cluster_std=0.60, random_state=0)\nX = X[:, ::-1] \n", "intent": "- We already known that given simple, well-separated data, *k*-means finds suitable clustering results.\n"}
{"snippet": "df_scaled = pd.DataFrame(scaled,columns=df.columns[:-1])\ndf_scaled.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import tree\ninput_file = \"C:/Downloads/Data Science and Machine Learning with Python/DataScience/DataScience/PastHires.csv\"\ndf = pd.read_csv(input_file, header = 0)\n", "intent": "First we'll load some fake data on past hires I made up. Note how we use pandas to convert a csv file into a DataFrame:\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\n_, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask = np.zeros(critics.shape[0], dtype=np.bool)\nmask[itest] = True\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "df = pd.read_csv('train.csv.gz', sep=',').dropna()\ndest = pd.read_csv('destinations.csv.gz')\ndf = df.sample(frac=0.01, random_state=99)\ndf.shape\n", "intent": "To be able to process locally, we randomly sample 1% of the records. After that, we still have a large number of records at 241,179.\n"}
{"snippet": "columns = data['alchemy_category'].unique()\nnew_df = pd.DataFrame(columns=columns)\n", "intent": "> \nPlot the rate of evergreen sites for all Alchemy categories.\n"}
{"snippet": "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\nX_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\nprint('Train dimension:');print(X_train.shape)\nprint('Test dimension:');print(X_test.shape)\nlb = LabelBinarizer()\ny_train = lb.fit_transform(y_train)\ny_test = lb.transform(y_test)\nprint('Train labels dimension:');print(y_train.shape)\nprint('Test labels dimension:');print(y_test.shape)\n", "intent": "As we can see our current data have dimension N * 28*28, we will start by flattening the image in N*784, and one-hot encode our target variable.\n"}
{"snippet": "df=pd.read_csv(\"../../data/raw/yelp.csv\")\n", "intent": "**Read the yelp.csv file and set it as a dataframe called yelp.**\n"}
{"snippet": "ss = StandardScaler() \n", "intent": "** Crie um objeto StandardScaler() chamado scaler. **\n"}
{"snippet": "ad_data = pd.read_csv('advertising.csv')\nad_data.head()\n", "intent": "** Leia o arquivo advertising.csv e grave-o em um DataFrame chamado ad_data. **\n"}
{"snippet": "loans = pd.read_csv('loan_data.csv')\nloans.head()\n", "intent": "** Use pandas para ler loan_data.csv como um DataFrame chamado loans. **\n"}
{"snippet": "loans = pd.read_csv('')\n", "intent": "** Use pandas para ler loan_data.csv como um DataFrame chamado loans. **\n"}
{"snippet": "    from sklearn.datasets import load_boston\n    boston = load_boston()\n    print(boston.DESCR)\n    boston_df = boston.data\n", "intent": "    from sklearn.datasets import load_boston\n    boston = load_boston()\n    print(boston.DESCR)\n    boston_df = boston.data\n"}
{"snippet": "data = pd.read_csv('bank.csv', header=0)\ndata = data.dropna()\nprint(data.shape)\nprint(list(data.columns))\n", "intent": "This dataset provides the customer information. It includes 41188 records and 21 fields.\n"}
{"snippet": "SP500_price = pd.read_csv(\"SP500_price.csv\", sep = \",\")\n", "intent": "Second Dataset: the S&P500 Index Price download\n"}
{"snippet": "aapl_fb_goog_prices = pd.read_csv(\"aapl_fb_goog_prices.csv\", sep = \",\")\n", "intent": "Third Dataset: Stock prices of Facebook, Apple and Google\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import preprocessing\nlabel_encoder=preprocessing.LabelEncoder()\n", "intent": "- Prepare your dataset for modeling\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(features_df, target_df, test_size=0.33, random_state=5) \n", "intent": "Split into 66% training set and 33% testing set\n"}
{"snippet": "df_feat = pd.DataFrame(scaler_features, columns=df.columns[:-1])\ndf_feat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "import numpy as np\nfrom sklearn import preprocessing\nlabels = np.array([1,5,3,2,1,4,2,1,3])\nlb = preprocessing.LabelBinarizer()\nlb.fit(labels)\nlb.transform(labels)\n", "intent": "Transforming your labels into one-hot encoded vectors is pretty simple with scikit-learn using `LabelBinarizer`. Check it out below!\n"}
{"snippet": "std_scale = preprocessing.StandardScaler().fit(drop[['SQFT', 'BDRMS', 'AGE']])\ndf_std = std_scale.transform(drop[['SQFT', 'BDRMS', 'AGE']])\n", "intent": "[StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\n", "intent": "Use an SVM, GridSearchCV and the iris dataset to find an optimal model.\nMake sure to search for optimal paramaters for kernel, C, \n"}
{"snippet": "df = pd.read_csv('train.tsv', sep = '\\t')\n", "intent": "Split the dataset in to train and test. We are using training data only for EDA.\n"}
{"snippet": "amazon = np.asarray(pd.read_csv(\"amazon.csv\", encoding = \"ISO-8859-1\"))\nrotten_tomatoes = np.asarray(pd.read_csv(\"rotten_tomatoes.csv\", encoding = \"ISO-8859-1\"))\ntrain = np.asarray(pd.read_csv(\"train.csv\", encoding = \"ISO-8859-1\"))\ntest = np.asarray(pd.read_csv(\"test.csv\", encoding = \"ISO-8859-1\"))\nholdout = np.asarray(pd.read_csv(\"holdout.csv\", encoding = \"ISO-8859-1\"))\n", "intent": "Let's start by reading the given files.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_s = scaler.fit_transform(X)\n", "intent": "- Plot the standardized mean cross-validated accuracy against the unstandardized. Which is better?\n- Why?\n"}
{"snippet": "table = pd.DataFrame({'probability':[0.0, 0.1, 0.2, 0.25, 0.5, 0.6, 0.8, 0.9,1.0]})\ntable['odds'] = table.probability/(1 - table.probability)\ntable\n", "intent": "**As an example we can create a table of probabilities vs. odds, as seen below.**\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nlabels = ['CRIM','RM','B','LSTAT']\nX_train, X_test, y_train, y_test = train_test_split(X[labels], y, train_size=0.7, random_state=8)\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nlr.score(X_test, y_test)\n", "intent": "- Score and plot.  \n- How do your metrics change?\n"}
{"snippet": "from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33) \n", "intent": "Split your data in to train and test sets.\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('H:/Technology/python/Dataset/pydata-book-master/ch06/ex1.csv')\nprint( df )\nprint( df.index )\nprint( df.columns )\nprint( df.values )\ndf\n", "intent": "<a id=\"csv\"></a>\n<hr>\n"}
{"snippet": "data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\n        'year': [2000, 2001, 2002, 2001, 2002],\n        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\nframe = pd.DataFrame(data)\nprint(\"TO STANDARD OUTPUT  with SEP\")\nframe.to_csv(sys.stdout, sep='|')\nprint(\"TO STANDARD OUTPUT  without HEADER \")\nframe.to_csv(sys.stdout, index=False, header=False)\n", "intent": "<a id=\"standardoutput\"></a>\n<hr>\n"}
{"snippet": "data = load_boston()\nX_ = data['data']\ny_ = data['target']\n", "intent": "__TODO__:\n  1. apply fixes for the data, to ensure statistical validity of the test and to prevent \"multiplicity\" issues\n"}
{"snippet": "def merge_columns(main, other):\n    result = pd.merge(left=main,right=other, how='outer', left_on='date', right_on='date')\n    return result\ntrends = pd.read_csv('bitcoin_trends.csv')\ndata = merge_columns(data, trends)\n", "intent": "Read in the Google Trends result and merge with the main data set.\n"}
{"snippet": "cv = CountVectorizer(min_df=NAME_MIN_DF)\nX_name = cv.fit_transform(merge['name'])\n", "intent": "Count vectorize name and category name columns.\n"}
{"snippet": "X = df_delay.loc[:, df_delay.columns != 'delay_log']\ny = df_delay[\"delay_log\"]\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.25, random_state=42)\n", "intent": "- Split the dataset into training and test, where test size is 0.25:\n"}
{"snippet": "lambdas = [.001,.005,1,5,10,50,100,500,1000]\nclfl = LassoCV(cv=10, alphas=lambdas, fit_intercept=True, normalize=False)\npreprocessing.StandardScaler()\nclfl.fit(X_train2, y_train2)\nprint('Lasso Train Score', clfl.score(X_train2, y_train2))\nprint('Lasso Test Score', clfl.score(X_test2, y_test2))\n", "intent": "- Use LASSO cross validation (10-fold) to select predictors:\n"}
{"snippet": "betas = np.transpose(logitCV.coef_[:,logitCV.coef_[0,:]>0])\nfeatures = X_train.columns[logitCV.coef_[0,:]>0]\nsignif_betas=pd.DataFrame([features, betas])\nsignif_betas\n", "intent": "5..(5pts) Given your model, comment on the importance of factors as related to whether a flight is delayed.\n"}
{"snippet": "df_sales_sample = pd.read_csv('df_sales_sample.csv')\n", "intent": "**Modeling & Evaluation**\n"}
{"snippet": "df_sales = pd.read_csv('df_sales.csv')\ndf_crime_summary_zip = pd.read_csv(\"crime_summary_zip.csv\")\ndf_crime_summary_zip\n", "intent": "**Modeling with Full sales data and crime counts for property zip-code for Year_Sold**\n"}
{"snippet": "reviews = []\nsentiments = []\nwith open(os.path.join('..', 'datasets', 'amazon-reviews.txt')) as f:\n    for line in f.readlines():\n        line = line.strip('\\n')\n        review, sentiment = line.split('\\t')\n        sentiment = np.nan if sentiment == '' else int(sentiment)\n        reviews.append(review)\n        sentiments.append(sentiment)\ndf = pd.DataFrame({'review': reviews, 'sentiment': sentiments})\n", "intent": "The data is about sentiments on Amazon reviews.\n"}
{"snippet": "reviews = []\nsentiments = []\nwith open(os.path.join('..', 'datasets', 'amazon-reviews.txt')) as f:\n    for line in f.readlines():\n        line = line.strip('\\n')\n        review, sentiment = line.split('\\t')\n        sentiment = np.nan if sentiment == '' else int(sentiment)\n        reviews.append(review.lower())\n        sentiments.append(sentiment)\ndf = pd.DataFrame({'review': reviews, 'sentiment': sentiments})\n", "intent": "The data is about sentiments on Amazon reviews.\n"}
{"snippet": "if not is_labels_encod:\n    encoder = LabelBinarizer()\n    encoder.fit(train_labels)\n    train_labels = encoder.transform(train_labels)\n    test_labels = encoder.transform(test_labels)\n    train_labels = train_labels.astype(np.float32)\n    test_labels = test_labels.astype(np.float32)\n    is_labels_encod = True\nprint('Labels One-Hot Encoded')\n", "intent": "** One-Hot Encoding:**\n"}
{"snippet": "assert is_features_normal, 'You skipped the step to normalize the features'\nassert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\ntrain_features, valid_features, train_labels, valid_labels = train_test_split(\n    train_features,\n    train_labels,\n    test_size=0.05,\n    random_state=832289)\nprint('Training features and labels randomized and split.')\n", "intent": "** Split Training and Validation Sets:**\n"}
{"snippet": "tv = TfidfVectorizer(max_features=MAX_FEATURES_ITEM_DESCRIPTION, ngram_range=(1, 3), stop_words='english')\nX_description = tv.fit_transform(merge['item_description'])\n", "intent": "TFIDF Vectorize item_description column.\n"}
{"snippet": "sac = pd.read_csv('datasets/Sacramentorealestatetransactions.csv')\nsac\n", "intent": "Load the Sacramento housing data\n"}
{"snippet": "sacr = pd.read_csv('../../DSI-SYD-1/classes/week-02/3.2-lab/assets/datasets/Sacramentorealestatetransactions.csv')\n", "intent": "<h1>Multiple Linear Regression</h1>\n"}
{"snippet": "import pandas as pd\nwheel = pd.read_csv('datasets/wheel.csv')\n", "intent": "The data is in wheel.csv.\nWe would like to understand the relationship between _seconds_ and _signal_\n"}
{"snippet": "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()\ntfids = tfidf.fit_transform(articles)\ntfids_test = tfids[:len(test),:]\ntfids_train = tfids[len(test):,:]\ncountvect = sklearn.feature_extraction.text.CountVectorizer()\ntfids_test\n", "intent": "<h1> Vectorizers </h1>\n"}
{"snippet": "data=pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/Bonus/titanic3.csv\");\nprint(\"Here are the first three rows:\")\ndata.iloc[0:3,:]\n", "intent": "Consider the Titanic dataset below\n"}
{"snippet": "import numpy as np\nimport pandas as pd\nfrom sklearn import datasets\niris = datasets.load_iris()\n", "intent": "Import packages and read data.\n"}
{"snippet": "call_adjmatrix = pd.read_csv('./call.adjmatrix', index_col=0)\ncall_graph     = nx.from_numpy_matrix(call_adjmatrix.as_matrix())\n", "intent": "You are going to read the graph from an adjacency list saved in the earlier exercises.\n"}
{"snippet": "(pca.fit_transform(X_nrm)).round(2)\n", "intent": "Compare the 2 below: they are the embedding of users\n"}
{"snippet": "df = pd.DataFrame({'img_name': i2fn, 'age': i2age, 'gender': i2gender, 'race': i2race})\ndf.shape\n", "intent": "Combine the above maps into a pandas data frame.\n"}
{"snippet": "lb = LabelBinarizer(sparse_output=True)\nX_brand = lb.fit_transform(merge['brand_name'])\n", "intent": "Label binarize brand_name column.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=100)\n", "intent": "Divide the data into training and test sets with 80-20 split using scikit-learn. Print the shapes of training and test feature sets.\n"}
{"snippet": "GPU3 = pd.DataFrame([9,10,9,11,10,13,12,9,12,12,13,12,13,10,11])\n", "intent": "He is trying a third GPU - GPU3.\n"}
{"snippet": "import pandas as pandas\nSource = pandas.read_csv(\"bigcity.csv\")\n", "intent": "Read the dataset given in file named 'bigcity.csv'.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=100)\n", "intent": "Check: train_test_split function\n"}
{"snippet": "insurance = pd.read_csv('insurancedata.csv')\n", "intent": "We'll work with the Insurance Data csv file.\n** Read in the Insurance Data csv file as a DataFrame called insurance.**\n"}
{"snippet": "train = pd.read_csv('titanic_train.csv')\n", "intent": "Load the data reading in the titanic_train.csv file into a pandas dataframe.\n"}
{"snippet": "wine_df = pandas.read_csv(\"winequality-red.csv\")\n", "intent": "Let us assume the data frame is named wine_df\n"}
{"snippet": "Y = pandas.DataFrame(wine_df[\"quality\"])\nX = wine_df.iloc[:,0:11]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3, random_state=100)\n", "intent": "The above mixture suggests, overall accuracy may not make sense for this use case and class level accuracy will provide more insights\n"}
{"snippet": "ratio_train = pandas.DataFrame(Y_train[\"quality\"].value_counts())\nratio_train[\"Ratio\"] = (ratio_train[\"quality\"]/1119)*100\nratio_train\n", "intent": "Post train test split, ensure the ratio of quality levels in the data set is maintained\n"}
{"snippet": "from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(taxi, test_size=0.3, random_state=42)\n", "intent": "To be quick, let's create a baseline model, without Machine learning, just a simple rate calculation\n"}
{"snippet": "Train = pandas.read_csv(\"train.csv\")\nTest = pandas.read_csv(\"test.csv\")\n", "intent": "Use read_csv to read csv file. This is similar to read.csv in R\n"}
{"snippet": "train = pd.read_csv(\"train.csv\")\ntest    = pd.read_csv(\"test.csv\")\nfull = train.append( test , ignore_index = True )\ntitanic = full[ :891 ]\ndel train , test\nprint ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)\n", "intent": "Now that our packages are loaded, let's read in and take a peek at the data.\n*Select the cell below and run it by pressing the play button.*\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "c) Randomly split data into training and testing - 80% training, 20% testing.\n"}
{"snippet": "link = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/1200px-Correlation_examples2.svg.png\"\nio.imshow(io.imread(link))\nio.show()\n", "intent": "* Looking for Correlations\n"}
{"snippet": "enc = preprocessing.OneHotEncoder()\nenc.fit([[0, 3, 4], [1, 1, 0], [0, 2, 1], [1, 0, 2]])  \nenc.transform([[0, 1, 3]]).toarray()\n", "intent": "One Hot Encoding - Useful for inputing categorical data into SVM and linear models.\n"}
{"snippet": "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,5))\nX_train = vectorizer.fit_transform(twenty_train_subset.data)\n", "intent": "We can put this together with our other tricks as well...but notice the running time hit\n"}
{"snippet": "vectorizer = TfidfVectorizer(stop_words='english')\nX_train = vectorizer.fit_transform(twenty_train_subset.data)\n", "intent": "We can use predict using our 20-newsgroup dataset above\n"}
{"snippet": "mean = spambase.mean()\nstd = spambase.std()\nmean_std_df = pd.DataFrame({'Mean': mean, 'Std': std})\nmean_std_df\n", "intent": "**c)** Display the mean and standard deviation of each attribute.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'spambase_test.csv')\nspambase_test = pd.read_csv(data_path, delimiter = ',')\nspambase_test.head(10)\n", "intent": "**a)** Load `./datasets/spambase_test.csv` dataset into a new pandas structure\n"}
{"snippet": "df = pd.read_csv('train.csv.gz', sep=',').dropna()\ndf = df.sample(frac=0.01, random_state=99)\n", "intent": "To be able to process locally, we will use 1% of data. After that, we still have a large number of 241,179 records.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'train_images_partB.csv')\ntrain_B = pd.read_csv(data_path, delimiter = ',')\ndata_path = os.path.join(os.getcwd(), 'datasets', 'valid_images_partB.csv')\nvalid_B = pd.read_csv(data_path, delimiter = ',')\nattributes = train_B.columns[1:501].tolist()\n", "intent": "Let's load the data.\n"}
{"snippet": "data_path = os.path.join(os.getcwd(), 'datasets', 'train_20news')\nnews_A = pd.read_csv(data_path + '_partA.csv', delimiter = ',')\nnews_B = pd.read_csv(data_path + '_partB.csv', delimiter = ',')\n", "intent": "<span style=\"color:red\">OK\n"}
{"snippet": "def get_dataset(path):\n    dataset = pd.read_csv(path)\n    np.random.seed(42)\n    dataset = dataset.reindex(np.random.permutation(dataset.index))\n    return dataset\ndataset = get_dataset('data/dataset.csv')\ndataset.head()\n", "intent": "First, we want to grab the dataset from the CSV file. Load it as a Pandas Dataframe so we can easily work with it in further steps.\n"}
{"snippet": "two = ['2.txt']+[0]*len(vocab)\nwith open('2.txt') as f:\n    x = f.read().lower().split()\nfor word in x:\n    two[vocab[word]]+=1\n", "intent": "<font color=green>We can see that most of the words in 1.txt appear only once, although \"cats\" appears twice.</font>\n"}
{"snippet": "import numpy as np\nimport pandas as pd\ndf = pd.read_csv('../TextFiles/moviereviews2.tsv', sep='\\t')\ndf.head()\n", "intent": "For this exercise you can load the dataset from `'../TextFiles/moviereviews2.tsv'`.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = df['review']\ny = df['label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "You may use whatever settings you like. To compare your results to the solution notebook, use `test_size=0.33, random_state=42`\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\ntext_clf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC()),\n])\ntext_clf.fit(X_train, y_train)  \n", "intent": "You may use whatever model you like. To compare your results to the solution notebook, use `LinearSVC`.\n"}
{"snippet": "data_db = pd.io.parsers.read_csv(db,header=0)\ndata_db\n", "intent": "import into pandas dataframe and view it\n"}
{"snippet": "data_db = pd.io.parsers.read_csv(db,header=0)\ndata_db.head(5)\n", "intent": "import into pandas dataframe and view it\n"}
{"snippet": "scaler = StandardScaler()\nX=scaler.fit_transform(X)\nX\n", "intent": "Standardize the dataset\n"}
{"snippet": "url = '../assets/dataset/bikeshare.csv'\nbikes = pd.read_csv(url, index_col='datetime', parse_dates=True)\n", "intent": "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n"}
{"snippet": "bank_md1 = pd.get_dummies(bank_a[['age','job','education','day_of_week','y']], drop_first = True)\nLogReg1 = LogisticRegression()\nX1 = bank_md1.drop('y', axis =1)\ny1 = bank_md1['y']\nx_train1, x_test1, y_train1, y_test1 = train_test_split(X1,y1, random_state =42)\nLogReg1.fit(x_train1, y_train1)\n", "intent": "**Build a Model**  \n*Model 1, using `age`, `job`, `education`, and `day_of_week`*\n"}
{"snippet": "columns = \"age sex bmi map tc ldl hdl tch ltg glu\".split()\ndiabetes = datasets.load_diabetes()\ndf = pd.DataFrame(diabetes.data, columns=columns)\ny = diabetes.target  \ndf.head()\n", "intent": "* Luego, cargamos el dataset y definimos los nombres de colmnas\n"}
{"snippet": "df_cor = pd.DataFrame.from_dict(cuentos_cor, orient=\"index\")\ndf_cor = pd.DataFrame.from_dict(cuentos_bor, orient=\"index\")\ndf_cor.to_csv(\"df_cor.csv\")\ndf_cor.to_csv(\"df_bor.csv\")\n", "intent": "***6. Guardalos en .csv***\n"}
{"snippet": "df_cortazar.to_csv('cortazar.csv')\n", "intent": "***6. Guardalos en .csv***\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=42)\n", "intent": "Haremos el split en training y test, para poder evaluar el clasificador.\n"}
{"snippet": "from sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=150, whiten=True, svd_solver='randomized', random_state=42)\n", "intent": "Sobre los datos de entrenamiento podemos entrenamos el modelo PCA para reducir las dimensiones.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "intent": "Hacemos el split entre train y test:\n"}
{"snippet": "sac = pd.read_csv('Sacramentorealestatetransactions.csv')\n", "intent": "Carguemos el dataset de Sacramento:\n"}
{"snippet": "pca = PCA(n_components=23)\npca.fit(X)\n", "intent": "Apply PCA. And we have 23 features in our data.\n"}
{"snippet": "sac = pd.read_csv('../Data/Sacramentorealestatetransactions.csv')\n", "intent": "Carguemos el dataset de Sacramento:\n"}
{"snippet": "df = pd.read_csv('breast-cancer.csv', header = None)\ndf.columns = ['ID', 'clump_Thickness', 'unif_cell_size', 'unif_cell_shape', 'adhesion', 'epith_cell_Size', 'bare_nuclei',\n              'bland_chromatin ','norm_nucleoli', 'mitoses', 'class_t']\n", "intent": "Importamos el dataset\n"}
{"snippet": "X = (df.iloc[:,1:9])\ny = df['class_t']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n", "intent": "Hacemos el split entre target y features\n"}
{"snippet": "df = pd.read_csv('../Data/breast-cancer.csv', header = None)\ndf.columns = ['ID', 'clump_Thickness', 'unif_cell_size', 'unif_cell_shape', 'adhesion', 'epith_cell_Size', 'bare_nuclei',\n              'bland_chromatin ','norm_nucleoli', 'mitoses', 'class_t']\n", "intent": "Importamos el dataset\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nimport pandas as pd\nimport json\ndata = pd.read_csv(\"../Data/stumbleupon.tsv\", sep='\\t')\ndata['boilerplate'].head()\n", "intent": "* Primero importaremos los datos, paquetes, etc.\n"}
{"snippet": "import pandas as pd\ndf = pd.DataFrame(c.execute(\"SELECT * from adult\").fetchall(), columns = adult_cols)\n", "intent": "** Transformar los datos en un dataframe de pandas**\n"}
{"snippet": "import pandas as pd\ndf = pd.read_csv('car.csv')\ndf.head()\n", "intent": "El primer paso es leer los datos en Pandas.\n"}
{"snippet": "pca = PCA(n_components=1)\npca.fit(X)\nX_pca = pca.transform(X)\nprint(\"original shape:   \", X.shape)\nprint(\"transformed shape:\", X_pca.shape)\n", "intent": "Ahora utilizamos PCA para reducir la cantidad de dimensiones de 2 a 1\n"}
{"snippet": "pca = PCA(0.50).fit(noisy)\npca.n_components_\n", "intent": "Vamos a correr PCA para retener con la cantidad de componentes principales necesaria para explicar el 50% de la varianza.\n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n", "intent": "Standardization of the data\n"}
{"snippet": "hide_code\nboston_data = datasets.load_boston()\nboston_df = pd.DataFrame(boston_data.data, columns=boston_data.feature_names)\nboston_df['MEDV'] = boston_data.target\n(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\nx_valid, y_valid = x_test[:51], y_test[:51]\nx_test, y_test = x_test[51:], y_test[51:]\n", "intent": "This database is very popular for studying regression and can be downloaded in several ways. Let's display the easiest ones of them.\n"}
{"snippet": "hide_code\nx_train, x_test, y_train, y_test = train_test_split(images, cat_brands, \n                                                    test_size = 0.2, \n                                                    random_state = 1)\nn = int(len(x_test)/2)\nx_valid, y_valid = x_test[:n], y_test[:n]\nx_test, y_test = x_test[n:], y_test[n:]\n", "intent": "Apply the function train_test_split and split the data into training and testing sets. Set up the size of the testing set - 20%.\n"}
{"snippet": "retail = pd.read_csv('data/retail_data.csv', index_col='CustomerID')\nheader = retail.columns.values\nretail_array = np.array(retail)\nX = retail_array[:,:-1].astype(float)\ny = retail_array[:,-1]\ny = preprocessing.LabelEncoder().fit_transform(y)\nXTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state=1)\n", "intent": "We will use the same previous dataset customer retails to evaluate AdaBoost\n"}
{"snippet": "retail = pd.read_csv('data/retail_data.csv', index_col='CustomerID')\nheader = retail.columns.values\nretail_array = np.array(retail)\nX = retail_array[:,:-1].astype(float)\ny = retail_array[:,-1]\nXTrain, XTest, yTrain, yTest = train_test_split(X, preprocessing.LabelEncoder().fit_transform(Y), random_state=1)\n", "intent": "We will use the `Pipeline` module to build the complete classification system for the customer retail we encountered in the previous sessions\n"}
{"snippet": "def get_matrix():\n    df = pd.merge(df_offers, df_transactions, how='left')\n    response_df = df.pivot_table(index=['customer_name'], columns=['offer_id'], values='n')\n    response_df.fillna(0, inplace=True)\n    matrix_df = response_df.reset_index()\n    del matrix_df['customer_name']\n    return matrix_df\n", "intent": "> - **Exercise**: Construct a plot showing $SS$ for each $K$  and pick $K$ using this plot. For simplicity, test $2 \\le K \\le 10$.\n"}
{"snippet": "with open('data/die_leiden_des_jungen_werthers_kapitel_1.txt','r') as f:\n    read_data=f.read()\nf.close\n", "intent": "__Given:__ Chapter 1 of 'die Leiden des jungen Werthers' (data/die_leiden_des_jungen_werthers_kapitel_1.txt)\n"}
{"snippet": "recons = np.dot(proj[:,:50],W[:,:50].T) + df[feat_cols].values.mean(axis=0)\ndf_recons = pd.DataFrame(recons,columns=feat_cols)\nplotMNIST(df_recons,  maxN=20, print_digits=False)\n", "intent": "<h2>Take a look at the reconstruction</h2>\n"}
{"snippet": "pca = repres.PCA(n_components=784)\nres = pca.fit_transform(df[feat_cols].values)\ndf['pca-one'] = res[:,0]\ndf['pca-two'] = res[:,1] \ndf['pca-three'] = res[:,2]\nsns.pairplot(df, hue=\"label\", x_vars=[\"pca-one\",\"pca-two\"], y_vars=[\"pca-two\",\"pca-three\"], plot_kws={\"alpha\": 0.2},  size=5,)\n", "intent": "<h2>Now, let's use scikit.learn's PCA </h3>\n"}
{"snippet": "fa = repres.FactorAnalysis(n_components=3)\nres = fa.fit_transform(df[feat_cols].values)\ndf['fa-one'] = res[:,0]\ndf['fa-two'] = res[:,1] \ndf['fa-three'] = res[:,2]\nsns.pairplot(df, hue=\"label\", x_vars=[\"fa-one\",\"fa-two\"], y_vars=[\"fa-two\",\"fa-three\"], plot_kws={\"alpha\": 0.2},  size=4,)\n", "intent": "<h2>Now, let's use scikit.learn's Factor Analysis</h3>\n"}
{"snippet": "data = pd.read_csv('./weather/daily_weather.csv')\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nCreating a Pandas DataFrame from a CSV file<br></p>\n"}
{"snippet": "from sklearn.manifold import TSNE\ntsne = TSNE(n_components=3, perplexity=40, n_iter=300)\nres = tsne.fit_transform(proj[:,:50])\ndf['tsne-one'] = res[:,0]\ndf['tsne-two'] = res[:,1] \ndf['tsne-three'] = res[:,2] \n", "intent": "<h2>What next?</h3>\n"}
{"snippet": "from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX2D = pca.fit_transform(X)\n", "intent": "Apply PCA to reduce to 2D.\n"}
{"snippet": "X2D_inv = pca.inverse_transform(X2D)\n", "intent": "Recover the 3D points projected on the plane (PCA 2D subspace).\n"}
{"snippet": "ev = pd.read_csv('/users/patricksmith/desktop/evergreen.tsv', delimiter='\\t')\nev.head()\n", "intent": "First, let's load in the dataset. It's always important to visually examine the data, so let's go ahead and give it a look\n"}
{"snippet": "X_newr = []\nfor i in in_images_names:\n    X_newr.append (cv2.imread('./new_images_resized/'+ i)[:,:,:3] )\nX_new = np.asarray( X_newr )\n", "intent": "All new images are uploaded in a single mini data set variable with name *X_new*.\n"}
{"snippet": "df_probs = pd.DataFrame(data=probabilities) \ndf_probs.head()\n", "intent": "df_probs = pd.DataFrame(data=probabilities)\ndf_probs.head()\n"}
{"snippet": "pca2 = PCA(n_components=2)\nnorm_dist_pca2 = pca2.fit_transform(norm_dist)  \n", "intent": "- How much variance is explained using the first two components?\n- Does the explained variance number make sense?\n"}
{"snippet": "pca = PCA(n_components=2)\nnorm_dist_pca = pca.fit_transform(norm_dist)\n", "intent": "- How much variance is explained using the first two components?\n- Does the explained variance number make sense?\n"}
{"snippet": "df = pd.read_csv('data/blood_donations.csv', index_col = 'Unnamed: 0')\n", "intent": "https://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center\n"}
{"snippet": "data = pd.read_csv('./weather/minute_weather.csv')\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nCreating a Pandas DataFrame from a CSV file<br><br></p>\n"}
{"snippet": "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\nXtis = vectorizer.fit_transform(df['quote'])\nY = (df['fresh'] == 'fresh').values.astype(np.int8)\nxtraintis, xtesttis, ytraintis, ytesttis = train_test_split(Xtis, Y, random_state=1)\n", "intent": "Do both together help?\n"}
{"snippet": "vectorizer = CountVectorizer(max_features=5000)\nXtis = vectorizer.fit_transform(df['quote'])\nY = (df['fresh'] == 'fresh').values.astype(np.int8)\nxtraintis, xtesttis, ytraintis, ytesttis = train_test_split(Xtis, Y, random_state=1)\n", "intent": "Do both together help?\n"}
{"snippet": "url='https://health.data.ny.gov/api/views/h8yk-ufg9/rows.csv?accessType=DOWNLOAD'\ncareplan_utilization = pd.read_csv(url)\n", "intent": "We read in the file.\n"}
{"snippet": "filtered = pd.DataFrame(combined.pivot_table(index=['Year','Payer','Plan','Measure'],values='Events',aggfunc=np.sum))\nfiltered = filtered.reset_index()\nfiltered = filtered[filtered.Measure == 'ER Visits']\n", "intent": "Now that we've seen the overall trending data, we are going to drill down to our ER visits.\n"}
{"snippet": "indicator = pd.DataFrame()\nindicator['indicator_number'] = CommInd['Indicator Number']\nindicator['indicator'] = CommInd['Indicator']\nindicator = indicator.drop_duplicates(subset='indicator_number',take_last=True)\nindicator\n", "intent": "Explore the indicators in the data file to help us select those we want to explore\n"}
{"snippet": "import csv\nf=open('Hospital_Inpatient_Discharges__SPARCS_De-Identified___2012 (1).csv')\ndata = pd.read_csv(f)\n", "intent": "First we read in the data.\n"}
{"snippet": "needs = pd.read_csv(os.path.join(DERIVED_DATA_PATH,'C16B_PUF_cleaned_needs.csv'), na_values=' ', index_col='ID')\nwellness = pd.read_csv(os.path.join(DERIVED_DATA_PATH,'C16B_PUF_cleaned_wellness.csv'), na_values=' ', index_col='ID')\nkm_X = needs.join(wellness).dropna() \n", "intent": "Below we're going to join the needs and wellness domains, as it's our intended feature space.\n"}
{"snippet": "profile = pd.read_csv(os.path.join(DERIVED_DATA_PATH,'profile.csv'))\nprofile.head()\n", "intent": "The dataset consists of XXXX elements comprised of XXXX unique songs from XXXX artists, spanning   XXX weeks.\n"}
{"snippet": "profile = pd.read_csv(os.path.join(DERIVED_DATA_PATH,'C16B_PUF_cleaned_profile.csv'), na_values=' ', index_col='ID')\nprofile.head(3)\n", "intent": "The dataset consists of XXXX elements comprised of XXXX unique songs from XXXX artists, spanning   XXX weeks.\n"}
{"snippet": "X = StandardScaler().fit_transform(select_df)\nX\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nScale the Features using StandardScaler\n<br><br></p>\n"}
{"snippet": "import os.path\ncolumns = [u'ID', u'AGEGRP', u'RACE', u'GENDER', u'MRSTAT', u'EDUC', u'BMI', u'GENHTH', u'MODACT', u'CLMBSV', u'PACMPL', u'PLMTKW', u'EACMPL', u'ENTCRF', u'PNINTF', u'PCEFUL', u'ENERGY', u'BLSAD', u'SCLACT', u'ASHLTH', u'ASEHLTH', u'DIFBTH', u'DIFDRS', u'DIFEAT', u'DIFCHR', u'DIFWLK', u'DIFTOL', u'DIFPRM', u'DIFMON', u'DIFMED', u'PHYHTH', u'MENHTH', u'PORHTH', u'BLIND', u'DEAF', u'DIFCON', u'DIFERR', u'MEMINT', u'HIGHBP', u'ANGCAD', u'CHF', u'AMI', u'OTHHRT', u'STROKE', u'COPD_E', u'GI_ETC', u'ATHHIP', u'ATHHAN', u'OSTEOPO', u'SCIATC', u'DIABET', u'DEPRES', u'ANYCAN', u'COLNCA', u'LUNGCA', u'BRSTCA', u'PROSCA', u'OTHCAN', u'PNIACT', u'PNISOC', u'AVGPN', u'FELTNP', u'FELTSD', u'CMPHTH', u'SMKFRQ', u'URNLKG', u'URNMAG', u'URNDOC', u'URNTRT', u'PAOTLK', u'PAOADV', u'BALTLK', u'FELL12MO', u'BAL12MO', u'FALLTLK', u'OSTTEST', u'WHOCMP', u'SRVDSP', u'RNDNUM', u'PCTCMP', u'COHORT', u'PLREG', u'SVLANG']\ndata = pd.read_csv(os.path.join(SOURCE_DATA_PATH,'C16B_PUF.csv'), names=columns, na_values=' ', index_col='ID')\n", "intent": "Good.  So let's read it into pandas so we can begin our data inspection.\n"}
{"snippet": "wellness.to_csv(os.path.join(DERIVED_DATA_PATH,'C16B_PUF_cleaned_wellness.csv'))\n", "intent": "Note: Values Ascending from not a problem at all to all the time\n"}
{"snippet": "data = pd.read_csv(CSV_PATH, na_values=' ')\n", "intent": "Good.  So let's read it into pandas so we can begin our data inspection.\n"}
{"snippet": "wellness.to_csv(os.path.join(DERIVED_DATA_PATH,'wellness.csv'))\n", "intent": "Note: Values Ascending from not a problem at all to all the time\n"}
{"snippet": "x = np.sort(x, axis=0)\nU_vectors = []\nfor i in range(N):\n    U_vectors.append( get_linear_transform(x[i,:][np.newaxis], theta['W'], theta['b'])[:,0] )\nU_vectors = np.array(U_vectors).T\n", "intent": "Next we calculate all the $\\tilde{\\mathbf{U}}$ vectors...\n"}
{"snippet": "audio_fname_template = '../data/SpeechEmotion-master/data/{:02d}.wav'\ndef load_wave(i):\n    fname = audio_fname_template.format(i)\n    rate, wave = scipy.io.wavfile.read(fname)\n    wave = (wave + 0.5) / (0x7FFF + 0.5)  \n    wave *= 1.0 / np.max(np.abs(wave)) \n    return wave, rate\nwave, rate = load_wave(0)\n", "intent": "The next functions are similar to the ESC-50 session, loading, plotting and plying an audio clip.\n"}
{"snippet": "sns.pairplot(pd.DataFrame({'arousal':arousal, 'valence': valence}), height=3, plot_kws=dict(marker='o'));\n", "intent": "Let's also look at the distribution of arousal and valence values (on the main diagonal) and their joint distributions (on the off-diagonal).\n"}
{"snippet": "import keras\n(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\n", "intent": "We start by loading the MNIST digits data we used in the [softmax model session](softmax-model.ipynb).\n"}
{"snippet": "(X_train, _), (_, _) = keras.datasets.mnist.load_data()\nX_train = (X_train.astype(np.float32) - 127.5) / 127.5\nX_train = np.expand_dims(X_train, axis=3)\n", "intent": "Let's load the MNIST dataset through Keras' API.\nWe only need the train set as we don't actually use the labels.\n"}
{"snippet": "def pd_centers(featuresUsed, centers):\n\tcolNames = list(featuresUsed)\n\tcolNames.append('prediction')\n\tZ = [np.append(A, index) for index, A in enumerate(centers)]\n\tP = pd.DataFrame(Z, columns=colNames)\n\tP['prediction'] = P['prediction'].astype(int)\n\treturn P\n", "intent": "Let us first create some utility functions which will help us in plotting graphs:\n"}
{"snippet": "img_path = '../data/Kobe_Bryant_2014.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n", "intent": "That's really nice.\nLoad an image, convert it to array, and preprocess it for ResNet50.\n"}
{"snippet": "df = pd.DataFrame(data=X, columns=diabetes.feature_names)\nsns.pairplot(df, plot_kws=dict(alpha=0.25));\n", "intent": "Let's look at the features (`X`):\n"}
{"snippet": "df['Mas Vnr Type'].fillna(value='None',inplace=True)\ndf['Mas Vnr Area'].fillna(value=0.0,inplace=True)\n", "intent": "- I will replace the NaN with 0.0 and None for Msn Vnr Area and Msn Vnr Type respectively. \n"}
{"snippet": "from sklearn.preprocessing import LabelBinarizer\nlabel_binarizer = LabelBinarizer()\ny_one_hot = label_binarizer.fit_transform(y_train)\nprint (y_one_hot.shape)\n", "intent": "Visualizing the normalized data set after being normalized and grayscale\n"}
{"snippet": "tfv=TfidfVectorizer(min_df=0, max_features=None, strip_accents='unicode',lowercase =True,\n                    analyzer='word', token_pattern=r'\\w{3,}', ngram_range=(1,1),\n                    use_idf=True,smooth_idf=True, sublinear_tf=True, stop_words = \"english\")  \n", "intent": "First, we will not go more in details in tweets and just apply a TF-IDF and a model to have a baseline.\n"}
{"snippet": "df = pd.read_csv(\"preprocessed_audio/mean_cqt.csv\", index_col=0)\ndf = df.div(df.sum(axis=1)+1e-6, axis=0)\ndf = df.dropna(axis=0, how='any')\n", "intent": "As we mentionned previously, we can use Mini Batch Kmeans but we have to ensure that our clusters are well balanced in term of qty and homogeneous.\n"}
{"snippet": "df = pd.read_csv(\"preprocessed_audio/sample_content.csv\", index_col=0)\n", "intent": "Now let's try to fin d a similar audio to a given audio\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nenc = LabelBinarizer()\ny_enc = enc.fit_transform(y.reshape(-1, 1))\nX_train, X_test, y_train, y_test = train_test_split(X_reduced, y_enc, test_size=0.20, random_state=42)\n", "intent": "If we want to keep 90 of the variance we are now at 400 dimensions, this should be fine for a simple ANN.\n"}
{"snippet": "tracks = tracks[tracks.index.isin(list_index)]\ngenres.to_csv(\"preprocessed_meta/genres.csv\")\ntracks.to_csv(\"preprocessed_meta/tracks.csv\")\nechonest[echonest.index.isin(list_index)].to_csv(\"preprocessed_meta/echonest.csv\")\nfeatures[features.index.isin(list_index)].to_csv(\"preprocessed_meta/features.csv\")\nunique_album = np.unique(tracks[('album', 'id')].values)\nunique_artist = np.unique(tracks[('artist', 'id')].values)\nartists[artists.index.isin(unique_artist)].to_csv(\"preprocessed_meta/artists.csv\")\nalbums[albums.index.isin(unique_album)].to_csv(\"preprocessed_meta/albums.csv\")\n", "intent": "Now let's filter our datasets and save them.\n"}
{"snippet": "def read_data(filename):\n  f = zipfile.ZipFile(filename)\n  for name in f.namelist():\n    return f.read(name).split()\n  f.close()\nwords = read_data(filename)\nprint 'Data size', len(words)\n", "intent": "Read the data into a string.\n"}
{"snippet": "X = np.load(\"preprocessed_audio/mfcc_norm.npy\")\nX_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.20, random_state=42)\n", "intent": "For this model, it's easier, a simple fully connected model will do the job. The complete dataset fit into memory and the training will be very fast.\n"}
{"snippet": "dataset = pd.read_csv(\"creditcard.csv\")\n", "intent": "First let's explore quickly the dataset. I won't explain all what we have as it is described on above\n"}
{"snippet": "dataset_raw = pd.read_csv(\"HR.csv\")\n", "intent": "Let's first import the dataset and take a quick look to the content\n"}
{"snippet": "X = dataset.drop(\"left\", axis=1)\ny = dataset[\"left\"]\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n", "intent": "We can now, scale the dataset and prepare our train and test dataset\n"}
{"snippet": "control = pd.read_csv(\"features_test.csv\", header=[0, 1, 2], skipinitialspace=True, index_col=0).reset_index()[[\"index\"]]\ncontrol.columns = [\"fname\"]\ncontrol = control.set_index(\"fname\")\n", "intent": "Now we have our prediction. The only point is that we removed 3 lines of bugged audio. We have now to add them with a random class\n"}
{"snippet": "house_data = pd.read_csv(\"house_data.csv\")\n", "intent": "On va commencer par explorer rapidement le dataset car il est assez simple\n"}
{"snippet": "tracks_OHE.to_csv(\"F:/Nicolas/DNUPycharmProjects/machine_learning/audio/FMA/fma_metadata/classes.csv\")\n", "intent": "The next step will require lot of memory so the best is to save this dataset, clean the memroy and restart from here\n"}
{"snippet": "enc = LabelBinarizer()\ny = enc.fit_transform(y.reshape(-1, 1))\n", "intent": "Now our dataset should be balanced. Let'(s apply a Label Bisarizer and count each class\n"}
{"snippet": "pca = PCA(n_components=0.99)\nnew_X = pca.fit_transform(X)\n", "intent": "Now, let's reduce dimensions as much as possible\n"}
{"snippet": "df_test = pd.read_csv('../data/titanic/test.csv')\ndf_test.head()\n", "intent": "Read the test data:\n"}
{"snippet": "X_train, X_test, Y_train, Y_test = train_test_split(X_moons, y_moons, test_size=0.2)\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)\n", "intent": "Now let's split the model with a train dataset and test dataset\n"}
{"snippet": "scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n", "intent": "As we goona use the sigmoid function as activation function, we need to have centered datas so we can use StandardScaler from sklearn\n"}
{"snippet": "X = pd.DataFrame(data = dataset.data, columns =dataset.feature_names)\ny = pd.Series(data = dataset.target)\ny.name = \"Label\"\ny = y.map({i : dataset.target_names[i] for i in range(3)}).to_frame()\ny_ohe = pd.get_dummies(dataset.target)\ny_ohe.columns = dataset.target_names\n", "intent": "The dataset provided by sklearn is not a usual dataset so let's first create dataframes as we usually have\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.25, random_state=42)\n", "intent": "Now we can split both datas with a trian and validation set\n"}
{"snippet": "analyzer = CountVectorizer().build_analyzer()\nCV = CountVectorizer(lowercase=True, stop_words=\"english\", analyzer=stem_and_filter)\nTF_matrix = CV.fit_transform(X)\n", "intent": "For tests, we can also generate the TF Matrix but we won't use it.\n"}
{"snippet": "svd = TruncatedSVD(n_components = 100)\nnormalizer = Normalizer(copy=False)\nTF_embedded = svd.fit_transform(TF_matrix)\nTF_embedded = normalizer.fit_transform(TF_embedded)\nTF_IDF_embedded = svd.fit_transform(TF_IDF_matrix)\nTF_IDF_embedded = normalizer.fit_transform(TF_IDF_embedded)\n", "intent": "Let's now apply the LSA and normalize every row afterward.\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nTFVectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\nTF_train = TFVectorizer.fit_transform(X_train)\nTF_val = TFVectorizer.transform(X_val)\ninv_voc = {v: k for k, v in TFVectorizer.vocabulary_.items()}\nsum_ = TF_train.sum(axis=0)\n", "intent": "Let's first have a look of the count of every ingredients.\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ny_train_enc = lb.fit_transform(y_train_filtered)\nohe =  OneHotEncoder(dtype=np.bool)\ny_train_OHE = ohe.fit_transform(y_train_enc.reshape(-1,1))\n", "intent": "Just for information, We can also look at the balance for every origines. The dataset is not very balanced and can create troubles\n"}
{"snippet": "df = pd.read_csv(\"F:/data/trading/stat_history.csv\", header =[0, 1], index_col=[0, 1])\ndf = df.reset_index()\ndf.columns = [' '.join(col).strip() for col in df.columns.values]\n", "intent": "Let's start from the initial dataframe of price we prepared in the first Notebook\n"}
{"snippet": "df_test['Survived'] = test_y\ndf_test[['PassengerId', 'Survived']] \\\n    .to_csv('../data/titanic/results-rf.csv', index=False)\n", "intent": "Create a DataFrame by combining the index from the test data with the output of predictions, then write the results to the output:\n"}
{"snippet": "scaled_data = pd.DataFrame(scaled_features, columns=bank_note.columns[:-1])\nscaled_data.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "milk = pd.read_csv('monthly-milk-production-pounds-p.csv', index_col='Month')\nmilk.columns\n", "intent": "** Use pandas to read the csv of the monthly-milk-production.csv file and set index_col='Month' **\n"}
{"snippet": "import pandas as pd\ndata = pd.read_csv('census_data.csv')\n", "intent": "** Read in the census_data.csv data with pandas**\n"}
{"snippet": "data = pd.read_csv('../datasets/santander.csv')\ndata.shape\n", "intent": "Load the Santander Customer Satisfaction dataset\n"}
{"snippet": "data = pd.read_csv('../datasets/santander.csv')\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "In the following, we will code the VarianceThreshold from scratch\n"}
{"snippet": "X_train, X_test, y_train, y_test, = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n", "intent": "Transposing a dataframe is costly if the dataframe is big. Therefore, we can use the alternative loop to find duplicated columns in bigger datasets.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(\n    num_data.drop(labels=['target', 'ID'], axis=1),\n    num_data['target'],\n    test_size=0.3,\n    random_state=0\n)\nX_train.shape, X_test.shape\n", "intent": "In all feature selection procedures, it is good practice to select the features by examining only the training set and to avoid overfit.\n"}
{"snippet": "correlation_matrix = X_train.corr()\ncorrelation_matrix = correlation_matrix.abs().unstack()\ncorrelation_matrix = correlation_matrix.sort_values(ascending=False)\ncorrelation_matrix = correlation_matrix[correlation_matrix >=0.8]\ncorrelation_matrix = correlation_matrix[correlation_matrix < 1]\ncorrelation_matrix = pd.DataFrame(correlation_matrix).reset_index()\ncorrelation_matrix.columns = ['feature1', 'feature2', 'corr']\ncorrelation_matrix.head()\n", "intent": "Build a dataframe with the correlation between features; remember that the absolute value is the important part, not the sign.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nfeatures = list(group.feature2.unique()) + ['v17']\nrf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\nrf.fit(X_train[features].fillna(0), y_train)\n", "intent": "Loooots of NULL values in here. We can build a ML algorithm using all the features from the above list, and select the most predictive ones.\n"}
{"snippet": "from keras.datasets import mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n", "intent": "The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays:\n"}
{"snippet": "df = pd.DataFrame(scaled_data, columns = project_data.columns[:-1])\ndf.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncountvectorizer = CountVectorizer()\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "X = countvectorizer.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "df = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n", "intent": "With principal component analysis, we are finding out those that are the most variant\n"}
{"snippet": "movie_titles = pd.read_csv('Movie_Id_Titles')\nmovie_titles.head()\n", "intent": "We can use the Movie_Id_Titles to grab the movie names and merge it with the dataframe.\n"}
{"snippet": "vect = CountVectorizer(ngram_range = (1,2), stop_words='english', min_df =2)\ndoc_matrix = pd.DataFrame(vect.fit_transform(df.text).toarray(), columns=vect.get_feature_names())\ntype(df)\n", "intent": "Note that most of the sentiments (473 out of 979 are neutral) and that this can potentially skew the data\n"}
{"snippet": "month = bikes.index.month\npd.pivot_table(bikes, index='season', columns=month, values='temp', aggfunc=np.count_nonzero).fillna(0)\n", "intent": " seasons: \n  *  1 = spring\n  * 2 = summer \n  * 3 = fall \n  * 4 = winter \n"}
{"snippet": "import pandas as pd\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/titanic.csv'\ntitanic = pd.read_csv(url, index_col='PassengerId')\ntitanic.head()\n", "intent": "Read the data into Pandas\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n", "intent": "Split the data into training and testing sets\n"}
{"snippet": "img_path = '/Users/fchollet/Downloads/cats_and_dogs_small/test/cats/cat.1700.jpg'\nfrom keras.preprocessing import image\nimport numpy as np\nimg = image.load_img(img_path, target_size=(150, 150))\nimg_tensor = image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\nimg_tensor /= 255.\nprint(img_tensor.shape)\n", "intent": "This will be the input image we will use -- a picture of a cat, not part of images that the network was trained on:\n"}
{"snippet": "import pandas as pd\nyelp = pd.read_csv('yelp.csv')\nyelp.head(1)\n", "intent": "Using the yelp reviews database  create a Naive Bayes model to predict the star rating for reviews\nRead `yelp.csv` into a DataFrame.\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X.values, y.values, random_state=123)\n", "intent": "* Split the data in train and test\n* Train a KNN model (K=5)\n* Evaluate the accuracy\n"}
{"snippet": "import pandas as pd  \ndata_temp = pd.DataFrame(iris.data, columns=iris.feature_names)\ndata_temp['target'] = iris.target\ndata_temp['target'] = data_temp['target'].astype('category')\ndata_temp['target'].cat.categories = iris.target_names\nsns.pairplot(data_temp, hue='target', palette=sns.color_palette(\"hls\", 3))\n", "intent": "This data is four dimensional, but we can visualize two of the dimensions\nat a time using a simple scatter-plot:\n"}
{"snippet": "X_poly = PolynomialFeatures(degree=2).fit_transform(X)\nX_test_poly = PolynomialFeatures(degree=2).fit_transform(X_test)\n", "intent": "Now we'll use this to fit a quadratic curve to the data.\n"}
{"snippet": "from sklearn.feature_selection import SelectPercentile, f_classif\nsel = SelectPercentile(f_classif, percentile=50)\nsel.fit(X, y)\nsel.get_support()\n", "intent": "There is still the question of how to select the parameter k\n"}
{"snippet": "import pandas as pd\nimport numpy as np\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\ncol_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\niris = pd.read_csv(url, header=None, names=col_names)\niris.head()\n", "intent": "We'll read the iris data into a DataFrame, and **round up** all of the measurements to the next integer:\n"}
{"snippet": "vect = CountVectorizer(lowercase=False)\nX_dtm = vect.fit_transform(X)\nX_dtm.shape\n", "intent": "- **lowercase:** boolean, True by default\n- Convert all characters to lowercase before tokenizing.\n"}
{"snippet": "seq_len = 500\ntrn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\ntest = sequence.pad_sequences(test, maxlen=seq_len, value=0)\n", "intent": "Making every trn idx consistent and choosing max len and padding sequences less than this length to zeros\n"}
{"snippet": "data = pd.read_csv('../data/aquastat/aquastat.csv.gzip', compression='gzip')\ndata.region = data.region.apply(lambda x: simple_regions[x])\ndata = data.loc[~data.variable.str.contains('exploitable'),:]\ndata = data.loc[~(data.variable=='national_rainfall_index')]\n", "intent": "http://www.fao.org/nr/water/aquastat/data/query/index.html\n"}
{"snippet": "import cv2\nimg = cv2.imread(img_path)\nheatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\nheatmap = np.uint8(255 * heatmap)\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\nsuperimposed_img = heatmap * 0.4 + img\ncv2.imwrite('/Users/fchollet/Downloads/elephant_cam.jpg', superimposed_img)\n", "intent": "Finally, we will use OpenCV to generate an image that superimposes the original image with the heatmap we just obtained:\n"}
{"snippet": "clf_train, clf_test = train_test_split(txclf,test_size=0.4)\n", "intent": "Let's split the data into a training set and a test set\n"}
{"snippet": "vectorizer = TfidfVectorizer(max_df=0.5,\n                                 min_df=2, stop_words='english')\ntrans = vectorizer.fit_transform(txclf.Proposal)\n", "intent": "Create a text vectorizer\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntext_clf = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', SGDClassifier(loss='hinge', penalty='l2',  \n                        alpha=1e-3, n_iter=5, random_state=42)),])\ntext_clf.fit(clf_train.Proposal,clf_train.Bucket)\n", "intent": "Wait but shouldn't we do this in a more extensible way so it's easy for us to add data later? \nYes let's make a pipeline!!!\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\n", "intent": "Housing Prices Dataset\n"}
{"snippet": "cv = CountVectorizer(tokenizer=myTokenizer,ngram_range=(1,1))\nvectorized_corpus = cv.fit_transform(corpus)\nvc = vectorized_corpus.toarray()\nprint(cv.vocabulary_)\n", "intent": "- Unigrams\n- Bigrams, Trigrams\n- N-Grams\n"}
{"snippet": "def load_data():\n    column_names = ['digit', 'intensity', 'symmetry']\n    sep = '\\s+'\n    features_train = pd.read_table('http://www.amlbook.com/data/zip/features.train', sep=sep, names=column_names)\n    features_test = pd.read_table('http://www.amlbook.com/data/zip/features.test', sep=sep, names=column_names)\n    return features_train, features_test\n", "intent": "Load training and testing data in pandas dataframes\n"}
{"snippet": "def create_one_vs_all_dataframe(df, classifiers):\n    one_vs_all = pd.DataFrame(df, copy=True)\n    for class_label, digit in classifiers.items():\n        labels = one_vs_all.loc[one_vs_all['digit'] == digit, 'digit']\n        labels.loc[:] = 1.0\n        one_vs_all[class_label] = labels\n    one_vs_all.fillna(-1.0, inplace=True)\n    return one_vs_all\n", "intent": "Create one-versus-all dataframe by adding outputs for the different classifiers to the training dataframe\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features,dfr['Helpful'], test_size=0.33, random_state=42)\n", "intent": "We will set up some basic functions to test our classifiers through them and choose the best performer based on the accuracy. \n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features,dfr['Helpful'], test_size=0.3, random_state=42)\n", "intent": "We will set up some basic functions to test our classifiers through them and choose the best performer based on the accuracy. \n"}
{"snippet": "import keras\nimport numpy as np\npath = keras.utils.get_file(\n    'nietzsche.txt',\n    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\ntext = open(path).read().lower()\nprint('Corpus length:', len(text))\n", "intent": "Let's start by downloading the corpus and converting it to lowercase:\n"}
{"snippet": "custdata14 = pd.read_csv('C:\\Springboard Data\\LoanStats14.csv',low_memory=False)\n", "intent": "We are going to be combining 2014 and 2015 dataset in one dataframe and call it \"custdata\"\n"}
{"snippet": "import pandas as pd\nbos = pd.DataFrame(boston.data)\nbos.head()\n", "intent": "Now let's explore the data set itself. \n"}
{"snippet": "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\nprint (\"y_train.shape = {}\".format(y_train.shape))\nprint (\"x_train.shape = {}\".format(x_train.shape))\nprint(len(y_train), 'train samples')\nprint(len(y_test), 'test samples')\n", "intent": "We'll use the same data as before, so we included this step already.\n"}
{"snippet": "tdm = cv.fit_transform(text)\n", "intent": "<img src=\"img/13.PNG\">\n"}
{"snippet": "imread(all_images_list[0]).shape\n", "intent": " - np.expand_dims -> Expand the shape of an array.\n  - Insert a new axis, corresponding to a given position in the array shape.\n"}
{"snippet": "le = preprocessing.LabelEncoder()\n", "intent": "- Encode labels with value between 0 and n_classes-1.\n"}
{"snippet": "dataset = load_boston()\nprint(dataset.keys())\n", "intent": "<h3>  Problem \nIn this part, you should download and analyze **\"Boston House Prices\"** dataset. <br>\nHere use a code below to download the  dataset: \n"}
{"snippet": "X_train, X_test, Y_train, Y_test= train_test_split(features, target, random_state= 0, test_size=0.4)\nprint('The shape of X_train is', X_train.shape)\nprint('The shape of X_test is', X_test.shape)\nprint('The shape of Y_train is', Y_train.shape)\nprint('The shape of Y_test is', Y_test.shape)\n", "intent": "<h4> Problem \n- Report the shape of each (training and test) datasets.\n"}
{"snippet": "X_train, X_test, Y_train, Y_test= train_test_split(features, target, random_state= 0, test_size=0.333)\nprint('The shape of X_train is', X_train.shape)\nprint('The shape of X_test is', X_test.shape)\nprint('The shape of Y_train is', Y_train.shape)\nprint('The shape of Y_test is', Y_test.shape)\n", "intent": "<h4> Problem \n- Report the shape of each (training and test) dataset.\n"}
{"snippet": "from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nhousing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\nhousing_cat_1hot\n", "intent": "We can convert each categorical value to a one-hot vector using a `OneHotEncoder`:\n"}
{"snippet": "dataset = load_boston()\nprint(dataset.keys())\n", "intent": "<h4> Problem \nIn this part, you should download and analyze **\"Boston House Prices\"** dataset. <br>\nUse a code below to download the  dataset: \n"}
{"snippet": "from sklearn.datasets import load_boston\nimport pandas as pd\nimport numpy as np\ndataset = load_boston()\nprint(dataset.keys())\n", "intent": "<h3>  Problem \nIn this part, you should download and analyze **\"Boston House Prices\"** dataset. <br>\nHere use a code below to download the  dataset: \n"}
{"snippet": "df = pd.read_csv('College_data', index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "X = cntvct.fit_transform(X)\n", "intent": "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**\n"}
{"snippet": "import numpy as np\nfrom sklearn import datasets\nfrom sklearn.linear_model import LinearRegression\ndataset = datasets.load_diabetes()\nmodel = LinearRegression()\nmodel.fit(dataset.data, dataset.target)\nprint(model)\n", "intent": "* Classical Regression\n* Handling Nonlinearity\n* Regularization\n* Dimensionality\n"}
{"snippet": "diab = datasets.load_diabetes()\n", "intent": "For more information on the dataset, see the original paper at \nhttp://web.stanford.edu/%7Ehastie/Papers/LARS/LeastAngle_2002.pdf.\n"}
{"snippet": "def examine_coefficients(model, df):\n    df = pd.DataFrame(\n        { 'Coefficient' : model.coef_[0] , 'Feature' : df.columns}\n    ).sort_values(by='Coefficient')\n    return df[df.Coefficient !=0 ]\n", "intent": "- You can either create the dummy features manually or use the `dmatrix` function from `patsy`\n- Remember to scale the feature variables as well!\n"}
{"snippet": "df.to_csv(\"all_data.csv\", encoding=\"utf-8\")\n", "intent": "Save all data to file, so that I can start by reading it in the future.\n"}
{"snippet": "names = ['sample_id','clump thikness','uniformity_size','uniformity_shape','adhesion','cell_size','bare_nuclei','bland_chromatin','normal_nucleoti','mitoses','class']\ndf = pd.read_csv(\"../../assets/datasets/breast-cancer-wisconsin.csv\",names=names,na_values=['?'])\ndf.head()\n", "intent": "The column names are taken from the dataset info file. Create an array\nwith the column names and assign them as the header when loading the\ncsv.\n"}
{"snippet": "cat_encoder = CategoricalEncoder()\nhousing_cat_reshaped = housing_cat.values.reshape(-1, 1)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)\nhousing_cat_1hot\n", "intent": "The `CategoricalEncoder` expects a 2D array containing one or more categorical input features. We need to reshape `housing_cat` to a 2D array:\n"}
{"snippet": "model2 = LogisticRegression(penalty='l1')\nmodel2.fit(X_norm, y)\ncoeffs = pd.DataFrame(model2.coef_, columns = iris.feature_names, index =iris.target_names)\ncoeffs\n", "intent": "**Check** Try changing the penalty to `l1`, do the coefficients change?\n**Optional Check** Check score with `cross_val_score` and select best model\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconmat = np.array(confusion_matrix(y, kmeans.labels_))\nconfusion = pd.DataFrame(conmat, index=['setosa','versicolor','virginica'],columns=['pred. setosa','pred. versicolor','pred. virginica'])\nconfusion\n", "intent": "Compute the Confusion Matrix to test the performance of the clustering analysis\n"}
{"snippet": "df_rep = df[df.Class==\"republican\"]\ndf_dem = df[df.Class==\"democrat\"]\ndf_rep.fillna(method='pad',axis=0,inplace=True)\ndf_dem.fillna(method='pad',axis=0,inplace=True)\ndf_rep.fillna(method='bfill',axis=0,inplace=True)\ndf_dem.fillna(method='bfill',axis=0,inplace=True)\ndf = pd.concat([df_rep,df_dem],axis=0)\n", "intent": "Next, let's define the x and y variables: \n"}
{"snippet": "from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXs = ss.fit_transform(X)\n", "intent": "Then, standardize the x variable for analysis\n"}
{"snippet": "Xdf = pd.DataFrame(X_reduced,columns=['PC1','PC2','PC3'])\n", "intent": "Create a dataframe from the PCA results\n"}
{"snippet": "PCA_set = PCA(n_components=5)\nY = PCA_set.fit_transform(xStand)\nY\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "airport_operations = pd.read_csv(\"./assets/datasets/airport_operations.csv\")\nairport_operations.head()\n", "intent": "You might want to restart your kernel between parts of this lab, to avoid crossover of variable names.\n"}
{"snippet": "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=10000, stop_words='english')\nX = vectorizer.fit_transform(docs_raw)\n", "intent": "Ok, let's fit a vectorizer to get some features. You could use a CountVectorizer or a TfidfVectorizer.\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX = pre_poll[['state','edu','age']]\ny = pre_poll['inclusion']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3)\n", "intent": "Split the data in the ordinary way, making sure you have a 70/30 split.\n"}
{"snippet": "cat_encoder = CategoricalEncoder(encoding=\"onehot-dense\")\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)\nhousing_cat_1hot\n", "intent": "Alternatively, you can specify the encoding to be `\"onehot-dense\"` to get a dense matrix rather than a sparse matrix:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n", "intent": "You should keep 25% of the data in the test set.\n"}
{"snippet": "cv = CountVectorizer(ngram_range=(1,2), max_features=2500, binary=True, stop_words='english')\nwords = cv.fit_transform(rt[\"quote\"])\n", "intent": "It is up to you what ngram range you want to select. **Make sure that `binary=True`**\n"}
{"snippet": "Xtrain, Xtest, ytrain, ytest = train_test_split(words.values, rt[\"fresh\"].values, test_size=0.25)\n", "intent": "You should keep 25% of the data in the test set.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime, random_state=0)\nlinreg = LinearRegression().fit(X_train, y_train)\nprint('Crime Dataset')\nprint('Linear model intercept : {}'.format(linreg.intercept_))\nprint('Linear model coeff :\\n {}'.format(linreg.coef_))\nprint('R squared score (training) : {:.3f}'.format(linreg.score(X_train, y_train)))\nprint('R squared score (test) : {:.3f}'.format(linreg.score(X_test, y_test)))\n", "intent": "Linear Regression on Crime data\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nMinMaxScaler().fit_transform(df)\n", "intent": "** Now scale the data to have a minimum of 0 and a maximum value of 1 using scikit-learn. **\n"}
{"snippet": "print(lm.coef_)\ncdf=pd.DataFrame(lm.coef_,x.columns,columns=['Coef'])\ncdf\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "df=pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "df_new=pd.DataFrame(df_scaled,columns=df.columns[:-1])\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "data=pd.read_csv(\"train.csv\")\ndata.shape\ndata.loc[11]\n", "intent": "Load the \"train.csv\" dataset. You will be using the same file for sampling training and testing points.\n"}
{"snippet": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nnum_pipeline = Pipeline([\n        ('imputer', Imputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\n", "intent": "Now let's build a pipeline for preprocessing the numerical attributes:\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1)\nX_test['Sex'].value_counts\n", "intent": "Split the dataset into train and test with a test size of 20% of total dataset.\n"}
{"snippet": "data =pd.read_csv(\"train.csv\")\n", "intent": "Load the \"train.csv\" dataset. You will be using the same file for sampling training and testing points.\n"}
{"snippet": "l = LabelEncoder()\nX['Sex'] = l.fit_transform(X.Sex)\n", "intent": "The values in the 'Sex' column are 'Male/Female'. So convert them into 1/0 using Label Encoding.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.4)\n", "intent": "Split the dataset into train and test with a test size of 20% of total dataset.\n"}
{"snippet": "data=pd.read_csv(\"train.csv\")\ndata.shape\n", "intent": "Load the \"train.csv\" dataset. You will be using the same file for sampling training and testing points.\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1)\n", "intent": "Split the dataset into train and test with a test size of 20% of total dataset.\n"}
{"snippet": "pred_data = pd.read_table('./prediction.txt', delim_whitespace = True)\nX_pred = pred_data[['G', 'P','Z']].values\nX_pred = np.concatenate((np.ones((X_pred.shape[0], 1), dtype = X_pred.dtype), X_pred), axis = 1)\nX_pred\n", "intent": "Import the prediction dataset\n"}
{"snippet": "top_10_zip=pd.DataFrame(data['zip'].value_counts().head(10))\ntop_10_zip.reset_index(inplace=True)\ntop_10_zip.columns=['ZIP','Count']\ntop_10_zip\n", "intent": "Top 10 Zipcodes for Emergency Calls\n"}
{"snippet": "top_10_twp=pd.DataFrame(data['twp'].value_counts().head(10))\ntop_10_twp.reset_index(level=0,inplace=True)\ntop_10_twp.columns=['Township','Count']\ntop_10_twp\n", "intent": "Top 10 townships for 911 calls\n"}
{"snippet": "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.most_frequent = pd.Series([X[c].value_counts().index[0] for c in X],\n                                       index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.most_frequent)\n", "intent": "We will also need an imputer for the string categorical columns (the regular `Imputer` does not work on those):\n"}
{"snippet": "poly=PolynomialFeatures(2)\nclus_quad=pd.DataFrame(poly.fit_transform(clus_info),columns=['1','lat','lng','lat^2','lat*lng','lng^2'])\n", "intent": "Lets find the average population density of each of the clusters \n"}
{"snippet": "top_10_zip=pd.DataFrame(data['zip'].value_counts().head(10))\ntop_10_zip.reset_index(level=0,inplace=True)\ntop_10_zip.columns=['ZIP','Count']\ntop_10_zip\n", "intent": "Top 10 Zipcodes for Emergency Calls\n"}
{"snippet": "loan=pd.read_csv('loan_data.csv')\n", "intent": "** Use pandas to read loan_data.csv as a dataframe called loans.**\n"}
{"snippet": "data=pd.read_csv('College_Data',index_col=0)\n", "intent": "** Read in the College_Data file using read_csv. Figure out how to set the first column as the index.**\n"}
{"snippet": "dataframe=pd.read_csv(\"KNN_Project_Data\")\n", "intent": "** Read the 'KNN_Project_Data csv file into a dataframe **\n"}
{"snippet": "dataframe_feat=pd.DataFrame(scaled,columns=dataframe.columns[:-1])\ndataframe.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer().fit(X)\n", "intent": "**Import CountVectorizer and create a CountVectorizer object.**\n"}
{"snippet": "X=yelp_class['text']\ny=yelp_class['stars']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n", "intent": "**Redo the train test split on the yelp_class object.**\n"}
{"snippet": "from sklearn.pipeline import FeatureUnion\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", num_pipeline),\n        (\"cat_pipeline\", cat_pipeline),\n    ])\n", "intent": "Finally, let's join the numerical and categorical pipelines:\n"}
{"snippet": "def impute(col,value):\n    trainSet[col] = trainSet[col].fillna(value)\n    testSet[col] = testSet[col].fillna(value)\nimpute('PoolQC','No')\n", "intent": "First of all, I'll try to impute missing data for features identified earlier especially for categoric features\n"}
{"snippet": "X = ratings[['user', 'movie']].values\ny = ratings['rating'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n", "intent": "Create a traditional (X, y) pairing of data and label, then split the data into training and test data sets.\n"}
{"snippet": "import json\nimport brewer2mpl\ndata = json.loads(open('mplrc.json','r').read())\nfor x in data.keys():\n    try:\n        mpl.rcParams[x] = data[x]\n    except ValueError:\n        pass\ncolors = brewer2mpl.get_map('Set1', 'qualitative', 8).mpl_colors\nmpl.rcParams['axes.color_cycle'] = colors\n", "intent": "A little style from the previous session.\n"}
{"snippet": "import sys\nsys.path.append('mglearn')\nimport make_blobs\nX, y = make_blobs.make_blobs(centers=2, n_samples=1000, random_state=0)\n", "intent": "At first the blobs are made for evaluation\n"}
{"snippet": "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())\n", "intent": "Keras returns classes as a single column, so we convert to one hot encoding\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n", "intent": "In order to check the effect of regularization on test data, split the data into train and test using sklearn.\n"}
{"snippet": "grouped_by_flight_df = pd.DataFrame(flights_df.groupby([flights_df.carrier,flights_df.flight, flights_df.dest]).count())\ngrouped_by_flight_df[grouped_by_flight_df.day==365]\n", "intent": "Which flights (i.e. carrier + flight + dest) happen every day? Where do they fly to?\n"}
{"snippet": "weather_df = pd.read_csv('weather.csv.bz2')\nweather_df.shape\n", "intent": "What weather conditions are associated with flight delays leaving NYC? Use graphics to explore.\n"}
{"snippet": "a = flights_df[flights_df.dest=='SEA']\nb = a.origin.value_counts(dropna=False)\nc = pd.DataFrame(b)\nc.reset_index(level=0,inplace=True)\nprint(c)\nprint('JFK',c.origin.loc[0]/c.origin.sum()*100)\nprint('EWR',c.origin.loc[1]/c.origin.sum()*100)\n", "intent": "(e) What proportion of flights to Seattle come from each NYC airport?\n"}
{"snippet": "import numpy as np\nfrom sklearn.model_selection import train_test_split\nX = np.array(ham_emails + spam_emails)\ny = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "intent": "Okay, before we learn too much about the data, let's not forget to split it into a training set and a test set:\n"}
{"snippet": "weather_df=pd.read_csv('weather.csv.bz2')\nweather_df.head()\n", "intent": "What weather conditions are associated with flight delays leaving NYC? Use graphics to explore.\n"}
{"snippet": "rotated_df = pd.DataFrame(PCA().fit_transform(X.as_matrix()))\nrotated_df.columns = ['pc'+str(i) for i in range(33)]\n", "intent": "Majority of the variance is explained by the first two components.\n"}
{"snippet": "housing_data = pd.read_csv(\"RealEstate.csv\")\nhousing_data.head()\n", "intent": "We are going to solve the problem of predicting **house prices** based on historical data. \n"}
{"snippet": "from sklearn.datasets import load_digits\ndigits = load_digits()\nprint(digits.DESCR)\n", "intent": "Apply PCA (or RandomizedPCA?) to the Hand Written Digits dataset.\n"}
{"snippet": "from statsmodels.sandbox.tools.tools_pca import pca\nprcomp = pca(X.T)\nprint \"Standard deviations:\\n{}\".format(np.sqrt(prcomp[2]))\nprint \"\\nRotation:\\n{}\".format(prcomp[3])\n", "intent": "Principal Components\n========================================================\nThe easy way:\n--------------------------------------------------------\n"}
{"snippet": "def pca_summary(prcomp):\n    return pd.DataFrame([np.sqrt(prcomp.explained_variance_), \n              prcomp.explained_variance_ratio_, \n              prcomp.explained_variance_ratio_.cumsum()],\n             index = [\"Standard deviation\", \"Proportion of Variance\", \"Cumulative Proportion\"], \n             columns = (map(\"PC{}\".format, range(1, len(prcomp.components_)+1))))\n", "intent": "Percent Variance Accounted For\n========================================================\n"}
{"snippet": "heptathlon_unscaled_pca = PCA().fit(X_heptathlon.copy())\npca_summary(heptathlon_unscaled_pca)\n", "intent": "* Why is it important? (in other words, why might unscaled variables cause a problem with PCA?)\n"}
{"snippet": "df = pd.read_csv('data/apib12tx.csv')\ndf\n", "intent": "Let's load the data and give it a quick look.\n"}
{"snippet": "df = pd.read_csv('data/apib12tx.csv')\n", "intent": "Let's load the data and give it a quick look.\n"}
{"snippet": "X_few = X_train[:3]\nX_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\nX_few_wordcounts\n", "intent": "Let's try this transformer on a few emails:\n"}
{"snippet": "df = pd.read_csv('sentiment.csv', usecols=[1,7,8,9,10])\ndf.Date = pd.to_datetime(df.Date)\ndf.Date = df.Date.dt.date\ndf = df.set_index(pd.DatetimeIndex(df.Date))\nprint(df.shape)\ndf.head()\n", "intent": "-------------------------\n"}
{"snippet": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(df3_mean[[('XRP','Price'),('XRP','Change %')]])\nprice_norm = scaler.transform(df3_mean[[('XRP','Price'),('XRP','Change %')]])\n", "intent": "Next to keep everything standardized, lets normalize the XRP Price data to 1\n"}
{"snippet": "xold=np.arange(0,10,0.5)\nxtrain, xtest = train_test_split(xold)\nxtrain = np.sort(xtrain)\nxtest = np.sort(xtest)\nprint(xtrain, xtest)\n", "intent": "Calculate covariance based on the kernel\n"}
{"snippet": "df = pd.read_csv(\"data/Kline2.csv\", sep=';')\ndf.head()\n", "intent": "We read back the Oceanic tools data\n"}
{"snippet": "dfd = pd.read_csv(\"data/distmatrix.csv\", header=None)\ndij=dfd.values\ndij\n", "intent": "And read in the distance matrix\n"}
{"snippet": "dfcorr = pd.DataFrame(medcorrij*100).set_index(df.culture.values)\ndfcorr.columns = df.culture.values\ndfcorr\n", "intent": "We'll data frame it to see clearly\n"}
{"snippet": "ofdata=pd.read_csv(\"data/oldfaithful.csv\")\nofdata.head()\n", "intent": "We start by considering waiting times from the Old-Faithful Geyser at Yellowstone National Park.\n"}
{"snippet": "hm2df=pm.trace_to_dataframe(tracehm2)\nhm2df.head()\n", "intent": "The slope and intercept are very highly correlated:\n"}
{"snippet": "dffull=pd.read_csv(\"data/religion.csv\")\ndffull.head()\n", "intent": "Let us assume that we have a \"population\" of 200 counties $x$:\n"}
{"snippet": "from sklearn.pipeline import Pipeline\npreprocess_pipeline = Pipeline([\n    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n])\nX_train_transformed = preprocess_pipeline.fit_transform(X_train)\n", "intent": "We are now ready to train our first spam classifier! Let's transform the whole dataset:\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nztrain, ztest, xtrain, xtest = train_test_split(z,x)\n", "intent": "Now we split into a training set and a test set.\n"}
{"snippet": "from sklearn.datasets import load_boston \nimport pandas as pd\nboston = load_boston()\ndf = pd.DataFrame(boston.data, columns = boston.feature_names)\ndf['MEDV'] = boston.target\nprint(boston.DESCR)\n", "intent": "Now it's time to learn how to perform linear regression. We will analyze the Boston housing dataset loaded in the cell below.\n"}
{"snippet": "dfFeat = pd.DataFrame(scaledFeatures,columns=df.columns[:-1])\ndfFeat.head()\n", "intent": "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**\n"}
{"snippet": "X_train, X_test, y_train, y_test = train_test_split(X,y,\n                                test_size=0.3,random_state=101)\n", "intent": "** Use model_selection.train_test_split from sklearn to split the data into training and testing sets. Set test_size=0.3 and random_state=101**\n"}
{"snippet": "import sklearn.cross_validation\ndata_train, data_test, target_train, target_test = sklearn.cross_validation.train_test_split(\n    data.data, data.target, test_size=0.20, random_state = 5)\nprint(data.data.shape, data_train.shape, data_test.shape)\n", "intent": "well not that great. Let's use a supervised classifier\nFirst, split our data in train and test set\n"}
{"snippet": "from sklearn.datasets import load_boston\nboston = load_boston()\ntype(boston)\n", "intent": "    from sklearn.datasets import load_boston\n    boston = load_boston()\n    print(boston.DESCR)\n    boston_df = boston.data\n"}
{"snippet": "dataframe = pd.read_csv(\"conversion_data.csv\")\ndataframe.head(10)\n", "intent": "Load data and data exploration\n"}
{"snippet": "from sklearn.cross_validation import train_test_split\nitrain, itest = train_test_split(range(critics.shape[0]), train_size=0.7)\nmask=np.ones(critics.shape[0], dtype='int')\nmask[itrain]=1\nmask[itest]=0\nmask = (mask==1)\n", "intent": "Let's set up the train and test masks first, and then we can run the cross-validation procedure.\n"}
{"snippet": "bonds['industry'] = bonds['industry'].fillna('Financial')\nbonds['subindustry'] = bonds['subindustry'].fillna('Commer Banks Non-US')\n", "intent": "Clean industry columns.\n"}
{"snippet": "from sklearn.svm import SVC\nfrom sklearn import datasets\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  \ny = iris[\"target\"]\nsetosa_or_versicolor = (y == 0) | (y == 1)\nX = X[setosa_or_versicolor]\ny = y[setosa_or_versicolor]\nsvm_clf = SVC(kernel=\"linear\", C=float(\"inf\"))\nsvm_clf.fit(X, y)\n", "intent": "The next few code cells generate the first figures in chapter 5. The first actual code sample comes after:\n"}
{"snippet": "data['interest_mean_by_bond'].fillna(data['interest_mean_by_bond'].mean(), inplace=True)\ndata['interest_mean_by_customer'].fillna(data['interest_mean_by_customer'].mean(), inplace=True)\ndata['interest_mean_by_bond_and_action'].fillna(data['interest_mean_by_bond'].mean(), inplace=True)\ndata['interest_mean_by_customer_and_action'].fillna(data['interest_mean_by_customer'].mean(), inplace=True)\n", "intent": "Handle missing values.\n"}
{"snippet": "from sklearn import datasets\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  \ny = iris[\"target\"]\nsetosa_or_versicolor = (y == 0) | (y == 1)\nX = X[setosa_or_versicolor]\ny = y[setosa_or_versicolor]\n", "intent": "Let's use the Iris dataset: the Iris Setosa and Iris Versicolor classes are linearly separable.\n"}
{"snippet": "roc_auc_score(y_test, test_probs)\n", "intent": "<b> What does this tell us about the model? Is it a good or bad model?<b>\n"}
{"snippet": "mean_cv_score = cross_val_score()\nprint (\"The cross validated accuracy score is {:.2f} percent\").format(mean_cv_score*100)\n", "intent": "How does the testing accuracy compare to the first one?\n<br><br><br><br>\nUse cross validation to derive a truer testing accuracy score\n"}
{"snippet": "train_y_hat = model.predict(train_X)\nprint np.sqrt(metrics.mean_squared_error(train_y, train_y_hat))\ntest_y_hat = model.predict(test_X)\nprint np.sqrt(metrics.mean_squared_error(test_y, test_y_hat))\n", "intent": "- (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error)\n"}
{"snippet": "def predict(clf, timeline):\n    messages = []\n    for tweet in timeline:\n        messages.append(tweet.text)\n    new_tweets = np.array(messages)\n    y_pred = clf.predict(new_tweets)\n    return y_pred\n", "intent": "- Use the SVM classifier to classify each tweet in timeline as a positive tweet or a negative tweet.\n"}
{"snippet": "print(\"Light Cars: \", linreg.predict([[0,0]])) \nprint(\"Heavy Cars: \", linreg.predict([[1,0]])) \nprint(\"Medium Cars: \", linreg.predict([[0,1]])) \n", "intent": "** Now let's use predict function to predict mpg for light, heavy and medium cars **\n"}
{"snippet": "RSS = sum((y-linreg.predict(X))**2) \nTSS = sum((y-y.mean())**2)\nR_Squared = 1 - float(RSS)/TSS\nprint(R_Squared)\n", "intent": "This is the definition of **R_Squared**. Let's first calculate it = this is the hard way!\n"}
{"snippet": "from sklearn.metrics import roc_curve, auc,roc_auc_score\ny_hat_probability = lm.predict_proba(X).T[1]  \nprint(y_hat_probability)\nprint(\"AUC is %f \" %roc_auc_score(y, y_hat_probability)) \nvals = roc_curve(y, y_hat_probability) \n", "intent": "Read more about ROC curves in puthon [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n"}
{"snippet": "member_data['Predicted'] = model1_fit.predict(member_data[train_cols])\n", "intent": "Using the .predict function I created a Predicted column to compare the original Renewed value to the model's prediction.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nprint \"RMSE:\", mean_squared_error(ys, predictions)\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(8)\nprint \"RMSE:\", mean_squared_error(ys, predictions)\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "print(accuracy_score(Y_test, Y_pred))\nprint(precision_score(Y_test, Y_pred))\nprint(recall_score(Y_test, Y_pred))\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(Y_test, Y_pred)\nprint(acc)\n", "intent": "Calculate the accuracy with the accuracy_score() function from sklearn\n"}
{"snippet": "from sklearn.metrics import classification_report\ncls_rep = classification_report(Y_test, Y_pred)\nprint(cls_rep)\n", "intent": "Create the classification report with the classification_report() function\n"}
{"snippet": "def find_mse(df, degree):\n    model, prediction = fit_reg_poly(df, degree)\n    prediction = np.ravel(prediction)\n    result = mean_squared_error(df['DepDelay'], model.predict(df['CRSDepTime'][:, np.newaxis]))\n    return result\n", "intent": "Write a function named `find_mse()` that returns the mean squared error of ridge regresion model, given the degree. Assume that alpha is always 1.0.\n"}
{"snippet": "print classification_report(y_test, y_pred)\n", "intent": "Very similar to training data.\n"}
{"snippet": "batch_size = 2\ninput_width = 4\ninput_height = 4\ninput_depth = 3\nfilter_size = 4\noutput_depth = 3\nstride = 2\npadding = 1\ndef relative_error(x, y):\n", "intent": "Let's test our layer on some dummy data:\n"}
{"snippet": "dout = np.random.randn(batch_size, pool_size, pool_size, input_depth)\ndx_num = eval_numerical_gradient_array(lambda x: max_pool_forward(x, pool_size, stride), X, dout)\nout = max_pool_forward(X, pool_size, stride)\ndx = max_pool_backward(dout, X, out, pool_size, stride)\nprint('Testing max_pool_backward function:')\ndiff = relative_error(dx, dx_num)\nif diff < 1e-12:\n    print('PASSED')\nelse:\n    print('The diff of a is too large, try again! a: ' + str(diff))\n", "intent": "And we again use numerical gradient checking to ensure that the backward function is correct: \n"}
{"snippet": "def relative_error(x, y):\n", "intent": "Let's test this on some dummy data.\n"}
{"snippet": "zip(temps,pfail, clf.predict(temps.reshape(-1,1)))\n", "intent": "The failures in prediction are, exactly where you might have expected them to be, as before.\n"}
{"snippet": "logreg = cv_and_fit(train_x, train_y, np.logspace(-4, 3, num=100))\npd.crosstab(test_y, logreg.predict(test_x), rownames=[\"Actual\"], colnames=[\"Predicted\"])\n", "intent": "We then do a cross-validated logistic regression. Note the large amount of the regularization. Why do you think this is the case?\n"}
{"snippet": "criteria = [part5_to_int_mapper['06200'], 2000, 0, 5, symptom_to_int_mapper['WARNING LIGHT ON']]\nint_to_symp_class_mapper[clf.predict([criteria])[0]]\n", "intent": "**Predict just one record.**  Here I will feed the prediction model a transmission part \n"}
{"snippet": "logreg.predict_proba([1, 0, 29, 0])[:, 1]\n", "intent": "Predict probability of survival for **Adam**: first class, no parents or kids, 29 years old, male.\n"}
{"snippet": "logreg.predict_proba([2, 0, 29, 0])[:, 1]\n", "intent": "Predict probability of survival for **Bill**: same as Adam, except second class.\n"}
{"snippet": "clf, ts_preds = classify_topics(nmf, td_norm, train['target'], test_data, check_random_state(0))\nprint(classification_report(test['target'], ts_preds, target_names=test['target_names']))\n", "intent": "The resulting classification report and confusion matrix are shown to demonstrate the quality of this classification method.\n"}
{"snippet": "new_observation = [[3, 5, 4, 2]]\nknn.predict(new_observation)\n", "intent": "- New observations are called \"out-of-sample\" data\n- Uses the information it learned during the model training process\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y, y_pred)\n", "intent": "Classification accuracy:\n- **Proportion** of correct predictions\n- Common **evaluation metric** for classification problems\n"}
{"snippet": "print (10 + 0 + 20 + 10)/4.\nfrom sklearn import metrics\nprint metrics.mean_absolute_error(true, pred)\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print (10**2 + 0**2 + 20**2 + 10**2)/4.\nprint metrics.mean_squared_error(true, pred)\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "import numpy as np\nprint np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.)\nprint np.sqrt(metrics.mean_squared_error(true, pred))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "print(model.predict(vect.transform(['having bad issue with gadget ',\n                                    'everything working great'])))\n", "intent": "1 is positive review and 0 is negative review\n"}
{"snippet": "L2_LOSS = 0.0015\nl2 = L2_LOSS * \\\n    sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred_Y, labels = Y)) + l2\n", "intent": "Again, we must properly name the tensor from which we will obtain predictions. We will use L2 regularization and that must be noted in our loss op:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint 'classification report results:\\r\\n' + classification_report(y_pred=knn4_pred,y_true=y_val)\n", "intent": "as we can see our assamption was indeed correct - categories 6,8 and 2 are those easiest to predict\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint('classification report results:\\r\\n' + classification_report(y_pred=knn4_pred,y_true=y_val))\n", "intent": "as we can see our assamption was indeed correct - categories 6,8 and 2 are those easiest to predict\n"}
{"snippet": "print(\"F1: %4f\" % metrics.f1_score(bank_labels_test, y_pred))\n", "intent": " F1 score conveys the balance between the precision and the recall.\n"}
{"snippet": "X_enc = encode(X_train[0:5])\nprint(\"X_enc.shape : \", X_enc.shape)\nX_rec = decode(np.random.random((2, 10)).astype(np.float32))\nprint(\"X_rec.shape : \", X_rec.shape)\nX_pred = model.predict(X_train[0:5])\npl.imshow(X_pred[0].squeeze(), interpolation='nearest', cmap=cm.binary)\npl.colorbar()\n", "intent": "A theano function to get the representation of a given input (without reconstructing it)\n"}
{"snippet": "y_3l = (y_enA * 4./9.) + (y_ccA * 2./9.) + (y_enB * 2./9.) + (y_ccB * 1./9.)\nprint('{:20s} {:2s} {:1.7f}'.format('3rd_layer:', 'logloss  =>', log_loss(y_test, y_3l)))\n", "intent": "Simple weighted average of the previous 4 predictions.\n"}
{"snippet": "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\nprint \"\\ntest loss: \", test_loss\nprint \"test accuracy: \", test_acc\n", "intent": "Let's evaluate our model on the test set:\n"}
{"snippet": "print \"predicting on test data...\"\ny_pred = model.predict(X_test)\nmu_pred = y_pred[:,:num_clusters*data_dim]\nmu_pred = np.reshape(mu_pred, [-1, num_clusters, data_dim])\nsigma_pred = y_pred[:,num_clusters*data_dim : num_clusters*(data_dim+1)] \npi_pred = y_pred[:,num_clusters*(data_dim+1):]\nz_pred = np.argmax(pi_pred, axis=-1)\n", "intent": "We can now make predictions on test data:\n"}
{"snippet": "def ndcg_score(preds, dtrain):\n    labels = dtrain.get_label()\n    top = []\n    for i in range(preds.shape[0]):\n        top.append(np.argsort(preds[i])[::-1][:5])\n    mat = np.reshape(np.repeat(labels,np.shape(top)[1]) == np.array(top).ravel(),np.array(top).shape).astype(int)\n    score = np.mean(np.sum(mat/np.log2(np.arange(2, mat.shape[1] + 2)),axis = 1))\n    return 'ndcg', score\n", "intent": "The evaluation metric is NDCG @k where k=5. So we select top 5 then get the average.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nregressor_mse = mean_squared_error(y_pred, y_test)\nimport math\nmath.sqrt(regressor_mse)\n", "intent": "So in our model, 60.7% of the variability in Y can be explained using X.\n"}
{"snippet": "y_prediction = regressor.predict(X_test)\ny_prediction\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nPerform Prediction using Linear Regression Model\n<br><br></p>\n"}
{"snippet": "y_prediction = regressor.predict(X_test)\ny_prediction\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nPerform Prediction using Decision Tree Regressor\n<br><br></p>\n"}
{"snippet": "y_pred = regressor.predict(X_test)\nprint('Liner Regression R squared: %.4f' % regressor.score(X_test, y_test))\n", "intent": "Done! We now have a working Linear Regression model.\n"}
{"snippet": "print(\"Average Precision Score: %4f\" % metrics.average_precision_score(bank_labels_test, y_pred))\n", "intent": "The Average Precision Score is quite low:\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\nlin_mae = mean_absolute_error(y_pred, y_test)\nprint('Liner Regression MAE: %.4f' % lin_mae)\n", "intent": "Calculate mean absolute error (MAE)\n"}
{"snippet": "predictions = humidity_classifier.predict(X_test)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nPredict on Test Set \n<br><br></p>\n"}
{"snippet": "accuracy_score(y_true = y_test, y_pred = predictions)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nMeasure Accuracy of the Classifier\n<br><br></p>\n"}
{"snippet": "test_x = test_data[:, 1:]\ntest_y = clf.predict(test_x)\n", "intent": "Take the decision trees and run it on the test data:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(test_y, \n                            predict_y, \n                            target_names=['Not Survived', 'Survived']))\n", "intent": "$$Precision = \\frac{TP}{TP + FP}$$ \n$$Recall = \\frac{TP}{TP + FN}$$ \n$$F1 = \\frac{2TP}{2TP + FP + FN}$$ \n"}
{"snippet": "y_pred = knn.predict(X)\n", "intent": "Now we'll use this classifier to *predict* labels for the data\n"}
{"snippet": "test_loss, test_acc = model.evaluate(test_images, test_labels)\n", "intent": "Let's evaluate the model on the test data:\n"}
{"snippet": "model.load_weights('pre_trained_glove_model.h5')\nmodel.evaluate(x_test, y_test)\n", "intent": "And let's load and evaluate the first model:\n"}
{"snippet": "some_data = housing.iloc[:4]\nsome_labels = housing_labels.iloc[:4]\nprint(\"Predictions:\\t\", prepare_select_and_predict_pipeline.predict(some_data))\nprint(\"Labels:\\t\\t\", list(some_labels))\n", "intent": "Let's try the full pipeline on a few instances:\n"}
{"snippet": "baseline_score = metrics.f1_score(bank_labels_test, y_pred)\nbaseline_score\n", "intent": "We have 1172 + 48 correct predictions and 29 + 108 false predictions\n"}
{"snippet": "X_test = preprocess_pipeline.transform(test_data)\ny_pred = svm_clf.predict(X_test)\n", "intent": "Great, our model is trained, let's use it to make predictions on the test set:\n"}
{"snippet": "y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\naccuracy_score(y_test, y_pred)\n", "intent": "Ah, this looks good! Let's select this model. Now we can test it on the test set:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_pred = lin_svr.predict(X_train_scaled)\nmse = mean_squared_error(y_train, y_pred)\nmse\n", "intent": "Let's see how it performs on the training set:\n"}
{"snippet": "y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)\nmse = mean_squared_error(y_train, y_pred)\nnp.sqrt(mse)\n", "intent": "Now let's measure the RMSE on the training set:\n"}
{"snippet": "y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\nmse = mean_squared_error(y_test, y_pred)\nnp.sqrt(mse)\n", "intent": "Looks much better than the linear model. Let's select this model and evaluate it on the test set:\n"}
{"snippet": "X_test_reduced = pca.transform(X_test)\ny_pred = rnd_clf2.predict(X_test_reduced)\naccuracy_score(y_test, y_pred)\n", "intent": "*Exercise: Next evaluate the classifier on the test set: how does it compare to the previous classifier?*\n"}
{"snippet": "loss = tf.losses.log_loss(y, y_proba)  \n", "intent": "But we might as well use TensorFlow's `tf.losses.log_loss()` function:\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score\nprecision_score(y_test, y_pred)\n", "intent": "Let's compute the model's precision and recall:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pred = dnn_clf.predict(X_test1)\naccuracy_score(y_test1, y_pred)\n", "intent": "The model is trained, let's see if it gets the same accuracy as earlier:\n"}
{"snippet": "accuracy_score(ytest, ytest_model)\n", "intent": "Compute the accuracy of the model:\n"}
{"snippet": "y_pred = dnn_clf_bn.predict(X_test1)\naccuracy_score(y_test1, y_pred)\n", "intent": "The best params are reached during epoch 48, that's actually a slower convergence than earlier. Let's check the accuracy:\n"}
{"snippet": "y_pred = dnn_clf.predict(X_train1)\naccuracy_score(y_train1, y_pred)\n", "intent": "Let's go back to the best model we trained earlier and see how it performs on the training set:\n"}
{"snippet": "y_pred = dnn_clf_dropout.predict(X_test1)\naccuracy_score(y_test1, y_pred)\n", "intent": "Let's check the accuracy:\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint('Logistic Regression:')\nprint(classification_report(y_test, y_pred_lr))\ncm = confusion_matrix(y_test, y_pred_lr)\nprint(cm)\n", "intent": "http://scikit-learn.org/stable/modules/model_evaluation.html\n- classification_report\n- confusion_matrix\n- accuracy_score\n- f1_score\n- ...\n"}
{"snippet": "labels_true = df_labels\nlabels_predict = db.labels_\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels_predict))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels_predict))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels_predict))\nprint(\"Adjusted Rand Index: %0.3f\"\n      % metrics.adjusted_rand_score(labels_true, labels_predict))\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(df_data_scaled, labels_predict))\n", "intent": "Compute the Evaluation Metrics:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\npred_scaled = linear_model.predict(X_test_scaled)\nprint('Truth:', y_test_scaled)\nprint('Predictions:', pred_scaled)\nprint('MSE:', mean_squared_error(y_test_scaled, pred_scaled))\nprint('R2:', r2_score(y_test_scaled, pred_scaled))\n", "intent": "Let's validate our model's performance by computing metrics and plotting the linear model.\n"}
{"snippet": "print('Logistic Regresion:')\nprint(classification_report(y_test, pred_logistic))\nprint('Logistic Regresion (balanced):')\nprint(classification_report(y_test, pred_logistic_bal))\nprint('SGD:')\nprint(classification_report(y_test, pred_sgd))\nprint('SGD (balanced):')\nprint(classification_report(y_test, pred_sgd_bal))\nprint('Baseline:')\nprint(classification_report(y_test, pred_baseline))\n", "intent": "8. metrics\n9. learning curve\n10. prediction\n"}
{"snippet": "y_pred_train = gs2.predict(Z_train)\nZ_train_pca = pca.transform(Z_train) \ny_pred_test = gs2.predict(Z_test)\nZ_test_pca = pca.transform(Z_test) \n", "intent": "This can be a way to determine why our errors are high\n"}
{"snippet": "y_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 1, 0]\nprint(classification_report(y_true, y_pred))\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n"}
{"snippet": "cvs = cross_val_score(rfc, Xtest, ytest, cv=10)\ncvs\n", "intent": "Use `cross_val_score` to perform k-fold cross validation (`k=10`) with this model:\n"}
{"snippet": "shutil.rmtree(OUTDIR, ignore_errors = True) \ntrain_and_evaluate(OUTDIR, num_train_steps = 2000)\n", "intent": "<h2>Run training</h2>\n"}
{"snippet": "shutil.rmtree(OUTDIR, ignore_errors = True) \ntrain_and_evaluate(OUTDIR, num_train_steps = 500)\n", "intent": "<h2>Run training</h2>\n"}
{"snippet": "def print_rmse(model, df):\n  metrics = model.evaluate(input_fn = make_eval_input_fn(df))\n  print('RMSE on dataset = {}'.format(name, np.sqrt(metrics['average_loss'])))\nprint_rmse(model, df_valid)\n", "intent": "Evaluate on the validation data (we should defer using the test data to after we have selected a final model).\n"}
{"snippet": "def train_and_evaluate(output_dir, num_train_steps):\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n", "intent": "<h2> tf.estimator.train_and_evaluate </h2>\n"}
{"snippet": "OUTDIR='mnist/learned'\nshutil.rmtree(OUTDIR, ignore_errors = True) \nhparams = {'train_steps': 1000, 'learning_rate': 0.01}\ntrain_and_evaluate(OUTDIR, hparams)\n", "intent": "This is the main() function\n"}
{"snippet": "def print_rmse(model, name, df):\n  metrics = model.evaluate(input_fn = make_input_fn(df, 1))\n  print('RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss'])))\nprint_rmse(model, 'validation', df_valid)\n", "intent": "Evaluate on the validation data (we should defer using the test data to after we have selected a final model).\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"test.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "train_preds = meta_model.predict(X_train_level2).clip(0.,20.) \nrmse_train_stacking = math.sqrt(mean_squared_error(y_train_level2, train_preds))\nstack_val_preds = meta_model.predict(X_val_level2).clip(0.,20.) \nstack_test_preds = meta_model.predict(X_test_level2).clip(0.,20.) \nrmse_val_stacking= math.sqrt(mean_squared_error(y_val, stack_val_preds))\nprint('Train  RMSE for stacking is %f' % rmse_train_stacking)\nprint('-------------')\nprint('Validation set RMSE for stacking is %f' % rmse_val_stacking)\n", "intent": "Compute R-squared on the train and test sets and also the test set predictions\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nimport numpy as np\nprint np.sqrt(mean_squared_error(y_test, dtr.predict(X_test)))\n", "intent": "Take a minute to discuss Root Mean Squared Error [RMSE](https://www.kaggle.com/wiki/RootMeanSquaredError)\n"}
{"snippet": "print(classification_report(ytest, ytest_model))\n", "intent": "Compute the average accuracy and its standard deviation:\n"}
{"snippet": "y = km.predict(data)\n", "intent": "Predict the clusters for each data point, store this as `y`\n"}
{"snippet": "y = km.predict(data)\nprint y\n", "intent": "Predict the clusters for each data point, store this as `y`\n"}
{"snippet": "MIN_RATING = 0.0\nMAX_RATING = 5.0\nITEMID = 1\nUSERID = 1\nsvd.predict(ITEMID, USERID, MIN_RATING, MAX_RATING)\n", "intent": "Try using `svd.predict()` to predict ratings for a given user and movie, $\\hat{r}_{ui}$\n"}
{"snippet": "predict = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint 'CONFUSION MATRIX'\nprint confusion_matrix(y_test, predict)\nprint '\\n\\nCLASSIFICATION REPROT'\nprint classification_report(y_test, predict)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "C = 5\ny_test_oh = np.eye(C)[Y_test.reshape(-1)]\nX_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\npred = model.predict(X_test_indices)\nfor i in range(len(X_test)):\n    x = X_test_indices\n    num = np.argmax(pred[i])\n    if(num != Y_test[i]):\n        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())\n", "intent": "You should get a test accuracy between 80% and 95%. Run the cell below to see the mislabelled examples. \n"}
{"snippet": "x_test = np.array(['not feeling happy'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "best_model = best_linear_model\ntest_predictions = best_model.decision_function(x_test)\ntest_score = roc_auc_score(y_test, test_predictions)\nprint(\"Test ROC AUC = {}\".format(test_score))\n", "intent": "... and the best model is...?\nIt seems that Linear SVM has larger auc-score.\n"}
{"snippet": "best_model = best_linear_model_tf\ntest_predictions = best_model.decision_function(x_test)\ntest_score = roc_auc_score(y_test, test_predictions)\nprint(\"Test ROC AUC = {}\".format(test_score))\n", "intent": "... and the best model is...? Still Linear SVM, but the Kernel version is very close to it.\n"}
{"snippet": "def predict(clf, timeline):\n    tweets = []\n    for i in timeline:\n        tweets.append(i.text)\n    y_pred = clf.predict(np.array(tweets))\n    return y_pred\n", "intent": "- Use the RandomForestClassifier to classify each tweet in timeline as a positive tweet or a negative tweet.\n"}
{"snippet": "def sum_squared_error(y_hat, y):\n    return np.sum(0.5*(y_hat - y)**2)\n", "intent": "A helper function for the Sum Squared Error\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Xnew = np.linspace(m.X.mean.min(), m.X.mean.max())[:, None]\nYnew = m.predict(Xnew)[0]\n", "intent": "What does the underlying trend explaining the data actually look like according to the model?\n"}
{"snippet": "metrics.silhouette_score(X, labels, metric='euclidean')\n", "intent": "What does the score indicate?\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nsilhouette_score(X, labels)\n", "intent": "_(pairplot with hue)_\n"}
{"snippet": "print(('Estimated number of clusters: %d' % n_clusters_))\nprint((\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels)))\nprint((\"Completeness: %0.3f\" % metrics.completeness_score(y, labels)))\nprint((\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels)))\n", "intent": "**7.2 Check the homogeneity, completeness, and V-measure against the stored rank `y`**\n"}
{"snippet": "print((\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, labels)))\n", "intent": "**9.3 Evaluate DBSCAN visually, with silhouette, and with the metrics against the true `y`.**\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrms = sqrt(mean_squared_error(test.Weekly_Sales, y_hat.naive))\nprint(rms)\n", "intent": "Let's use RMSE to check the accuracy of our model on the test data set.\n"}
{"snippet": "rms = sqrt(mean_squared_error(test.Weekly_Sales, y_hat_avg.avg_forecast))\nprint(rms)\n", "intent": "Now, let's calculate RMSE to check to accuracy of our model.\n"}
{"snippet": "def find_mse(df, degree):\n    mod, pred = fit_reg_poly(df, degree)\n    result = mean_squared_error(df['DepDelay'], pred)\n    return result\n", "intent": "Write a function named `find_mse()` that returns the mean squared error of ridge regresion model, given the degree. Assume that alpha is always 1.0.\n"}
{"snippet": "rms = sqrt(mean_squared_error(test.Weekly_Sales, y_hat_avg.SES))\nprint(rms)\n", "intent": "Now, let's use RMSE to check to accuracy of our model.\n"}
{"snippet": "rms = sqrt(mean_squared_error(test.Weekly_Sales, y_hat_avg.Holt_Winter))\nprint(rms)\n", "intent": "Let's look at the RMSE to check accuracy.\n"}
{"snippet": "rms = sqrt(mean_squared_error(test.Weekly_Sales, y_hat_avg.SARIMA))\nprint(rms)\n", "intent": "Let's check the RMSE for accuracy of the model.\n"}
{"snippet": "y_pred_class = knn.predict(X)\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y, y_pred_class))\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "lasso = Lasso(alpha=optimal_lasso.alpha_)\nlasso_scores = cross_val_score(lasso, Xs, y, cv=10)\nprint(lasso_scores)\nprint(np.mean(lasso_scores))\n", "intent": "---\nIs it better than the Linear regression? Is it better than Ridge? What do the differences in results imply about the issues with the dataset?\n"}
{"snippet": "enet = ElasticNet(alpha=optimal_enet.alpha_, l1_ratio=optimal_enet.l1_ratio_)\nenet_scores = cross_val_score(enet, Xs, y, cv=10)\nprint(enet_scores)\nprint(np.mean(enet_scores))\n", "intent": "---\nHow does it compare to the Ridge and Lasso regularized regressions?\n"}
{"snippet": "xlist = [20,21]\nXlist = sm.add_constant(xlist)\nresults.predict(Xlist)\n", "intent": "Hint: You'll need to use a list - don't forget your intercept!\n"}
{"snippet": "linreg.predict(2)\n", "intent": "**Confirm that this is the same value we would get when using the built-in `.predict()` method of the `LinearRegression` object.**\n"}
{"snippet": "logit_pred_proba = logit_simple.predict_proba(X_test)[:,1]\n", "intent": "**Create a confusion matrix of predictions on our test set using `metrics.confusion_matrix`**.\n"}
{"snippet": "CR_pred_train_y=CR_clf.predict(CR_train_X)\n", "intent": "Predict labels on the training set, and name the returned variable as `CR_pred_train_y`\n"}
{"snippet": "metrics.accuracy_score(y_test2,y_pred3)\n", "intent": "**Evaluate the model metrics now**\n"}
{"snippet": "y_pred = linreg.predict(X)\nglass['y_pred'] = y_pred\n", "intent": "**Using the `LinearRegression` object we have fit, create a variable that are our predictions for `ri` for each row's `al` in the data set.**\n"}
{"snippet": "print(metrics.mean_squared_error(y_test, y_test.apply(np.mean, broadcast=True)))\n", "intent": "How does this compare to what we achieved with linear regression. Is our model making an actual improvement?\n"}
{"snippet": "pred = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test, pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "y_pred = lg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "pred = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "pred = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "preds = model.evaluate(X_test, Y_test)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))\n", "intent": "Let's see how this model (trained on only two epochs) performs on the test set.\n"}
{"snippet": "CR_pred_test_y=CR_clf.predict(CR_test_X)\n", "intent": "Predict the labels on the test set. Name the returned variable as `CR_pred_test_y`\n"}
{"snippet": "print(clf.predict([[3., +2.5]]))\n", "intent": "What about at `[3., +2.5]`?\n"}
{"snippet": "from scipy.stats import pearsonr\npearsonr(gender_test, D.predict(X_test).ravel())\n", "intent": "The pearson correlation coefficient between gender and the classifier output also clearly highlights this dependency.\n"}
{"snippet": "D.predict(G.predict(z_test))\n", "intent": "What about unseen data?\n"}
{"snippet": "def print_rmse(model, name, df):\n  metrics = model.evaluate(input_fn = make_input_fn(df, 1))\n  print 'RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss']))\nprint_rmse(model, 'validation', df_valid)\n", "intent": "Evaluate on the validation data (we should defer using the test data to after we have selected a final model).\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_true, y_pred)\n", "intent": "$$\\text{accuracy} = \\frac{TP + TN}{N}$$\n"}
{"snippet": "from scipy.stats import gaussian_kde\ndef make_kde(sample):\n    xs = np.linspace(-4, 4, 101)\n    kde = gaussian_kde(sample)\n    ys = kde.evaluate(xs)\n    return pd.Series(ys, index=xs)\n", "intent": "To see what these overlapping distributions look like, I'll plot a kernel density estimate (KDE).\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\ny_pred_prob = model.predict_proba(X)[:,1]\nauc = roc_auc_score(y, y_pred_prob)\n", "intent": "And compute the AUC.\n"}
{"snippet": "trainer.load('/home/cy/chainer_best_model_converted_to_pytorch_0.7053.pth')\nopt.caffe_pretrain=True \n_bboxes, _labels, _scores = trainer.faster_rcnn.predict(img,visualize=True)\nvis_bbox(at.tonumpy(img[0]),\n         at.tonumpy(_bboxes[0]),\n         at.tonumpy(_labels[0]).reshape(-1),\n         at.tonumpy(_scores[0]).reshape(-1))\n", "intent": "You'll need to download pretrained model from [google dirve](https://drive.google.com/open?id=1cQ27LIn-Rig4-Uayzy_gH5-cW-NRGVzY) \n"}
{"snippet": "def get_accuracy(ypred, ytest):\n    return accuracy_score(y_pred=ypred, y_true=ytest)*100\n", "intent": "Get the performance (accuracy) of your algorithm given ytest\n"}
{"snippet": "bn_model.evaluate(samp_conv_val_feat, samp_val_labels)\n", "intent": "We should find that the new model gives identical results to those provided by the original VGG model.\n"}
{"snippet": "conv_val_feat = vgg640.predict(val, batch_size=32, verbose=1)\nconv_trn_feat = vgg640.predict(trn, batch_size=32, verbose=1)\n", "intent": "We can now pre-compute the output of the convolutional part of VGG.\n"}
{"snippet": "inp = np.expand_dims(conv_val_feat[0], 0)\nnp.round(lrg_model.predict(inp)[0],2)\n", "intent": "We have to add an extra dimension to our input since the CNN expects a 'batch' (even if it's just a batch of one).\n"}
{"snippet": "p = top_model.predict(arr_hr[:20])\n", "intent": "Now we can pass any image through this CNN and it will produce it in the style desired!\n"}
{"snippet": "def eval_keras(input):\n    preds = model.predict(input, batch_size=128)\n    predict = np.argmax(preds, axis = 2)\n    return (np.mean([all(real==p) for real, p in zip(labels_test, predict)]), predict)\n", "intent": "To evaluate, we don't want to know what percentage of letters are correct but what percentage of words are.\n"}
{"snippet": "preds = model.evaluate(X_test, Y_test)\nprint(\"Loss = \" + str(preds[0]))\nprint(\"Test Accuracy = \" + str(preds[1]))\n", "intent": "Let's see how this model (trained on only two epochs) performs on the test set.\n"}
{"snippet": "predictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')\n", "intent": "**Expected Output**:\n<table style=\"width:40%\">\n  <tr>\n    <td>**Cost after iteration 9000**</td>\n    <td> 0.218607 </td> \n  </tr>\n</table>\n"}
{"snippet": "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n", "intent": "Let's try our model again, this time with separate training and validation sets.\n"}
{"snippet": "preds = np.stack([t.predict(X_valid) for t in m.estimators_])\npreds[:,0], np.mean(preds[:,0]), y_valid[0]\n", "intent": "We'll grab the predictions for each individual tree, and look at one example.\n"}
{"snippet": "print(accuracy_score(ytest, gaussian_ypredict))\n", "intent": "Compute the accuracy of the model:\n"}
{"snippet": "preds = predict(net, md.val_dl)\n", "intent": "Now that we have the parameters for our model, we can make predictions on our validation set.\n"}
{"snippet": "preds = predict(net2, md.val_dl).max(1)[1]\nplots(x_imgs[:8], titles=preds[:8])\n", "intent": "Now we can check our predictions:\n"}
{"snippet": "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n", "intent": "Now let's try out our agent to see if it raises any errors.\n"}
{"snippet": "def evaluate(agent, env, n_games=1):\n", "intent": "Let's build a function that measures agent's average reward.\n"}
{"snippet": "Xfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)\n", "intent": "As before, we need to coerce these *x* values into a ``[n_samples, n_features]`` features matrix, after which we can feed it to the model:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "Finally, we can use the ``accuracy_score`` utility to see the fraction of predicted labels that match their true value:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "Now that we have predicted our model, we can gauge its accuracy by comparing the true values of the test set to the predictions:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y, y_model)\n", "intent": "Finally, we compute the fraction of correctly labeled points:\n"}
{"snippet": "rng = np.random.RandomState(0)\nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\nynew = model.predict(Xnew)\n", "intent": "Now let's generate some new data and predict the label:\n"}
{"snippet": "youtput = rforest_model.predict(Xtest)\nprint(accuracy_score(ytest, youtput))\n", "intent": "Compute the accuracy of the model:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(digits.target, labels)\n", "intent": "Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:\n"}
{"snippet": "labels = model.predict(patches_hog)\nlabels.sum()\n", "intent": "Finally, we can take these HOG-featured patches and use our model to evaluate whether each patch contains a face:\n"}
{"snippet": "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\nf1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\n", "intent": "**Warning**: the following cell may take a very long time (possibly hours depending on your hardware).\n"}
{"snippet": "theta_min = np.matrix(result2[0])\npredictions = predict(theta_min, X2)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y2)]\naccuracy = (sum(map(int, correct)) % len(correct))\nprint 'accuracy = {0}%'.format(accuracy)\n", "intent": "Finally, we can use the prediction function from part 1 to see how accurate our solution is on the training data.\n"}
{"snippet": "x = preprocess_input(image_224_batch.copy())\npreds = model.predict(x)\n", "intent": "`image_224_batch` is now compatible with the input shape of the neural network, let's make a prediction.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\ntest_preds = model.predict([user_id_test, item_id_test])\nprint(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\nprint(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))\n", "intent": "Now that the model is trained, the model MSE and MAE look nicer:\n"}
{"snippet": "labels_to_encode = np.array([[3]])\nmodel.predict(labels_to_encode)\n", "intent": "We can use the `predict` method of the Keras embedding model to project a single integer label into the matching embedding vector:\n"}
{"snippet": "labels_to_encode = np.array([[3], [3], [0], [9]])\nmodel.predict(labels_to_encode)\n", "intent": "Let's do the same for a batch of integers:\n"}
{"snippet": "representation = base_model.predict(img_batch)\nprint(\"Shape of representation:\", representation.shape)\n", "intent": "The base model can transform any image into a flat, high dimensional, semantic feature vector:\n"}
{"snippet": "val_scores = np.array(cross_val_score(rforest_model, X, y, cv=10))\n", "intent": "Use `cross_val_score` to perform k-fold cross validation (`k=10`) with this model:\n"}
{"snippet": "def model_perplexity(model, X, y, verbose=1):\n    predictions = model.predict(X, verbose=verbose)\n    return perplexity(y, predictions)\n", "intent": "Let's measure the perplexity of the randomly initialized model:\n"}
{"snippet": "def model_perplexity(model, X, y, verbose=0):\n    predictions = model.predict(X, verbose=verbose)\n    return perplexity(y, predictions)\n", "intent": "Let's measure the perplexity of the randomly initialized model:\n"}
{"snippet": "output_test = model.predict(x_test)\ntest_casses = np.argmax(output_test, axis=-1)\nprint(\"test accuracy:\", np.mean(test_casses == y_test))\n", "intent": "**Exercice**\n - compute model accuracy on test set\n"}
{"snippet": "encoder_a.training = False\nencoder_c.training = False\ndecoder.training = False\nsamples = data.sample(n=100)\nfor (i, row) in samples.iterrows():\n    evaluate((row.french, row.english), encoder_a, encoder_c, decoder, french_vocab, english_vocab)\n", "intent": "To evaluate the learned model, simply execute the following\n"}
{"snippet": "def regr_metrics(act, pred):\n    return (math.sqrt(metrics.mean_squared_error(act, pred)), \n     metrics.mean_absolute_error(act, pred))\n", "intent": "It will be helpful to have some metrics on how good our prediciton is.  We will look at the mean squared norm (L2) and mean absolute error (L1).\n"}
{"snippet": "pred_test = lda_clf.predict(X_test)\nprint('Prediction accuracy for the test dataset')\nprint('{:.2%}'.format(metrics.accuracy_score(y_test, pred_test)))\n", "intent": "To verify that over model was not overfitted to the training dataset, let us evaluate the classifier's accuracy on the test dataset:\n"}
{"snippet": "print(evaluate('Th', 200, temperature=0.2))\n", "intent": "Lower temperatures are less varied, choosing only the more probable outputs:\n"}
{"snippet": "print(evaluate('Th', 200, temperature=1.4))\n", "intent": "Higher temperatures more varied, choosing less probable outputs:\n"}
{"snippet": "def evaluate_randomly():\n    pair = random.choice(pairs)\n    output_words, decoder_attn = evaluate(pair[0])\n    output_sentence = ' '.join(output_words)\n    print('>', pair[0])\n    print('=', pair[1])\n    print('<', output_sentence)\n    print('')\n", "intent": "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:\n"}
{"snippet": "def predict(clf, timeline):\n    X_test = np.array([tweet.text for tweet in timeline])\n    y_pred = clf.predict(X_test)\n    return y_pred\n", "intent": "- Use the RandomForestClassifier to classify each tweet in timeline as a positive tweet or a negative tweet.\n"}
{"snippet": "def get_rmse_log(net, X_train, y_train):\n    num_train = X_train.shape[0]\n    clipped_preds = nd.clip(net(X_train), 1, float('inf'))\n    return np.sqrt(2 * nd.sum(square_loss(\n        nd.log(clipped_preds), nd.log(y_train))).asscalar() / num_train)\n", "intent": "Below defines the root mean square loss between the logarithm of the predicted values and the true values used in the competition.\n"}
{"snippet": "output_test = model.predict(x_test)\ntest_casses = np.argmax(output_test, axis=-1)\nprint(\"Test accuracy:\", np.mean(test_casses == target_test))\n", "intent": "**Exercices**\n- Compute model accuracy on test set\n"}
{"snippet": "gb.predict(ds[pd.isnull(ds.Biscuits)].drop(\"Biscuits\",axis=1))\n", "intent": "Looks like only biscuit can be approximated based on the other data, so let's predict the missing value from original dataset\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)  \nprint(\"Test accuracy:\", scores[1])  \n", "intent": "Once you have trained your model, it's time to see how well it performs on unseen test data.\n"}
{"snippet": "def train_predict(learner, training_data, y):\n    cv = 10\n    learner = learner\n    scores = cross_val_score(learner, training_data, y, cv = cv, scoring = \"accuracy\")\n    print(\"Model's mean accuracy using %s is: %0.2f%s (+/- %0.2f)\" %(learner.__class__.__name__, scores.mean() * 100, \"%\", scores.std() * 2))\n    return (scores.mean())\n", "intent": "* DeciisionTreeClassifer\n* Multinomial Naive Bayes\n* Support Vector Machine\n* AdaBoost\n* RandomForest Classifier\n* Neural Network\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model_run.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "lasso_test_pred = np.expm1(LassoMd.predict(final_test_df.values))\nENet_test_pred = np.expm1(ENetMd.predict(final_test_df.values))\nKRR_test_pred = np.expm1(KRRMd.predict(final_test_df.values))\nGBoost_test_pred = np.expm1(GBoostMd.predict(final_test_df.values))\nfinalMd = (lasso_test_pred + ENet_test_pred + KRR_test_pred + GBoost_test_pred) / 4\nfinalMd\n", "intent": "np.expm1 ( ) is used to calculate exp(x) - 1 for all elements in the array. \n"}
{"snippet": "print('Predicting ri for al=2 through linear equation: ', linreg.intercept_ + linreg.coef_ * 2)\nprint('Predicting ri for al=2 through model: ', linreg.predict(2))\nmodel_coef = zip(feature_cols, linreg.coef_)\nprint('Model Coefficients:')\nfor values in model_coef:\n    print(values)  \n", "intent": "Linear regression equation: $y = \\beta_0 + \\beta_1x$\n"}
{"snippet": "print('Predicting ri for al=3 through linear equation: ', 1.51699012 - 0.0024776063874696235)\nprint('Predicting ri for al=3 through model: ', linreg.predict(3))\n", "intent": "<b>Interpretation:</b> A 1 unit increase in 'al' is associated with a 0.0025 unit decrease in 'ri'.\n"}
{"snippet": "def find_mse(df, degree):\n    est, pred = fit_reg_poly(df, degree, alpha=1.0)\n    np.ravel(pred)\n    result = mean_squared_error(np.array(df['DepDelay']), pred)\n    return result\n", "intent": "Write a function named `find_mse()` that returns the mean squared error of ridge regresion model, given the degree. Assume that alpha is always 1.0.\n"}
{"snippet": "X_test = test[feature_cols]\ny_test = test.price\ny_pred = treereg.predict(X_test)\nprint('Predicted Values: ', y_pred)\nprint('RMSE of the regressor tree model: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "Question: Using the tree diagram above, what predictions will the model make for each observation?\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(knn, X, y, cv=5, scoring='accuracy').mean()\n", "intent": "- Train/test split is **faster and more flexible**\n- Cross-validation provides a **more accurate** estimate of out-of-sample performance\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "While the model may not be as accurate, we have avoided the problem of overfitting. \n"}
{"snippet": "from sklearn.metrics import r2_score\nwr_height_r2 = r2_score(height, wr_height_hat)\nprint 'regression R^2:', wr_height_r2\n", "intent": "---\nRecall that the $R^2$ metric calculates the variance explained by your model over the baseline model.\nThe formula, to refresh your memory, is:\n"}
{"snippet": "wr_height_no_r2 = r2_score(height, wr_height_hat_no)\nprint 'regression R^2:', wr_height_r2\nprint 'regression outliers removed R^2:', wr_height_no_r2\n", "intent": "---\nWhich performs better? Why do you think that is?\n"}
{"snippet": "enet = ElasticNet(alpha=optimal_enet.alpha_, l1_ratio=optimal_enet.l1_ratio_)\nenet_scores = cross_val_score(enet, Xn, y, cv=10)\nprint enet_scores\nprint np.mean(enet_scores)\n", "intent": "---\nHow does it compare to the other regularized regressions?\n"}
{"snippet": "def calc_confusion_matrix(mod, X, y, margins=False):\n    pred = pd.Series(mod.predict(X), name='Predicted')\n    true = pd.Series(y, name='True')\n    confusion = pd.crosstab(true, pred, margins=margins)\n    return confusion\n", "intent": "---\nWhat do they tell you about the models?\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nmean_squared_error(y_true, y_pred)\n", "intent": "$$ MSE = \\frac{1}{n}\\sum (Y_{i}-\\hat{Y_{i}})^{2} $$\n"}
{"snippet": "score = model.evaluate(x_test1, y_test1, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "def predict(clf, timeline):\n    messages = []\n    for tweet in timeline:\n        messages.append(tweet.text)\n    new_tweets = np.array(messages)\n    y_pred = clf.predict(new_tweets)\n    return y_pred\n", "intent": "- Use the RandomForestClassifier to classify each tweet in timeline as a positive tweet or a negative tweet.\n"}
{"snippet": "SVC_score_on_training = metrics.accuracy_score(y_2_train, SVC_prediction_on_training)\nSVC_precision_on_training = metrics.precision_score(y_2_train, SVC_prediction_on_training, pos_label=\"REAL\")\nSVC_recall_on_training = metrics.recall_score(y_2_train, SVC_prediction_on_training, pos_label=\"REAL\")\nSVC_f1_on_training = metrics.f1_score(y_2_train, SVC_prediction_on_training, pos_label=\"REAL\")\nprint(\"accuracy for SVC on training dataset2:   %0.3f\" % SVC_score_on_training)\nprint(\"precision for SVC on training dataset2:   %0.3f\" % SVC_precision_on_training)\nprint(\"recall for SVC on training dataset2:   %0.3f\" % SVC_recall_on_training)\nprint(\"f1 for SVC on training dataset2:   %0.3f\" % SVC_f1_on_training)\n", "intent": "**Prediction of Labels(Fake/Real) on TRAIN - VALIDATION - TEST data**\n"}
{"snippet": "X_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore, accuracy = nn.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (accuracy))\n", "intent": "**Testing the model 1**\n"}
{"snippet": "pos_cnt, neg_cnt, pos_ok, neg_ok = 0, 0, 0, 0\nfor x in range(len(X_validate)):\n    result = nn.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n    if np.argmax(result) == np.argmax(Y_validate[x]):\n        if np.argmax(Y_validate[x]) == 0: neg_ok += 1\n        else: pos_ok += 1\n    if np.argmax(Y_validate[x]) == 0: neg_cnt += 1\n    else: pos_cnt += 1\nprint(\"pos_acc\", pos_ok/pos_cnt*100, \"%\")\nprint(\"neg_acc\", neg_ok/neg_cnt*100, \"%\")\n", "intent": "**Printing results for model 1**\n"}
{"snippet": "X_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore, accuracy = nn.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (accuracy))\n", "intent": "**Testing the model 2**\n"}
{"snippet": "pos_cnt, neg_cnt, pos_ok, neg_ok = 0, 0, 0, 0\nfor x in range(len(X_validate)):\n    result = nn.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n    if np.argmax(result) == np.argmax(Y_validate[x]):\n        if np.argmax(Y_validate[x]) == 0: neg_ok += 1\n        else: pos_ok += 1\n    if np.argmax(Y_validate[x]) == 0: neg_cnt += 1\n    else: pos_cnt += 1\nprint(\"pos_acc\", pos_ok/pos_cnt*100, \"%\")\nprint(\"neg_acc\", neg_ok/neg_cnt*100, \"%\")\n", "intent": "**Printing results for model 2**\n"}
{"snippet": "X_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore, accuracy = nn.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (accuracy))\n", "intent": "**Testing the model 3**\n"}
{"snippet": "pos_cnt, neg_cnt, pos_ok, neg_ok = 0, 0, 0, 0\nfor x in range(len(X_validate)):\n    result = nn.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n    if np.argmax(result) == np.argmax(Y_validate[x]):\n        if np.argmax(Y_validate[x]) == 0: neg_ok += 1\n        else: pos_ok += 1\n    if np.argmax(Y_validate[x]) == 0: neg_cnt += 1\n    else: pos_cnt += 1\nprint(\"pos_acc\", pos_ok/pos_cnt*100, \"%\")\nprint(\"neg_acc\", neg_ok/neg_cnt*100, \"%\")\n", "intent": "**Printing results for model 3**\n"}
{"snippet": "X, Y, categories = dataset_2()\ncount = count_V(X, Y)\npred = model_b(count, Y)\nconf.append(precision_recall_fscore_support(Y[0], pred[0], average = \"weighted\")) \nacc.append(metrics.accuracy_score(Y[0], pred[0], normalize=True, sample_weight=None))\nconf.append(precision_recall_fscore_support(Y[1], pred[1], average = \"weighted\")) \nacc.append(metrics.accuracy_score(Y[1], pred[1], normalize=True, sample_weight=None))\nconf.append(precision_recall_fscore_support(Y[2], pred[2], average = \"weighted\")) \nacc.append(metrics.accuracy_score(Y[2], pred[2], normalize=True, sample_weight=None))\n", "intent": "model b + dataset 2 + train + validation + test\n"}
{"snippet": "X, Y, categories = dataset_2()\ntfidf = tfidf_V(X, Y)\npred = model_a(tfidf, Y)\nconf.append(precision_recall_fscore_support(Y[1], pred[1], average = \"weighted\")) \nacc.append(metrics.accuracy_score(Y[1], pred[1], normalize=True, sample_weight=None))\nX, Y, categories = dataset_1()\ncount = count_V(X, Y)\npred = model_b(count, Y)\nconf.append(precision_recall_fscore_support(Y[1], pred[1], average = \"weighted\")) \nacc.append(metrics.accuracy_score(Y[1], pred[1], normalize=True, sample_weight=None))\n", "intent": "model a + dataset 2 + test \n<br>\nmodel b + dataset 1 + test\n"}
{"snippet": "expected = y_test\npredicted = model.predict(X_test)\naccuracy = accuracy_score(expected, predicted)\nprint( \"Accuracy = \" + str( accuracy ) )\n", "intent": "- Back to [Table of Contents](\nNow let's use the model we just fit to make predictions on our test dataset, and see what our accuracy score is:\n"}
{"snippet": "pretrained_tagger = nltk.data.load('taggers/maxent_treebank_pos_tagger/english.pickle')\npenn_taggs = nltk.corpus.treebank.tagged_sents()\nbrwn_taggs = nltk.corpus.brown.tagged_sents()\npenn_pt_score = pretrained_tagger.evaluate(penn_taggs[:1000])\nscores_list.append((\"Task 1.2\", penn_pt_score))\nbrwn_pt_score = pretrained_tagger.evaluate(brwn_taggs[:1000])\nscores_list.append((\"Task 1.5\", brwn_pt_score))\n", "intent": "    MaxEntropy Classifier: An NLKT pre-trained POS tagger\n"}
{"snippet": "print(linreg.predict([0,0])) \nprint(linreg.predict([1,0])) \nprint(linreg.predict([0,1])) \n", "intent": "** Now let's use predict function to predict mpg for light, heavy and medium cars **\n"}
{"snippet": "RSS = sum((y-linreg.predict(X))**2) \nTSS = sum((y-y.mean())**2)\nR_Squared = 1 - float(RSS)/TSS\nprint(R_Squared)\n", "intent": "This is the definition of R_Squared. Let's first calculate it = this is the hard way!\n"}
{"snippet": "metrics.accuracy_score(ypred, ytest)\n", "intent": "We can check the accuracy of this classifier:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(digits.target, labels)\n", "intent": "Just for kicks, let's see how accurate our K-Means classifier is **with no label information:**\n"}
{"snippet": "model_labels16 = np.zeros_like(clusters16)\nfor i in range(10):\n    mask = (clusters16 == i)\n    model_labels16[mask] = mode(digits.target[mask])[0]    \naccuracy_score(digits.target, model_labels16)   \n", "intent": "And let's check the score:\n"}
{"snippet": "D_loss, G_loss = lsgan_loss(logits_real, logits_fake)\nD_train_step = D_solver.minimize(D_loss, var_list=D_vars)\nG_train_step = G_solver.minimize(G_loss, var_list=G_vars)\n", "intent": "Create new training steps using the LSGAN loss:\n"}
{"snippet": "print \"Accuracy is \", accuracy_score(y_test,y_pred)*100\n", "intent": "**Is there an improvement in the Accuracy of your modified model? **\n"}
{"snippet": "logit_1.predict([[3], [4]])\n", "intent": "- Prediction can be made with the `predict` method:\n"}
{"snippet": "def report_results(model, X, y):\n    pred = model.predict(X)        \n    acc = accuracy_score(y, pred)\n    f1 = f1_score(y, pred)\n    prec = precision_score(y, pred)\n    rec = recall_score(y, pred)\n    result = {'f1': f1, 'acc': acc, 'precision': prec, 'recall': rec}\n    return result\n", "intent": "best params:\n{'svc__C': 0.03125, 'svc__degree': 1.0, 'svc__gamma': 0.03125, 'svc__kernel': 'linear'}\nbest cv score:\n0.87025\n"}
{"snippet": "LDA.predict(x_tm)\n", "intent": "- Predict with the LDA model:\n"}
{"snippet": "x_new = np.array([2.8, 5.6, 3.2, 6.7]).reshape(1,-1)\nprint gnb.predict(x_new)\nprint gnb.predict_proba(x_new)\n", "intent": "- Use `predict()` and `predict_proba()` methods to predict the new data point `x_new = [2.8, 5.6, 3.2, 6.7]`.\n"}
{"snippet": "random_divide = cv.KFold(150, 5, random_state=1)\nscores = cv.cross_val_score(logit, iris.data, iris.target, \n                            cv=random_divide)\nscores\n", "intent": "Here is a case that we randomly divide the data set by **KFold**:\n"}
{"snippet": "stratify_divide = cv.StratifiedKFold(iris.target, 5, \n                                     random_state=0)\nscores = cv.cross_val_score(logit, iris.data, \n                            iris.target, \n                            cv=stratify_divide)\nscores\n", "intent": "We can also use **StratifiedKFold** to split the dataset more evenly among different classes\n"}
{"snippet": "scores = cv.cross_val_score(model, X, y ,cv=5)\n1 - scores\n", "intent": "- Use the function **cross_val_score** to implement 5-fold cross validation with a **Linear Discriminant Analysis** model. Report the test errors.\n"}
{"snippet": "svm_model.predict(svm_model.support_vectors_)\n", "intent": "- Which classes do they belong to?\n"}
{"snippet": "test = data.loc[np.random.choice(data.index,size = 100000,replace=False)]\nypred = predict_tip(test)\nprint \"final mean_squared_error:\", metrics.mean_squared_error(ypred,test.Tip_percentage)\nprint \"final r2_score:\", metrics.r2_score(ypred,test.Tip_percentage)\n", "intent": "Make predictions on a sample of 100000 transactions\n"}
{"snippet": "forest_predictions = rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test, forest_predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "accuracy_score(predictions, y_test)\n", "intent": "Let's see how accurate the model is.\n"}
{"snippet": "predictions = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "from sklearn.metrics import recall_score\nprint 'Recall = ', recall_score(t_test, y_test, average=None)\n", "intent": "The \"recall\" score measures the percentage of correctly classified cases for each class, and can be used to diagnose this kind of cases.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint classification_report(t_test, y_test)\n", "intent": "We can also request a summary table with `classification_report`.\n"}
{"snippet": "from sklearn import metrics\nprint metrics.adjusted_rand_score(Y,Y_est)\n", "intent": "Estimate the error with sklearn.metrics.adjusted_rand_score\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_pred_train = model.predict(X_train)\nmean_squared_error(y_train, y_pred_train)\n", "intent": "That was easy. What's the training MSE?\n"}
{"snippet": "y_pred_test = model.predict(X_test)\n", "intent": "First, make predictions.\n"}
{"snippet": "mean_squared_error(y_test, y_pred_test)\n", "intent": "Now see how good they are.\n"}
{"snippet": "pred = p.predict(X_test)\nsklearn.metrics.accuracy_score(y_test, pred)\n", "intent": "Then we can evaluate the behavior of the classifier on the held-out set\n"}
{"snippet": "prepared_data = prepare_data(data)\nprint(\"This model predicted your input as\", first_model.predict(prepared_data).argmax())\n", "intent": "(tip: don't expect it to work)\n"}
{"snippet": "accuracy_score(y_test, prediction)\n", "intent": "Let us compute the accuracy of out classifier model.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print (metric)\n    skf = StratifiedKFold(n_splits, True, skf_seed) \n    NNF = NearestNeighborsFeats(n_jobs=-1, k_list=k_list, metric=metric)\n    preds = cross_val_predict(NNF, X, Y, cv = skf)\n    np.save('data/knn_feats_%s_train.npy' % metric, preds)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "predictions = []\nfor tree in rf.estimators_:\n    predictions.append(tree.predict_proba(X_val)[None, :])\n", "intent": "**Step 2:** Get predictions for each tree in Random Forest separately.\n"}
{"snippet": "scores = []\nfor pred in cum_mean:\n    scores.append(accuracy_score(y_val, np.argmax(pred, axis=1)))\n", "intent": "**Step 5:** Get accuracy scores for each `n_estimators` value\n"}
{"snippet": "y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\ny_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\ny_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\ny_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)\n", "intent": "Now you can create predictions for the data. You will need two types of predictions: labels and scores.\n"}
{"snippet": "MAX_TO_PRINT = 5\npredictions = estimator.predict(input_fn=test_input)\ni = 0\nfor p in predictions:\n    true_label = y_test[i]\n    predicted_label = p['class_ids'][0]\n    print(\"Example %d. True: %d, Predicted: %d\" % (i, true_label, predicted_label))\n    i += 1\n    if i == MAX_TO_PRINT: break\n", "intent": "Here's how you would print individual predictions.\n"}
{"snippet": "test_input_fn = create_test_input_fn()\npredictions = estimator.predict(test_input_fn)\ni = 0\nfor prediction in predictions:\n    true_label = census_test_label[i]\n    predicted_label = prediction['class_ids'][0]\n    print(\"Example %d. Actual: %d, Predicted: %d\" % (i, true_label, predicted_label))\n    i += 1\n    if i == 5: break\n", "intent": "The Estimator returns a generator object. This bit of code demonstrates how to retrieve predictions for individual examples.\n"}
{"snippet": "result = model.evaluate(input_fn=test_input_fn)\n", "intent": "Once the model has been trained, we can evaluate its performance on the test-set.\n"}
{"snippet": "predictions = model.predict(input_fn=predict_input_fn)\n", "intent": "The model can also be used to make predictions on new data.\n"}
{"snippet": "predictions = model.predict(input_fn=test_input_fn)\n", "intent": "To get the predicted classes for the entire test-set, we just use its input-function:\n"}
{"snippet": "print(classification_report(test1['target'], pred1, target_names=test1['target_names']))\n", "intent": "Let us take a look at how well our model performs:\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def print_rmse(model, name, input_fn):\n  metrics = model.evaluate(input_fn=input_fn, steps=1)\n  print 'RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['loss']))\nprint_rmse(model, 'validation', make_input_fn(df_valid))\n", "intent": "Evaluate on the validation data (we should defer using the test data to after we have selected a final model).\n"}
{"snippet": "tree_predictions = tree.predict(X_test)\n", "intent": "Make a prediction with the trained model on the test data.\n"}
{"snippet": "forest_predictions = rf.predict(X_test)\n", "intent": "Make predictions for the test data.\n"}
{"snippet": "tuned_forest_predictions = locally_best_forest.predict(X_test)\naccuracy_score(y_test, tuned_forest_predictions)\n", "intent": "Make predictions for the test data.\n"}
{"snippet": "print(\"Normalized Accuracy Score for your model is {:.3f}\".format(accuracy_score(y_true, y_pred)))\nprint(\"Correctly classified samples {:d} out of {} instances\".format(accuracy_score(y_true, y_pred, normalize=False),\n                                                          len(Y_test)))\n", "intent": "___\n+ If {normalize == True}, return the correctly classified samples (float), else it returns the number of correctly classified samples (int).\n"}
{"snippet": "print(classification_report(y_pca_test, y_pred_dt_smote_pca))\n", "intent": "** Classification Report **\n___\n"}
{"snippet": "print(classification_report(y_pca_test, y_pred_rf_smote_pca))\n", "intent": "** Classification Report **\n___\n"}
{"snippet": "cls_p_score_rf_smote_pca = precision_score(y_pca_test, y_pred_rf_smote_pca, pos_label=\"CERTIFIED\")\nprint(\"Precision score is {:.2f}\".format(cls_p_score_rf_smote_pca))\ncls_r_score_rf_smote_pca = recall_score(y_pca_test, y_pred_rf_smote_pca, pos_label=\"CERTIFIED\")\nprint(\"Recall Score is {:.2f}\".format(cls_r_score_rf_smote_pca ))\n", "intent": "** Precision Score **\n___\n"}
{"snippet": "import torchbearer\nloss_function = nn.CrossEntropyLoss()\noptimiser = optim.Adam(model.parameters())\ntrial = torchbearer.Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'])\ntrial.with_generators(trainloader, test_generator=testloader)\ntrial.run(epochs=10)\nresults = trial.evaluate(data_key=torchbearer.TEST_DATA)\nprint(results)\n", "intent": "We can use the torchbearer `Trial` class to do all the hard work in training and evaluating for us:\n"}
{"snippet": "cls_p_score_et_smote_pca = precision_score(y_pca_test, y_pred_et_smote_pca, pos_label=\"CERTIFIED\")\nprint(\"Precision score is {:.2f}\".format(cls_p_score_et_smote_pca))\ncls_r_score_et_smote_pca = recall_score(y_pca_test, y_pred_et_smote_pca, pos_label=\"CERTIFIED\")\nprint(\"Recall Score is {:.2f}\".format(cls_r_score_et_smote_pca ))\n", "intent": "** Precision and Recall Score **\n___\n"}
{"snippet": "classifier_p_score_w = precision_score(y_true, y_pred, average='weighted')\nprint(\"Precision Value averaged by 'weighted' is {:.2f}\".format(classifier_p_score_w))\nclassifier_p_score_mi = precision_score(y_true, y_pred, average='micro')\nprint(\"Precision Value averaged by 'micro' is {:.2f}\".format(classifier_p_score_mi))\n", "intent": "+ The precision Value returned is the weighted average of the precision of each class for the multiclass task.\n"}
{"snippet": "img = prepare_image('imgs/dog2.jpg')\nout = model_vgg16_conv.predict(img)\ny_pred = np.argmax(out)\nprint (y_pred)\nprint (synset.loc[y_pred].synset)\n", "intent": "<img src='imgs/dog2.jpg'/>\n"}
{"snippet": "img = prepare_image('imgs/sloth.jpg')\nout = model_vgg16_conv.predict(img)\ny_pred = np.argmax(out)\nprint (y_pred)\nprint (synset.loc[y_pred].synset)\n", "intent": "<img src='imgs/sloth.jpg'/>\n"}
{"snippet": "knn.predict([[3, 5, 4, 2]])\n", "intent": "In order to **make a prediction**, the new observation must have the **same features as the training observations**, both in number and meaning.\n"}
{"snippet": "dout = np.random.randn(batch_size, pool_size, pool_size, input_depth)\ndx_num = eval_numerical_gradient_array(lambda x: max_pool_forward(x, pool_size, stride), X, dout)\nout = max_pool_forward(X, pool_size, stride)\ndx = max_pool_backward(dout, X, out, pool_size, stride)\nprint('Testing max_pool_backward function:')\ndiff = relative_error(dx, dx_num)\nif diff < 1e-12:\n    print 'PASSED'\nelse:\n    print 'The diff of %s is too large, try again!' % diff\n", "intent": "And we again use numerical gradient checking to ensure that the backward function is correct: \n"}
{"snippet": "predictions = model.predict(X) \n", "intent": "Make the predictions by the model:\n"}
{"snippet": "predictions = model.predict(X)\n", "intent": "Make the predictions by the model:\n"}
{"snippet": "predicted = model2.predict(X_test)\nprint (predicted)\n", "intent": "We now need to predict class labels for the test set. We will also generate the class probabilities, just to take a look.\n"}
{"snippet": "assert(theta.shape == (3,1))\nprint(\"Theta: \", theta.t())\nprint(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta, y_test))\n", "intent": "We can now print out the error achieved on the test set, as well as the parameter vector:\n"}
{"snippet": "predicted= cross_val_predict(clf,X,y,cv=10)\nmetrics.accuracy_score(y,predicted)\n", "intent": "Cross validation Predict: generate cross-validated estimates for each input data point\n"}
{"snippet": "from sklearn.metrics import adjusted_rand_score\nadjusted_rand_score(y, labels)\n", "intent": "sklearn also has a scoring for this: adjusted rand score: \n"}
{"snippet": "scores = cross_val_score(classifier, X, y)\nprint(scores)\nprint(np.mean(scores))\n", "intent": "Try it! How many folds did it use? You can control that. Try reproducing your own result.\n"}
{"snippet": "mols = list(dff.MurMol)\nsmarts,smi,img,mcsM = MCS_Report(mols,threshold=0.8,ringMatchesRingOnly=True)\nimg\n", "intent": "And now we'll extract the MCS core:\n"}
{"snippet": "model_1_predict = model_1.predict(test[feature_columns])\n", "intent": "**Let's use the model for prediction**\n"}
{"snippet": "y_pred = text_clf.predict(X_test)\nprint('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))\nprint(classification_report(y_test, y_pred))\n", "intent": "Now that the model is trained, let's evaluate it on the test set:\n"}
{"snippet": "test1 = [2.3, 2.5];\nsvc.predict([test1])\n", "intent": "Now test some points:\n"}
{"snippet": "gnb.predict([[15,30]])\n", "intent": "Predict some new points\n"}
{"snippet": "dtree_score = cross_val_score(dtree, features, labels, n_jobs=-1).mean()\nprint(\"{0} -> DTree: {1})\".format(columns, dtree_score))\n", "intent": "Estimate the accuracy of our DecisionTreeClassifier\n"}
{"snippet": "alpha = 0.00001\ntheta_gd = torch.rand((X_train.shape[1], 1))\nfor e in range(0, 10000):\n    gr = linear_regression_loss_grad(theta_gd, X_train, y_train)\n    theta_gd -= alpha * gr\nprint(\"Gradient Descent Theta: \", theta_gd.t())\nprint(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta_gd, y_test))\n", "intent": "Now let's try using gradient descent:\n"}
{"snippet": "predictions = dtree.predict(test_data[columns].values)\n", "intent": "Try to predict the data using our Decision Tree:\n"}
{"snippet": "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\nY_test = svr.predict(X_test)\npl.plot(X_test,Y_test,c=\"g\", label=\"SVR\")\npl.scatter(X, y, c=\"k\", label=\"data\")\npl.legend()\npl.show()\n", "intent": "Plot the points and the SVR result\n"}
{"snippet": "dmtest = xgb.DMatrix(np.asarray(test)) \npred_prob2 = bst.predict(dmtest)\n", "intent": "Accuracy score : 73.53%\n"}
{"snippet": "cv.cross_val_score(grid.best_estimator_, X, y)\n", "intent": "Here is the performance of the best estimator.\n"}
{"snippet": "learn.sched.plot_loss()\n", "intent": "We can see the loss-spikes at the start of each SGDR cycle:\n"}
{"snippet": "model.predict([np.array([3]), np.array([6])])\n", "intent": "e can use the model to generate predictions by passing a pair of ints - a user id and a movie id. For instance, this predicts that user \n"}
{"snippet": "def get_next(inp):\n    idxs = [np.array(char_indices[c])[np.newaxis] for c in inp]\n    p = model.predict(idxs)\n    return chars[np.argmax(p)]\n", "intent": "With 8 pieces of context instead of 3, we'd expect it to do better; and we see a loss of ~1.8 instead of ~2.0\n"}
{"snippet": "def get_nexts(inp):\n    idxs = [char_indices[c] for c in inp]\n    arrs = [np.array(i)[np.newaxis] for i in idxs]\n    p = model.predict([np.zeros(n_fac)[np.newaxis,:]] + arrs)\n    print(list(inp))\n    return [chars[np.argmax(o)] for o in p]\n", "intent": "This is what a sequence model looks like. We pass in a sequence and after every character, it returns a guess.\n"}
{"snippet": "model.evaluate(conv_val_feat, [val_bbox, val_labels])\n", "intent": "*NOTE: a powerful model will crop out these fish and run them through a second model*\n"}
{"snippet": "def mean_absolute_error(y, y_hat): \n    raise NotImplementedError()\n    raise NotImplementedError()\n    raise NotImplementedError()\n    return mae\n", "intent": "$$MAE = \\frac{1}{N} \\sum_{n=1}^N \\left| y_n - \\hat{y}_n \\right|$$\n"}
{"snippet": "preds = model.predict([conv_test_feat, test_sizes], batch_size=batch_size*2)\n", "intent": "---\nAll following code not run:\n"}
{"snippet": "preds = model.predict_classes(val_data, batch_size=batch_size)\nprobs = model.predict_proba(val_data, batch_size=batch_size)[:,0]\n", "intent": "We can look at the earlier prediction examples visualizations by redefiing *probs* and *preds* and re-using our earlier code.\n"}
{"snippet": "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid), \n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n", "intent": "Let's try our model again, this time with separate training and validation sets.\n"}
{"snippet": "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid), \n           m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n", "intent": "Now we'll run our model again, but with the separate training and validation sets:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(model, X, y, cv=14, scoring='accuracy')\nprint(scores)\n", "intent": "**Now we are going to implement a K-fold cross validation test to get a more generalized accuracy score.**\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nfrom scipy.stats import sem\nscores = cross_val_score(pipeline, twenty_train_small.data,\n                         twenty_train_small.target, cv=3, n_jobs=-1)\nscores.mean(), sem(scores)\n", "intent": "Such a pipeline can then be cross validated or even grid searched:\n"}
{"snippet": "X_new = vectorizer.transform(['This movie is not remarkable, touching, or superb in any way'])\nclf.predict(X_new)\n", "intent": "Yes. Since this review contains mostly positive words and one negative, I would assume that this type of review will be classified incorrectly.\n"}
{"snippet": "labels =list(crf.classes_)\ny_pred = crf.predict(X_valid)\nmetrics.flat_f1_score(y_valid, y_pred,\n                      average='weighted', labels=labels)\n", "intent": "Let's now use the trained model to make predictions. \n"}
{"snippet": "y_pred_test = crf.predict(X_test)\nmetrics.flat_f1_score(y_test, y_pred_test,\n                      average='weighted', labels=labels)\n", "intent": "Let's also make predictions on the test set and see the results. \n"}
{"snippet": "import math\nassert math.isclose(0.33316349496726444, \n                    mean_absolute_error(pd.Series(np.random.RandomState(10).rand(10)), \n                                        pd.Series(np.random.RandomState(20).rand(10))))\nassert math.isclose(0.39070816989559587, \n                    mean_absolute_error(pd.Series(np.random.RandomState(30).rand(10)), \n                                        pd.Series(np.random.RandomState(20).rand(10))))\nassert math.isclose(0.2567117528634928, \n                    mean_absolute_error(pd.Series(np.random.RandomState(30).rand(10)), \n                                        pd.Series(np.random.RandomState(500).rand(10))))\n", "intent": "Expected output:\n```\nMAE: 3.2729446379969387\n```\n"}
{"snippet": "import numpy as np\nfrom scipy.optimize import check_grad\nfrom gradient_check import eval_numerical_gradient_array\ndef rel_error(x, y):\n      return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n", "intent": "<h2 align=\"center\">BackProp and Optimizers</h2>\n"}
{"snippet": " def softmax_loss(x, y):\n    loss = 0.0\n    dx = np.zeros_like(x)\n    for i in range(x.shape[0]):\n        loss += (np.log(np.exp(x[i]).sum()) - x[i, y[i]]) / x.shape[0]\n        dx[i] = 1.0 / (np.exp(x[i]).sum()) * np.exp(x[i])\n        dx[i, y[i]] -= 1\n    dx /= x.shape[0]\n    return loss, dx\n", "intent": "<h3>Softmax Loss Layer</h3>\n<img src=\"img/loss.png\" width=\"300\">\n<img src=\"img/log.png\" width=\"600\">\n"}
{"snippet": "pred = X_test.dot(w_optimal)\nroc_auc_score(y_test, pred)\n", "intent": "Now predict output of logistic regression for the test sample and compute AUC\n"}
{"snippet": "test_pred = clf.predict_proba(test_at_all)\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "metrics.f1_score(y_train, clf.predict(X_train), average=\"macro\")\n", "intent": "The over-fitting we saw previously can be quantified by computing the\nf1-score on the training data itself:\n"}
{"snippet": "from sklearn.model_selection import ShuffleSplit\ncv = ShuffleSplit(n_splits=5)\ncross_val_score(clf, X, y, cv=cv)\n", "intent": "We can use different splitting strategies, such as random splitting\n"}
{"snippet": "def test_func(x, err=0.5):\n    return np.random.normal(10 - 1. / (x + 0.1), err)\ndef compute_error(x, y, p):\n    yfit = np.polyval(p, x)\n    return np.sqrt(np.mean((y - yfit) ** 2))\n", "intent": "We'll create a dataset like in the example above, and use this to test our\nvalidation scheme.  First we'll define some utility routines:\n"}
{"snippet": "from sklearn.metrics import roc_curve, auc\nYpr = clf.predict_proba(X)\nfpr, tpr, thresholds = roc_curve(Y, Ypr[:,1]) \n", "intent": "[Courbe ROC](http://www.xavierdupre.fr/app/mlstatpy/helpsphinx/c_metric/roc.html)\n"}
{"snippet": "pmodel1 = rf.predict_proba(np_test)[:, 1]\npmodel2 = rfv.predict_proba(np_test_v)[:, 1]\n", "intent": "Avec une courbe [ROC](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) par exemple.\n"}
{"snippet": "def mean_squared_error(y, y_hat):\n    raise NotImplementedError()\n    raise NotImplementedError()\n    raise NotImplementedError()\n    return mse\n", "intent": "$$MSE = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2$$\n"}
{"snippet": "start = time.time()\ntest_data['label_pick'] = k_means.predict(test_data[['pickup_longitude','pickup_latitude']])\ntest_data['label_drop'] = k_means.predict(test_data[['dropoff_longitude','dropoff_latitude']])\ntest_cl = pd.merge(test_data, centroid_pickups, how='left', on=['label_pick'])\ntest_cl = pd.merge(test_cl, centroid_dropoff, how='left', on=['label_drop'])\nend = time.time()\nprint(\"Time Taken by above cell is {}.\".format(end-start))\n", "intent": "4.. Extracting cluster features - \n"}
{"snippet": "test_pred = rf_baseline.predict(X_test_real)\nsimulated_pred_rf = rf_baseline.predict(X_test_simulated)\nsimulated_pred_sample_mean = np.mean(y_train)\n", "intent": "See if the simulated data matter\n"}
{"snippet": "X_test_counts = vect.transform(X_test)\nX_test_tfidf = tfidf.transform(X_test_counts)\ny_pred = y_pred = clf.predict(X_test_tfidf)\n", "intent": "* Transform (no fitting) the test data with the same CountVectorizer and TfidfTransformer\n* Predict labels on these tfidf values.\n"}
{"snippet": "def precision(actual, preds):\n    return np.sum(actual * preds == 1) / np.sum(preds == 1)\nprint(precision(y_test, preds_nb))\nprint(precision_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def recall(actual, preds):\n    return np.sum(actual * preds == 1) / np.sum(actual == 1)\nprint(recall(y_test, preds_nb))\nprint(recall_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def f1(preds, actual):\n    pre = precision(preds, actual)\n    re = recall(preds, actual)\n    return 2 * (pre * re) / (pre + re)\nprint(f1(y_test, preds_nb))\nprint(f1_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "preds_tree = tree_mod.predict(X_test) \npreds_rf = rf_mod.predict(X_test)\npreds_ada = ada_mod.predict(X_test)\npreds_reg = reg_mod.predict(X_test)\n", "intent": "> **Step 5:** Use each of your models to predict on the test data.\n"}
{"snippet": "def mse(actual, preds):\n    return np.average((actual - preds) ** 2) \nprint(mse(y_test, preds_tree))\nprint(mean_squared_error(y_test, preds_tree))\nprint(\"If the above match, you are all set!\")\n", "intent": "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. \n"}
{"snippet": "y_pre1 = model1.predict(testing_data)\ny_pre2 = model2.predict(testing_data)\ny_pre3 = model3.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "import math\nassert math.isclose(0.16469788257519086, \n                    mean_squared_error(pd.Series(np.random.RandomState(10).rand(10)), \n                                       pd.Series(np.random.RandomState(20).rand(10))))\nassert math.isclose(0.22325626250313846, \n                    mean_squared_error(pd.Series(np.random.RandomState(30).rand(10)), \n                                       pd.Series(np.random.RandomState(20).rand(10))))\nassert math.isclose(0.13478449093337383, \n                    mean_squared_error(pd.Series(np.random.RandomState(30).rand(10)), \n                                       pd.Series(np.random.RandomState(500).rand(10))))\n", "intent": "Expected output: \n```\nMSE: 21.8977792176875\n```\n"}
{"snippet": "test_x = test_data[:, 1:]\ntest_y = clf.predict(test_x)\ntest_y[0:100]\n", "intent": "Take the decision trees and run it on the test data:\n"}
{"snippet": "report = classification_report(\n    test.Play,\n    test_output,\n    target_names=[\"No Play\", \"Play\"]\n)\nprint(report)\n", "intent": "** Classifier Performance **\n"}
{"snippet": "examples = ['free viagra', \"Hi Bob, how about a game of golf tomorrow?\", \"Come to Sam GU's free lunch!\"]\nexample_counts = vectorizer.transform(examples)\npredictions = classifier.predict(example_counts)\npredictions\n", "intent": "Let's try out couple examples.\n"}
{"snippet": "print ('predicted:', spam_detect_model.predict(tfidf4)[0])\nprint ('expected:', messages.label[3])\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "report = classification_report(\n    test['Species'],\n    test_output,\n    target_names=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n)\nprint(report)\n", "intent": "** Classifier Performance **\n"}
{"snippet": "knn.predict([[5.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0]])\n", "intent": "Let's make a prediction according to the column:\n"}
{"snippet": "x = km.fit_predict(df1)\nx\n", "intent": "Now, export the cluster identifiers to a list. Notice my values are 0 - 3. One value for each cluster.\n"}
{"snippet": "predicted = model2.predict(X_test)\npredicted\n", "intent": "We now need to predict class labels for the test set. We will also generate the class probabilities, just to take a look.\n"}
{"snippet": "lm2.predict(50)\n", "intent": "**C. Scikit-learn Prediction**\n"}
{"snippet": "def root_mean_squared_error(y, y_hat): \n    raise NotImplementedError()\n    raise NotImplementedError()\n    return rmse\n", "intent": "$$RMSE = \\sqrt{MSE}$$\n"}
{"snippet": "adam = np.array([1, 0, 29, 0]).reshape(1,-1) \nlogreg.predict_proba(adam)[:, 1]\n", "intent": "Predict probability of survival for **Adam**: first class, no parents or kids, 29 years old, male.\n"}
{"snippet": "bill = np.array([2, 0, 29, 0]).reshape(1,-1)\nlogreg.predict_proba(bill)[:, 1]\n", "intent": "Predict probability of survival for **Bill**: same as Adam, except second class.\n"}
{"snippet": "susan = pd.Series([1, 0, 29, 1]).reshape(1,-1)\nlogreg.predict_proba(susan)[:, 1]\n", "intent": "Predict probability of survival for **Susan**: same as Adam, except female.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"RMSE:\", mean_squared_error(ys, predictions)\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "linreg.predict(3)\n", "intent": "**Interpretation:** A 1 unit increase in 'al' is associated with a 0.0025 unit decrease in 'ri'.\n"}
{"snippet": "print metrics.classification_report(y, preds)\n", "intent": "**Exercise** Calculate:\nAccuracy\nSensitivity\nSpecificity\nPrecision by hand\n<br><br><br><br><br><br><br><br><br><br><br><br><br>\n"}
{"snippet": "rfc_pred = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nprint (confusion_matrix(y_test,predictions))\nprint (classification_report(y_test,predictions))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "def predict(i):\n    image = mnist.test.images[i]\n    actual_label = np.argmax(mnist.test.labels[i])\n    prediction = tf.argmax(y,1)\n    predicted_label = sess.run(prediction, feed_dict={images: [image]})\n    print (\"Predicted: %d, actual: %d\" % (predicted_label, actual_label))\n    pylab.imshow(mnist.test.images[i].reshape((28,28)), cmap=pylab.cm.gray_r) \n    return predicted_label, actual_label\npredict(5)\n", "intent": "A method to make predictions on a single image\n"}
{"snippet": "import math\nassert math.isclose(0.4058298690032448, \n                    root_mean_squared_error(pd.Series(np.random.RandomState(10).rand(10)), \n                                            pd.Series(np.random.RandomState(20).rand(10))))\nassert math.isclose(0.4725000132308342, \n                    root_mean_squared_error(pd.Series(np.random.RandomState(30).rand(10)), \n                                            pd.Series(np.random.RandomState(20).rand(10))))\nassert math.isclose(0.36713007358887645, \n                    root_mean_squared_error(pd.Series(np.random.RandomState(30).rand(10)), \n                                            pd.Series(np.random.RandomState(500).rand(10))))\n", "intent": "Expected output:\n```\nRMSE: 4.679506300635516\n```\n"}
{"snippet": "logreg.predict([[5.0]])[0]\n", "intent": "Predicting the state of the battery at 5.0 hours and at 5.2 hours gives:\n"}
{"snippet": "lgrg.predict([[650,20]])[0]\n", "intent": "So if, say, a customer had a credit score of 650 and was requesting a loan of \\$2000, what does our model predict?\n"}
{"snippet": "kumquat = [[1,0,0,0,1,0,0]]\nfnb.predict(kumquat)\n", "intent": "Now that we've created our model, let's see how well we can predict a few other fruit:<br>\nPredict a kumquat?  \n"}
{"snippet": "green_apple = [[1,1,0,1,0,0,0]]\nfnb.predict(green_apple)\n", "intent": "Predict a green apple?  \n"}
{"snippet": "naked_mole_rat = [[4,1,3,0,0,0]] \nanimals.predict(naked_mole_rat)\n", "intent": "Now let's see if it can predict some animals, like the naked mole rat, or the giant tortoise:\n"}
{"snippet": "platypus = [[4,1,1,1,0,1]]\nanimals.predict(platypus)\n", "intent": "Indeed it does!  What about a really weird choice, like a platypus?  They are warm-blooded, semi-aquatic, egg-laying mammals that have fur.\n"}
{"snippet": "kumquat = [[2,0,0]] \nftr.predict(kumquat)\n", "intent": "We can see if this model can accurately predict a kumquat.  (Orange, segmented, small) translates to `[2,0,0]` in this case:\n"}
{"snippet": "metrics.r2_score(train['Data'], train_regr)\n", "intent": "We compute the $R^2$ and MAE for the model on the *training set*:\n"}
{"snippet": "metrics.r2_score(test['Data'], intercept + slope*test.index)\n", "intent": "Then we compare to the measures for the model on the *test set*:\n"}
{"snippet": "y_hat = model.predict(x_test)\ncifar10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n", "intent": "This may give you some insight into why the network is misclassifying certain objects.\n"}
{"snippet": "metrics.r2_score(train1['Data'], train_regr)\n", "intent": "We compute the $R^2$ and MAE for the model on the *training set*:\n"}
{"snippet": "metrics.r2_score(test1['Data'], intercept + slope*test1.index)\n", "intent": "Then we compare to the measures for the model on the *test set*:\n"}
{"snippet": " metrics.r2_score((x_q,y_q), (x_q,intercept + slope*x_q))\n", "intent": "Inspection of the graph, combined with the very low $R^2$ score\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predict_fn = lambda x: rf.predict_proba(encoder.transform(x))\n", "intent": "Note that our predict function first transforms the data into the one-hot representation\n"}
{"snippet": "sklearn.metrics.accuracy_score(labels_test, rf.predict(encoder.transform(test)))\n", "intent": "This classifier has perfect accuracy on the test set!\n"}
{"snippet": "print(neigh.predict([[1.1]]))\nprint(neigh.predict_proba([[0.9]]))\n", "intent": "Predict and Print Results\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "predict = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "print(classification_report(y_test,for_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df['Cluster'],km.labels_))\nprint(classification_report(df['Cluster'],km.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predict = KNN.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "pred = reg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "pred = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "pred = svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "y = km.predict(data)\nprint y\n", "intent": "Predict the clusters for each data point\n"}
{"snippet": "MIN_RATING = 0.0\nMAX_RATING = 5.0\nITEMID = 1\nUSERID = 1\nsvd.predict(ITEMID, USERID, MIN_RATING, MAX_RATING)\n", "intent": "Predict rating for a given user and movie, $\\hat{r}_{ui}$\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, KFold\ncv = KFold(len(df2),n_folds=5, shuffle=True)\nperf = cross_val_score(lm3, X, y, cv=cv)\nprint perf.mean(), perf.std()\n", "intent": "Try Mean of Cross Validation Score:\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model. Can you get something over 85%?\n"}
{"snippet": "score = marvin_model.evaluate(marvin_dataset[\"X_train\"], marvin_dataset[\"y_train\"], verbose=1)\nprint(\"Accuracy is: {} \".format(score[1]))\nmarvin_metrics = {\"Accuracy\": score}\n", "intent": "Accuracy is used as Evaluation metric\n"}
{"snippet": "predicted = marvin_model.predict_classes(input_message)\nacc = marvin_model.predict(input_message)[0][predicted[0]]\nprint(\"The image has the number {} with {} accuracy\".format(predicted, acc))\nfinal_prediction = predicted[0]\n", "intent": "Do prediction and show accuracy.\n"}
{"snippet": "sorted_labels = sorted(\n    labels, \n    key=lambda name: (name[1:], name[0])\n)\nprint(metrics.flat_classification_report(\n    y_test, y_pred, labels=sorted_labels, digits=3\n))\n", "intent": "Inspect per-class results in more detail:\n"}
{"snippet": "crf = rs.best_estimator_\ny_pred = crf.predict(X_test)\nprint(metrics.flat_classification_report(\n    y_test, y_pred, labels=sorted_labels, digits=3\n))\n", "intent": "As you can see, quality is improved.\n"}
{"snippet": "x_test = np.array(['i like cheese'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in EXAMPLES:\n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n    prediction = model.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    print(\"source:\", example)\n    print(\"output:\", ''.join(output))\n", "intent": "You can now see the results on new examples.\n"}
{"snippet": "loss, acc = model.evaluate(X_dev, Y_dev)\nprint(\"Dev set accuracy = \", acc)\n", "intent": "Finally, let's see how your model performs on the dev set.\n"}
{"snippet": "y_test1 = model.predict(X_test1)\n", "intent": "Predizione sul dataset di test:\n"}
{"snippet": "y_test_5 = model_5.predict(X_test1)\ny_test_5_prob = model_5.predict_proba(X_test1)\nmetrics.roc_auc_score(db_result['FLG_DEF_6M'].tolist(), y_test_5_prob[:,1])\n", "intent": "Verifico la bonta' del modello:\n"}
{"snippet": "note_predictions = classifier.predict(X_test)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\ny_pred = clf.predict(X_test)\nprint (\"Test accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\nprint ()\nprint(metrics.classification_report(y_test, y_pred, target_names=iris.target_names))\n", "intent": "Scikit-learn provides implementation of all methods you need.\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\ny_pred = clf.predict(X_test)\nprint (\"Test accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\nprint ()\nprint(metrics.classification_report(y_test, y_pred))\n", "intent": "Implement all evaluation methods you have learned in the Scikit-learn tutorial. Decide which model performs best on the given problem.\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\ny_pred = clf_pipeline.predict(X_test)\nprint (\"Test accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\nprint ()\nprint(metrics.classification_report(y_test, y_pred))\n", "intent": "Implement all evaluation methods you have learned in the Scikit-learn tutorial. Decide which model performs best on the given problem.\n"}
{"snippet": "y_pred = model.predict(X_test_flat)\nprint(y_pred.shape)\n", "intent": "First we need to convert probability vectors to class indices.\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nprint (\"Test accuracy: {:.4f}\".format(accuracy_score(y_test_class, y_pred_class)))\nprint ()\nprint(metrics.classification_report(y_test_class, y_pred_class, digits=4))\n", "intent": "We can use the scikit-learn functions now.\n"}
{"snippet": "from numpy import int32\ny_pred = model.predict(X_test)\ny_pred = (y_pred >= 0.5).astype(int32)\n", "intent": "Predict target values and convert probabilities to binary values.\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nprint (\"Test accuracy: {:.4f}\".format(accuracy_score(y_test, y_pred)))\nprint ()\nprint(metrics.classification_report(y_test, y_pred, digits=4))\n", "intent": "Print evaluation metrics\n"}
{"snippet": "y_pred_cv = clf_cv.predict(Xtestlr)\nprint(accuracy_score(y_pred_cv, ytestlr))\n", "intent": "The GridSearch chose a different $C$ than we have origially chosen. \n"}
{"snippet": "print (np.sum((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "pred = rcf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "prediction = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predictions1 = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "predictions = sv.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "import sklearn.metrics\ndef try_a_b_c(a,b,c):\n    this_try = [underlying_function(w, a, b, c) for w in wheel.seconds]\n    return sklearn.metrics.mean_squared_error(this_try, wheel.signal)\n", "intent": "We need a function that says how close a match a particular set of\nparameters are: let's use the mean squared error to make sense of it.\n"}
{"snippet": "metrics.silhouette_score(x, labels, metric='euclidean')\n", "intent": "Compute the Silhoutte Score\n"}
{"snippet": "print('Estimated number of clusters: %d' % n_clusters_)\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(y, labels))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels))\n", "intent": "Now, we can use a handy chunk of code from the Scitkit documentation to measure the performance of our model \n"}
{"snippet": "random_pred = random_for.predict_proba(test[rhs_columns]); random_pred\nfpr, tpr, _ = roc_curve(test['inclusion'], random_pred[:, 1])\nroc_auc = auc(fpr, tpr)\nprint(\"Area under the ROC curve : %f\" % roc_auc) \n", "intent": "Print the AUC for Random Forest\n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "sklearn.metrics.mean_squared_error(boston.target, lm.predict(X))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "- What types of data points do you think the model is having the most difficult time with?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint(\"Accuracy: \", accuracy_score(decision_tree_classifier.predict(test_features), test_labels))\n", "intent": "Let's look at the accuracy of our build model.\n"}
{"snippet": "print(\"Accuracy: \", accuracy_score(svm_classifier.predict(test_features), test_labels))\n", "intent": "Again, let's look at the accuracy of the machine learning model.\n"}
{"snippet": "print(\"Accuracy: \", accuracy_score(gaussian_nb_classifier.predict(test_features), test_labels))\n", "intent": "Again, let's look at the accuracy of the machine learning model.\n"}
{"snippet": "print(\"Accuracy: \", accuracy_rmse(svr_model.predict(test_features), test_labels))\n", "intent": "Let's quickly check the accuracy.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nfinal_scores = []\nfor class_name in class_names:\n    test_target = test_labels[class_name]\n    score = roc_auc_score(test_labels[class_name], predictions[class_name])\n    print('ROC AUC score for class {} is {}'.format(class_name, score))\n    final_scores.append(score)\nprint('Average ROC AUC score is {}'.format(np.mean(final_scores)))\n", "intent": "Finally, let's calculate the average ROC AUC score for the test set:\n"}
{"snippet": "roc_auc_score(y_test, prob)\n", "intent": "600 6 0.8224773007456205\n600 5 std 0.8404221862156795\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "Let's evaluate our decision tree.\n"}
{"snippet": "clf.predict_proba([[1.0, 0.3, 1.4, 2.1]])\n", "intent": "If we have a flower that has:\n- Sepal.Length = 1.0\n- Sepal.Width = 0.3\n- Petal.Length = 1.4\n- Petal.Width = 2.1\n"}
{"snippet": "auc = sklearn.metrics.roc_auc_score(y_test, y_pred[:, 1])\nauc\n", "intent": "The first column in y_pred is the P(0), i.e. P(not fraud), and the second column is P(1/fraud).\n"}
{"snippet": "def compute_loss(HL, Y):\n    m = Y.shape[1]\n    loss = -(1./m) * np.sum(np.multiply(Y, np.log(HL)))\n    loss = np.squeeze(loss)      \n    assert(loss.shape == ())\n    return loss\n", "intent": " $Loss = -\\frac{1}{m}\\sum_{i=1}^{m}Y^{(i)}\\log(HL^{(i)})+(1-Y^{(i)})\\log(1-HL^{(i)})$\n"}
{"snippet": "pred_train = predict(train_set_x_new, train_set_y_new, parameters)\n", "intent": "Let's see the accuray we get on the training data.\n"}
{"snippet": "pred_test = predict(test_set_x, test_set_y, parameters)\n", "intent": "We get ~ 88% accuracy on the training data. Let's see the accuray on the test data.\n"}
{"snippet": "predictions = nn_model.predict(test_set_x)\npredictions = np.argmax(predictions, axis = 1)\npredictions\n", "intent": "Now, let's make predictions on the test dataset.\n"}
{"snippet": "X_test_rfe_15 = X_test[col_15]\nX_test_rfe_15 = sm.add_constant(X_test_rfe_15, has_constant='add')\nX_test_rfe_15.info()\ny_pred = lm_15.predict(X_test_rfe_15)\n", "intent": "Note that the model with 15 variables gives about 93.9% r-squared, though that is on training data. The adjusted r-squared is 93.3.\n"}
{"snippet": "y_pred = mlr.predict(X_test)\ny_pred\n", "intent": "From the above result we may infern that if TV price increses by 1 unit it will affect sales by 0.045 units.\n"}
{"snippet": "X_test_counts = vect.transform(X_test)\nX_test_tfidf = tfidf.transform(X_test_counts)\ny_pred = clf.predict(X_test_tfidf)\n", "intent": "* Transform (no fitting) the test data with the same CountVectorizer and TfidfTransformer\n* Predict labels on these tfidf values.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "model_top.evaluate(validation_data, validation_labels)\n", "intent": "Loss and accuracy :\n"}
{"snippet": "y_pred = linreg.predict(X_test)\nfrom sklearn import metrics\nprint (\"MSE:\",metrics.mean_squared_error(y_test, y_pred))\nprint (\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "`PE = 453.694859258 - 1.97887108*AT - 0.23229086*V + 0.0628722*AP - 0.15832456*RH`\n"}
{"snippet": "all_preds_val = np.stack([m.predict(X_val, batch_size=256) for m in ensemble_model])\n", "intent": "Let us now see how the ensemble model does on our validation set\n"}
{"snippet": "predictions = model.predict(input_test, batch_size=1024)\n", "intent": "The above metric is actually calculating how many letters correct, but we want to know how many of the words we spell correctly.\n"}
{"snippet": "p = top_model.predict(arr_lr[0:100])\n", "intent": "Let us now get the first 100 predictions of the model\n"}
{"snippet": "all_preds_val = np.stack([m.predict(valid_features_to_dense, batch_size=256) for m in ensemble_model])\n", "intent": "Checking how the ensemble model does on our validation set ...\n"}
{"snippet": "style_losses = 0\nstyle_wgts = [0.05,0.05,0.2,0.3,0.4]\nfor act_layer, tar_layer, w in zip(output_layers, style_targets, style_wgts):\n    curr_loss = K.mean(style_loss(act_layer[0], tar_layer[0]))\n    style_losses = curr_loss*w + style_losses\n", "intent": "Let us select the output of a single content image conv activations\n"}
{"snippet": "loss = 0\nfor act_layer, tar_layer in zip(mul_conv_layer_outputs, style_targets):\n    curr_loss = K.mean(style_loss(act_layer[0], tar_layer[0]))\n    loss = curr_loss + loss\n", "intent": "Rest of the steps are similar to the content image steps.\n"}
{"snippet": "score = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Training Accuracy:\", score[1])\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Model Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "pred = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model. \n"}
{"snippet": "print(type(model.evaluate(x_test, y_test, verbose=0)))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Model metric names: \", model.metrics_names)\nprint(score)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "y_pred = rf.predict(X_test)\nprint(\"ACCURACY ON TEST SET: {0:.3f}\".format(accuracy_score(y_test, y_pred)))\nprint(\"CLASSIFICATION REPORT:\\n\", \n      str(classification_report(y_test, y_pred, digits=3)))\nprint(\"\\n CLASSIFICATION REPORT: \\n\")\nprint(confusion_matrix(y_test, y_pred))\n", "intent": "Getting the prediction on the test set with the DTC and comparing to the truth.\n"}
{"snippet": "y_pred = dtc.predict(X_test)\nprint(\"ACCURACY ON TEST SET: {0:.3f}\".format(accuracy_score(y_test, y_pred)))\nprint(\"CLASSIFICATION REPORT:\\n\", \n      str(classification_report(y_test, y_pred, digits=3)))\n", "intent": "Getting the prediction on the test set with the DTC and comparing to the truth.\n"}
{"snippet": "y_hat = model.predict(X_poly)\nprint(y_hat)\n", "intent": "- Use function() predict of model to predict value for each row data of X_poly\n- Prediction is called y_hat which has the same shape to y\n"}
{"snippet": "print np.mean((bos.PRICE-lm.predict(X))**2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "split = 0.67\nX, XT, Z, ZT = loadDataset(split)\nprint 'Train set: ' + repr(len(X))\nprint 'Test set: ' + repr(len(XT))\nk = 3\nYT = predict(X, Z, XT, k)\naccuracy = getAccuracy(YT, ZT)\nprint('Accuracy: ' + repr(accuracy))\n", "intent": "Should output an accuracy of 0.95999999999999996.\n"}
{"snippet": "predictions = {}\ntest_accuracy = {}\nfor k in models.keys():\n    predictions[k] = [np.argmax(models[k].predict(np.expand_dims(feature, axis=0))) for feature in test[k]]\n    test_accuracy[k] = 100*np.sum(np.array(predictions[k])==np.argmax(test_targets, axis=1))/len(predictions[k])\n    print('%s test accuracy: %.4f%%' % (k, test_accuracy[k]))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This classifier is no longer overfitting, so thats better. But we could still improve the end accuracy result.\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "Let's grab predictions off our test set and see how well it did!\n"}
{"snippet": "y_mult_pred = multiple_linreg.predict(X_mult)\nmetrics.r2_score(y_mult, y_mult_pred)\n", "intent": "Does this model have a better R<sup>2</sup> value?\n"}
{"snippet": "print(\"MAE for fake data:\",metrics.mean_absolute_error(fake_y_true, fake_y_pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors/residuals:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print(\"MSE for fake data:\",metrics.mean_squared_error(fake_y_true,fake_y_pred))\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "print(\"RMSE for fake data:\",np.sqrt(metrics.mean_squared_error(fake_y_true, fake_y_pred)))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "y_pred = linreg.predict(X)\nmetrics.r2_score(y, y_pred)\n", "intent": "Let's confirm the R-squared value for our simple linear model using `scikit-learn's` prebuilt R-squared scorer:\n"}
{"snippet": "y_pred = best_single_tree.predict(X_test)\ny_pred\n", "intent": "**Question:** Using the tree diagram above, what predictions will the model make for each test sample observation?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_test_predict=occupancy_tree.predict(X_occ_test)\naccuracy_score(y_occ_test,y_test_predict)     \n", "intent": "  * Evaluate the model using `accuracy_score` on the testing data.\n  * Is the accuracy score above chance? What is chance accuracy here?\n"}
{"snippet": "print(\"Prediction: \",knn.predict(test))   \n", "intent": "**what id \"should\" the classifier predict?**\nHopefully, you said id 2.\n"}
{"snippet": "print(\"Prediction: \",knn.predict(test))\n", "intent": "**what id \"should\" the classifier predict?**\nHopefully, you said id 2.\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "cross_val_scores = np.abs(cross_val_score(full_pipeline,abalone_data,y,cv=10,scoring=\"mean_squared_error\"))\nrmse_cross_val_scores = np.sqrt(cross_val_scores)\nprint(\"Mean 10-fold rmse: \", np.mean(rmse_cross_val_scores))   \nprint(\"Std 10-fold rmse: \", np.std(rmse_cross_val_scores))   \n", "intent": "And now let's run the whole pipe through the `cross_val_score` object:\n"}
{"snippet": "cross_val_scores = np.abs(cross_val_score(full_pipeline,abalone_data,y,cv=10,scoring=\"mean_squared_error\"))\nrmse_cross_val_scores = np.sqrt(cross_val_scores)\nprint(\"Mean 10-fold rmse: \", np.mean(rmse_cross_val_scores))\nprint(\"Std 10-fold rmse: \", np.std(rmse_cross_val_scores))\n", "intent": "And now let's run the whole pipe through the `cross_val_score` object:\n"}
{"snippet": "predictions = classifier.predict(X_test)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "predictions = random_forest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "classification_report(y_test,predictions)\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "train_y_pred = \nmse_train = metrics.mean_squared_error(df_train_y, train_y_pred)\nvalid_y_pred = \nmse_valid = metrics.mean_squared_error(df_valid_y, valid_y_pred)\ntest_y_pred = \nmse_test = metrics.mean_squared_error(df_test_y, test_y_pred)\n", "intent": "Fit the model on the selected subset of features and calculate the performance on all datasets.\n"}
{"snippet": "test_y_pred_dt = dt.predict(df_test_X)\nmse_test_dt = metrics.mean_squared_error(df_test_y, test_y_pred_dt)\n", "intent": "Calculate the predictions and performance (MSE) on the test set for *dt*.\n"}
{"snippet": "def predict(models, weights, X):\n    n_data = len(X)\n    T = 0\n    y = np.zeros(n_data)\n    for i, h in enumerate(models):\n        y0 = weights[i] * h.predict(X)  \n        y += y0\n        T += np.sum(y0)\n    y = np.sign(y - T / (n_data*len(models)))\n    return y\n", "intent": "We define a prediction function to help with measuring accuracy:\n"}
{"snippet": "print('accuracy (train): %5.2f'%(metric(y_train, predict(models, weights, X_train))))\nprint('accuracy (test): %5.2f'%(metric(y_test, predict(models, weights, X_test))))\n", "intent": "And the final accuracy:\n"}
{"snippet": "print('predicted:', spam_detect_model.predict(tfidf4)[0])\nprint('expected:', messages.label[3])\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "print np.sqrt(metrics.mean_squared_error(y_true, y_pred))\nprint metrics.mean_squared_error(y_true, y_pred) ** 0.5\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "roc_auc_score(college['admit'], model.predict_proba(college[factors])[:,1])\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(model, X, y, scoring='roc_auc', cv=5)\nprint('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))\n", "intent": " http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred, target_names = iris.target_names))\n", "intent": "* <b>Classification reports</b> - a text report with important classification metrics (e.g. precision, recall)\n"}
{"snippet": "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\nfrom sklearn.cross_validation import cross_val_score, KFold\ncv = KFold(len(df),n_folds=5, shuffle=True)\nperf = cross_val_score(lg, X, y, cv=cv, scoring=\"accuracy\")\nprint perf.mean(), perf.std()\n", "intent": "I performed cross-validation on the data with 5 folds and obtained the mean accuracy and its standard deviation.\n"}
{"snippet": "test_predictions = model.predict_proba(X_test).argmax(axis=-1)\ntest_answers = y_test.argmax(axis=-1)\ntest_accuracy = np.mean(test_predictions==test_answers)\nprint(\"\\nTest accuracy: {} %\".format(test_accuracy*100))\nassert test_accuracy>=0.92,\"Logistic regression can do better!\"\nassert test_accuracy>=0.975,\"Your network can do better!\"\nprint(\"Great job!\")\n", "intent": "So far our model is staggeringly inefficient. There is something wring with it. Guess, what?\n"}
{"snippet": "denoising_mse = autoencoder.evaluate(apply_gaussian_noise(X_test),X_test,verbose=0)\nprint(\"Final MSE:\", denoising_mse)\nfor i in range(5):\n    img = X_test[i]\n    visualize(img,encoder,decoder)\n", "intent": "__Note:__ if it hasn't yet converged, increase the number of iterations.\n__Bonus:__ replace gaussian noise with masking random rectangles on image.\n"}
{"snippet": "test_preds = best_alpha * X_test_level2[:,0] + (1 - best_alpha) * X_test_level2[:,1] \nr2_test_simple_mix = r2_score(y_test, test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "train_preds = lr_meta.predict(X_train_level2) \nr2_train_stacking = r2_score(y_train_level2, train_preds)\ntest_preds = lr_meta.predict(X_test_level2)\nr2_test_stacking = r2_score(y_test, test_preds)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint (classification_report(messages['label'], all_predictions))\n", "intent": "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png' width=400 />\n"}
{"snippet": "train_eval_accuracy = estimator.evaluate(input_fn=predict_train_input_fn)\ntest_eval_accuracy  = estimator.evaluate(input_fn=predict_test_input_fn)\nprint(\"Training set accuracy: {accuracy}\".format(**train_eval_accuracy))\nprint(\"Test set accuracy: {accuracy}\".format(**test_eval_accuracy))\n", "intent": "Lets now run the predictions for both training and test sets.\n(Try increasing number of steps and/or num of hidden layers)\n"}
{"snippet": "print(\"Predicting people's names on the test set\")\nt0 = time()\ny_pred = clf.predict(X_test_pca)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n", "intent": "Quantitative evaluation of the model quality on the test set\n"}
{"snippet": "test = data[50:60]\npredict = neigh.predict(test) \nactual = list(target[50:60]) \nprint predict\nprint actual\n", "intent": "Prepare Test Data and Predict\n"}
{"snippet": "prediction_input_fn = learn_io.pandas_input_fn(\n    x=my_feature, y=targets, num_epochs=1, shuffle=False)\npredictions = list(linear_regressor.predict(input_fn=prediction_input_fn))\nmean_squared_error = metrics.mean_squared_error(predictions, targets)\nprint \"Mean Squared Error (on training data): %0.3f\" % mean_squared_error\nprint \"Root Mean Squared Error (on training data): %0.3f\" % math.sqrt(mean_squared_error)\n", "intent": "Let's make predictions on that training data, to see how well we fit the training data.\n"}
{"snippet": "y_probs_aa = gs_lrap.predict_proba(X_test)\nnew_test_thresh = [1 if x <= 0.10 else 0 for x in y_probs_aa.T[1]]\nprint \"50% thresh confusion matrix\"\nprint(confusion_matrix(y_test ,y_pred_aa))\nprint \"----------\"\nprint \"90% thresh confusion matrix\"\nprint(confusion_matrix(y_test ,new_test_thresh))\n", "intent": "This was bad for my overall model since it overly anticipated false positives and ruined my precision score. \n"}
{"snippet": "y_pred_knn = neighbs.predict(X_test)\nprint(confusion_matrix(y_test ,y_pred_knn))\n", "intent": "Very nearly the same, slightly more are accurately predicted to live. \n"}
{"snippet": "def predict(X, coeffs):\n    threshold = .5\n    bool_t = hypothesis( X, coeffs)>threshold \n    final=[]\n    for item in bool_t:\n        if item ==True:\n            final.append(1)\n        else:\n            final.append(0)\n    return np.array(final)\n", "intent": "Create a predict function which will predict any values strictly greater than 0.5 to be 1 when the hypothesis function is called.\n"}
{"snippet": "confusion_matrix(y_test_cv,multiNB.predict(X_test_cv))\n", "intent": ">  (0,0) = is True Negative \n> (1,0) = False Negative\n> (0,1) = False Positive\n> (1,1) = True Positives\n"}
{"snippet": "from sklearn.metrics import f1_score\nprint('F1 score for Count Vectorizer with Multi Naive Bayes {:.2%}'.format(f1_score(y_test_cv,multiNB.predict(X_test_cv))))\n", "intent": "----\nCalcute the $F_1$ score \n<br>\n<details><summary>\nClick here for the $F_1$ score...\n</summary>\n<br>\n$F_1$ = 80.86%\n</details>\n"}
{"snippet": "predictions = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "Yhat=lm.predict(X)\nYhat[0:5]   \n", "intent": " We can output a prediction \n"}
{"snippet": "Y_hat = lm.predict(Z)\n", "intent": " First lets make a prediction \n"}
{"snippet": "ypipe=pipe.predict(Z)\nypipe[0:4]\n", "intent": " Similarly,  we can normalize the data, perform a transform and produce a prediction  simultaneously\n"}
{"snippet": "Yhat=lm.predict(X)\nYhat[0:4]\n", "intent": "We can predict the output i.e., \"yhat\" using the predict method, where X is the input variable:\n"}
{"snippet": "mean_squared_error(df['price'], Yhat)\n", "intent": " we compare the predicted results with the actual results \n"}
{"snippet": "Y_predict_multifit = lm.predict(Z)\n", "intent": " we produce a prediction \n"}
{"snippet": "mean_squared_error(df['price'], Y_predict_multifit)\n", "intent": " we compare the predicted results with the actual results \n"}
{"snippet": "r_squared = r2_score(y, p(x))\nr_squared\n", "intent": "We apply the function to get the value of r^2\n"}
{"snippet": "mean_squared_error(df['price'], p(x))\n", "intent": " We can also calculate the MSE:  \n"}
{"snippet": "predictions = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "-1*cross_val_score(lre,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')\n", "intent": " We can use negative squared error as a score by setting the parameter  'scoring' metric to 'neg_mean_squared_error'. \n"}
{"snippet": "yhat_train=lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_train[0:5]\n", "intent": "Prediction using training data:\n"}
{"snippet": "yhat_test=lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_test[0:5]\n", "intent": " Prediction using test data: \n"}
{"snippet": "yhat=poly.predict(x_test_pr )\nyhat[0:5]\n", "intent": " We can see the output of our model using the method  \"predict.\" then assign the values to \"yhat\".\n"}
{"snippet": "yhat=RigeModel.predict(x_test_pr)\n", "intent": " Similarly, you can obtain a prediction: \n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('\\n', 'Test accuracy:', score[1])\n", "intent": "The result is better than random guessing (0.1), but hopefully a CNN will do much better.\n"}
{"snippet": "score = model.evaluate(X_test, y_test, verbose=0)\naccuracy = 100*score[1]\nprint('Test accuracy: %.4f%%' % accuracy)\n", "intent": "We do not expect to perform better than random chance, which in this case is 10%.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_transfer_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100 * np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1)) / len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "predictions = svc_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "pred_train = lreg.predict(X_train)\npred_test = lreg.predict(X_test)\n", "intent": "Now run a prediction on both the X training set and the testing set.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))\n0.692708333333\n", "intent": "Classification accuracy: percentage of correct predictions`\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmodel_output_0 = []\nmodel_output_1 = []\nfor row in input_data:\n    model_output_0.append(predict_with_network(row, weights_0))\n    model_output_1.append(predict_with_network(row, weights_1))\nmse_0 = mean_squared_error(model_output_0, target_actuals)\nmse_1 = mean_squared_error(model_output_1, target_actuals)\nprint(\"Mean squared error with weights_0: %f\" %mse_0)\nprint(\"Mean squared error with weights_1: %f\" %mse_1)\n", "intent": "First we need to implement a loss function to measure the process at which we learn to approximate the real distribution. \n"}
{"snippet": "from sklearn.exceptions import NotFittedError\nfor name, model in fitted_models.items():\n    try:\n        pred = model.predict(X_test)\n        print(name, 'has been fitted.')\n    except NotFittedError as e:\n        print(repr(e))\n", "intent": "<br>\n**Finally, run this code to check that the models have been fitted correctly.**\n"}
{"snippet": "pred = fitted_models['rf'].predict(X_test)\n", "intent": "Predict the test set using the fitted random forest.\n"}
{"snippet": "print(r2_score(y_test, pred))\nprint(mean_absolute_error(y_test, pred))\n", "intent": "Finally, we use the scoring functions we imported to calculate and print $R^2$ and MAE.\n"}
{"snippet": "print(metrics.accuracy_score(y_test, predicted))\n", "intent": "<p>As we can see, the classifier is predicting a 1 (having an affair) any time the probability in the second column is greater than 0.5.</p>\n"}
{"snippet": "note_predictions = list(classifier.predict(X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "y_pred = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "Now let's predict using the trained model.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint confusion_matrix(df['Cluster'], kmeans.labels_)\nprint \nprint classification_report(df['Cluster'], kmeans.labels_)\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions = clf.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "y_pred = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "** Agora preveja valores para os dados de teste. **\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "Agora vamos prever o uso do modelo treinado.\n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "Vamos prever os valores do y_test e avaliar o nosso modelo.\n** Preveja a classe de not.fully.paid para os dados X_test. **\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "Then you can re-run predictions on this grid object just like you would with a normal model.\n"}
{"snippet": "print('Estimated number of clusters: %d' % n_clusters_)\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(y, labels))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels))\n", "intent": "**7.2 Check the homogeneity, completeness, and V-measure against the stored rank `y`**\n"}
{"snippet": "print(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, labels))\n", "intent": "**9.3 Evaluate DBSCAN visually, with silhouette, and with the metrics against the true `y`.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n", "intent": "***\nFurther useful checks are the **classification report** and the **confusion matrix**,  \nthey give detailed Info on mis-classifications:\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(ytest, ypred))\n", "intent": "***\n**Detailed classification report**\n"}
{"snippet": "predictions = classifier.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predictions = classifier.predict(X_test)\npredictions\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predictions = nb.predict(X_test)\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nypred = m.predict(Xtrain)\nconfusion_matrix(ytrain, ypred)\n", "intent": "With a *skewed dataset*, a confusion matrix is more robust:\n"}
{"snippet": "leo = np.array([[22, 3, 0]])\nkate = np.array([[25, 1, 1]])\nprint(m.predict(leo))\nprint(m.predict(kate))\n", "intent": "Create a data set for additional passengers and predict whether they will survive:\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    d_loss = ls_discriminator_loss(score_real, score_fake)\n    g_loss = ls_generator_loss(score_fake)\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Before running a GAN with our new loss function, let's check it:\n"}
{"snippet": "scores = cross_val_score(logreg, features_array, target, cv=5,\n                         scoring='roc_auc')\nscores.min(), scores.mean(), scores.max()\n", "intent": "`cross_val_score` reports accuracy by default be it can also be used to report other performance metrics such as ROC-AUC or f1-score:\n"}
{"snippet": "x_val, y_val = prepare_to_test(X, profiles, Y)\nlogging.info(\"- Predict using trained model\")\ny_pred = estimator.predict(x_val, verbose=0)    \ny_pred = pred_to_targets(y_pred)\nlogging.info(\"- Compute map7 score\")\nprint map7_score2(y_val, y_pred, clients_last_choice[TARGET_LABELS].values)\n", "intent": "Check score on the data 2016/05\n"}
{"snippet": "x_val, y_val = prepare_to_test(X, Y)\ny_pred = estimator.predict(x_val, verbose=0)\nprint \"Score on the whole dataset : \", map7_score2(y_val, y_pred, clients_last_choice[TARGET_LABELS].values)\n", "intent": "Check score on the same data\n"}
{"snippet": "logging.info(\"- Compute map7 score\")\nprint map7_score(y_val, y_preds, clients_last_choice[LC_TARGET_LABELS].values)\nlogging.info(\"- Compute max map7 score\")\nprint map7_score(y_val, y_val, clients_last_choice[LC_TARGET_LABELS].values)\n", "intent": "Check score on the data 2016/05\n"}
{"snippet": "y_val = targets_str_to_indices(Y[target_labels].values)\nlogging.info(\"- Compute map7 score\")\nprint map7_score(y_val, y_preds, Y[LAST_TARGET_LABELS].values)\nlogging.info(\"- Compute max map7 score\")\nprint map7_score(y_val, y_val, Y[LAST_TARGET_LABELS].values)\n", "intent": "Check score on the data 2016/05\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncvscores = cross_val_score(model, X, y, cv = 5, n_jobs=-1)\nprint \"CV score: {:.3} +/- {:.3}\".format(cvscores.mean(), cvscores.std())\n", "intent": "As usual we can calculate the cross validated score to judge the quality of the model.\n"}
{"snippet": "metrics.silhouette_score(dn1, labels, metric='euclidean')\n", "intent": "Compute the Silhoutte Score\n"}
{"snippet": "metrics.accuracy_score(predY,y)\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "metrics.silhouette_score(y,predY,metric=\"euclidean\")\n", "intent": "Compute the silhoutte coefficient to see how consistent our data is within the clusters\n"}
{"snippet": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    with get_session() as sess:\n        d_loss, g_loss = sess.run(\n            lsgan_loss(tf.constant(score_real), tf.constant(score_fake)))\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])\n", "intent": "Test your LSGAN loss. You should see errors less than 1e-7.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nroc_auc_score(y_valid,y_pred)\n", "intent": "Check Confusion matrix\n"}
{"snippet": "y_hat = knn.predict(wine_df)\n(y_hat == y).mean()\n", "intent": "<a id='cv-knn5'></a>\n"}
{"snippet": "lasso = Lasso(alpha=optimal_lasso.alpha_)\nlasso_scores = cross_val_score(lasso, Xs, y, cv=10)\nprint lasso_scores\nprint np.mean(lasso_scores)\n", "intent": "---\nIs it better than the Linear regression? Is it better than Ridge? What do the differences in results imply about the issues with the dataset?\n"}
{"snippet": "enet = ElasticNet(alpha=optimal_enet.alpha_, l1_ratio=optimal_enet.l1_ratio_)\nenet_scores = cross_val_score(enet, Xs, y, cv=10)\nprint enet_scores\nprint np.mean(enet_scores)\n", "intent": "---\nHow does it compare to the Ridge and Lasso regularized regressions?\n"}
{"snippet": "y_pred_dtree = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "y_pred_rfc = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print (classification_report(y_test, y_pred_rfc))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "y_pred= logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print(classification_report(y_test, y_pred))\n", "intent": "**Classification report**\n"}
{"snippet": "D_loss, G_loss = lsgan_loss(logits_real, logits_fake)\nD_train_step = D_solver.minimize(D_loss, var_list=D_vars)\nG_train_step = G_solver.minimize(G_loss, var_list=G_vars)\n", "intent": "Create new training steps so we instead minimize the LSGAN loss:\n"}
{"snippet": "pred = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "pred = clf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print classification_report(y_test, pred)\nprint\nprint confusion_matrix(y_test, pred)\nfp = confusion_matrix(y_test, pred)[0][1]\nprint \nprint fp\nexperiment.log_metric(\"confusion matrix\", fp)\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "y_pred = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "resnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy_resnet50 = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_resnet50)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"RMSE:\", (mean_squared_error(ys, predictions)**0.5)\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "df['probability'] = model.predict_proba(df[features]).T[1]\n", "intent": "```model.predict_proba``` will give you the probability of an outcome, instead of just the outcome:\n"}
{"snippet": "silhouette_avg = silhouette_score(X1, labels)\nprint(\"The average silhouette_score is :\", silhouette_avg)\n", "intent": "Compute the Silhoutte Score, AMI and inertia to measure your analysis\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "metrics.silhouette_score(X_scaled, labels, metric='euclidean') \n", "intent": "And to compute the clusters' silhouette coefficient:\n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, r2_score, roc_auc_score, auc\nfrom sklearn.metrics import log_loss, hinge_loss, hamming_loss\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import roc_curve, precision_recall_curve\npred = logreg.predict(X_test)\nprint(classification_report(y_test, pred))\n", "intent": "<b>Reviewing classification scores to understand model fit</b>\n"}
{"snippet": "tree_pred=tree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "rfc_pred=rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(confusion_matrix(y_test,rfc_pred))\nprint('\\n')\nprint(classification_report(y_test,rfc_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint('\\n')\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "knn_pred=KNN.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "log_pred=logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "svm_pred=svm_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    pass\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "labels = pipeline.predict(samples)\n", "intent": "**Step 4:** Obtain the cluster labels for `samples` by using the `.predict()` method of `pipeline`, assigning the result to `labels`.\n"}
{"snippet": "labels = pipeline.predict(movements)\n", "intent": "**Step 3:** Use the `.predict()` method of the pipeline to predict the labels for `movements`.\n"}
{"snippet": "accuracy_score(clf.predict(Xtestlr), ytestlr)\n", "intent": "Using grid search cross validation provided a different optimal value for C with a higher accuracy score than the previous method.\n"}
{"snippet": "y_pred = reg.predict(X_test)\n", "intent": "Predicting values on X_test\n"}
{"snippet": "print(r2_score(y_test, y_pred)*200)\n", "intent": "Checking accuracy of model\n"}
{"snippet": "y_pred = regressor.predict(X_test)\n", "intent": "Predicting values on X_test\n"}
{"snippet": "skfold = StratifiedKFold(n_splits=5)\nscores = cross_val_score(logreg, iris.data, iris.target, cv=skfold)\nscores, scores.mean()\n", "intent": "RepeatedStratifiedKFold\n"}
{"snippet": "predictions = list(classifier.predict(input_fn=pred_fn))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "note_predictions = list(classifier.predict(input_fn=pred_fn))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.001.\n"}
{"snippet": "print(classification_report(y_test, predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\ny_test = college_data['Cluster']\npredictions = kmeans.labels_\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "y_pred = regressor.predict(X_test)  \n", "intent": "fit => our regressor has learnt the correlations of the training sets variables and has learnt to predict salary based on experience.\n"}
{"snippet": "gan_loss = tfgan.gan_loss(\n    conditional_gan_model, gradient_penalty_weight=1.0)\nevaluate_tfgan_loss(gan_loss)\n", "intent": "<a id='conditional_loss'></a>\n"}
{"snippet": "infogan_loss = tfgan.gan_loss(\n    infogan_model,\n    gradient_penalty_weight=1.0,\n    mutual_information_penalty_weight=1.0)\nevaluate_tfgan_loss(infogan_loss)\n", "intent": "<a id='infogan_loss'></a>\nThe loss will be the same as before, with the addition of the mutual information loss.\n"}
{"snippet": "crossed_lat_lon_fc = tf.feature_column.crossed_column(\n    [latitude_bucket_fc, longitude_bucket_fc], int(5e3))\nfc = [crossed_lat_lon_fc]\nest = tf.estimator.LinearRegressor(fc, model_dir=os.path.join(logdir, 'crossed'))\nest.train(in_fn(train_ds), steps = 5000)\nest.evaluate(in_fn(test_ds))\n", "intent": "The single-cell \"holes\" in the figure are caused by cells which do not contain examples.\n"}
{"snippet": "fc = [\n    latitude_bucket_fc,\n    longitude_bucket_fc,\n    crossed_lat_lon_fc]\nest = tf.estimator.LinearRegressor(fc, model_dir=os.path.join(logdir, 'both'))\nest.train(in_fn(train_ds), steps = 5000)\nest.evaluate(in_fn(test_ds))\n", "intent": "The model generalizes better if it also has access to the raw categories, outside of the cross. \n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(estimator=model, X=X, y=y, cv=5)\nprint(\"Test  r2:%.2f\" % scores.mean())\ncv = KFold(n_splits=5, random_state=42)\nscores = cross_val_score(estimator=model, X=X, y=y, cv=cv)\nprint(\"Test  r2:%.2f\" % scores.mean())\n", "intent": "Scikit-learn provides user-friendly function to perform CV:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(estimator=model, X=X, y=y, cv=5)\nscores.mean()\ndef balanced_acc(estimator, X, y, **kwargs):\n    return metrics.recall_score(y, estimator.predict(X), average=None).mean()\nscores = cross_val_score(estimator=model, X=X, y=y, cv=5, scoring=balanced_acc)\nprint(\"Test  ACC:%.2f\" % scores.mean())\n", "intent": "Scikit-learn provides user-friendly function to perform CV:\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.001.\n"}
{"snippet": "mean_absolute_error(xgb_test_fold, mlp_test_fold)\n", "intent": "And testing that these labels completely match (there's still a little rounding error due to log-exp conversion):\n"}
{"snippet": "def compute_score(clf, X, y,scoring='accuracy'):\n    xval = cross_val_score(clf, X, y, cv = 5,scoring=scoring)\n    return np.mean(xval)\n", "intent": "To evaluate our model we'll be using 5-fold cross validation with the Accuracy metric.\nTo do that, we'll define a small scoring function. \n"}
{"snippet": "class_predict = log_model2.predict(X_test)\nprint metrics.accuracy_score(Y_test,class_predict)\n", "intent": "Now we can use predict to predict classification labels for the next test set, then we will reevaluate our accuracy score!\n"}
{"snippet": "from sklearn import metrics\npredicted = model.predict(X_test)\nexpected = Y_test\nprint metrics.accuracy_score(expected,predicted)\n", "intent": "Now we'll go ahead and see how well our model did!\n"}
{"snippet": "predicted = model.predict(X_test)\nexpected = Y_test\n", "intent": "Now we predict the outcomes from the Testing Set:\n"}
{"snippet": "print metrics.accuracy_score(expected, predicted)\n", "intent": "Finally we can see the metrics for performance:\n"}
{"snippet": "print(svm.predict(X_train))\nprint(y_train)\n", "intent": "3) Apply / evaluate\n"}
{"snippet": "print(\"Loss: \", model.evaluate(x_test, y_test, verbose=0))\n", "intent": "Now that we have trained our model we can evaluate it on our testing set. It is also just one line of code.\n"}
{"snippet": "cifar_model.evaluate(cifar_x_test, y_test_cat)\n", "intent": "Now we can evaluate on our test set.\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.3f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.001.\n"}
{"snippet": "y_pred = imdb_model.predict(x_test_proc)\n", "intent": "Now we can look at our predictions and the sentences they correspond to.\n"}
{"snippet": "y_model = model.predict(...)\n", "intent": "Now let's use the model.predict method to see what values our model predict for the data. We'll use the X test data we set aside a few cells before. \n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model) \n", "intent": "Great, now lets use y_model and compare it to the test data we set aside and see how our model performed!\n"}
{"snippet": "validation_prediction_delta_lad = (model_delta_lad.predict(feature_numpy_validation) + data_training_validation['gap_t(j-1)'].to_numpy().astype(float)).tolist()\n", "intent": "* Predict gap_delta\n"}
{"snippet": "def seq2seq_loss(input, target):\n    sl,bs = target.size()\n    sl_in,bs_in,nc = input.size()\n    if sl>sl_in: input = F.pad(input, (0,0,0,0,0,sl-sl_in))\n    input = input[:sl]\n    return F.cross_entropy(input.view(-1,nc), target.view(-1))\n", "intent": "The loss function used for this model is standard cross entropy\n"}
{"snippet": "pred = kmodel.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "metrics.mean_squared_error(y, model1.predict(X))\n", "intent": "Compare to the MSE for the model estimated on the entire dataset.\n"}
{"snippet": "metrics.confusion_matrix(y, gs.best_estimator_.predict(X))\n", "intent": "Confusion matrix of 'best' model:\n"}
{"snippet": "metrics.accuracy_score(y, model1.predict(X))\n", "intent": "Compute classification accuracy.\n"}
{"snippet": "roc_auc_score(df['admit'], lm.predict(feature_set)\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "pred_probs = model1.predict_proba(X)[:,1]\n", "intent": "Compute predicted probabilities for GBM (`y` = 1).\n"}
{"snippet": "def specificity_score(y_true, y_pred):\n    cm = metrics.confusion_matrix(y_true, y_pred)\n    return cm[0,0] / cm[0,:].sum()\n", "intent": "Define a function to compute specificity.\n"}
{"snippet": "sensitivities = np.zeros(cutoffs.size)\nspecificities = np.zeros(cutoffs.size)\nfor i, cutoff in enumerate(cutoffs):\n    sensitivities[i] = metrics.recall_score(y, pred_probs >= cutoff)\n    specificities[i] = specificity_score(y, pred_probs >= cutoff)\n", "intent": "Compute sensitivity and specificity at the cut-off values defined above.\n"}
{"snippet": "metrics.roc_auc_score(y, pred_probs)\n", "intent": "Compute area under the ROC curve (AUC).\n"}
{"snippet": "metrics.confusion_matrix(y, model2.predict(X))\n", "intent": "Compute confusion matrix for the 'best' model.\n"}
{"snippet": "labels = kmeans.fit_predict(crypto_close.transpose())\n", "intent": "Fit the model and retrieve cluster assignments.\n"}
{"snippet": "mses = ms.cross_val_score(spls, boroughs, feelings, scoring='neg_mean_squared_error', cv=three_fold_cv)\nnp.mean(-mses)\n", "intent": "Compute average MSE across folds.\n"}
{"snippet": "aucs = ms.cross_val_score(rf1, X, y, scoring='roc_auc', cv=ten_fold_cv)\nnp.mean(aucs)\n", "intent": "Compute average AUC across folds.\n"}
{"snippet": "aucs = ms.cross_val_score(rf2, X_tfidf, y, scoring='roc_auc', cv=ten_fold_cv)\nnp.mean(aucs)\n", "intent": "Compute average AUC across folds.\n"}
{"snippet": "scores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=5)\nprint(scores)\nprint(scores.mean())\n", "intent": "K-Fold cross validation is just as easy; let's use a K of 5:\n"}
{"snippet": "metrics.f1_score(y_valid, lr_gs.best_estimator_.predict(X_valid))\n", "intent": "Check $F_{1}$ score and confusion matrix on the validation set.\n"}
{"snippet": "aucs = ms.cross_val_score(ssvc, X, y, scoring='roc_auc', cv=ten_fold_cv)\nnp.mean(aucs)\n", "intent": "Compute average AUC across folds.\n"}
{"snippet": "pred_probs = nn.predict(X_scaled)[:,0]\n", "intent": "Use network to predict probabilities.\n"}
{"snippet": "-ms.cross_val_score(ssvc, X, y, scoring='neg_mean_squared_error', cv=split)\n", "intent": "Compute MSE for split.\n"}
{"snippet": "hires['Prediction'] = gs.best_estimator_.predict(X)\nhires['Hires'].plot()\nhires.loc[split.test_fold >= 0, 'Prediction'].plot(xlim=('2016-01-01', '2017-12-31'))\n", "intent": "Plot original time series and prediction from January 2016 onwards.\n"}
{"snippet": "metrics.silhouette_score(X, labels, metric='euclidean')\n", "intent": "Compute the Silhoutte Score\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nprint (\"RMSE:\", mean_squared_error(ys, predictions))\nprint (\"MAE:\", mean_absolute_error(ys, predictions))\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint (\"RMSE:\", mean_squared_error(ys, predictions))\nprint (\"MAE:\", mean_absolute_error(ys, predictions))\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nimport math\nprint (\"RMSE:\", math.sqrt(mean_squared_error(ys, predictions)))\nprint (\"MAE:\", mean_absolute_error(ys, predictions))\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(y, p4(x))\nprint(r2)\n", "intent": "Looks pretty good! Let's measure the r-squared error:\n"}
{"snippet": "print(accuracy_score(y_test, predictions))\nprint(precision_score(y_test, predictions))\nprint(recall_score(y_test, predictions))\nplot_roc(y_test, predictions)\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "metrics.silhouette_score(X, labels, metric='euclidean')\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "X2 = [[5.4,  3.9,  1.7,  0.4], [6.3,  3.3,  6. ,  2.5]]\nknn.predict(X2)\n", "intent": "> __Q:__ *why does the X1 test data above have two brackets?*\n  _hint:_ remember the shape of our input data\nLet's try another test data\n"}
{"snippet": "y_pred = logreg.predict(X_test)\n", "intent": "Now that we have trained the model with the training data, let's see the prediction results from the test data\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred,\n                            target_names=['No', 'Yes']))\n", "intent": "True Negative = 1486\nTrue Positive = 325\nFalse Positive = 29\nFalse Negative = 23\n"}
{"snippet": "mse_scaled = metrics.mean_squared_error(usa_house_fitted_inv['Price'], \n                           usa_house_fitted_inv['Price predicted'])\nnumpy.sqrt(mse_scaled)\n", "intent": "RMSE root mean squared error. This is useful as it tell you in terms of the dependent variable, what the mean error in prediction is.\n"}
{"snippet": "y_train_predict = dtr.predict(X_train)\n", "intent": "Predict the training set.\n"}
{"snippet": "y_test_predict = dtr.predict(X_test)\n", "intent": "Predict the test set.\n"}
{"snippet": "r2_score(y_train, y_train_predict)\n", "intent": "Calculate the r2 score for the test set.\n"}
{"snippet": "print(svc.predict([[200000, 40]]))\n", "intent": "Or just use predict for a given point:\n"}
{"snippet": "y_test_pred = clf.predict(X_test)\n", "intent": "Predict the test set.\n"}
{"snippet": "r2_score(y_train, y_train_pred, multioutput='variance_weighted')\n", "intent": "Calculate the r2 score for the training set.\n"}
{"snippet": "r2_score(y_test, y_test_pred, multioutput='variance_weighted')\n", "intent": "Calculate the r2 score for the test set.\n"}
{"snippet": "from sklearn import metrics\ny_test = kmeans.predict(X_test)\nsilhouette_score = metrics.silhouette_score(X_test, y_test, metric=\"euclidean\")\nprint(silhouette_score)\n", "intent": "e.g., silhouette coefficient\n"}
{"snippet": "score = Resnet_model.evaluate(test_Resnet,test_targets,verbose=0)\nprint(\"Test Accuracy: %.3f%%\" %score[1])\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = classifier.predict(X_test)  \nprint(pd.crosstab(y_test, y_pred,rownames=['true'], colnames=['pred']))\nprint(\"Acc = \",np.sum(y_test==y_pred)/len(y_test))\n", "intent": "prediction on the test set with the accuracy and the confusion matrix\n"}
{"snippet": "print(\"test mean squared error: {:.3f}\".format(\n        ed.evaluate('mean_squared_error', data={X: X_test, y_post: y_test})))\n", "intent": "Evaluate predictions from the posterior predictive.\n"}
{"snippet": "y_pred = regressor.predict(X)\ny_pred[:10]\n", "intent": "As you can see the predicted value is not very far away from the actual value.\nNow let's try to predict the price for all the houses in the dataset.\n"}
{"snippet": "cross_val_score(classifier, X, y, cv=5)\n", "intent": "As you can see, the function uses three folds by default. You can change the number of folds using the cv argument:\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(np.array(trainY), p4(np.array(trainX)))\nprint(r2)\n", "intent": "...even though it fits the training data better:\n"}
{"snippet": "grid.predict(X)\n", "intent": "Then, as with all models, we can use ``predict`` or ``score``:\n"}
{"snippet": "xception_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_xception]\ntest_accuracy = 100*np.sum(np.array(xception_predictions)==np.argmax(test_targets, axis=1))/len(xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import cross_validation\nX_train = X[:X.shape[0]]\ny_train = y[:X.shape[0]]\ny_pred = cross_validation.cross_val_predict(logres, X_train, y_train,cv=10)\nprint(metrics.classification_report(y_train, y_pred))\n", "intent": "Another method of evaluting this model is to use <b>cross validation</b>.\n"}
{"snippet": "y_predicted = text_clf.predict(X_test)\n", "intent": "Get the predicted classes for X_test\n"}
{"snippet": "from sklearn import metrics\nmetrics.accuracy_score(y_test, y_predicted)\n", "intent": "Calculate accuracy of class predictions\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_predicted, digits=4))\n", "intent": "Print the classification report\n"}
{"snippet": "documents_to_predict = [\"Home Runs Game\", \"Car Engine Noises\", \"Computer GIFs\"]\npredictions = text_clf.predict(documents_to_predict)\n", "intent": "Document contents to list form\n"}
{"snippet": "metric.accuracy_score(y_true, y_pred, normalize=False)\n", "intent": "The absolute accuracy is measured as follow.\n"}
{"snippet": "def train(self, training_data, epochs, mini_batch_size, learning_rate):\n    training_data = list(training_data)\n    for i in range(epochs):\n        mini_batches = self.create_mini_batches(training_data, mini_batch_size)\n        cost = sum(map(lambda mini_batch: self.update_params(mini_batch, learning_rate), mini_batches))\n        acc = self.evaluate(training_data)\n        print(\"Epoch {} complete. Total cost: {}, Accuracy: {}\".format(i, cost, acc))\n", "intent": "We shall implement backpropagation with stochastic mini-batch gradient descent to optimize our network. \n"}
{"snippet": "from IPython.display import HTML\ndef classify(img):\n    img = img[len('data:image/png;base64,'):].decode('base64')\n    img = cv2.imdecode(np.fromstring(img, np.uint8), -1)\n    img = cv2.resize(img[:,:,3], (28,28))\n    img = img.astype(np.float32).reshape((1, 784))/255.0\n    return model.predict(img)[0].argmax()\nHTML(html+script)\n", "intent": "Now, try if your model recognizes your own hand writing.\nWrite a digit from 0 to 9 in the box below. Try to put your digit in the middle of the box.\n"}
{"snippet": "Res50_predictions = [np.argmax(Res50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Res50]\ntest_accuracy = 100*np.sum(np.array(Res50_predictions)==np.argmax(test_targets, axis=1))/len(Res50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "outer_cv = 5\ninner_cv = 2\ngs = GridSearchCV(estimator=pipe_svc,\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  cv=inner_cv)\nscores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=outer_cv)\nprint('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n", "intent": "How to code?\nUse grid search instead of classifier/pipeline with cross_val_score()\n"}
{"snippet": "from sklearn.metrics import precision_score, recall_score, f1_score\nprint('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\nprint('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\nprint('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n", "intent": "Previously we have been using accuracy as the score for optimization\n* e.g. grid search\nWe can easily plug in other performance metrics.\n"}
{"snippet": "from scipy.misc import comb\nimport math\nimport numpy as np\ndef ensemble_error(num_classifier, base_error):\n    k_start = math.ceil(num_classifier/2)\n    probs = [comb(num_classifier, k)*(base_error**k)*((1-base_error)**(num_classifier-k)) for k in range(k_start, num_classifier+1)]\n    return sum(probs)\n", "intent": "We have used the following code to compute and plot the ensemble error from individual classifiers for binary classification:\n"}
{"snippet": "y_lin_pred = lr.predict(X)\ny_quad_pred = pr.predict(X_quad)\n", "intent": "Quadratic polynomial fits this dataset better than linear polynomial\nNot always a good idea to use higher degree functions\n* cost\n* overfit\n"}
{"snippet": "y_train_pred = nn.predict(X_train)\nif sys.version_info < (3, 0):\n    acc = ((np.sum(y_train == y_train_pred, axis=0)).astype('float') /\n           X_train.shape[0])\nelse:\n    acc = np.sum(y_train == y_train_pred, axis=0) / X_train.shape[0]\nprint('Training accuracy: %.2f%%' % (acc * 100))\n", "intent": "The process converged around 800 epochs.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(knn_pipeline, x_train, y_train, cv=3)\n", "intent": "As promised, cross validation tools work directly with the pipeline object.\n"}
{"snippet": "meta_test_predictions = np.mean(predictions_test,axis=1) >.5 \nprint(\"Test accuracy (Classify by majority vote): \", accuracy_score(y_test, meta_test_predictions))\n", "intent": "Perhaps the simplest way of combining the models predictions (above) is a majority vote. Let's compute the test score under this rule\n"}
{"snippet": "tuning_scores = [x.score(x_tune,y_tune) for x in models]\nweights = np.array(tuning_scores)/np.sum(tuning_scores)\nprint(\"First five weights:\", weights[0:5])\nweighted_predictions = np.dot(predictions_test, weights) > .5\naccuracy_score(y_test,weighted_predictions)\n", "intent": "**Speaker note**: point out that we've got the old one-tuning-set problem, and CV can assist\n"}
{"snippet": "def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print('>', pair[0])\n        print('=', pair[1])\n        output_words, attentions = evaluate(encoder, decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n        print('<', output_sentence)\n        print('')\n", "intent": "We can evaluate random sentences from the training set and print out the\ninput, target, and output to make some subjective quality judgements:\n"}
{"snippet": "linreg_test_prediction = X_test_level2[:, 0]\nlbg_test_prediction = X_test_level2[:, 1]\ntest_preds = best_alpha * linreg_test_prediction + (1 - best_alpha) * lbg_test_prediction \nr2_test_simple_mix = r2_score(y_test, test_preds) \nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print (metric)\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=skf_seed)\n    NNF = NearestNeighborsFeats(n_jobs=4, k_list=k_list, metric=metric)\n    preds = cross_val_predict(NNF, X, y=Y, cv=skf)\n    np.save(data_path.joinpath(f'knn_feats_{metric}_train.npy'), preds)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "y_pred = add_noise(clf.predict_proba(test_ada[variables])[:, 1])\n", "intent": "Compute prediction and add noise\n"}
{"snippet": "pred = km.predict(iris_test.iloc[:, :4])\ncenters = np.array([list(km.cluster_centers_[lab]) for lab in pred])\ndistances = np.sqrt(np.sum((iris_test.iloc[:, :4] - centers)**2, axis=1))\nis_out = [distances[i] > percentiles[lab] for i, lab in enumerate(pred)]\n", "intent": "3 - Use these percentiles to determine outliers in the test data and create a 3d plot using the first three attributes and highlight outliers\n"}
{"snippet": "print(confusion_matrix(y_true=ytest, y_pred=ypred))\nprint('TPR', recall_score(ytest, ypred))\nprint('PRE', precision_score(y_true=ytest, y_pred=ypred))\nprint('F1', f1_score(y_true=ytest, y_pred=ypred))\nprint('ACC', accuracy_score(y_true=ytest, y_pred=ypred))\n", "intent": "5 - Check your results in part (4) using `sklearn`.\n"}
{"snippet": "ypredprob = pipe.predict_proba(Xtest)\n", "intent": "6 - Plot the precision and recall curve for your fit.\n"}
{"snippet": "r2_score(y_1var, y_1var_pred)\n", "intent": "The slope of the equation represents the rate of change of `r` with respect to `double`.\n"}
{"snippet": "score = SegModel.evaluate(valX, valY, verbose=0)\nprint('Final Dice on validation set: {:.04f}'.format(1-score))\n", "intent": "After the training is complete, we evaluate the model again on our validation data to see the results.\n"}
{"snippet": "score = RegModel.evaluate(testX, testY, verbose=0)\nprint('Final loss on test set: {:.03e}'.format(score))\n", "intent": "After the training is complete, we evaluate the model on our test data to see the results.\n"}
{"snippet": "scores = cross_val_score(clf, iris.data, iris.target, cv=5)\nprint(scores)\nprint(scores.mean())\n", "intent": "K-Fold cross validation is just as easy; let's use a K of 5:\n"}
{"snippet": "labels = kmeans.predict(X)\n", "intent": "Encontra os clusters para cada motivo de compra.\n"}
{"snippet": "y_pred = list(classifier.predict(X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "y_pred = rfmodel.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(data['Cluster'], kmeans.labels_))\nprint('\\n')\nprint(classification_report(data['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "preds = model.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions = logreg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred = model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predictions = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "y_pred = svclassifier.predict(X_test)  \n", "intent": "**Question 3.8:** Use the classifier to predict the outcome of our `X_test`. \n"}
{"snippet": "from theano import tensor as T\nraise NotImplementedError(\"TODO: add any other imports you need\")\ndef evaluate(x, y, expr, x_value, y_value):\n    raise NotImplementedError(\"TODO: implement this function.\")\nx = T.iscalar()\ny = T.iscalar()\nz = x + y\nassert evaluate(x, y, z, 1, 2) == 3\nprint(\"SUCCESS!\")\n", "intent": "This exercise requires you to compile a Theano function and call it to execute `\"x + y\"`.\n"}
{"snippet": "ypred = lm_fit.predict(xsample)\nTSS = np.sum((ysample - np.mean(ysample))**2)\nRSS = np.sum((ysample - ypred)**2)\nRsq_sample = (TSS - RSS)/TSS\nRsq_sample\n", "intent": "First let's compute the Rsquared for the last sample:\n"}
{"snippet": "X_pred = np.arange(0,100).reshape(-1,1)\nyhat = neigh.predict(X_pred)\n", "intent": "Let's create a vector of predictions:\n"}
{"snippet": "def print_rmse(model, name, df):\n  metrics = model.evaluate(input_fn = make_input_fn(df, 1))\n  print ('RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss'])))\nprint_rmse(model, 'validation', df_valid)\n", "intent": "Evaluate on the validation data (we should defer using the test data to after we have selected a final model).\n"}
{"snippet": "from math import sqrt\nfrom sklearn.metrics import mean_squared_error\ndef evaluate_forecasts(test, forecasts, n_seq):\n    for i in range(n_seq):\n        actual = [row[i] for row in test]\n        predicted = [forecast[i] for forecast in forecasts]\n        rmse = sqrt(mean_squared_error(actual, predicted))\n        print('t+%d RMSE: %f' % ((i + 1), rmse))\nevaluate_forecasts(actual, forecasts, 30)\n", "intent": "Evaluate the RMSE for each forecast time step\n"}
{"snippet": "from math import sqrt\nfrom sklearn.metrics import mean_squared_error\ndef evaluate_forecasts(test, forecasts, n_lag, n_seq):\n    for i in range(n_seq):\n        actual = [row[i] for row in test]\n        predicted = [forecast[i] for forecast in forecasts]\n        rmse = sqrt(mean_squared_error(actual, predicted))\n        print('t+%d RMSE: %f' % ((i + 1), rmse))\nevaluate_forecasts(actual, forecasts, 1, 30)\n", "intent": "Evaluate the RMSE for each forecast time step\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy Xception: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predicted_y = list(classifier.predict(input_fn=pred_fn))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "predicted_y = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "predicted_y = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "precision_score(y_true, y_pred, average=None)\n", "intent": "Precision, recall, and F1-scores for each class (without averaging)\n"}
{"snippet": "predicted_y = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predicted_Y = lr.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predicted_y = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predicted_y = Pipeline_model.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "predicted_y = svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "bikes.loc[:,'predictions'] = lr_all.predict(X)\n", "intent": "- Store `lr_all`'s fitted values in a new `predictions` column of the `bikes` DataFrame.\n"}
{"snippet": "bikes.loc[:, 'predictions'] = lr_all.predict(X)\n", "intent": "- Store `lr_all`'s fitted values in a new `predictions` column of the `bikes` DataFrame.\n"}
{"snippet": "AdB_scores = cross_val_score(AdB, X, Y)\nAdB_scores.mean()\n", "intent": "Now compare CV scores\n"}
{"snippet": "tree_scores = cross_val_score(clf, X, Y)\ntree_scores.mean()\n", "intent": "What about a plain tree?\n"}
{"snippet": "precision_score(y_true, y_pred, average='micro')\n", "intent": "Micro-averaged precision, recall, and F1\n"}
{"snippet": "predicitons = model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predict = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint('MSE - OLS train: %.3f, test: %.3f' % (\n        mean_squared_error(ytrain, ytrain_pred), \n        mean_squared_error(ytest, ytest_pred)))\nprint('R^2 0LS train: %.3f, test: %.3f' % (\n        r2_score(ytrain, ytrain_pred),\n        r2_score(ytest, ytest_pred)))\n", "intent": "Now since we have already trained and tested our model , we need to evaluate using MSE and R^2\n"}
{"snippet": "from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\ndef rsquare_meansquare_error(train_y, test_y, train_X, test_X, test, best_model):\n    print ('MSE ' + test + ' train data: %.2f, test data: %.2f' % (\n        mean_squared_error(train_y, y_train_pred),\n        mean_squared_error(test_y, y_test_pred)))\n", "intent": "You need to know if your model performed well on the test data. \nEvaluation using MSE and R^2\n"}
{"snippet": "rsquare_meansquare_error(y_train, y_test, X_train, X_test, \"Random Forest Regression tree\", rfr_best)\n", "intent": "You need to know if your model performed well on the test data. \nEvaluation using MSE and R^2.\nWe already created the function above\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(Universities['Cluster'],kmeans.labels_))\nprint(classification_report(Universities['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "Now that we have fit our model, let's evaluate its performance by predicting off the test values!\n"}
{"snippet": "Prediction=Logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "metrics.accuracy_score(df['target'], df['labels'])\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "precision_score(y_true, y_pred, average='macro')\n", "intent": "Macro-averaged precision, recall, and F1\n"}
{"snippet": "print metrics.classification_report(df['target'], df['labels'])\n", "intent": "Calculate the Precision, Recall, and F - Score to see the test's accuracy\n"}
{"snippet": "logodds = logreg.predict_proba(2)[:, 1] + 4.1804038614510901\nodds = np.exp(logodds)\nprob = odds/(1 + odds)\nprob\n", "intent": "**Interpretation:** A 1 unit increase in 'al' is associated with a 4.18 unit increase in the log-odds of 'assorted'.\n"}
{"snippet": "random_pred = forest.predict_proba(X_test)\nfpr, tpr, _ = roc_curve(y_test, random_pred[:, 1])\nroc_auc = auc(fpr, tpr)\nprint(\"Area under the ROC curve : %f\" % roc_auc) \n", "intent": "Print the AUC for Random Forest\n"}
{"snippet": "print('Estimated number of clusters: %d' % n_clusters)\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(y, labels))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels))\n", "intent": "Now, we can use a handy chunk of code from the Scitkit documentation to measure the performance of our model \n"}
{"snippet": "metrics.silhouette_score(new_X, labels, metric='euclidean')\n", "intent": "Compute the Silhoutte Score\n"}
{"snippet": "test_preds = best_alpha * X_test_level2[:,0] + (1 - best_alpha) * X_test_level2[:,1]\nr2_test_simple_mix = r2_score(y_test, test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "train_preds = lr_boh.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2, train_preds)\ntest_preds = lr_boh.predict(X_test_level2)\nr2_test_stacking = r2_score(y_test, test_preds)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "pred_lr = lr.predict(test)\npred_lgb = model_lgb.predict(test)\nX_test_level2 = np.c_[pred_lr, pred_lgb]\n", "intent": "Now I predict on the test set & concatenate test predictions to get test meta-features.\n"}
{"snippet": "pred_lr = lr.predict(X_test.values)\npred_lgb = model_lgb.predict(X_test)\nX_test_level2 = np.c_[pred_lr, pred_lgb] \n", "intent": "Now I predict on the test set & concatenate test predictions to get test meta-features.\n"}
{"snippet": "y_est = model.predict(x_test)\nmse = np.mean(np.square(y_test_true-y_est))\nprint(mse)\n", "intent": "This model is slightly better than without any penalty on the weights.\n"}
{"snippet": "model.predict([\n        [+10.0, -10.0],   \n        [-10.0, +10.0],   \n])\n", "intent": "Which class is \"1\"?  Yellow or Magenta?  Let's classify some extreme points to find out:\n"}
{"snippet": "pointsRDD.map(lambda pt: (model.predict(pt.features), pt.label)).take(10)\n", "intent": "Let's compare the predictions to the real values for the first few points:\n"}
{"snippet": "loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred) \nprint(loss)\n", "intent": "Define the loss function.\n"}
{"snippet": "def myGPpredict(x_new, x_data, y_data, K_inv, theta, sig):\n    K_et = cov_vect(x_new,x_data,theta,sig)\n    mu = K_et.dot(K_inv.dot(y_data))\n    k_xx = cov_function(x_new,x_new,theta,sig)\n    sigma = k_xx - K_et.dot(K_inv.dot(K_et.T))\n    return mu[0],sigma\n", "intent": "1.1 Code the function that predict the mean and the standard deviation of the Gaussian process using the previous defined function\n"}
{"snippet": "print y_test.shape\nprint X_test.shape\nprint X_test[:5,1]\nX_test_centered = X_test-np.mean(X_test,axis=0)\nkFold = sklearn.cross_validation.KFold(X_test.shape[0],n_folds=5)\nscore =[]\nfor _,idx in kFold:\n    score.append(sklearn.metrics.accuracy_score(y_test[idx],clf.predict(X_test[idx])))\nprint np.mean(score), np.std(score)\n", "intent": "Test the performance of our tuned KNN classifier on the test set.\n"}
{"snippet": "clflog.predict(Xtest)\n", "intent": "In `sklearn`, `clf.predict(test_data)` makes predictions on the assumption that a 0.5 probability threshold is the appropriate thing to do.\n"}
{"snippet": "confusion_matrix(ytest, t_repredict(clflog, 0.1, Xtest))\n", "intent": "See how the false negatives get suppressed?\n"}
{"snippet": "metrics.f1_score(y_train, clf.predict(X_train))\n", "intent": "The over-fitting we saw previously can be quantified by computing the\nf1-score on the training data itself:\n"}
{"snippet": "expected = y_test\npredicted = model.predict(X_test)\naccuracy = accuracy_score(expected, predicted)\nprint( \"Accuracy = \" + str( accuracy ) )\n", "intent": "Let's see how we're doing in terms of precision, recall, and accuracy on our test set.\n"}
{"snippet": "y_est = model.predict(x_test)[:,None]\nmse = np.mean(np.square(y_test_true-y_est))\nprint(mse)\n", "intent": "The MSE is significantly better than both the above models.\n"}
{"snippet": "print(\"Fit a model X_train, and calculate MSE with Y_train:\", np.mean((Y_train - lm.predict(X_train)) ** 2))\nprint(\"Fit a model X_train, and calculate MSE with X_test, Y_test:\", np.mean((Y_test - lm.predict(X_test)) ** 2))\n", "intent": "Now, calculate the mean squared error using just the test data and compare to mean squared from using all the data to fit the model. \n"}
{"snippet": "print(np.sum((faithful.eruptions - resultsW0.predict(X)) ** 2))\n", "intent": "The residual sum of squares: \n"}
{"snippet": "print(np.mean((faithful.eruptions - resultsW0.predict(X)) ** 2))\n", "intent": "Mean squared error: \n"}
{"snippet": "y_model = model.predict(Xtest)\n", "intent": "Now let's use the model.predict method to see what values our model predict for the data. We'll use the X test data we set aside a few cells before. \n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "Great, now lets use y_model and compare it to the test data we set aside and see how our model performed!\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test,predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=1)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "predictions = list(classifier.predict(X_test, as_iterable = True))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "prediction = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "model.evaluate(test_x, test_data['cnt'], batch_size=256)\n", "intent": "Try and beat my score of 0.36\n"}
{"snippet": "print(classification_report(y_test, prediction_rfc))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "pred = mnb.predict(msg_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "print(confusion_matrix(label_test, pred))\nprint(classification_report(label_test, pred))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "pred2 = pipeline.predict(msg2_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "pred = model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "predicted_y = np.zeros((test_len,2))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,2)\n    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\npredicted_y =np.argmax(predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)\n", "intent": "<h2> 4.4 Building a random model (Finding worst-case log-loss) </h2>\n"}
{"snippet": "roc_values = []\nfor feature in ['Sex', 'Cabin', 'Embarked', 'Cabin']:\n    roc_values.append(roc_auc_score(y_test, X_test_enc[feature])) \n", "intent": "The strings were replaced by probabilities.\n"}
{"snippet": "test_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.1.1.3. Feature Importance, Correctly classified point</h4>\n"}
{"snippet": "test_point_index = 100\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.1.1.4. Feature Importance, Incorrectly classified point</h4>\n"}
{"snippet": "pred_y = grid.predict(test_x)\n", "intent": "Finally predict the values on the actual test set with given params:\n"}
{"snippet": "test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.3.2.4. Feature Importance, Inorrectly Classified point</h4>\n"}
{"snippet": "test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.3.3.2. For Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 100\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actuall Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.5.3.2. Inorrectly Classified point</h4>\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "Now we make some predictions on our model\nWe give the model data it hasnt see yet\n"}
{"snippet": "vals = np.array([100,30,10000,7,3]).reshape(1,-1)\nmodel.predict(vals)\n", "intent": "Now we can use the model to predict avg grade for a given set of features\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "predict the test set \n"}
{"snippet": "lm.predict(np.array([80000, 5, 6, 4,20000]).reshape(1, -1))\n", "intent": "Estimate the value of the following house:<br> \n* 6 rooms \n* 4 bedrooms\n* Area income: 80000\n* 5 years old\n* Area population: 20000\n"}
{"snippet": "logmodel.predict(np.array([3, 59, 0, 14]).reshape(1, -1))\n", "intent": "Check if email with 3 rec, len=59, no attachment and subject len=14 is a spam\n"}
{"snippet": "logmodel.predict(np.array([12, 159, 0, 24]).reshape(1, -1))\n", "intent": "Check if email with 12 rec, len=159, no attachment and subject len=24 is a spam\n"}
{"snippet": "model.evaluate(test_x, test_data['cnt'], batch_size=256)\n", "intent": "It's important that you understand the difference between `model.predict` and `model.evaluate`.\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nprint ('S Score with the Transformed Data', silhouette_score(seeds_new_transform, km_fit.labels_))\nprint ('S Score with the  Non Transformed Data', silhouette_score(seeds_new, km_fit.labels_))\n", "intent": "_(pairplot with hue)_\n"}
{"snippet": "y_test_predict_logreg = logregcv_fit.predict(X_test)\ny_logreg_pp = logregcv_fit.predict_proba(X_test)\n", "intent": "**9.B Calculate the predicted labels and predicted probabilities on the test set with the Ridge logisitic regression.**\n"}
{"snippet": "yhat_ridge = lr_ridge.predict(X_test)\nyhat_ridge_pp = lr_ridge.predict_proba(X_test)\n", "intent": "**9.B Calculate the predicted labels and predicted probabilities on the test set with the Ridge logisitic regression.**\n"}
{"snippet": "knn1_predictions = knn1_model.predict(X)\nacc = metrics.accuracy_score(y.values, knn1_predictions)\nprint(acc)\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "logreg.predict_proba([[0]])\n", "intent": "The probability of admittance with an average gpa:\n"}
{"snippet": "ridge_pred = lr_ridge.predict(X_test)\nridge_pp = lr_ridge.predict_proba(X_test)\n", "intent": "**9.B Calculate the predicted labels and predicted probabilities on the test set with the Ridge logisitic regression.**\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccs = cross_val_score(knn, X, y, cv=10)\nprint accs\nprint np.mean(accs)\n", "intent": "Use 10 folds. How does the mean performace across folds compare to the baseline accuracy?\n"}
{"snippet": "y_pred = knn.predict(X_test)\ny_pp = knn.predict_proba(X_test)\n", "intent": "**Calculate the predicted labels and predicted probabilities on the test set.**\n"}
{"snippet": "def run_gradient_descent(X, y, initial_beta_array, step_size, iterations=500):\n    beta_array = initial_beta_array\n    mses = []\n    mses.append(mean_squared_error(X, y, beta_array))\n    beta_arrays = []\n    for i in range(iterations):\n        beta_array = beta_update_function(X, y, beta_array, step_size)\n        mses.append(mean_squared_error(X, y, beta_array))\n        beta_arrays.append(beta_array)\n", "intent": "This is the function that wraps the gradient update with some number of iterations. It is the same, but takes an array of beta coefficients.\n"}
{"snippet": "logistic.predict_proba(x_test[:3])\n", "intent": "Predicting the probabilities for the first 3 images:\n"}
{"snippet": "elasticnet = ElasticNet(alpha=optimal_enet.alpha_, l1_ratio=optimal_enet.l1_ratio_)\nelasticnet_scores = cross_val_score(elasticnet, Xs, Y.values.ravel(), cv=10)\n", "intent": "---\nHow does it compare to the Ridge and Lasso regularized regressions?\n"}
{"snippet": "clusters = kmeans.fit_predict(iris_df)\ncentroids = kmeans.cluster_centers_\n", "intent": "**3.2 Compute the labels and centroids.**\n"}
{"snippet": "pred_prob=classifier.predict_proba(pred_test)\npred_prob[0,1]\n", "intent": "Instead of doing a Yes/No prediction, we can instead do a probability computation to show the probability for the prospect to buy the product\n"}
{"snippet": "browsing_data = np.array([1,0,1,0,0]).reshape(1, -1)\nprint(\"After checking reviews: propensity :\",classifier.predict_proba(browsing_data)[:,1] )\n", "intent": "It goes up. Next, he checks out reviews.\n"}
{"snippet": "new_data = np.array([100,0,50,0,0,0]).reshape(1, -1)\nnew_pred=model.predict(new_data) \nprint(\"The CLV for the new customer is : $\",new_pred[0])\n", "intent": "Let us say we have a new customer who in his first 3 months have spend 100,0,50 on your website. Let us use the model to predict his CLV.\n"}
{"snippet": "print (model.n_iter_)\nprint (silhouette_score(df[features], df['clusters']))\n", "intent": "<a id='scaling'></a>\n"}
{"snippet": "from sklearn.model_selection import cross_val_score, cross_val_predict\naccs = cross_val_score(knn,X,y, cv=10)\nprint (accs)\nprint (np.mean(accs))\n", "intent": "Use 10 folds. How does the mean performace across folds compare to the baseline accuracy?\n"}
{"snippet": "y_pred  = knn.predict(X_test)\ny_pp = knn.predict_proba(X_test)\nprint (y_pp)\n", "intent": "**Calculate the predicted labels and predicted probabilities on the test set.**\n"}
{"snippet": "y_pred  = knn.predict(X_test)\ny_pp = knn.predict_proba(X_test)\nprint (y_pred)\nprint (y_pp)\n", "intent": "**Calculate the predicted labels and predicted probabilities on the test set.**\n"}
{"snippet": "logistic.predict_proba(test_x[:3].reshape(3,-1))\n", "intent": "Predicting the probabilities for the first 3 images:\n"}
{"snippet": "print('All features')\nscores = cross_val_score(lr, X, y)\nprint(scores, '\\n', 'Mean: ', scores.mean())\nselected_xs = df.iloc[:, rfe.support_].copy()\nprint('Feature Selection via RFE (3 columns)')\nscores = cross_val_score(lr, selected_xs, y)\nprint(scores, '\\n', 'Mean: ', scores.mean())\n", "intent": "We can then compare how well this performs compared to all columns:\n"}
{"snippet": "print(pipeline.score(X_train, y_train))\npredictions = pipeline.predict(X_train)\nprint(confusion_matrix(y_train, predictions))\nprint(classification_report(y_train, predictions))\n", "intent": "Then we'll score it and run the predictions from the training set.\n"}
{"snippet": "print(pipeline.score(X_test, y_test))\npredictions = pipeline.predict(X_test)\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))\n", "intent": "And finally, we'll score and predict with our test set. We do not need to refit this to the test set!\n"}
{"snippet": "print(model1.predict(X_test)[0:5])\n", "intent": "Or we can get predictions as well:\n"}
{"snippet": "scores = cross_val_score(lr, X, boston_y, cv=5)\nprint(scores.mean(), scores.std())\n", "intent": "We may also want to see an overall score for these folds. Typically, we will take the average $R^2$ across each fold:\n"}
{"snippet": "scores = cross_val_score(lr, X, boston_y, cv=5)\nprint(scores.mean())\n", "intent": "We may also want to see an overall score for these folds. Typically, we will take the average $R^2$ across each fold:\n"}
{"snippet": "confusion_matrix(set1['ytest'], digitstwo_log_set1.predict(set1['Xtest']))\n", "intent": "> YOUR TURN NOW: Calculate the confusion matrix for the regularized logistic regression\n"}
{"snippet": "confusion_matrix(set1['ytest'], digitstwo_log_set2.predict(set1['Xtest']))\n", "intent": "From the department of not-kosher things to do, (why?) we calculate the performance of this classifier on `set1`.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(setf['ytest'], digitspca_log2.predict(setf['Xtest']), )\n", "intent": "And a confusion matrix...\n"}
{"snippet": "logistic.predict_proba(x_test[:3].reshape(3,-1))\n", "intent": "Predicting the probabilities for the first 3 images:\n"}
{"snippet": "evaluate(predicted_knn, iris_test)\n", "intent": "Then let's apply this function to our predictions:\n"}
{"snippet": "np.mean((regr.predict(x_test)-y_test)**2)\n", "intent": "Now, we will compute metrics that can be used to assess fit:\n"}
{"snippet": "poly.predicted = lg.predict(x_test_poly)\npoly.predicted\n", "intent": "Now we can also do prediction:\n"}
{"snippet": "y_hat_train_0_1 = y_hat_train >.5\ny_hat_test_0_1 = y_hat_test >.5\nprint(\"Training Set Accuracy for Linear Regression: \", accuracy_score(y_train, y_hat_train_0_1))\nprint(\"Test Set Accuracy for Linear Regression: \", accuracy_score(y_test, y_hat_test_0_1))\n", "intent": "Notice it is possible for us to convert the above values into a binary classification by the following code.\n"}
{"snippet": "predicted = logit.predict(test[['female','First','Second','age']])\nthreshold = .5\nexpected = test['survived']\nprint(metrics.classification_report(expected, predicted))\n", "intent": "What does the blue line actually mean in this scenario? How might our model be different if we were at a different position on the line? \n"}
{"snippet": "predicted = logit.predict(test[['female','First','Second','age']])\nthreshold = .5\nexpected = test['survived']\nprint(metrics.classification_report(expected, predicted))\n", "intent": "What does the red line actually mean in this scenario? How might our model be different if we were at a different position on the line? \n"}
{"snippet": "score = lambda model, x_test, y_test: pd.Series([model.score(x_test, y_test), \n                                                 model.score(x_test[y_test==0], y_test[y_test==0]),\n                                                 model.score(x_test[y_test==1], y_test[y_test==1]), \n                                                 model.score(x_test[y_test==2], y_test[y_test==2]), \n                                                 cost(y_test, model.predict(x_test))],\n                                                index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1', 'accuracy on class 2', 'total cost'])\n", "intent": "We'll define a function for computing the accuracy rates so that we can conveniently call it later.\n"}
{"snippet": "scores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=5)\nscores\n", "intent": "K-Fold cross validation is just as easy; let's use a K of 5:\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"foto7.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "letter = [char2int[letter] for letter in \"white supremacists are \"]\nsentence = [int2char[l] for l in letter]\nfor i in range(150):\n    if sentence[-1]=='<END>':\n        break\n    p = model.predict(np.array(letter)[None,:])\n    letter.append(np.random.choice(len(char2int),1,p=p[0][-1])[0])\n    sentence.append(int2char[letter[-1]])\nprint(''.join(sentence))\n", "intent": "Feel free to change the starting sentence as you please. But remember simple letters.\n"}
{"snippet": "predictions = lm.predict(X_test)\nprint (\"Type of the predicted object:\", type(predictions))\nprint (\"Size of the predicted object:\", predictions.shape)\n", "intent": "**Prediction using the lm model**\n"}
{"snippet": "print(\"Mean absolute error (MAE):\", metrics.mean_absolute_error(y_train,train_pred))\nprint(\"Mean square error (MSE):\", metrics.mean_squared_error(y_train,train_pred))\nprint(\"Root mean square error (RMSE):\", np.sqrt(metrics.mean_squared_error(y_train,train_pred)))\n", "intent": "**Regression evaluation metrices for train**\n"}
{"snippet": "print(\"Mean absolute error (MAE):\", metrics.mean_absolute_error(y_test,predictions))\nprint(\"Mean square error (MSE):\", metrics.mean_squared_error(y_test,predictions))\nprint(\"Root mean square error (RMSE):\", np.sqrt(metrics.mean_squared_error(y_test,predictions)))\n", "intent": "**Regression evaluation metrices for test**\n"}
{"snippet": "def rmse(predictions, actuals): \n    return math.sqrt(((predictions-actuals)**2).mean())\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train),  \n           rmse(m.predict(X_valid), y_valid),  \n           m.score(X_train, y_train),  \n           m.score(X_valid, y_valid)]  \n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(\"RMSE Train\\t\\tValid\\t\\t   R2 train\\t\\tR2 valid\\t\\t OOB\\n\", res)\n", "intent": "Let's try our model again, this time with separate training and validation sets.\n"}
{"snippet": "metrics.r2_score(preds, y_valid)\n", "intent": "Somehow our $R^2$ is better than this... its ok...\n"}
{"snippet": "y_pred = pipe_base_model_cv.predict(X_test)\nscore = f1_score(y_test, y_pred, average='macro')\nprint(score)\n", "intent": "Accuracy on test set\n"}
{"snippet": "y_testpred = model.predict(x_test)\ntpredictions = [round(value) for value in y_testpred]\ntaccuracy = accuracy_score(y_test, tpredictions)\nprint(\"Train Accuracy : %.2f%%\" % (taccuracy * 100.0))\n", "intent": "The Feature Importance plot shows which features are contributing more for the model development. The more the F1 Score the more important they are.\n"}
{"snippet": "split = 0.75\nX_train, X_test, y_train, y_test = load_dataset(split)\nprint('Training set: {0} samples'.format(X_train.shape[0]))\nprint('Test set: {0} samples'.format(X_test.shape[0]))\nk = 3\ny_pred = predict(X_train, y_train, X_test, k)\naccuracy = compute_accuracy(y_pred, y_test)\nprint('Accuracy = {0}'.format(accuracy))\n", "intent": "Should output an accuracy of 0.9473684210526315.\n"}
{"snippet": "predictions = regr.predict(test[x])\nSSreg = np.mean((predictions - test[y]) ** 2)\nSStot =  np.mean((test[y] - np.mean(test[y])) ** 2)\nprint 1 - SSreg/SStot\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\nknn.predict(X_new)\n", "intent": "- Returns a NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "y_pred_train = regressor.predict(X_train)\n", "intent": "And predict. First let us try the training set:\n"}
{"snippet": "y_pred_test = regressor.predict(X_test)\n", "intent": "Let's try the test set:\n"}
{"snippet": "cv = ShuffleSplit(len(iris.target), n_iter=5, test_size=.2)\ncross_val_score(classifier, X, y, cv=cv)\n", "intent": "You can use all of these cross-validation generators with the cross_val_score method:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))\n", "intent": "f1-score is the geometric average of precision and recall.\n"}
{"snippet": "y_test = gaussian.predict(test_df)\n", "intent": "Ahora ya podemos aplicar el modelo a nuestros datos de test:\n"}
{"snippet": "roc_knn_p = roc_curve(y_test, result_k_p.predict_proba(x_test).T[1])\ngetROCcurve(roc_knn_p, 'ROC Curve Optimized for Precision w/ AUC: ')\n", "intent": "[See the sklearn plotting example here.](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\nprint(\"Done: {}\".format(getTime()))\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\nprint(\"Done: {}\".format(getTime()))\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in testData]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\nprint(\"Done: {}\".format(getTime()))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y, y_pred))\n", "intent": "Classification accuracy:\n- **Proportion** of correct predictions\n- Common **evaluation metric** for classification problems\n"}
{"snippet": "y_predict = KNN.predict(X_train)\n", "intent": "Use the model to make a prediction on the test set. Assign the prediction to a variable named y_pred\n"}
{"snippet": "from sklearn.metrics import classification_report\ny_pred = GV_GBC.predict(X_test)\nprint(classification_report(y_pred, y_test))\n", "intent": "The error metrics. Classification report is particularly convenient for multi-class cases.\n"}
{"snippet": "y_pred = LR_L2.predict(X_test)\nprint(classification_report(y_pred, y_test))\n", "intent": "Check the errors and confusion matrix for the logistic regression model.\n"}
{"snippet": "y_pred = list()\ny_prob = list()\ncoeff_labels = ['lr', 'l1', 'l2']\ncoeff_models = [lr, lr_l1, lr_l2]\nfor lab,mod in zip(coeff_labels, coeff_models):\n    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\ny_pred = pd.concat(y_pred, axis=1)\ny_prob = pd.concat(y_prob, axis=1)\ny_pred.head()\n", "intent": "* Predict and store the class for each model.\n* Also store the probability for the predicted class for each model. \n"}
{"snippet": "y_pred = list()\ny_prob = list()\ncoeff_labels = ['lr', 'l1', 'l2']\ncoeff_models = [lr, lr_l1, lr_l2]\nfor lab,mod in zip(coeff_labels, coeff_models):\n    y_pred.append(pd.Series(mod.predict(X_test_new), name=lab))\n    y_prob.append(pd.Series(mod.predict_proba(X_test_new).max(axis=1), name=lab))\ny_pred = pd.concat(y_pred, axis=1)\ny_prob = pd.concat(y_prob, axis=1)\ny_pred.head()\n", "intent": "* Predict and store the class for each model.\n* Also store the probability for the predicted class for each model. \n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\ndef measure_error(y_true, y_pred, label):\n    return pd.Series({'accuracy':accuracy_score(y_true, y_pred),\n                      'precision': precision_score(y_true, y_pred),\n                      'recall': recall_score(y_true, y_pred),\n                      'f1': f1_score(y_true, y_pred)},\n                      name=label)\n", "intent": "A function to return error metrics.\n"}
{"snippet": "y_train_pred_gr = GR.predict(X_train)\ny_test_pred_gr = GR.predict(X_test)\ntrain_test_gr_error = pd.concat([measure_error(y_train, y_train_pred_gr, 'train'),\n                                 measure_error(y_test, y_test_pred_gr, 'test')],\n                                axis=1)\n", "intent": "These test errors are a little better than the previous ones. So it would seem the previous example overfit the data, but only slightly so.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_train_pred_gr_sugar = GR_sugar.predict(X_train)\ny_test_pred_gr_sugar  = GR_sugar.predict(X_test)\ntrain_test_gr_sugar_error = pd.Series({'train': mean_squared_error(y_train, y_train_pred_gr_sugar),\n                                         'test':  mean_squared_error(y_test, y_test_pred_gr_sugar)},\n                                          name='MSE').to_frame().T\ntrain_test_gr_sugar_error\n", "intent": "The error on train and test data sets. Since this is continuous, we will use mean squared error.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ny_train_pred = DTC.predict(X_train)\ny_test_pred  = DTC.predict(X_test)\ntrain_test_error = pd.Series({'train': mean_squared_error(y_train, y_train_pred),\n                                         'test':  mean_squared_error(y_test, y_test_pred)},\n                                          name='MSE').to_frame().T\ntrain_test_error\n", "intent": "A function to return error metrics.\n"}
{"snippet": "print((10 + 0 + 20 + 10)/4.)\nfrom sklearn import metrics\nprint(metrics.mean_absolute_error(true, pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "data_faces_pred_faces = reg_model_faces.predict(faces_response_vec.reshape(-1,1))[:,0]\nfaces_sse = np.sum((data_faces - data_faces_pred_faces) **2)\nfaces_sse\n", "intent": "Let's calculate the SSE for this faces model.\n"}
{"snippet": "data_faces_pred_full = reg_model_full.predict(designmat_full)[:,0]\nfull_sse = np.sum((data_faces-data_faces_pred_full)**2)\nprint('Just using Faces and bodies, SSE is: %.02f, adding all categories SSE is: %.02f' % (faces_bodies_sse, full_sse))\n", "intent": "And let's predict and calculate the SSE again, so we can compare it with the faces bodies model.\n"}
{"snippet": "data_faces_pred_faces = reg_model_faces.predict(faces_response_vec.reshape(-1,1))[:,0]\n", "intent": "As we can see, these values correspond to `slope` and `intercept` computed above. \nLet's see how good this model is by calculating the SSE:\n"}
{"snippet": "data_faces_pred_full = reg_model_full.predict(designmat_full)[:,1]\nfull_sse = np.sum((data_faces-data_faces_pred_full)**2)\nprint('Just using Faces and bodies, SSE is: %.02f, adding all categories SSE is: %.02f' % (faces_bodies_sse, full_sse))\n", "intent": "And let's predict and calculate the SSE again for the faces voxel, so we can compare it with the faces bodies model.\n"}
{"snippet": "pred_fake_train = model_fake.predict(fake_x_train)\ncorr_fake_train = correlate(pred_fake_train, fake_y_train)[0]\nprint('Correlation with training data: %.04f' % (corr_fake_train))\n", "intent": "Calculate model performance by correlating the predicted and real training data\n"}
{"snippet": "pred_fake_test = model_fake.predict(fake_x_test)\ncorr_fake_test = correlate(pred_fake_test, fake_y_test)[0]\nprint('Correlation with testing data: %.04f' % (corr_fake_test))\n", "intent": "Calculate model performance by correlating the predicted and real testing data\n"}
{"snippet": "pred_fake_train_outlier = model_fake_outlier.predict(fake_x_train)\ncorr_fake_train_outlier = correlate(pred_fake_train_outlier, fake_y_train_outlier)[0]\nprint('Correlation with training data: %.04f' % (corr_fake_train_outlier))\n", "intent": "Calculate model performance by correlating the predicted and real training data with an outlier \n"}
{"snippet": "pred_fake_test_outlier = model_fake_outlier.predict(fake_x_test)\ncorr_fake_test_outlier = correlate(pred_fake_test, fake_y_test)[0]\nprint('Correlation with testing data: %.04f' % (corr_fake_test_outlier))\n", "intent": "Calculate model performance by correlating the predicted and real testing data with an outlier\n"}
{"snippet": "pred_train = model_train.predict(designmat_response_train)\npred_test = model_train.predict(designmat_response_test)\n", "intent": "Now we'll predict the both the training and test data using the two design matrices we created above. \n"}
{"snippet": "print((10**2 + 0**2 + 20**2 + 10**2)/4.)\nprint(metrics.mean_squared_error(true, pred))\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "pred_gradient = model_gradient.predict(smoothed[1260:].reshape(-1,100))\npred_acc_gradient = correlate(data_test, pred_gradient)\npred_acc_gradient[pred_acc_gradient>.9] = np.nan\n", "intent": "Calculate the prediction accuracy of this gradient encoding model\n"}
{"snippet": "pred_sem3_train = model_semantic3.predict(designmat_sem3_train)\n", "intent": "Predict the training data and correlate those predictions with the real response amplitudes.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy_Resnet50 = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_Resnet50)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "xx, yy, zz, aa = np.mgrid[-0:10:0.5, 0:10:0.5, 0:10:0.5, 0:10:0.5]\ngrid = np.c_[xx.ravel(), yy.ravel(), zz.ravel(), aa.ravel()]\nprobs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n", "intent": "As you can see this is performing much better; however a few times the neural netweok is getting caught in the local minima\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nfrom sklearn.metrics import silhouette_samples\nlabels = km.labels_\nsilhouette_score(X,labels,metric='euclidean')\n", "intent": "But which is right?\nNote that the model also stores what the points for the model are in the `.labels_` attribute.\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nlabels = km.labels_\nsilhouette_score(X,labels,metric='euclidean')\n", "intent": "But which is right?\nNote that the model also stores what the points for the model are in the `.labels_` attribute.\n"}
{"snippet": "f1_score(y,y_pred)\n", "intent": "F1 score for ridge regression:\n"}
{"snippet": "from sklearn.metrics import mean_squared_error \ndef rmse(ytrue, ypredicted): \n    return np.sqrt(mean_squared_error(ytrue, ypredicted))\n", "intent": "* Write a function `rmse` that takes in truth and prediction values and returns the root-mean-squared error. User sklearn's `mean_squared_error`. \n"}
{"snippet": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\ndef measure_error(y_true, y_pred, label): \n    return pd.Series({'accuracy': accuracy_score(y_true, y_pred),\n                      'precision': precision_score(y_true, y_pred), \n                      'recall': recall_score(y_true, y_pred), \n                      'f1': f1_score(y_true, y_pred)}, \n                      name = label)\n", "intent": "There are 161 nodes and a maximum depth of 19.  A function to return error metrics. \n"}
{"snippet": "import numpy as np\nprint(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.))\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nprint \"RMSE:\", np.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "accuracy_score(Y, labels)\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "silhouette_score(X, Y)\n", "intent": "Compute the silhoutte coefficient to see how consistent our data is within the clusters\n"}
{"snippet": "print classification_report(Y, labels)\n", "intent": "Calculate the Precision, Recall, and F - Score to see the test's accuracy\n"}
{"snippet": "print('Training set r2: ', r2_score(train_results.TOTAL_VALUE, train_results.predicted))\nprint('Validation set r2: ', r2_score(valid_results.TOTAL_VALUE, valid_results.predicted))\n", "intent": "We can use the metrics that scikit-learn provides.\n"}
{"snippet": "classificationSummary(valid_y, rf.predict(valid_X))\n", "intent": "Confusion matrix and metrics\n"}
{"snippet": "from sklearn.metrics import classification_report\npredictions = grid_search.predict(X_test)\nprint grid_search.score(X_test, predictions)\nprint classification_report(y_test, predictions)\n", "intent": "By default, the `GridSearchCV` object will be re-fit using the best values of the hyperparameters.  \nPredictions can then be made using the object.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nprint cross_val_score(pipeline, X, y, n_jobs=-1)\n", "intent": " Now we will print the accuracies of the three cross validation folds.\n"}
{"snippet": "xx = np.linspace(0, 12, 100)\nxx = xx.reshape(xx.shape[0], 1)\nyy = regressor.predict(xx)\n", "intent": "The folloing will create some points that we can use to visualize the regression hyperlane.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "**Classification accuracy:** percentage of correct predictions\n"}
{"snippet": "loss, accuracy = model.evaluate(X_test, y_test)\nprint('loss:', loss)\nprint('accuracy:', accuracy)\n", "intent": "Now we can evaluate the model on the test data to get a sense of how we did.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nmat = confusion_matrix(labels, pred_labels)\nacc = accuracy_score(labels, pred_labels)\nprint(acc)\nprint(mat)\n", "intent": "Check the performance of the model\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nmat = confusion_matrix(test_labels, pred_labels)\nacc = accuracy_score(test_labels, pred_labels)\nprint(acc)\nprint(mat)\n", "intent": "Check the performance of the model\n"}
{"snippet": "predict = model.predict(test_images, verbose=1)\n", "intent": "Freeze the layers that remain in our model.\n"}
{"snippet": "print (\"Predicted %d, Label: %d\" % (list(classifier.predict(test_data[0:1]))[0], test_labels[0]))\ndisplay(0)\n", "intent": "We can make predictions on individual images as well. Note: the predict method accepts an array of samples as input, and returns a generator.\n"}
{"snippet": "def print_proba_table(prob_list, stride=1):\n    mnist_classes = [i for i in range(len(prob_list[0]))]\n    print(\"Class:\", *mnist_classes, sep=\"\\t\")\n    print(\"index\", *[\"---\" for i in range(len(mnist_classes))], sep=\"\\t\")\n    counter = 0\n    for prob in prob_list[::stride]:\n        print(counter*stride, *[round(prob[i], 3) for i in range(len(mnist_classes))], sep=\"\\t\")\n        counter += 1\nprint_proba_table(clf.predict_proba(X_test), stride=4)\n", "intent": "`clf.predict` tells us the actual predictions made on the test set.\n"}
{"snippet": "print_proba_table(clf.predict_proba(X_test), stride=10)\n", "intent": "Not so easy now, is it? But is 94.8% accuracy good \"enough\"? Depends on your application.\n"}
{"snippet": "print(confusion_matrix(y_test,rfc_pred))\nprint(classification_report(y_test,rfc_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(college_data['Cluster'],km.labels_))\nprint(classification_report(college_data['Cluster'],km.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "print((TP + TN) / float(TP + TN + FP + FN))\nprint(metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "**Classification Accuracy:** Overall, how often is the classifier correct?\n"}
{"snippet": "y_min_predicted = model.predict(x_min)\ny_max_predicted = model.predict(x_max)\nprint(f\"Actual Min Value: {y_min_actual}\")\nprint(f\"Predicted Min Value: {y_min_predicted}\")\nprint(f\"Actual Max Value: {y_max_actual}\")\nprint(f\"Predicted Max Value: {y_max_predicted}\")\n", "intent": "We can also use the predict function to calculate predicted values\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\npredicted = model.predict(X)\nmse = mean_squared_error(y, predicted)\nr2 = r2_score(y, predicted)\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"R-squared (R2 ): {r2}\")\n", "intent": "There are a variety of ways to quantify the model, but MSE and R2 are very common\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\npredictions = model.predict(X_test_scaled)\nMSE = mean_squared_error(y_test_scaled, predictions)\nr2 = model.score(X_test_scaled, y_test_scaled)\nprint(f\"MSE: {MSE}, R2: {r2}\")\n", "intent": "Step 5) Quantify your model using the scaled data\n"}
{"snippet": "def compute_loss(X, y, w):\n    X = expand(X)\n    return np.mean([max(0, 1 - y[i] * w.dot(X[i])) for i in range(X.shape[0])], axis=0)\ndef compute_grad(X, y, w):\n    X = expand(X)\n    return np.mean([-1 * (y[i] * w.dot(X[i]) < 1) * y[i] * X[i] for i in range(X.shape[0])], axis=0)\n", "intent": "The loss you should try to minimize is the Hinge Loss:\n$$ L =  {1 \\over N} \\sum_{i=1}^N max(0,1-y_i \\cdot  w^T x_i) $$\n"}
{"snippet": "y_pred = net.forward(X_test)\ny_pred = [np.argmax(y_pred[i]) for i in range(len(y_pred))]\ny_test_ = [np.argmax(y_test[i]) for i in range(len(y_test))]\nprint accuracy_score(y_pred, y_test_)\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)\n", "intent": "Play full session with an untrained agent\n"}
{"snippet": "encoded_values = encoder.predict(train_features)\n", "intent": "Replace 'srch_destination_id' with trained reduced destination features \n"}
{"snippet": "forest_prediction = rfor.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print (classification_report(y_test,forest_prediction))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "print((FP + FN) / float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "**Classification Error:** Overall, how often is the classifier incorrect?\n- Also known as \"Misclassification Rate\"\n"}
{"snippet": "prediction = KNN_model.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(classification_report(y_test, rfc_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "new_predict = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "x_test = np.array(['you are not happy'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "x_test = np.array(['SFO is nice'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(metrics.roc_auc_score(y_test, y_pred_prob))\n", "intent": "AUC is the **percentage** of the ROC plot that is **underneath the curve**:\n"}
{"snippet": "def get_loss(x, y):\n    return tf.reduce_mean(tf.square(get_preds(x) - y))\n", "intent": "The following functions evaluate the loss and the its gradient.\n"}
{"snippet": "mpg = mod.predict(97)\nprint(mpg[0][0])\n", "intent": "It would expect about 24.6 mpg\n"}
{"snippet": "print (clf.predict([[150, 0]]))\n", "intent": "<h3>Make Prediction</h3>\n"}
{"snippet": "print cross_val_score(RidgeCV ,X, y)\n", "intent": "---\nIs it better than the Linear regression? If so, why would this be?\n"}
{"snippet": "print cross_val_score(ElasticNetCV ,X, y)\n", "intent": "---\nHow does it compare to the other regularized regressions?\n"}
{"snippet": "from sklearn.cross_validation import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(KNN, X, y, cv=10, n_jobs=-1)\nprint 'Mean Cross-Validated Score:', np.mean(scores)\n", "intent": "Use 10 folds. How does the performace compare to the baseline accuracy?\n"}
{"snippet": "for i in range(10):\n    print(i, np.bincount((np.array(model.predict(testX))[:,i] >= 0.5).astype(np.int_)))\n", "intent": "Now let's look at our predictions in the same way:\n"}
{"snippet": "from sklearn import metrics\ny_pred = clf.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))\n", "intent": "Try some other values for yourself. Does the prediction make sense?\nOK, that works fine. Now let's see how good our classifier is on our test set.\n"}
{"snippet": "train_and_evaluate(svc_1, X_train, X_test, y_train, y_test)\n", "intent": "Let's measure precision and recall on the evaluation set, for _each class_. \n"}
{"snippet": "style_layer = 'relu2_1'\nlayer = vgg_net[style_layer]\nfeats, height, width, channels = [x.value for x in layer.get_shape()]\nsize = height * width * channels\nfeatures = tf.reshape(layer, (-1, channels))\nstyle_gram_matrix = tf.matmul(tf.transpose(features), features) / size\nstyle_expected = style_features[style_layer]\nstyle_losses.append(2 * tf.nn.l2_loss(style_gram_matrix - style_expected) / style_expected.size)\n", "intent": "Now we extract the style layer information\n"}
{"snippet": "preds = estimator.predict(input_fn=get_input_fn(my_test_data, 1, 1, shuffle=False))\nprint()\nfor p, s in zip(preds, sentences):\n    print('sentence:', s)\n    print('good review:', p[0], 'bad review:', p[1])\n    print('-' * 10)\n", "intent": "Now, let's generate predictions for the sentences\n"}
{"snippet": "RN50_predictions = [np.argmax(RN50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_RN50]\ntest_accuracy = 100*np.sum(np.array(RN50_predictions)==np.argmax(test_targets, axis=1))/len(RN50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import precision_score\nprint \"precision for category 0:\"\nPrec=precision_score(Y, Z,labels=\"0\",average=\"micro\")*100\nprint Prec\n", "intent": "i.b) Another way to calculate the precision for category 0:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nAcc=accuracy_score(Y, Z)*100\nprint('Precision for the model = %.1f%%'%(Acc))\n", "intent": "ii.a) One way to calculate the accuracy:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nAcc=accuracy_score(Y, Z)*100\nprint('Accuracy for the model = %.1f%%'%(Acc))\n", "intent": "ii.a) One way to calculate the accuracy:\n"}
{"snippet": "predict = LR.predict(x_train)\nprint(\"Mean squared error is: %.4f\"%mean_squared_error(y_train, predict))\n", "intent": "<img src=\"https://i.stack.imgur.com/eG03B.png\", width=300, height = 250>\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nprint(mean_squared_error(bos.PRICE, lm.predict(X)))\nprint(np.mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_pred_prob = logi_reg.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nConcatRocData('Logistic Regression', fpr, tpr, auc)\n", "intent": "85% of not survival (0) prediction are good  => 152 / (152+27)  \n80% of survival (1)  prediction are good => 93 / (93+23)\n"}
{"snippet": "bag_preds = bag_mod.predict(testing_data) \nrf_preds = rf_mod.predict(testing_data)\nada_preds = ada_mod.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "def evaluate(predictions, true_labels):\n    correct = 0\n    incorrect = 0\n    for i in range(len(predictions)):\n        if predictions[i] == true_labels[i]:\n            correct += 1\n        else:\n            incorrect += 1\n    print(\"\\tAccuracy:   \", correct / len(predictions))\n    print(\"\\tError rate: \", incorrect / len(predictions))\n", "intent": "And predictions evaluator:\n"}
{"snippet": "def recall(actual, preds):\n    return None \nprint(recall(y_test, preds_nb))\nprint(recall_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def f1(preds, actual):\n    return None \nprint(f1(y_test, preds_nb))\nprint(f1_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def precision(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    pred_pos = (preds==1).sum()\n    return tp/(pred_pos)\nprint(precision(y_test, preds_nb))\nprint(precision_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def recall(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    act_pos = (actual==1).sum()\n    return tp/act_pos\nprint(recall(y_test, preds_nb))\nprint(recall_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def f1(preds, actual):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    pred_pos = (preds==1).sum()\n    prec = tp/(pred_pos)\n    act_pos = (actual==1).sum()\n    recall = tp/act_pos\n    return 2*(prec*recall)/(prec+recall)\nprint(f1(y_test, preds_nb))\nprint(f1_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def mse(actual, preds):\n    return np.sum((actual-preds)**2)/len(actual)\nprint(mse(y_test, preds_tree))\nprint(mean_squared_error(y_test, preds_tree))\nprint(\"If the above match, you are all set!\")\n", "intent": "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. \n"}
{"snippet": "def mse(actual, preds):\n    return None \nprint(mse(y_test, preds_tree))\nprint(mean_squared_error(y_test, preds_tree))\nprint(\"If the above match, you are all set!\")\n", "intent": "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. \n"}
{"snippet": "dbscan = \nclustering_labels_3 = dbscan.fit_predict(dataset_2)\n", "intent": "What happens if we run DBSCAN with the default parameter values?\n"}
{"snippet": "def precision(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    pred_pos = (preds==1).sum()\n    return float(tp)/(pred_pos)\nprint(precision(y_test, preds_nb))\nprint(precision_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "Now that I have fit our model, I evaluate its performance by predicting off the test values!\n"}
{"snippet": "def f1(preds, actual):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    pred_pos = (preds==1).sum()\n    prec = float(tp)/(pred_pos)\n    act_pos = (actual==1).sum()\n    recall = float(tp)/act_pos\n    return 2*(prec*recall)/(prec+recall)\nprint(f1(y_test, preds_nb))\nprint(f1_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "print(bmi_life_model.predict([21.07931]))\nlaos_life_exp = bmi_life_model.predict([21.07931])\nlaos_life_exp = bmi_life_model.predict([21.07931])[0][0]\nlaos_life_exp\n", "intent": "3. Predict using the model\nPredict using a BMI of 21.07931 and assign it to the variable laos_life_exp.\n"}
{"snippet": "print(\"Model prediction =\", model.predict([X_test[0]]))\nprint(\"Hand computed prediction =\", model.coef_[0] * X_test[0] + model.intercept_)\nprint(\"Correct output =\", y_test[0])\n", "intent": "In the same way as before, we can compute the estimated output with the `predict()` method.\n"}
{"snippet": "print(\"Model prediction =\", model.predict([x_test[0]]))\nprint(\"Hand computed prediction =\", model.coef_[0] * x_test[0] + model.intercept_)\nprint(\"Correct output =\", y_test[0])\n", "intent": "In the same way as before, we can compute the estimated output with the `predict()` method.\n"}
{"snippet": "scores = cross_val_score(model_knn, X, y, cv=10)\nprint(scores)\n", "intent": "with majority vote method\n"}
{"snippet": "X = spam.iloc[:, 1:201]\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(knn, X, y, cv=10)\nprint scores\nprint np.mean(scores)\n", "intent": "Use 10 folds. How does the performace compare to the baseline accuracy?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ndef my_accuracy(trained_model):\n    return None\nassert my_accuracy(trained_knn) == accuracy_score(trained_knn['y_test'],\n                                                  trained_knn['y_test_pred']), \\\n         'Those are not the same'\n", "intent": "Complete the following \"roll your own\" method for calculating accuracy.\n"}
{"snippet": "from sklearn.metrics import precision_score\ndef my_precision(trained_model):\n    return None\nassert my_precision(trained_knn) == precision_score(trained_knn['y_test'],\n                                                    trained_knn['y_test_pred']), \\\n         'Those are not the same'\n", "intent": "Complete the following \"roll your own\" method for calculating precision.\n"}
{"snippet": "from sklearn.metrics import recall_score\ndef my_recall(trained_model):\n    return None\nassert my_recall(trained_knn) == recall_score(trained_knn['y_test'],\n                                              trained_knn['y_test_pred']), \\\n         'Those are not the same'\n", "intent": "Complete the following \"roll your own\" method for calculating recall.\n"}
{"snippet": "predict = gmm.predict(features).collect()\n", "intent": "Note that we are looking at optimistic in-sample errors.\n"}
{"snippet": "Y_pred = clf.predict(X_test_pca)\n", "intent": "Now we want to use the test set to see how well we did (cross-validation).\n"}
{"snippet": "from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nprint(classification_report(Y_test, Y_pred, target_names=lfw_people.target_names))\nprint(confusion_matrix(Y_test, Y_pred, labels=range(len(lfw_people.target_names))))\n", "intent": "There are nice convenient ways to display the results.\n"}
{"snippet": "VGG19_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "(model.predict(X) == y).mean()\n", "intent": "This is an in-sample prediction. Training error in both sklearn and statsmodels. Both are equivalent\n"}
{"snippet": "y_predict = lr.predict(X)\ny_predict[:10]\n", "intent": "All supervised estimators have a **`predict`** method that accepts **`X`**, a numpy array with data you would like to get the predicted label.\n"}
{"snippet": "new_scores = cross_val_score(lr, X, y, cv=10)\nnew_scores\n", "intent": "If you let the **`cv`** parameter in **`cross_val_score`** be an int, it will perform stratified kfold cross validation with that many folds.\n"}
{"snippet": "y_pred_test = regressor.predict(X_test)\n", "intent": "Next, let's try the test set:\n"}
{"snippet": "cv = ShuffleSplit(n_splits=5, test_size=.2)\ncross_val_score(classifier, X, y, cv=cv)\n", "intent": "You can use all of these cross-validation generators with the `cross_val_score` method:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))\n", "intent": "The values of all these values above are in the closed interval [0, 1], where 1 means a perfect score.\n"}
{"snippet": "y_yhat = test.map(lambda x: (x.label, model.predict(x.features)))\nerr = y_yhat.filter(lambda x: x[0] != x[1]).count() / float(test.count())\nprint(\"Error = \" + str(err))\n", "intent": "Evaluate on test data\n"}
{"snippet": "strengths = gmm.predict_proba(district[['median_price']])\n", "intent": "Further, if you want the probabilities around the assignments, you can use the `predict_proba` method:\n"}
{"snippet": "print('KMeans: ARI =', metrics.adjusted_rand_score(y, cluster_labels))\nprint('Agglomerative CLustering: ARI =', \n      metrics.adjusted_rand_score(y, ag.labels_))\n", "intent": "Calculate the Adjusted Rand Index (`sklearn.metrics`) for the resulting clustering and for ` KMeans` with the parameters from the 4th question.\n"}
{"snippet": "sgd_holdout_mse = mean_squared_error(y_valid, \n                                        sgd_reg.predict(X_valid_scaled))\nsgd_holdout_mse\n", "intent": "Make a prediction for hold-out  set `(X_valid_scaled, y_valid)` and check MSE value.\n"}
{"snippet": "X_new = [[3, 5,  4, 2], [5, 4, 3, 2]]\ntarget_idx = list(knn.predict(X_new))\ntarget_val = list()\nfor i in range(len(target_idx)):\n    target_val.append(str(target_idx[i]) + ': ' + str(iris.target_names[target_idx[i]]))\nprint(target_val)  \n", "intent": "- Returns the NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y, y_pred))\n", "intent": "<b>Classification Accuracy</b>\n- Proportion of correct predictions\n- Common evaluation metric for classification problems\n"}
{"snippet": "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n", "intent": "<b>Computing RMSE for the Sales predictions</b>\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "<b>Classification Accuracy:</b> Percentage of correct predictions.\n"}
{"snippet": "print((TP + TN)/float(TP + TN + FP + FN))\nprint(metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "<b>1. Classification Accuracy: </b> Overall, how often is the classifier correct?\n"}
{"snippet": "print((FP + FN)/float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "<b>2. Classification Error: </b> Overall, how often is the classifier incorrect?\n- Also known as 'Misclassification Rate'.\n"}
{"snippet": "print(\"Mean squared error on test data:\")\nprint(ed.evaluate('mean_squared_error', data={X: x_test, y_post: y_test}))\nprint(\"Mean absolute error on test data:\")\nprint(ed.evaluate('mean_absolute_error', data={X: x_test, y_post: y_test}))\n", "intent": "Calculate evalution metrics.\n"}
{"snippet": "print(metrics.roc_auc_score(y_test, y_pred_prob[0]))\n", "intent": "AUC is the percentage of the ROC plot that is underneath the curve:\n"}
{"snippet": "print('Residual sum of squares: %.2f' % np.mean((model.predict(X) - y) ** 2))\n", "intent": "Calculating RSS (Residual Sum of Squares) cost function value for our model:\n"}
{"snippet": "from sklearn.metrics import f1_score\nprint('F1 Score: %s' % f1_score(y_test_binarized, predictions_binarized))\n", "intent": "- F1 Score = F1 Score summarizes precision and recall with a single statistic.\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test_binarized, predictions_binarized, target_names=['male'], labels=[1]))\n", "intent": "- classification_report - This is a convenience function provided by scikit-learn that reports precision, recall and F1 score:\n"}
{"snippet": "precision = cross_val_score(classifier, X_train, y_train, cv=5, scoring='precision')\nprint(\"Precision: \", np.mean(precision))\nrecall = cross_val_score(classifier, X_train, y_train, cv=5, scoring='recall')\nprint(\"Recall: \", np.mean(recall))\n", "intent": "- Precision and Recall\n"}
{"snippet": "import numpy as np\nfrom sklearn.metrics import hamming_loss, jaccard_similarity_score\nprint(hamming_loss(np.array([[0.0, 1.0], [1.0, 1.0]]), np.array([[0.0, 1.0], [1.0, 1.0]])))\n", "intent": "- Multi-label classification performance metrics\n    - Hamming Loss\n    - Jaccard Similarity\n"}
{"snippet": "cluster_assignments = estimator.predict(image_flattened)\n", "intent": "Next, we predict the cluster assignment for each of the pixels in the original image.\n"}
{"snippet": "col_predict = ['price', 'grade', 'sqft_living', 'sqft_above']\nX_test = df[col_predict].copy()\nfor i in col_predict:\n    if i!='bathrooms':\n        X_test[i] = np.log(X_test[i])\nmodel.predict(X_test)\n", "intent": "The $R^2$ score seems to be fine, now we can use the full dataset `df` to predict potential values for <font color='blue'>**bathrooms**</font>:\n"}
{"snippet": "predict_value = lm_for_impute.predict(boston_impute_df.drop(['AGE','y'],axis=1))\n", "intent": "Previously we have fit our model, now it is time to do some prediction:\n"}
{"snippet": "tt = np.linspace(qso['time'].min(), qso['time'].max(), 500)\nqso_gp_sqex1_pred, qso_gp_sqex1_mse = qso_gp_sqex1.predict(tt.reshape(-1,1), eval_MSE=True)\nqso_gp_ex1_pred, qso_gp_ex1_mse = qso_gp_ex1.predict(tt.reshape(-1,1), eval_MSE=True)\n", "intent": "Finally, we can plot the model with its confidence band.\n"}
{"snippet": "note_predictions = list(classifier.predict(X_test, as_iterable=True))   \n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "print(classification_report(y_test, predictions))\nprint('\\n')\nprint(confusion_matrix(y_test, predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df['Cluster'], kmeans.labels_))\nprint('\\n')\nprint(classification_report(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, predictions))\nprint('\\n')\nprint(classification_report(y_test, predictions))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "new_predictions = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "Y_p = lr.predict(X)\nprint Y_p[:10]\n", "intent": "To predict prices we will use `lr.predict()` function.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint accuracy_score(y_test, y_pred)\n", "intent": "Function [`accuracy_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n"}
{"snippet": "Xfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)\n", "intent": "```\nXfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)```\n"}
{"snippet": "rf_cv = cross_val_score( \nprint('The RF model FoM = {:.4f} +/- {:.4f}'.format( \n", "intent": "**Problem 3c**\nUse 10-fold cross validation to estimate the FoM for the random forest model.\n"}
{"snippet": "rng = np.random.RandomState(0)\nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\nynew = model.predict(Xnew)\n", "intent": "```\nrng = np.random.RandomState(0)\nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\nynew = model.predict(Xnew)```\n"}
{"snippet": "yprob = model.predict_proba(Xnew)\nyprob[-8:].round(2)\n", "intent": "```\nyprob = model.predict_proba(Xnew)\nyprob[-8:].round(2)```\n"}
{"snippet": "test_vectors = model_vectorizer.transform(test.data)\nlabels = model_classifier.predict(test_vectors)\n", "intent": "```\ntest_vectors = model_vectorizer.transform(test.data)\nlabels = model_classifier.predict(test_vectors)```\n"}
{"snippet": "classPredictA = gnb_clean.predict(news_clean_AA)\n", "intent": "Plot the (normalised) confusion matrix for the training data. Label axes appropriately.\n"}
{"snippet": "mse4 = mean_squared_error(y, cpredict4)\nprint(\"\\nThe root mean squared error is \" + str(np.sqrt(mse4)))\nprint(\"\\nThe mean absolute error is \" +str(mean_absolute_error(y, cpredict4)))\nprint(\"\\nThe correlation coefficient is \" + str(np.corrcoef(y, cpredict4)[0,1]))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "Resnet_predictions = [np.argmax(Resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet]\ntest_accuracy = 100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(classification_report(y_test,rfc_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "print('predicted:', spam_detect_model.predict(tfidf4)[0])\nprint('expected:', messages.label[5])\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "predicts = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "rf_cv = cross_val_score(rf_clf, train_X, train_y, cv=10)\nprint('The RF model FoM = {:.4f} +/- {:.4f}'.format(np.mean(rf_cv), np.std(rf_cv, ddof=1)))\n", "intent": "**Problem 3c**\nUse 10-fold cross validation to estimate the FoM for the random forest model.\n"}
{"snippet": "bag_y_pred = bagging.predict(testing_data)\nrf_y_pred = randomForest.predict(testing_data)\nab_y_pred = adaBoost.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "def precision(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    pred_pos = (preds==1).sum()\n    return tp/(pred_pos)\nprint(precision(y_test, nb_pred))\nprint(precision_score(y_test, nb_pred))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def recall(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    act_pos = (actual==1).sum()\n    return tp/act_pos \nprint(recall(y_test, nb_pred))\nprint(recall_score(y_test, nb_pred))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def f1(preds, actual):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1)))\n    pred_pos = (preds==1).sum()\n    prec = tp/(pred_pos)\n    act_pos = (actual==1).sum()\n    recall = tp/act_pos\n    return 2*(prec*recall)/(prec+recall)\nprint(f1(y_test, nb_pred))\nprint(f1_score(y_test, nb_pred))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "lr_pred = lr.predict(X_test)\nrf_pred = rf.predict(X_test)\nab_pred = ab.predict(X_test)\ndt_pred = dt.predict(X_test)\n", "intent": "> **Step 5:** Use each of your models to predict on the test data.\n"}
{"snippet": "def mse(actual, preds):\n    return np.sum((actual-preds)**2)/len(actual)\nprint(mse(y_test, dt_pred))\nprint(mean_squared_error(y_test, dt_pred))\nprint(\"If the above match, you are all set!\")\n", "intent": "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. \n"}
{"snippet": "test_image_predicted_label_probabilities = model.predict(test_image_tensor)[0]\nfor idx, prob in enumerate(test_image_predicted_label_probabilities):\n    print(labels[idx].ljust(11), ': %.05f' % prob)\n", "intent": "Let's look at the predicted label probabilities:\n"}
{"snippet": "activations = activation_model.predict(test_image_tensor)\n", "intent": "Next, we pass in the test image tensor into the above activation model to get all the layer outputs:\n"}
{"snippet": "predictions = lgmodel.predict(X_test)\n", "intent": "** Agora preveja valores para os dados de teste. **\n"}
{"snippet": "phot_y = np.empty_like(test_y)\nphot_gal = np.logical_not(test_X[:,0] - test_X[:,-1] < 0.145)\nphot_y[phot_gal] = 'GALAXY'\nphot_y[~phot_gal] = 'STAR'\nprint(\"The baseline FoM = {:.4f}\".format(accuracy_score(test_y, phot_y)))\n", "intent": "**Problem 5a** \nCalculate the FoM for the SDSS photometric model on the test set. \n"}
{"snippet": "recall_score(y_train_0, y_train_pred)\n", "intent": "Recall -> True Positive Rate -> ratio of positive instances correctly detected by the classifier -> TP / TP + FN \n"}
{"snippet": "y = km.predict(data)\n", "intent": "Predict the clusters for each data point\n"}
{"snippet": "clf.predict(df[iris.feature_names])\npd.crosstab(df.species, clf.predict(df[iris.feature_names]))\n", "intent": "Now predict the classes and check the crosstabs:\n"}
{"snippet": "results = clf.predict(X_test)\npd.crosstab(y_test, results)\n", "intent": "This tree is much simpler than the original tree based on all the data.\n"}
{"snippet": "scores = cross_val_score(clf, X_train, y_train, cv=10)\nprint scores\nprint np.mean(scores)\n", "intent": "What does cross-validation look like if we use only our Train set?\n"}
{"snippet": "scores = cross_val_score(clf, X_train, y_train, cv=10)\nprint scores\nprint np.mean(scores)\n", "intent": "Does limiting the max depth affect our average model performance with this training data?\n"}
{"snippet": "scores_precision = cross_validation.cross_val_score(f, wine.values, grape.values, cv=5, scoring='precision')\nscores_precision\n", "intent": "Furthermore, we can customize the scoring method by specifying the `scoring` parameter:\n"}
{"snippet": "def mean_square_error(X, y, beta):\n    return (X.dot(beta) - y).dot(X.dot(beta) - y) / X.shape[0]\nmse = mean_square_error(X_bt, y, beta)\nprint('MSE: %.02f RMSE: %.02f' % (mse, np.sqrt(mse)))\n", "intent": "**Question:** To quantify the error, we can use the mean square error (MSE). Implement this the quadratic loss function given above:\n"}
{"snippet": "from numpy import log, exp\ndef log_loss(X, y, beta):\n    return np.sum(log(1 + exp(-y * X.dot(beta)))) / X.shape[0]\n", "intent": "**Question:** Implement the loss function of a logistic regression:\n"}
{"snippet": "new_preds = rf_clf.predict(new_X)\nprint(\"The model has an accuracy of {:.4f}\".format(accuracy_score(new_y, new_preds)))\n", "intent": "**Problem 6b**\nCalculate the accuracy of the model predictions on the new data.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(my_ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "bn_solver.evaluate(mnist.test.images, mnist.test.labels)\n", "intent": "* With the batchnormalization, the loss is lower and it's more accurate too!\n"}
{"snippet": "result = model.evaluate(x=data.test.images,\n                        y=data.test.labels)\n", "intent": "Now that the model has been trained we can test its performance on the test-set. This also uses numpy-arrays as input.\n"}
{"snippet": "y_pred = model.predict(x=images)\n", "intent": "Get the predicted classes as One-Hot encoded arrays.\n"}
{"snippet": "y_pred = model.predict(x=data.test.images)\n", "intent": "We can plot some examples of mis-classified images from the test-set.\nFirst we get the predicted classes for all the images in the test-set:\n"}
{"snippet": "result = model2.evaluate(x=data.test.images,\n                         y=data.test.labels)\n", "intent": "Once the model has been trained we can evaluate its performance on the test-set. This is the same syntax as for the Sequential Model.\n"}
{"snippet": "y_pred = model2.predict(x=data.test.images)\n", "intent": "We can plot some examples of mis-classified images from the test-set.\nFirst we get the predicted classes for all the images in the test-set:\n"}
{"snippet": "y_pred = model3.predict(x=images)\n", "intent": "We then use the restored model to predict the class-numbers for those images.\n"}
{"snippet": "predict(image_path='images/parrot_cropped1.jpg')\n", "intent": "We can then use the VGG16 model on a picture of a parrot which is classified as a macaw (a parrot species) with a fairly high score of 79%.\n"}
{"snippet": "missing_ages = np.where(np.isnan(titanic_df['Age']))[0]\nimpute_X_missing = \nX_missing_minmax = scaler.transform(impute_X_missing)\nage_preds = lr_age.predict(X_missing_minmax)\n", "intent": "**Problem 3n**\nUse the age regression model to predict the ages for passengers with missing data.\n"}
{"snippet": "predict(image_path=image_paths_test[0])\n", "intent": "We can also try an image from our new test-set, and again the VGG16 model is very confused.\n"}
{"snippet": "result = model.evaluate(x=data.test.images,\n                        y=data.test.labels)\n", "intent": "We then evaluate its performance on the test-set.\n"}
{"snippet": "result = model.evaluate(x_test_pad, y_test)\n", "intent": "Now that the model has been trained we can calculate its classification accuracy on the test-set.\n"}
{"snippet": "y_pred = model.predict(x=x_test_pad[0:1000])\ny_pred = y_pred.T[0]\n", "intent": "In order to show an example of mis-classified text, we first calculate the predicted sentiment for the first 1000 texts in the test-set.\n"}
{"snippet": "model.predict(tokens_pad)\n", "intent": "We can now use the trained model to predict the sentiment for these texts.\n"}
{"snippet": "tf.logging.set_verbosity(tf.logging.INFO) \nshutil.rmtree(path = OUTDIR, ignore_errors = True)\ntf.summary.FileWriterCache.clear() \ntf.estimator.train_and_evaluate(estimator = model, \n                                train_spec = train_spec, \n                                eval_spec = eval_spec)\n", "intent": "Run the following cell to start the training and evaluation as you specified them above:\n"}
{"snippet": "shutil.rmtree(OUTDIR, ignore_errors = True) \ntf.summary.FileWriterCache.clear() \ntrain_and_evaluate(OUTDIR, num_train_steps = 500)\n", "intent": "<h2>Run training</h2>\n"}
{"snippet": "shutil.rmtree(OUTDIR, ignore_errors = True) \ntf.summary.FileWriterCache.clear() \ntrain_and_evaluate(OUTDIR, num_train_steps = 2000)\n", "intent": "<h2>Run training</h2>\n"}
{"snippet": "shutil.rmtree(path = \"babyweight_trained_dnn\", ignore_errors = True) \ntrain_and_evaluate(\"babyweight_trained_dnn\")\n", "intent": "Finally, we train the model!\n"}
{"snippet": "missing_ages = np.where(np.isnan(titanic_df['Age']))[0]\nimpute_X_missing = titanic_df[['Pclass', 'SibSp', 'Parch', 'Fare', 'female', 'male', 'S', 'Q', 'C']].iloc[missing_ages]\nX_missing_minmax = scaler.transform(impute_X_missing)\nage_preds = lr_age.predict(X_missing_minmax)\n", "intent": "**Problem 3n**\nUse the age regression model to predict the ages for passengers with missing data.\n"}
{"snippet": "OUTDIR = \"mnist/learned\"\nshutil.rmtree(path = OUTDIR, ignore_errors = True) \nhparams = {\"train_steps\": 1000, \"learning_rate\": 0.01}\ntrain_and_evaluate(OUTDIR, hparams)\n", "intent": "This is the main() function\n"}
{"snippet": "OUTDIR = \"mnist/learned\"\nshutil.rmtree(OUTDIR, ignore_errors = True) \nhparams = {\"train_steps\": 1000, \"learning_rate\": 0.01}\ntrain_and_evaluate(OUTDIR, hparams)\n", "intent": "This is the main() function\n"}
{"snippet": "predict = model.predict(X_test)\n", "intent": "    predict = model.predict(X_test)\n"}
{"snippet": "labels = res.predict(X)\n", "intent": "We can get the means of the two Gaussians and \n"}
{"snippet": "ResNet50_predictions = [np.argmax(model_ResNet50.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "banknotes_predictions = list(classifier.predict(X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "print(metrics.roc_auc_score(y_test, y_pred_prob))\n", "intent": "**Note**: The number of thresholds = the number of unique probabilities in our prediction array\n"}
{"snippet": "print(\"Accuracy: %0.3f\" % dt.score(X_test, y_test))\nprint('ROC AUC: %0.3f' % roc_auc_score(y_test, dt.predict_proba(X_test)[:,1]))\n", "intent": "Notice two of the default parameters: **min_samples_leaf = 1** and **max_depth = None**\n"}
{"snippet": "predictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')\n", "intent": "**Expected Output**:\n<table style=\"width:40%\">\n  <tr>\n    <td>**Cost after iteration 9000**</td>\n    <td> 0.218607 </td> \n  </tr>\n</table>\n"}
{"snippet": "metrics.mean_squared_error([1, 2, 3, 4, 5], [5, 4, 3, 2, 1])\n", "intent": "While the opposite scenario should have a mean squared error of 8:\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.2f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "ym = np.mean(y_tr) \ny_tr_pred = regr.predict(X_tr)\nRSS_tr = np.sum((y_tr_pred-y_tr)**2)\nTSS_tr = np.sum((y_tr-ym)**2)\nRsq_tr = 1- RSS_tr/TSS_tr\nprint(\"R^2 = {0:f}\".format(Rsq_tr))\n", "intent": "We next compute the MSE and the R^2 on the training data\n"}
{"snippet": "def predict(a, beta):\n    yhat = hypothesis(a,beta)\n    return yhat\n", "intent": "You should see the likelihood increasing as number of Iterations increase.\n"}
{"snippet": "y_pred = predict_y(W, b, X_test, 3)\nprint('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))\n", "intent": "Next we determine what percentage the neural network correctly predicted the handwritten digit correctly on the test set\n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "We can then apply the model to unseen data and use the model to predict the estimated outcome using the ``predict`` method:\n"}
{"snippet": "y_pred_test = regressor.predict(X_test)\n", "intent": "As we can see in the plot above, the line is able to capture the general slope of the data, but not many details.\nNext, let's try the test set:\n"}
{"snippet": "print(\"Mean squared error: %.2f\"\n      % np.mean((lm.predict(X) - bos.PRICE) ** 2))\n", "intent": "This is simple the mean of the residual sum of squares.\n------ \nYour turn: Calculate the mean squared error and print it.\n"}
{"snippet": "predict = rfm.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "y_pred = clf.predict(X)\nprint(y,y_pred)\n", "intent": "First, let's predict the species from the measurements.  Because the classifier is clearly not perfect, we expect some mis-classifications.\n"}
{"snippet": "predicted  = lrm.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predicted = pl.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "pred = knn.predict(X_Test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "print(classification_report(y_Test,pred))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "predictions = lm.predict(X_Test)\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "predictions = logmodel.predict(X_Test)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(df['Cluster'], kmeans.labels_), '\\n')\nprint(confusion_matrix(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = nb.predict(X_test)\n", "intent": "**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    dimension =  x_train.shape[0]  \n    w,b = initialize_weights_and_bias(dimension)\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)\n", "intent": "* We make prediction.\n* Now lets put them all together.\n"}
{"snippet": "from sklearn.metrics import homogeneity_score\nfor name, est in estimators.items():\n    print('completeness', name, homogeneity_score(df[name],df['species']))\n    print('homogeneity', name, homogeneity_score(df['species'],df[name]))\n", "intent": "The plot looks good, but it isn't clear how good the labels are until we compare them with the true labels. \n"}
{"snippet": "cross_val_score(model_NB, X_train_countVectorizer, yelp_data_final_update['Review_category_useful_notuseful'], cv=5)\n", "intent": "Naive Bayes model on useful vs not useful:\n"}
{"snippet": "cross_val_score(model_NB, X_train_countVectorizer, yelp_data_final_update['Review_category_cool_notcool'], cv=5)\n", "intent": "Naive Bayes model on cool vs not cool:\n"}
{"snippet": "cross_val_score(model_NB, X_train_countVectorizer, yelp_data_final_update['Review_category_funny_notfunny'], cv=5)\n", "intent": "Naive Bayes model on funny vs not funny:\n"}
{"snippet": "h = .02\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nZ = clf_linear.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\npylab.contourf(xx,yy,Z,cmap=pylab.cm.Paired)\npylab.axis('off')\npylab.scatter(X[:,0],X[:,1],c=colormap[y], s=50,cmap=pylab.cm.Paired)\n", "intent": "Let's see how the linear SVM separated the space.\n"}
{"snippet": "print 'Training MSE: ', np.mean((regr1.predict(X_train) - y_train)**2)\nprint 'Test MSE: ', np.mean((regr1.predict(X_test) - y_test)**2)\n", "intent": "We can calculate the Mean Squared Error on the test set:\n"}
{"snippet": "X_test = np.random.random((200, 100))\ny_test = np.random.randint(0, 10, 200)\ny_test = to_categorical(y_test)\nloss_and_metrics = model.evaluate(X_test, y_test, batch_size=32)\nprint('\\n', loss_and_metrics)\nclasses = model.predict_classes(X_test, batch_size=32)\nproba = model.predict_proba(X_test, batch_size=32)\n", "intent": "- Like `sklearn`, we have nice predict and evaluation functions!\n- Here's an example:\n"}
{"snippet": "print(\"R-Squared Score: {}\".format(r2_score(y_test, y_pred)))\nprint(\"Mean Absolute Error: {}\".format(mean_absolute_error(y_test, y_pred)))\nprint(\"Root Mean Squared Error: {}\".format(np.sqrt(mean_squared_error(y_test, y_pred))))\n", "intent": "<h1>Challenge 5</h1>\n"}
{"snippet": "mean_squared_error(y_test, test_set_pred1)\n", "intent": "Mean Square = \n0.066664630206035261\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2_score(y_test, test_set_pred1)\n", "intent": "R2 score for 30 days stock & CMC\n0.99973097446459092\n"}
{"snippet": "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\nuc = UnigramChunker(train_sents)\nprint(uc.evaluate(test_sents))\n", "intent": "Trainieren und evaluieren Sie den UnigramChunker auf dem CoNLL 2000 Korpus.\n"}
{"snippet": "def vae_loss(x, x_decoded_mean):\n    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n    return xent_loss + kl_loss\n", "intent": "The loss function encourages a good reconstruction and discourages a deviation of the learned latent z distribution from the N(0,1) distribution.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_dog_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print (np.mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(knn, transformed_data, transformed_labels, cv=5)\n", "intent": "Let's cross validate each.\n"}
{"snippet": "knn.predict(wine_data_test)\n", "intent": "And finally, let's use our fitted model to predict on new data.\n"}
{"snippet": "cross_val_score(tree, wine_data, wine_labels, cv=4)\n", "intent": "And let's cross-validate again. Let's only run it four times.\n"}
{"snippet": "cross_val_score(gnb, wine_data, wine_labels, cv=4)\n", "intent": "And of course, let's cross-validate to see how well we did. Let's only run it four times.\n"}
{"snippet": "lr.predict(wine_mag_test)\n", "intent": "And finally, we predict.\n"}
{"snippet": "print('Number of errors: %i' % sum(clf.predict(X)!=y))\nplot_decision_regions(X,y,clf)\n", "intent": "*How can we evaluate the model??*\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nscaled_X_val = scaler.transform(X_val)                  \ny_pred_dt = clf_dt.predict(scaled_X_val)\ny_pred_nb = clf_nb.predict(scaled_X_val)                      \nacc_dt = accuracy_score(y_pred_dt,y_val)                            \nacc_nb = accuracy_score(y_pred_nb,y_val) \nprint(\"The accuracy of Decision Tree: {} %\".format(acc_dt))\nprint(\"The accuracy of Gaussian Naive Bayes: {} %\".format(acc_nb))\n", "intent": "Q12. Using the accuracy_score function, determine the accuracy of the two classifiers.\n"}
{"snippet": "cost = tf.losses.mean_squared_error(output_reshape, predictions)\n", "intent": "Very standard way of defining costs/optimizer etc..\n"}
{"snippet": "feature_cols = vehicles.columns[1:]\npreds1 = treereg1.predict(oos[feature_cols])\npreds2 = treereg2.predict(oos[feature_cols])\npreds3 = treereg3.predict(oos[feature_cols])\nprint(preds1)\nprint(preds2)\nprint(preds3)\n", "intent": "Q: How do we get predictions from our models?\n"}
{"snippet": "predict = clf.predict(x_test)\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "metrics.mean_absolute_error(y_test,predict)\n", "intent": "**Print out the coefficients of the model**\n"}
{"snippet": "print(classification_report(y_test,pred))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "Resnet_predictions = [np.argmax(Resnet.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet]\ntest_accuracy = 100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "ypred2 = clf2.predict(Xtest)\n", "intent": "Random Forest Regressor does worse than classifier with probabilities.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in resnet_test]\ntest_accuracy = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%% ' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import precision_score,recall_score\nprec_dt = precision_score(y_pred_dt,y_val)\nprec_nb = precision_score(y_pred_nb,y_val)\nrecall_dt = recall_score(y_pred_dt,y_val)\nrecall_nb = recall_score(y_pred_nb,y_val)\nprint(\"The precision of Decision Tree: {} %\".format(prec_dt))\nprint(\"The precision of Gaussian Naive Bayes: {} %\".format(prec_nb))\nprint(\"The recall of Decision Tree: {} %\".format(recall_dt))\nprint(\"The recall of Gaussian Naive Bayes: {} %\".format(recall_nb))\n", "intent": "Q13. Determine the precision and recall using precision_score and recall_score.\n"}
{"snippet": "probas = np.asarray( [ clf.predict_proba(X) for clf in classifiers_ ] )\navg_proba = np.average( probas, axis = 1, weights = None )\navg_proba\n", "intent": "Code when you wish to obtain the predicted probability for the final class.\n"}
{"snippet": "import numpy as np\nrandom_input = np.random.rand(1, 1, 3, 3)\nprediction = predictor.predict({'input': random_input.tolist()})\nprint(prediction)\n", "intent": "Invoking prediction:\n"}
{"snippet": "Resnet50_predictions = [np.argmax(trans_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(college['Cluster'], kmeans.labels_))\nprint(classification_report(college['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "prediction = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "** Create a scatterplot of the real test values versus the predicted values. **\n"}
{"snippet": "prediction = logmodel.predict(X_test)\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "prediction = model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\nkmeans.predict(X_new)\n", "intent": "Of course, we can predict the labels of new instances:\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_dt = f1_score(y_pred_dt,y_val)\nf1_nb = f1_score(y_pred_nb,y_val)\nprint(\"The F1-score of Decision Tree: {} %\".format(f1_dt))\nprint(\"The F1-score of Gaussian Naive Bayes: {} %\".format(f1_nb))\n", "intent": "Q14. Determine the F1-score of the two classifiers.\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "Run more predictions\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\nprint np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n", "intent": "---\nCross-validate the $R^2$ of an ordinary linear regression model with 10 cross-validation folds.\nHow does it perform?\n"}
{"snippet": "y_pred = ridgeregcv.predict(X_test)\nprint np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n", "intent": "---\nIs it better than the Linear regression? If so, why might this be?\n"}
{"snippet": "y_pred = lassoreg.predict(X_test)\nprint np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n", "intent": "---\nIs it better than the Linear regression? Is it better than Ridge? What do the differences in results imply about the issues with the dataset?\n"}
{"snippet": "y_pred_class=knn.predict(x)\nfrom sklearn import metrics\nprint metrics.accuracy_score(y, y_pred_class)\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "def mean_squared_error(X, y, beta_array):\n    y_hat = np.dot(X, beta_array)\n    mean_sq_err = np.mean((y_true - y_hat)**2)\n    return mean_sq_err\n", "intent": "This function calculates the mean of the squared errors using a dot product between the `X` predictor matrix and the `beta_array`:\n"}
{"snippet": "scores = cross_val_score(knn, xs, y, cv=10)\nprint scores\nprint np.mean(scores)\n", "intent": "Plot the cross-validated mean accuracy for each score. What is the best accuracy?\n"}
{"snippet": "alive = np.random.randint(1, size=len(Y_test))\ndead = np.random.randint(1, size=len(Y_test)) + 1\ncointoss = np.random.randint(2, size=len(Y_test))\nhalf = np.random.randint(1, size=len(Y_test)) + 0.5\nprint ('alive:','\\t\\t', round(metrics.log_loss(Y_test, alive), 9), '\\n'\n      'dead:', '\\t\\t', round(metrics.log_loss(Y_test, dead), 9), '\\n'\n      'Coin:', '\\t\\t', round(metrics.log_loss(Y_test, cointoss), 9), '\\n'\n      'half:', '\\t\\t', round(metrics.log_loss(Y_test, half), 9))\n", "intent": "Need review the R code: \n"}
{"snippet": "alive = np.random.randint(1, size=len(Y_test)) + 1\ndead = np.random.randint(1, size=len(Y_test))\ncointoss = np.random.randint(2, size=len(Y_test))\nhalf = np.random.randint(1, size=len(Y_test)) + 0.5\nprint ('alive:','\\t', round(metrics.log_loss(Y_test, alive, eps=0.99), 9), '\\n'\n      'dead:', '\\t', round(metrics.log_loss(Y_test, dead, eps=0.99), 9), '\\n'\n      'Coin:', '\\t', round(metrics.log_loss(Y_test, cointoss, eps=0.99), 9), '\\n'\n      'half:', '\\t', round(metrics.log_loss(Y_test, half), 9))\n", "intent": "Need review the R code: \n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores1 = cross_val_score(clf1,X,y,cv=6)              \nav_score1 = sum(scores1)/len(scores1)            \nscores2 = cross_val_score(clf2,X,y,cv=6)             \nav_score2 = sum(scores2)/len(scores2)            \nprint(\"Average Cross Validation Score for clf1: {}\".format(av_score1))\nprint(\"Average Cross Validation Score for clf2: {}\".format(av_score2))\n", "intent": "Q9. Calculate the 6-fold cross validation score using the cross_val_score() function. Parameter 'cv' defines the number of folds.\n"}
{"snippet": "plot_fit(x_test, predict(x_test, w0, w1), x, y)\n", "intent": "Let's try plotting the result again.\n"}
{"snippet": "w0, w1 = w\nplot_fit(x_test, predict(x_test, w0, w1), x, y)\n", "intent": "Plotting this solution, as before:\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nY_test_pred = ((np.dot(Phi_test, v_opt[1::]) + v_opt[0]) >= 0)*1 \naccuracy_score(Y_test, Y_test_pred)\n", "intent": "This time we should get a better result for the accuracy on the test set.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nnaive_score = accuracy_score(y_true, y_naive_prediction)\nprint(\"Accuracy score of naive model: {}\".format(naive_score))\n", "intent": "Checking accuracy score of naive model\n"}
{"snippet": "logistic_regression_auc = roc_auc_score(y_test, pred_prob)\nlogistic_regression_accuracy = accuracy_score(y_test, predicted_labels)\nprint(\"Accuracy score of Logistic Regression Model: {}\".format(logistic_regression_accuracy))\nprint(\"AUROC of Logistic Regression Model: {}\".format(logistic_regression_auc))\n", "intent": "Checking accuracy score of Logistic Regression Model\n"}
{"snippet": "test_data_pred_prob = log_regression_model.predict_proba(test)[:,1]\n", "intent": "Next, prediction for test set has to be made. To check how well model perform it needs to be submitted on Kaggle to get results.\n"}
{"snippet": "def evaluate(x, y, expr, x_value, y_value):\n    pass\nx = T.iscalar()\ny = T.iscalar()\nz = x + y\nassert evaluate(x, y, z, 1, 2) == 3\nprint(\"SUCCESS!\")\n", "intent": "Now try compiling and running a simple function:\n"}
{"snippet": "ss = ((fit.predict(bodyfat[subsets[3]]) - bodyfat['fat'])**2).sum()\n", "intent": "Using the model output, we can extract the residual sums of squares and use this to calculate AIC.\n"}
{"snippet": "model_selection.cross_val_score(f, wine.values, grape.values, cv=5,\n                                 scoring='f1_weighted')\n", "intent": "Furthermore, we can customize the scoring method by specifying the `scoring` parameter:\n"}
{"snippet": "from sklearn.metrics import explained_variance_score,mean_squared_error,r2_score\ndef performance_metrics(y_true,y_pred):\n    rmse = mean_squared_error(y_true,y_pred)\n    r2 = r2_score(y_true,y_pred)\n    explained_var_score = explained_variance_score(y_true,y_pred)\n    return rmse,r2,explained_var_score\n", "intent": "Q9. Complete the function below, which returns the above mentioned metrics. Import the necessary tools via scikit-learn\n"}
{"snippet": "predi = list(clasifier.predict(X_test, as_iterable=True))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "predi = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions = logi.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predi = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predi = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "y_pred = thesvm.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "score = sa_model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "data = loadmat(os.path.join('Data', 'spamTest.mat'))\nXtest, ytest = data['Xtest'].astype(float), data['ytest'][:, 0]\nprint('Evaluating the trained Linear SVM on a test set ...')\np = utils.svmPredict(model, Xtest)\nprint('Test Accuracy: %.2f' % (np.mean(p == ytest) * 100))\n", "intent": "Execute the following cell to load the test set and compute the test accuracy.\n"}
{"snippet": "model_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_tensors]\ntest_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=1))/len(model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = regressor.predict(X_val)\nrmse,r2,explained_var_score = performance_metrics(y_val,y_pred)\nprint(\"Root mean squared error:{} \\nR2-score:{} \\nExplained variance score:{}\".format(rmse,r2,explained_var_score))\n", "intent": "Q11. Generate predictions from the validation set, and output the above-mentioned scores.\n"}
{"snippet": "Series(cross_val_score(logreg, features_array, target, cv=5, scoring='roc_auc')).describe()\n", "intent": "`cross_val_score` reports accuracy by default be it can also be used to report other performance metrics such as ROC-AUC or f1-score:\n"}
{"snippet": "accuracy_score(pipe.predict(X_test), y_test)\n", "intent": "Functions that are applicable to estimators are also applicable to Pipelines. That is one of the most powerful premise of the pipeline after all. \n"}
{"snippet": "import seaborn as sns\nsns.heatmap(confusion_matrix(pipe.predict(X_test), y_test), annot=True,  fmt='');\n", "intent": "By now, we know that this score is not very meaningful, let's look at the confusion matrix!\n"}
{"snippet": "x.value = 3\ny.value = 4\nf.evaluate()\n", "intent": "And we can run this graph to compute $f$ at any point, for example $f(3, 4)$.\n"}
{"snippet": "x.value = DualNumber(3.0)\ny.value = DualNumber(4.0)\nf.evaluate()\n", "intent": "Now let's see if the dual numbers work with our toy computation framework:\n"}
{"snippet": "x.value = DualNumber(3.0, 1.0)  \ny.value = DualNumber(4.0)       \ndfdx = f.evaluate().eps\nx.value = DualNumber(3.0)       \ny.value = DualNumber(4.0, 1.0)  \ndfdy = f.evaluate().eps\n", "intent": "Yep, sure works. Now let's use this to compute the partial derivatives of $f$ with regards to $x$ and $y$ at x=3 and y=4:\n"}
{"snippet": "tuned_forest_predictions = locally_best_forest.predict(X_test) \naccuracy_score(y_test, tuned_forest_predictions) \n", "intent": "Make predictions for the test data.\n"}
{"snippet": "tree_predictions = tree.predict(X_test) \n", "intent": "Make a prediction with trained model for test data.\n"}
{"snippet": "forest_predictions = rf.predict(X_test) \n", "intent": "Make a prediction for test data.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nscaled_X_val = None                  \ny_pred_dt = clf_dt.predict(scaled_X_val)\ny_pred_nb = clf_nb.predict(scaled_X_val)                      \nacc_dt = None                            \nacc_nb = None\nprint(\"The accuracy of Decision Tree: {} %\".format(acc_dt))\nprint(\"The accuracy of Gaussian Naive Bayes: {} %\".format(acc_nb))\n", "intent": "Q12. Using the accuracy_score function, determine the accuracy of the two classifiers.\n"}
{"snippet": "tscv = TimeSeriesSplit(n_splits=7)\nscores = cross_val_score(logit, X_train_sparse, y_train, cv=tscv, scoring='roc_auc')\nnp.mean(scores)\n", "intent": "0.92729257732863013\n"}
{"snippet": "test_pred = searchCV.predict_proba(X_test)[:,1]\n", "intent": "0.93372138833945972\n"}
{"snippet": "prediction = np.around(model.predict(np.expand_dims(inputs_test[0], axis=0))).astype(np.int)[0]\nprint(\"Actual: %s\\tEstimated: %s\" % (outputs_test[0].astype(np.int), prediction))\nprint(\"That means it's a %s\" % outputs_vals[prediction.astype(np.bool)][0])\n", "intent": "Predict the class of a single flower.\n"}
{"snippet": "austen_sentence_vectors = vectorizer.transform(austen_test_sentences)   \nausten_sentence_classifications = classifier.predict(austen_sentence_vectors)   \nausten_sentence_classifications[:20]                                    \n", "intent": "    Write a script that prints Austen-like sentences written \n    by Melville, and Melville-like sentences written by Austen.\n"}
{"snippet": "print((np.sum((bos.PRICE - lm.predict(X)) ** 2)) / (len(bos.PRICE - lm.predict(X))))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "Resnet_predictions = [np.argmax(Resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet]\nresnet_test_accuracy = 100*np.sum(np.array(Resnet_predictions)==np.argmax(test_targets, axis=1))/len(Resnet_predictions)\nprint('Test accuracy: %.4f%%' % resnet_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = lm.predict( X_test)\n", "intent": "Now that we have fit our model, let's evaluate its performance by predicting off the test values!\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "** Calculate the Mean Absolute Error, Mean Squared Error, and the Root Mean Squared Error.**\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "**Using Grid object to predict the class !! **\n"}
{"snippet": "predict=dtc.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "y_pred = dt.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "y_pred = rf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test, y_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "y_pred = lg.predict(X_val)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred = svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "def evaluate(sess, X, Y):\n    predicted = tf.cast(tf.arg_max(inference(X), 1), tf.int32)\n    print (sess.run(tf.reduce_mean(tf.cast(tf.equal (predicted, Y), tf.float32))))\n", "intent": "- The training function is also the same\n- For evaluation of accuracy, we need a slight change from the sigmoid version:\n"}
{"snippet": "def evaluate(sess, X, Y):\n    print(sess.run(inference([[55., 40.]])))\n    print(sess.run(inference([[50., 70.]])))\n    print(sess.run(inference([[90., 20.]])))\n    print(sess.run(inference([[90., 70.]])))\n", "intent": "We evaluate the resulting model:\n"}
{"snippet": "clf.predict(np.matrix([1, 2, 1, 1]))\n", "intent": "+ Can you modify the code to get a different result?\n"}
{"snippet": "print \"predicted outcome\", knn.predict(iris.data[:,2:])\nprint \"observed outcome\", iris.target\n", "intent": "+ Lets print out the raw outcome var vs predicted outcome\n"}
{"snippet": "print(classification_report(y_test,predict))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "(data['recommendation'] == gmm.predict(X)).value_counts()\n", "intent": "The clusters above are the assidgned political party found for the individual voters by the GMM model trained on the candidate data.\n"}
{"snippet": "print (sklearn.metrics.classification_report(y_test, y_pred))\n", "intent": "[Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n"}
{"snippet": "preds_test = model_sk.predict(X_test)\npreds_train = model_sk.predict(X_train)\n", "intent": "Predicting both train and test sets to evaluate model\n"}
{"snippet": "def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true.ravel() - y_pred.ravel()) / y_true.ravel())) * 100\n", "intent": "There is no MAPE implementation in sklearn (because this metric is undefined when real value is zero). Below one can find my own implementation\n"}
{"snippet": "def relative_error(grad, grad_num):\n    return np.sum((grad - grad_num) ** 2, axis=1) * 1. / np.sum(grad ** 2, axis=1)\n", "intent": "Plotting error curves\n"}
{"snippet": "y_pred_test = model_knn_sklearn.predict(X_test)\n", "intent": "Predict answers on the test set\n"}
{"snippet": "acc = accuracy_score(y_test, y_pred_test)\n", "intent": "Accuracy score with sklearn function\n"}
{"snippet": "y_pred_test = model_knn_weighted_sklearn.predict(X_test)\n", "intent": "Predict answers on the test set\n"}
{"snippet": "preds_test = model.predict(X_test)\npreds_train = model.predict(X_train)\n", "intent": "Predicting both train and test sets to evaluate model\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df['Cluster'],kms.labels_))\nprint(classification_report(df['Cluster'],kms.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "print(\"CONFUSION MATRIX:\")\nprint(confusion_matrix(y_test, y_pred))\nprint(\"----------------------------------------\\nMETRICS:\")\nprint(classification_report(y_test, preds_test))\n", "intent": " Confusion matrix, Precision, Recall, F1-Scores\n"}
{"snippet": "score_train = model_dense.evaluate(X_train.reshape((len(X_train), img_cols * img_rows)), y_train, verbose=0)\nscore_test = model_dense.evaluate(X_test.reshape((len(X_test), img_cols * img_rows)), y_test, verbose=0)\n", "intent": "Table to store the results of the experiments\n"}
{"snippet": "score_train = model_cnn.evaluate(X_train, y_train, verbose=0)\nscore_test = model_cnn.evaluate(X_test, y_test, verbose=0)\n", "intent": "Final evaluation of the model:\n"}
{"snippet": "score_train = model_dense.evaluate(X_train.reshape((-1, img_cols * img_rows)), y_train, verbose=0)\nscore_test = model_dense.evaluate(X_test.reshape((-1, img_cols * img_rows)), y_test, verbose=0)\n", "intent": "Table to store the results of the experiments\n"}
{"snippet": "score_train = model_column_onelayer.evaluate(X_train, y_train, verbose=0)\nscore_test = model_column_onelayer.evaluate(X_test, y_test, verbose=0)\n", "intent": "Final evaluation of the model:\n"}
{"snippet": "score_train = model_column_bidir.evaluate(X_train, y_train, verbose=0)\nscore_test = model_column_bidir.evaluate(X_test, y_test, verbose=0)\n", "intent": "Final evaluation of the model:\n"}
{"snippet": "score_train = model_pixel_bidir.evaluate(X_train.reshape(-1, img_cols * img_rows, 1), y_train, verbose=1)\nscore_test = model_pixel_bidir.evaluate(X_test.reshape(-1, img_cols * img_rows, 1), y_test, verbose=1)\n", "intent": "Final evaluation of the model:\n"}
{"snippet": "score_train = model_cnn_rnn_bidir.evaluate(X_train.reshape(-1, img_rows, img_cols, 1), y_train, verbose=1)\nscore_test = model_cnn_rnn_bidir.evaluate(X_test.reshape(-1, img_rows, img_cols, 1), y_test, verbose=1)\n", "intent": "Final evaluation of the model:\n"}
{"snippet": "preds_train = model.predict(X_train)\npreds_val = model.predict(X_val)\n", "intent": "And final accuracy of classification:\n"}
{"snippet": "predict=knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n", "intent": "Below there are two functions that to plot the results and calculate different metrics.\nAlso I implement MAPE metric.\n"}
{"snippet": "def gram_matrix_test(correct):\n    style_image = 'styles/starry_night.jpg'\n    style_size = 192\n    feats, _ = features_from_img(style_image, style_size)\n    student_output = gram_matrix(feats[5].clone()).cpu().data.numpy()\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.0001.\n"}
{"snippet": "def tv_loss_test(correct):\n    content_image = 'styles/tubingen.jpg'\n    image_size =  192\n    tv_weight = 2e-2\n    content_img = preprocess(PIL.Image.open(content_image), size=image_size)\n    student_output = tv_loss(content_img, tv_weight).cpu().data.numpy()\n    error = rel_error(correct, student_output)\n    print('Error is {:.3f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.0001.\n"}
{"snippet": "def content_loss_test(correct):\n    content_layer = 3\n    content_weight = 6e-2\n    c_feats = sess.run(model.extract_features()[content_layer], {model.image: content_img_test})\n    bad_img = tf.zeros(content_img_test.shape)\n    feats = model.extract_features(bad_img)[content_layer]\n    student_output = sess.run(content_loss(content_weight, c_feats, feats))\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ncontent_loss_test(answers['cl_out'])\n", "intent": "Test your content loss. You should see errors less than 0.0001.\n"}
{"snippet": "def gram_matrix_test(correct):\n    gram = gram_matrix(model.extract_features()[5])\n    student_output = sess.run(gram, {model.image: style_img_test})\n    error = rel_error(correct, student_output)\n    print('Maximum error is {:.3f}'.format(error))\ngram_matrix_test(answers['gm_out'])\n", "intent": "Test your Gram matrix code. You should see errors less than 0.0001.\n"}
{"snippet": "def tv_loss_test(correct):\n    tv_weight = 2e-2\n    t_loss = tv_loss(model.image, tv_weight)\n    student_output = sess.run(t_loss, {model.image: content_img_test})\n    error = rel_error(correct, student_output)\n    print('Error is {:.3f}'.format(error))\ntv_loss_test(answers['tv_out'])\n", "intent": "Test your TV loss implementation. Error should be less  than 0.0001.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "It looks like this classifier is worse, but perhaps more robust in general and generalizes better to new data.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "We can get test accuracy above `95%` after 12 epochs, but there is still a lot of margin for improvements via parameter tuning.\n"}
{"snippet": "print 'time_from_registration, %.3f' % roc_auc_score(df_all.label, df_all.time_from_registration)\nprint 'title_seen_cnt, %.3f' % roc_auc_score(df_all.label, df_all.title_seen_cnt)\n", "intent": "Let's check if some features give us a good baseline:\n"}
{"snippet": "predict = lr.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "otherlabels = rf.predict(otherfeatures)\n", "intent": "If you have the features of some previously unseen objects, you can then predict their class using the Random Forest's ```predict``` method:\n"}
{"snippet": "predictions = list(classifier.predict(X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(cd['Cluster'],kmeans.labels_))\nprint(classification_report(cd['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict default payment next month for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, clf.predict(X_test)))\n", "intent": "<div class=\"alert alert-success\">\n</div>\n"}
{"snippet": "scores = model_selection.cross_val_score(clf, iris.data, iris.target, cv=5)\nprint(scores)\nprint(scores.mean())\n", "intent": "K-Fold cross validation is just as easy; let's use a K of 5:\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(y, p4(int(x))    \nprint(r2)\n", "intent": "Looks pretty good! Let's measure the r-squared error:\n"}
{"snippet": "print(svc.predict([[150000, 40]]))\n", "intent": "Or just use predict for a given point:\n"}
{"snippet": "f1_score(test_label_y, predicted)\n", "intent": "<b>accuracy_score for bag of words with alpha 0.001  is 0.874</b>\n"}
{"snippet": "print(classification_report(y_test,predict))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "f1_score(test_label_y, predicted)\n", "intent": "<b>Accuracy for Multinomial NB with BOW is 0.897</b>\n"}
{"snippet": "print(\"precesion for MultinomialNB with BOW with alpha 1.701 is\",precision_score(test_label_y, predicted, average=\"macro\"))\nprint(\"Recall for MultinomialNB with BOW with alpha 1.701 is\",recall_score(test_label_y, predicted, average=\"macro\"))  \n", "intent": "<b>F1 score for Multinomial NB with BOW is 0.939</b>\n"}
{"snippet": "f1_score(test_label_y, predicted_tfidf)\n", "intent": "<b>accuracy_score for TF-iDF with alpha 0.001 is 0.890</b>\n"}
{"snippet": "forecast = model.predict(data_test_joined_2_regressor)\nforecast\n", "intent": "**Make a prediction**\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\nknn.predict(X_new)\nX_test = X\nprediction_result = knn.predict(X_test)\nprint(Y)\nprint(prediction_result)\nY == prediction_result\n", "intent": "- Returns a NumPy array, and we keep track of what the numbers \"mean\"\n- Can predict for multiple observations at once\n"}
{"snippet": "print(metrics.mean_absolute_error(y_true, y_pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print(metrics.mean_squared_error(y_true, y_pred))\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "print(np.sqrt(metrics.mean_squared_error(y_true, y_pred)))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "resnet50_pred = [np.argmax(resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\nresnet50_test_accuracy = 100*np.sum(np.array(resnet50_pred)==np.argmax(test_targets, axis=1))/len(resnet50_pred)\nprint('Test accuracy: %.4f%%' % resnet50_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predict=svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "score <- evaluate(model, x = x_test, y = y_test)\nprint(score)\n", "intent": "After training we can get the final loss for the test set by using the evaluate() fucntion.\n"}
{"snippet": "x_input = array([70, 80, 90])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)\n", "intent": "After the model is fit, we can use it to make a prediction.\n"}
{"snippet": "x_input = array([[70,75], [80,85], [90,95]])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)\n", "intent": "The shape of the input for making a single prediction must be 1 sample, 3 time steps, and 3 features, or [1, 3, 3]\n"}
{"snippet": "def get_embeddings(filenames):\n  faces = [extract_face(f) for f in filenames]\n  samples = asarray(faces, 'float32')\n  samples = preprocess_input(samples, version=2)\n  model = VGGFace(model='resnet50', \n                  include_top=False, input_shape=(224, 224, 3),\n  pooling='avg')\n  yhat = model.predict(samples) \n  return yhat\n", "intent": "Predicting face embeddings for a list of photographs.\n"}
{"snippet": "bg_fg_labels = model.predict(region_means)\nprint(bg_fg_labels)\n", "intent": "We see K-means algorithm found two clusters centered around 160 and the other clustered centered around 57. \n"}
{"snippet": "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint('loss:', loss)\nprint('accuracy:', accuracy)\n", "intent": "Now we can evaluate the model on the test data to get a sense of how we did.\n"}
{"snippet": "img, x = get_image(\"../assets/kitty.jpg\")\npredictions = model.predict(x)\nimshow(img)\nfor pred in decode_predictions(predictions)[0]:\n    print(\"predicted %s with probability %0.3f\" % (pred[1], pred[2]))\n", "intent": "We load an image into memory, convert it into an input vector, and see the model's top 5 predictions for it.\n"}
{"snippet": "img, x = get_image('../data/101_ObjectCategories/airplanes/image_0003.jpg')\nprobabilities = model_new.predict([x])\nprint(probabilities)\n", "intent": "To predict a new image, simply run the following code to get the probabilities for each class.\n"}
{"snippet": "from sklearn.metrics import classification_report\npred = kmeans.labels_\ny_test = new_df['Cluster']\nprint(classification_report(y_test,pred))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "metrics.adjusted_rand_score(clustering_KMeans.labels_,clustering_ward.labels_)  \n", "intent": "L'Adjusted Random Index est un coefficient qui permet de comparer deux clusterings : \nhttp://scikit-learn.org/stable/modules/clustering.html\n"}
{"snippet": "prediction = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,prediction))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predict(trX)\n", "intent": "Theano's \"predict\" output\n"}
{"snippet": "pred = classifier.predict(X_test)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "tfidf_pred = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(expected, predicted)\nprint( \"Accuracy = \" + str( accuracy ) )\n", "intent": "Accuracy is the ratio of the correct predictions (both positive and negative) to all predictions. \n$$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$\n"}
{"snippet": "test_pred = lcv.predict_proba(X_test_sparse)[:, 1]\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "predictions = lr.predict(new_data_reindexed)\npredictions = np.maximum(predictions, 0)\npredictions.astype(int) \n", "intent": "All good! Our new data has the same format as our training DataFrame, and we can make predictions.\n"}
{"snippet": "pred_test_lda = lda_clf.predict(X_test)\nprint('Prediction accuracy for the test dataset')\nprint('{:.2%}'.format(metrics.accuracy_score(y_test, pred_test_lda)))\n", "intent": "To verify that over model was not overfitted to the training dataset, let us evaluate the classifier's accuracy on the test dataset:\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(8)\nprint \"RMSE:\", np.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "predict(2016, 'Kansas', 'Connecticut') \n", "intent": "Lets follow kansas through the tournament\n"}
{"snippet": "predict(2016, 'Villanova', 'Oklahoma') \n", "intent": "Lets follow Villanova\n"}
{"snippet": "predict(1984, 'California', 'Hawaii')\n", "intent": "Future and past prediction will not work and the predict method will use data from the closest year where there is data available instead.\n"}
{"snippet": "predict(2017, 'Hawaii', 'California')\n", "intent": "Future prediction will not work and the predict method will use data from the most recent year where there is data available instead.\n"}
{"snippet": "from timeit import timeit\ndef GLC_time_evaluate(data, time, rmse):\n    model = []\n    time.append(timeit(lambda : GLC_linear_regression(data, model), number=1))\n    rmse.append(model[0].get('training_rmse'))\n", "intent": "Let's define an additional function for each package that will handle timing and evaluating our models.\n"}
{"snippet": "np_time = []\nnp_rmse = []\nrows_range = range(5000, 90001, 2500)\nfor n_rows in rows_range:\n    sf = data.head(n_rows)\n    SKL_time_evaluate(sf, np_time, np_rmse)\n", "intent": "Time scikit-learn backed by SFrame on the first 90K rows of data and save the results.\n"}
{"snippet": "n = len(X_test_pca)\np = len(X_test_pca.columns)\nstart2=time()\nt_y=t_knn.predict(X_test_pca)\nend2=time()\nRSS = sum((y_test-t_y)**2)\nt_aic=2*p+n*math.log(RSS/n)\nt_bic = n*math.log(RSS/n)+p*math.log(n)\nt_r2=adj_r2_score(knn,X_test_pca,y_test)\n", "intent": "Evaluate the model with metrics\n"}
{"snippet": "n = len(X_test_pca)\np = len(X_test_pca.columns)\nstart2=time()\np_y=ada_neigh.predict(X_test_pca)\nend2=time()\nRSS = sum((y_test-p_y)**2)\nAIC=2*p+n*np.log(RSS/n)\nBIC = n*np.log(RSS/n)+p*np.log(n)\nR2=adj_r2_score(ada_neigh,X_test_pca,y_test)\n", "intent": "Evaluate the model with metrics\n"}
{"snippet": "knn.predict_proba([[3, 5, 4, 2],])\n", "intent": "You can also do probabilistic predictions:\n"}
{"snippet": "model = VGG19(weights='imagenet', include_top=False)\nvgg19_feature_train = model.predict(x_train)\nvgg19_feature_test = model.predict(x_test)\n", "intent": "Build VGG19 and extract the last-layer features for both the training set and the test set.\n"}
{"snippet": "model2 = ResNet50(weights='imagenet', include_top=False)\nresnet_feature_train = model2.predict(x_train)\nresnet_feature_test = model2.predict(x_test)\n", "intent": "Build ResNet50 and extract the last-layer features for both the training set and the test set.\n"}
{"snippet": "def train_and_test(n):\n    random.seed(n)\n    return predict(df_train, df_test, model_type='chained_rf', output_prediction=True, seed=n).f_label_1_predicted\nn = 1000\nr = random.randint(1, 100000000)\nresult = parallelize(range(r, r+n), train_and_test)\npredicted_test = sum(result)/n\npredicted_test = df[['date']].join(predicted_test).dropna()\npredicted_test['date'] = (predicted_test['date'] + pd.DateOffset(1)).map(lambda x: '%s/%s/%s' % (x.date().day, x.date().month, x.date().year))\npredicted_test.columns = ['date', 'label']\n", "intent": "Below are the predictions we made on the test set. The results were uploaded to Kaggle and got a 52.9% AUC score.\n"}
{"snippet": "roc_auc_score(y_test, lm.predict(X_test))\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "print metrics.classification_report(C.TrueAdmit, C.RF1preds)\n", "intent": "Random Forest wit `n_classifiers = 1` has pretty different precision than with the default which is 10\n"}
{"snippet": "tr_resolver = neural_net.evaluate(tr_word_lstm, tr_attn_layer, tr_scorer, all_words, all_markables, coref_features.minimal_features)\n", "intent": "Let's evaluate on the entire dataset.\n"}
{"snippet": "tags = model.predict(sentence)\nprint (tags[0:3])\n", "intent": "- we provide the `predict()` function that returns the set of tags obtained for the specific input by the model.\n"}
{"snippet": "shutil.rmtree('babyweight_trained_dnn', ignore_errors = True) \ntrain_and_evaluate('babyweight_trained_dnn')\n", "intent": "Finally, we train the model!\n"}
{"snippet": "shutil.rmtree('babyweight_trained_wd', ignore_errors = True) \ntrain_and_evaluate('babyweight_trained_wd')\n", "intent": "Finally, we train the model!\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, ypred)\n", "intent": "We can check our classification accuracy by comparing the true values of the test set to the predictions:\n"}
{"snippet": "print \"Class predictions according to Sklearn:\" \nprint sentiment_model.predict_proba(sample_test_matrix)\n", "intent": "**Checkpoint**: Make sure your probability predictions match the ones obtained from GraphLab Create.\n"}
{"snippet": "print('Estimated number of clusters: %d' % n_clusters_)\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(y, labels))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels))\nprint(\"Adjusted Rand Index: %0.3f\"\n      % metrics.adjusted_rand_score(y, labels))\nprint(\"Adjusted Mutual Information: %0.3f\"\n      % metrics.adjusted_mutual_info_score(y, labels))\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, labels))\n", "intent": "Now, we can use a handy chunk of code from the Scitkit documentation to measure the performance of our model \n"}
{"snippet": "loss, accuracy = model.evaluate(X_test, y_test)\nprint('loss:', loss)\nprint('accuracy:', accuracy)\n", "intent": "We can then evaluate the model much like we would in sklearn: \n"}
{"snippet": "print classification_report(insults_test[\"Insult\"], predicted, target_names=['Insult', \"Neutral\"])\n", "intent": "Check the classification report\n"}
{"snippet": "wing_size_predictions = insects_regression.predict(X_insects)\n", "intent": "3\\. Use the `predict` method to compute predicted values your data (could be the data you used to train the model, or another dataset entirely.\n"}
{"snippet": "wells_predictions = wells_regression.predict(X_wells)\nprint(wells_predictions[:10])\n", "intent": "Now that the regression has been fir, we can use the `predict` method to forecast whether our model thinks a family will switch wells.\n"}
{"snippet": "insects_predictions = p.predict(X_insects)\n", "intent": "Since the last object in our pipeline is a `LinearRegression`, the pipeline has a `predict` method.\n"}
{"snippet": "np.sum((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "sklearn.metrics.mean_squared_error(bos.PRICE, lm.predict(X))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "predicted = model2.predict(X_test)\nprint(predicted)\n", "intent": "We now need to predict class labels for the test set. We will also generate the class probabilities, just to take a look.\n"}
{"snippet": "score = model.evaluate(x_train, y_train)\nprint(\"\\n Training Accuracy:\", score[1])\nscore = model.evaluate(x_test, y_test)\nprint(\"\\n Testing Accuracy:\", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "cross_val_score(model_lr,features,target,cv=10).mean()\n", "intent": "**3) Implement cross-validation for your logistic regression model. Select the number of folds. Explain your choice.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,model.predict(fit_test))\nprint cm,'\\n'\nprint \"Precision:\",float(cm[1,1])/(cm[0,1]+cm[1,1])\nprint \"Recall:\",float(cm[1,1])/(cm[1,0]+cm[1,1]),'\\n'\n", "intent": "**4) Display the confusion matrix, classification report, and AUC.**\n"}
{"snippet": "print metrics.confusion_matrix(y_test, predicted)\nprint metrics.classification_report(y_test,predicted)\n", "intent": "This above returns the same score we calculated earlier for the model.\n"}
{"snippet": "y_pred = clf.predict(cnn_codes_test)\nprint(\"For params: %s we got results: acc: %.2f, F1: %.2f\" % \n      (str(clf.best_params_), acc(y_test, y_pred), f1_score(y_test, y_pred, average='micro')))\n", "intent": "And finally, I found the best parameters and got results:\n"}
{"snippet": "results = []\nfor model in models:\n    y_pred = model['model'].predict(cnn_codes_test)\n    print(\"Testing model with parameters %s is done\" % str(model['params']))\n    results.append(y_pred)\n", "intent": "The difference between their accuracies is small, so I decided to use all models. \n"}
{"snippet": "score = mod.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\ndef print_mean_squared_error():\n    mse =  \n    print(\"Mean squared error: {:.2f}\".format(mse))\n", "intent": "Write a function that:\n* Takes y_test en y_pred as parameters.\n* Calculates the mean squared error\n* Prints the result.\n"}
{"snippet": "print_mean_squared_error(y_test, y_pred)\n", "intent": "Let's calculate the mean squared error for our model.\n"}
{"snippet": "h=.1\nx_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\ny_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\nxx, yy = numpy.meshgrid(np.arange(x_min, x_max, h), numpy.arange(y_min, y_max, h))\nZ = logclf.predict(numpy.c_[xx.ravel(), yy.ravel()])\n", "intent": "* First we plot the decision boundary, followed by training and testing data points.\n"}
{"snippet": "print_r2_score(y_test, y_pred)\n", "intent": "Now we calculate the $R^2$ score for our trained model.\n"}
{"snippet": "with np.load('data/mystery_data_new.npz') as data:\n    popularity_new = data['popularity_new']\nprint \"Predicted L2 Error:\", l2_error(popularity_new, predicted_popularity_new)\n", "intent": "At the end of the year, we tally up the popularity numbers for each celeb and check how well we did on our predictions.\n"}
{"snippet": "print(classification_report(y_test,y_pred))\n", "intent": "It looks like our KNN model is already pretty good. Let's check the `classification_report`.\n"}
{"snippet": "from collections import Counter\nprint(Counter(y_clean))\ny_clean.shape[0]\nbase_line = np.ones(y_clean.shape[0])*4\nprint('Estimated baseline performance:  ',accuracy_score(y_clean, base_line))\n", "intent": "Estimate the baseline performance.\n"}
{"snippet": "COD = r2_score(y,cv_predict_multi)\nMAE = mean_absolute_error(y,cv_predict_multi)\nRMSE = np.sqrt(mean_squared_error(y, cv_predict_multi))\nCC = np.corrcoef(y,cv_predict_multi)\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "auto_numeric_log = np.log(auto_numeric)\nX_multi_log = auto_numeric_log.drop(['price'],axis=1).values\ny_log = auto_numeric_log['price']\ncv_predict_multi_log = cross_val_predict(new_lr, X_multi_log, y_log, cv = kf)\nCOD = r2_score(y_log,cv_predict_multi_log)\nMAE = mean_absolute_error(y_log,cv_predict_multi_log)\nRMSE = np.sqrt(mean_squared_error(y_transf, cv_predict_multi_log))\nCC = np.corrcoef(y_log,cv_predict_multi_log)\n", "intent": "Now re-build a Linear Regression model on the transformed dataset and report the R^2, RMSE, MAE and CC metrics.\n"}
{"snippet": "from sklearn.metrics import silhouette_score\ndef get_silhouette_score(data, model):\n    pass\nprint \"Silhouette Score for KMeans with 5 clusters = %lf\" % 0.0\nprint \"Silhouette Score for KMeans with 25 clusters = %lf \" % 0.0\nprint \"Silhouette Score for KMeans with 50 clusters = %lf \" % 0.0\n", "intent": "**c.** Calculate the Silhouette Score using 500 sample points for all the kmeans models.\n"}
{"snippet": "print fit_model_and_score(train_cats_features, train_cats_response, validation_cats_features, validation_cats_response)\n", "intent": "**b.** Now that you've added the categorical data, let's see how it works with a linear model!\n"}
{"snippet": "preds = model.predict(test_sample)\nerrors = [i for i in xrange(0, len(test_sample)) if preds[i] != test_labels_sample[i]]\nfor i in errors:\n    pass \n", "intent": "* Next, visualize the nearest neighbors of cases where the model makes erroneous predictions\n"}
{"snippet": "h=.1\nx_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\ny_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logclf.predict(np.c_[xx.ravel(), yy.ravel()])\nprint yy\nprint xx.ravel()\n", "intent": "* First we plot the decision boundary, followed by training and testing data points.\n"}
{"snippet": "trainPredict = model.predict(X_train)[:,0]\ntestPredict = model.predict(X_test)[:,0]\nfrom sklearn.metrics import mean_squared_error\nprint \"MSE train %.3f\"%mean_squared_error(trainPredict, y_train)\nprint \"MSE test  %.3f\"%mean_squared_error(testPredict, y_test)\n", "intent": "Obtenemos predicciones para los datos de entrenamiento y para los de test.\n"}
{"snippet": "preds =  model.predict(X_test)[:,0]\n", "intent": "predict test prices\n"}
{"snippet": "predict = lg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print 'predicted:', spam_detector.predict(tfidf4)[0]\nprint 'expected:', messages.label[3]\n", "intent": "Let's try classifying our single random message:\n"}
{"snippet": "all_predictions = spam_detector.predict(messages_tfidf)\nprint all_predictions\n", "intent": "Hooray! You can try it with your own texts, too.\nA natural question is to ask, how many messages do we classify correctly overall?\n"}
{"snippet": "print classification_report(messages['label'], all_predictions)\n", "intent": "From this confusion matrix, we can compute precision and recall, or their combination (harmonic mean) F1:\n"}
{"snippet": "predictions = nb_detector.predict(msg_test)\nprint confusion_matrix(label_test, predictions)\nprint classification_report(label_test, predictions)\n", "intent": "And overall scores on the test set, the one we haven't used at all during training:\n"}
{"snippet": "print svm_detector.predict([\"Hi mom, how are you?\"])[0]\nprint svm_detector.predict([\"WINNER! Credit for free!\"])[0]\n", "intent": "So apparently, linear kernel with `C=1` is the best parameter combination.\nSanity check again:\n"}
{"snippet": "print 'before:', svm_detector.predict([message4])[0]\nprint 'after:', svm_detector_reloaded.predict([message4])[0]\n", "intent": "The loaded result is an object that behaves identically to the original:\n"}
{"snippet": "def ols_predict(x_input, y_intercept, coefficients):\n    y_intercept + (coefficient)(Xvar)\n    return \n", "intent": "Below, write the function that would use this output to predict new data elements (finish the return statement).\n"}
{"snippet": "cm = sklearn.metrics.confusion_matrix(y_train,clf.predict(X_train))\ncm_norm = cm/cm.sum(axis=1)[:, np.newaxis]\n", "intent": "Plot the (normalised) confusion matrix for the training data. Label axes appropriately.\n"}
{"snippet": "r2 = r2_score(auto_numeric['price'], predicted3)\nrmse = np.sqrt(mean_squared_error(auto_numeric['price'], predicted3))\nmae = mean_absolute_error(auto_numeric['price'], predicted3) \ncoeff = np.corrcoef(auto_numeric['price'],predicted3)[0,1]\nprint(\"r2: {}\\tRMSE: {}\\tMAE: {}\\tCC: {}\".format(r2, rmse, mae, coeff))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "mymodel_predictions = [np.argmax(mymodel.predict(np.expand_dims(feature, axis=0))) for feature in test_network]\ntest_accuracy = 100*np.sum(np.array(mymodel_predictions)==np.argmax(test_targets, axis=1))/len(mymodel_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "fpred = forest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,fpred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions = log.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print(classification_report(y_test,predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "preds = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "h=.1\nx_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\ny_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logclf.predict(np.c_[xx.ravel(), yy.ravel()])\n", "intent": "* First we plot the decision boundary, followed by training and testing data points.\n"}
{"snippet": "split = len(null_collapsed_df)/2\ntrain = null_collapsed_df[:split]\ntest  = null_collapsed_df[split:]\ntest['Prediction'] = train['Expected'].median()\nbasic_trainer.evaluate(test)\n", "intent": "The Null DF has no usable input data, so we'll simply use the median Expected value as the prediction. It doesn't count on the leaderboard anyway.\n"}
{"snippet": "def KM_error(data, centroids, assignment):\n    total_error = 0\n    for i in range(data.shape[0]):\n        total_error += np.linalg.norm(data[i,:]   -   centroids[assignment[i]])**2\n    return total_error \n", "intent": "Complete this function that calculates the reconstruction error of a clustering assignment according to the lecture notes.\n"}
{"snippet": "def km_error(data, centroids, assignment):\n    total_error = 0\n    for i in range(data.shape[0]):\n        total_error += np.linalg.norm(data[i,:] - centroids[assignment[i]])**2\n    return total_error \n", "intent": "Complete this function that calculates the reconstruction error of a clustering assignment according to the lecture notes.\n"}
{"snippet": "def km_error(data, centroids, assignment):\n    total_error = 0\n    for i in range(data.shape[0]):\n        total_error += np.linalg.norm(data[i,:] - centroids[int(assignment[i])])**2\n    return total_error \n", "intent": "Complete this function that calculates the reconstruction error of a clustering assignment according to the lecture notes.\n"}
{"snippet": "def km_error(data, centroids, assignment):\n    total_error = 0\n    for i in range(data.shape[0]):\n        total_error += np.linalg.norm(data[i,:]   -   centroids[assignment[i]])**2\n    return total_error \n", "intent": "Complete this function that calculates the reconstruction error of a clustering assignment according to the lecture notes.\n"}
{"snippet": "predict=model.predict(data[['Population','Violent_crime']])\nprint(\"Mean squared error: %.2f\"\n      % np.mean((predict - data['Property_crime']) ** 2))\n", "intent": "<b>Calculate mean square error:</b>\n"}
{"snippet": "model1.predict([10000,750])\n", "intent": "A person with FICO score 750 and want to request a loan amount $10,000. From the above model, we can know the interest rate he need to pay monthly.\n"}
{"snippet": "model.predict([[10000,750]])\n", "intent": "The f-score for \"1\" prediction is lower than 50%. There are many green cross (differences between predicted result and real result) in above plot. \n"}
{"snippet": "data['predict']=model.predict(data[['Sepal_length', 'Sepal_width', 'Petal_length', 'Petal_width']])\ndata['diff'] = np.where(data['spec_num'] == data['predict'], 1, 0)\ndata['diff'].value_counts()\ndata['diff'].unique()\npredict = data[data['diff'] == 0]\n", "intent": "<h2>Run cross validation the logistic model</h2>\n"}
{"snippet": "y_pred = clf.predict(X_test)\ndef print_cluster(images, y_pred, cluster_number):\n    images = images[y_pred==cluster_number]\n    y_pred = y_pred[y_pred==cluster_number]\n    print_digits(images, y_pred, max_n=10)\nfor i in range(10):\n     print_cluster(images_test, y_pred, i)\n", "intent": "Predict and show predicted clusters.\n"}
{"snippet": "r2_score(y_test, load_pred)\n", "intent": "**1.4 (5 points)** To validate the regression, find $R^2$ of your predction (Your answer needs to be rounded to 2-decimal places).\n"}
{"snippet": "incept_predictions = [np.argmax(incept_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(incept_predictions)==np.argmax(test_targets, axis=1))/len(incept_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(pipeline, X_train, y_train)\n", "intent": "Cross-validation with a pipeline\n---------------------------------\n"}
{"snippet": "def get_accuracy(model, datas, labels):\n    preds = np.array([np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in datas])\n    accuracy = 100*np.sum(preds==np.argmax(labels, axis=1))/len(preds)\n    return accuracy\ndef get_report(trained_model):\n    print('Train accuracy: %.4f%%' % get_accuracy(trained_model, train_tensors, train_targets))\n    print('Valid accuracy: %.4f%%' % get_accuracy(trained_model, valid_tensors, valid_targets))\n    print('Test accuracy:  %.4f%%' % get_accuracy(trained_model, test_tensors, test_targets))\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "y_pred = classifier.predict(X_test)\nclassification_report(y_test, y_pred)\n", "intent": "Metrics for each class: precision, recall, f1-score and support. \nhttps://en.wikipedia.org/wiki/Precision_and_recall\n"}
{"snippet": "y_pred = classifier.predict(X_test)\nprint(classification_report(y_test, y_pred))\n", "intent": "Metrics for each class: precision, recall, f1-score and support. \nhttps://en.wikipedia.org/wiki/Precision_and_recall\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy_Xception = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy_Xception)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "pred=dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "preds=rtf.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn import metrics\nprint \"Addjusted rand score:{:.2}\".format(metrics.adjusted_rand_score(y_test, y_pred))\nprint \"Homogeneity score:{:.2} \".format(metrics.homogeneity_score(y_test, y_pred)) \nprint \"Completeness score: {:.2} \".format(metrics.completeness_score(y_test, y_pred))\nprint \"Confusion matrix\"\nprint metrics.confusion_matrix(y_test, y_pred)\n", "intent": "Show different performance metrics, compared with \"original\" clusters (using the knowb number class)\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix \nprint(classification_report(data['Cluster'],kmeans.labels_))\nprint(confusion_matrix(data['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "pred=knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predict=lr.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predict))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "preds=nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "preds=pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "preds=model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "glass[\"y_pred\"] = linreg.predict(X)\n", "intent": "- Add a column `y_pred` to `glass` that stores the model's fitted values for the refractice index.\n"}
{"snippet": "linreg.predict(2)\n", "intent": "- Confirm that this is the same value we would get when using the built-in `.predict()` method of the `LinearRegression` object.\n"}
{"snippet": "log_preds = learn.predict(is_test=True)\npreds = np.argmax(log_preds, axis=1) \nprobs = np.exp(log_preds[:,1])\n", "intent": "**Without using TTA (run either of them)**\n"}
{"snippet": "metrics.mean_squared_error(y, np.exp(y_pred_log))\n", "intent": "Not a fair comparison! MSE for the second model is in log-space.\n"}
{"snippet": "X_test = test.loc[:, feature_cols]\ny_test = test.loc[:, 'price']\ny_pred = treereg.predict(X_test)\ny_pred\n", "intent": "**Bonus**: Use the fitted model to check your answers.\n"}
{"snippet": "from sklearn.model_selection import KFold\nkf = KFold(5, shuffle=True)\nnp.mean(-cross_val_score(dtr, X, y, cv=kf, scoring='neg_mean_squared_error'))\n", "intent": "- Train your model and calculates its MSE in five-fold cross-validation.\n"}
{"snippet": "silhouette_score(iris_scaled, km.labels_)\n", "intent": "Here we get the average silhouette score for all points in our dataset:\n"}
{"snippet": "silhouette_score(iris_features_df_scaled, km.labels_)\n", "intent": "Here we get the average silhouette score for all points in our dataset:\n"}
{"snippet": "X_test_scaled = scaler.transform(X_test)\ntest_preds = knc.predict(X_test_scaled)\ntest_preds\n", "intent": "Now let's scale the whole validation dataset and predict the whole thing!\n"}
{"snippet": "train_preds = knc.predict(X_train_scaled)\ntrain_preds[:10]\n", "intent": "Now, let's see how well it learned the training set.\n"}
{"snippet": "test_preds = knr.predict(X_test_scaled)\ntest_preds[:10]\n", "intent": "Let's use our trained KNeighborsRegressor to predict the value for our scaled datapoint:\n"}
{"snippet": "train_preds = kn.predict(X_ts)\n", "intent": "Now, let's see how well it learned the training set.\n"}
{"snippet": "confusion = np.zeros((len(classes), len(classes)))\nfor ii, train_class in enumerate(classes):\n    for jj in range(ii, len(classes)):\n        confusion[ii, jj] = roc_auc_score(y == train_class, y_pred[:, jj])\n        confusion[jj, ii] = confusion[ii, jj]\n", "intent": "Compute confusion matrix using ROC-AUC\n"}
{"snippet": "train_preds = knc.predict(X_train_scaled)\n", "intent": "Now, let's see how well it learned the training set.\n"}
{"snippet": "test_preds = knr.predict(X_test_scaled)\ntest_preds[:5]\n", "intent": "Let's use our trained KNeighborsRegressor to predict the value for our scaled datapoint:\n"}
{"snippet": "y_pred_class = knn.predict(X_sc)\nfrom sklearn import metrics\nprint metrics.accuracy_score(y, y_pred_class)\n", "intent": "- Is it better than baseline?\n- Is it legitimate?\n"}
{"snippet": "kfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n      cross_val_score(tree, iris.data, iris.target, cv=kfold)))\n", "intent": "This way, we can verify that it is indeed a really bad idea to use three-fold (nonstratified) cross-validation on the iris dataset:\n"}
{"snippet": "from sklearn.model_selection import ShuffleSplit\nshuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\nscores = cross_val_score(tree, iris.data, iris.target, cv=shuffle_split)\nprint(\"Cross-validation scores:\\n{}\".format(scores))\n", "intent": "The following code splits the dataset into 50% training set and 50% test set for 10 iterations:\n"}
{"snippet": "xval_chance = (-1 * logreg.intercept_[0])/logreg.coef_[0][0]\nprint xval_chance*logreg.coef_[0][0] + logreg.intercept_\nprint xval_chance\nlogreg.predict_proba([[xval_chance]])\n", "intent": "Since we only have $\\beta_0$ and $\\beta_1$,\n$$ \\beta_0 + \\beta_1x = 0$$\n$$ -\\beta_0  = \\beta_1x $$\n$$ x = \\frac{-\\beta_0}{\\beta_1} $$\n"}
{"snippet": "def accuracy_score(actual, predicted):\n    return np.array(actual == predicted).mean()\n", "intent": "Run this code block to define an accuracy function.\n"}
{"snippet": "accuracy_score(predictions, titanic_target[:5])\n", "intent": "Run this block of code to test the accuracy of the prediction.\n"}
{"snippet": "test_ypred = gd_best.predict(test_x)\nprint ('Gradient Boost Test MSE:', mean_squared_error(test_ypred, test_y))\nprint ('Gradient Boost Test R2:',r2_score(test_ypred, test_y))\n", "intent": "Report the final MSE and R^2.\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "clf.predict([[1,1,1,1]])\n", "intent": "We can use the decision tree for prediction.\nFirst lets predict the values of a feature vector containing [1,1,1,1]:\n"}
{"snippet": "final_pred = lin_model.predict(X_test)\nprint('Test set rmse: ', rmse(final_pred, y_test))\n", "intent": "Finally, select one final model to make predictions for your test set. This is often the model that performed best on the validation data.\n"}
{"snippet": "svm_test_pred = svm_model.predict(X_test)\nnp.mean(svm_test_pred == y_test)\n", "intent": "Choose your best classifier and use it to predict on the test set. Report the mean accuracy and confusion matrix. \n"}
{"snippet": "lin_val_pred = lin_model.predict(X_val_my_feats)\nsecond_train_error = rmse(lin_pred, y_train)\nsecond_val_error = rmse(lin_val_pred, y_val)\nprint(\"Training RMSE:\", second_train_error)\nprint(\"Test RMSE:\", second_val_error)\n", "intent": "**Question 1.5:** What is the rmse for both the prediction of X_train and X_val?\n"}
{"snippet": "scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Error: %.2f%%\" % (100-scores[1]*100))\n", "intent": "Let's evaluate our model on the test dataset.\n"}
{"snippet": "y_pred_lstm = lstm_model.predict(XTest_batch)\nprint(yTest.shape)\nprint(y_pred_lstm.shape)\nprint(\"MAE: {0:.2f}\".format(mean_absolute_error(yTest, _3d_to_2d(y_pred_lstm)[:125])))\nprint(\"MSE: {0:.2f}\".format(mean_squared_error(yTest, _3d_to_2d(y_pred_lstm)[:125])))\n", "intent": "Generate predictions and check the mean squared error!\n"}
{"snippet": "print metrics.classification_report(y_test, y_pred)\n", "intent": "There is a special `classification_report` function that gives some of the main performance metrics:\n"}
{"snippet": "print regr.predict(bill_gates['sqft_living'].reshape(-1, 1))\n", "intent": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Bill_gates%27_house.jpg/2560px-Bill_gates%27_house.jpg\">\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmse_train = mean_squared_error(yhat_train, y_train)\nmse_valid = mean_squared_error(yhat_valid, y_valid)\nprint(\"Training MSE:   {:.3f}\".format(mse_train))\nprint(\"Validation MSE: {:.3f}\".format(mse_valid))\n", "intent": "Finally, we can use the `mean_square_error` method to compute the MSE of the predictions.  \n"}
{"snippet": "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "yhat_train = deg9regpipe.predict(X_train)\nyhat_valid = deg9regpipe.predict(X_valid)\nmse_train = mean_squared_error(yhat_train, y_train)\nmse_valid = mean_squared_error(yhat_valid, y_valid)\nprint(\"Training MSE:   {:.3f}\".format(mse_train))\nprint(\"Validation MSE: {:.3f}\".format(mse_valid))\nxplot = np.linspace(-60,60,100).reshape(-1,1)\nyplot = deg9regpipe.predict(xplot) \ndam_plot([(X_train, y_train, \"training\"), (X_valid, y_valid, \"validation\")], [(xplot, yplot, \"model\")])\n", "intent": "Again, we'll fit the model, print errors, and make a plot. How do the resulting errors and plot compare to the linear model? \n"}
{"snippet": "predictions_train = clf.predict(df_train[features])\nprobs_train = clf.predict_proba(df_train[features])\ndisplay(predictions_train)\n", "intent": "    We are able to achive more that 99% accuracy for TRAIN and ~98% accuracy for TEST VALIDATION\n"}
{"snippet": "print(iris_model.predict(iris_features_test))\nprint(iris_target_test)\n", "intent": "Compare predictions with actual results\n"}
{"snippet": "iris_model.predict_proba(iris_features_test)\n", "intent": "Predict probabilities\n"}
{"snippet": "validation_data = np.load('features_validation.npy')\nval_pred_class = model.predict_classes(validation_data,verbose=0) \nprint('Accuracy on validation set: ',np.mean(val_pred_class.ravel()==val_labels)*100,'%')\nprint('\\nVal loss & val_acc')\nprint(model.evaluate(validation_data,val_labels,verbose=0))\n", "intent": "<a id='sec5'></a>\n___\n"}
{"snippet": "y_predict = pipe.predict(x_test)\n", "intent": "Now we can see how how good is our fit by testing the model on data it hasn't seen yet. \n"}
{"snippet": "print (\"Fit a model X_train, and calculate MSE with Y_train:\", np.mean((Y_train - lm.predict(X_train)) ** 2))\nprint (\"Fit a model X_train, and calculate MSE with X_test, Y_test:\", np.mean((Y_test - lm.predict(X_test)) ** 2))\n", "intent": "Now, calculate the mean squared error using just the test data and compare to mean squared from using all the data to fit the model. \n"}
{"snippet": "print (np.sum((faithful.eruptions - resultsW0.predict(X)) ** 2))\n", "intent": "The residual sum of squares: \n"}
{"snippet": "print (np.mean((faithful.eruptions - resultsW0.predict(X)) ** 2))\n", "intent": "Mean squared error: \n"}
{"snippet": "resnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_pred = knn.predict(X1)\ny_true = df_Target.values\n", "intent": "Build Confusion Matrix\n"}
{"snippet": "metrics.silhouette_score(dn, labels, metric='euclidean')\n", "intent": "Find the Silhoutte Score and plot\n"}
{"snippet": "metrics.silhouette_score(newDF, labels, metric='euclidean')\n", "intent": "Compute the Silhoutte Score to measure your analysis\n"}
{"snippet": "metrics.accuracy_score(labels, y)\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "metrics.silhouette_score(y, labels, metric=\"euclidean\")\n", "intent": "Compute the silhoutte coefficient to see how consistent our data is within the clusters\n"}
{"snippet": "print metrics.classification_report(y,labels)\n", "intent": "Calculate the Precision, Recall, and F - Score to see the test's accuracy\n"}
{"snippet": "def vae_loss(x, x_decoded_mean):\n    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n    return xent_loss + kl_loss\n", "intent": "We will also need to build our intermediate layer taking into account the loss from both the regular learner and the mean and variance vectors\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint('Accuracy: {:0.2f}'.format(accuracy_score(y_test, y_pred)))\n", "intent": "[sklearn.metrics.accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_Model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "tree_predictions = tree.predict(X_test) \n", "intent": "Make a prediction with the trained model on the test data.\n"}
{"snippet": "pred = model.predict(np.sign(data[cols]))\npred[:15]\n", "intent": "In the prediction, a `+1` means a positive return is expected and a `-1` means a negative return is expected.\n"}
{"snippet": "y_pred = treereg.predict(X_test)\ny_pred\n", "intent": "<a id=\"testing\"></a>\n---\nTesting model trained on training data on testing split\n"}
{"snippet": "bikes.loc[:,'predictions'] = lr_temp.predict(X)\nbikes\n", "intent": "- Store `lr_all`'s fitted values in a new `predictions` column of the `bikes` DataFrame.\n"}
{"snippet": "glass.loc[:,'y_pred'] = linreg.predict(X)\nglass\n", "intent": "- Add a column `y_pred` to `glass` that stores the model's fitted values for the refractice index.\n"}
{"snippet": "logit_pred_proba = logit_simple.predict_proba(X_test)[:,1] \ny_pred = logit_pred_proba > .5\nmetrics.confusion_matrix(y_test, y_pred)\n", "intent": "-- it has no predictive power over a null model that just predicts 0 every time\n"}
{"snippet": "X_test = test.loc[:, feature_cols]\ntreereg.predict(X_test)\n", "intent": "**Bonus**: Use the fitted model to check your answers.\n"}
{"snippet": "from sklearn.metrics import r2_score\ny_pred_train = model.predict(X_train)\nprint ('r squared on train data {}%'.format(round(r2_score(y_train, y_pred_train)*100,2)))\ny_pred = model.predict(X_test)\n", "intent": "R^2 (coefficient of determination) regression score function.\n"}
{"snippet": "print(np.sqrt(metrics.mean_squared_error(y_predict,y_test)))\n", "intent": "from sklearn import metrics\n"}
{"snippet": "S = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nS\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "cv_scores = cross_val_score(rf, X_train, y_train, cv=3)\n", "intent": "Perfrom cross-validation.\n"}
{"snippet": "best = clf.best_estimator_\nprint 'Best estimator: ', best\nscores = cross_validation.cross_val_score(best, X_test, y_test, cv=10)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n", "intent": "Then we can check the accuracy on the **Test Set**:\n"}
{"snippet": "dt_pred = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "y_predict = mod.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "y_pred = logmod.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "expectedF = Y_test\npredictedF = classifierForest.predict(X_test)\npredictedS = classifierSVC.predict(X_test)\nprint(expectedF)\nprint(predictedF)\nprint(predictedS)\n", "intent": "Let's test how good the system is doing\n"}
{"snippet": "y_predF = classifierForest.predict(X_real)\ny_predS = classifierSVC.predict(X_real)\n", "intent": "Then we make the predictions with both classifiers\n"}
{"snippet": "decoded_imgs = autoencoder.predict(x_test)\n", "intent": "Epoch 50/50\n60000/60000 [==============================] - 4s - loss: 0.0958 - val_loss: 0.0942\n"}
{"snippet": "theta_min = np.matrix(result[0])\npredictions = predict(theta_min, X)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y)]\naccuracy = (sum(map(int, correct)) % len(correct))\nprint ('accuracy = {0}%'.format(accuracy))\n", "intent": "We can then use this function to score the training accuracy of our classifier.\n"}
{"snippet": "theta_min = np.matrix(result2[0])\npredictions = predict(theta_min, X2)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y2)]\naccuracy = (sum(map(int, correct)) % len(correct))\nprint(\"accuracy = {}%\".format(accuracy))\n", "intent": "Finally, we can use the prediction function from part 1 to see how accurate our solution is on the training data.\n"}
{"snippet": "forest_predictions = rf.predict(X_test) \n", "intent": "Make predictions for the test data.\n"}
{"snippet": "w1 = 0\nw2 = 0\nw3 = 0\nw4 = 0\ndef mlpredict(dp):\n    return w1 + w2*dp['maxtemp'] + w3*dp['mintemp'] + w4*dp['maxvib']\n", "intent": "Note: Regression, in this context means that the output is a continuous dependent variable.\n"}
{"snippet": "mse = sklearn.metrics.mean_squared_error(Y_test, Y_pred)\nprint(mse)\n", "intent": "Falta adicionar o r_squared\n"}
{"snippet": "y_pred = my_tree.predict(X_train)\naccuracy = metrics.accuracy_score(y_train, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_train, y_pred))\nprint(metrics.confusion_matrix(y_train, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_train, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the decision tree on the training set\n"}
{"snippet": "y_pred = my_tree.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred))\nprint(metrics.confusion_matrix(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the tree on the validation dataset\n"}
{"snippet": "y_pred = my_tree.predict(X_train)\naccuracy = metrics.accuracy_score(y_train, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_train, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_train, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the decision tree on the **training set**\n"}
{"snippet": "y_pred = my_tree.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the decision tree on the **validation set**\n"}
{"snippet": "y_pred = my_tuned_tree.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluate the model on a stratified test set\n"}
{"snippet": "y_pred = my_model.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the model on the **validation set**\n"}
{"snippet": "y_pred = my_model.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the decision tree on the **validation set**\n"}
{"snippet": "tuned_forest_predictions = locally_best_forest.predict(X_test) \naccuracy_score(y_test,tuned_forest_predictions)\n", "intent": "Make predictions for the test data.\n"}
{"snippet": "accuracy = metrics.accuracy_score(Y_test, y_hat)\nprint(\"Accuracy:-->\" ,accuracy)\n", "intent": "Cross_val_score were calculated on fewer examples so might have small scores compare to simple holdout strategy evaluation experiment. \n"}
{"snippet": "y_hat = my_tuned_model.predict(X_test)\naccuracy = metrics.accuracy_score(Y_test, y_hat) \nprint(accuracy)\n", "intent": "<b> Note:</b> I am using test.csv dataset (10000 examples) as a hold out dataset to evaluate the tuned model.\n"}
{"snippet": "print(\"****** Test Data ********\")\ny_pred = my_tuned_model.predict(np.asfarray(X_test))\nprint(\"Accuracy : - \" , metrics.accuracy_score(y_test, y_pred))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\ndisplay(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n", "intent": "Evaluate the model on a test dataset\n"}
{"snippet": "scores = model.evaluate(x_test, y_test_wide, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])\nscores\n", "intent": "Score some test data\n"}
{"snippet": "x_unseen = 0.78\nmodel.predict(x_unseen)\n", "intent": "This model can now be use to make predictions for *y* given new values of *x*:\n"}
{"snippet": "bills = np.arange( 10, 70, 5 )\nfor bill in bills:\n    predict_tip = model.predict(bill)\n    print(\"Predicted tip for meal costing %.2f = %.2f\" % ( bill,  predict_tip ) )\n", "intent": "We can make predictions from this model:\n"}
{"snippet": "test_x = x[0:5]\nmodel.predict(test_x)\n", "intent": "Let's try to predict the first five values of the original data (note: normally we would use a separate test dataset in a real evaluation).\n"}
{"snippet": "budgets = np.arange( 0, 400, 50 )\nfor spend in budgets:\n    predict_sales = model.predict(spend)\n    print(\"Predicted sales for TV advertising spend of %.2f = %.2f\" % ( spend,  predict_sales ) )\n", "intent": "We can now use this to make predictions\n"}
{"snippet": "test_x = x[0:1]\nprint(test_x)\nprint(\"Predicted Sales = %.2f\" % model.predict(test_x))\nprint(\"Actual Sales = %.2f\" % df[\"Sales\"].iloc[0])\n", "intent": "Again we can make predictions for sales based on new values for the 3 input features:\n"}
{"snippet": "print(\"Mean squared error (train): %.3f\" % mean_squared_error(y_train, linreg.predict(X_train_scaled)))\nprint(\"Mean squared error (test): %.3f\" % mean_squared_error(y_holdout, linreg.predict(X_holdout_scaled)))\n", "intent": "**<font color='red'>Question 1:</font> What are mean squared errors of model predictions on train and holdout sets?**\n"}
{"snippet": "xinput = np.array([[3, 5, 4, 2], [3, 5, 2, 2]])\npred_class_numbers = knn.predict(xinput)\nprint( iris.target_names[pred_class_numbers] )\n", "intent": "We can also predict for multiple input examples at once:\n"}
{"snippet": "predicted = model.predict(dataset_test)\npredicted\n", "intent": "Make predictions for the test set, based on the model that we just built:\n"}
{"snippet": "print(classification_report(target_test, predicted, target_names=[\"negative\",\"positive\"]))\n", "intent": "We can quickly compute a summary of these statistics using scikit-learn's provided convenience function:\n"}
{"snippet": "acc_scores =  cross_val_score(model, data, target, cv=10, scoring=\"accuracy\")\nprint(acc_scores)\nprint(type(acc_scores))\n", "intent": "Similarly, for 10-fold cross validation we get an array with 10 accuracy scores, one for each fold:\n"}
{"snippet": "score1 = cross_val_score(model1, X, Y,cv=5, scoring=\"accuracy\")\nprint(\"For k=1 : \", score1)\nscore2 = cross_val_score(model2, X, Y,cv=5, scoring=\"accuracy\")\nprint(\"For k=3 : \", score2)\nscore3 = cross_val_score(model3, X, Y,cv=5, scoring=\"accuracy\")\nprint(\"For k=5 : \", score3)\nprint(\"Mean score for :::\")\nprint(\"k=1 :\", score1.mean())\nprint(\"k=3 :\", score2.mean())\nprint(\"k=5 :\", score3.mean())\n", "intent": "Step 4. Use 5-fold cross-validation to evaluate the accuracy achieved by a KNN classifier on the *Diabetes* dataset for: K=1, K=3, K=5\n"}
{"snippet": "y_pred = nb_classifier.predict(count_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluation on the holdout test dataset\n"}
{"snippet": "y_pred = my_tree.predict(count_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluation on the holdout test dataset\n"}
{"snippet": "y_pred = my_svm.predict(count_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluation on the holdout test dataset\n"}
{"snippet": "y_pred = my_nn.predict(count_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluation on the holdout test dataset\n"}
{"snippet": "print(\"Mean squared error (train): %.3f\" % mean_squared_error(y_train, lasso_cv.predict(X_train_scaled)))\nprint(\"Mean squared error (test): %.3f\" % mean_squared_error(y_holdout, lasso_cv.predict(X_holdout_scaled)))\n", "intent": "**<font color='red'>Question 4:</font> What are mean squared errors of tuned LASSO predictions on train and holdout sets?**\n"}
{"snippet": "class_rep_knn = classification_report(test_labels,pred_labels_knn)\nprint(\"Decision Tree: \\n\", class_rep_tree)\nprint(\"Logistic Regression: \\n\", class_rep_log)\nprint(\"Nearest Neighbors: \\n\", class_rep_knn)\n", "intent": "Repeat the code above to compare nearest neighbors with the other two models.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nprint \"RMSE:\", mean_squared_error(ys, predictions) ** .5\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"RMSE:\", mean_squared_error(ys, predictions) ** .5\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "acc = accuracy_score(y_test, y_pred)\nprint(acc)\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "resnet50_prediction = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_prediction) == np.argmax(test_targets, axis=1))/len(resnet50_prediction)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint('The training accuracy is', train_accuracy)\nprint('The testing accuracy is', test_accuracy)\n", "intent": "Now, let's see how our model does, let's calculate the accuracy over both the training and the testing set.\n"}
{"snippet": "Z = logreg.predict(X_testing)\n", "intent": "<hr />\n<b>\n With the training data fit, we can now run prediction on the test data using the '*predict()*' function\n</b>\n"}
{"snippet": "y_predict = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_predict)\n", "intent": "<b>Then we score the predicted output from model on our test data against our ground truth test data.</b>\n"}
{"snippet": "y_predict = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_predict)\n", "intent": "<b>Then we score the predicted output from the model on our test data against our ground truth test data.</b>\n"}
{"snippet": "roc_auc_score(y_valid, ctb_valid_pred)\n", "intent": "**We got almost 0.75 ROC AUC on a hold-out set.**\n"}
{"snippet": "ex_2d_x = np.array([[400, 400]], dtype=np.float32)\npredict = k_means_estimator.predict(input_fn=lambda: input_fn_2d(ex_2d_x), as_iterable=False)\n", "intent": "*Note that the predict() function expects the input exactly like how we specified the feature vector*\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=1)\nprint\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Evaluation based on the evaluation metrics passed at compile time can be run on your test data with the evaluate method.\n"}
{"snippet": "mdir = tempfile.mkdtemp()\nenv = gym.make('CartPole-v0')\nenv = wrappers.Monitor(env, mdir, force=True)\nfor episode in range(100):\n    state = env.reset()\n    done = False\n    while not done:\n        action = np.argmax(model.predict(state.reshape(1, 4))[0])\n        nstate, reward, done, info = env.step(action)\n        state = nstate\n", "intent": "Cool, not bad at all!\nLet's test this fully trained agent and see how it does.\n"}
{"snippet": "mdir = tempfile.mkdtemp()\nenv = gym.make('CartPole-v0')\nenv = wrappers.Monitor(env, mdir, force=True)\nfor episode in range(100):\n    state = env.reset()\n    done = False\n    while not done:\n        action = np.argmax(model.predict(state.reshape(1, 4))[0])\n        nstate, reward, done, info = env.step(action)\n        state = nstate\n", "intent": "How about running your fully-trained greedy agent?\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nsilhouette_score(iris.data, cls[1])\n", "intent": "Now calculate the Silhouette Coefficient for the total cluster.\n"}
{"snippet": "labelPred = valObs.map(lambda lp: (lp.label, linRegSGD.predict(lp.features)))\nvalLRSGDMAE = calcMAE(labelPred)\nprint(\"Validation MAE:\\n\\tBenchmark: = {0:.3f}\\n\\tLRSGD = {1:.3f}\"\n      .format(valBenchMAE, valLRSGDMAE))\nprint(\"\\nThe difference between Benchmark and SGD = {0:.3f}\".format(valBenchMAE-valLRSGDMAE))\n", "intent": "We will check the new model on the validation set:\n"}
{"snippet": "predLabel = testObsNum.map(lambda lp:(modelNum.predict(lp.features), lp.label))\nmetrics = BinaryClassificationMetrics(predLabel)\nAUROC = metrics.areaUnderROC\nprint(AUROC)\n", "intent": "Let us try out the model on the test set, and see how they fare:\n"}
{"snippet": "y_train_pred = BikeOLSModel.predict(x_train)\ny_test_pred = BikeOLSModel.predict(x_test)\nr2_test = r2_score(y_test, y_test_pred)\nr2_train = r2_score(y_train, y_train_pred)\nBikeOLS_r2scores = {'test': r2_test, 'training': r2_train}\nprint('Training and test R2 scores are: ', BikeOLS_r2scores)\n", "intent": "<HR>\nTraining and test R2 scores are:  {'test': 0.40540416900870035, 'training': 0.4065387827969087}\n<HR>\n"}
{"snippet": "split = 0.67\nX_train, X_test, y_train, y_test = load_dataset(split)\nprint('Training set: {0} samples'.format(X_train.shape[0]))\nprint('Test set: {0} samples'.format(X_test.shape[0]))\nk = 3\ny_pred = predict(X_train, y_train, X_test, k)\naccuracy = compute_accuracy(y_pred, y_test)\nprint('Accuracy = {0}'.format(accuracy))\n", "intent": "Should output an accuracy of 0.9473684210526315.\n"}
{"snippet": "np.sqrt(mean_squared_error(np.ones(len(temp_valid['total_bedrooms']))*temp_train['total_bedrooms'].mean(),\n                           temp_valid['total_bedrooms']))\n", "intent": "RMSE on a validation set is 64.5. Let's compare this with the best constant prediction - what if we fill NaNs with mean value:\n"}
{"snippet": "mse = cross_val_score(linreg, X, y, scoring='mean_squared_error', cv=10)\nrmse = np.mean(np.sqrt(-mse))\nprint (rmse)\n", "intent": "Use 10-fold cross-validation to calculate the RMSE for the linear regression model.\n"}
{"snippet": "all_pred = spam_detect_model.predict(message_tfidf) \n", "intent": "Prediction on all messages\n"}
{"snippet": "predictions = pipeline.predict(msg_test)\n", "intent": "Now we can directly pass data into the pipeline and the pre processing will be done along side the training of the model.\n"}
{"snippet": "test = data[50:60]\nprint(neigh.predict(test))\n", "intent": "Prepare Test Data and Predict\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50.predict(np.expand_dims(feature,axis=0))) for feature in test_Resnet50]\naccuracy = 100 * np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets,axis=1))/len(Resnet50_predictions)\nprint ('Testing accuracy: {:3f}%'.format(accuracy))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "def cv_score(X, y, n_neighbors=4, weights='distance', n_samples=X.shape[0]):\n", "intent": "* Use cross validation to evaluate the performance of model\n* Use MRAE as performance metric\n"}
{"snippet": "n_neighbors_list = np.arange(1, 7)\nMRAEs_list = []\nfor n_neighbors in n_neighbors_list:\n    print('when n_neighbors:', n_neighbors)\n    MRAEs = cv_score(X, y, n_neighbors)\n    print('MRAE is between %.3f +/- %.3f \\n' % (MRAEs.mean(), MRAEs.std()))\n    MRAEs_list.append(MRAEs.mean())\n", "intent": "* use cross validataion to select the optimal k, which is 2 (see the plot below)\n"}
{"snippet": "n_neighbors_list = np.arange(1, 7)\nMRAEs_list = []\nfor n_neighbors in n_neighbors_list:\n    print('when n_neighbors:', n_neighbors)\n    MRAEs = cv_score(X, y, n_neighbors, n_samples=10000)\n    print('MRAE is between %.3f +/- %.3f \\n' % (MRAEs.mean(), MRAEs.std()))\n    MRAEs_list.append(MRAEs.mean())\n", "intent": "* use cross validataion to select the optimal k, which is 2 (see the plot below)\n"}
{"snippet": "mse = sklearn.metrics.mean_squared_error(Y_test, Y_pred)\nprint(mse)\n", "intent": "**Mean Squared Error**\n"}
{"snippet": "y_pred = mlog.predict(X_test)\nprint('Weighted f1 score for test dataset',f1_score(y_test, y_pred, average='weighted'))\n", "intent": "Weighted f1 score for test dataset\n"}
{"snippet": "pred =knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "print(classification_report(y_test,pred),confusion_matrix(y_test,pred))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "y_pred = linreg.predict(X)\nglass['y_pred'] = y_pred\nglass.head()\n", "intent": "**Using the `LinearRegression` object we have fit, create a variable that are our predictions for `ri` for each row's `al` in the data set.**\n"}
{"snippet": "my_car.set_loss(loss_function)\n", "intent": "Next, we will set the loss function, using the loss function that you defined in part a.\n"}
{"snippet": "forecast = my_model.predict(future_dates)\nforecast.tail(3)\n", "intent": "Let's create our forecast and analize our prediction components.\n"}
{"snippet": "forecast = my_model.predict(future_dates)\nmy_model.plot_components(forecast);\n", "intent": "Let's create our forecast and analize our prediction components.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\npredictions = net.forward(X)\nloss = criterion.forward(predictions, Y)\nans = [np.argmax(p) for p in predictions]\ncheck = [np.argmax(p) for p in Y]\naccuracy_score(ans,check)\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "def gram_matrix(input_tensor):\n  channels = int(input_tensor.shape[-1])\n  a = tf.reshape(input_tensor, [-1, channels])\n  n = tf.shape(a)[0]\n  gram = tf.matmul(a, a, transpose_a=True)\n  return gram / tf.cast(n, tf.float32)\ndef get_style_loss(base_style, gram_target):\n", "intent": "Again, we implement our loss as a distance metric . \n"}
{"snippet": "model.load_weights(checkpoint_path)\nloss,acc = model.evaluate(test_images, test_labels)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n", "intent": "Then load the weights from the checkpoint, and re-evaluate:\n"}
{"snippet": "print('Accuracy score', accuracy_score(y_test, y_pred))\n", "intent": "Let's also compute accuracy\n"}
{"snippet": "import pdb\ndef buggy_loss(y, y_hat):\n  pdb.set_trace()\n  huber_loss(y, y_hat)\nprint(\"Type 'exit' to stop the debugger, or 's' to step into `huber_loss` and \"\n      \"'n' to step through it.\")\ntry:\n  buggy_loss(1.0, 2.0)\nexcept:\n  pass\n", "intent": "Check out exercise 2 towards the bottom of this notebook for a hands-on look at how eager simplifies model debugging.\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "- on untrained data (X_test)\n"}
{"snippet": "def muffin_or_cupcake(flour, sugar):\n    if(model.predict([[flour, sugar]]))==0:\n        print('You\\'re looking at a muffin recipe!')\n    else:\n        print('You\\'re looking at a cupcake recipe!')\n", "intent": "__Step 6:__ Predict New Case\n"}
{"snippet": "predict_D1=modelD1.predict(X_D1_val[:,None])\n", "intent": "Generamos las predicciones:\n"}
{"snippet": "from sklearn import metrics\nprint('MSE1:', metrics.mean_squared_error(Y_D1_val, predict_D1))\n", "intent": "Calculamos la Media del Error Cuadrado para el modelo 1 (variable predictora 'RM')\n"}
{"snippet": "predict_fullmodel=full_model.predict(X_val)\n", "intent": "* Report the mean square error on the test set\n"}
{"snippet": "print('MSE_MLP:', metrics.mean_squared_error(Y_val, prediction_MLP))\n", "intent": "* Report the mean square error on the test set.\n"}
{"snippet": "print('MSE_centr_model:', metrics.mean_squared_error(Y_val, predict_centr_model))\n", "intent": "Report the mean square error on the test set.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint(\"\")\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "y_pred_train = from_log(model_grid.predict(X_ohe_train))\ny_pred_valid = from_log(model_grid.predict(X_ohe_valid))\n", "intent": "Making a predictions on train and validation data:\n"}
{"snippet": "new_pred = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "test_point_indexes=100\nno_features=100\npredicted_clss=sig_clf.predict(test_x_onehotCoding[test_point_indexes])\nprint(\"Predicted Class :\", predicted_clss[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_indexes]),4))\nprint(\"Actual Class :\", test_y[test_point_indexes])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_features]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_indexes],test_df['Gene'].iloc[test_point_indexes],test_df['Variation'].iloc[test_point_indexes], no_features)\n", "intent": "<h4>4.1.1.4. Feature Importance, Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 300\nno_feature = 1000\npredicted_cls = sig_clf.predict(test_x_onehotCoding)\nprint(\"Predicted Class :\", predicted_cls[test_point_index])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.3.3.2. For Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 300\nno_feature = 1000\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actuall Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>4.5.3.2. Inorrectly Classified point</h4>\n"}
{"snippet": "test_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>5.1.1.3. Feature Importance, Correctly classified point</h4>\n"}
{"snippet": "test_point_index = 300\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h4>5.1.1.4. Feature Importance, Incorrectly classified point</h4>\n"}
{"snippet": "test_point_index = 300\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n", "intent": "<h5>5.3.1.3.2. Incorrectly Classified point</h5>\n"}
{"snippet": "MSE = np.mean((rgrs.predict(x_ts) - y_ts) ** 2)\nprint(\"Residual sum of squares:\", MSE )\nRSS = rgrs.score(x_ts, y_ts)\nprint('The R^2 score for the Model:', RSS)\n", "intent": "e) Evalute the R^2 on **testing** data. Is this good? Bad? Why?\n"}
{"snippet": "y_pred = model.predict_proba(X_test)\ny_pred[:5]\n", "intent": "After having trained a logistic regression model we predict the clicks for the test set\n"}
{"snippet": "y_pred_train_2 = from_log(model_grid.predict(X_ohe_train_2))\ny_pred_valid_2 = from_log(model_grid.predict(X_ohe_valid_2))\n", "intent": "Again make predictions on train and valid datasets and plot real and predicted prices:\n"}
{"snippet": "def mean_squared_error(a, b):\n    return tf.reduce_mean(tf.square(a - b))\n", "intent": "This function creates a TensorFlow operation for calculating the Mean Squared Error between the two input tensors.\n"}
{"snippet": "y_pred = one_class_clf.predict(X[~y.index.isin(idxs)])\n", "intent": "Since the *RandomUnderSampler* returns the indexes, we can use them to get the rest of the samples in order to validate the classifier.\n"}
{"snippet": "y_pred = nb_clf.predict(X[~y.index.isin(idxs)])\ny_true = y[~y.index.isin(idxs)]\n", "intent": "Using the same approach, I'll validate it with the rest of the samples\n"}
{"snippet": "confidence_intervals = np.arange(.975, .9999, .0001)\nbest = (0, 0, 0, 0)\nfor interval in confidence_intervals:\n    t = chi2.ppf(interval, 5) ** .5\n    y_pred = [1 if y_ > t else 0 for y_ in y_score]\n    auc = average_precision_score(y_true=y_true, y_score=y_pred)\n    if auc > best[1]:\n        precision, recall, _ = precision_recall_curve(y_pred, y_true)\n        best = (interval, auc, precision, recall)\nprint best\n", "intent": "Here with confidence interval 97.5%, the AUC was .05 <br />\nThis isn't a poor classifier. So, let's try to tune the confidence interval.\n"}
{"snippet": "t = chi2.ppf(0.975, 5) ** .5\ny_pred = [1 if y_ > t else 0 for y_ in y_score]\nauc = average_precision_score(y_true=y_true, y_score=y_pred)\nprint auc\n", "intent": "Now, let's predict using the confidence interval\n"}
{"snippet": "confidence_intervals = np.arange(.999000, .999999, .000001)\nbest = (0, 0, 0, 0)\nfor interval in confidence_intervals:\n    t = chi2.ppf(interval, 5) ** .5\n    y_pred = [1 if y_ > t else 0 for y_ in y_score]\n    auc = average_precision_score(y_true=y_true, y_score=y_pred)\n    if auc > best[1]:\n        precision, recall, _ = precision_recall_curve(y_pred, y_true)\n        best = (interval, auc, precision, recall)\nprint best\n", "intent": "Here, we had a poor auc of .051 <br />\nLet's try to tune the confidence interval\n"}
{"snippet": "X = df_final[['student_Yes', 'balance', 'income']]\ny = df_final[['default_Yes']]\ny = np.ravel(y)\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(logregl2, X, y, cv=5)\nscores\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "mse = metrics.mean_squared_error(y_test, predictions)\nd_mse = metrics.mean_squared_error(y_test, d_predictions)\nprint \"MSE / LR = {0:.4}, Dummy = {1:0.4}\".format(mse, d_mse)\n", "intent": "$$ MAE(y, \\hat y) = \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} ( y_i - \\hat y_i )^2 $$\n"}
{"snippet": "mae = metrics.mean_absolute_error(y_test, predictions)\nd_mae = metrics.mean_absolute_error(y_test, d_predictions)\nprint \"MAE / LR = {0:.4}, Dummy = {1:0.4}\".format(mae, d_mae)\n", "intent": "$$ MAE(y, \\hat y) = \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} | y_i - \\hat y_i | $$\n"}
{"snippet": "y_pred_train_rf_best = from_log(rf_best.predict(X_ohe_train_rf))\ny_pred_valid_rf_best = from_log(rf_best.predict(X_ohe_valid_rf))\n", "intent": "As earlier, let's make a predictions on validation set and plot real and predicted values on train and validation datasets:\n"}
{"snippet": "print 'MAE %.2f' % mean_absolute_error(y, y_hat)\nprint 'MSE %.2f' % mean_squared_error(y, y_hat)\n", "intent": "Residuals:\n* $\\frac{1}{n} \\sum_i |\\hat{y}^{(i)}-y^{(i)}|$\n* $\\frac{1}{n} \\sum_i (\\hat{y}^{(i)}-y^{(i)})^2$\n"}
{"snippet": "print('evaluating 800 epochs model')\nscore = model_800.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\nprint('Test error:', (1-score[2])*100, '%')\nprint('evaluating 56 epochs model')\nscore = model_56.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\nprint('Test error:', (1-score[2])*100, '%')\n", "intent": "First the evaluations for the network of the previous notebook, with 800 and 56 epochs of training.\n"}
{"snippet": "print('evaluating new model')\nscore = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\nprint('Test precision:', score[2]*100, '%')\nprint('Test error:', (1-score[2])*100, '%')\n", "intent": "As we can see, after only 100 iterations the error on the training set was already pretty low. \n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nprint(\"Train MSE:\", mean_squared_error(Y_train,model.predict(X_train)))\nprint(\"Test MSE:\", mean_squared_error(Y_test,model.predict(X_test)))\n", "intent": "measure mean squared error\n"}
{"snippet": "sns.regplot(bos.PRICE, lm.predict(X))\n", "intent": "The results appear to be normally distributed. There are some values below 0, which we should remove, as prices for houses cannot be negative.\n"}
{"snippet": "score, acc = model.evaluate(X_test_pad, y_test)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n", "intent": "It definitely trains faster...\n"}
{"snippet": "img = prepare_image('imgs/dog2.jpg')\nout = model.predict(img)\ny_pred = np.argmax(out)\nprint (y_pred)\nprint (synset.loc[y_pred].synset)\n", "intent": "<img src='imgs/dog2.jpg'/>\n"}
{"snippet": "img = prepare_image('imgs/sloth.jpg')\nout = model.predict(img)\ny_pred = np.argmax(out)\nprint (y_pred)\nprint (synset.loc[y_pred].synset)\n", "intent": "<img src='imgs/sloth.jpg'/>\n"}
{"snippet": "r2 =  metrics.r2_score(test_y, pred_y) \nprint( \"mean squared error = \", metrics.mean_squared_error(test_y, pred_y))\nprint( \"r2 score (coef determination) = \", metrics.r2_score(test_y, pred_y))\n", "intent": "<h2>R2 score</h2>\n<br />\n"}
{"snippet": "test_predictions = final_model.predict(X_test_scaled)\n", "intent": "Make predictions for test samples:\n"}
{"snippet": "def accuracy(tp, fp, fn, tn):\n    return (\"insert your code here\")\ndef recall():\ndef specificity ():\ndef precision():\ndef NegativePredictiveValue():\ndef f1_score(tp, fp, fn, tn):\n", "intent": "Refer to either class notes or textbook for these definitions. \n"}
{"snippet": "from sklearn import metrics\nprint (\"MSE:\", metrics.mean_squared_error(y_hat, y))\nprint (\"R^2:\", metrics.r2_score(y_hat, y))\nprint (\"var:\", y.var())\n", "intent": "* From Sklearn metrics import MSE and R-square methods and apply to the problem\n"}
{"snippet": "expected = y\npredicted = model.predict(X)\nprint(metrics.classification_report(expected, predicted))\nprint(metrics.confusion_matrix(expected, predicted))\n", "intent": "**Using either confusion matrix or classification report, see which one seems to be apt in NB ?**\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, r2_score\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(y_test, y_pred))\nprint('R^2 score: %.2f' % r2_score(y_test, y_pred))\n", "intent": "related game: http://guessthecorrelation.com/\n"}
{"snippet": "sgd_clf.predict([X[36000]])\n", "intent": "Since it gives a output of **True**, hence our binary classifier correctly identified the digit 5 from our dataset.\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_train_5, y_train_pred)\n", "intent": "2/F1 = 1/recall + 1/precision\n1. f1/2 < smaller of recall and precision\n"}
{"snippet": "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n", "intent": "**Evaluating the accuracy of SGDClassifier**\n"}
{"snippet": "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\nconf_mx = confusion_matrix(y_train, y_train_pred)\nconf_mx\n", "intent": "First we'll make the Confusion Matrix. For this we need predictions.\n"}
{"snippet": "knn_clf.predict([some_digit])\n", "intent": "**KNeighborsClassifier** supports **multilabel classification** but not all classifiers do.\n"}
{"snippet": "preds_proba = rf.predict_proba(X_val_analize)\ntrue_class_probs = preds_proba[:,1]\n(true_class_probs > 0.023).sum()\n", "intent": "Forest is highly biased onto False assumptions, let's pick elements with the probability being true not > 0.5, but >bias.\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(twenty_test.target, predicted,\n    target_names=twenty_test.target_names))\nmetrics.confusion_matrix(twenty_test.target, predicted)\n", "intent": "scikit-learn further provides utilities for more detailed performance analysis of the results:\n"}
{"snippet": "scaled_svm_clf1.predict([[5.5, 1.7]])\n", "intent": "Step 3: Test the model using a sample data\n"}
{"snippet": "Xception_predictions = [np.argmax(my_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(labels_test_exception, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "accuracy = np.mean(model.predict(X_test) == y_test.argmax(axis=1))\nprint(\"Accuracy Rate: {}%\".format(round(accuracy, 2) * 100))\n", "intent": "* Note how much were overfitting here (training and validation results are diverging).\n"}
{"snippet": "from sklearn.utils import shuffle\nclass PerceptronClassifier(object):\n    def __init__(self):\n        self.weights = None\n        self.bias = None\n        self.iter = 0\n    def fit(self, X, y, epochs=100):\n        return np.dot(x, self.weights) + self.bias\n    def predict(self, x):\n", "intent": "<a name=\"POS-2\">Solution for POS Exercise 2</a>\n"}
{"snippet": "mse = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint (mse)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "y_pred = LR.predict(x_test1)\nLR.score(x_test1,y_test1)\n", "intent": "**Use the Model to predict on x_test and evaluate the model using metric(s) of Choice.**\n"}
{"snippet": "predictions = dTree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "p = np.array([0.1, 0.1, 0.05, 0.6, 0.3], dtype=np.float32)\ny = np.array([0, 0, 0, 1, 0], dtype=np.float32)\ndef poisson_loss(y, p):\n    return (p - y * np.log(p)).mean()\npoisson_loss(y, p)\n", "intent": "* https://github.com/fchollet/keras/pull/479/commits/149d0e8d1871a7864fc2d582d6ce650512de371c\n"}
{"snippet": "coeffs = np.arange(0.1, 0.91, 0.1)\nmse = [mean_squared_error(y_test, ridge_pred * c + vw_pred * (1 - c)) for c in coeffs]\nm = np.argmin(mse)\ncoeffs[m], mse[m]\n", "intent": "Let's try to combine predictions\n"}
{"snippet": "print('R^2: {:.3f}'.format(r2_score(y_m, cvp_m)))\nprint('RMSE: {:.3f}'.format(mean_squared_error(y_m, cvp_m)**(0.5)))\nprint('MAE: {:.3f}'.format(mean_absolute_error(y_m, cvp_m)))\nprint('CC: {:.3f}'.format(np.corrcoef(cvp_m, y_m)[1,0]))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "y=tf.nn.embedding_lookup(np.eye(8238,dtype=np.float32),y_)\nloss = -tf.reduce_sum(y*tf.log(tf.clip_by_value(y_hat,1e-10,1.0)))+tf.nn.l2_loss(U)+tf.nn.l2_loss(V)\n", "intent": "We need to modify the loss function a bit for technical reasons.\n"}
{"snippet": "A_pre = best_model.predict(test[['B','C','D','G','H']])\n", "intent": "**c) Use the model to predict \"A\" using the testing dataset. Report the OS prediction accuracy. (10pts)**   \n"}
{"snippet": "test_pred = logitCV.predict_proba(test_X)[:,1]\ntest_pred = np.array(list(\"{:.6f}\".format(x) for x in test_pred))\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "def squared_error(ys_original,ys_line):\n    return sum((ys_line-ys_original)**2)\n", "intent": "<img src=\"rsq.png\" />\n"}
{"snippet": "deep_features_model.evaluate(image_testing_SFrame)\n", "intent": "As we can see, deep features provide us with significantly better accuracy (about 78%)\n"}
{"snippet": "predictions = []\nfor row in questions[1:]:\n    predition = clf.predict([row])\n    print(features_to_english(row), \"=>\", fast_or_slow(predition))\n    predictions.append(predition[0])\nprint(predictions)\n", "intent": "Please note that the set do not have to be same as in the training model\n"}
{"snippet": "from azureml import services\n@services.publish(workdspace_id, authorization_token)\n@services.types(crim=float, zn=float, indus=float, chas=float, nox=float, rm=float, age=float, \n                dis=float, rad=float, tax=float, ptratio=float, black=float, lstat=float)\n@services.returns(float)\ndef demoservice(crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat):\n    feature_vector = [crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat]\n    return lm.predict(feature_vector)\n", "intent": "Azure ML API Services are not yet functional\n"}
{"snippet": "loss, accuracy = model.evaluate(x_test, \n                                utils.onehot(targets_test, num_classes), \n                                batch_size=128)\nprint()\nprint('Test Loss:', loss)\nprint('Test Accuracy', accuracy)\n", "intent": "When the model is fitted Keras also makes it easy to evaluate  the performance. Like above all it takes is a single function call.\n"}
{"snippet": "adjusted_rand_score(y2, ag.labels_)\n", "intent": "Score ARI for K-MEANS and Agglomerative Clustering:\n"}
{"snippet": "probabilities = logistic_mod.predict_proba(X_test)\nprint(probabilities[:15,:])\n", "intent": "Next, execute the code in the cell below to compute and display the class probabilities for each case. \n"}
{"snippet": "probabilities = logistic_mod.predict_proba(x_test)\nprint(probabilities[:15,:])\n", "intent": "Next, execute the code in the cell below to compute and display the class probabilities for each case. \n"}
{"snippet": "nr.seed(498)\ncv_estimate = ms.cross_val_score(rf_clf, Features, Labels, \n                                 cv = outside) \nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate))\nprint('SDT of the metric       = %4.3f' % np.std(cv_estimate))\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate):\n    print('Fold %2d    %4.3f' % (i+1, x))\n", "intent": "Now, you will run the code in the cell below to perform the outer cross validation of the model. \n"}
{"snippet": "nr.seed(498)\ncv_estimate = ms.cross_val_score(ab_clf, Features, Labels, \n                                 cv = outside) \nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate))\nprint('SDT of the metric       = %4.3f' % np.std(cv_estimate))\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate):\n    print('Fold %2d    %4.3f' % (i+1, x))\n", "intent": "Now, you will run the code in the cell below to perform the outer cross validation of the model. \n"}
{"snippet": "probabilities = ab_mod.predict_proba(X_test)\nprint_metrics(y_test, probabilities, 0.5)    \n", "intent": "Now, execute the code in the cell below to score and evaluate the model.\nOnce you have executed the code, answer **Question 4** on the course page.\n"}
{"snippet": "nr.seed(498)\ncv_estimate = ms.cross_val_score(clf, Features, Labels, \n                                 cv = outside) \nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate))\nprint('SDT of the metric       = %4.3f' % np.std(cv_estimate))\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate):\n    print('Fold %2d    %4.3f' % (i+1, x))\n", "intent": "Now, you will run the code in the cell below to perform the outer cross validation of the model. \n"}
{"snippet": "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "Let's calculate some regression evaluation metrics.\n"}
{"snippet": "predictions = logmodel.predict(X_test)\n", "intent": "Let's see how well the model works by putting its predictions up against the real thing.\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))\n", "intent": "So how did our model perform?\n"}
{"snippet": "xgb = XGBClassifier(random_state=33, n_jobs=4)\nkf = KFold(random_state=33,n_splits=5,shuffle=True)\nprint ('Mean ROC-AUC CV score:', np.mean(cross_val_score(xgb, X_train, y_train, scoring='roc_auc',cv=kf)))\n", "intent": "Let's check quality of XGBoost via CV with 5 shuffling folds.\n"}
{"snippet": "sms_predictions = spam_detection_model.predict(sms_tfidf)\nprint(','.join(list(p for p in sms_predictions)[:500]))\n", "intent": "No to put the entire dataset through the model and see how well it performs.\n"}
{"snippet": "print(confusion_matrix(y_test, terse_predictions))\nprint(classification_report(y_test, terse_predictions))\n", "intent": "Finally, create a confusion matrix and classification report to see how we did.\n"}
{"snippet": "y_pred = gp.predict(x_train[:, None])\nrmse = __________________\nprint '(in-sample) rmse =', rmse\n", "intent": "We can measure the training error (RMSE):\n"}
{"snippet": "def test_train_error(theta0):\n    gp = GaussianProcess(theta0=theta0, nugget=.5)\n    __________________\n    __________________\n    __________________\n    return dict(__________________=__________________,)\ntest_train_error(100)\n", "intent": "Refactor this into a function:\n"}
{"snippet": "y_pred = gp.predict(x_train[:, None])\nrmse = np.sqrt(np.mean((y_pred - y_train)**2))\nprint '(in-sample) rmse =', rmse\n", "intent": "We can measure the training error (RMSE):\n"}
{"snippet": "x_test= np.random.uniform(0, 10, size=1000)\ny_test = np.random.normal(np.cos(x_test), dy)\ny_pred = gp.predict(x_test[:, None])\nrmse = np.sqrt(np.mean((y_pred - y_test)**2))\nprint '(out-of-sample) rmse =', rmse\n", "intent": "To measure the \"test\" error, we need different data.  Since this is a simulation, we can have as much as we want:\n"}
{"snippet": "import sklearn.cross_validation\nscores = sklearn.cross_validation.cross_val_score(clf, X, y)\nprint scores.mean(), scores.std()\n", "intent": "Does that look good?  Too good?  If it looks too good to be true, it probably is...\n"}
{"snippet": "n = len(y)\ncv = sklearn.cross_validation.StratifiedKFold(n, n_folds=10, shuffle=True)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "I think it is important to stratify the sampling, although our book does not make a big deal about that:\n"}
{"snippet": "cv = sklearn.cross_validation.StratifiedKFold(___________________________)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "Oops, what did I get wrong here?  Can you fix it?\n"}
{"snippet": "xgb = XGBClassifier(random_state=33, n_jobs=4)\nkf = KFold(random_state=33,n_splits=5,shuffle=True)\nprint ('Mean ROC-AUC CV score:', np.mean(cross_val_score(xgb, X_train, y_train, scoring='roc_auc',cv=kf)))\n", "intent": "Let's check quality again.\n"}
{"snippet": "cv = sklearn.cross_validation.StratifiedShuffleSplit(________________________________)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "Oops, what did I get wrong again?\n"}
{"snippet": "cv = sklearn.cross_validation.LeavePLabelOut(df.site, p=2)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "There is also a version that holds out $P$ labels, and lets you choose $P$:\n"}
{"snippet": "n = len(y)\ncv = sklearn.cross_validation.LeaveOneOut(n)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "What did I not mention that whole time?  Leave-one-out cross-validation.  That is because it takes forever.\n"}
{"snippet": "scores = []\nfor rep in range(10):\n    print rep,\n    cv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n    scores += [sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)]\nprint\n", "intent": "Let's return to the stratified, repeated, 10-fold cross-validation approach:\n"}
{"snippet": "scores = []\nfor rep in range(10):\n    print rep,\n    cv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True, random_state=________________________________)\n    scores += [sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)]\nprint\nprint np.mean(scores)\n", "intent": "To make this really, really fair, we should use the same random splits in each experiment...\n"}
{"snippet": "import sklearn.metrics\ny_pred = ['Other' for i in range(len(y))]\nsklearn.metrics.accuracy_score(y, y_pred, sample_weight=sample_weight)\n", "intent": "What is in `sample_weights`?\n"}
{"snippet": "clf.predict_proba(X[test[:10]])\n", "intent": "Did I promise a little more for this week in the syllabus?\nProbability prediction:\n"}
{"snippet": "cv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\nscores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)\nprint scores.mean(), scores.std()\n", "intent": "I think it is important to stratify the sampling, although our book does not make a big deal about that:\n"}
{"snippet": "scores = []\nfor rep in range(10):\n    print rep,\n    cv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n    scores += list(sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv))\n", "intent": "And I don't want you to ever rely on just 10 samples.  How can you do ten repetitions of 10-fold cross validation?\n"}
{"snippet": "xgb = XGBClassifier(random_state=33, n_jobs=4,max_depth=4, min_child_weight=1, n_estimators=200)\nkf = KFold(random_state=33,n_splits=5,shuffle=True)\nprint ('Mean ROC-AUC CV score:', np.mean(cross_val_score(xgb, X_train, y_train, scoring='roc_auc',cv=kf)))\n", "intent": "Now check new hyperparameters via our CV.\n"}
{"snippet": "scores = []\nfor rep in range(10):\n    print rep,\n    cv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True, random_state=123456+rep)\n    scores += [sklearn.cross_validation.cross_val_score(clf, X, y, cv=cv)]\nprint\nprint np.mean(scores)\n", "intent": "To make this really, really fair, we should use the same random splits in each experiment...\n"}
{"snippet": "np.round(clf.predict_proba(X[test[:10]]), 2)\n", "intent": "Use `np.round` to make this look nicer:\n"}
{"snippet": "c.predict_proba(X[test[0]])\n", "intent": "We can make probability predictions for most classifiers in `sklearn`:\n"}
{"snippet": "y_pr_pred = c.predict_proba(X[test])\n", "intent": "So yes...  How does it do on things it say are 80% chance real, overall?\n"}
{"snippet": "score = model.evaluate(X_test, Y_test, verbose=0)\n", "intent": "Evaluate the model on test data\n"}
{"snippet": "complete_ar_score = adjusted_rand_score(iris.target, complete_pred)\navg_ar_score = complete_ar_score = adjusted_rand_score(iris.target, avg_pred)\n", "intent": "**Exercise**:\n* Calculate the Adjusted Rand score of the clusters resulting from complete linkage and average linkage\n"}
{"snippet": "X_test_counts = vect.transform(X_test)\nX_test_tfidf = tfidf.transform(X_test)\ny_pred = clf.predict(X_test_tfidf)\n", "intent": "* Transform (no fitting) the test data with the same CountVectorizer and TfidfTransformer\n* Predict labels on these tfidf values.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nscoreTrain = model.evaluate(x_train,y_train, verbose=0)\nprint(\"Accuracy: \", score[1])\nprint(\"Accuracy of Train :\", scoreTrain[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "y_pred = model.predict(X_test.values.reshape(-1, 1))\ny_pred\n", "intent": "In `scikit-learn` this will use the `.predict()` function\n"}
{"snippet": "skf_5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\ncv_scores = cross_val_score(gcv.best_estimator_, X_crossvalid, y_crossvalid, cv=skf_5, scoring='neg_log_loss')\n", "intent": "Split with 3 folds was used to reduce parameters search time. Cross-validation can be run with large amount of splits.\n"}
{"snippet": "y_pred_mean = [y_train.mean()] * len(y_test)\nprint('RMSE (model):', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('RMSE: (dumb model):', np.sqrt(metrics.mean_squared_error(y_test, y_pred_mean)))\n", "intent": "One example is a model that always predicts the mean value\n"}
{"snippet": "y_pred_p = lr.predict_proba(X_test)\ny_pred_l1_p = lr_l1_penalty.predict_proba(X_test)\ny_pred_l2_p = lr_l2_penalty.predict_proba(X_test)\n", "intent": "We can get predicted probabilities as well as predictions\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\nauc = roc_auc_score(y_test_2, y_pred_2)\nprint(f\"AUC: {auc}\")\n", "intent": "from [https://www.ncss.com/software/ncss/roc-curves-ncss](https://www.ncss.com/software/ncss/roc-curves-ncss)\n"}
{"snippet": "y_pred = lr.predict(X_test)\nprint(roc_auc_score(y_test, y_pred))\nconfusion_matrix(y_test, y_pred)\n", "intent": "Looking better! Let's evaluate both the basic and new model on their respective test sets\n"}
{"snippet": "import numpy as np\nfrom sklearn.model_selection import cross_val_score\naccuracy_scores = cross_val_score(knn, X_train, y_train, scoring=\"f1\")\nnp.mean(accuracy_scores)\n", "intent": "You might have noticed the classes are skewed - use an appropriate metric for evaluation\n"}
{"snippet": "Xfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)\n", "intent": "Again, we have to coerce our data into a `[n_samples, n_features]` feature matrix:\n"}
{"snippet": "yfit = model.predict(Xtest)\n", "intent": "Predict the labels of the test set:\n"}
{"snippet": "def predict(i):\n    image = mnist.test.images[i]\n    actual_label = np.argmax(mnist.test.labels[i])\n    prediction = tf.argmax(y,1)\n    predicted_label = sess.run(prediction, feed_dict={images: [image]})\n    print (\"Predicted: %d, actual: %d\" % (predicted, actual))\n    pylab.imshow(mnist.test.images[i].reshape((28,28)), cmap=pylab.cm.gray_r) \n    return predicted_label, actual_label\npredict(5)\n", "intent": "A method to make predictions on a single image\n"}
{"snippet": "import xgboost as xgb\nxgb_clf = xgb.XGBClassifier()\nscores = cross_val_score(xgb_clf, X, y, cv=shuffle_validator, scoring='accuracy')\nprint('Accuracy: {:.4f} (+/- {:.2f})'.format(scores.mean(), scores.std()))\n", "intent": "**Just to finish with my new favorite**\n"}
{"snippet": "y_pred = random_forest.predict_proba(X_valid)\n", "intent": "Compare with baselines of 4 classes: \n"}
{"snippet": "np.mean((y_test - regr.predict(X_test))**2)**0.5\n", "intent": "Linear regression model RMSE result over test_daata\n"}
{"snippet": "def get_sentiment(review):\n    vectorized_review = update_input_layer(review).reshape(1,-1)\n    res = model.predict(vectorized_review)\n    if(res >= 0.5):\n        return \"POSITIVE\"\n    else:\n        return \"NEGATIVE\"\n", "intent": "Let's try the model to predict some reviews:\n"}
{"snippet": "def style_loss(feats, style_layers, style_targets, style_weights):\n    style_loss = 0.\n    for i, layer in enumerate(style_layers):\n        gram = gram_matrix(feats[layer])\n        style_loss += style_weights[i]*torch.sum((gram - style_targets[i])**2)\n    return style_loss\n", "intent": "Next, implement the style loss:\n"}
{"snippet": "class CombinedClassifier:\n    def __init__(self, clf1, clf1_features, clf2, clf2_features):\n        self.clf1 = clf1\n        self.clf2 = clf2\n        self.clf1_features = clf1_features\n        self.clf2_features = clf2_features\n    def predict(self, X): pass\n    def predict_proba(self, X): pass\nclf3 = CombinedClassifier(clf1, numerical, clf2, dummy_categorical)\n", "intent": "Build a third model (named `clf3`) that combine these two classifiers by completing the following class definition.\n"}
{"snippet": "from treeinterpreter import treeinterpreter as ti, utils\nselected_rows = [31, 85]\nselected_df = X_train.iloc[selected_rows,:].values\nprediction, bias, contributions = ti.predict(rf, selected_df)\n", "intent": "Using `treeintrerpreter` I obtain 3 objects: predictions, bias (average value of the dataset) and contributions.\n"}
{"snippet": "reduced_x_features\ngradient_boosted.predict_proba([[45, 0, 6, 1, 15000, 9.62, 6.91]])\n", "intent": "This one has perfect performance so we will use that to predict a new campaign:\n"}
{"snippet": "evaluate('hard', ['better', 'good']) \n", "intent": "Embeddings can also reveal details like grammatical properties:\n"}
{"snippet": "evaluate('china', ['moscow', 'russia']) \n", "intent": "They can also help us answer questions about the world:\n"}
{"snippet": "evaluate('nurse', ['man', 'doctor']) \n", "intent": "However, embeddings can also reveal problematic *biases* in language. \n"}
{"snippet": "from sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import cross_val_score\ntscv = TimeSeriesSplit(n_splits=5)\ncv = cross_val_score(lr, X_train_scaled, y_train, scoring = 'neg_mean_absolute_error', cv=tscv)\nmae = cv.mean()*(-1)\nmae\n", "intent": "For Cross Validation (CV) on Time Series data, we will use **TimeSeries Split** for CV.\nLet's see Mean Absolute Error for our simplest model\n"}
{"snippet": "cr = classification_report(y_test,predictions)\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "labels = kmeans.predict(data)\nprint(labels)\n", "intent": "From our *k*-means model we just built, we can see the labels to which each data point is assigned.\n"}
{"snippet": "yhat = neigh.predict(X_test)\nyhat[0:5]\n", "intent": "we can use the model to predict the test set:\n"}
{"snippet": "yhat = clf.predict(X_test)\nyhat [0:5]\n", "intent": "After being fitted, the model can then be used to predict new values:\n"}
{"snippet": "from sklearn.metrics import f1_score\nf1_score(y_test, yhat, average='weighted') \n", "intent": "You can also easily use the __f1_score__ fron sklearn library:\n"}
{"snippet": "from sklearn.metrics import jaccard_similarity_score\njaccard_similarity_score(y_test, yhat)\n", "intent": "Lets try jaccard index for accuracy:\n"}
{"snippet": "salary_pred = lin_reg.predict(test_set)\nsalary_pred\n", "intent": "Now we have a model and can call the `predict` function on it with inputs. \n"}
{"snippet": "r2_score(test_set_full[\"Salary\"], salary_pred)\n", "intent": "There's also a separate `r2_score` method that will calculate the $r^2$.\n"}
{"snippet": "next_25 = stepwise_fit.predict(n_periods=25)\nnext_25\n", "intent": "After your model is fit, you can forecast future values using the `predict` function, just like in sci-kit learn:\n"}
{"snippet": "def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n", "intent": "We will define a loss metric - namely - *Mean Absolute Percentage Error* which calculated Mean Absolute Error in percentage\n"}
{"snippet": "predicted_class = result.predict(train_cols)\naccuracy_score(data.admit, predicted_class)\n", "intent": "Answer: A one unit increase in GPA doubles the odds of being admitted.  \n"}
{"snippet": "reg.predict(np.array([2,3,40]).reshape(1,-1))\n", "intent": "Let's see how much power consumption the fitted tree predicts for a Wednesday at 2am if it is 40F:\n"}
{"snippet": "clf.predict([[2., 2.]])\n", "intent": "After being fitted, the model can then be used to predict new values:\n"}
{"snippet": "def square_error(s, s_est):\n    y = np.mean(np.power((s - s_est), 2))\n    return y\nMSE_tr = square_error(S_tr, s_hat)\nMSE_tst = square_error(S_tst, s_hat)\nprint('Average square error in the training set (baseline method): {0}'.format(MSE_tr))\nprint('Average square error in the test set (baseline method): {0}'.format(MSE_tst))    \n", "intent": "Compute the mean square error over training and test sets, for the baseline estimation  method.\n"}
{"snippet": "def square_error(s, s_est):\n    y = np.mean(np.power((s - s_est), 2))\n    return y\nprint('Average square error in the training set (baseline method): {0}'.format(MSE_tr))\nprint('Average square error in the test set (baseline method): {0}'.format(MSE_tst))    \n", "intent": "Compute the mean square error over training and test sets, for the baseline estimation  method.\n"}
{"snippet": "transfer_model_predictions = [np.argmax(transfer_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(transfer_model_predictions)==np.argmax(test_targets, axis=1))/len(transfer_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "roc_auc_score(df['admit'], probas[:,1])\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "dtree_pred = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(colleges['Cluster'], model.labels_))\nprint(classification_report(colleges['Cluster'], model.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "ridge_cv_score = cross_val_score(model_ridge, X_train, y_train, scoring='neg_mean_absolute_error',\n                                 cv=TimeSeriesSplit(n_splits=5))\nlgb_cv_score = cross_val_score(model_lgb, X_train, y_train, scoring='neg_mean_absolute_error',\n                                 cv=TimeSeriesSplit(n_splits=5))\n", "intent": "Let's recheck cross_val_score for models.\n"}
{"snippet": "dataset_tempe['LR prediction'] = lr_tempe.predict(X_tempe_train)\ndataset_tempe['RFR prediction'] = regr_rf.predict(X_tempe_train)\ndataset_tempe['SVR prediction'] = svr_rbf.predict(X_tempe_train)\nprint('Dataframe shape:', dataset_tempe.shape)\n", "intent": "After training the regressor, each regression model is fed with input of feature matrix.\n"}
{"snippet": "dataset_tempe_extend['LR future prediction'] = lr_tempe.predict(X_tempe_train_extend_inter)\n", "intent": "The extended feature matrix is fed to Linear Regression model to predict the global temperature in future.\n"}
{"snippet": "dataset_tempe['predicted'] = LR_tempe.predict(X_tempe_train)\ndataset_tempe.head()\n", "intent": "Here, the global temperature anomaly from original data is compared with temperature anomaly predicted by trained modeled.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(km.labels_, df['Cluster']))\nprint(classification_report(km.labels_, df['Cluster']))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predictions = svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "y_hat = line_reg.predict(train_data[['X']])\n", "intent": "We can also use the trained model to render predictions.\n"}
{"snippet": "preds_bag = model_bag.predict(testing_data)\npreds_rf = model_rf.predict(testing_data)\npreds_ab = model_ab.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "def precision(actual, preds):\n    tp = len(np.intersect1d(np.where(preds==1), np.where(actual==1))) \n    pred_pos = np.sum(preds_nb==1) \n    return tp/(pred_pos)\nprint(precision(y_test, preds_svm))\nprint(precision_score(y_test, preds_svm))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "y_pred_full = isof.predict(X_full[['x1','x2']])\n", "intent": "Very fast! How about the quality?\n"}
{"snippet": "def f1(preds, actual):\n    return 2/(1/precision(preds, actual) + 1/(recall(preds, actual)))\nprint(f1(y_test, preds_nb))\nprint(f1_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "preds_tree = tree_mod.predict(X_test)\npreds_rf = rf_mod.predict(X_test)\npreds_ada = ada_mod.predict(X_test)\npreds_reg =reg_mod.predict(X_test)\n", "intent": "> **Step 5:** Use each of your models to predict on the test data.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score\ny_train_predicted = model.predict_proba(X_train)[:,1]\ny_test_predicted = model.predict_proba(X_test)[:,1]\nprint('Train AUC %.4f' % roc_auc_score(y_train,y_train_predicted))\nprint('Test AUC %.4f' % roc_auc_score(y_test,y_test_predicted))\n", "intent": "We will predict probabilities of our TARGET=1,\n  P(TARGET=1|X) and use it for finding AUC_ROC metric.\n"}
{"snippet": "eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n    scores = model_selection.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n", "intent": "Apply the weighted voting schema we just implemented and test it.\n"}
{"snippet": "clf.predict(X.head())\n", "intent": "The classifier is now trained on data! We can use it to do stuff like predicting iris types based on features we supply.\n"}
{"snippet": "print((np.sum((bos.PRICE - lm.predict(X)) ** 2))/len(bos.PRICE))\nprint((np.mean((bos.PRICE - lm.predict(X)) ** 2)))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "print classification_report(y_test, KNNOP_pred)\n", "intent": "The recall on survival has gone up while dropping the precision slightly from our first model.\n"}
{"snippet": "pred_quant = res.predict()\npred_quant\n", "intent": "This generated a fit of $y = 3 x + 1$. Let's see what a linear regression yields.\n"}
{"snippet": "print(np.mean((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "nb_test = pg.NaiveBayes.from_samples(pg.ExponentialDistribution, df, y, verbose=False)\nnb_test.predict_proba(df);\n", "intent": "Last but not least - timings (as I said - very fast, even on sparse data)\n"}
{"snippet": "y_val_pred = clf.predict(X_validation)\naccuracy_score(Y_validation, y_val_pred)\n", "intent": "i) Classification accuracy\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(clf.predict(X_validation), Y_validation)\n", "intent": "iii) Confusion matrix\n"}
{"snippet": "print(classification_report(Y_validation, y_val_pred))\n", "intent": "iv) Classification Report\n"}
{"snippet": "confusion_matrix(clf.predict(X_validation), Y_validation)\n", "intent": "iii) Confusion matrix\n"}
{"snippet": "print(classification_report(Y_validation, y_val_pred))\n", "intent": "iv) Classification Report including Prediction, recall, F1 score\n"}
{"snippet": "y_val_pred = model.predict(X_validation)\naccuracy_score(Y_validation, y_val_pred)\n", "intent": "i) classification accuracy\n"}
{"snippet": "confusion_matrix(model.predict(X_validation), Y_validation)\n", "intent": "iii) confusion matrix\n"}
{"snippet": "y = clf.predict_proba(test_df)\n", "intent": "Make predictions on the test data\n"}
{"snippet": "x_test = np.array(['play ball'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "score = -cross_val_score(model, train_data, train_targets, cv=kf, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\nprint(score)\n", "intent": "Let's train a baseline model with the default parameters:\n"}
{"snippet": "vals = np.array([100,30,20000,7,3]).reshape(1,-1)\nmodel.predict(vals)\n", "intent": "Now we can use the model to predict avg grade for a given set of features\n"}
{"snippet": "vals = np.array([100,30,7,3]).reshape(1,-1)\nmodel.predict(vals)\n", "intent": "Now we can use the model to predict avg grade for a given set of features\n"}
{"snippet": "lm.predict(120.0)\n", "intent": "Once we have a model, we can make prediction:\n"}
{"snippet": "Employee.loc[6, 'Year'] = lm.predict(120.0)\nEmployee\n", "intent": "We may then impute the missing '`Year`' with our prediction.\n"}
{"snippet": "cv_score_dt=cross_val_score(decision_tree_model,X_test,y_test,cv=30)\n", "intent": "Cross Validating Model\n---\n"}
{"snippet": "cv_score_svm=cross_val_score(svm_model,X_test,y_test,cv=20)\n", "intent": "Cross Validating the Model\n---\n"}
{"snippet": "iris['predicted'] = model.predict(x)\niris\n", "intent": "It's helpful to attach our predictions back to our dataframe to see what they are in context to the original training set.\n"}
{"snippet": "note_predictions = classifier.predict(X_test)\nnote_predictions = list(note_predictions)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "dnn_predictions = dnn_classifier.predict(x=X_test, as_iterable=False)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "def gb_mse_cv(params, random_state=random_state, cv=kf, X=train_data, y=train_targets):\n    params = {'n_estimators': int(params['n_estimators']), \n              'max_depth': int(params['max_depth']), \n             'learning_rate': params['learning_rate']}\n    model = LGBMRegressor(random_state=random_state, **params)\n    score = -cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\n    return score\n", "intent": "The interface of hyperop.fmin differs from Grid or Randomized search. First of all we need to create a function to minimize.\n"}
{"snippet": "pred_rforest = rforest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(metrics.classification_report(y_test, pred_rforest))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "print(confusion_matrix(data.Cluster, kmeans.labels_))\nprint(classification_report(data.Cluster, kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "pred_init = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "print(confusion_matrix(y_test, pred_init))\nprint(classification_report(y_test, pred_init))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "pred_pipe = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "pred = svc_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "results = model.evaluate(X_test, y_test)\n", "intent": "- Keras model can be evaluated with evaluate() function\n- Evaluation results are contained in a list\n    - Doc (metrics): https://keras.io/metrics/\n"}
{"snippet": "ResNetFifty_predictions = [np.argmax(ResNetFifty_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNetFifty]\ntest_accuracy = 100*np.sum(np.array(ResNetFifty_predictions)==np.argmax(test_targets, axis=1))/len(ResNetFifty_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "valid_mae = mean_absolute_error(y_valid, ridge_pred)\nvalid_mae, np.expm1(valid_mae)\n", "intent": "As we can see, the prediction is far from perfect, and we get MAE $\\approx$ 1.3 that corresponds to $\\approx$ 2.7 error in \n"}
{"snippet": "predictions = my_model.predict(test_X)\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))\n", "intent": "We similarly evaluate a model and make predictions as we would do in scikit-learn.\n"}
{"snippet": "glass.loc[:, 'predictions'] = linreg.predict(X)\n", "intent": "- Add a column `y_pred` to `glass` that stores the model's fitted values for the refractice index.\n"}
{"snippet": "from sklearn.metrics import roc_auc_score, log_loss\nlr_preds = lr.predict_proba(xtest)[:,1]\nprint(\"auc:      {}\".format(roc_auc_score(ytest, lr_preds)))\nprint(\"log_loss: {}\".format(log_loss(ytest,lr_preds)))\n", "intent": "Let us compute the score on one fold\n"}
{"snippet": "gbm_preds = gbm.predict_proba(xtest)[:,1]\nprint(\"auc:      {}\".format(roc_auc_score(ytest, gbm_preds)))\nprint(\"log_loss: {}\".format(log_loss(ytest,gbm_preds)))\n", "intent": "Once fitted, we use the model to predict\n"}
{"snippet": "num_sampled = 10\nloss = tf.reduce_mean(\n  tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n                 num_sampled, vocabulary_size))\n", "intent": "We can then compute the loss\n"}
{"snippet": "cross_val_score(clf, X_train, y_train_0, cv=10, scoring='accuracy')\n", "intent": "What if you would like to perform 10-fold CV test? How would you do that\n"}
{"snippet": "precision_score(y_train_0, y_train_pred) \n", "intent": "Note the result here may vary from the video as the results from the confusion matrix are different each time you run it.\n"}
{"snippet": "recall_score(y_train_0, y_train_pred) \n", "intent": "Note the result here may vary from the video as the results from the confusion matrix are different each time you run it.\n"}
{"snippet": "f1_score(y_train_0, y_train_pred)\n", "intent": "Note the result here may vary from the video as the results from the confusion matrix are different each time you run it.\n"}
{"snippet": "predicted = regressor.predict(X['test'])\nmse = mean_squared_error(y['test'], predicted)\nprint (\"Error: %f\" % mse)\n", "intent": "Evaluate our hypothesis using test set. The mean squared error (MSE) is used for the evaluation metric.\n"}
{"snippet": "multireg.predict([0,0])\n", "intent": "** Now let's use predict function to predict mpg for light, heavy and medium cars **\n"}
{"snippet": "RSS = sum((y-linreg.predict(X))**2) \nTSS = sum((y-y.mean())**2) \nR_Squared = 1 - float(RSS)/TSS\nprint(R_Squared)\n", "intent": "This is the definition of R_Squared. Let's first calculate it = this is the hard way!\n"}
{"snippet": "from sklearn.metrics import roc_curve, auc,roc_auc_score\ny_hat_probability = lm.predict_proba(X).T[1]  \nprint(y_hat_probability)\nprint(roc_auc_score(y, y_hat_probability))\nvals = roc_curve(y, y_hat_probability)  \n", "intent": "Answer: Raise the threshold for predicting that emails are spam to be higher, which would make less email classified as spam\n"}
{"snippet": "accuracy_score = classifier.evaluate(X_test, y_test)[\"accuracy\"]\nprint('Accuracy: {0:f}'.format(accuracy_score))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "iris_predictions = list(classifier.predict_classes(X_test))\nprint(classification_report(y_test,  iris_predictions))\nprint('\\n')\nprint(confusion_matrix(y_test,  iris_predictions))\nprint('\\n')\n", "intent": "** Now create a classification report and a Confusion Matrix. Does anything stand out to you?**\n"}
{"snippet": "print(confusion_matrix(y_test, rfc_pred))\nprint(classification_report(y_test, rfc_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "print(confusion_matrix(df['Cluster'], kmeans.labels_))\nprint(\"\\n\")\nprint(classification_report(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "print(classification_report(y_test, predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "print(confusion_matrix(y_test, pred))\nprint('\\n')\nprint(classification_report(y_test, pred))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "print(accuracy_score(gs.predict(Xtestlr), ytestlr))\n", "intent": "Yes GridSearchCV gives the same value for C which is 0.1 as obtained earlier by explicitly looping through different values of C.\n"}
{"snippet": "print 'Silhouette Score:', metrics.silhouette_score(X_scaled, labels, metric='euclidean')\n", "intent": "And to compute the clusters' silhouette coefficient:\n"}
{"snippet": "print 'Silhouette Score:', metrics.silhouette_score(X_scaled, labels, metric='euclidean')\n", "intent": "Compute the Silhoutte Score to measure your analysis\n"}
{"snippet": "r2_test = r2_score(lin_model, test_feats, test_targets)\nprint \"r2_score:\", r2_test  \n", "intent": "Compute the R$^2$ score on the **test** data using the above function.\n"}
{"snippet": "y_pred_train = least_squares.predict(X_train)\ny_pred_test = least_squares.predict(X_test)\nprint(\"Train MSE: %.4f\" % mean_squared_error(y_pred_train, y_train))\nprint(\"Test MSE: %.4f\" % mean_squared_error(y_pred_test, y_test))\n", "intent": "*Exercise*: Compute the MSE for the train and test set. How does it compare to the situation before?\n"}
{"snippet": "from sklearn import metrics\ny_pred = clf_svm.predict(X_test)\nprint \"Test Precision: {}\".format(metrics.precision_score(y_test, y_pred))\nprint \"Test Recall: {}\".format(metrics.recall_score(y_test, y_pred))\nprint \"Test F-Score: {}\".format(metrics.f1_score(y_test, y_pred))\n", "intent": "The scikit-learn metrics package offers the basic evaluation routines.\n"}
{"snippet": "x = np.array(trainX)\ny = np.array(trainY)\np4 = np.poly1d(np.polyfit(x, y, 3))\nfrom sklearn.metrics import r2_score\nr2 = r2_score(np.array(trainY), p4(np.array(trainX)))\nprint(r2)\n", "intent": "Try measuring the error on the test data using different degree polynomial fits. What degree works best?\n"}
{"snippet": "labels = kmeans.fit_predict(X)\nlabels\n", "intent": "  * Train K-means op de dataset en haal de uiteindelijke labels op door middel van `fit_predict()`.\n"}
{"snippet": "kmeans.fit_predict(X)\n", "intent": "  * Train K-means op de dataset en haal de uiteindelijke labels op door middel van `fit_predict()`.\n"}
{"snippet": "errors_manhattan = 1 - f1_score(y_test, y_pred_manhattan, average=None)\nerrors_manhattan\n", "intent": "* Bereken de F1 score per target (digit) en per distance-measure\n* Zet de F1 scores om in error scores (1 - F1 score)\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint confusion_matrix(ytest, clf.predict(xtest))\n", "intent": "We might be less accurate bit we are certainly not overfit.\n"}
{"snippet": "WordFreqTe=np.zeros((len(datasetTest.data),len(vocabTrain)))\nWordFreqTe.shape\nfor idx1, tewords in enumerate(dsTestlower):\n    for idx2, vword in enumerate(vocabTrain):\n        WordFreqTe[idx1,idx2]=tewords.count(vword)\nypred =clf.predict(WordFreqTe)\n", "intent": "  * Classificeer de test data met behulp van de getrainde Naive Bayes.\n  * Bereken de gemiddelde F1-score (average='macro')\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "  * Classificeer de test data aan de hand van de best scorende SVM-parameter settings.\n"}
{"snippet": "errors = 1 - f1_score(y_test, y_pred, average=None)\nerrors\n", "intent": "* Bereken per digit de error tussen de voorspelde waarden en de targets door middel van (1 - F1 score)\n"}
{"snippet": "y_pred = clfMulPar.predict(X_test)\n", "intent": "  * Classificeer de test data aan de hand van de best scorende SVM-parameter settings.\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "  * Classificeer de test data aan de hand van de best scorende MLP-parameter settings.\n"}
{"snippet": "errors = 1 - f1_score(y_test, y_pred, average=None)\nerrors\n", "intent": "  * Bereken per digit de error tussen de voorspelde waarden en de targets door middel van (1 - F1 score)\n"}
{"snippet": "labels_sw = .fit_predict(X_sw)\n", "intent": "Elke window een label geven en kijken wat het resultaat is.\n"}
{"snippet": "y_predSVM = clfSVM.predict(X_pred_c)\n", "intent": "SVM geeft het beste resultaat. Dus we gebruiken die om de voorspelling te doen.\n"}
{"snippet": "labels_sw = km.fit_predict(X_sw)\n", "intent": "Elke window een label geven en kijken wat het resultaat is.\n"}
{"snippet": "metrics.fowlkes_mallows_score(y, labels)\n", "intent": "*Fowlkes-Mallows* can be used when the ground truth class assignments of the samples is known.\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nX = extended[['2015 Volume Sold (Liters) sum']]\ny = extended[['2016 Sales Q1']]\nransac = sklearn.linear_model.RANSACRegressor()\ncross_val_score(ransac, X, y, cv=10)\n", "intent": "We can do better than a test/train split,  by doing cross validation.\n"}
{"snippet": "print('precision:', precision_score(expected_labels, our_labels)) \nprint('recall:', recall_score(expected_labels, our_labels)) \nprint('accuracy:', accuracy_score(expected_labels, our_labels)) \n", "intent": "Precision/Recall/Accuracy stats\n"}
{"snippet": "from sklearn.model_selection import KFold\nkf = KFold(5, shuffle=True)\nnp.mean(-cross_val_score(dtr, X, y, cv= kf,\n                        scoring='neg_mean_squared_error'))\n", "intent": "- Train your model and calculates its MSE in five-fold cross-validation.\n"}
{"snippet": "predicted = model2.predict(X_test)\nprint predicted\n", "intent": "We now need to predict class labels for the test set. We will also generate the class probabilities, just to take a look.\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"sh-expo.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "log_preds, y = learn.TTA() \nprobs = np.mean(np.exp(log_preds),0)\naccuracy_np(probs, y), metrics.log_loss(y, probs)\n", "intent": "Training loss and validation loss are getting closer and smaller. We are on right track.\n"}
{"snippet": "def predict_result(model,x_test,img_size_target,batch_size): \n    x_test_reflect =  np.array([np.fliplr(x) for x in x_test])\n    preds_test1 = model.predict([x_test],batch_size=batch_size).reshape(-1, img_size_target, img_size_target)\n    preds_test2_refect = model.predict([x_test_reflect],batch_size=batch_size).reshape(-1, img_size_target, img_size_target)\n    preds_test2 = np.array([ np.fliplr(x) for x in preds_test2_refect] )\n    preds_avg = (preds_test1 +preds_test2)/2\n    return preds_avg\n", "intent": "Again plot some sample images including the predictions.\n"}
{"snippet": "preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\nmask_valid = np.array([downsample(x) for x in y_valid])\n", "intent": "Again plot some sample images including the predictions.\n"}
{"snippet": "test_acc, test_batches = 0, 0\nfor x_batch, y_batch in get_batches((X_test, y_test), batch_size):\n    net.training = False\n    predictions = net.forward(x_batch)\n    test_acc += accuracy_score(np.argmax(predictions, axis=1), y_batch)\n    test_batches += 1\nprint('\\t test accuracy:\\t %s' % (test_acc / test_batches * 100))\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "print(classification_report(y_test,predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "def predict(ratings, similarity, type='item'):\n    if type == 'item':\n        pass\n    return \n", "intent": "<img src=\"user_sim.gif\">\n<img src=\"item_sim.gif\">\n"}
{"snippet": "print metrics.accuracy_score(test_label,p)\n", "intent": "*Hint : Use metrics.accuracy_score*\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nprint \"MSE:\", mean_squared_error(ys, predictions)\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(8)\nprint \"RMSE:\", (mean_squared_error(ys, predictions))**.5\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "xs.append(4)\nys.append(50)\npredictions.append(8)\nprint \"RMSE:\", mean_squared_error(ys, predictions)\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "print(classification_report(Y_test, Y_pred))\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(ypred, ytest))\n", "intent": "These quantities are also related to the ($F_1$) score, which is defined as the harmonic mean of precision and recall.\n$F1 = 2\\frac{P \\times R}{P+R}$\n"}
{"snippet": "labels = model.predict(patches_hog)\nlabels.sum()\n", "intent": "Finally, we can take these HOG-featured patches and use our model to evaluate whether each patch contains an airplane:\n"}
{"snippet": "sand_pred = gnb.predict(data_test)\nS = sand_pred.reshape(M_tst, N_tst)\n", "intent": "And run the model we trained on the first image\n"}
{"snippet": "prediction = knn.predict(X_test)\nprediction\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ny_pred = predict_y(W, b, X_test, 3)\naccuracy_score(y_test, y_pred)\n", "intent": "Let's load a convenience function from scikit-learn to assess the overall accuracy\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(y_pred, y_test))\n", "intent": "    1. Compute the F1 (or other accuracy) scores for the TF ANN\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(y_true, y_pred)\n", "intent": "Number of correct classified inputs, by the total number of inputs.\n    (True postive + True negative) / Total nb\n"}
{"snippet": "print(np.mean((bos.PRICE - lm.predict(X))**2))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "score = accuracy_score(clf.predict(Xtestlr), ytestlr)\nscore\n", "intent": "The same best value for c was obtained using GridSearchCV\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "This classifier does not seem to be better. The test score for this classifier (71%) is lower than the original which was 77%\n"}
{"snippet": "model.load_weights('mnist.model.best.hdf5')\nscore = model.evaluate(x_test, y_test, verbose=0)\nscorex = 100*score[1]\nprint(\"Accuracy: %.2f%%\" % scorex )\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "InceptionV3Preditction = [np.argmax(model_inc.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(InceptionV3Preditction)==np.argmax(test_targets, axis=1))/len(InceptionV3Preditction)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "prediction = model.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "predictions = list(classifier.predict(X_test, as_iterable=True))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, prediction))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "pre = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "print(confusion_matrix(y_test,pre))\nprint('\\n')\nprint(classification_report(y_test,pre))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "prediction = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "pred = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print((TP + TN) / float(TP + TN + FP + FN))\nprint(metrics.accuracy_score(y_test, pred))\n", "intent": "**Classification Accuracy:** Overall, how often is the classifier correct?\n"}
{"snippet": "print((FP + FN) / float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, pred))\n", "intent": "**Classification Error:** Overall, how often is the classifier incorrect?\n- Also known as \"Misclassification Rate\"\n"}
{"snippet": "predictions = svm.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "y_pred = knn.predict(X_test)\nprint(\"Test set predictions :\\n {}\".format(y_pred))\n", "intent": "<h3>Evaluating Model</h3>\n"}
{"snippet": "print(classification_report(y_test, predictions))\nprint(\"\\n\")\nprint(confusion_matrix(y_test, predictions))\n", "intent": "** Now create a classification report and a Confusion Matrix. Does anything stand out to you?**\n"}
{"snippet": "x_test = np.array(['she is my better half'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\nx_test = np.array(['i am her better half'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "y_hat = model.predict(x_test)\ncifar10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', \n                  'frog', 'horse', 'ship', 'truck']\n", "intent": "This may give you some insight into why the network is misclassifying certain objects.\n"}
{"snippet": "dog_breed_predictions = [\n    np.argmax(model.predict(np.expand_dims(tensor, axis=0))) \n    for tensor in test_tensors]\ntest_accuracy = (\n    100 * np.sum(\n        np.array(dog_breed_predictions) == \n        np.argmax(test_targets, axis=1)) /\n    len(dog_breed_predictions))\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%.\n"}
{"snippet": "VGG16_predictions = [\n    np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) \n    for feature in test_VGG16]\ntest_accuracy = (\n    100 * np.sum(\n        np.array(VGG16_predictions) == np.argmax(test_targets, axis=1)) /\n    len(VGG16_predictions))\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "xception_predictions = [\n    np.argmax(xception_model.predict(np.expand_dims(feature, axis=0))) \n    for feature in test_Xception]\ntest_accuracy = (\n    100 * np.sum(\n        np.array(xception_predictions) == \n        np.argmax(test_targets, axis=1)) /\n    len(xception_predictions))\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import mutual_info_score\nmax_mutual_info = 0\ncount = 0\ncolumn = \"\"\nfor col1 in marketing_dataframe.columns.values:\n    temp = mutual_info_score(marketing_dataframe[col1],marketing_dataframe[\"outcome\"])\n    if temp > max_mutual_info:\n        max_mutual_info = temp\n        column = col1\nprint \"The highest Mutual Information is\",max_mutual_info,\"for column\",column\n", "intent": "Which attribute has the highest Mutual Information with the 'outcome' attribute?\n"}
{"snippet": "x = [[0.2, .15, 0.68, 0.05, 0.328]]\ngrid_knn.predict(x)\n", "intent": "This simple technique gives us the best K value.\nWe can use the best model from grid_knn to make predictions.\n"}
{"snippet": "silhouette_score(X, labs2)\n", "intent": "Derive silhouette score of country data and labels\n"}
{"snippet": "point = np.array([[0, 0]])\nmodel.predict(point)\n", "intent": "Make prediction on point (0,0). Works same way as sklearn.\n"}
{"snippet": "preds = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "new_data = np.asarray([0.18,0.15]).reshape(1,-1)\npred3 = knn3.predict(new_data)\npred5 = knn5.predict(new_data)\nprint (\"The knn3 model thinks new_data belongs to class {}\".format(pred3[0]))\nprint (\"The knn5 model thinks new_data belongs to class {}\".format(pred5[0]))\n", "intent": "Apply model on a new point\n"}
{"snippet": "knn3.predict_proba(new_data)\n", "intent": "Look at class probabilities\n"}
{"snippet": "metrics.mean_absolute_error(y_true, y_pred)\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "metrics.mean_squared_error(y_true, y_pred)\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "pred_labels = lr.predict(X)\n", "intent": "Plot the probabilities and the predictions\n"}
{"snippet": "null_acc = np.repeat(0.522, repeats=(y_test.shape[0]))\nlog_loss(y_test, null_acc)\n", "intent": "Now let's add some context to the log loss value by using the null accuracy\n"}
{"snippet": "accuracy_score(y_test, labels_60)\n", "intent": "Does this give a better accuracy score?\n"}
{"snippet": "model.predict_proba([[0,0]])\n", "intent": "What if you want the probabilities of each class?\n"}
{"snippet": "preds_r = rtree.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "new_text = [\"I had a decent time at this restaurant. \\\nThe food was delicious but the service was very poor. \\\nI recommend the salad but do not eat the french fries.\"]\nnew_text_transform = vect.transform(new_text)\nprint nb.predict(new_text_transform)\nprint nb.predict_proba(new_text_transform)\n", "intent": "How do you assess this model? \n<br>\nLet's try it on some new text\n"}
{"snippet": "preds = y_hat * y_std + y_mean\ny_true = y_test * y_std + y_mean\ncorr, mae, rae, rmse, r2 = compute_error(y_true, preds)\nprint(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr, mae, rmse, r2))\n", "intent": "We can evaluate the (new) predictions:\n"}
{"snippet": "preds = y_hat * y_std + y_mean\ny_true = y_test * y_std + y_mean\ncorr, mae, rae, rmse, r2 = compute_error(y_true, preds)\nprint(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr, mae, rmse, r2))\n", "intent": "And lets check the error statistics...\n"}
{"snippet": "y_hat = alpha + np.dot(X_test, beta)\npreds = y_hat * y_std + y_mean\ny_true = y_test * y_std + y_mean\ncorr, mae, rae, rmse, r2 = compute_error(y_true, preds)\nprint(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr, mae, rmse, r2))\n", "intent": "Using the estimated values, we can make predictions for the test set:\n"}
{"snippet": "samples = fit.extract(permuted=True)  \ny_hat = np.mean(samples[\"alpha\"].T + np.dot(X_test, samples[\"beta\"].T), axis=1)\npreds = y_hat * y_std + y_mean\ny_true = y_test * y_std + y_mean\ncorr, mae, rae, rmse, r2 = compute_error(y_true, preds)\nprint(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr, mae, rmse, r2))\n", "intent": "Extract results and compute error statistics:\n"}
{"snippet": "def compute_error(trues, predicted):\n    corr = np.corrcoef(predicted, trues)[0,1]\n    mae = np.mean(np.abs(predicted - trues))\n    rae = np.sum(np.abs(predicted - trues)) / np.sum(np.abs(trues - np.mean(trues)))\n    rmse = np.sqrt(np.mean((predicted - trues)**2))\n    r2 = max(0, 1 - np.sum((trues-predicted)**2) / np.sum((trues - np.mean(trues))**2))\n    return corr, mae, rae, rmse, r2\n", "intent": "Compute error statistics of the model's imputations:\n"}
{"snippet": "train_preds = lr.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2, train_preds)\ntest_preds = lr.predict(X_test_level2)\nr2_test_stacking = r2_score(y_test, test_preds)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print (metric)\n    skf = StratifiedKFold(n_splits=n_splits, random_state=skf_seed, shuffle=True)\n    NNF = NearestNeighborsFeats(n_jobs=20, k_list=k_list, metric=metric)\n    preds = cross_val_predict(NNF, X, Y, cv=skf)   \n    np.save('data/knn_feats_%s_train.npy' % metric, preds)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "train_gini = 2*metrics.roc_auc_score(train_X['Cancer'], logistic_fit.predict()) - 1\nprint(\"The Gini Index for the model built on the Train Data is : \", train_gini)\ntest_gini = 2*metrics.roc_auc_score(test_X['Cancer'], logistic_fit.predict(test_X)) - 1\nprint(\"The Gini Index for the model built on the Test Data is : \", test_gini)\n", "intent": "    - Coefficient Stability - sign and p-values\n"}
{"snippet": "print(confusion_matrix(y_test, preds_r))\nprint(\"\\n\")\nprint(classification_report(y_test, preds_r))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "Y_pred = model.predict(X_val)\nY_pred_classes = np.argmax(Y_pred,axis = 1) \nY_true = np.argmax(Y_val,axis = 1) \nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \nplot_confusion_matrix(confusion_mtx, classes = range(10)) \n", "intent": "<h1><a name=\"cm\">Confusion matrix</a></h1>\n"}
{"snippet": "print(classification_report(predict, y_test))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df['Cluster '], kmeans.labels_))\nprint('\\n\\n')\nprint(classification_report(df['Cluster '], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"timg.jpeg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "x_test = np.array(['not happy'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "loss, acc = model.evaluate(X_dev, Y_dev)\nprint(\"Dev set accuracy = \", loss,acc)\n", "intent": "Finally, let's see how your model performs on the dev set.\n"}
{"snippet": "print((FP + FN) / float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred_logreg))\n", "intent": "AKA: \"Missclassification Rate\"\n"}
{"snippet": "print(TP / float(TP + FN))\nprint(metrics.recall_score(y_test, y_pred_logreg))\n", "intent": "How sensitive is the classifier to detecting positive instances?\nAKA \"True Positive Rate\" or \"Recall\"\n"}
{"snippet": "print((FP + FN) / float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred))\n", "intent": "AKA: \"Missclassification Rate\"\n"}
{"snippet": "predictions = lm.predict(X_test)\npredictions[0:10]\n", "intent": "Let's grab predictions off our test set and see how well it did!\n"}
{"snippet": "np.mean((bos.PRICE - lm.predict(X))**2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "plot_clusters(clust.predict(X), clust.cluster_centers_)\n", "intent": "How'd `scikit-learn` do?\n"}
{"snippet": "print \"Complete Match: \", (model.predict(X) == predict(model, X, n_jobs=4)).all()\nprint model.predict(X[:20])\nprint predict(model, X[:20], n_jobs=4)\n", "intent": "Now let's make sure that we are getting the same results for both of them.\n"}
{"snippet": "(model.predict_proba(X[:100]) - predict_proba(model, X[:100], n_jobs=4)).sum()\n", "intent": "We can also connfirm that this is producing the correct result.\n"}
{"snippet": "metrics.accuracy_score(y_test, output)\n", "intent": "Getting the accuracy on the test set.\n"}
{"snippet": "train_output = forest.predict(X_train)\n", "intent": "Using model to precit training set values (example of what is not supposed to be done)\n"}
{"snippet": "from sklearn.metrics import explained_variance_score\nevs = explained_variance_score(y_test, y_predict)\nprint('EVS: {}'.format(round(evs,2)))\n", "intent": "**Explained Variance**  \nAnother **performance measure** we can use is the **Explained Variance Score (EVS)**.\n"}
{"snippet": "predictions = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. \n"}
{"snippet": "score = logistic.predict_proba(X)\nscore_lalonde = dataV2.copy()\nscore_lalonde[\"prop_0\"] = score[:,0]\nscore_lalonde[\"prop_1\"] = score[:,1]\nscore_lalonde.head(10)\n", "intent": "Once our data is trained, we now calculate the propensity scores:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(df['Cluster'], kmeans.labels_))\nprint(classification_report(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions_Resnet50 = [np.argmax(model_Resnet50.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy_Resnet50 = 100*np.sum(np.array(predictions_Resnet50)==np.argmax(test_targets, axis=1))/len(predictions_Resnet50)\nprint('Resnet50 test accuracy: %.4f%%' % test_accuracy_Resnet50)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print('predicted:', spam_detector.predict(tfidf4)[0])\nprint('expected:', messages.label[3])\n", "intent": "Let's try classifying our single random message:\n"}
{"snippet": "all_predictions = spam_detector.predict(messages_tfidf)\nprint(all_predictions)\n", "intent": "Hooray! You can try it with your own texts, too.\nA natural question is to ask, how many messages do we classify correctly overall?\n"}
{"snippet": "print(classification_report(messages['label'], all_predictions))\n", "intent": "From this confusion matrix, we can compute precision and recall, or their combination (harmonic mean) F1:\n"}
{"snippet": "predictions = nb_detector.predict(msg_test)\nprint (confusion_matrix(label_test, predictions))\nprint (classification_report(label_test, predictions))\n", "intent": "And overall scores on the test set, the one we haven't used at all during training:\n"}
{"snippet": "print (svm_detector.predict([\"Hi mom, how are you?\"])[0])\nprint (svm_detector.predict([\"WINNER! Credit for free!\"])[0])\n", "intent": "So apparently, linear kernel with `C=1` is the best parameter combination.\nSanity check again:\n"}
{"snippet": "resnet50_pred=[np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\nresnet50_tacc=100*np.sum(np.array(resnet50_pred)==np.argmax(test_targets, axis=1))/len(resnet50_pred)\nprint('Test accuracy on Resnet50 model: {}'.format(resnet50_tacc))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "np.mean((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "preds = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "lambda_ = \ntrain_loss = get_cross_entropy_loss(logits=predictions_student, labels=batch_train_labels)\ntrain_loss += lambda_ * distill_kl_loss\n", "intent": "**Define the joint training loss**\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nMSE = mean_squared_error(y_test_scaled, predictions)\nr2 = model.score(X_test_scaled, y_test_scaled)\nprint(f\"MSE: {MSE}, R2: {r2}\")\n", "intent": "Step 5) Quantify your model using the scaled data\n"}
{"snippet": "print 'Sum of squared Errors: %f' % np.sum((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "print 'Mean squared Error: %f' % np.mean((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "prune_acc_train = round(accuracy_score(optimal_tree.predict(X_train), y_train), 3)\nnoprune_acc_train = round(accuracy_score(dtree.predict(X_train), y_train), 3)\nprint('The pruned tree has an accuracy score of {score} on training data.'.format(score=prune_acc_train))\nprint('The un-pruned tree has an accuracy score of {score} on training data.'.format(score=noprune_acc_train))\n", "intent": "(i) Compare the training error rates between the pruned and un-pruned trees. Which is higher?\n"}
{"snippet": "prune_acc_test = round(accuracy_score(optimal_tree.predict(X_test), y_test), 3)\nnoprune_acc_test = round(accuracy_score(dtree.predict(X_test), y_test), 3)\nprint('The pruned tree has an accuracy score of {score} on test data.'.format(score=prune_acc_test))\nprint('The un-pruned tree has an accuracy score of {score} on test data.'.format(score=noprune_acc_test))\n", "intent": "(j) Compare the test error rates between the pruned and unpruned trees. Which is higher?\n"}
{"snippet": "rfc_pred = rForest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(rfc_pred, y_test))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "tree_pred = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, preds))\nprint(\"\\n\")\nprint(classification_report(y_test, preds))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "print(classification_report(y_test, rf_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(colleges['Cluster'], km.labels_))\nprint(classification_report(colleges['Cluster'], km.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "y_pred1 = model1.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "y_pred = logm.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred = log_reg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "y_pred = pipeline.predict(X_test)\nconfusion_matrix(y_test, y_pred)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "scores = cross_val_score(lr, X, y, cv=10, scoring='neg_mean_squared_error')\nscores\n", "intent": "Use 10-fold cross-validation to calculate the RMSE for the linear regression model.\n"}
{"snippet": "predictF = dforest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print('Decision Tree: \\n')\nprint(classification_report(y_test,predictions))\nprint('\\n')\nprint('Decision Forest: \\n')\nprint(classification_report(y_test,predictF))\nprint('\\n')\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "preds = pipe.predict(X_test)\nprint(confusion_matrix(y_test, preds))\nprint(\"\\n\")\nprint(classification_report(y_test, preds))\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "R50_predictions = [np.argmax(R50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_R50]\ntest_accuracy = 100*np.sum(np.array(R50_predictions)==np.argmax(test_targets, axis=1))/len(R50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print 'R2 with PCA with 60 components: ', r2_score(y_test,pipe.predict(X_test))\n", "intent": "Insight: with this rudimental, untuned linear regression on 60 principal components we obtained an R2 of 58%\n"}
{"snippet": "print 'R2 with PCA with 60 components: ', r2_score(y_test,pipe.predict(X_test))\nprint 'RMSE with PCA with 60 components: ', mean_squared_error(y_test,pipe.predict(X_test))\n", "intent": "Insight: with this rudimental, untuned linear regression on 60 principal components we obtained an R2 of 58%\n"}
{"snippet": "test_predictions = lin_reg.predict(X_test)\n", "intent": "Finally, let's see how either of the two models performs against our Test Set\n"}
{"snippet": "best_model = gs.best_estimator_\ntest_predictions = best_model.predict(X_test)\nprint 'Test R2: ',r2_score(y_test, test_predictions)\nprint 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions))\nprint 'Test MAE: ',mean_absolute_error(y_test, test_predictions)\n", "intent": "Now testing with Test Dataset:\n"}
{"snippet": "model, predictions, r2, mse, mae, rmse = nonlinear_reg(X_train_intns, y_train, 2)\ntest_predictions = model.predict(X_test)\nprint 'Test R2: ',r2_score(y_test, test_predictions)\nprint 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions))\nprint 'Test MAE: ',mean_absolute_error(y_test, test_predictions)\n", "intent": "Now doing nonlinear regressions with Interaction Features\n"}
{"snippet": "test_predictions_lin_reg = lin_reg.predict(X_test)\ntest_predictions_tree_reg = best_model_dtree.predict(X_test)\ntest_predictions_best_model_svr = best_model_svr.predict(X_test)\ntest_predictions_best_model_kneigh = best_model_kneigh.predict(X_test)\n", "intent": "Now we evaluate the predictions on the test set\n"}
{"snippet": "model.predict(x)\n", "intent": "Predicting on the training points should return very good results, and in fact, does.\n"}
{"snippet": "np.linalg.norm(model.predict(x) - y)**2 / np.linalg.norm(y)**2\n", "intent": "I personally like to divide this error by the squared norm of y to get the ratio of the total variance reduced.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\npreds = svc.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nyValidationPrediction = gnb_prediction\nconfusionMatrixValidation = confusion_matrix(yvalidation,yValidationPrediction)\nlogLossValidation = log_loss(yvalidation,yValidationPrediction)\nplot_confusion_matrix(confusionMatrixValidation)\nconfusionMatrixValidation\n", "intent": "By using the prediction of the Gaussian Naive Bayes model, compute and display the confusion matrix on the validation set.\n"}
{"snippet": "test_loss, test_acc = model.evaluate(test_images, test_labels)\n", "intent": "Evaluate the model on the `test_images` and `test_labels` tensors.\nDocumentation : https://keras.io/models/model/\n"}
{"snippet": "results = model.predict(X_test)\npd.crosstab(y_test, results)\n", "intent": "This tree is much simpler than the original tree based on all the data.\n"}
{"snippet": "from sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.cm as cm\nimport numpy as np\nrange_n_clusters = XXXXX\nfor n_clusters in range_n_clusters:\n    Z = XXXXX\n    cluster_labels=XXXXX\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters ={},\".format(n_clusters)+\" the average silhouette_score is :{}\".format(silhouette_avg))\n", "intent": "http://scikit-learn.org/stable/modules/clustering.html\nPlease fullfill the the XXXXX part.\n"}
{"snippet": "Resnet50_model_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_model_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "complete_ar_score = adjusted_rand_score(iris.target, complete_pred)\ncomplete_ar_score\navg_ar_score = adjusted_rand_score(iris.target, avg_pred)\navg_ar_score\n", "intent": "**Exercise**:\n* Calculate the Adjusted Rand score of the clusters resulting from complete linkage and average linkage\n"}
{"snippet": "print(\"Accuracy = %f\" %(skm.accuracy_score(test_truth,test_pred)))\n", "intent": "We now compute some commonly used measures of prediction \"goodness\".  \nFor more detail on these measures see\n[6],[7],[8],[9]\n"}
{"snippet": "print(\"Accuracy = %f\" %(skm.accuracy_score(test_truth,test_pred)))\nprint(\"Precision = %f\" %(skm.precision_score(test_truth,test_pred)))\nprint(\"Recall = %f\" %(skm.recall_score(test_truth,test_pred)))\nprint(\"F1 score = %f\" %(skm.f1_score(test_truth,test_pred)))\n", "intent": "As before, we now compute some commonly used measures of prediction \"goodness\". \n"}
{"snippet": "def predict(theta, X):\n    h_theta = h_of_theta(theta, X)\n    return np.round(h_theta)\np = predict(optimal_theta, X)\nprint '\\nTraining Accuracy: ', np.mean((p == y) * 100)\n", "intent": "Let's compute the accuracy on the training set:\n"}
{"snippet": "train_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\n", "intent": "With your model fit we can now make predictions on both our training and testing sets.\n"}
{"snippet": "input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={'images': mnist.test.images}, y=mnist.test.labels,\n    batch_size=batch_size, shuffle=False)\nmodel.evaluate(input_fn)\n", "intent": "Evaluate the Model. Define the input function for evaluating.\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(df['Real'], df['Predicted']))\nprint('MSE:', metrics.mean_squared_error(df['Real'], df['Predicted']))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(df['Real'], df['Predicted'])))\n", "intent": "**Validation and Evaluation**\n"}
{"snippet": "predictions = lm.predict(X_test)\nlen(X_train)\n", "intent": "** Use lm.predict() to predict off the X_test set of the data.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_pred = clf.predict(X_test)\ny_true = y_test\nconfusion_matrix(y_true, y_pred)\n", "intent": "How do they compare to the true values (of species?)\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_pred = clf.predict(X_test)\ny_true = y_test\nconfusion_matrix( y_pred, y_true, labels=[\"Normal\", \"Failed\"])\n", "intent": "How do they compare to the true values \n"}
{"snippet": "model.load_weights('pre_trained_spacy_model.h5')\nloss, acc = model.evaluate(x_test, y_test)\nprint(model.metrics_names)\nprint(loss, acc)\n", "intent": "And let's load and evaluate the first model:\n"}
{"snippet": "model.load_weights('task_specific_spacy_model.h5')\nloss, acc = model.evaluate(x_test, y_test)\nprint(model.metrics_names)\nprint(loss, acc)\n", "intent": "Lets try with the second model:\n"}
{"snippet": "def get_projected_rides(minute):\n    best_model = polynomial_regression_fit(times, counts, 2)\n    projections = polynomial_regression_predict(best_model.params, 2, times)\n    return projections[minute]\nmins = 1323\nrides = get_projected_rides(mins)\nprint \"The number of rides at\", mins, \"is\", rides\n", "intent": "The best fit by R-squared, AIC and BIC is a second degree polynomial expression.\n"}
{"snippet": "predictions = dtree.predict(X_test)\n", "intent": "**Erstelle die Vorhersagen (en. predictions) aus den Testdaten und werte dann Classification Report und Confusion Matrix aus.**\n"}
{"snippet": "X_new = [ [3, 5, 4, 2], \n          [5, 4, 3, 2] ]\nknn.predict(X_new)\n", "intent": "- Returns a NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "predictions = model.predict(X_test)\n", "intent": "Schauen wir uns nun die Vorhersagen des trainierten Modells an.\n"}
{"snippet": "model1.predict(data)\n", "intent": "y = 1 / (1 + np.exp(-(w.T).dot(data.T)))\ny\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.mean_absolute_error(y_test, y_pred))\n", "intent": "Mean absolute error is the average absolute value of the difference between the predicted and actual values:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print(metrics.mean_squared_error(y_test, y_pred))\n", "intent": "Mean Squared Error (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "from sklearn import metrics\nprint \"The accuracy is %.2f\" % metrics.accuracy_score([1,1], [1,1])\n", "intent": "You may have also noticed that I often do something like\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint \"The accuracy is %.2f\" % accuracy_score([1,1], [1,1])\n", "intent": "But I could also do something like this\n"}
{"snippet": "print 'accuracy_score:', accuracy_score(Y_test, Y_pred)\nprint 'precision:', precision_score(Y_test, Y_pred)\nprint 'recall:', recall_score(Y_test, Y_pred)\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "print cross_val_score(dt, X, y, cv=5).mean()\nprint cross_val_score(rf, X, y, cv=5).mean()\nprint cross_val_score(et, X, y, cv=5).mean()\n", "intent": "Let's compare the 3 models (re-init Decision Tree)\n"}
{"snippet": "metrics.accuracy_score(y, labels)\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "knn.predict([[3, 5, 4, 2]])\n", "intent": "In order to make a **prediction**, the new observation must have the **same features as the training observations**, both in number and meaning.\n"}
{"snippet": "print metrics.classification_report(y, labels)\n", "intent": "Calculate the Precision, Recall, and F - Score to see the test's accuracy\n"}
{"snippet": "X_prob = logreg.predict_proba(X_test)\nprint X_prob[:10]\nprint \" \"\nprint X_prob[:10].T\nprint \" \"\nprint X_prob[:10].T[1]\nprint \"\"\nprint X_prob[:10][1]\n", "intent": "* print_metrics(x, y, y_pred, y_test, X_test, model)\n* metrics_list(x, y, X_test, Y_test, model, model_name)\n"}
{"snippet": "y_hat = knn.predict(wine_df)\n(y_hat == y). mean()\n", "intent": "<a id='cv-knn5'></a>\n"}
{"snippet": "model.predict_proba(point)\n", "intent": "Probabilities of each class.\n"}
{"snippet": "cross_val_score(model, X, y, cv = 5,scoring =\"precision\").mean()\n", "intent": "Cross validate with precision and recall\n"}
{"snippet": "def get_error(X, y, actual, model):\n    y = np.transpose([y])\n    actual = np.transpose([actual])\n    model = model.predict(X)\n    model = np.transpose([model])\n    train_error = (model - y) ** 2\n    train_error = np.average(train_error)\n    test_error = (model - actual) ** 2\n    test_error = np.average(test_error)\n    return train_error, test_error\n", "intent": "Hint: Try looking at the predict method in the LinearRegression object\n"}
{"snippet": "def get_error(X, y, actual, model):\n", "intent": "Hint: Try looking at the predict method in the LinearRegression object\n"}
{"snippet": "actual = train_df[target]\npred = dt.predict(train_df[predictors])\nmse = np.mean((actual - pred) ** 2)\nmse\n", "intent": "mean((actual - pred) ** 2)\n"}
{"snippet": "predict = list(learn.predict(X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "score = model.evaluate(test_images, test_labels, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "Was it worth the wait?\n"}
{"snippet": "predict = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predict = naive_bayes.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predict_pipeline = pipeline.predict(X_test1)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "iris_svm_predict = svm.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "final_model = grid.best_estimator_ \nprediction = final_model.predict(X_test) \nprint('confusion matrix:\\n')\nprint(metrics.confusion_matrix(prediction, Y_test))\nprint(\"\")\nprint('classification report\\n')\nprint(metrics.classification_report(prediction, Y_test, digits = 4))\n", "intent": "<div class=\"span5 alert alert-info\">\n<h2><center> Predict with the best estimator\n"}
{"snippet": "predictions_lg = linear_regression.predict(car_test['cyl displ hp weight accel yr'.split()])\npredictions_lg[:10]\n", "intent": "8.2 Generate predictions with a model\n-------------------------------------\n"}
{"snippet": "cv_scores = model_selection.cross_val_score(linreg, dd_examples, dd_targets, \n                                             cv=kfold, \n                                             scoring='neg_mean_squared_error', \n                                             n_jobs=-1) \nprint(cv_scores) \n", "intent": "There is an easier way, with `cross_validation`, which will do this routine for you.\n"}
{"snippet": "y_pred = fit.predict(X)\nprint(metrics.r2_score(y, y_pred))\n", "intent": "The categorical variable (Romance) was significant. \n"}
{"snippet": "pred_label = ['PG-13' for d in range(len(X_test))]\nacc_pred = accuracy_score(y_test,pred_label)\nprint \"Always Predict PG-13 Accuracy Score: \", acc_pred\n", "intent": "Make a baseline stupid predictor that always predicts the label that is present the most in the data. Calculate its accuracy on a test set.\n"}
{"snippet": "score, acc = model.evaluate(x_test, y_test,\n                            batch_size=32,\n                            verbose=2)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n", "intent": "OK, let's evaluate our model's accuracy:\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100.0*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y, y_pred)\nimport numpy as np\nprint np.mean(y == y_pred)\nprint knn.score(X, y)\n", "intent": "Let's compute the accuracy. This is known as **training accuracy** because we are testing on the same data we used to train the model.\n"}
{"snippet": "score = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Training Accuracy:\", score[1])\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Testing Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "predicted_price=lr.coef_*x_train+lr.intercept_\nlr_predict=lr.predict(x_train)\n", "intent": "By looking into the attributes of your model, write down an equation for predicting the price of a car given the engine-power.\n"}
{"snippet": "print ('the coefficient of determination is:', r2_score(target_price,ml_predict))\nprint ('the Root Squared Error is:', sqrt(mean_squared_error(target_price,ml_predict)))\nprint ('the Mean Absolute Error is :', mean_absolute_error(target_price,ml_predict))\nprint ('the correlation coefficient is:', np.corrcoef(target_price,ml_predict))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "new_auto_numeric=auto_numeric.copy(deep=True)\nnew_auto_numeric['engine-size']=transformed_size\ntrain_transformed=new_auto_numeric.drop('price',axis=1)\ntransf_predict = cross_val_predict(lr, train_transformed, target_price, cv = kf)\nprint ('the coefficient of determination is:', r2_score(target_price,transf_predict))\nprint ('the Root Squared Error is:', sqrt(mean_squared_error(target_price,transf_predict)))\nprint ('the Mean Absolute Error is :', mean_absolute_error(target_price,transf_predict))\nprint ('the correlation coefficient is:', np.corrcoef(target_price,transf_predict))\n", "intent": "Now re-build a Linear Regression model on the transformed dataset and report the R^2, RMSE, MAE and CC metrics.\n"}
{"snippet": "result=confusion_matrix(y,clf.predict(X))\nprint(result)\n", "intent": "**d)** Compute the resulting confusion_matrix by using the builtin scikit-learn class and display the result. \n"}
{"snippet": "print(\"The Coefficient of Determination (R^2) is: {:.5f}\".format(r2_score(y_mlr,result_mlr)))\nprint(\"The Root Mean Squared Error (RMSE) is: {:.5f}\".format(np.sqrt(mean_squared_error(y_mlr,result_mlr))))\nprint(\"The Mean Absolute Error (MAE) is: {:.5f}\".format(mean_absolute_error(y_mlr,result_mlr)))\nprint(\"The Correlation Coefficient (CC) is: \\n{}\".format(np.corrcoef(result_mlr,y_mlr)))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "score = model.evaluate(test_images, test_labels, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "But, even with just 10 epochs, we've outperformed our Tensorflow version considerably!\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(inception_model.predict(np.expand_dims(features, axis=0))) for features in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "IcebergModel.evaluate(x=X_band,y=target)\n", "intent": "Evaluating the model on the traning data and it gets a 96.5 % accuarcy . \n"}
{"snippet": "from sklearn.metrics import classification_report\npred_label = IcebergModel.predict(x=X_band)\npred_label[pred_label>0.5]=1\npred_label[pred_label<=0.5]=0\nprint(classification_report(target,pred_label))\n", "intent": "Classification report on the training data .  \n"}
{"snippet": "score = model.evaluate(features, targets)\nprint(\"\\n Training Accuracy:\", score[1])\nscore = model.evaluate(features_test, targets_test)\nprint(\"\\n Testing Accuracy:\", score[1])\n", "intent": "**Note:** Worth coming back to this later once I better understand how keras and NN work to try to get better results.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df['Cluster'], kmeans.labels_))\nprint(classification_report(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = lr.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "metrics = model_res.evaluate(test_res, test_targets)\nprint('Test accuracy: {:.4f}'.format(metrics[1]))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics.cluster import adjusted_rand_score\nprint \"Comparing Y (truth) to BMM:   \", adjusted_rand_score(Y, Y_BMM)\nprint \"Comparing Y (truth) to KMeans:\", adjusted_rand_score(Y, Y_KMeans)\n", "intent": "> Report the rand index score using the class code as ground truth label for both algorithms and comment on your findings.\n"}
{"snippet": "from sklearn import metrics\nimport numpy as np\nnp.sqrt(metrics.mean_squared_error(y_test, predictions))\n", "intent": "Take a minute to discuss Root Mean Squared Error [RMSE](https://www.kaggle.com/wiki/RootMeanSquaredError)\n"}
{"snippet": "trn_features = model.predict(trn_data, batch_size=batch_size)\nval_features = model.predict(val_data, batch_size=batch_size)\n", "intent": "...and their 1,000 imagenet probabilties from VGG16--these will be the *features* for our linear model:\n"}
{"snippet": "predictions = lstm_basic_model.predict(X_test)\n", "intent": "**Prediction using basic LSTM model**\n"}
{"snippet": "def model_lstm_score(model, X_train, y_train, X_test, y_test):\n    trainScore = model.evaluate(X_train, y_train, verbose=0)\n    print('Train Score: %.8f MSE (%.8f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n    testScore = model.evaluate(X_test, y_test, verbose=0)\n    print('Test Score: %.8f MSE (%.8f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n    return trainScore[0], testScore[0]\n", "intent": "**Error/accuracy score of basic LSTM model**\n"}
{"snippet": "opt_predictions = optimized_lstm_model.predict(X_test)\n", "intent": "**Prediction using Optimized LSTM model**\n"}
{"snippet": "model_lstm_score(optimized_lstm_model,X_train, y_train, X_test, y_test)\n", "intent": "**Error/accuracy score of Optimized LSTM model**\n"}
{"snippet": "predictions = gru_basic_model.predict(X_test)\n", "intent": "**Prediction using Basic GRU model**\n"}
{"snippet": "def model_gru_score(model, X_train, y_train, X_test, y_test):\n    trainScore = model.evaluate(X_train, y_train, verbose=0)\n    print('Train Score: %.8f MSE (%.8f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n    testScore = model.evaluate(X_test, y_test, verbose=0)\n    print('Test Score: %.8f MSE (%.8f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n    return trainScore[0], testScore[0]\n", "intent": "**Error/accuracy score of Basic GRU model**\n"}
{"snippet": "opt_predictions = gru_opt_model.predict(X_test)\n", "intent": "**Prediction using Optimized GRU model**\n"}
{"snippet": "model_gru_score(gru_opt_model, X_train, y_train, X_test, y_test)\n", "intent": "**Error/accuracy score of Optimized GRU model**\n"}
{"snippet": "predictions = regression_model.predict(X_test)\n", "intent": "   **Use the model for data prediction **\n"}
{"snippet": "preds = lm.predict_classes(val_features, batch_size=batch_size)\nprobs = lm.predict_proba(val_features, batch_size=batch_size)[:,0]\nprobs[:8]\n", "intent": "Calculate predictions on validation set, so we can find correct and incorrect examples:\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y, y_pred)\n", "intent": "* Determiner un metrique tel que Accuracy\n"}
{"snippet": "nb.predict(unknown_dtm)\n", "intent": "Question: Why do we use this one? Why did we need to define a new one for the cross validation step above?\n"}
{"snippet": "list(zip(['dickinson', 'anthem'], nb.predict(unknown_dtm), nb.predict_proba(unknown_dtm)))\n", "intent": "Let's zip this together with the name of the poems to make sense of the output\n"}
{"snippet": "X_train, X_test, X_validation = np.split(X, [int(.5 * len(X)), int(.7 * len(X))])\nY_train, Y_test, Y_validation = np.split(Y, [int(.5 * len(Y)), int(.7 * len(Y))])\nplot_train_test_error(*calc_train_test_error(X_train, X_test, Y_train, Y_test))\n", "intent": "**Exercise 3 **  Vary the train ,test and validation ratios and observe how overfitting changes.\n"}
{"snippet": "print(lr.predict(X_train))\nprint(y_train)\n", "intent": "Apply the model and evaluate it on the training data:\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores_sklearn = cross_val_score() \n", "intent": "Compare the result of your implementation with the result of the ``cross_val_score`` method in scikit-learn.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 86%?\n"}
{"snippet": "my_model_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_xception]\ntest_accuracy = 100*np.sum(np.array(my_model_predictions)==np.argmax(test_targets, axis=1))/len(my_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"RMSE:\", mean_squared_error(ys, predictions)**0.5\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "preds = model.predict_classes(val_data, batch_size=batch_size)\nprobs = model.predict_proba(val_data, batch_size=batch_size)[:,0]\nprobs[:8]\n", "intent": "We can look at the earlier prediction examples visualizations by redefining *probs* and *preds* and re-using our earlier code.\n"}
{"snippet": "metrics.silhouette_score(Xs, labels, metric='euclidean')\n", "intent": "Compute the Silhoutte Score\n"}
{"snippet": "rf_pred = rf.predict_proba(df_test[rhs_columns])\nfpr, tpr, _ = roc_curve(df_test['inclusion'], rf_pred[:,1])\nroc_auc = auc(fpr,tpr)\nprint(\"Area under the ROC Curve, RF: %f\" % roc_auc)\n", "intent": "Print the AUC for Random Forest\n"}
{"snippet": "rf.predict_proba(X_test.head())\n", "intent": "We can see the probabilities of churn\n"}
{"snippet": "model.load_weights('imdb.model.best.hdf5')\nscore = model.evaluate(x_train, y_train)\nprint(\"\\n Training Accuracy:\", score[1])\nscore = model.evaluate(x_test, y_test)\nprint(\"\\n Testing Accuracy:\", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "RESNET50_predictions = [np.argmax(RESNET50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_RESNET50]\ntest_accuracy = 100*np.sum(np.array(RESNET50_predictions)==np.argmax(test_targets, axis=1))/len(RESNET50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(classification_report(y_test, rfc_predictions))\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(confusion_matrix(y_test, predictions))\nprint('\\n')\nprint(classification_report(y_test, predictions))\n", "intent": "** Create a confusion matrix and classification report.**\n"}
{"snippet": "pred = dnn.predict(X_test)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "cv_scores = cross_val_score(RF_clf,X_train, y_train, cv=10, scoring='accuracy')\nprint('The accuracy scores for the iterations are {}'.format(cv_scores))\nprint('The mean accuracy score is {}'.format(cv_scores.mean()))\n", "intent": "Compute k-fold cross validation on training dataset and see mean accuracy score\n"}
{"snippet": "model.predict([np.array([3]), np.array([6])])\n", "intent": "We can use the model to generate predictions by passing a pair of ints - a user id and a movie id. For instance, this predicts that user \n"}
{"snippet": "Y_sample = clf2.predict(X_sample)\nY_sample\n", "intent": "predict the labels for the kaggle sample set\n"}
{"snippet": "prediccion_logreg = modelo_logreg.predict(Xtest)\ncf_mat = confusion_matrix(ytest, prediccion_logreg)\nprint(\"Clientes correctamente clasificados: {}\".format(cf_mat[0][0] + cf_mat[1][1]))\nprint(\"Clientes buenos clasificados como malos: {}\".format(cf_mat[1][0]))\nprint(\"Clientes malos clasificados como buenos: {}\".format(cf_mat[0][1]))\n", "intent": "El modelo tuvo un rendimiento excelente. Vamos a visualizar los falsos positivos y negativos que obtuvo el modelo.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(ytest, clf.predict(xtest)))\n", "intent": "Training phase did a good prediction but testing prediction was low, still I might not consider as a better classifier.\n"}
{"snippet": "choser_Lr_test=chosen_Lr.predict(Test_X)\nprint classification_report(Test_y,choser_Lr_test)\n", "intent": "Checking metrics on the Test set. We see that we almost get the same metrics as the training set. Which means that we did not overfit.\n"}
{"snippet": "Xc_predictions = [np.argmax(Xc_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xc]\ntest_accuracy = 100*np.sum(np.array(Xc_predictions)==np.argmax(test_targets, axis=1))/len(Xc_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_hat = model.predict(x_test)\ny_hat_two = model_two.predict(x_test)\ncifar10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n", "intent": "This may give you some insight into why the network is misclassifying certain objects.\n"}
{"snippet": "Inception_predictions = [np.argmax(Inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Inception_predictions)==np.argmax(test_targets, axis=1))/len(Inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=1)\nprint(\"Accuracy: \", score)\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "train_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\nprint(train_predict.shape)\nprint(test_predict.shape)\n", "intent": "With your model fit we can now make predictions on both our training and testing sets.\n"}
{"snippet": "val_pseudo = bn_model.predict(conv_val_feat, batch_size=batch_size)\n", "intent": "To do this, we simply calculate the predictions of our model...\n"}
{"snippet": "predictions  = dforest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print (classification_report(y_test, predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions_rfc = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(data['Cluster'],kmeans.labels_))\nprint(classification_report(data['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "name, model = (\"glove_small_tfidf\", etree_glove_small_tfidf)\nprint(name, cross_val_score(model, X, y, cv=5).mean())\n", "intent": "benchmark all the things!\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(gtdb_test.target, predicted, target_names=gtdb_test.target_names))\n", "intent": "An accuracy of 85.8% is a much better choice. Here is a more detailed performance analysis of the results:\n"}
{"snippet": "t0 = time.time()\npredicted = gs_all_clf.predict(gtdb_test.data)\nt1 = time.time()\nprint(t1-t0, time.ctime(t1))\nnp.mean(predicted == gtdb_test.target)\n", "intent": "The comparitive accuracy is 89.7% - much better!\n"}
{"snippet": "scaler.transform([[4.7, 3.1]])\nprint(clf2.decision_function(scaler.transform([[4.7, 3.1]])))\nclf2.predict(scaler.transform([[4.7, 3.1]]))\n", "intent": "Let us evaluate on the previous instance to find the three-class prediction. Scikit-learn tries the three classifiers. \n"}
{"snippet": "y_pred=clf.predict(X_test4)\nprint (metrics.classification_report(y_test, y_pred, target_names=['setosa','versicolor','virginica']))\n", "intent": "Measure precision & recall in the testing set, using all attributes, and using only petal measures\n"}
{"snippet": "def plot_gen(G, n_ex=16):\n    plot_multi(G.predict(noise(n_ex)).reshape(n_ex, 28,28), cmap='gray')\n", "intent": "This is just a helper to plot a bunch of generated images.\n"}
{"snippet": "print(\"Score of model with test data defined above:\")\nprint(rf_model.score(X_test, y_test))\nprint()\npredicted = rf_model.predict(X_test)\nprint(\"Classification report:\")\nprint(metrics.classification_report(y_test, predicted)) \nprint()\n", "intent": "Let's look at the classification performance on the test data:\n"}
{"snippet": "label_idx = model_rf.predict([random_iris])\nlabel_idx\n", "intent": "Can we use our model to predict the type?\n"}
{"snippet": "age_z = lin_reg.predict(random_patient)\nage_z\n", "intent": "Now let's use our model to predict!\n"}
{"snippet": "y_pred = clf.predict(X_test)\n", "intent": "**This looks really promising!!! Checking the predicted values now**\n"}
{"snippet": "acc= accuracy_score(Y_test, y_pred)\nacc\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "out_scores, out_boxes, out_classes = predict(sess, \"family2.jpg\")\n", "intent": "Run the following cell on the \"test.jpg\" image to verify that your function is correct.\n"}
{"snippet": "arr = bst.predict(xgboost.DMatrix(x_test_new[['team1', 'team2', 'p1', 'p2']]))\narr\n", "intent": "Wow! Finally our model better than constant predictions! Congratulations! Don't hesitate, submit!\n"}
{"snippet": "acc = accuracy_score(Y_test, Y_pred)\nacc\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "model.load('checkpoints/ckpt--150')\nmodel.predict()\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "def data_D(sz, G):\n    real_img = X_train[np.random.randint(0,n,size=sz)]\n    X = np.concatenate((real_img, G.predict(noise(sz))))\n    return X, [0]*sz + [1]*sz\n", "intent": "Create a batch of some real and some generated data, with appropriate labels, for the discriminator.\n"}
{"snippet": "part_test = np.array(['57111'])\ncomplaint_test = np.array(['BRAKE PEDAL IS SOFT'])\nX_new_part_labelencoded = enc_label.transform(part_test)\nX_new_part_onehot = enc_onehot.transform(X_new_part_labelencoded.reshape(-1,1))\nX_new_complaint_counts = count_vect.transform(complaint_test)\nX_new_complaint_tfidf = tfidf_transformer.transform(X_new_complaint_counts)\nX_new_combined_tfidf = sparse.hstack((X_new_part_onehot, X_new_complaint_tfidf), format='csr')\npredicted = clf.predict(X_new_combined_tfidf)\nprint(predicted)\n", "intent": "This should return 0:\n"}
{"snippet": "cross_val_score(basic_pipe, train, y, cv=kf).mean()\n", "intent": "We can also cross-validate with it as well and get a similar score as we did with our scikit-learn column transformer pipeline from above.\n"}
{"snippet": "test = logreg.predict_proba(X_test)\nprint(test)\n", "intent": "- If y_pred_prob > 0.3, than y_pred_class = 1\n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Let's try out our model on the test dataset of dog images. This will be our \"test accuracy\". \n"}
{"snippet": "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "After we test the model again we see that we improved test accuracy up to 55%\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Let's try out our model on the test dataset of dog images.\n"}
{"snippet": "yhat= LR.predict(X_test)\nacc=np.mean(yhat==y_test)\nacc\n", "intent": "Now we can predict using our test set:\n"}
{"snippet": "yhat=lr.predict(X_test[['PROF']])\nyhat[0:5]\n", "intent": "We can make a prediction:\n"}
{"snippet": "yhat=lr.predict(X_test)\nyhat[0:5]\n", "intent": " We can make a prediction:\n"}
{"snippet": "try:\n    print(accuracy_score(predicted_labels, expected_labels))\n    print(confusion_matrix(y_pred=predicted_labels, y_true=expected_labels))\nexcept NameError:\n    print(\"You shouldn't know the answers, but this results in ~ 0.71 accuracy score\")\n", "intent": "<img src=\"../img/locally_best_tree.png\">\n"}
{"snippet": "roc_auc_score(df['admit'], lm.predict(X))\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "lin_test_pred = lin_reg.predict(X_test)\nsecond_train_error = rmse(lin_pred, y_train)\nsecond_test_error = rmse(lin_test_pred, y_test)\nprint(\"Training RMSE:\", second_train_error)\nprint(\"Test RMSE:\", second_test_error)\n", "intent": "**Question 1.5:** What is the rmse for both the prediction of X_train and X_test?\n"}
{"snippet": "def mean_squared_error(X, y, fit_func):\n    return ((fit_func(X).squeeze() - y.squeeze()) ** 2).mean()\n", "intent": "Minimize\n$$C(\\mathbf{w}) = \\sum_j (\\mathbf{x}_j^T \\mathbf{w} - y_j)^2$$\n"}
{"snippet": "(X.dot(reg.coef_.T) == reg.predict(X)).all()\n", "intent": "<center><img src=\"images/row_mult.png\" style=\"height: 200px;\"></img></center>\n"}
{"snippet": "from sklearn.metrics import accuracy_score\ntest_acc = accuracy_score(y_true=y_test, y_pred=clf.predict(X_test))\nprint('Test Accuracy: %.2f%%' % (100 * test_acc))\n", "intent": "Below, we can see that the test accuracy of the imbalanced set, and it went from .846 to .916. \n"}
{"snippet": "from sklearn.metrics import accuracy_score\ntest_acc = accuracy_score(y_true=y_test, y_pred=clf1.predict(X_test))\nprint('Test Accuracy: %.2f%%' % (100 * test_acc))\n", "intent": "Below, we can see the test accuracy after tuning Random Forest without SMOT. \n"}
{"snippet": "from sklearn.metrics import classification_report\nprint('Test Accuracy: %.2f%%' % (100 * test_acc))\nprint ()\nprint (classification_report(y_test, y_pred_test))\n", "intent": "We can see below that even though I drastically improved recall, precision is pretty low compared to the recall. \n"}
{"snippet": "from sklearn.metrics import classification_report\nprint('Test Accuracy: %.2f%%' % (100 * test_acc))\nprint ()\nprint (classification_report(y_test, y_pred_test))\n", "intent": "I got similar precision and recall as in the model for both genders.\n"}
{"snippet": "rmse = sqrt(mean_squared_error(y3, y3_pred))\nprint('Root Mean Squared Error:{:.3f}'.format(rmse))\nprint('Mean Absolute Error:{:.3f}'.format(mean_absolute_error(y3, y3_pred)))\nprint('Correlation Coefficient (CC):{:.3f}'.format(np.corrcoef(y3,y3_pred)[0,1]))      \n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "eval_result = model.evaluate(input_fn=input_test)\naverage_loss = eval_result[\"average_loss\"]\nprint(\"Average loss: %s\" % average_loss)\n", "intent": "Next we'll evaluate the trained model.\n"}
{"snippet": "train_sizes=[50,100,1000,5000]\nfor train_size in train_sizes:\n    clf, logistic=train_models(train_size, train_vector, train_labels)\n    predictions_svm=clf.predict(test_vector_small)\n    predictions_log=logistic.predict(test_vector_small)\n    print (\"support vector\")\n    print (\"train size: \",train_size,\"has accuracy:\",accuracy_score(test_labels_small, predictions_svm))\n    print\n    print (\"logistic\")\n    print (\"train size: \",train_size,\"has accuracy:\",accuracy_score(test_labels_small, predictions_log))\n", "intent": "Try training the model with different test sets\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df['Cluster'], kmeans.labels_))\nprint()\nprint(classification_report(df['Cluster'], kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predict = nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predict = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "from __future__ import division\ndef calculate_training_error(pred, true):\n", "intent": "This function will be used to calculate the training error for the Naive Bayes models\n"}
{"snippet": "dec_pred = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "for_pred = forest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "classification_report(y_test, for_pred)\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "pred = knn.predict(x_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"x\": eval_data},\n    y=eval_labels,\n    num_epochs=1,\n    shuffle=False)\neval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\nprint(eval_results)\n", "intent": "After training, evaluate the model.\n"}
{"snippet": "x_test = np.array(['I won the prize'])\nX_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\nprint(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n", "intent": "Now you can try it on your own example. Write your own sentence below. \n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(y, p4(x))\nprint(r2)\n", "intent": "Now we measure $R^2$ error:\n"}
{"snippet": "examples = ['Free Viagra call today!', \"I'm going to attend the Linux users group tomorrow.\"]\nexample_counts = count_vectorizer.transform(examples)\nprint(example_counts)\npredictions = classifier.predict(example_counts)\npredictions \n", "intent": "And there we have it: a trained spam classifier. We can try it out by constructing some examples and predicting on them.\n"}
{"snippet": "print (\"Predicted %d, Label: %d\" % (classifier.predict(test_data[0]), test_labels[0]))\ndisplay(0)\n", "intent": "We can make predictions on individual images using the predict method\n"}
{"snippet": "print(\"Accuracy score: {:.4f}\".format(accuracy_score(y_train, no_survivors)))\n", "intent": "Now use the appropriate functions from `sklearn` (already imported) to compare with your results. \n"}
{"snippet": "from sklearn.metrics import confusion_matrix\nX = news_A_clean.drop('class', axis=1)\ny_pred = gnb.predict(X)\ny_true = news_A_clean['class']\ncm = confusion_matrix(y_true, y_pred)\ndef plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n", "intent": "Plot the (normalised) confusion matrix for the training data. Label axes appropriately.\n"}
{"snippet": "print('Prediction for (4,-2.5):', lr_mod.predict([[4,-2.5]]))\n", "intent": "You will be provided with one or more specific points for which you will asked to generate predictions. \n"}
{"snippet": "confmatr = confusion_matrix(y, classifier.predict(X))\nconfmatr\n", "intent": "**d)** Compute the resulting confusion_matrix by using the builtin scikit-learn class and display the result. \n"}
{"snippet": "confmatrix = confusion_matrix(y_true=y, y_pred=classifier.predict(X))\nconfmatrix\n", "intent": "Plot the (normalised) confusion matrix for the training data. Label axes appropriately.\n"}
{"snippet": "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"x\": predict_data_batch[0]},\n    y=None,  \n    num_epochs=1,\n    shuffle=False)\npredict_results = mnist_classifier.predict(input_fn=predict_input_fn)\nfor i, p in enumerate(predict_results):\n    print(\"Correct label: %s\" % predict_data_batch[1][i])\n    print(\"Prediction: %s\" % p)\n", "intent": "Now make a few predictions.\n"}
{"snippet": "fullDim_adjustedRandScore = adjusted_rand_score(kmeans.labels_, y)\nfullDim_adjustedRandScore\n", "intent": "But since the adjusted rand score function is tolerant to these kind of difference we would get the same result if we had executed:\n"}
{"snippet": "confmatrix = confusion_matrix(y_true=y, y_pred=classifier.predict(X))\nconfmatrix\n", "intent": "<span style=\"color:red\">the second confusion matrix is ok (ok for your wrong model that is) but the whole answer is presented in a very confusing way\n"}
{"snippet": "inceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(inceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(inceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = model.evaluate(x_test, y_test)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "predictions = dtree.predict(test_x)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "predictions = rfc.predict(test_x)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(test_y, predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions = list(classifier.predict(test_x, as_iterable=True))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(data['Cluster'], km.labels_))\nprint(classification_report(data['Cluster'], km.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "results = m.evaluate(input_fn=generate_input_fn(test_file), steps=100)\nprint('evaluate done')\nprint('Accuracy: %s' % results['accuracy'])\n", "intent": "Let's see how the model did. We will evaluate all the test data.\n"}
{"snippet": "predictions = model.predict(test_x)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predictions = nb.predict(test_x)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predictions = pipeline.predict(test_x)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "trans_resnet_predictions = [np.argmax(trans_resnet.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(trans_resnet_predictions)==np.argmax(test_targets, axis=1))/len(trans_resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import metrics\nprint \"R-squared = %f\" % metrics.r2_score(dataset['Y'],a*dataset['BMI']+b)\n", "intent": "So the error $R^2 = 0.296$. We can also use the scikit-learn module `metrics` to compute $R^2$ with one command: \n"}
{"snippet": "theta1 = compomics_import.plot_regression_path(simple['x1'],simple['y'],0.1,30)\nprint \"theta1 = %f\" % theta1\nprint \"R-squared = %f\" % metrics.r2_score(simple['y'],theta1*simple['x1'])\n", "intent": "We can trace the updates made by the function `linear_regression()` and plot the iteration path on the cost function $J$:\n"}
{"snippet": "from sklearn import metrics\nprint metrics.r2_score(y,cv_predictions)\n", "intent": "We can now compute an estimate of the generalization performance of the model using these predictions:\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\ncv_predictions = cross_val_predict(model,X,y,cv=5)\nprint cv_predictions\n", "intent": "In scikit-learn we can use the `cross_val_predict()` function in the module `sklearn.cross_validation` to do the same thing:\n"}
{"snippet": "print \"R-squared = %f\" % metrics.r2_score(target[folds==1],model.predict(data_norm_poly_norm[folds==1]))\n", "intent": "what is the $R^2$ performance on the test set?\n"}
{"snippet": "test_file  = str(\"adult.test.csv\") \nresults = m.evaluate(input_fn=generate_input_fn(test_file), \n                     steps=200)\nprint('evaluate done')\nprint(results)\nprint('Accuracy: %s' % results['accuracy'])\n", "intent": "Let's see how the model did. We will evaluate all the test data.\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y_test,predictions)\n", "intent": "Now we can use the $sklearn.metrics$ to compute the accuracy of these predictions.\n"}
{"snippet": "print metrics.accuracy_score(y_test,predictions_zero)\n", "intent": "What is the accuracy of this model?\n"}
{"snippet": "print metrics.roc_auc_score(dataset_mel['label'], dataset_mel['Eccentricity'])\n", "intent": "<strong>Exercise</strong>\n- What is the AUC for the feature \"Solidity\" in the data set \"dataset_mel\"?\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nprint accuracy_score(target, km.labels_)\n", "intent": "- What is the accuracy of the K-means++ class assignments?\n"}
{"snippet": "accuracy_score(y_test, y_pred)\n", "intent": "`How did our model perform?`\n"}
{"snippet": "accuracy = (TP + TN)/float(TP + TN + FP + FN)\nprint(accuracy)\nprint(accuracy_score(y_test, y_pred))\n", "intent": "`The same as what we saw using the confusion matrix from scikit's metrics module!`\n"}
{"snippet": "def tf_node_loss(logits, labels):\n  return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n", "intent": "Define a per-node loss function for training.\n"}
{"snippet": "y_pred = img_kmeans.fit_predict(img_new)\n", "intent": "We can use our prediction to index into \n"}
{"snippet": "        for t, transition in enumerate(episode):\n            total_return = sum(discount_factor**i * t.reward for i, t in enumerate(episode[t:]))\n            estimator_value.update(transition.state, total_return)\n            baseline_value = estimator_value.predict(transition.state)            \n            advantage = total_return - baseline_value\n            estimator_policy.update(transition.state, advantage, transition.action)\n", "intent": "<img src='./picture/reinforce.png' width=600 height=550 />\n<img src='./picture/reinforce_baseline.png' width=600 height=550 />\n"}
{"snippet": "test_file  = str(\"adult.test.csv\") \nresults = m.evaluate(input_fn=generate_input_fn(test_file, num_epochs=1, shuffle=False), \n                     steps=None)\nprint('evaluate done')\nprint('\\nAccuracy: %s' % results['accuracy'])\n", "intent": "Let's see how the model did. We will evaluate all the test data.\n"}
{"snippet": "print \"accuracy  {:.3}\".format(accuracy_score(ytest, ypred2))\nprint \"precision {:.3}\".format(precision_score(ytest, ypred2))\nprint \"recall    {:.3}\".format(recall_score(ytest, ypred2))\n", "intent": "Results from testing the model on the unbalanced data\nThese are the definitive metrics that inform us about how well this classifier will perform.\n"}
{"snippet": "print \"accuracy  {:.3}\".format(accuracy_score(ytest, ypred3))\nprint \"precision {:.3}\".format(precision_score(ytest, ypred3))\nprint \"recall    {:.3}\".format(recall_score(ytest, ypred3))\n", "intent": "Results from testing the model on the unbalanced data\nThese are the definitive metrics that inform us about how well this classifier will perform.\n"}
{"snippet": "h = .02  \nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n", "intent": "<h2>Visualize Classifier</h2>\n<p>As mentioned above, the technique of using a meshgrid allows us to show multidimentional descision boundaries</p>\n"}
{"snippet": "h = .02  \nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n", "intent": "<h2>Visualize Classifier</h2>\n<p>As mentioned above, the technique of using a meshgrid allows us to show multidimentional descision boundaries</p>\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model2.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "roc_auc_score(df2['parsed_salary'], lm.predict(feature_set))\n", "intent": "The following calculates the area under these curves (AUC):\n"}
{"snippet": "print('Confusion Matrix: (note that False is first row and column)')\nprint(metrics.confusion_matrix(valid_dataframe['Quasar'],RFPredictions))\nprint('\\n')\nprint('Classification Report:')\nprint(metrics.classification_report(valid_dataframe['Quasar'],RFPredictions))\nprint('\\n')\nprint('Validation score: %.4f' % RFScore)\n", "intent": "Below is the confusion matrix, precision, recall, and f1-score on the validation set.\n"}
{"snippet": "CNPredictions = 1 - np.argmax(CNPredictions, axis=1)\nTrueValid = 1 - np.argmax(valid_dataframe[['EncodedQuasar','EncodedNonQuasar']].values, axis=1)\nprint('Confusion Matrix: (note that Non-quasar is first row and column)')\nprint(metrics.confusion_matrix(TrueValid,CNPredictions))\nprint('\\n')\nprint('Classification Report:')\nprint(metrics.classification_report(TrueValid,CNPredictions))\n", "intent": "Below is the confusion matrix, precision, recall, and f1-score on the validation set.\n"}
{"snippet": "def print_rmse(model, name, input_fn):\n  metrics = model.evaluate(input_fn=input_fn, steps=1)\n  print ('RMSE on {} dataset = {} USD'.format(name, np.sqrt(metrics['average_loss'])*SCALE))\n", "intent": "*Step 5: Evaluate Regressors Using a Reduced Feature Space*\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "my_model = my_model_2\nmy_preds = my_model.predict(test_data)\npreds = my_preds\nmost_likely_labels = decode_predictions(preds, top=1, class_list_path='../input/resnet50/imagenet_class_index.json')\nfor i, img_path in enumerate(img_paths[:2]):\n    display(Image(img_path))\n    print(most_likely_labels[i])\n", "intent": "**Method 2: Results**\n"}
{"snippet": "b_size = 32                         \nepochs = 5                          \nmodel.load_weights('./notMNIST.hdf5')\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\nscore = model.evaluate(test_dataset, test_labels, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n", "intent": "The Network can be visualised as below:\n"}
{"snippet": "predictions = model.predict(im)  \nprint(predictions)\n", "intent": "Here, we use the **model.predict** function of Keras to generate prediction for this image. \n"}
{"snippet": "test_preds = np.matmul(X_test_level2, np.array([best_alpha, 1-best_alpha]))\nr2_test_simple_mix = r2_score(y_test, test_preds)\nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "train_preds = lr.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2, train_preds) \ntest_preds = lr.predict(X_test_level2)\nr2_test_stacking = r2_score(y_test, test_preds) \nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)\n", "intent": "Compute R-squared on the train and test sets.\n"}
{"snippet": "def get_regression_metrics(model, actual, predicted):\n    regr_metrics = {\n                        'Root Mean Squared Error' : metrics.mean_squared_error(actual, predicted)**0.5,\n                        'Mean Absolute Error' : metrics.mean_absolute_error(actual, predicted),\n                        'R^2' : metrics.r2_score(actual, predicted),\n                        'Explained Variance' : metrics.explained_variance_score(actual, predicted)\n                   }\n    df_regr_metrics = pd.DataFrame.from_dict(regr_metrics, orient='index')\n    df_regr_metrics.columns = [model]\n    return df_regr_metrics\n", "intent": "Create a helper function to calculate regression metrics\n"}
{"snippet": "clf.predict_proba(vectorizer.transform(['This movie is not remarkable or superb']))\n", "intent": "No, because the \"not\" negates the whole sentiment of remarkable, touching, and superb. It clearly is a rotten review\n"}
{"snippet": "print(classification_report(y_test, pred))\nprint('\\n')\nprint(\"Confusion Metric: \\n\", confusion_matrix(y_test, pred))\nprint('\\n')\nprint(\"Accuracy Score: \", accuracy_score(y_test, pred))\n", "intent": "Evaluating the model\n"}
{"snippet": "print(\"MAE: \", metrics.mean_absolute_error(y_test, pred))\nprint(\"MSE: \", metrics.mean_squared_error(y_test, pred))\nprint(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, pred)))\n", "intent": "** Calculating Errors **\n"}
{"snippet": "complete_ar_score = adjusted_rand_score(iris.target, complete_pred)\navg_ar_score = adjusted_rand_score(iris.target, avg_pred)\n", "intent": "**Exercise**:\n* Calculate the Adjusted Rand score of the clusters resulting from complete linkage and average linkage\n"}
{"snippet": "print('MAE: ', metrics.mean_absolute_error(y_test, pred))\nprint('MSE: ',metrics.mean_squared_error(y_test, pred))\nprint('RMSE: ',np.sqrt(metrics.mean_squared_error(y_test, pred)))\n", "intent": "** Calculating Errors **\n"}
{"snippet": "print(metrics.r2_score(y_test, pred))\n", "intent": "** R2 Score for the model **\n"}
{"snippet": "print('MAE: ', metrics.mean_absolute_error(y_test, pred))\nprint('MSE: ', metrics.mean_squared_error(y_test, pred))\nprint('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, pred)))\n", "intent": "** Errors for the model **\n"}
{"snippet": "for i in range(10):\n    p4 = np.poly1d(np.polyfit(x, y, i))\n    r2_train = r2_score(np.array(trainY), p4(np.array(trainX)))\n    r2_test = r2_score(testy, p4(testx))\n    print \"degree: %s\" % i\n    print \"train R^2: %s\" % r2_train\n    print \"test R^2: %s\" % r2_test\n    print \"\"\n", "intent": "Try measuring the error on the test data using different degree polynomial fits. What degree works best?\n"}
{"snippet": "train_11_pred = grid_logit_1.best_estimator_.predict(\n    non_title_train['Title'].values)\ntrain_11_avg_pre = average_precision_score(y_train_1,\n                                           train_11_pred)\ntrain_auc_11_score = roc_auc_score(y_train_1, train_11_pred)\ntrain_f1_11_score = f1_score(y_train_1, train_11_pred)\nprint(\"Training data scores:\")\nprint(\"AUC score:\", train_auc_11_score)\nprint(\"Average precision:\", train_11_avg_pre)\nprint(\"f1_score:\", train_f1_11_score)\n", "intent": "**model performance**\n"}
{"snippet": "X_new = np.array([[5, 2.9, 1, 0.2]])\nprediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(prediction))\nprint(\"Predicted target name: {}\".format(\n       iris_dataset['target_names'][prediction]))\n", "intent": "Let's create a new example and ask the kNN model to classify it\n"}
{"snippet": "y_pred = knn.predict(X_train)\nprint(\"Test set predictions:\\n {}\".format(y_pred))\n", "intent": "Feeding all test examples to the model yields all predictions\n"}
{"snippet": "print(\"Shape of probabilities: {}\".format(lr.predict_proba(X_test).shape))\nprint(\"Predicted probabilities:\\n{}\".format(\n      lr.predict_proba(X_test[:6])))\n", "intent": "The output of predict_proba is a _probability_ for each class, with one column per class. They sum up to 1.\n"}
{"snippet": "print(\"f1_score of random forest: {:.3f}\".format(\n        f1_score(y_test, rf.predict(X_test))))\nprint(\"f1_score of svc: {:.3f}\".format(f1_score(y_test, svc.predict(X_test))))\n", "intent": "Note that the F1-measure completely misses these subtleties\n"}
{"snippet": "print metrics.mean_absolute_error(y_true, y_pred)\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print(classification_report(y_test, pred))\n", "intent": "Precision, recall, F1-score\n- Now yield 10 per-class scores\n"}
{"snippet": "predicted_rfc = rfc.predict(X_train)\npredicted_gbc = gbc.predict(X_train)\npredicted_etc = etc.predict(X_train)\npredicted_abc = abc.predict(X_train)\npredicted_svc = svc.predict(X_train)\npredicted_knc = knc.predict(X_train)\npredicted_dtc = dtc.predict(X_train)\npredicted_ptc = ptc.predict(X_train)\npredicted_lrc = lrc.predict(X_train)\n", "intent": "Now we'll use the _predict_ method over our training atributes to build every prediction object. \n"}
{"snippet": "print(metrics.classification_report(expected, predicted_rfc))\nprint(metrics.classification_report(expected, predicted_gbc))\nprint(metrics.classification_report(expected, predicted_etc))\nprint(metrics.classification_report(expected, predicted_abc))\nprint(metrics.classification_report(expected, predicted_svc))\nprint(metrics.classification_report(expected, predicted_knc))\nprint(metrics.classification_report(expected, predicted_dtc))\nprint(metrics.classification_report(expected, predicted_ptc))\nprint(metrics.classification_report(expected, predicted_lrc))\n", "intent": "If you feel confortable to see every classification report, feel free to execute this code below (will be deprecated in next version). \n"}
{"snippet": "predictions_rfc = rfc.predict(X_test)\npredictions_gbc = gbc.predict(X_test)\npredictions_etc = etc.predict(X_test)\npredictions_abc = abc.predict(X_test)\npredictions_svc = svc.predict(X_test)\npredictions_knc = knc.predict(X_test)\npredictions_dtc = dtc.predict(X_test)\npredictions_ptc = ptc.predict(X_test)\npredictions_lrc = lrc.predict(X_test)\n", "intent": "Now we'll predict with our test dataset to see the adherence of our models. \n"}
{"snippet": "mse_rfc = mean_squared_error(predictions_rfc, Y_test)\nmse_abc = mean_squared_error(predictions_abc, Y_test)\nmse_etc = mean_squared_error(predictions_etc, Y_test)\nmse_gbc = mean_squared_error(predictions_gbc, Y_test)\nmse_svc = mean_squared_error(predictions_svc, Y_test)\nmse_knc = mean_squared_error(predictions_knc, Y_test)\nmse_dtc = mean_squared_error(predictions_dtc, Y_test)\nmse_ptc = mean_squared_error(predictions_ptc, Y_test)\nmse_lrc = mean_squared_error(predictions_lrc, Y_test)\n", "intent": "Let's store our Mean Squared Error for each classifier.\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\npredicted_home_prices = melbourne_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)\n", "intent": "Once we have a model, here is how we calculate the mean absolute error:\n"}
{"snippet": "test_fiter_more.cross_val_accuracy_score()\n", "intent": "**train test score**\n"}
{"snippet": "scores = model.evaluate(X, Y)\nprint \"Training Dataset %s: %.2f\"%(model.metrics_names[0], scores[1])\nprint \"Training Dataset %s: %.2f%%\"%(model.metrics_names[1], scores[1]*100)\n", "intent": "In this part, we can calculate the accuracy fo this model on training dataset \n"}
{"snippet": "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\nresult = cross_val_score(estimator, X, y_onehot, cv=kfold)\nprint(\"Accuracy: %.2f%% (+-%.2f%%)\"%(result.mean()*100, result.std()*100))\n", "intent": "10-fold cross validation\n"}
{"snippet": "print metrics.mean_squared_error(y_true, y_pred)\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "scores = loaded_model.evaluate(X, Y, verbose=0)\nprint(\"Traning %s: %.2f%%\"%(model.metrics_names[1], scores[1]*100))\n", "intent": "Evaluate the loaded model\n"}
{"snippet": "preds = lm.predict_classes(val_features, batch_size=batch_size)\nprobs = lm.predict_proba(val_features, batch_size=batch_size)[:,0]\nprint('\\n', preds[:8])\nprint(probs[:8])\n", "intent": "Calculate predictions on validation set, so we can find correct and incorrect examples:\n"}
{"snippet": "conv_val_feat = vgg640.predict(val, batch_size=batch_size, verbose=1)\nconv_trn_feat = vgg640.predict(trn, batch_size=batch_size, verbose=1)\n", "intent": "We can now pre-compute the output of the convolutional part of VGG.\n"}
{"snippet": "def data_D(sz, G):\n    real_img = X_train[np.random.randint(0,n,size=sz)]\n    X = np.concatenate((real_img, G.predict(noise(sz))))\n    return X, [0]*sz + [1]*sz  \n", "intent": "Create a batch of some real and some generated data, with appropriate labels, for the discriminator.\n"}
{"snippet": "p = top_model.predict(arr_hr[0:1])\np.shape\n", "intent": "Now we can pass any image through this CNN and it will produce it in the style desired!\n"}
{"snippet": "y_pred = classifier.predict(X_test)\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n", "intent": "** Now create a classification report and a Confusion Matrix. Does anything stand out to you?**\n"}
{"snippet": "y_predictions = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "y_predictions = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(df['Cluster'], km.labels_))\nprint(classification_report(df['Cluster'], km.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = lm.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "print('predicted:', spam_detect_model.predict(tfidf4))\nprint('expected:', messages['label'][3])\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "y_predict = nb.predict(X_test) \n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, y_predict))\nprint(classification_report(y_test, y_predict))\n", "intent": "** Create a confusion matrix and classification report using these predictions and y_test **\n"}
{"snippet": "y_predict = pipeline.predict(X_test)\nprint(confusion_matrix(y_test, y_predict))\nprint(classification_report(y_test, y_predict))\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "kpred = knn.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "pred  = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "X_test = test[feature_cols]\ny_test = test.price\ny_pred = treereg.predict(X_test)\ny_pred\n", "intent": "**Question:** Using the tree diagram above, what predictions will the model make for each observation?\n"}
{"snippet": "print(classification_report(y_test, pred_rf))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "y_pred=best_model.predict(dtest)\n", "intent": "Look at evaluation metrics with the scikit-learn function on the validation data. \n"}
{"snippet": "adult_10_clusters = adult_models[0].predict(adult_train_scaled)\nadult_11_clusters = adult_models[1].predict(adult_train_scaled)\nadult_12_clusters = adult_models[2].predict(adult_train_scaled)\nadult_13_clusters = adult_models[3].predict(adult_train_scaled)\nadult_14_clusters = adult_models[4].predict(adult_train_scaled)\n", "intent": "Compare to 0.801917 peak performance on project 1 and 0.666667 dummy performance.\n"}
{"snippet": "adult_clusters = adult_models[2].predict(adult_train_scaled)\n", "intent": "Compare to 0.801917 peak performance on project 1 and 0.666667 dummy performance.\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(train_diff, ar1ma1.fittedvalues)\nprint(r2)\n", "intent": "Looks like the combo has the lowest AIC score\n"}
{"snippet": "eigen_total = sum(eigenValues)\ndef eigen_score(x):\n   ExplainedVariance = (eigenValues[x]/eigen_total) * 100\n   return \"Explained Variance\", x+1, ExplainedVariance\n", "intent": "Next, Calculate the explained variance\n"}
{"snippet": "print \"Accuracy \" + str(accuracy_score(Y_test, Y_pred))\nprint \"Precision \" + str(precision_score(Y_test, Y_pred))\nprint \"Recall \" + str(recall_score(Y_test, Y_pred))\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "y_train_predict = svm.predict(X_train)\nprint(y_train_predict)\nprint(y_train)\n", "intent": "3) Apply / evaluate\n"}
{"snippet": "from sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, pred_test)\n", "intent": "or >> MSE: from sklearn.metrics import mean_squared_error\n"}
{"snippet": "fpr[\"lr\"], tpr[\"lr\"]  = roc_mean(fpr_list, tpr_list)\nauc_mean[\"lr\"] = roc_auc_score(y_test, probs, average=None)\nplot_ROC(fpr[\"lr\"], tpr[\"lr\"], auc_mean[\"lr\"])\n", "intent": "plot the mean ROC curve\n"}
{"snippet": "cross_val_score(grid_search, X, y, cv=5)\n", "intent": "Nested Cross-Validation\n-------------------------\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error, mean_squared_error\nprint \"RMSE:\", mean_squared_error(ys,predictions)\nprint \"MAE: \", mean_absolute_error(ys,predictions)\n", "intent": "print \"RMSE:\", mean_squared_error(ys, predictions)\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n"}
{"snippet": "y_pred=knn.predict(X_test)\ny_pp=knn.predict_proba(X_test)\ny_pp\n", "intent": "**Calculate the predicted labels and predicted probabilities on the test set.**\n"}
{"snippet": "print('Estimated number of clusters: %d' % n_clusters_)\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, labels))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(y, labels))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(y, labels))\nprint(\"Adjusted Rand Index: %0.3f\"\n      % metrics.adjusted_rand_score(y, labels))\nprint(\"Adjusted Mutual Information: %0.3f\"\n      % metrics.adjusted_mutual_info_score(y, labels))\nprint(\"Silhouette Score: %0.3f\"\n      % metrics.silhouette_score(X, labels))\n", "intent": "Now, we can use a handy chunk of code from the Scitkit documentation to measure the performance of our model \n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nimport math\nprint \"RMSE:\", math.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"RMSE:\", math.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nimport numpy as np\nprint \"RMSE:\", np.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(8)\nprint \"RMSE:\",np.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "acc = accuracy_score(y_test, pred)\nprint acc\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "fpr[\"rf\"], tpr[\"rf\"]  = roc_mean(fpr_list, tpr_list)\nauc_mean[\"rf\"] = roc_auc_score(y_test, probs, average=None)\nplot_ROC(fpr[\"rf\"], tpr[\"rf\"], auc_mean[\"rf\"])\n", "intent": "plot the ROC mean curve\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nsil_score = silhouette_score(X, labels1).mean()\nprint sil_score\n", "intent": "Compute the silhoutte coefficient to see how consistent our data is within the clusters\n"}
{"snippet": "submission = tpot_classifier.predict(titanic_submission)\n", "intent": "And now we are ready to predict and classify the test results\n"}
{"snippet": "submission[\"Survived\"] = model.predict(submission)\n", "intent": "Im going to add a new Column Survived and use the model to predict the \"Survived\" values . such as below..\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print 'predicted:', spam_detect_model.predict(tfidf4)[0]\nprint 'expected:', messages.label[3]\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "def compute_rmse(y_pred, y_true):\n    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))\ndef evaluate(estimate,test=movielens_test):\n    ids_to_estimate = zip(test['user_id'], test['movie_id'])\n    estimated = np.array([estimate(u,i) for (u,i) in ids_to_estimate])\n    real = test.rating.values\n    return compute_rmse(estimated, real)\n", "intent": "``evaluate`` will compute the precision of the recommender system by using the RMSE metric:\n"}
{"snippet": "def rec1(user_id, item_id,train=movielens_train):\n    return 1\nprint 'Error: %s' % evaluate(rec1)\n", "intent": "+ Write a function that, given a user, scores any film with the mean scoring of that user.\n"}
{"snippet": "elast = ElasticNet(alpha=optimal_elast.alpha_, l1_ratio=optimal_elast.l1_ratio_)\nelast_scores = cross_val_score(elast, scaled_data, y, cv=10)\nprint 'R^2: ',elast_scores\nprint 'R^2 mean: ',np.mean(elast_scores)\n", "intent": "---\nHow does it compare to the other regularized regressions?\n"}
{"snippet": "print(clf.predict([[2., 2.]]))\n", "intent": "Now, we try to classify the test data with coordinates (2,2). The classifier classifies it as category 1 which is same as the coordinate (1,1).\n"}
{"snippet": "fpr[\"nn\"], tpr[\"nn\"]  = roc_mean(fpr_list, tpr_list)\nauc_mean[\"nn\"] = roc_auc_score(y_test, probs, average=None)\nplot_ROC(fpr[\"nn\"], tpr[\"nn\"], auc_mean[\"nn\"])\n", "intent": "plot the mean ROC curve\n"}
{"snippet": "from sklearn import metrics\ny_pred = clf_svm.predict(X_test)\nprint(\"Test Precision: {}\".format(metrics.precision_score(y_test, y_pred)))\nprint(\"Test Recall: {}\".format(metrics.recall_score(y_test, y_pred)))\nprint(\"Test F-Score: {}\".format(metrics.f1_score(y_test, y_pred)))\n", "intent": "The scikit-learn metrics package offers the basic evaluation routines.\n"}
{"snippet": "y_pred = model.predict(X_te)\nscore = y_te == y_pred\nscore = model.score(X_te, y_te)\nsum(score)\n", "intent": "Make prediction on test data\n"}
{"snippet": "bio.show_event_error(*errors[0])\n", "intent": "These errors you can then inspect in detail via `show_event_error`:\n"}
{"snippet": "cv_scores = []\nfor n in range(20,300):\n    cv_score = my_cross_val_score(X_train[:n], y_train[:n], num_folds=5)\n    cv_scores.append(cv_score)\n", "intent": "Let's try the latter first:\n"}
{"snippet": "cv_scores = []\nfor k in range(2,51):\n    cv_score = my_cross_val_score(X_train, y_train, num_folds=k)\n    cv_scores.append(cv_score)\n", "intent": "What if we use all the data and vary the number of folds?\n"}
{"snippet": "print(\"Estimated Logistic Coefficients: {}\".format(str(gd.coeffs)))\nprint(\"Predicted Values on Training Data: {}\".format(str(gd.predict(X)[:5])))\n", "intent": "Now that the model has been fit, we can see the estimated coefficients and the final predictions on the training data.\n"}
{"snippet": "print('Cross validated score with 5 folds (scaled pipeline): {}'\n      .format(np.mean(cross_val_score(p_scaled, X1, y1, scoring='accuracy', cv=5))))\nprint('Cross validated score with 5 folds (unscaled pipeline): {}'\n      .format(np.mean(cross_val_score(p_unscaled, X1, y1, scoring='accuracy', cv=5))))\n", "intent": "5\\. Use `cross_val_score` to compute the average `accuracy` of a 5-fold cross validation of the scaled model versus the unscaled model.\n"}
{"snippet": "model.predict(Xtest), ytest\n", "intent": "End of the aside.\nLet's consider the full matrix of predicted and actual values.\n"}
{"snippet": "silhouette_score(wine, wine['cluster'], metric = 'euclidean')\n", "intent": "- Silhouette score over range of K\n- SSE / Inertia over range of K\n"}
{"snippet": "test_svm = test_norm[features_for_svm].copy()\ntest_svm = pd.concat([pd.get_dummies(test_norm.alchemy_category), test_svm], axis=1)\nsvm_pred = svm_classifier.predict(test_svm)\nsvm_pred = svm_all_train.predict_proba(test_svm)[:,1]\n", "intent": "Using the model **with** alchemy category\n"}
{"snippet": "def precision(actual, preds):\n    true_pos = len(np.intersect1d(np.where(preds == 1), np.where(actual == 1)))\n    pred_pos = (preds == 1).sum()\n    return true_pos / pred_pos\nprint(precision(y_test, preds_nb))\nprint(precision_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 4**: Fill in the below function to calculate precision, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def recall(actual, preds):\n    true_pos = len(np.intersect1d(np.where(preds == 1), np.where(actual == 1)))\n    act_pos = (actual == 1).sum()\n    return true_pos / act_pos\nprint(recall(y_test, preds_nb))\nprint(recall_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def f1(preds, actual):\n    return 2 * precision(preds, actual) * recall(preds, actual) / (precision(preds, actual) + recall(preds, actual))\nprint(f1(y_test, preds_nb))\nprint(f1_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "lr_pred = lr.predict(X_test)\ndtr_pred = dtr.predict(X_test)\nrfr_pred = rfr.predict(X_test)\nabr_pred = abr.predict(X_test)\n", "intent": "> **Step 5:** Use each of your models to predict on the test data.\n"}
{"snippet": "def mse(actual, preds):\n    return np.sum(np.square(actual - preds)) / len(actual)\nprint(mse(y_test, preds_tree))\nprint(mean_squared_error(y_test, preds_tree))\nprint(\"If the above match, you are all set!\")\n", "intent": "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. \n"}
{"snippet": "bc_pred = bc.predict(testing_data)\nrfc_pred = rfc.predict(testing_data)\nabc_pred = abc.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "from sklearn.metrics import confusion_matrix\ny_pred = model.predict(X_test)\nprint \"\\n=======confusion matrix==========\"\nprint confusion_matrix(y_test, y_pred)\n", "intent": "Print the confusion matrix for the decision tree model\n"}
{"snippet": "all_predictions = spam_detect_model.predict(messages_tfidf)\nprint all_predictions\n", "intent": "Now we want to determine how well our model will do overall on the entire dataset.\n"}
{"snippet": "x, y = make_xy(critics, vectorizer)\nprob = clf.predict_proba(x)[:, 0]\nprint(len(prob))\n", "intent": "We can see mis-predictions as well.\n"}
{"snippet": "y_pred = clf.predict(X_test)\ny_true = y_test\n", "intent": "- Accuracy\n- Precision\n- Recall\n- F1 Score(f-score)\n"}
{"snippet": "test_pred = model.predict(X_test)\nprint(model.score(X_test,Y_test))\n", "intent": "A little lower training score than before, but let's see how our test score is\n"}
{"snippet": "model.predict_proba(np.array([1500,60]))\n", "intent": "For fun, let's predict if a 1500 sq ft house with a 60 quality rating will be in imy budget. \n"}
{"snippet": "print(metrics.classification_report(Y_test, test_preds))\n", "intent": "However, the confusion matrix tells a different story. 11/16 (~70%) are false positives\n"}
{"snippet": "expected = yTest\npredicted = dtModel.predict(xTest)\nprint(metrics.confusion_matrix(expected, predicted))\n", "intent": "This model has **scored 66.52%**\n"}
{"snippet": "y_pred = logit_cv2.predict_proba(X_test)[:, 1]\ny_pred.shape\n", "intent": "my best cv csore 0.98267878\n"}
{"snippet": "test_pred = logit_cv2.predict_proba(X_test)[:, 1]\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "test_pred = logit_cv.predict_proba(X_test)[:, 1]\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(Y_test, Y_pred)\nprint(acc)\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "print(accuracy_score(gs_cv.predict(Xtestlr),ytestlr))\n", "intent": "It does not give the same value for C. The above grid search seems to indicate that C of 0.001 would be the best. \n"}
{"snippet": "from sklearn.model_selection import StratifiedKFold,KFold\nkf=KFold(n_splits=5, shuffle=True, random_state=2017)\ndef auc_to_gini_norm(auc_score):\n    return 2*auc_score-1\ndef eval_rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\ndef eval_rmsle(y, pred):\n    return mean_squared_error(y, pred)**0.5\n", "intent": "** Cross validation **\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = clf.predict(train[columns])\nprint(roc_auc_score(train['high_income'], predictions))\n", "intent": "* Print out the AUC score between `predictions` and the `high_income` column of `train`.\n"}
{"snippet": "predictions= lgr.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print (classification_report(y_test,pred2))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "prediction= lg.predict(df_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "print (classification_report(y_test,prediction))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\ny_pred = clf_pipeline.predict(X_test)\nprint (\"Test accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\nprint ()\nprint(metrics.classification_report(y_test, y_pred, digits=4))\n", "intent": "Compute common classification metrics and evaluate the models. Decide which model performs best on the given problem.\n"}
{"snippet": "from sklearn import metrics\nprint()\nprint(\"ML MODEL REPORT\")\nprint(\"Accuracy: {}\".format(metrics.accuracy_score(y_test, y_pred)))\nprint(\"Confusion matrix:\")\nprint(metrics.confusion_matrix(y_test, y_pred))\nprint(metrics.classification_report(y_test, y_pred,\n                                            target_names=names))\n", "intent": "Evaluate the models using standard methods.\n"}
{"snippet": "X_test = full_matrix[idx_split:,:]\ntest_pred = lgb_model.predict_proba(X_test)[:, 1]\n", "intent": "Make prediction for the test set and form a submission file.\n"}
{"snippet": "def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n", "intent": "We first define a rmsle evaluation function \n"}
{"snippet": "predictions = list(classifier.predict(X_test))\npredictions\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "X_new =np.array([[3, 5, 4, 2], [5, 4, 3, 2]])\nknn.predict(X_new)\n", "intent": "- Returns a NumPy array\n- Can predict for multiple observations at once\n"}
{"snippet": "from sklearn import metrics\nprint metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "**Classification accuracy:** percentage of correct predictions\n"}
{"snippet": "print (TP + TN) / float(TP + TN + FP + FN)\nprint metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "**Classification Accuracy:** Overall, how often is the classifier correct?\n"}
{"snippet": "print (FP + FN) / float(TP + TN + FP + FN)\nprint 1 - metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "**Classification Error:** Overall, how often is the classifier incorrect?\n- Also known as \"Misclassification Rate\"\n"}
{"snippet": "y_pred = np.dot(X_test[:, causl], w_causl)\nfrom sklearn import metrics\nrmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\nprint(\"Percentage of variance explained (using all SNPs): %.2f\\nRMSE: %2f\" % \\\n    (metrics.explained_variance_score(y_test, y_pred), rmse))\n", "intent": "Let us check how good the true model predicts the phenotypes on the test set\n"}
{"snippet": "y_pred = model_l1_cv.predict(X_test)\n", "intent": "Let us now look at how good the model predicts the phenotype on the test set.\n"}
{"snippet": "multi_normal, y_pred = anomaly.predict(X, Xval, e, Xtest, ytest)\n", "intent": "1. use CV data to find the best $\\epsilon$\n2. use all data (training + validation) to create model\n3. do the prediction on test data\n"}
{"snippet": "predictions_cv = cross_validation.cross_val_predict(clf, xtest, ytest, cv=10)\npredictions = clf.predict_proba(xtest) \n", "intent": "Do the same thing on the test data. We also grab the predictions for the test data as a probability that the event in question results in injury.\n"}
{"snippet": "weight_avg=logit_lv3_train_pred*0.5+ xgb_lv3_train_pred*0.5\nprint(auc_to_gini_norm(roc_auc_score(y_train, weight_avg)))\n", "intent": "We can always still do a simple weight average, to bring the two together and see if there any extra juice to be squeezed\n"}
{"snippet": "X_new = np.array([[0.8]])\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n", "intent": "Now we have an ensemble containing three trees. It can make predictions on a new instance simply by\nadding up the predictions of all the trees:\n"}
{"snippet": "ext_df['prob'] = model.predict_proba(X)[:,1]\next_df['prediction'] = ext_df.prob.apply(lambda x: 1 if x>=0.5 else 0)\next_df = ext_df.sort_values(by = 'prob', ascending = False)\n", "intent": "Results looks a little to good. Let's check the observations scores and the weights assigned to the input features:\n"}
{"snippet": "predictions=model.predict(final_test_tensors, verbose=1)\n", "intent": "Predict complete test set\n"}
{"snippet": "mseFull = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint (mseFull)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "print metrics.accuracy_score(new_data['loan_given'], new_data['predicted_class'])\n", "intent": "My prediction of the accuracy of the model for unseen test data is:\n"}
{"snippet": "rss = np.sum((bos.PRICE - lm.predict(X)) ** 2)\nprint(rss/len(bos))\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "sum(logistic.predict(XX) == YY) / len(XX)\n", "intent": "The score is calculated as \n"}
{"snippet": "class IsolationForest2(IsolationForest):    \n    def predict(self, X):\n        func =  -self.decision_function(X)\n        return (func - min(func)) / (max(func) - min(func))\n", "intent": "Plot the graph how the quality depends on **n_estimators**.\n"}
{"snippet": "inception_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "logit_val_auc=roc_auc_score(y_localval, logit_valprediction)\nlogit_val_gininorm=2*logit_val_auc-1\nprint('Logistic Regression Validation AUC is {:.6f}'.format(logit_val_auc))\nprint('Logistic Regression Validation Normalised Gini Coefficient is {:.6f}'.format(logit_val_gininorm))\n", "intent": "The prediction step is rather straightforward - exactly the same as the one for Random Forest\n"}
{"snippet": "Xtr = x_train[train_idx]\nXv = x_train[valid_idx]\nvgg_bottleneck = VGG16(weights='imagenet', include_top=False, pooling=POOLING)\nvgg_bottleneck.compile('sgd','mse')\ntrain_vgg_bf = vgg_bottleneck.predict(Xtr, batch_size=32, verbose=1)\nvalid_vgg_bf = vgg_bottleneck.predict(Xv, batch_size=32, verbose=1)\nprint('VGG train bottleneck features shape: {} size: {:,}'.format(train_vgg_bf.shape, train_vgg_bf.size))\nprint('VGG valid bottleneck features shape: {} size: {:,}'.format(valid_vgg_bf.shape, valid_vgg_bf.size))\n", "intent": "[See here](https://github.com/keras-team/keras/issues/9394) for the bug I mention below\n"}
{"snippet": "predictions = lgb_model.predict(test_x)\npredictions  = np.expm1(predictions)\n", "intent": "---\n<a id=\"predictions\"></a>\n"}
{"snippet": "b4 = vgg.get_batches(batch_path, batch_size=8)\nimgs,labels = next(b4)\npredictions = vgg.predict(imgs, True)\npredicted_labels = predictions[2]\nplots(imgs, titles=predicted_labels)\n", "intent": "Test a batch and see how it does\n"}
{"snippet": "test_input_fn = create_test_input_fn(df_test)\npredictions = linear_estimator.predict(test_input_fn)\ni = 0\nfor prediction in predictions:\n    true_label = df_test['label'][i]\n    predicted_label = prediction['class_ids'][0]\n    print(\"Example %d. Actual: %d, Predicted: %d\" % (i, true_label, predicted_label))\n    i += 1\n    if i == 5: break\n", "intent": "The Estimator returns a generator object. This bit of code demonstrates how to retrieve predictions for individual examples.\n"}
{"snippet": "score = tl_model.evaluate(test_tl, test_targets)\nprint('Test Accuracy: {}%', score[-1]*100)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "model.load_weights=('best_model.hdf5')\ntrain_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\n", "intent": "With your model fit we can now make predictions on both our training and testing sets.\n"}
{"snippet": "scores = cross_val_score(lr, features_array, target, cv=5, scoring='roc_auc')\nscores.min(), scores.mean(), scores.max()\n", "intent": "`cross_val_score` reports accuracy by default be it can also be used to report other performance metrics such as ROC-AUC or f1-score:\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nfrom scipy.stats import sem\nscores = cross_val_score(pipeline, twenty_train_small.data,\n                         twenty_train_small.target, cv=3, n_jobs=3)\nscores.mean(), sem(scores)\n", "intent": "Such a pipeline can then be cross validated or even grid searched:\n"}
{"snippet": "print('R-squared=', metrics.explained_variance_score(y, y_pred))\n", "intent": "* Better than Linear Regression without cross validation\n"}
{"snippet": "n_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=2017).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, train_target, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n", "intent": "**Define a cross validation strategy**\n"}
{"snippet": "scores_neigh = cross_val_score(neigh, df2_reduced, target_variable, cv=5)\nprint(\"Cross-validated scores for each step: \\n{}\".format(scores_neigh))\n", "intent": "Check the accuracy of models with <b>cross validation</b>.<br>\n"}
{"snippet": "scores_regr = cross_val_score(regr, df2_reduced, target_variable, cv=10)\nscores_neigh = cross_val_score(neigh, df2_reduced, target_variable, cv=10)\nscores_tree = cross_val_score(tree, df2_reduced, target_variable, cv=10)\nprint(\"Accuracy LogisticRegression: %0.2f (+/- %0.2f)\" % (scores_regr.mean(), scores_regr.std() * 2))\nprint(\"Accuracy KNeighborsClassifier: %0.2f (+/- %0.2f)\" % (scores_neigh.mean(), scores_neigh.std() * 2))\nprint(\"Accuracy DecisionTreeClassifier: %0.2f (+/- %0.2f)\" % (scores_tree.mean(), scores_tree.std() * 2))\n", "intent": "<i>Cross-validation to understand best performing algorithm</i>\n"}
{"snippet": "y_pred = tree.predict(df2_test)\ny_pred\n", "intent": "<i>Make predictions with DecisionTreeClassifier because it was the most accurate method on train data based on cross-validation</i>\n"}
{"snippet": "predictions = clf.predict_proba( [\n        [3, 5, 0],\n        [3, 6, 0],\n        [3, 7, 0]\n        ]\n)\npredictions\n", "intent": "Now compare reality (above) with our prediction engine....\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "re-run predictions on this grid object\n"}
{"snippet": "start = round(len(filtered_fare)/2)\ny_pred = fare_model_2.predict(start=start, end=len(filtered_fare)+48, dynamic=True)\npred = [np.nan for _ in range(0, start)] + list(y_pred)\npred = np.array(pred)\n", "intent": "With a durbin_watson statistic of around 2 ~ we've gotten rid of our serial correlation!  Now let's plot the data!\n"}
{"snippet": "X = np.array(means)\nX = X.reshape(-1, 1)\nmodel = GeneralMixtureModel.from_samples(\n    [NormalDistribution, ExponentialDistribution],\n    n_components=3, X=X)\nlabels = model.predict(X)\nlabel_mapping = [\"Normally Distributed\", \"Exponentially Distributed\"]\nfor elem in zip(set(labels), np.bincount(labels)):\n    print(label_mapping[elem[0]], elem[1])\nmodel.plot( n=100000, edgecolor='c', color='c', bins=50 )\n", "intent": "Now that we have some sense of our data, let's see if we can take things further by fitting a distribution to it.\n"}
{"snippet": "inception_predictions = [np.argmax(inception_model.predict(np.expand_dims(feature, axis=0))) \\\n                         for feature in test_inception]\ntest_accuracy = 100*np.sum(np.array(inception_predictions)==np.argmax(test_targets, axis=1))/ \\\n    len(inception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Train Accuracy: \", score[1])\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "def rmsle(preds, true):\n    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n    return float(rmsle)\n", "intent": "__Define utility function__\n"}
{"snippet": "def recall(actual, preds):\n    tp = len(np.intersect1d(np.where(actual == 1), np.where(preds == 1)))\n    ap = (actual == 1).sum()\n    return tp/ap \nprint(recall(y_test, preds_nb))\nprint(recall_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 5**: Fill in the below function to calculate recall, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "def f1(actual, preds):\n    return 2 * (precision(actual, preds) * recall(actual, preds)) / (precision(actual, preds) + recall(actual, preds)) \nprint(f1(y_test, preds_nb))\nprint(f1_score(y_test, preds_nb))\nprint(\"If the above match, you got it!\")\n", "intent": "> **Step 6**: Fill in the below function to calculate f1-score, and then compare your answer to the built in to assure you are correct.\n"}
{"snippet": "pred_lin = mod_lin.predict(X_test)\npred_dec = mod_dec.predict(X_test)\npred_ada = mod_ada.predict(X_test)\npred_rf = mod_rf.predict(X_test)\n", "intent": "> **Step 5:** Use each of your models to predict on the test data.\n"}
{"snippet": "def mse(actual, preds):\n    return ((actual - preds)**2).mean() \nprint(mse(y_test, pred_dec))\nprint(mean_squared_error(y_test, pred_dec))\nprint(\"If the above match, you are all set!\")\n", "intent": "> **Step 8:** Your turn fill in the function below and see if your result matches the built in for mean_squared_error. \n"}
{"snippet": "y_pred_bag = model_bag.predict(testing_data)\ny_pred_rf = model_rf.predict(testing_data)\ny_pred_ada = model_ada.predict(testing_data)\n", "intent": "> **Step 4:** Now that you have fit each of your models, you will use each to `predict` on the **testing_data**.\n"}
{"snippet": "y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n", "intent": "Testing the model\nNow, let's see how our model does, let's calculate the accuracy over both the training and the testing set.\n"}
{"snippet": "mean, std = naturality_score(test_loader, batch_size=batch_size, \n                             gpu_id=gpu_id, weights_path=capsNN_weights_path, model=None)\n", "intent": "Let us first see, what would be the highest possible score, by computing the modified inception score for original MNIST validation images.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\naccs = cross_val_score(knn, x, y, cv=10)\nprint(accs)\nprint(np.mean(accs))\n", "intent": "Use 10 folds. How does the mean performace across folds compare to the baseline accuracy?\n"}
{"snippet": "y_pred = knn.predict(X_test)\ny_pp = knn.predict_proba(X_test)\ny_pp\n", "intent": "**Calculate the predicted labels and predicted probabilities on the test set.**\n"}
{"snippet": "def rmsle(preds, true):\n    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n    return float(rmsle)\n", "intent": "__Define utility functions__\n"}
{"snippet": "from sklearn.metrics import mean_absolute_error\npredicted_home_prices = melbourne_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)\n", "intent": "The calculation of mean absolute error in the Melbourne data is\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\nscores = cross_val_score(my_pipeline, X, y, \n                         scoring='neg_mean_absolute_error',\n                         cv=5)\nprint(scores)\n", "intent": "Finally get the cross-validation scores:\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\ncross_val_score(clf, X=digits.data, y=digits.target, cv=cv)\n", "intent": "We can do all this in one line using the `cross_val_score` method\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(\"Report of SVM prediction\")\nprint(classification_report(y_test, svm_predictions, target_names=uni_list_of_cuisine))\nprint(\"Report of logistic regression prediction\")\nprint(classification_report(y_test, lr_predictions, target_names=uni_list_of_cuisine))\nprint(\"Report of neural network prediction\")\nprint(classification_report(y_test, mlp_predictions, target_names=uni_list_of_cuisine))\nprint(\"Report of combined prediction\")\nprint(classification_report(y_test, predictions))\n", "intent": "The accuracy of the algorithms are compared based on precision, f1-score, etc. The reports are shown below.\n"}
{"snippet": "print(\"Report of SVM prediction\")\nprint(classification_report(y_test, svm_predictions, target_names=uni_list_of_cuisine))\nprint(\"Report of logistic regression prediction\")\nprint(classification_report(y_test, lr_predictions, target_names=uni_list_of_cuisine))\nprint(\"Report of neural network prediction\")\nprint(classification_report(y_test, mlp_predictions, target_names=uni_list_of_cuisine))\nprint(\"Report of combined prediction\")\nprint(classification_report(y_test, predictions))\n", "intent": "The accuracy of the algorithms are compared based on precision, f1-score, etc. The reports are shown below.\n"}
{"snippet": "new_observation = [3, 5, 4, 2]\nknn.predict(new_observation)\n", "intent": "- New observations are called \"out-of-sample\" data\n- Uses the information it learned during the model training process\n"}
{"snippet": "y_pred_gs = clf_gs_cv.predict(Xtestlr)\naccuracy_score(y_pred_gs, ytestlr)\n", "intent": "The ``GridSearchCV`` method returned a different ``C`` and different score.\n"}
{"snippet": "num_test = X_test.shape[0]\npredicted = mlp1.predict(X_test)\nnum_correct = [target[predicted[index]] for index, target in enumerate(y_test)]\naccuracy = sum(num_correct) / num_test\nprint(\"Accuracy on test set with (NH = 10):\\t\" + str(accuracy))\n", "intent": "<h4> Check the accuracy on test set </h4>\n"}
{"snippet": "num_test = X_test.shape[0]\npredicted = mlp2.predict(X_test)\nnum_correct = [target[predicted[index]] for index, target in enumerate(y_test)]\naccuracy = sum(num_correct) / num_test\nprint(\"Accuracy on test set with (NH = 20):\\t\" + str(accuracy))\n", "intent": "<h4> Check the accuracy on test set </h4>\n"}
{"snippet": "def rmsle(preds, true):\n    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n    return float(rmsle)\ndef eval_rmsle(y, pred):\n    return mean_squared_error(y, pred)**0.5\n", "intent": "__Define Utility Function__\n"}
{"snippet": "y_pred = my_tree.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred) \nmodel_valid_accuracy_comparisons[\"Simple Tree\"] = accuracy\nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_valid), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the tree on the validation dataset\n"}
{"snippet": "y_pred = my_tree.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nmodel_test_accuracy_comparisons[\"Simple Tree\"] = accuracy\nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the tree on the validation dataset\n"}
{"snippet": "y_pred = my_tree.predict(X_train)\naccuracy = metrics.accuracy_score(y_train, y_pred) \nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_train, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_train), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the decision tree on the **training set**\n"}
{"snippet": "y_pred = my_tree.predict(X_valid)\naccuracy = metrics.accuracy_score(y_valid, y_pred) \nmodel_valid_accuracy_comparisons[\"Better Tree\"] = accuracy\nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_valid), y_pred, rownames=['True'], colnames=['Predicted'], margins=True, dropna = False)\n", "intent": "Assess the performance of the decision tree on the **validation set**\n"}
{"snippet": "y_pred = my_tuned_tree.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nmodel_test_accuracy_comparisons[\"Tuned Tree\"] = accuracy\nprint(\"Accuracy: \" +  str(accuracy))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluate the performance of the tuned tree\n"}
{"snippet": "print(\"****** Test Data ********\")\ny_pred = my_tuned_model.predict(np.asfarray(X_test))\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\ndisplay(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n", "intent": "Evaluate the model on a test dataset\n"}
{"snippet": "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, km.labels_))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(y, km.labels_))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(y, km.labels_))\n", "intent": "We save you some time and write these performance metrics for you.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nyhat_train = model.predict(x_train)\nacc_train = accuracy_score(y_train, yhat_train)\nprint(\"accuracy score for train dataset: \")\nprint(str(acc_train*100) + \" %\\n\")\nyhat_test = model.predict(x_test)\nacc_test = accuracy_score(y_test, yhat_test)\nprint(\"accuracy score for test dataset: \")\nprint(str(acc_test*100) + \" %\\n\")\n", "intent": "After the model has been fit, we can use the model to predict class labels from the data.\n"}
{"snippet": "p_valid = np.zeros_like(y_valid)\nfor flip in [False, True]:\n    temp_x = x_valid\n    if flip:\n        temp_x = img.flip_axis(temp_x, axis=2)\n    p_valid += 0.5 * np.reshape(model.predict(temp_x, verbose=1), y_valid.shape)\n", "intent": "Little bit of TTA, predict on both horisontal orientations.\n"}
{"snippet": "y_localpred=xgr.predict(x_localval)\n", "intent": "**Validation with x_localval**\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(features, axis=0))) for features in test_VGG19]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('\\nTest Accuracy: %.4f%%' % test_accuracy )                          \n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_expect = y_test\ny_predict = clf.predict(x_test)\nprint(metrics.classification_report(y_expect, y_predict))\n", "intent": "Wow 97% is a really a good score for our model!! \n"}
{"snippet": "feature_1 = model_1_xgboost.predict(xgb.DMatrix(xgtrain))\nfeature_2 = model_2_rf.predict(xgtrain)\nfeature_3 = model_3_et.predict(xgtrain)\nfeature_4 = model_4_xgboost.predict(xgb.DMatrix(xgtrain2))\nfeature_5 = model_5_rf.predict(xgtrain2)\nfeature_6 = model_6_et.predict(xgtrain2)\nfeature_7 = model_7_ridge.predict(xgtrain2)\nfeature_8 = model_8_lasso.predict(xgtrain2)\nfeature_9 = model_11_kr.predict(xgtrain2)\n", "intent": "Stacking the above 9 models to see if it produces better output\n"}
{"snippet": "feature_1 = model_1_xgboost.predict(xgb.DMatrix(xgtrain))\nfeature_2 = model_2_rf.predict(xgtrain)\nfeature_3 = model_3_et.predict(xgtrain)\nfeature_7 = model_7_ridge.predict(xgtrain)\nfeature_8 = model_8_lasso.predict(xgtrain)\nfeature_9 = model_9_sgd.predict(xgtrain)\nfeature_10 = model_10_perceptron.predict(xgtrain)\nfeature_11 = model_11_kr.predict(xgtrain)\nfeature_12 = model_12_svr.predict(xgtrain)\n", "intent": "Stacking the above 9 models to see if it produces better output\n"}
{"snippet": "print clf.predict(scaler.transform([[4.7, 3.1]]))\nprint clf.decision_function(scaler.transform([[4.7, 3.1]]))\n", "intent": "Let's how our classifier can predict the class of a certain instance, given its sepal length and width:\n"}
{"snippet": "from sklearn import metrics\ny_train_pred = clf.predict(X_train)\nprint metrics.accuracy_score(y_train, y_train_pred)\n", "intent": "Let's see how good our classifier is on our training set, measuring accuracy:\n"}
{"snippet": "y_pred = clf.predict(X_test)\nprint metrics.accuracy_score(y_test, y_pred)\n", "intent": "To get a better idea of the expected performance of our classifier on unseen data, que must measure  accuracy on the testing set\n"}
{"snippet": "print metrics.classification_report(y_test, y_pred, target_names=iris.target_names)\nprint metrics.confusion_matrix(y_test, y_pred)\n", "intent": "Let's try some additinoal measures: Precision, Recall and F-score, and show the confusion matrix\n"}
{"snippet": "from scipy.stats import sem\ndef mean_score(scores):\n", "intent": "Calculate the mean and standard error of cross-validation accuracy\n"}
{"snippet": "import html\ntext_input = 'Guten Morgen'\nresult = predictor.predict(text_input)\nprint(html.unescape(result))\n", "intent": "Now it's your time to play. Input a sentence in German and get the translation in English by simply calling predict. \n"}
{"snippet": "def lsq_loss(y_true, y_pred):\n    return np.mean(0.5 * (y_true - y_pred)**2)\n", "intent": "We will also need to specify a loss function. For now let's use the least squared loss, which we will code below.\n"}
{"snippet": "'MSE loss:', lsq_loss(XOR_out, acts[-1])\n", "intent": "You'll notice that these are now for each observation.\n"}
{"snippet": "def ndcg5_score(preds, dtrain):\n    labels = dtrain.get_label()\n    top = []\n    for i in range(preds.shape[0]):\n        top.append(np.argsort(preds[i])[::-1][:5])\n    mat = np.reshape(np.repeat(labels,np.shape(top)[1]) == np.array(top).ravel(),np.array(top).shape).astype(int)\n    score = np.mean(np.sum(mat/np.log2(np.arange(2, mat.shape[1] + 2)),axis = 1))\n    return 'ndcg5', score\n", "intent": "To see the model performance as it advance we are going to define the score function, for this competition is the *NDCG5*:\n"}
{"snippet": "quote = vectorizer.transform(['This movie is not, touching, or superb in any way'])\nclf.predict_proba(quote)\n", "intent": "> It is a clear misclassification, which is not suprising since most words in the review are positive words\n"}
{"snippet": "y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\nprint(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred)))\nprint(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_test_pred)))\n", "intent": "8.once you're satisfied with training, check the R2score on the test set\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Model 1 Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "predictions=svc_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "new_data = np.linspace(0,10,10)\ntest_in_fn = train_in_func = tf.estimator.inputs.numpy_input_fn({'x':new_data},shuffle=False)\nlist(est.predict(input_fn=test_in_fn))\n", "intent": "Running on a brand new test data\n"}
{"snippet": "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\nXception_test_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy of Xception: %.4f%%' % Xception_test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "import html\nresult = predictor.predict(\"I love translation\")\nprint(html.unescape(result))\n", "intent": "Now it's your time to play. Input a sentence in English and get the translation in French by simply calling predict. \n"}
{"snippet": "expected = y_test\npredicted = model.predict(X_test)\naccuracy = accuracy_score(expected, predicted)\nprint( \"Accuracy = \" + str( accuracy ) )\n", "intent": "- Back to [Table of Contents](\nNow let's use the model we just fit to make predictions on our test dataset, and see what our accuracy score is:\n"}
{"snippet": "sns.regplot(lm.predict(),lm.resid,fit_reg = False)\n", "intent": "What I'm doing below is plotting the fitted values (i.e. predicted training values versus the residuals).\n"}
{"snippet": "probability_pred = lm.predict_proba(df[['gre','gpa',1,2,3]])[:,1]\npredictions = (probability_pred >= 1) + 0   \n(lm.predict(df[['gre','gpa',1,2,3]]) == predictions).sum()\n", "intent": "If $P(Y=1|X) > .5$, predict Y = 1 otherwise predict 0.\nLet's verify this manually:\n"}
{"snippet": "ref_pred = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test, ref_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "spam_detect_model.predict(tfidf4)[0]\n", "intent": "Classify our single random message and see how we do.\n"}
{"snippet": "all_pred = spam_detect_model.predict(messages_tfidf)\n", "intent": "Run the prediction on all messages in the tfidf:\n"}
{"snippet": "predictions = [0 if x<0.5 else 1 for x in predictor.predict(x_test)]\nwith open('ann_result.csv', 'w') as output_file:\n    output_file.write('PassengerId,Survived\\n')\n    output_file.write('\\n'.join(['{},{}'.format(id, prediction) for id,prediction in zip(test_ids,\n                                                                                         predictions)]))\n", "intent": "After complete the training, we can make the predictions and generate the submition file:\n"}
{"snippet": "y_pred = [a['predictions'] for a in estimator.predict(test_input)]\n", "intent": "** Create a prediction input function and then use the .predict method off your estimator model to create a list or predictions on your test data. **\n"}
{"snippet": "import html\ntext_input = \"Hey, how you're doing?\"\nresult = predictor.predict(text_input)\nprint(html.unescape(result))\n", "intent": "Now it's your time to play. Input a sentence in English and get the translation in French by simply calling predict. \n"}
{"snippet": "from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(model.labels_, data['Cluster']))\nprint(classification_report(model.labels_, data['Cluster']))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "predictions = log_model.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "pipe_predictions = pipe.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "predictions = lm.predict(X=x_test)\n", "intent": "**Make predictions based on the fitted model**\n"}
{"snippet": "pred = logistic.predict_proba(X)\ndf['propensity score'] = pred[:, 1]\ndf.head()\n", "intent": "We assign this score to each observation in the dataframe\n"}
{"snippet": "preddtree = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "random_pred = rtree.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "print(classification_report(y_test,random_pred))\nprint('\\n')\nprint(confusion_matrix(y_test,random_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions_int = 10**best_int_ridge.predict(price_test_final[int_feats])\npredictions_int_df = pd.DataFrame.from_items([\n    ('PID', price_test_final.index),\n    ('SalePrice', predictions_int)\n])\n", "intent": "Our predictions approximate the original distribution. \n"}
{"snippet": "sample = [6.4,3.2,4.5,1.5]\npredictor.predict(sample)\n", "intent": "We can now use this endpoint to classify. Run an example prediction on a sample to ensure that it works.\n"}
{"snippet": "from sklearn.metrics import accuracy_score,adjusted_rand_score\nprint(accuracy_score(digits.target, labels))\nprint(adjusted_rand_score(digits.target, labels))\n", "intent": "Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:\n"}
{"snippet": "def kl_loss(pij, qij):\n", "intent": "Implement a function to compute the KL-divergence between two distribution $p_{ij}$ and $q_{ij}$.\n"}
{"snippet": "lgb_model = lgb.train(lgb_params, lgb.Dataset(X_trn, y_trn), train_round)\npredictions = lgb_model.predict(X_tst)\n", "intent": "**trian use all data**\n"}
{"snippet": "test_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('Test accuracy:', test_acc)\n", "intent": "Next, compare how the model performs on the test dataset:\n"}
{"snippet": "predictions = model.predict(test_images)\n", "intent": "With the model trained, we can use it to make predictions about some images.\n"}
{"snippet": "predictions = model.predict(img)\nprint(predictions)\n", "intent": "Now predict the image:\n"}
{"snippet": "results = model.evaluate(test_data, test_labels)\nprint(results)\n", "intent": "And let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy.\n"}
{"snippet": "import sklearn as sk\nfrom astropy.table import Table\nprecision = sk.metrics.precision_score(y_test, test_pred, average=None) * 100.\nrecall = sk.metrics.recall_score(y_test, test_pred, average=None) * 100.\nplotScatter(precision, recall, np.arange(n_classes), 'Precison', 'Recall')    \n", "intent": "Compute the precison and recall of each class and visualize which classes needs more data augmentation.\n"}
{"snippet": "y_pred = knn.predict(X)\n", "intent": "- Every SNLS object is associated to an SED in pickle catalog\n"}
{"snippet": "words = [\"awesome\", \"blazing\"]\npayload = {\"instances\" : words}\nresponse = bt_endpoint.predict(json.dumps(payload))\nvecs = json.loads(response)\nprint(vecs)\n", "intent": "The payload should contain a list of words with the key as \"**instances**\". BlazingText supports content-type `application/json`.\n"}
{"snippet": "print('\nprint('Homogeneity {}'.format(metrics.homogeneity_score(y, labels)))\nprint('completeness {}'.format(metrics.completeness_score(y, labels)))\nprint('V-measure {}'.format(metrics.v_measure_score(y, labels)))\n", "intent": "**7.2 Check the homogeneity, completeness, and V-measure against the stored rank `y`. (Optional! We won't cover this until Wednesday!)**\n"}
{"snippet": "print(cross_val_score(lr, ss_like_pca_df.iloc[:,:1], y, cv=5).mean())\n", "intent": "- What is the mean cross val score?\n"}
{"snippet": "Incep_predictions = [np.argmax(Incep_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Inception]\ntest_accuracy = 100*np.sum(np.array(Incep_predictions)==np.argmax(test_targets, axis=1))/len(Incep_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score)\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "tweets = [\"love the weather\",\"\npredictions = pipeline.predict(tweets)\npredictions\n", "intent": "It's worth noting that as have the classifiers, we can try them a couple of new tweets.\n"}
{"snippet": "new_data = np.asarray([0.18,0.15]).reshape(1,-1)\npred3 = knn3.predict(new_data)\npred5 = knn5.predict(new_data)\nprint \"The knn3 model thinks new_data belongs to class {}\".format(pred3[0])\nprint \"The knn5 model thinks new_data belongs to class {}\".format(pred5[0])\n", "intent": "Apply model on a new point\n"}
{"snippet": "accuracy_score(y_test, labels_70)\n", "intent": "Does this give a better accuracy score?\n"}
{"snippet": "def clean_tweets(tweets):\n    tweets = [re.sub(hashtag_pattern, 'HASHTAG', t) for t in tweets]\n    tweets = [re.sub(twitter_handle_pattern, 'USER', t) for t in tweets]\n    return [re.sub(url_pattern, 'URL', t) for t in tweets]\ndef test_tweets(tweets, model):\n    tweets = clean_tweets(tweets)\n    features = countvectorizer.transform(tweets)\n    predictions = model.predict(features)\n    return list(zip(tweets, predictions))\n", "intent": "Use the `test_tweet` function below to test your classifier's performance on a list of tweets. Write your tweets \n"}
{"snippet": "sampleData = dfCheck[:1]\nsampleDataFeatures = np.asarray(sampleData.drop('Outcome',1))\nsampleDataFeatures = (sampleDataFeatures - means)/stds\npredictionProbability = diabetesLoadedModel.predict_proba(sampleDataFeatures)\nprediction = diabetesLoadedModel.predict(sampleDataFeatures)\nprint('Probability:', predictionProbability)\nprint('prediction:', prediction)\n", "intent": "We will now use the first record to make our prediction.\n"}
{"snippet": "predictions = []\ndistance_from_hyperplane = []\nfor array in np.array_split(new_test_data_matrix[:,:-1], 100):\n    result = linear_predictorf.predict(array)\n    predictions += [r['predicted_label'] for r in result['predictions']]\n    distance_from_hyperplane += [r['score'] for r in result['predictions']]\ndistance_from_hyperplane_test_fair = np.array(distance_from_hyperplane)    \npredictions_test_fair = np.array(predictions)\npd.crosstab(np.where(new_test_data_matrix[:,-1] == 1, 1, 0), predictions_test_fair,\n            rownames=['actuals'], colnames=['predictions'])\n", "intent": "Now, we can calculate the predictions using our fair linear model.\n"}
{"snippet": "def test_tweets(tweets, model):\n    tweets = [clean(tweet) for tweet in tweets]\n    features = vectorizer.transform(tweets)\n    predictions = model.predict(features)\n    return list(zip(tweets, predictions))\n", "intent": "Use the `test_tweet` function below to test your classifier's performance on a list of tweets. Write your tweets \n"}
{"snippet": "y_pred = model1.predict(X_train1)\nprint('train data: ',accuracy_score(y_train1, y_pred))\ny_pred = model1.predict(X_test1)\nprint('test data: ',accuracy_score(y_test1, y_pred))\n", "intent": "First lets go back to our original best model ('Model1') and see if it is overfitting the training data:\n"}
{"snippet": "epsilon = 1e-7 \nloss = tf.losses.log_loss(y, y_proba) \n", "intent": "Build it from scratch, and try TF's built in functions.\n"}
{"snippet": "from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        pass\n    def predict(self, X):\n        return np.zeros((len(X),1),dtype=bool)\n", "intent": "90% accuracy! Great no?!?! Not so fast buster!!!! Lets create a reallllly bad classifier that predicts 0 for everything, and see how it performs.\n"}
{"snippet": "print(\"RF scores: \", cross_val_score(forest_clf,X_train,y_train, cv=3,scoring='accuracy'))\nprint(\"SGD scores: \", cross_val_score(sgd_clf,X_train,y_train, cv=3, scoring='accuracy'))\n", "intent": "Let's take a look at how the random forest classifer performs!\n"}
{"snippet": " predictions = list(classifier.predict(X_test, as_iterable=True))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "from sklearn.metrics import confusion_matrix,classification_report\nprint('Confusion Matrix:')\nprint(confusion_matrix(df['Cluster'],kmeans.labels_))\nprint('')\nprint('Classification Reprt:')\nprint(classification_report(df['Cluster'],kmeans.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "scores = cross_val_score(clf, iris.data, iris.target, cv=5)\npoly_scores = cross_val_score(clf_poly, iris.data, iris.target, cv=5)\nprint(\"Scores for linear kernel \\n-------\")\nprint(scores)\nprint(scores.mean())\nprint(\"\\n Scores for poly kernal\\n-------\")\nprint(poly_scores)\nprint(poly_scores.mean())\nprint(clf_poly.predict([[ 5.8,  2.8,  5.1,  2.4], [2, 1, 0, 2, 0, 2]]))\nprint(Y_test)\n", "intent": "Now we can use K-Fold cross validation instead to split the data into K (5 in this case) sections keeping one for testing\n"}
{"snippet": "test_preds = best_alpha * pred_lr + (1 - best_alpha) * pred_lgb \nr2_test_simple_mix = r2_score(y_test, test_preds) \nprint('Test R-squared for simple mix is %f' % r2_test_simple_mix)\n", "intent": "Now use the $\\alpha$ you've found to compute predictions for the test set \n"}
{"snippet": "input = {\n  'observations': np.ones((observation_space_mapping[roboschool_problem])),\n}\nresult = predictor.predict(input)\nresult\n", "intent": "Now let us predict the actions using a dummy observation\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfor metric in ['minkowski', 'cosine']:\n    print (metric)\n    skf = StratifiedKFold(n_splits=n_splits, random_state=skf_seed, shuffle=True) \n    NNF = NearestNeighborsFeats(n_jobs=4, k_list=k_list, metric=metric)\n    preds = cross_val_predict(NNF, X, Y, cv=skf)\n    np.save('data/knn_feats_%s_train.npy' % metric, preds)\n", "intent": "Compute features for train, using out-of-fold strategy.\n"}
{"snippet": "raw_preds = model.predict(X_test)\n", "intent": "We first generate raw predictions and then compare those outcomes with the observed results.\n"}
{"snippet": "print(\"Mean squared error of the baseline model\")\nmean_squared_error(df['value'],df['Baseline Prediction'])\n", "intent": "**Mean squared error of our baseline and see how it looks**\n"}
{"snippet": "mean_squared_error(df['value'],df['Baseline Prediction'])\n", "intent": "**Mean squared error of our baseline and see how it looks**\n"}
{"snippet": "from sklearn.metrics import recall_score\nrecall_score(y_test, ypf)\n", "intent": " - Use [Kavin Markham's notebook](https://github.com/justmarkham/scikit-learn-videos/blob/master/09_classification_metrics.ipynb) as reference\n"}
{"snippet": "print((10 + 0 + 20 + 10) / 4)\nfrom sklearn import metrics\nprint(metrics.mean_absolute_error(true, pred))\n", "intent": "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "import numpy as np\nprint((10**2 + 0**2 + 20**2 + 10**2) / 4)\nprint(metrics.mean_squared_error(true, pred))\n", "intent": "**Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "import numpy as np\nprint(np.sqrt(((10**2 + 0**2 + 20**2 + 10**2) / 4)))\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "classification_error = (FP + FN) / float(TP + TN + FP + FN)\nprint(classification_error)\nprint(1 - metrics.accuracy_score(y_test, y_pred_class))\n", "intent": "**Classification Error:** Overall, how often is the classifier incorrect?\n- Also known as \"Misclassification Rate\"\n"}
{"snippet": "scores = predictor.predict(input_image.asnumpy())\n", "intent": "Now we can use the predictor object to classify the input image:\n"}
{"snippet": "y_pred_class = nb.predict(X_test_dtm)\n", "intent": "**Naive bayes is fast as seen above**\n- This matters when we're using 10-fold cross-validation with a large dataset\n"}
{"snippet": "y_pred_class = logreg.predict(X_test_dtm)\n", "intent": "**This is a lot slower than Naive Bayes**\n- Naive Bayes cannot take negative numbers while Logistic Regression can\n"}
{"snippet": "metrics.accuracy_score(y_test, y_pred_class)\n", "intent": "This is a good model if you care about the probabilities.\n"}
{"snippet": "print((10 + 0 + 20 + 10)/4.)\nfrom sklearn import metrics\nprint(metrics.mean_absolute_error(true, pred))\n", "intent": "Mean Absolute Error (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n"}
{"snippet": "print((10**2 + 0**2 + 20**2 + 10**2)/4.)\nprint(metrics.mean_squared_error(true, pred))\n", "intent": "Mean Squared Error (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n"}
{"snippet": "import numpy as np\nprint(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.))\nprint(np.sqrt(metrics.mean_squared_error(true, pred)))\n", "intent": "Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n"}
{"snippet": "X_new = [[3,5,4,2], [5,4,3,2]]\nknn.predict(X_new)    \n", "intent": " - New observation are called 'out of sample' data\n    - Uses the information it learned during the model training process\n"}
{"snippet": "from sklearn import metrics \nprint metrics.accuracy_score(y, y_pred)\n", "intent": " - PROPORTION OF OUR CORRECT PREDICTIONS\n    - COMMON ELALUATION METRIC FOR CLASSIFICATION PROBLEMS\n"}
{"snippet": "acc = accuracy_score(y_test, y_pred)\nprint acc\n", "intent": "Calculate the accuracy, precision, and recall. What can these three metrics tell you about your model?\n"}
{"snippet": "iris_predictor.predict([6.4, 3.2, 4.5, 1.5]) \n", "intent": "Invoking prediction:\n"}
{"snippet": "predctions = lm.predict(X_test)\n", "intent": "<h3>Predections </h3>\n"}
{"snippet": "y_pred_baseline = baseline_fit.predict(x_train_constant)\nrmse_baseline = sm.tools.eval_measures.rmse(y_train,y_pred_baseline)\nprint(\"Baseline RMSE is:\",rmse_baseline)\n", "intent": "**Observations: Here we can see that newspaper sales are not statistially significant and therefore this validates our earlier observation.**\n"}
{"snippet": "loss = tf.losses.mean_squared_error(y_label, y_network)\n", "intent": "Now we can define the loss function, which our network is going to optimize the weights for:\n"}
{"snippet": "AUC_scores_mix=[]\nw_range=range(10, 100, 1)\nfor w in w_range:\n    y_pred_mix=(w/100)*y_pred_nw2+y_pred_nw*((100-w)/100)\n    AUC_scores_mix.append(roc_auc_score(y_test_genres, y_pred_mix, average='macro'))\n", "intent": "We identify the best accuracy of the model from the weights of the predictions obtained in the images and text models.\n"}
{"snippet": "AUC_scores_mix=[]\nw_range=range(10, 100, 1)\nfor w in w_range:\n    y_pred_mix=(w/100)*y_pred_genres2+y_pred_genres*((100-w)/100)\n    AUC_scores_mix.append(roc_auc_score(y_test_genres, y_pred_mix, average='macro'))\n", "intent": "To make the merge of the models is identified how much should weigh the prediction of each model in such a way that the roc_AUC_score is maximized.\n"}
{"snippet": "w_0 = np.random.rand()\nw_1 = np.random.rand()\nw_0, w_1, my_loss(w_0, w_1, X, Y)\n", "intent": "Those are huge gradients! \n"}
{"snippet": "y_genetic = genetic_regressor.predict(np.c_[x0.ravel(), x1.ravel()]).reshape(x0.shape)\nscore_genetic = genetic_regressor.score(X_test, y_test)\ny_tree = tree_regressor.predict(np.c_[x0.ravel(), x1.ravel()]).reshape(x0.shape)\nscore_tree = tree_regressor.score(X_test, y_test)\ny_neural = neural_regressor.predict(np.c_[x0.ravel(), x1.ravel()]).reshape(x0.shape)\nscore_neural = neural_regressor.score(X_test, y_test)\n", "intent": "All three regressors will try and evaluate the function over our space:\n"}
{"snippet": "resids = bos.PRICE - lm.predict(X)\nMSE = np.mean( resids**2)\nprint(MSE)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "print(np.mean(cross_val_score(reg,X_train,y_train)))\nprint(np.mean(cross_val_score(rf,X_train,y_train)))\nprint(np.mean(cross_val_score(reg2,X_train2,y_train2)))\nprint(np.mean(cross_val_score(cv.best_estimator_,X_train2,y_train2)))\n", "intent": "Random forest regressor seems to be better\n"}
{"snippet": "import numpy as np\nrandom_input = np.random.rand(1, 1, 3, 3)\nprediction = predictor.predict({'inputs': random_input.tolist()})\nprint(prediction)\n", "intent": "Invoking prediction:\n"}
{"snippet": "for clf in classifiers:    \n    train_predict(clf, X_train, y_train, X_test, y_test)\n", "intent": "Evaluating all classifiers.\n"}
{"snippet": "dummy_clf = DummyClassifier(strategy='most_frequent', random_state=1)\ntrain_predict(dummy_clf, X_train, y_train, X_test, y_test)\n", "intent": "Creating a Dummy Classifier which Predicts the Majority Class and XGBoost untuned model\n"}
{"snippet": "scores = sklearn.model_selection.cross_val_score(log_reg, X, y, cv=5)\nprint 'cross-val scores: ', scores\nprint 'mean cross-val: ', scores.mean(), '+/-', scores.std()\n", "intent": "The model fitted on the training dataset has a 79% accuracy, which is close to the accuracy of the model on itself, suggesting good generalization.\n"}
{"snippet": "y_pred = sklearn.model_selection.cross_val_predict(log_reg, X, y, cv=3)\nprint 'accuracy:', sklearn.metrics.accuracy_score(y, y_pred)\nprint 'precision:', sklearn.metrics.precision_score(y, y_pred)\nprint 'recall:', sklearn.metrics.recall_score(y, y_pred)\n", "intent": "Cross-validated accuracy is closer to 79% on average, and remains fairly consistent, suggesting good generalizability.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ndef mean(numbers):\n    return float(sum(numbers)) / max(len(numbers), 1)\nscores = cross_val_score(regr, X_train, y_train)\navg_score=mean(scores)\nprint'Cross validation score examples are ' + str(scores[:3])\nprint'Cross validation average score is ' + str(avg_score)\nprint(\"Because this cross validation has very high average score which are close to 1, so this model is robust\")\n", "intent": "6) Using cross validation, check that the model you built in 5) is robust (generalizes to other random folds of the dataset).\n"}
{"snippet": "predictions = lrmodel.predict( X_test)\n", "intent": "***\nNow that we have fit our model, let's evaluate its performance by predicting off the test values!\n"}
{"snippet": "from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n", "intent": "***\nFinally, lets evaluate our model's performance by calculating the residual sum of squares, and the various loss errors.\n"}
{"snippet": "predictions = lr_model.predict(X_test)\n", "intent": "Let's grab predictions off our test set and see how well it did!\n"}
{"snippet": "print(VGG19_model.evaluate(test_VGG19, test_targets))\nprint(VGG19_model.evaluate(train_VGG19, train_targets))\nprint(VGG19_model.evaluate(valid_VGG19, valid_targets))\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "img_out = cml.predict({input_name: data})[output_name]\n", "intent": "Now, invoke the model and grab the output\n"}
{"snippet": "print(classification_report(y_test, rf_predictions))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "predictions = SVC_model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "clf.predict(np.matrix([6.3, 3.3, 6.0, 2.5]))\n", "intent": "+ Can you modify the code to get a different result?\n"}
{"snippet": "score = model.evaluate(X_test,Y_test,verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:',score[1])\n", "intent": "EVALUATION AND PREDICTION\n"}
{"snippet": "values = dataset.values   \nX = values[:,:8]\ny = values[:,8]\nmodel = LinearDiscriminantAnalysis()\nkfold = KFold(n_splits =3 , random_state = 7)\nresult = cross_val_score(model,X, y, cv= kfold, scoring = 'accuracy' )\nprint('result.mean():', result.mean())\n", "intent": "We now have a dataset that we could use to evaluate an algorithm sensitive to missing values like LDA.\n"}
{"snippet": "prob_y_0 = clf_0.predict_proba(X)\nprob_y_0 = [p[1] for p in prob_y_0]\nprint(\"roc_auc_score::\", roc_auc_score(y,prob_y_0))\n", "intent": "Ok... and how does this compare to the original model trained on the imbalanced dataset?\n"}
{"snippet": "from sklearn.model_selection import cross_val_score, StratifiedKFold\ncv=StratifiedKFold(n_splits=10, random_state=RANDOM_STATE)\nscores = cross_val_score(classifier, cpm, class_labels, cv=cv)\n", "intent": "**10-fold Cross-validation by explicitly calling the StratifiedKFold method to control the random_state param.**\n"}
{"snippet": "preds = lr.predict(X_valid)\n", "intent": "Wouldn't you expect the coefficient for `t2m_fc_mean` to be around 0 and the bias to be close to one? Why is this not the case?\n"}
{"snippet": "df = create_sub(rf.predict(X_test), 'rf.csv')\n", "intent": "That's a little better. But even after trying a range of parameter combinations I was not able to beat the linear regression score. Hmmm?\n"}
{"snippet": "pred1 = dtree.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "y_pred = my_tuned_tree.predict(X_test)\nprint(metrics.classification_report(y_test, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Evaluate the model on a stratified test set\n"}
{"snippet": "y_pred = my_model.predict(X_valid)\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the model on the **validation set**\n"}
{"snippet": "y_pred = my_model.predict(X_valid)\nprint(metrics.classification_report(y_valid, y_pred))\nprint(\"Confusion Matrix\")\npd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n", "intent": "Assess the performance of the decision tree on the **validation set**\n"}
{"snippet": "accuracy1 = metrics.accuracy_score(pred1, y_test)\naccuracy1\n", "intent": "Evaluate the trained classifier\n"}
{"snippet": "score = cross_val_score(clf1, X_train_plus_valid, y_train_plus_valid, cv=10)\n", "intent": "Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier\n"}
{"snippet": "pred5 = my_tuned_model.predict(X_test)\n", "intent": "Evaluate the performance of the model selected by the grid search on a hold-out dataset\n"}
{"snippet": "my_model.predict(X_train)\nfor i in range(my_model.n_estimators_):\n    print(\"** \", i, \" \", my_model.base_estimator_types[i])\n    y_pred = my_model.last_X_stack_queries[:, i]\n    accuracy = metrics.accuracy_score(y_train, y_pred) \n    print(\"Accuracy: \" +  str(accuracy))\n    print(metrics.classification_report(y_train, y_pred))\n    print(\"Confusion Matrix\")\n    display(pd.crosstab(np.array(y_train), y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n", "intent": "Measure the perfromance of the individual models in the stack\n"}
{"snippet": "my_model = SuperLearnerClassifier()\nscores = cross_val_score(my_model, X_train_plus_valid, y_train_plus_valid, cv=cv_folds, n_jobs=-1, verbose = 2)\nprint(scores)\n", "intent": "Perfrom a 10-fold cross validation experiment to evaluate the perofrmance of the SuperLearnerClassifier\n"}
{"snippet": "my_model = SuperLearnerClassifier(use_probs=True, stack_layer_classifier_type=\"logreg\")\nscores = cross_val_score(my_model, X_train_plus_valid, y_train_plus_valid, cv=cv_folds, n_jobs=-1, verbose = 2)\nprint(scores)\n", "intent": "Compare the performance of the ensemble when a label based stack layer training set and a probability based stack layer training set is used.\n"}
{"snippet": "pred2 = r_tree.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "my_model = SuperLearnerClassifier(use_probs=True, include_base_features_at_stack = True, stack_layer_classifier_type = \"logreg\")\nscores = cross_val_score(my_model, X_train_plus_valid, y_train_plus_valid, cv=cv_folds, n_jobs=-1, verbose = 2)\nprint(\"Mean accuracy \", mean(scores))\nprint(scores)\n", "intent": "Evaluate the impact of adding original descriptive features at the stack layer.\n"}
{"snippet": "print(\"Individual accuracies\")\ny_pred = my_model.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred) \nprint(\"** Ensemble  Accuracy: \" +  str(accuracy))\nfor i in range(my_model.n_estimators_):\n    y_pred = my_model.last_X_stack_queries[:, i]\n    accuracy = metrics.accuracy_score(y_test, y_pred) \n    print(\"** \", i, \" \", my_model.base_estimator_types[i], \" Accuracy: \" +  str(accuracy))\n", "intent": "Measure the strength of the individual classifiers within the ensemble by measureing the accuracy of their predictions on a test set. \n"}
{"snippet": "kappa_matrix = np.zeros((my_model.n_estimators_, my_model.n_estimators_))\nfor i in range(my_model.n_estimators_):\n    for j in range(my_model.n_estimators_):\n        kappa = cohen_kappa_score(my_model.last_X_stack_queries[:, i], my_model.last_X_stack_queries[:, j], labels=None, weights=None)\n        kappa_matrix[i][j] = kappa\nprint(kappa_matrix)\n", "intent": "Measrue the disagreement between base estimators by calculating the Cohen's kappa metric between each of their classicications.\n"}
{"snippet": "pred2 = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "def get_silhouette_score(x, estimator):\n    cls = estimator\n    cls_labels = cls.fit_predict(x)\n    score = silhouette_score(x, cls_labels)\n    return score\n", "intent": "We can see the silhouette score for **the best model, `KMeans(2, max_iter=500, init='k-means++', algorithm='elkan', n_init=8)`**\n"}
{"snippet": "pred = lasso.predict(X_test)\n", "intent": "Seuraavaksi on aika ennustaa malli koko harjoitusdatalla ja tarkastella sen antaman ennusteen luotettavuutta.\n"}
{"snippet": "pred = ridge.predict(X_test)\n", "intent": "Ennustetaan malli koko datasetille ja tarkastetaan selitysaste.\n"}
{"snippet": "from theano import tensor as T\nraise NotImplementedError(\"TODO: add any other imports you need\")\ndef evaluate(x, y, expr, x_value, y_value):\n    raise NotImplementedError(\"TODO: implement this function.\")\nx = T.iscalar()\ny = T.iscalar()\nz = x + y\nassert evaluate(x, y, z, 1, 2) == 3\nprint \"SUCCESS!\"\n", "intent": "This exercise requires you to compile a Theano function and call it to execute `\"x + y\"`.\n"}
{"snippet": "from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n", "intent": "Evaluate model using k fold cross validation\n"}
{"snippet": "log.predict(X_test)\nlog.score(X,y)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "scores = model.evaluate(X_test_pad, y_test, verbose=0)  \nprint(\"Test accuracy:\", scores[1])  \n", "intent": "Once you have trained your model, it's time to see how well it performs on unseen test data.\n"}
{"snippet": "own_predictions = [np.argmax(own_model.predict(np.expand_dims(feature, axis=0))) for feature in test_own]\ntest_accuracy = 100*np.sum(np.array(own_predictions)==np.argmax(test_targets, axis=1))/len(own_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "ridge_scores = cross_val_score(opt_ridge, predictors, y, cv=10)\nprint \"Cross-validated scores:\", ridge_scores\nprint \"Mean: \", np.mean(ridge_scores)\n", "intent": "---\nIs it better than the Linear regression? If so, why would this be?\n"}
{"snippet": "elastic_scores = cross_val_score(opt_elastic, predictors, y, cv=10)\nprint \"Cross-validated scores:\", elastic_scores\nprint \"Mean: \", np.mean(elastic_scores)\n", "intent": "---\nHow does it compare to the other regularized regressions?\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom math import sqrt\nprint \"RMSE:\", sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"RMSE:\", sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "metrics.silhouette_score(dn, labels, metric='euclidean')\nlen(dn)\n", "intent": "Find the Silhoutte Score and plot\n"}
{"snippet": "metrics.accuracy_score(labels,y)\n", "intent": "Plot the predicted vs actual classifcations to see how our clustering analysis compares\n"}
{"snippet": "metrics.silhouette_score(y,labels,metric='euclidean')\n", "intent": "Check the centroids to see where each cluster is lying \n"}
{"snippet": "pred = ir.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "roc_auc_score(y_test, clf.predict_proba(X_test)), accuracy_score(y_test, clf.predict(X_test))\n", "intent": "**ROC AUC and share of correct predictions on validation set.**\n"}
{"snippet": "linscores = linmodel.evaluate(X_test.reshape((-1,28*28)), \n                              Y_test, \n                              verbose=2)\nprint(\"%s: %.2f%%\" % (linmodel.metrics_names[1], linscores[1]*100))\n", "intent": "For a better measure of the quality of the model, let's see the model accuracy for the test data. \n"}
{"snippet": "linpredictions = linmodel.predict(X_test.reshape((-1,28*28)))\nshow_failures(linpredictions)\n", "intent": "Here are the first 10 test digits the linear model classified to a wrong class:\n"}
{"snippet": "scores = model.evaluate(X_test.reshape((-1,28*28)), Y_test, verbose=2)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "Accuracy for test data.  The model should be somewhat better than the linear model. \n"}
{"snippet": "linscores = linmodel.evaluate(X_test.reshape((-1, 28*28)), Y_test, verbose=2)\nprint(\"%s: %.2f%%\" % (linmodel.metrics_names[1], linscores[1]*100))\n", "intent": "For a better measure of the quality of the model, let's see the model accuracy for the test data. \n"}
{"snippet": "linpredictions = linmodel.predict(X_test.reshape((-1, 28*28)))\nshow_failures(linpredictions)\n", "intent": "Here are the first 10 test digits the linear model classified to a wrong class:\n"}
{"snippet": "scores = model.evaluate(X_test.reshape((-1, 28*28)), Y_test, verbose=2)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n", "intent": "Accuracy for test data.  The model should be somewhat better than the linear model. \n"}
{"snippet": "print np.hstack([knr.predict(xin[:5]),(S*y)[:5]])\n", "intent": " The sub-blocks show the windows of the the `y` data that are being\nprocessed by the nearest neighbor estimator. For example,\n"}
{"snippet": "print np.allclose(knr.predict(xin),S*y)\n", "intent": " Or, more concisely checking all entries for approximate equality,\n"}
{"snippet": "def computeTestScores(test_x, test_y, clf, cv):\n    kFolds = sklearn.cross_validation.KFold(test_x.shape[0], n_folds=cv)\n    scores = []\n    for _, test_index in kFolds:\n        test_data = test_x[test_index]\n        test_labels = test_y[test_index]\n        scores.append(sklearn.metrics.accuracy_score(test_labels, clf.predict(test_data)))\n    return scores\n", "intent": "Test the performance of our tuned KNN classifier on the test set.\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "** Now take that grid model and create some predictions using the test set and create classification reports and confusion matrices for them**\n"}
{"snippet": "model.predict(x_test)\n", "intent": "Our fairly naive approach achieves an accuracy of 88%. With state-of-the-art approaches, one should be able to get close to 95%.\n"}
{"snippet": "model_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogXception]\ntest_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=1))/len(model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "clf.predict_proba(X_test)\nfor i in range(0, len(clf.predict_proba(X_train_unlabeled))):\n    print(i, clf.predict_proba(X_train_unlabeled)[i])\n    if clf.predict_proba(X_train_unlabeled)[i][0] > clf.predict_proba(X_train_unlabeled)[i][1]:\n        if clf.predict_proba(X_train_unlabeled)[i][0] > 0.9:\n", "intent": "[response here]\n[response here]\n"}
{"snippet": "cm_test = confusion_matrix(test_target,\n                 clf_tf_idf.predict(vectorizer_tf_idf.transform(test_data)))\ncm_test\n", "intent": "there are many false negatives; maybe because slightly neutral reviews were more often classified as negative.\n"}
{"snippet": "from sklearn.metrics import recall_score\nprint('Recall: {:.2%}'.format(recall_score(test_target, y_predict)))\n", "intent": "----\nCalcute the recall on the test data\n<br>\n<details><summary>\nClick here for the recall score...\n</summary>\n<br>\n78.38%\n</details>\n"}
{"snippet": "cm = confusion_matrix(y, gnb.predict(X))\ncm_norm = cm/cm.sum(axis=1)[:, np.newaxis]\ndef plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n", "intent": "Plot the (normalised) confusion matrix for the training data. Label axes appropriately.\n"}
{"snippet": "rmse = np.sqrt(mean_squared_error(b, multib_pred))\nprint (\"The Root mean squared error is : {}\" .format(rmse))\nprint (\"The Mean absolute error is : {}\" .format(mean_absolute_error(b, multib_pred)))\nprint (\"The Correlation Coefficient is : {}\" .format((np.corrcoef(b, multib_pred))[0][1]))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "print (\"The Coefficient of Determination is : {}\" .format(r2_score(y, y_pred)))\nrmse = np.sqrt(mean_squared_error(y, y_pred))\nprint (\"The Root mean squared error is : {}\" .format(rmse))\nprint (\"The Mean absolute error is : {}\" .format(mean_absolute_error(y, y_pred)))\nprint (\"The Correlation Coefficient is : \\n  {}\" .format((np.corrcoef(y, y_pred))[0][1]))\n", "intent": "<font color='red'>TASK MARK: 2</font>\n<br>\n<font color='green'>COMMENT:  Avoid spurious precision. </font>\n"}
{"snippet": "print np.sum((bos.PRICE - lm.predict(X)) ** 2)\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "ResNet50_model_predictions = [np.argmax(my_ResNet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_model_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "scores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=10)\nprint scores\nprint scores.mean()\n", "intent": "K-Fold cross validation is just as easy; let's use a K of 5:\n"}
{"snippet": "dt_scores = cross_val_score(decision_tree_classifier, X, y, cv=10)\nsb.boxplot(dt_scores)\n", "intent": "Alright! We finally have our demo classifier. Let's create some more visuals of its performance so we have something to put in our report.\n"}
{"snippet": "line_Yhat_query = line_reg.predict(\n    np.reshape(X_query, (len(X_query),1)))\n", "intent": "Use the regression model to render predictions at each `X_query` point.\n"}
{"snippet": "line_Yhat = line_reg.predict(train_data[['X']])\n", "intent": "To plot the residual we will also predict the $Y$ value for all the training points:\n"}
{"snippet": "score = model.evaluate(test_x, test_y_onehot, batch_size=128)\nprint(\"LOSS (evaluated on the test dataset)=     {}\".format(score[0]))\nprint(\"ACCURACY (evaluated on the test dataset)= {}\".format(score[1]))\n", "intent": "Now, you'll probably want to evaluate or save the trained model.\n"}
{"snippet": "pred_y=model.predict(test_x).argmax(axis=1)\nfrom sklearn.metrics import classification_report\nprint( classification_report(test_y,pred_y) )\n", "intent": "Output the classification report (see if the trained model works well on the test data):\n"}
{"snippet": "score = model.evaluate(x_test_e, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "idx = 83\nexp = explainer.explain_instance(newsgroups_test.data[idx], c.predict_proba, num_features=6)\nprint('Document id: %d' % idx)\nprint('Probability(christian) =', c.predict_proba([newsgroups_test.data[idx]])[0,1])\nprint('True class: %s' % class_names[newsgroups_test.target[idx]])\n", "intent": "We then generate an explanation with at most 6 features for an arbitrary document in the test set.\n"}
{"snippet": "print \"Fit a model X_train, and calculate MSE with Y_train:\", np.mean((Y_train - lm.predict(X_train)) ** 2)\nprint \"Fit a model X_train, and calculate MSE with X_test, Y_test:\", np.mean((Y_test - lm.predict(X_test)) ** 2)\n", "intent": "Now, calculate the mean squared error using just the test data and compare to mean squared from using all the data to fit the model. \n"}
{"snippet": "classifier = tf.estimator.LinearClassifier(feature_columns=[age])\nclassifier.train(train_inpf)\nresult = classifier.evaluate(test_inpf)\nclear_output()  \nprint(result)\n", "intent": "The following will train and evaluate a model using only the `age` feature:\n"}
{"snippet": "classifier = tf.estimator.LinearClassifier(feature_columns=my_numeric_columns)\nclassifier.train(train_inpf)\nresult = classifier.evaluate(test_inpf)\nclear_output()\nfor key,value in sorted(result.items()):\n  print('%s: %s' % (key, value))\n", "intent": "You could retrain a model on these features by changing the `feature_columns` argument to the constructor:\n"}
{"snippet": "classifier = tf.estimator.LinearClassifier(feature_columns=my_numeric_columns+my_categorical_columns)\nclassifier.train(train_inpf)\nresult = classifier.evaluate(test_inpf)\nclear_output()\nfor key,value in sorted(result.items()):\n  print('%s: %s' % (key, value))\n", "intent": "It's easy to use both sets of columns to configure a model that uses all these features:\n"}
{"snippet": "results = model.evaluate(test_inpf)\nclear_output()\nfor key,value in sorted(result.items()):\n  print('%s: %0.2f' % (key, value))\n", "intent": "After the model is trained, evaluate the accuracy of the model by predicting the labels of the holdout data:\n"}
{"snippet": "result_batch = mobilenet_classifier.predict(image_batch)\n", "intent": "Now run the classifier on the image batch.\n"}
{"snippet": "result = model.predict(image_batch)\nresult.shape\n", "intent": "Test run a single batch, to see that the result comes back with the expected shape.\n"}
{"snippet": "result_batch = model.predict(image_batch)\nlabels_batch = label_names[np.argmax(result_batch, axis=-1)]\nlabels_batch\n", "intent": "Run the image batch through the model and comvert the indices to class names.\n"}
{"snippet": "predictions_single = model.predict(img)\nprint(predictions_single)\n", "intent": "Now predict the image:\n"}
{"snippet": "example_batch = normed_train_data[:10]\nexample_result = model.predict(example_batch)\nexample_result\n", "intent": "Now try out the model. Take a batch of `10` examples from the training data and call `model.predict` on it.\n"}
{"snippet": "print np.sum((faithful.eruptions - resultsW0.predict(X)) ** 2)\n", "intent": "The residual sum of squares: \n"}
{"snippet": "print(classification_report(y_test, y_pred_gbc))\n", "intent": "From the confusion matrix we can conclude that the model was able to predict **97%** correct class. Which is great model.\n"}
{"snippet": "def run_final_classifier(path, forest=os.path.join('.', 'trained_classifier.p')):\n    clf = pickle.load(open('trained_classifier.p', 'r'))\n    images = listdir(path)\n    print \"%-40s\" % \"filename\", \"%-40s\" % \"predicted_class\"\n    print \"\".join([\"-\" for i in range(60)])\n    for image in images:\n        predict = clf.predict(extractFeaturesFromOneImage(path + '/' + image))\n        print \"%-40s\" % image, \"%-40s\" % predict\n", "intent": "4) **A function that can take it a path and a pickled forest**\n"}
{"snippet": "accs = []\nfor k in k_values:\n    knn = \n    scores = cross_val_score(\n    accs.append(np.mean(\n", "intent": "Plot the cross-validated mean accuracy for each score. What is the best accuracy?\n"}
{"snippet": "expected = y\npredicted = clf.predict(X)\nconfusionMatrix = confusion_matrix(expected,predicted)\nprint(confusionMatrix)\n", "intent": "**d)** Compute the resulting confusion_matrix by using the builtin scikit-learn class and display the result. \n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nprint \"RMSE:\", mean_squared_error(ys, predictions)**.5\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "print(neigh.predict(df.iloc[500:,2:]))\nprint(neigh.predict_proba(df.iloc[500:,2:]))\n", "intent": "Prepare test data and predict\n"}
{"snippet": "print(neigh.predict([[1.5]]))\nprint(neigh.predict_proba([[1.5]]))\n", "intent": "Predict and Print Results\n"}
{"snippet": "metrics.silhouette_score(matrix, labels, metric=\"euclidean\")\n", "intent": "Find the Silhoutte Score and plot\n"}
{"snippet": "logreg.predict_proba(3)[:, 1]\n", "intent": "**Interpretation:** A 1 unit increase in 'al' is associated with a 4.18 unit increase in the log-odds of 'assorted'.\n"}
{"snippet": "print np.mean((faithful.eruptions - resultsW0.predict(X)) ** 2)\n", "intent": "Mean squared error: \n"}
{"snippet": "accuracy_score([1,2,3],[3,2,1])\n", "intent": "0.99 wow it should be overfitting\n"}
{"snippet": "m_lin.predict(...)\n", "intent": "Predict the charges for person a (age 20, bmi 20, smoker) and person b (age 50, bmi 20, non-smoker)\n"}
{"snippet": "m_tree.predict(...)\n", "intent": "Predict the charges for person a (age 20, bmi 20, smoker) and person b (age 50, bmi 20, non-smoker)\n"}
{"snippet": "import sklearn.metrics as met\ndef err(y, yhat):\n    print(np.sqrt(met.mean_squared_error(y_true=y, y_pred=yhat)))\n    print(met.mean_absolute_error(y_true=y, y_pred=yhat))\n", "intent": "Make a convenience function err that prints the root mean square error (rmse) and the mean absolute error (mae). Print rmse and mae for test data.\n"}
{"snippet": "yhat_tree_train = m_tree.predict(X=X_train)\nprint(np.sqrt(met.mean_squared_error(y_true=y_train, y_pred=yhat_tree_train)))\nprint(met.mean_absolute_error(y_true=y_train, y_pred=yhat_tree_train))\n", "intent": "Compute the rmse and mae for the train set.\n"}
{"snippet": "yhat_tree_test = m_tree.predict(X=X_test)\nprint(np.sqrt(met.mean_squared_error(y_true=y_test, y_pred=yhat_tree_test)))\nprint(met.mean_absolute_error(y_true=y_test, y_pred=yhat_tree_test))\n", "intent": "Compute the rmse and mae for the test set.\n"}
{"snippet": "err(y_train, m_svm.predict(X_train))\n", "intent": "What are the errors for both the train and test sets?\n"}
{"snippet": "err(y_train, m_nn.predict(X_train))\n", "intent": "What are the errors for both the train and test sets?\n"}
{"snippet": "err(y_train, m_rf.predict(X_train))\n", "intent": "What are the errors for both the train and test sets?\n"}
{"snippet": "newX = np.array([1, 4, 120])\nresults.predict(newX)\n", "intent": "What if a new car had 4 cylinders and 120 horsepower? \n"}
{"snippet": "yhat_nb_test = m_nb.predict(X=X_test)\n", "intent": "Predict the labels of the test data, using the trained model\n"}
{"snippet": "yhat_logit_test = m_logit.predict(X=X_test)\n", "intent": "Predict the labels of the test data, using the trained model.\n"}
{"snippet": "yhat_tree_test = m_tree.predict(X=X_test)\nconfusion_matrix_plot(y_test, yhat_tree_test)\n", "intent": "What does the confusion matrix look like?\n"}
{"snippet": "yhat_rf_test = m_rf.predict(X=X_test)\nconfusion_matrix_plot(y_test, yhat_rf_test)\n", "intent": "What does the confusion matrix look like?\n"}
{"snippet": "yhat_nn_test = m_nn.predict(X=X_test)\nconfusion_matrix_plot(y_test, yhat_nn_test)\n", "intent": "Show the confusion matrix for the test set.\n"}
{"snippet": "y_pred = clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\n", "intent": "Now that the classifier is trained, we can use it to predect the classes of our test data and have a look at the accuracy.\n"}
{"snippet": "probs = clf.predict_proba(X_test)\n", "intent": "But since the accuracy is only part of the story, let's get out the probability of belonging to each class so that we can generate the ROC curve.\n"}
{"snippet": "y_pred = clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\n", "intent": "Let's see if the accuracy has improved\n"}
{"snippet": "probs = clf.predict_proba(X_test)\n", "intent": "The accuracy is more or less unchanged, but we do get a slightly better ROC curve\n"}
{"snippet": "newPoint_1 = np.array([6,9])\nnewPoint_2 = np.array([2,4])\nprint classifier.predict([newPoint_1, newPoint_2])\nfig_ml_in_10\n", "intent": "Predict a Label\n===============\n"}
{"snippet": "cross_val_score(model_lr, X=X, y=ending_fact, scoring='accuracy')\n", "intent": "ngram_range = (1, 2)\n"}
{"snippet": "print X.shape\ncross_val_score(model_lr, X=X, y=y, scoring='accuracy')\n", "intent": "ngram_range=(1, 10)\n"}
{"snippet": "print X.shape\ncross_val_score(model_lr, X=X, y=y, scoring='accuracy')\n", "intent": "ngram_range=(1, 11)\n"}
{"snippet": "y = model.predict(data)\ny_class = np.argmax(y, axis=1)\n", "intent": "Now, we will obtain the predictions from the model.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn import metrics\ny_pred_1 = knn.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred_1))\n", "intent": "Train your model using the training set then use the test set to determine the accuracy\n"}
{"snippet": "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntestSet_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % testSet_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "y_avg = np.ones(len(y_test)) * np.mean(y_train)\nmse_avg = mean_squared_error(y_test, y_avg)\nprint(np.mean(y_train))\nprint(np.ones(len(y_test)))\nprint(\"\\n y_avg\\n\\n\", y_avg)\nprint(mse_avg)\nmae_avg = mean_absolute_error(y_test, y_avg)\n", "intent": "This section predicts the housing prices using average values of training samples.\n"}
{"snippet": "from sklearn import metrics\nmse=metrics.mean_squared_error(y_test,predictions)\nrmse=np.sqrt(mse)\nrmse\n", "intent": "plotting residuals. normally distributed residuals means model was a good choice for the data\n"}
{"snippet": "sklearn.metrics.accuracy_score(y, y_hat, normalize=True)\n", "intent": "Accuracy for Multi Class\n========================\n$$ \\frac{N_{\\text{true}}}{N_{\\text{all}}} $$\n"}
{"snippet": "prediction = kM.predict(df['Cluster'],kM.la)\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "pred = knn.predict(X_test)\n", "intent": "Let's evaluate our KNN model!\n"}
{"snippet": "print (classification_report(predictions, y_test))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "pipeline_pred = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "spam_detect_model.predict(tfidf20)[0]\n", "intent": "Let's try classifying our single random message and checking how we do:\n"}
{"snippet": "roc_auc_score(df[\"admit\"], predicted_classes)\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "network_model_predictions = [np.argmax(network_model.predict(np.expand_dims(feature, axis=0))) for feature in test_data]\ntest_accuracy = 100*np.sum(np.array(network_model_predictions)==np.argmax(test_targets, axis=1))/len(network_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predicted_y_dnn = classifier.predict(X_test)['classes']\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "predicted_y_rf = randForest.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "import numpy as np\nfor n in xrange(3):\n    idx = np.random.permutation(X.index)\n    X_shuffle, y_shuffle = X.loc[idx] , y.loc[idx]\n    l = cross_val_score(model, X_shuffle, y_shuffle, cv=5)\n    print l, l.mean()\n", "intent": "- This would definitely be useful if we aren't sure the data has been sorted in some way.\n- We could also shuffle our data around, to avoid this\n"}
{"snippet": "metrics.explained_variance_score(Y_test,predicted_Y)\n", "intent": "We could quickly calculate $R^2$ to find if we are doing fine on the model fit\n"}
{"snippet": "predicted_class = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "predicted_class = mnb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "predicted_class_2 = pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "predictions = svm_model.predict(X_test)\n", "intent": "Now let's predict using the trained model.\n"}
{"snippet": "import math\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nprint \"MSE:\", mean_squared_error(ys, predictions)\nprint \"RMSE:\", math.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "xs.append(4)\nys.append(17)\npredictions.append(30)\nprint \"MSE:\", mean_squared_error(ys, predictions)\nprint \"RMSE:\", math.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "Now let's add an outlier to the data.\n"}
{"snippet": "test = features[50:61]\npredict = df.predict(test)\nactual = list(target[50:61])\nprint predict\nprint actual\n", "intent": "Prepare Test Data and Predict\n"}
{"snippet": "best_score = -999\nn_clusters = 0\nfor i in range(2, 31):\n    temp_score = gmm_score(i)\n    if(temp_score > best_score):\n        best_score = temp_score\n        n_clusters = i\n    print(\"Total score for %s clusters is %s\" % (i, temp_score))\nprint(\"The best score was %s for %s clusters\" % (best_score,n_clusters))\n", "intent": "* Report the silhouette score for several cluster numbers you tried. \n* Of these, which number of clusters has the best silhouette score?\n"}
{"snippet": "y_pred = model.predict(X)\n", "intent": "So we're almost $12,800 off, on average.\n"}
{"snippet": "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "Xception_predictions = [np.argmax(XceptionModel.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\ntest_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "ResNet50_predictions = [np.argmax(ResNet50_model.predict(np.expand_dims(feature, axis=0))) \n                        for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==\n                           np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = pipeline.predict(X2_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "nb.predict_proba(X_test_dtm)[false_positives][0]\n", "intent": "It's also possible to see how strong a model's prediciton is. Lets see how strongly the model classified the above examples.\n"}
{"snippet": "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\ny_pred_prob\n", "intent": "Despite the mix of positive and negative terms in both of the above examples, the model decisively predicted the classes incorrectly.\n"}
{"snippet": "knn.predict([[3, 5, 1, .2]])\n", "intent": "In order to **make a prediction**, the new observation must have the **same features as the training observations**, both in number and meaning.\n"}
{"snippet": "df['Predictions'] = reg.predict(x_values)\ndf[\"Pred_Error\"] = df['Predictions'] - df['Outcome']\ndf.head()\n", "intent": "So now we have simple trained dataset. now to make a prediction.\n"}
{"snippet": "def predict(W, b, X):\n    m = X.shape[1]\n    WT = T.transpose(W, 0, 1)\n    A = ...\n    return A\n", "intent": "**Expected Output:**\n```\nW = tensor([[ 0.1903], [ 0.1226]])\nb = tensor(1.9254)\ndW = tensor([[ 0.6775], [ 1.4163]])\ndb = tensor(0.2192)\n```\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nprint -cross_val_score(model, X, y, cv=10, scoring=\"mean_absolute_error\").mean()\nprint -np.median(cross_val_score(model, X, y, cv=10, scoring=\"median_absolute_error\"))\n", "intent": "Now with cross-validation.\n"}
{"snippet": "certainty = clf.predict_proba(transformed[:, :-222])\n", "intent": "We may also look at prediction confidences to find if there are cases where the random forest is uncertain which value is most probable:\n"}
{"snippet": "y_predictions = logistic.predict(x)\n", "intent": "Then, we do the predicitons.\n"}
{"snippet": "print(classification_report(y, y_predictions))\n", "intent": "In addition we printed classification report.\n"}
{"snippet": "df['propensity_score'] = logistic.predict_proba(x)[:,1]\ndf.head()\n", "intent": "The last part of this task is to add propensity scores to the dataframe.\n"}
{"snippet": "y_pred = best_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n", "intent": "Now that we have found our best model, we will evaluate its accuracy on the test set as well as display its confusion matrix.\n"}
{"snippet": " def softmax_loss(x, y):\n    return loss, dx\n", "intent": "<h3>Softmax Loss Layer</h3>\n<img src=\"img/loss.png\" width=\"300\">\n<img src=\"img/log.png\" width=\"600\">\n"}
{"snippet": " def softmax_loss(x, y):\n    probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n    probs /= np.sum(probs, axis=1, keepdims=True)\n    N = x.shape[0]\n    loss = -np.sum(np.log(probs[np.arange(N), y]+1e-8)) / N\n    dx = probs.copy()\n    dx[np.arange(N), y] -= 1\n    dx /= N\n    return loss, dx\n", "intent": "<h3>Softmax Loss Layer</h3>\n<img src=\"img/loss.png\" width=\"300\">\n<img src=\"img/log.png\" width=\"600\">\n"}
{"snippet": "def compute_loss(X, y, w):\ndef compute_grad(X, y, w):\n", "intent": "The loss you should try to minimize is the Hinge Loss:\n$$ L =  {1 \\over N} \\sum_{i=1}^N max(0,1-y_i \\cdot  w^T x_i) $$\n"}
{"snippet": " def softmax_loss(f, y):\n    return loss, dx\n", "intent": "<h3>Softmax Loss Layer</h3>\n<img src=\"img/loss.png\" width=\"300\">\n<img src=\"img/log.png\" width=\"600\">\n"}
{"snippet": "y_pred = np.array(zip(*model.predict_proba(X))[1])\ny_pred[:10]\n", "intent": "Note that each prediction gives a probability for each class: not setosa, yes setosa. We want the latter.\n"}
{"snippet": "print(\"Mean absolute error (MAE):\", metrics.mean_absolute_error(y_test,predictions))\nprint(\"Mean square error (MSE):\", metrics.mean_squared_error(y_test,predictions))\nprint(\"Root mean square error (RMSE):\", np.sqrt(metrics.mean_squared_error(y_test,predictions)))\n", "intent": "**Regression evaluation metrices**\n"}
{"snippet": "print(classification_report(y_test,predictions))\n", "intent": "**As expected, the classification report card is bad**\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "**Then you can re-run predictions on this grid object just like you would with a normal model**\n"}
{"snippet": "print(classification_report(y_test,grid_predictions))\n", "intent": "**Classification report shows improved F1-score**\n"}
{"snippet": "print metrics.roc_auc_score(y_test, y_pred_prob)\n", "intent": "AUC is the **percentage** of the ROC plot that is **underneath the curve**:\n"}
{"snippet": "print(\"Mean squared error: %.2f\"\n      % mean_squared_error(Y_test, pred_test))\n", "intent": "Now we will get the mean square error\n"}
{"snippet": "class_predict = log_model2.predict(X_test)\n", "intent": "Now we can use predict to predict classification labels for the next test set, then we will reevaluate our accuracy score!\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(messages['label'], all_predictions))\n", "intent": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png\" width=\"400\">\n"}
{"snippet": "Xfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)\n", "intent": "- (Like before, we need to reshape these *x* values into a ``[n_samples, n_features]`` features matrix.)\n"}
{"snippet": "print roc_auc_score(y, y_pred_proba)  \n", "intent": "Let's first look at this without cross-validation.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(ytest, y_model)\n", "intent": "- How's the accuracy?\n"}
{"snippet": "rng = np.random.RandomState(0)\nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\nynew = model.predict(Xnew)\n", "intent": "- Generate some new data and predict the label:\n"}
{"snippet": "from sklearn import metrics\nprint(metrics.classification_report(ypred, ytest))\n", "intent": "- View the classification report and confusion matrix for this classifier.\n"}
{"snippet": "from sklearn.metrics import accuracy_score\naccuracy_score(digits.target, labels)\n", "intent": "- Check how accurate our unsupervised clustering was in finding similar digits within the data:\n"}
{"snippet": "N_EPOCHS = 5\nfor epoch in range(N_EPOCHS):\n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n", "intent": "We then train the model through multiple epochs, an epoch being a complete pass through all examples in the split.\n"}
{"snippet": "N_EPOCHS = 5\nfor epoch in range(N_EPOCHS):\n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n", "intent": "Finally, we train our model...\n"}
{"snippet": "test_loss, test_acc = evaluate(model, test_iterator, criterion)\nprint(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')\n", "intent": "...and get our new and vastly improved test accuracy!\n"}
{"snippet": "test_loss, test_acc = evaluate(model, test_iterator, criterion)\nprint(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')\n", "intent": "...and get our best test accuracy yet! \n"}
{"snippet": "print(classification_report(y_test,rfc_pred))\nprint(confusion_matrix(y_test,rfc_pred))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "print \"Classification report for classifier\", model.__class__.__name__, \"\\n\"\nprint metrics.classification_report(expected, predicted), \"\\n\"\nprint \"Confusion matrix:\" \nprint metrics.confusion_matrix(expected, predicted)\n", "intent": "We could use `sklearn`'s `classification_report` and `confusion_matrix` to quickly view a bunch of performance metrics.\n"}
{"snippet": "print(classification_report(y_test,y_pred))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "y_pred = regressor.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "def classify_tweets(tfidf, classifier, unlabeled_tweets):\n    processed_tweets = process_all(unlabeled_tweets)\n    stripped_text = processed_tweets[\"text\"].apply(strip, rare_words=rare_words)\n    X = tfidf.transform(stripped_text)\n    pred = classifier.predict(X)\n    return pred\nclassifier = learn_classifier(X, y, 'best')\n", "intent": "We're almost there! It's time to write a nice little wrapper function that will use our model to classify unlabeled tweets from tweets_test.csv file.\n"}
{"snippet": "resnet_predictions = [np.argmax(resnet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet]\ntest_accuracy = 100*np.sum(np.array(resnet_predictions)==np.argmax(test_targets, axis=1))/len(resnet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "vgg_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_res50]\ntest_accuracy = 100*np.sum(np.array(vgg_predictions)==np.argmax(test_targets, axis=1))/len(vgg_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = dct.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "print(classification_report(pred,y_test))\n", "intent": "**Now create a classification report from the results. Do you get anything strange or some sort of warning?**\n"}
{"snippet": "MSE = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nMSE\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "np.unique(model.predict(D), return_counts=True)\n", "intent": "That looks quite promising.\nUnfortunately, the model has put a lot of tags into the same cluster again:\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "new_bad_review = \"This movie really sucked. I can't believe how long it dragged on. The actors are absolutely terrible.\"\nfeatures = tfidf.transform([new_bad_review])\nmodel.predict(features)\n", "intent": "We can also use our model to classify new reviews, all we have to do is extract the tfidf features from the raw text and send them to the model:\n"}
{"snippet": "new_bad_review = \"This movie really sucked. I can't believe how long it dragged on. The actors are absolutely talentless.\"\nfeatures = tfidf.transform([new_review])\nmodel.predict(features)\n", "intent": "We can also use our model to classify new reviews, all we have to do is extract the tfidf features from the raw text and send them to the model:\n"}
{"snippet": "svc.predict([[3, 1], [2,4]])\n", "intent": "Once we have learned the boundary then we can predict the label of novel samples\n"}
{"snippet": "pipeline.predict([\"10 things you need to do...\"])\n", "intent": "What? That was it?! \nThat's right. You've just built a machine learning classifier for clickbait and it was that easy to train. Let's test it out:\n"}
{"snippet": "predicted_labels = cross_val_predict(pipeline, training.full_content, training.label)\npipeline_performance(training.label, predicted_labels)\n", "intent": "We had pretty high accuracy but can we do better?\nRather than using just the title, we could use the title and description together.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "from sklearn.metrics import mean_squared_error, mean_absolute_error\nprint \"RMSE:\", np.sqrt(mean_squared_error(ys, predictions))\nprint \"MAE:\", mean_absolute_error(ys, predictions)\n", "intent": "First do the calculation by hand to see how large each term is\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n"}
{"snippet": "preds_base = logregcv.predict_proba(X_train_base)[:,1]\nfpr1, tpr1, thresholds = roc_curve(y_train_base, preds_base)\nroc_auc1 = auc(fpr1, tpr1)\nprint(\"AUC of the base model:\", roc_auc1)\n", "intent": "- Check AUC of the base model:\n"}
{"snippet": "y_pred = model.predict(X_test_pca)\nprint \"Overall prediction accuracy:\", model.score(X_test_pca, y_test)\nprint \nprint classification_report(y_test, y_pred, target_names=target_names)\nprint confusion_matrix(y_test, y_pred, labels=range(n_classes))\n", "intent": "Let's evaluate this 'best' model.\n"}
{"snippet": "train_y_hat = model.predict(train_X)\nprint np.sqrt(metrics.mean_squared_error(train_y, train_y_hat))\ntest_y_hat = model.predict(test_X)\nprint np.sqrt(metrics.mean_squared_error(test_y, test_y_hat))\n", "intent": "(Check http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html as needed)\n"}
{"snippet": "score = model.evaluate(x_test, y_test, verbose=0)\nscore2 = model2.evaluate(x_test, y_test, verbose=0)\nscore3 = model3.evaluate(x_test, y_test, verbose=0)\nscore4 = model4.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: \", score[1])\nprint(\"Accuracy2: \", score2[1])\nprint(\"Accuracy3: \", score3[1])\nprint(\"Accuracy4: \", score4[1])\n", "intent": "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?\n"}
{"snippet": "Resnet50_predictions=[np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "lr2.predict_proba(X_test_std[0,:].reshape(1, -1))\n", "intent": "Predict the probability of a pattern in the test set.\n"}
{"snippet": "(lr2.predict_proba(X_test_std[0,:].reshape(1, -1)),\nlr3.predict_proba(X_test_std[0,:].reshape(1, -1)))\n", "intent": "Predict the probability of a pattern in the test set.\n"}
{"snippet": "resnet50_model_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_model_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_model_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = linear_model.predict( X_test)\npredictions\n", "intent": "Calculate the accuracy score for the above model.\n"}
{"snippet": "y_pred = dnn_clf_bn.predict(X_test1)\naccuracy_score(y_test1, y_pred)\n", "intent": "The best params are reached during epoch 2, that's much faster than earlier. Let's check the accuracy:\n"}
{"snippet": "y_pred = dnn_clf.predict(X_train1)\naccuracy_score(y_train1, y_pred)\n", "intent": "Since batch normalization did not help, let's go back to the best model we trained earlier and see how it performs on the training set:\n"}
{"snippet": "uniques, idx = np.unique(data.beer_beerid, return_index=True)  \npred = pd.Series(model.predict(X[idx, :]), index=data.beer_beerid[idx], name=\"predictions\") \\\n    .sort(ascending=False, inplace=False)\npred_name = pd.Series(pred.values, beer_names[pred.index], name=\"predictions\")\nprint \"Top recommendations for %s.\" % user\nprint pred_name[:5]\n", "intent": "Pretty bad cross-validation scores, but mind you we have only a handful reviews.\n"}
{"snippet": "InceptionV3_predictions = [np.argmax(InceptionV3_model.predict(np.expand_dims(feature, axis=0))) \n                           for feature in test_InceptionV3]\ntest_accuracy = 100*np.sum(np.array(InceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(InceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "inceptionV3_predictions = [np.argmax(inceptionV3_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inceptionV3]\ntest_accuracy = 100*np.sum(np.array(inceptionV3_predictions)==np.argmax(test_targets, axis=1))/len(inceptionV3_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print(metrics.classification_report(y_test,predictions))\n", "intent": "<font color=green>The total number of confusions dropped from **287** to **256**. [241+46=287, 246+10=256]</font>\n"}
{"snippet": "print(metrics.classification_report(y_test,predictions))\n", "intent": "<font color=green>The total number of confusions dropped even further to **209**.</font>\n"}
{"snippet": "this_voxel = 10\nkp,ai = best_model[this_voxel]\na = alphas[ai]\nfm = preproc.construct_feature_matrix(np.atleast_2d(wedge_features[:,kp]).T,hrf_length=L)\nyhat = model_grid.ix[kp,a].predict(fm)\n", "intent": "Check out predictions for one of the voxels -- for both the training and validation data.\n"}
{"snippet": "X_new = [[0], [10]]\nlr.predict(X_new)\n", "intent": "- Returns a NumPy array, and we keep track of what the numbers \"mean\".\n- Can predict for multiple observations at once.\n"}
{"snippet": "def evaluate(model, test_x, test_y):\n    y_prob = model.predict(test_x, verbose=0)\n    y_pred = np.argmax(y_prob, axis=-1)\n    y_true = np.argmax(test_y, 1)\n    score, accuracy = model.evaluate(test_x, test_y, batch_size=32)\n    print(\"\\nAccuracy = {:.4f}\".format(accuracy))\n    print(\"\\nError Rate = {:.4f}\".format(1. - accuracy))\n    return accuracy\n", "intent": "This method defines a few evaluation metrics that will be used to evaluate the performance of a trained model.\n"}
{"snippet": "y_probs_logit = clf.predict_proba(X_test)\ny_probs_logit_left = y_probs_logit[:,1]\n", "intent": "Guardemos las probabilidades de ambas clases en un array y $p(y=1)$ en otro:\n"}
{"snippet": "hide_code\nmodel.load_weights('weights.best.model.hdf5')\nscore = model.evaluate(x_test, y_test)\nscore\n", "intent": "We should have an accuracy greater than 10%. Let's try to reach the level 60-70%.\n"}
{"snippet": "beer = np.random.choice(beer_names.index)\nbeer_idx = (data.beer_beerid == beer).values\nX_beer = X[beer_idx, :][0]  \nprint \"%s will give beer %s a rating of %.1f\" % (user, beer_names[beer], model.predict(X_beer)[0])\n", "intent": "Pretty spectacular. \nJust for the fun of it, how will this user rate a random beer?\n"}
{"snippet": "hide_code\nmodel.load_weights('weights.best.model.hdf5')\nscore = model.evaluate(x_test, y_test)\nscore\n", "intent": "We should have an accuracy greater than 3%\n"}
{"snippet": "hide_code\ngray_model.load_weights('weights.best.gray_model.hdf5')\ngray_score = gray_model.evaluate(x_test2, y_test2)\ngray_score\n", "intent": "Try to reach an accuracy greater than 50%\n"}
{"snippet": "hide_code\nmulti_model.load_weights('weights.best.multi.hdf5')\nmulti_scores = multi_model.evaluate(x_test3, y_test3_list, verbose=0)\nprint(\"Scores: \\n\" , (multi_scores))\nprint(\"First label. Accuracy: %.2f%%\" % (multi_scores[3]*100))\nprint(\"Second label. Accuracy: %.2f%%\" % (multi_scores[4]*100))\n", "intent": "We should have an accuracy greater than 3% for the first target (letter) and greater than 50% for the second target (background).\n"}
{"snippet": "hide_code\ngray_multi_model.load_weights('weights.best.gray_multi.hdf5')\ngray_multi_scores = gray_multi_model.evaluate(x_test4, y_test4_list, verbose=0)\nprint(\"Scores: \\n\" , (gray_multi_scores))\nprint(\"First label. Accuracy: %.2f%%\" % (gray_multi_scores[3]*100))\nprint(\"Second label. Accuracy: %.2f%%\" % (gray_multi_scores[4]*100))\n", "intent": "We should have an accuracy greater than 3% for the first target (letter) and greater than 50% for the second target (background).\n"}
{"snippet": "hide_code\ngray_model.load_weights('weights.best.gray_model.hdf5')\ngray_score = gray_model.evaluate(x_test2, y_test2)\ngray_score\n", "intent": "Try to reach an accuracy greater than 80%\n"}
{"snippet": "hide_code\nmulti_model.load_weights('weights.best.multi.hdf5')\nmulti_scores = multi_model.evaluate(x_test3, y_test3_list, verbose=0)\nprint(\"Scores: \\n\" , (multi_scores))\nprint(\"First label. Accuracy: %.2f%%\" % (multi_scores[3]*100))\nprint(\"Second label. Accuracy: %.2f%%\" % (multi_scores[4]*100))\n", "intent": "We should have an accuracy greater than 3% for the first target (letter) and greater than 25% for the second target (background).\n"}
{"snippet": "hide_code\ngray_multi_model.load_weights('weights.best.gray_multi.hdf5')\ngray_multi_scores = gray_multi_model.evaluate(x_test4, y_test4_list, verbose=0)\nprint(\"Scores: \\n\" , (gray_multi_scores))\nprint(\"First label. Accuracy: %.2f%%\" % (gray_multi_scores[3]*100))\nprint(\"Second label. Accuracy: %.2f%%\" % (gray_multi_scores[4]*100))\n", "intent": "We should have an accuracy greater than 3% for the first target (letter) and greater than 25% for the second target (background).\n"}
{"snippet": "print (metrics.classification_report(yTest, predRF))\nprint (\"Overall Accuracy:\", round(metrics.accuracy_score(yTest, predRF),2))\n", "intent": "Use the `classification_report` from the `metrics` module and the `accuracy_score` to see how well the classifier is doing\n"}
{"snippet": "from lib import gfx  \nfrom sklearn import metrics\nfrom math import sqrt\nrfr_fare = np.expm1(rfr_model.predict(X_test)) \ny_pred = rfr_fare\nrmse_score = sqrt(metrics.mean_squared_error(y_test, y_pred))\nprint('\\n\ngfx.plot_actual_vs_predicted(y, y_test, y_pred)\n", "intent": "Evaluates the model on the holdout set:\n"}
{"snippet": "scores = cross_val_score(linreg, X, y, cv=10, scoring='mean_squared_error')\n", "intent": "Use 10-fold cross-validation to calculate the RMSE for the linear regression model.\n"}
{"snippet": "Resnet50_predictions = [np.argmax(transfer_model.predict(np.expand_dims(feature, axis = 0))) for feature in test_Resnet50]\ntest_accuracy = 100 * np.sum(np.array(Resnet50_predictions) == np.argmax(test_targets, axis = 1)) / len(Resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "print classification_report(y_train,rfc.predict(X_train))\n", "intent": "Check the performance in trainging:\n"}
{"snippet": "DogResnet50_predictions = [np.argmax(DogResnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_DogResnet50]\ntest_accuracy = 100*np.sum(np.array(DogResnet50_predictions)==np.argmax(test_targets, axis=1))/len(DogResnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_net]\ntest_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "roc_auc_score(df['admit'], lm.predict(df[features]))\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "metrics.silhouette_score(km_X, labels, metric='euclidean')\n", "intent": "Let's plot the SSE and SC to see where our optimum k lies.\n"}
{"snippet": "Yhat = np.squeeze(np.array([model.predict(x.reshape(1, *x.shape)) for x in X]))\n", "intent": "Predict and compare to truth:\n"}
{"snippet": "W1 = fully_connected(nfeatures, nhidden) \nW2 = fully_connected(nhidden, ncats)\nWs = [W1, W2]\nacc = accuracy(predict(Ws, X_test), Y_test)\nprint(\"Accuracy ({}): {:.2f}\".format(0, acc))\ntrainer = Trainer(X_train, Y_train, AdamOptimizer())\nwhile trainer.epochs < 10:\n    trainer.train(Ws)\nacc = accuracy(predict(Ws, X_test), Y_test)\nprint(\"Accuracy ({}): {}\".format(trainer.epochs, acc))\n", "intent": "Let's try with the Adam optimizer, which usually doesn't require us to change its parameters:\n"}
{"snippet": "y_hat = knn.predict(X_test)\nprint(y_hat)\nprint(y_test)\nprint('Accuracy:', (y_hat == y_test).mean())\n", "intent": "Predict the labels (Iris species) for the test data and compare with the real labels:\n"}
{"snippet": "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\nknn.predict(X_new)\n", "intent": "- Returns a NumPy array, and we keep track of what the numbers \"mean\"\n- Can predict for multiple observations at once\n"}
{"snippet": "y_pred = model.predict(X_val, batch_size=500)\n", "intent": "Stability is reach quite fast. we can now evaluate it on the validation set.\n"}
{"snippet": "accuracy_score(y_val, y_ohe)\n", "intent": "Strange for a prediction of 94%. Let's look at the accuracy on validation set manually\n"}
{"snippet": "y_ohe2 = (y_pred>0.32).astype(int)\naccuracy_score(y_val, y_ohe2)\n", "intent": "Only 28% ... still low but we should keep in mind that we may predict multiclass so we should consider as true all class for example above 0.3\n"}
{"snippet": "y_pred = model.predict(X_test)\n", "intent": "Now let's predict performances for the test set. We will be able to compare it to real perfs.\n"}
{"snippet": "clf.predict_proba(iris.data[:1, :])\n", "intent": "Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf:\n"}
{"snippet": "dnn_model = tf.estimator.DNNClassifier(hidden_units=[10,20,10,10], feature_columns=feat_cols, n_classes=2)\ndnn_model.train(input_fn=input_func, steps=1000)\ndnn_model.evaluate(eval_input_func)\n", "intent": "76% means a little improvement... let's use more layers\n"}
{"snippet": "from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions));\n", "intent": "** Create a classification report for the model.**\n"}
{"snippet": "item_prediction = predict(train_data_matrix, item_similarity, flag='item')\nuser_prediction = predict(train_data_matrix, user_similarity, flag='user')\n", "intent": "And now we apply the predict formula in each case\n"}
{"snippet": "grid_predictions = grid.predict(X_test)\n", "intent": "Now, let's rerun everything\n"}
{"snippet": "roc_auc_score(df['admit'], predicted_classes)\n", "intent": "Finally, you can use the `roc_auc_score` function to calculate the area under these curves (AUC).\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nscores = cross_val_score(treereg, X, y, cv=14, scoring='mean_squared_error')\nnp.mean(np.sqrt(-scores))\n", "intent": "min samples = ln(depth)/number of samples\n    - because the number of leaves is 2^d\n"}
{"snippet": "model.load_weights(file_path)\nprint('Predicting....')\ny_pred = model.predict(test_data, batch_size=1024,verbose=1)\n", "intent": "The model is overfit\n"}
{"snippet": "print(\"Final model:\")\nprint(w.get_value())\nprint(b.get_value())\nprint(\"target values for D:\")\nprint(test_set[1])\nprint(\"prediction on D:\")\nprint(predict(test_set[0]))\n", "intent": "Now estimate the number of errors made by the classifier on the training set itself.\n"}
{"snippet": "from sklearn import metrics\ny_pred = linreg.predict(X)\nmetrics.mean_squared_error(y, y_pred)\n", "intent": "Evaluate the model using the MSE\n"}
{"snippet": "y_pred_class = logreg.predict(X_test)\n", "intent": "Make predictions on the testing set and calculate the accuracy\n"}
{"snippet": "for i in range(n_B):\n    print(i, np.sqrt(mean_squared_error(y_pred[i], y_test)))\n", "intent": "Results of each tree\n"}
{"snippet": "errors = np.zeros(n_estimators)\nfor i in range(n_estimators):\n    y_pred_ = trees[i].predict(X_train.iloc[samples_oob[i]])\n    errors[i] = 1 - metrics.accuracy_score(y_train.iloc[samples_oob[i]], y_pred_)\n", "intent": "Estimate the oob error of each classifier\n"}
{"snippet": "ResNet50_predictions = [np.argmax(my_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet50]\ntest_accuracy = 100*np.sum(np.array(ResNet50_predictions)==np.argmax(test_targets, axis=1))/len(ResNet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "train_predict = model.predict(X_train)\ntest_predict = model.predict(X_test)\nvalid_predict = model.predict(X_valid)\n", "intent": "With your model fit we can now make predictions on both our training and testing sets.\n"}
{"snippet": "scores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=5)\nprint scores\nprint scores.mean()\n", "intent": "K-Fold cross validation is just as easy; let's use a K of 5:\n"}
{"snippet": "testPre = text_clf.predict(clf_test.Proposal)\n", "intent": "Let's use our trained model to predict values in the test set\n"}
{"snippet": "custom_predictions = [np.argmax(custom_model.predict(np.expand_dims(feature, axis=0))) for feature in test_custom]\ntest_accuracy = 100*np.sum(np.array(custom_predictions)==np.argmax(test_targets, axis=1))/len(custom_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "train_set = in_dta[:25]\nval_set = in_dta[25:]\nval_test_error(train_set, val_set, test_set)\n", "intent": "Split in_dta into a training set (first 25 examples) and validation set (last 10 examples) and evaluate validation and test errors\n"}
{"snippet": "train_set = in_dta[25:]\nval_set = in_dta[:25]\nval_test_error(train_set, val_set, test_set)\n", "intent": "Split in_dta into a training set (first 10 examples) and validation set (last 25 examples) and evaluate validation and test errors\n"}
{"snippet": "def disagreement_probability(weights, line, points):\n        x = Line.transform_inputs(points)\n        y = line.evaluate(points)\n        g = np.array([np.sign(np.dot(weights, xn)) for xn in x])\n        return 100. * np.sum(y != g) / len(y)\n", "intent": "Calculate disagreement probability\n"}
{"snippet": "import numpy as np\nprint (np.sum((bos.PRICE - lm.predict(X)) ** 2))\n", "intent": "Let's calculate the residual sum of squares \n$$ S = \\sum_{i=1}^N r_i = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\n"}
{"snippet": "mse= np.mean((bos.PRICE -lm.predict(X))**2)\nprint(mse)\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "ResNet_predictions = [np.argmax(ResNet_model.predict(np.expand_dims(feature, axis=0))) for feature in test_ResNet]\ntest_accuracy = 100*np.sum(np.array(ResNet_predictions)==np.argmax(test_targets, axis=1))/len(ResNet_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "a = metrics.precision_score(y_test, y_svc, pos_label='four')\nb = metrics.recall_score(y_test, y_svc, pos_label='four')\n", "intent": "$2ab \\over {a + b}$\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(y, p4(x))\nprint r2\n", "intent": "Looks pretty good! Let's measure the r-squared error:\n"}
{"snippet": "pred = knn_1.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "pred= logreg.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "from sklearn.metrics import f1_score\nprint(f1_score(y_test, y_pred))\n", "intent": "* We can now compute the F-score using these predictions\n"}
{"snippet": "from sklearn.model_selection import cross_val_predict, cross_val_score\nfrom sklearn.metrics import classification_report, roc_curve, auc\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import label_binarize\ndef print_crossval_eval_measures(model, df, target):\n    for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n        scores = cross_val_score(model, df, target, cv=10, scoring=metric)\n        print('mean', metric, ':', scores.mean(), '\\nall scores :', scores, \"\\n\")\nprint_crossval_eval_measures(model1, X, y)\n", "intent": "Part 10 refers to performing cross validation on the whole dataset and comparing the resulting scores to the train test split method.\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nsilhouette_score(features[['mpg','disp']],km.labels_)\n", "intent": "Silhouette measures how similar an observation is to its own cluster compared to the closest cluster\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nsilhouette_score(adult,labels,metric='euclidean')\n", "intent": "Compute the Silhoutte Score to measure your analysis\n"}
{"snippet": "from sklearn.metrics import accuracy_score\nacc = accuracy_score(y, kmeans.labels_)\nprint(acc)\n", "intent": "Compute the accuracy score using scikit to see how accurate our analysis is\n"}
{"snippet": "from sklearn.metrics import silhouette_score\nsilhouette_score(X,kmeans.labels_)\n", "intent": "Compute the silhoutte coefficient to see how consistent our data is within the clusters\n"}
{"snippet": "from sklearn.metrics import classification_report\ncls_rep = classification_report(Y, kmeans.labels_)\nprint(cls_rep)\n", "intent": "Calculate the Precision, Recall, and F - Score to see the test's accuracy\n"}
{"snippet": "svc.predict([[200000, 40]])\n", "intent": "Or just use predict for a given point:\n"}
{"snippet": "from sklearn.metrics import accuracy_score,confusion_matrix\npredictions=nb.predict(X_test)\nprint(accuracy_score(predictions, y_test))\nprint(confusion_matrix(predictions, y_test))\n", "intent": "Score the Naive Bayes classifier on the test data\n"}
{"snippet": "print(accuracy_score(predictions, y_test))\nprint(confusion_matrix(predictions, y_test))\n", "intent": "Produce the accuracy score of the logistic regression from the test set\n"}
{"snippet": "predicttions = logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "bank_predictions = list(classifier.predict(X_test))\n", "intent": "** Use the predict method from the classifier model to create predictions from X_test **\n"}
{"snippet": "model_class.evaluate(input_func,steps=1000)\ninput_pred_func=tf.estimator.inputs.pandas_input_fn(x=X_test,shuffle=False,num_epochs=1,batch_size=8)\n", "intent": "** Create a prediction input function. Remember to only supprt X_test data and keep shuffle=False. **\n"}
{"snippet": "prediction=log_model.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "prediction=model.predict(X_test)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "prediction =model1.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "pred=model.predict(X_test)\n", "intent": "**Use the predict method to predict values using your KNN model and X_test.**\n"}
{"snippet": "from sklearn.metrics import r2_score\nr2 = r2_score(np.array(trainY), p4(np.array(trainX)))\nprint r2\n", "intent": "...even though it fits the training data better:\n"}
{"snippet": "dem_probs = clf.predict(X_pred)\nprint((dem_probs/100) * 538)\n", "intent": "Calculate the number of electoral college votes the Democratic candidate will given the data on each date in the prediction file. \n"}
{"snippet": "pred=rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "predict=logmodel.predict(X_test)\n", "intent": "** Now predict values for the testing data.**\n"}
{"snippet": "pred=nb.predict(X_test)\n", "intent": "Time to see how our model did!\n**Use the predict method off of nb to predict labels from X_test.**\n"}
{"snippet": "pred=pipeline.predict(X_test)\n", "intent": "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**\n"}
{"snippet": "pred=model.predict(X_test)\n", "intent": "**Now get predictions from the model and create a confusion matrix and a classification report.**\n"}
{"snippet": "train_preds = meta_lr.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2, train_preds)\ntest_preds = meta_lr.predict(X_test_level2)\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint(\"Train RMSE for stacking is \", sqrt(mean_squared_error(y_train_level2, train_preds)))\n", "intent": "Compute the R-squared and RMSE for X_train_level2 and compute the final predictions on X_test_level2\n"}
{"snippet": "y_pred = linreg.predict(X_test)\n", "intent": "y = 0.0465645678742*TV + 0.179158122451*Radio + 0.00345046471118*Newspaper + 2.87696662232\n"}
{"snippet": "y_train_pred = model.predict(X_train)\ny_train_pred.shape\n", "intent": "Next we need to generate some predictions on the training data and calculate the error for each instance.\n"}
{"snippet": "resnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]\ntest_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%.\n"}
{"snippet": "result = [np.round(anomaly_score(x, dist), 4) for x in val_errors]\nprint(result)\n", "intent": "The higher the number, the more likely the sample is to be an anomaly.\n"}
{"snippet": "print(test_VGG16[0].shape)\nprint(np.expand_dims(test_VGG16[0], axis=0).shape)  \nVGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)\n", "intent": "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below.\n"}
{"snippet": "def get_next_char(inp):\n    idxs = [char_idx[c] for c in inp]\n    arrs = [np.array(i)[np.newaxis] for i in idxs]\n    p = model.predict(arrs)\n    i = np.random.choice(range(vocab_size), p=p.ravel())\n    return chars[i]\n", "intent": "Helper function to get fourth predicted character given three\n"}
{"snippet": "def predict_single_batch(x, max=True):\n    x = np.tile(x, batch_size).reshape((batch_size, in_len))\n    if max:\n        return model.predict(x, batch_size=batch_size)[0, in_len-1].argmax()\n    else:\n        return np.random.choice(range(vocab_size), p=model.predict(x, batch_size=batch_size)[0,in_len-1])\n", "intent": "Helper method to predict on single batch for stateful networks\n"}
{"snippet": "preds = model.predict(test_features, batch_size=64, verbose=1)\n", "intent": "Get the predictions\n"}
{"snippet": "predict2 = rfc.predict(X_test)\n", "intent": "Let's predict off the y_test values and evaluate our model.\n** Predict the class of not.fully.paid for the X_test data.**\n"}
{"snippet": "from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(df['Cluster'],kM.labels_))\nprint(classification_report(df['Cluster'],kM.labels_))\n", "intent": "** Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels.**\n"}
{"snippet": "mseTotal = np.mean((bos.PRICE - lm.predict(X)) ** 2)\nprint mseTotal\n", "intent": "***\nThis is simple the mean of the residual sum of squares.\n**Your turn:** Calculate the mean squared error and print it.\n"}
{"snippet": "pred_probs = logr.predict_proba(X)[:,1]\n", "intent": "The first column indicates the predicted probability of class 0, and the second column indicates the predicted probability of class 1.\n"}
{"snippet": "train_features = train_data[:, 1:]\ntrain_target = train_data[:, 0]\nclf = clf.fit(train_features, train_target)\nscore = clf.score(train_features, train_target)\n\"Mean accuracy of Random Forest: {0}\".format(score)\n", "intent": "Fit the training data and create the decision trees:\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X, y)\n", "intent": "Let's fit a K-neighbors classifier\n"}
{"snippet": "for n_neighbors in [1, 5, 10, 20, 30]:\n    knn = KNeighborsClassifier(n_neighbors)\n    knn.fit(X_train, y_train)\n    print(n_neighbors, knn.score(X_test, y_test))\n", "intent": "Using this, we can ask how this changes as we change the model parameters, in this case the number of neighbors:\n"}
{"snippet": "from keras import models\nfrom keras import layers\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n", "intent": "And here's the Keras implementation, very similar to the MNIST example you saw previously:\n"}
{"snippet": "history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\n", "intent": "Now let's train our network for 20 epochs:\n"}
{"snippet": "dtree = DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "dpt_model = models.Sequential()\ndpt_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\ndpt_model.add(layers.Dropout(0.5))\ndpt_model.add(layers.Dense(16, activation='relu'))\ndpt_model.add(layers.Dropout(0.5))\ndpt_model.add(layers.Dense(1, activation='sigmoid'))\ndpt_model.compile(optimizer='rmsprop',\n                  loss='binary_crossentropy',\n                  metrics=['acc'])\n", "intent": "Let's add two `Dropout` layers in our IMDB network to see how well they do at reducing overfitting:\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n", "intent": "We will be using the same model architecture as before:\n"}
{"snippet": "model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(x_train, y_train,\n                    epochs=10,\n                    batch_size=32,\n                    validation_data=(x_val, y_val))\nmodel.save_weights('pre_trained_glove_model.h5')\n", "intent": "Let's compile our model and train it:\n"}
{"snippet": "from keras.layers import Dense\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n                    epochs=10,\n                    batch_size=128,\n                    validation_split=0.2)\n", "intent": "Let's train a simple recurrent network using an `Embedding` layer and a `SimpleRNN` layer:\n"}
{"snippet": "optimizer = keras.optimizers.RMSprop(lr=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\n", "intent": "Since our targets are one-hot encoded, we will use `categorical_crossentropy` as the loss to train the model:\n"}
{"snippet": "layer_dict = dict([(layer.name, layer) for layer in model.layers])\nloss = K.variable(0.)\nfor layer_name in layer_contributions:\n    coeff = layer_contributions[layer_name]\n    activation = layer_dict[layer_name].output\n    scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n    loss += coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling\n", "intent": "Now let's define a tensor that contains our loss, i.e. the weighted sum of the L2 norm of the activations of the layers listed above.\n"}
{"snippet": "preparation_and_feature_selection_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, k))\n])\n", "intent": "Looking good... Now let's create a new pipeline that runs the previously defined preparation pipeline, and adds top k feature selection:\n"}
{"snippet": "prepare_select_and_predict_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n    ('svm_reg', SVR(**rnd_search.best_params_))\n])\n", "intent": "Question: Try creating a single pipeline that does the full data preparation plus the final prediction.\n"}
{"snippet": "param_grid = [\n        {'preparation__num_pipeline__imputer__strategy': ['mean', 'median', 'most_frequent'],\n         'feature_selection__k': list(range(1, len(feature_importances) + 1))}\n]\ngrid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n                                scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\ngrid_search_prep.fit(housing, housing_labels)\n", "intent": "Question: Automatically explore some preparation options using `GridSearchCV`.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)\n", "intent": "Now let's compare the decision tree model to a random forest.\n"}
{"snippet": "logits = X_valid.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\naccuracy_score = np.mean(y_predict == y_valid)\naccuracy_score\n", "intent": "Let's make predictions for the validation set and check the accuracy score:\n"}
{"snippet": "logits = X_valid.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\naccuracy_score = np.mean(y_predict == y_valid)\naccuracy_score\n", "intent": "Because of the additional $\\ell_2$ penalty, the loss seems greater than earlier, but perhaps this model will perform better? Let's find out:\n"}
{"snippet": "logits = X_test.dot(Theta)\nY_proba = softmax(logits)\ny_predict = np.argmax(Y_proba, axis=1)\naccuracy_score = np.mean(y_predict == y_test)\naccuracy_score\n", "intent": "And now let's measure the final model's accuracy on the test set:\n"}
{"snippet": "from sklearn.svm import LinearSVR\nlin_svr = LinearSVR(random_state=42)\nlin_svr.fit(X_train_scaled, y_train)\n", "intent": "Let's train a simple `LinearSVR` first:\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(random_state=42)\n", "intent": "*Exercise: Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set.*\n"}
{"snippet": "rnd_clf2 = RandomForestClassifier(random_state=42)\nt0 = time.time()\nrnd_clf2.fit(X_train_reduced, y_train)\nt1 = time.time()\n", "intent": "*Exercise: Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster?*\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing.data, housing.target.reshape(-1, 1))\nprint(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])\n", "intent": "Compare with Scikit-Learn\n"}
{"snippet": "reset_graph()\nn_epochs = 1000\nlearning_rate = 0.01\nX = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\ny = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\ntheta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\ny_pred = tf.matmul(X, theta, name=\"predictions\")\nerror = y_pred - y\nmse = tf.reduce_mean(tf.square(error), name=\"mse\")\n", "intent": "Same as above except for the `gradients = ...` line:\n"}
{"snippet": "with tf.Session() as sess:\n    init.run()\n    print(z.eval())\n    print(sess.run(grads))\n", "intent": "Let's compute the function at $a=0.2$ and $b=0.3$, and the partial derivatives at that point with regards to $a$ and with regards to $b$:\n"}
{"snippet": "kmeans = KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "reset_graph()\n", "intent": "First let's reset the default graph.\n"}
{"snippet": "y_proba = tf.sigmoid(logits)\n", "intent": "In fact, TensorFlow has a nice function `tf.sigmoid()` that we can use to simplify the last line of the previous code:\n"}
{"snippet": "reset_graph()\n", "intent": "Ok, next let's reset the default graph:\n"}
{"snippet": "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n", "intent": "Now we can create the `FileWriter` that we will use to write the TensorBoard logs:\n"}
{"snippet": "reset_graph()\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n", "intent": "Implementing Leaky ReLU in TensorFlow:\n"}
{"snippet": "reset_graph()\nn_inputs = 28 * 28  \nn_hidden1 = 300\nn_hidden2 = 100\nn_outputs = 10\n", "intent": "Let's train a neural network on MNIST using the Leaky ReLU. First let's create the graph:\n"}
{"snippet": "reset_graph()\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n", "intent": "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:\n"}
{"snippet": "def selu(z,\n         scale=1.0507009873554804934193349852946,\n         alpha=1.6732632423543772848170429916717):\n    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))\n", "intent": "Here's a TensorFlow implementation (there will almost certainly be a `tf.nn.selu()` function in future TensorFlow versions):\n"}
{"snippet": "threshold = 1.0\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ngrads_and_vars = optimizer.compute_gradients(loss)\ncapped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n              for grad, var in grads_and_vars]\ntraining_op = optimizer.apply_gradients(capped_gvs)\n", "intent": "Now we apply gradient clipping. For this, we need to get the gradients, use the `clip_by_value()` function to clip them, then apply them:\n"}
{"snippet": "kmeans.fit(df.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./my_model_final.ckpt\")\n", "intent": "Now you can start a session, restore the model's state and continue training on your data:\n"}
{"snippet": "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n", "intent": "Or we could use the graph's `get_tensor_by_name()` method:\n"}
{"snippet": "reset_graph()\nn_inputs = 28 * 28  \nn_hidden1 = 300\nn_outputs = 10\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\ny = tf.placeholder(tf.int64, shape=(None), name=\"y\")\nwith tf.name_scope(\"dnn\"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")\n", "intent": "Let's implement $\\ell_1$ regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity):\n"}
{"snippet": "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\nclipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\nclip_weights2 = tf.assign(weights2, clipped_weights2)\n", "intent": "We can do this as well for the second hidden layer:\n"}
{"snippet": "he_init = tf.contrib.layers.variance_scaling_initializer()\ndef dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None,\n        activation=tf.nn.elu, initializer=he_init):\n    with tf.variable_scope(name, \"dnn\"):\n        for layer in range(n_hidden_layers):\n            inputs = tf.layers.dense(inputs, n_neurons, activation=activation,\n                                     kernel_initializer=initializer,\n                                     name=\"hidden%d\" % (layer + 1))\n        return inputs\n", "intent": "We will need similar DNNs in the next exercises, so let's create a function to build this DNN:\n"}
{"snippet": "dnn_clf = DNNClassifier(random_state=42)\ndnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)\n", "intent": "Let's see if we get the exact same accuracy as earlier using this class (without dropout or batch norm):\n"}
{"snippet": "dnn_clf_bn = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n                           n_neurons=90, random_state=42,\n                           batch_norm_momentum=0.95)\ndnn_clf_bn.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)\n", "intent": "Good, now let's use the exact same model, but this time with batch normalization:\n"}
{"snippet": "hidden5_out = tf.get_default_graph().get_tensor_by_name(\"hidden5_out:0\")\n", "intent": "Let's start by getting a handle on the output of the last frozen layer:\n"}
{"snippet": "dnn_clf_5_to_9 = DNNClassifier(n_hidden_layers=4, random_state=42)\ndnn_clf_5_to_9.fit(X_train2, y_train2, n_epochs=1000, X_valid=X_valid2, y_valid=y_valid2)\n", "intent": "Let's compare that to a DNN trained from scratch:\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS',axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "reset_graph()\nX = tf.placeholder(shape=(None, height, width, channels), dtype=tf.float32)\nconv = tf.layers.conv2d(X, filters=2, kernel_size=7, strides=[2,2],\n                        padding=\"SAME\")\n", "intent": "Using `tf.layers.conv2d()`:\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, INCEPTION_V3_CHECKPOINT_PATH)\n", "intent": "Exercise: Open a session and use the `Saver` to restore the pretrained model checkpoint you downloaded earlier.\n"}
{"snippet": "n_outputs = len(flower_classes)\nwith tf.name_scope(\"new_output_layer\"):\n    flower_logits = tf.layers.dense(prelogits, n_outputs, name=\"flower_logits\")\n    Y_proba = tf.nn.softmax(flower_logits, name=\"Y_proba\")\n", "intent": "Then we can add the final fully connected layer on top of this layer:\n"}
{"snippet": "with tf.device(\"/gpu:0\"):  \n    layer1 = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\nwith tf.device(\"/gpu:1\"):  \n    layer2 = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n", "intent": "Do **NOT** do this:\n"}
{"snippet": "reset_graph()\nn_inputs = 28*28\nX = tf.placeholder(tf.float32, shape=[None, n_inputs])\nhidden1 = tf.nn.elu(tf.matmul(X, W1) + b1)\nhidden2 = tf.nn.elu(tf.matmul(hidden1, W2) + b2)\nhidden3 = tf.nn.elu(tf.matmul(hidden2, W3) + b3)\noutputs = tf.matmul(hidden3, W4) + b4\n", "intent": "Finally, we can create a Stacked Autoencoder by simply reusing the weights and biases from the Autoencoders we just trained:\n"}
{"snippet": "reset_graph()\nn_inputs = 28 * 28\nn_hidden1 = 300\nn_hidden2 = 150  \nn_hidden3 = n_hidden1\nn_outputs = n_inputs\nlearning_rate = 0.01\n", "intent": "Using Gaussian noise:\n"}
{"snippet": "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.sigmoid)\n", "intent": "Note that the coding layer must output values from 0 to 1, which is why we use the sigmoid activation function:\n"}
{"snippet": "logits = tf.layers.dense(hidden1, n_outputs)\noutputs = tf.nn.sigmoid(logits)\nxentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits)\nreconstruction_loss = tf.reduce_mean(xentropy)\n", "intent": "To speed up training, you can normalize the inputs between 0 and 1, and use the cross entropy instead of the MSE for the cost function:\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Let's reset the default graph, in case you re-run this notebook without restarting the kernel:\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "tweak_labels = np.tile(mnist.test.labels[:n_samples], caps2_n_dims * n_steps)\nwith tf.Session() as sess:\n    saver.restore(sess, checkpoint_path)\n    decoder_output_value = sess.run(\n            decoder_output,\n            feed_dict={caps2_output: tweaked_vectors_reshaped,\n                       mask_with_labels: True,\n                       y: tweak_labels})\n", "intent": "Now let's feed these tweaked output vectors to the decoder and get the reconstructions it produces:\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nx1_train = df_train.x1.values.reshape(-1, 1) \nprint('Coefficient (w1)', model.coef_)\nprint('Intercept (w0)', model.intercept_)\n", "intent": "We'll use [sklearn.linear_model.LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n"}
{"snippet": "from keras.models import load_model\nyolo_model = load_model(model_path, compile=False)\nyolo_model.summary()\n", "intent": "Finally, this is the model architecture, as seen by keras.\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dense\nsimple_model = Sequential()\nsimple_model.add(Dense(1, input_dim=4, activation='sigmoid')) \nsimple_model.summary()\n", "intent": "Finally, let's try something simpler.\nLet's create a 1-layer network that can do linear regression.\n"}
{"snippet": "deeper_model = Sequential()\ndeeper_model.add(Dense(256, input_dim=16, activation='relu'))\ndeeper_model.add(Dense(1, activation='sigmoid'))\ndeeper_model.summary()\n", "intent": "How about a 2-layer network to make it a deep neural network?\n"}
{"snippet": "from keras.layers import Flatten, Dense, Dropout\nclassifier = Sequential()\nclassifier.add(Flatten(input_shape=features_train.shape[1:])) \nclassifier.add(Dense(64, activation='relu'))\nclassifier.add(Dropout(0.5))\nclassifier.add(Dense(3, activation='softmax'))\nclassifier.summary()\n", "intent": "Note that this classifier does not have to be related to the architecture of the original network.\nIt simply treats the inputs as opaque features.\n"}
{"snippet": "from sklearn.svm import SVC\nmodel_svc = SVC(gamma='auto')\nscores = cross_validate(model_svc, Z_train, y_train, cv=5,\n                        return_train_score=True,\n                        return_estimator=True)\nscores\n", "intent": "- cross validation\n- learning curve\n"}
{"snippet": "from sklearn.svm import SVC\nmodel_svc = SVC(gamma=0.001, C=10)\nscores = cross_validate(model_svc, Z_train, y_train, cv=5,\n                        return_train_score=True,\n                        return_estimator=True)\nscores\n", "intent": "- cross validation\n- learning curve\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nscores_nb = cross_validate(GaussianNB(), Z_train, y_train, cv=5,\n                           return_train_score=False, return_estimator=True)\nprint('validation scores', scores_nb['test_score'])\n", "intent": "Naive Bayes uses the Bayes Theorem (multiplication if the probabilities of X) to compute the probability of y.\n"}
{"snippet": "knn.fit(X_train,y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(verbose=True, random_state=42)\nsgd_clf.fit(Z_train, y_train)\nscores = sgd_clf.score(Z_test, y_test)\nprint(scores) \n", "intent": "1. SGD + Logistic Regression\n2. SVM with some non-linear kernel\n3. Compare evaluation metrics\n"}
{"snippet": "regressor = linear_model.LinearRegression()\n", "intent": "We pick the `LinearRegression` model to do linear regression of the to-be-selected features and `bathrooms`\n"}
{"snippet": "lof =  LocalOutlierFactor(int(k),n_jobs=-1,p=13,contamination=ratio)\n", "intent": "Since this is a high dimention space, we are not able to visualize it.\n"}
{"snippet": "lof =  LocalOutlierFactor(int(k),n_jobs=-1,p=14,contamination=ratio)\n", "intent": "Since this is a high dimention space, we are not able to visualize it.\n"}
{"snippet": "    model = Sequential()\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Flatten(input_shape = (32,32,3)))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a [dropout](https://keras.io/layers/core/\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, SAVE_FILE)\n    for image_data in resized_images_data:\n        top_3_predictions = sess.run(tf.nn.top_k(softmax, k=3), feed_dict = {x: image_data, keep_prob : 1})\n        print(top_3_predictions)\n", "intent": "**Answer:**\nIt seems that my model is very certain (in fact 100% certain) about its classifications.\n"}
{"snippet": "from datetime import timedelta\nltg30_blobs = goesio.get_ltg_blob_paths(irdt + timedelta(minutes=30), timespan_minutes=15)\nltg30 = goesio.create_ltg_grid(ltg30_blobs, griddef, influence_km)\nmodel = LogisticRegression()\ndf = create_prediction_df(ref, ltg30, griddef)\nx = df.drop(['lat', 'lon', 'ltg'], axis=1)\nmodel = model.fit(x, df['ltg'])\nprint(model.coef_, model.intercept_)\nprint('Model accuracy={}%'.format(100*model.score(x, df['ltg'])))\n", "intent": "How about if we try to predict lightning 30 minutes into the future?\n"}
{"snippet": "a = tf.placeholder(dtype=tf.int32, shape=(None,))  \nb = tf.placeholder(dtype=tf.int32, shape=(None,))\nc = tf.add(a, b)\nwith tf.Session() as sess:\n  result = sess.run(c, feed_dict={\n      a: [3, 4, 5],\n      b: [-1, 2, 3]\n    })\n  print(result)\n", "intent": "<h2> Using a feed_dict </h2>\nSame graph, but without hardcoding inputs at build stage\n"}
{"snippet": "def compute_area(sides):\n  return list_of_areas\nwith tf.Session() as sess:\n  area = compute_area(tf.constant([\n      [5.0, 3.0, 7.1],\n      [2.3, 4.1, 4.8]\n    ]))\n  result = sess.run(area)\n  print(result)\n", "intent": "Extend your code to be able to compute the area for several triangles at once.\nYou should get: [6.278497 4.709139]\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "def custom_estimator(features, labels, mode, params):  \n  input_layer = tf.feature_column.input_layer(features, params['feature_columns'])\n  predictions = tf.layers.dense(input_layer,10,activation=tf.nn.relu)\n  predictions = tf.layers.dense(input_layer,1,activation=None)\n  .....REST AS BEFORE\n", "intent": "Modify the custom_estimator function to be a neural network with one hidden layer, instead of a linear regressor\n"}
{"snippet": "def linear_model(img):\n  X = tf.reshape(img,[-1,HEIGHT*WIDTH]) \n  W = tf.get_variable(\"W\", [HEIGHT*WIDTH,NCLASSES], \n                      initializer = tf.truncated_normal_initializer(stddev=0.1,seed = 1))\n  b = tf.get_variable(\"b\",NCLASSES, initializer = tf.zeros_initializer)\n  ylogits = tf.matmul(X,W)+b\n  return ylogits, NCLASSES\n", "intent": "Let's start with a very simple linear classifier. All our models will have this basic interface -- they will take an image and return logits.\n"}
{"snippet": "def linear_model(img):\n  return ylogits, NCLASSES\n", "intent": "Let's start with a very simple linear classifier. All our models will have this basic interface -- they will take an image and return logits.\n"}
{"snippet": "sess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run([init])\nsaver = tf.train.Saver(max_to_keep=1)\n", "intent": "This resets all neuron weights and biases to initial random values\n"}
{"snippet": "with tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  users_topfeats = {}\n  for ind in range(num_users):\n    top_feats = sess.run(find_user_top_feats(ind))\n    users_topfeats[users[ind]] = list(top_feats)\n", "intent": "We'll create a tensorflow session to compute these values.\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    user_topmovies = {}\n    num_to_recommend = tf.reduce_sum(tf.cast(tf.equal(users_movies, \n                                                      tf.zeros_like(users_movies)), dtype = tf.float32), axis = 1)\n    for ind in range(num_users):\n      top_movies = sess.run(find_user_top_movies(ind, tf.cast(num_to_recommend[ind], dtype = tf.int32)))\n      user_topmovies[users[ind]] = list(top_movies)\n", "intent": "As before, we create a tf.Session to compute the movie recommendations. \n"}
{"snippet": "def try_out():\n  with tf.Session() as sess:\n    fn = read_dataset(tf.estimator.ModeKeys.EVAL, \n                    {'input_path': 'data', 'batch_size': 4, 'nitems': 5668, 'nusers': 82802})\n    feats, _ = fn()\n    print feats['input_rows'].eval()\n    print feats['input_rows'].eval()\ntry_out()\n", "intent": "This code is helpful in developing the input function. You don't need it in production.\n"}
{"snippet": "with tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  users_topfeats = {}\n  for i in range(num_users):\n    top_feats = sess.run(find_user_top_feats(i))\n    users_topfeats[users[i]] = list(top_feats)\n", "intent": "We'll create a tensorflow session to compute these values.\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    user_topmovies = {}\n    num_to_recommend = tf.reduce_sum(tf.cast(tf.equal(users_movies, \n                                                      tf.zeros_like(users_movies)), dtype = tf.float32), axis = 1)\n    for ind in range(num_users):\n      top_movies = sess.run(find_user_top_movies(ind, tf.cast(num_to_recommend[ind], dtype = tf.int32)))\n      user_topmovies[users[ind]] = list(top_movies)         \n", "intent": "As before, we create a tf.Session to compute the movie recommendations. \n"}
{"snippet": "lm.fit(X_train,y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "happyModel.fit(x = X_train, y = Y_train, epochs = 10, batch_size = 20)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model = load_vgg_model(\"pretrained-model/imagenet-vgg-verydeep-19.mat\")\n", "intent": "Next, as explained in part (2), let's load the VGG16 model.\n"}
{"snippet": "inference_model = music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 50)\n", "intent": "Run the cell below to define your inference model. This model is hard coded to generate 50 values.\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nreg = RandomForestRegressor(min_samples_leaf=5)\nreg.fit(train[predictors], train[\"cnt\"])\n", "intent": "By taking the nonlinear predictors into account, the decision tree regressor appears to have much higher accuracy than linear regression.\n"}
{"snippet": "dt.fit(x_train, y_train)\n", "intent": "The scores show that the tree performs well in classification. Therefore, fitting can now be performed.\n"}
{"snippet": "dt = DecisionTreeClassifier(criterion='gini')\n", "intent": "A **Decision Tree** is applied next. **Random Forests** as they can lead to severe overfitting of the model.\n"}
{"snippet": "dt.fit(new_train, new_train_y)\n", "intent": "Accuracy of 1.0 achieved here. The tree is then fitted and explored a little more.\n"}
{"snippet": "from sklearn import linear_model\nmeta_model = linear_model.LinearRegression(n_jobs=10)\nmeta_model.fit(X_train_level2, y_train_level2)\n", "intent": "Fit a linear regression model to the meta-features. Use the same parameters as in the model above.\n"}
{"snippet": "lgb1=LGBMRegressor(n_jobs=8)\nlgb1.fit(X_train,y_train,\n        eval_set=(X_val,y_val),\n        early_stopping_rounds=1,\n        eval_metric=lambda y_t,y_p:('error',score(y_t,y_p),False),\n       )\n", "intent": "lightgbm has early_stopping mechanism by providing validation data\nI get validation score 0.9632\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nspam_detect_model = MultinomialNB().fit(messages_tfidf, messages['label'])\n", "intent": "We'll be using scikit-learn here, choosing the [Naive Bayes](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) classifier to start with:\n"}
{"snippet": "from keras.layers import Dense\nfrom keras.models import Sequential\ndef create_model():\n    model = Sequential()\n    model.add(Dense(6, input_dim=4, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    return model\n", "intent": "Now set up an actual MLP model using Keras:\n"}
{"snippet": "model = ols(\"totalPr ~ duration + nBids + cond + sellerRate + stockPhoto + wheels\", \n            data=mk_df_reduced).fit()\nprint(model.summary())\nprint(\"R-squared is %.4f\" % model.rsquared)\nprint(\"R-squared adjusted is %.4f\" % model.rsquared_adj)\n", "intent": "Now, let's make a regression model with all of these together\n"}
{"snippet": "feature_data_down_two = feature_data_down_one.drop(\"stockPhoto\", axis=1)\nbaseline_performance = 0.73957\nfor feature in feature_data_down_two.columns:\n    feature_data_temp = feature_data_down_two.drop(feature, axis=1)\n    reg_up = linear_model.LinearRegression()\n    reg_up.fit(feature_data_temp, target_data)\n    r2_up = reg_up.score(feature_data_temp, target_data)\n    r2_adj_up = 1 - (1-r2_up)*(len(target_data)-1) / (len(target_data) - feature_data_temp.shape[1]-1)\n    if r2_adj_up > baseline_performance:\n        print(\"We can improve R-squared to %.5f by leaving out %s\" % (r2_adj_up, feature))   \n", "intent": "Again, the model can be improved by leaving out 'stockPhoto'. Let's try one more time.\n"}
{"snippet": "feature_data_down_three = feature_data_down_two.drop(\"nBids\", axis=1)\nbaseline_performance = 0.74006\nfor feature in feature_data_down_three.columns:\n    feature_data_temp = feature_data_down_three.drop(feature, axis=1)\n    reg_up = linear_model.LinearRegression()\n    reg_up.fit(feature_data_temp, target_data)\n    r2_up = reg_up.score(feature_data_temp, target_data)\n    r2_adj_up = 1 - (1-r2_up)*(len(target_data)-1) / (len(target_data) - feature_data_temp.shape[1]-1)\n    if r2_adj_up > baseline_performance:\n        print(\"We can improve R-squared to %.5f by leaving out %s\" % (r2_adj_up, feature))     \n", "intent": "Again, we've improved. So let's take out nBids and try again\n"}
{"snippet": "scores = []\nfor k in n_neighbors:\n    clf = neighbors.KNeighborsClassifier(k)\n    clf.fit(X_train, y_train)\n    scores.append(clf.score(X_test, y_test))\n", "intent": "Now that we have our k-values to try, let's find a good `KNeighborsClassifier`:\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\nmnb.fit(freq_train_dtm, y_train)\nmnb.score(freq_test_dtm, y_test)\n", "intent": "For the frequency model, the [`MultinomialNB`](http://scikit-learn.org/stable/modules/naive_bayes.html\n"}
{"snippet": "tfidf_nb = MultinomialNB()\ntfidf_nb.fit(tfidf_train_dtm, y_train)\ntfidf_nb.score(tfidf_test_dtm, y_test)\n", "intent": "Finally, let's try the same with our **TFIDF** model:\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nazm_mnd = MultinomialNB()\nazm_mnd.fit(train_dtm, y_train)\n", "intent": "Use `sklearn` to build a `MultinomialNB` classifier against your training data.\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(train_dtm, y_train)\n", "intent": "Use `sklearn` to build a `MultinomialNB` classifier against your training data.\n"}
{"snippet": "pipeline.fit(msg_train,label_train)\n", "intent": "Now we can directly pass message text data and the pipeline will do our pre-processing for us! We can treat it as a model/estimator API:\n"}
{"snippet": "km.fit(data)\n", "intent": "Train the classifier on all the data\n"}
{"snippet": "km = KMeans(n_clusters=4,init='k-means++',random_state=1,n_init=10,max_iter=300)\n", "intent": "Initialize a new kmeans classifier\n"}
{"snippet": "kmd = KMeans(n_clusters=20,init='k-means++',random_state=1,n_init=10,max_iter=300)\nkmd.fit(X_digits,Y_digits )\n", "intent": "Try creating a KMeans clusterer with 20 classes (obviously 10 would be ideal, but let's try 20 first).  Fit the model to the digits data.\n"}
{"snippet": "km = KMeans(n_clusters=4, random_state=1)\n", "intent": "Initialize a new kmeans classifier\n"}
{"snippet": "km = KMeans(n_clusters=20)\nkm.fit(X_digits)\n", "intent": "Try creating a KMeans clusterer with 20 classes (obviously 10 would be ideal, but let's try 20 first).  Fit the model to the digits data.\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(32,32,3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "dtc = DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "dtc.fit(X_train, y_train)\n", "intent": "**Create predictions from the test set and create a classification report and a confusion matrix.**\n"}
{"snippet": "model = KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\n", "intent": "Time to train a model!\n** Import MultinomialNB and create an instance of the estimator and call is nb **\n"}
{"snippet": "from sklearn.svm import SVC\nmodel = SVC()\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "grid = GridSearchCV(estimator=SVC(), param_grid=param_grid, verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), -1, 1))\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "pred, W, b = model(X_train, Y_train, word_to_vec_map)\nprint(pred)\n", "intent": "Run the next cell to train your model and learn the softmax parameters (W,b). \n"}
{"snippet": "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)\n", "intent": "Fit the Keras model on `X_train_indices` and `Y_train_oh`. We will use `epochs = 50` and `batch_size = 32`.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nparameters = { 'C':np.linspace(1e-3,1e3,5)}\nclf = GridSearchCV(SVC(), parameters, scoring=\"roc_auc\")\nclf = clf.fit(x_train, y_train)\nbest_linear_model = clf\n", "intent": "Select the best linear model among $C \\in [\\mathtt{1.e-3}, .., \\mathtt{1.e3}]$.\n"}
{"snippet": "parameters = { 'C':np.linspace(1e-3,1e3,5)}\nlm_tf = GridSearchCV(SVC(), parameters, scoring=\"roc_auc\")\nlm_tf = lm_tf.fit(x_train, y_train)\nbest_linear_model_tf = lm_tf\n", "intent": "Select the best linear model among $C \\in [\\mathtt{1.e-3}, .., \\mathtt{1.e3}]$.\n"}
{"snippet": "def mySigmoid(x):\n    return 1/ (1+np.exp(-x))\nsample=np.random.multivariate_normal(mean=np.array([0,0,0]),cov=np.array([[1,-0.25,0.75],[-0.25,1,0.5],[0.75,0.5,2]]),size=1000)\nmonteCarloApproximation = np.mean(mySigmoid(sample.dot(np.array([2/3,1/6,1/6]))))\nprint(\"MonteCarlo:\",monteCarloApproximation)\nprint(\"FiniteDifferenceHessian:\",approximatedIntegral)\n", "intent": "Compare the results\n"}
{"snippet": "decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(training_X, training_y)\n", "intent": "Construct calssification algorithms and train them.\n"}
{"snippet": "nb.fit(X_train,y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "model = LogisticRegression()\n", "intent": "As a base classifier we will use Logistic Regression. Although it also can procces multiclass data, we will use it as a binary classifier.\n"}
{"snippet": "ovr_classifier = OneVsRestClassifier(clone(model))\novr_classifier.fit(train_X, train_y)\n", "intent": "Apply **ovr** for our model.\n"}
{"snippet": "oc_classifier = OutputCodeClassifier(\n    clone(model),\n    code_size=2.)\noc_classifier.fit(train_X, train_y)\n", "intent": "Apply **Output Coding**.\n"}
{"snippet": "append_categoricals = Pipeline([\n        ('append_cats', DataFrameSelector(attribute_names=one_hot_transformer.encoded_columns))  \n    ])\n", "intent": "Pipeline that simply gets the categorical/encoded columns from the previous transformation (which used `oo-learning`)\n"}
{"snippet": "model = ExtraTreesClassifier(\n                      random_state=42,        \n                     )\n", "intent": "Choose the transformations to tune, below:\n"}
{"snippet": "model = RandomForestClassifier(\n                      random_state=42,        \n                     )\n", "intent": "Choose the transformations to tune, below:\n"}
{"snippet": "model.fit(Xsmall, ysmall, batch_size=500, epochs=40,verbose = 2)\nmodel.save_weights('model_weights/best_RNN_small_textdata_weights.hdf5')\n", "intent": "Now lets fit our model!\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nrfreg = RandomForestRegressor()\nrfreg\n", "intent": "<a id=\"tuning\"></a>\n"}
{"snippet": "k = 4\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(X)\n", "intent": "- Select a number of clusters of your choice based on your visual analysis above.\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\ngrid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "test_acc = []\nfor i in range(1, X_train.shape[0]+1):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    test_acc.append(knn.score(X_test, y_test))\n", "intent": "- Store the test accuracy in a list.\n- Plot the test accuracy vs. the number of neighbors.\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(Xs, y)\n", "intent": "**10.E: Fit a `KNeighborsClassifier` with the best number of neighbors.**\n"}
{"snippet": "lr = LinearRegression()\ntype(lr)\n", "intent": "**Step 2:** \"Instantiate\" the \"estimator.\"\n- \"Estimator\" is scikit-learn's term for \"model.\"\n- \"Instantiate\" means \"make an instance of.\"\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = RandomForestClassifier(n_estimators=50, max_features='sqrt')\nclf = clf.fit(train, target)\n", "intent": "<img src=\"images/random_forest.jpg\" width=\"600\" height=\"400\" />\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlg = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "y = tf.nn.softmax(tf.matmul(x, W) + b)\n", "intent": "$y_{i} = exp^{netinput_{i}}/\\sum_{i} exp^{netinput_{i}} $\n"}
{"snippet": "graph = tf.get_default_graph()\n", "intent": "We already have a default graph that is empty.\n"}
{"snippet": "sess = tf.Session()\nsess.run(input_value)\n", "intent": "If we want to give value to input_value, we have to run the sesseion\n"}
{"snippet": "sess = tf.Session()\n", "intent": "As before, here we'll train the network. Instead of flattening the images though, we can pass them in as 28x28x1 arrays.\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, epochs=40, batch_size=16)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model.fit(X_train, Y_train, epochs = 2, batch_size = 32)\n", "intent": "Run the following cell to train your model on 2 epochs with a batch size of 32. On a CPU it should take you around 5min per epoch. \n"}
{"snippet": "plot_model(model, to_file='model.png')\nSVG(model_to_dot(model).create(prog='dot', format='svg'))\n", "intent": "Finally, run the code below to visualize your ResNet50. You can also download a .png picture of your model by going to \"File -> Open...-> model.png\".\n"}
{"snippet": "classifier = Sequential()\nclassifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\", input_dim=11))\nclassifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\nclassifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\nclassifier.summary()\n", "intent": "Initialize ANN: Define sequence of layers or define graph, here we will use the first way\n"}
{"snippet": "from sklearn.svm import LinearSVR\nlin_svr = LinearSVR()\nlin_svr.fit(X_train_scaled, y_train)\n", "intent": "Let's train a simple `LinearSVR` first:\n"}
{"snippet": "tf.reset_default_graph()\nn_epochs = 1000\nlearning_rate = 0.01\nX = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\ny = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\ntheta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\ny_pred = tf.matmul(X, theta, name=\"predictions\")\nerror = y_pred - y\nmse = tf.reduce_mean(tf.square(error), name=\"mse\")\n", "intent": "Same as above except for the `gradients = ...` line:\n"}
{"snippet": "tf.reset_default_graph()\ndef relu(X):\n    with tf.name_scope(\"relu\"):\n        w_shape = (int(X.get_shape()[1]), 1)                          \n        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")    \n        b = tf.Variable(0.0, name=\"bias\")                             \n        z = tf.add(tf.matmul(X, w), b, name=\"z\")                      \n        return tf.maximum(z, 0., name=\"max\")                          \n", "intent": "Even better using name scopes:\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "First let's reset the default graph.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Ok, next let's reset the default graph:\n"}
{"snippet": "sess = tf.Session()\n", "intent": "As before, here wi'll train the network. Instead of flattening the images though, we can pass them in as 28x28x1 arrays.\n"}
{"snippet": "from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)\n", "intent": "Let's now try decision tree\n"}
{"snippet": "input = model.layers[0].input\noutput = model.layers[-2].output\nshort_model = Model(input, output)\n", "intent": "Remove last layer from the network, use these outputs as new features.\n"}
{"snippet": "a = tf.placeholder(dtype=tf.int32, shape=(None,))  \nb = tf.placeholder(dtype=tf.int32, shape=(None,))\nc = tf.add(a, b)\nwith tf.Session() as sess:\n  result = sess.run(c, feed_dict={\n      a: [3, 4, 5],\n      b: [-1, 2, 3]\n    })\n  print result\n", "intent": "<h2> Using a feed_dict </h2>\nSame graph, but without hardcoding inputs at build stage\n"}
{"snippet": "def linear_model(img):\n  X = tf.reshape(img,[-1,HEIGHT*WIDTH]) \n  W = tf.get_variable(\"W\", [HEIGHT*WIDTH,NCLASSES], \n                      initializer = tf.truncated_normal_initializer(stddev=0.1,seed = 1))\n  b = tf.get_variable(\"b\",NCLASSES, initializer = tf.zeros_initializer)\n  ylogits = tf.matmul(X,W)+b\n  return ylogits, NCLASSES\n", "intent": "A simple low-level matrix multiplication\n"}
{"snippet": "def dnn_model(img, mode):\n  X = tf.reshape(img, [-1, HEIGHT*WIDTH]) \n  h1 = tf.layers.dense(X, 300, activation=tf.nn.relu)\n  h2 = tf.layers.dense(h1,100, activation=tf.nn.relu)\n  h3 = tf.layers.dense(h2, 30, activation=tf.nn.relu)\n  ylogits = tf.layers.dense(h3, NCLASSES, activation=None)\n  return ylogits, NCLASSES\n", "intent": "A 3-hidden layer network that uses tf.layers to reduce the boilerplate\n"}
{"snippet": " def dnn_dropout_model(img, mode):\n  X = tf.reshape(img, [-1, HEIGHT*WIDTH]) \n  h1 = tf.layers.dense(X, 300, activation=tf.nn.relu)\n  h2 = tf.layers.dense(h1,100, activation=tf.nn.relu)\n  h3 = tf.layers.dense(h2, 30, activation=tf.nn.relu)\n  h3d = tf.layers.dropout(h3, rate=0.1, training=(mode == tf.estimator.ModeKeys.TRAIN))\n  ylogits = tf.layers.dense(h3d, NCLASSES, activation=None)\n  return ylogits, NCLASSES\n", "intent": "A 3-hidden layer network with a dropout layer between layer 3 and the output\n"}
{"snippet": "def try_out():\n  with tf.Session() as sess:\n    fn = read_dataset(tf.estimator.ModeKeys.EVAL, \n                    {'input_path': 'data', 'batch_size': 8, 'nitems': 5668, 'nusers': 82802})\n    feats, _ = fn()\n    print feats['input_rows'].eval()\n", "intent": "This code is helpful in developing the input function. You don't need it in production.\n"}
{"snippet": "def try_out():\n  with tf.Session() as sess:\n    fn = read_dataset(tf.estimator.ModeKeys.EVAL, \n                    {'input_path': 'gs://{}/wals/preproc_tft/'.format(BUCKET), \n                     'batch_size': 8, 'nitems': 5668, 'nusers': 82802})\n    feats, _ = fn()\n    print feats['input_rows'].eval()\n", "intent": "This code is helpful in developing the input function. You don't need it in production.\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS', axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "ans = fixed_model(x_var)\n", "intent": "Run the following cell to evaluate the performance of the forward pass running on the CPU:\n"}
{"snippet": "def rand(a, b):\n    return (b-a)*random.random() + a\ndef makeMatrix(I, J, fill=0.0):\n    return np.zeros([I,J])\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\ndef dsigmoid(y):\n    return (y * (1- y))\n", "intent": "This process will eventually result in our own NN class\nWhen we initialize the neural networks, the weights have to be randomly assigned.  \n"}
{"snippet": "a = np.zeros((3,3))\nta = tf.convert_to_tensor(a)\n", "intent": "Input external data into TensorFlow\n"}
{"snippet": "input1 = tf.placeholder(tf.float32)\ninput2 = tf.placeholder(tf.float32)\noutput = tf.mul(input1, input2)\nwith tf.Session() as sess:\n    print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))\n", "intent": "A ** feed_dict ** is a python dictionary mapping from tf.\nplaceholder vars (or their names) to data (numpy arrays, lists, etc.).\n"}
{"snippet": "with tf.variable_scope(\"foo\"):\n    v = tf.get_variable(\"v\", [1])\nassert v.name == \"foo/v:0\"\n", "intent": "** Case 1: reuse set to false **\n- Create and return new variable\n"}
{"snippet": "with tf.variable_scope(\"foo\"):\n    v = tf.get_variable(\"v\", [1])\nwith tf.variable_scope(\"foo\", reuse=True):\n    v1 = tf.get_variable(\"v\", [1])\nassert v1 == v    \n", "intent": "** Case 2: Variable reuse set to true **\n- Search for existing variable with given name. Raise ValueError if none found\n"}
{"snippet": "simay_upp = np.triu(simay,k=1) \ndef sim_art(n):   \n    flat = simay_upp.flatten()\n    indices = np.argpartition(flat, -n)[-n:]\n    indices = indices[np.argsort(-flat[indices])]\n    loc = np.unravel_index(indices, simay.shape)\n    print [docids.keys()[docids.values().index(loc[0][n-1])], loc[0][n-1]]\n    print [docids.keys()[docids.values().index(loc[1][n-1])], loc[1][n-1]]\n", "intent": "<p> From the heatmap above we can know that documentID 16 has more similarity with other documents \n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(solver='lbfgs').fit(X, y);\n", "intent": "Now we can fit the model.\n"}
{"snippet": "from torchvision import datasets, transforms\ntransform = transforms.Compose([transforms.ToTensor(),\n                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                             ])\ntrainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n", "intent": "The same as we saw in part 3, we'll load the MNIST dataset and define our network.\n"}
{"snippet": "images, labels = next(iter(trainloader))\nimg = images[0].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "def test_discriminator(true_count=267009):\n    tf.reset_default_graph()\n    with get_session() as sess:\n        y = discriminator(tf.ones((2, 784)))\n        cur_count = count_params()\n        if cur_count != true_count:\n            print('Incorrect number of parameters in discriminator. {0} instead of {1}. Check your achitecture.'.format(cur_count,true_count))\n        else:\n            print('Correct number of parameters in discriminator.')\ntest_discriminator()\n", "intent": "Test to make sure the number of parameters in the discriminator is correct:\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nrf_exp = RandomForestRegressor(n_estimators= 1000, random_state=42)\nrf_exp.fit(train_features, train_labels);\n", "intent": "The rf_exp uses the same number of decision trees (n_estimators) but is trained on the longer dataset with 3 additional features.\n"}
{"snippet": "rf_small = RandomForestRegressor(n_estimators=10, max_depth = 3, random_state=42)\nrf_small.fit(train_features, train_labels)\ntree_small = rf_small.estimators_[5]\nexport_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\ngraph.write_png('small_tree.png');\n", "intent": "Smaller tree for visualization.\n"}
{"snippet": "V_data = [1., 2., 3.]\nV = torch.Tensor(V_data)\nprint V\nM_data = [[1., 2., 3.], [4., 5., 6]]\nM = torch.Tensor(M_data)\nprint M\nT_data = [[[1.,2.], [3.,4.]],\n          [[5.,6.], [7.,8.]]]\nT = torch.Tensor(T_data)\nprint T\n", "intent": "Tensors can be created from Python lists with the torch.Tensor() function.\n"}
{"snippet": "x = torch.Tensor([ 1., 2., 3. ])\ny = torch.Tensor([ 4., 5., 6. ])\nz = x + y\nprint z\n", "intent": "You can operate on tensors in the ways you would expect.\n"}
{"snippet": "x = torch.randn((2,2))\ny = torch.randn((2,2))\nz = x + y \nvar_x = autograd.Variable( x )\nvar_y = autograd.Variable( y )\nvar_z = var_x + var_y \nprint var_z.grad_fn\nvar_z_data = var_z.data \nnew_var_z = autograd.Variable( var_z_data ) \nprint new_var_z.grad_fn\n", "intent": "Understanding what is going on in the block below is crucial for being a successful programmer in deep learning.\n"}
{"snippet": "for test in test_data:\n    model.zero_grad()\n    preds = model(test, ROOT_ONLY)\n    labels = test.labels[-1:] if ROOT_ONLY else test.labels\n    for pred, label in zip(preds.max(1)[1].data.tolist(), labels):\n        num_node += 1\n        if pred == label:\n            accuracy += 1\nprint(accuracy/num_node * 100)\n", "intent": "In paper, they acheived 80.2 accuracy. \n"}
{"snippet": "gbc.fit(Xtr, ytr)\n", "intent": "[[ go back to the top ]](\n"}
{"snippet": "def get_ll_layers():\n    return [ \n        BatchNormalization(input_shape=(4096,)),\n        Dropout(0.5),\n        Dense(2, activation='softmax') \n        ]\n", "intent": "The functions automate creating a model that trains the last layer from scratch, and then adds those new layers on to the main model.\n"}
{"snippet": "def get_dense_layers():\n    return [\n        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n        Flatten(),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(1000, activation='softmax')\n        ]\n", "intent": "This is our usual Vgg network just covering the dense layers:\n"}
{"snippet": "def test_generator(true_count=1858320):\n    tf.reset_default_graph()\n    with get_session() as sess:\n        y = generator(tf.ones((1, 4)))\n        cur_count = count_params()\n        if cur_count != true_count:\n            print('Incorrect number of parameters in generator. {0} instead of {1}. Check your achitecture.'.format(cur_count,true_count))\n        else:\n            print('Correct number of parameters in generator.')\ntest_generator()\n", "intent": "Test to make sure the number of parameters in the generator is correct:\n"}
{"snippet": "bnl1.set_weights([var0, mu0, mu0, var0])\nbnl4.set_weights([var2, mu2, mu2, var2])\n", "intent": "After inserting the layers, we can set their weights to the variance and mean we just calculated.\n"}
{"snippet": "def fit_model(model, batches, val_batches, nb_epoch=1):\n    model.fit_generator(batches, samples_per_epoch=batches.n, nb_epoch=nb_epoch, \n                        validation_data=val_batches, nb_val_samples=val_batches.n)\n", "intent": "We'll define a simple function for fitting models, just to save a little typing...\n"}
{"snippet": "def embedding_input(name, n_in, n_out):\n    inp = Input(shape=(1,), dtype='int64', name=name)\n    emb = Embedding(n_in, n_out, input_length=1)(inp)\n    return inp, Flatten()(emb)\n", "intent": "Create inputs and embedding outputs for each of our 3 character inputs\n"}
{"snippet": "dense_in = Dense(n_hidden, activation='relu')\n", "intent": "This is the 'green arrow' from our diagram - the layer operation from input to hidden.\n"}
{"snippet": "dense_hidden = Dense(n_hidden, activation='tanh')\n", "intent": "This is the 'orange arrow' from our diagram - the layer operation from hidden to hidden.\n"}
{"snippet": "dense_out = Dense(vocab_size, activation='softmax')\n", "intent": "This is the 'blue arrow' from our diagram - the layer operation from hidden to output.\n"}
{"snippet": "model = Model([c[0] for c in c_ins], c_out)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n", "intent": "So now we can create our model.\n"}
{"snippet": "model=Sequential([\n        Embedding(vocab_size, n_fac, input_length=cs),\n        SimpleRNN(n_hidden, activation='relu', inner_init='identity'),\n        Dense(vocab_size, activation='softmax')\n    ])\n", "intent": "This is nearly exactly equivalent to the RNN we built ourselves in the previous section.\n"}
{"snippet": "model=Sequential([\n        SimpleRNN(n_hidden, return_sequences=True, input_shape=(cs, vocab_size),\n                  activation='relu', inner_init='identity'),\n        TimeDistributed(Dense(vocab_size, activation='softmax')),\n    ])\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam())\n", "intent": "This is the keras version of the theano model that we're about to create.\n"}
{"snippet": "idx = 0\ntarget_y = 6\nX_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\nX_fooling = make_fooling_image(X_tensor[idx:idx+1], target_y, model)\nscores = model(Variable(X_fooling))\nassert target_y == scores.data.max(1)[1][0, 0], 'The model is not fooled!'\n", "intent": "Run the following cell to generate a fooling image:\n"}
{"snippet": "model=Sequential([\n        GRU(n_hidden, return_sequences=True, input_shape=(cs, vocab_size),\n                  activation='relu', inner_init='identity'),\n        TimeDistributed(Dense(vocab_size, activation='softmax')),\n    ])\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam())\n", "intent": "Identical to the last keras rnn, but a GRU!\n"}
{"snippet": "def gate(x, h, W_h, W_x, b_x):\n    return nnet.sigmoid(T.dot(x, W_x) + b_x + T.dot(h, W_h))\n", "intent": "Here's the definition of a gate - it's just a sigmoid applied to the addition of the dot products of the input vectors.\n"}
{"snippet": "model = Model([inp, sz_inp], x)\nmodel.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "When we compile the model, we have to specify all the input layers in an array.\n"}
{"snippet": "model.fit([conv_feat, trn_sizes], trn_labels, batch_size=batch_size, nb_epoch=3, \n             validation_data=([conv_val_feat, val_sizes], val_labels))\n", "intent": "And when we train the model, we have to provide all the input layers' data in an array.\n"}
{"snippet": "vocab = sorted(set(flatten(stories)))\nvocab.insert(0, '<PAD>')\nvocab_size = len(vocab)\n", "intent": "Create vocabulary of corpus and find size, including a padding element.\n"}
{"snippet": "def emb_sent_bow(inp):\n    emb = TimeDistributed(Embedding(vocab_size, emb_dim))(inp)\n    return Lambda(lambda x: K.sum(x, 2))(emb)\n", "intent": "We use <tt>TimeDistributed</tt> here to apply the embedding to every element of the sequence, then the <tt>Lambda</tt> layer adds them up\n"}
{"snippet": "K.set_value(answer.optimizer.lr, 1e-2)\nhist=answer.fit(inps, answers_train, **parms, nb_epoch=4, batch_size=32,\n           validation_data=(val_inps, answers_test))\n", "intent": "And it works extremely well\n"}
{"snippet": "f = Model([inp_story, inp_q], match)\n", "intent": "We can look inside our model to see how it's weighting the sentence embeddings.\n"}
{"snippet": "K.set_value(answer.optimizer.lr, 5e-3)\nhist=answer.fit(inps, answers_train, **parms, nb_epoch=8, batch_size=32,\n           validation_data=(val_inps, answers_test))\n", "intent": "Fitting this model can be tricky.\n"}
{"snippet": "dfDelayData = dfDelayData[dfDelayData.DepDelay.notnull()]\ndfDelayData = dfDelayData.join(pd.get_dummies(dfDelayData['UniqueCarrier'], prefix='carrier'))\ndfDelayData = dfDelayData.join(pd.get_dummies(dfDelayData['DayOfWeek'], prefix='dow'))\nmodel = lm.LogisticRegression()\nfeatures = [i for i in dfDelayData.columns if 'dow_' in i] \n", "intent": "In the section below, I am predicting the probability of delay based upon the time of the day.\n"}
{"snippet": "def conv(x, nf, sz, wd, p):\n    x = Convolution2D(nf, sz, sz, init='he_uniform', border_mode='same', \n                          W_regularizer=l2(wd))(x)\n    return dropout(x,p)\n", "intent": "Convolutional layer:\n* L2 Regularization\n* 'same' border mode returns same width/height\n* Pass output through Dropout\n"}
{"snippet": "model.fit(x_train, y_train, 64, 20, validation_data=(x_test, y_test), **parms)\n", "intent": "This will likely need to run overnight + lr annealing...\n"}
{"snippet": "def meanshift(data):\n    X = np.copy(data)\n    for it in range(5):\n        for i, x in enumerate(X):\n            dist = np.sqrt(((x-X)**2).sum(1))\n            weight = gaussian(dist, 2.5)\n            X[i] = (np.expand_dims(weight,1)*X).sum(0) / weight.sum()\n    return X\n", "intent": "In our implementation, we choose the bandwidth to be 2.5. \nOne easy way to choose bandwidth is to find which bandwidth covers one third of the data.\n"}
{"snippet": "def meanshift(data):\n    X = torch.FloatTensor(np.copy(data))\n    for it in range(5):\n        for i, x in enumerate(X):\n            dist = torch.sqrt((sub(x, X)**2).sum(1))\n            weight = gaussian(dist, 3)\n            num = mul(weight, X).sum(0)\n            X[i] = num / weight.sum()\n    return X\n", "intent": "And the implementation of meanshift is nearly identical too!\n"}
{"snippet": "inp,outp=get_model(arr_hr)\nmodel_hr = Model(inp, outp)\ncopy_weights(top_model.layers, model_hr.layers)\n", "intent": "Since the CNN is fully convolutional, we can use it one images of arbitrary size. Let's try it w/ the high-res as the input.\n"}
{"snippet": "x = torch.Tensor(5, 3); x\n", "intent": "Tensors are similar to numpy's ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n"}
{"snippet": "x = torch.randn(3)\nx = Variable(x, requires_grad = True)\n", "intent": "Because PyTorch is a dynamic computation framework, we can take the gradients of all kinds of interesting computations, even loops!\n"}
{"snippet": "input = Variable(torch.randn(1, 1, 32, 32)).cuda()\nout = net(input); out\n", "intent": "The input to the forward is a `Variable`, and so is the output.\n"}
{"snippet": "outputs = net(Variable(images).cuda())\n_, predicted = torch.max(outputs.data, 1)\n' '.join('%5s'% classes[predicted[j][0]] for j in range(4))\n", "intent": "Okay, now let us see what the neural network thinks these examples above are:\n"}
{"snippet": "X = iris[[\"petal_length\"]]\ny = iris[\"petal_width\"]\nmodel = linear_model.LinearRegression()\nresults = model.fit(X, y)\nprint results.intercept_, results.coef_ \n", "intent": "Now let's use scikit-learn to find the best fit line.\n"}
{"snippet": "def get_contin(feat):\n    name = feat[0][0]\n    inp = Input((1,), name=name+'_in')\n    return inp, Dense(1, name=name+'_d', init=my_init(1.))(inp)\n", "intent": "Helper function for continuous inputs.\n"}
{"snippet": "model = Model([inp, inp_dec], x)\nmodel.compile(Adam(), 'sparse_categorical_crossentropy', metrics=['acc'])\n", "intent": "We can now train, passing in the decoder inputs as well for teacher forcing.\n"}
{"snippet": "ms = MeanShift(bandwidth=bw, bin_seeding=True, min_bin_freq=5)\nms.fit(y_targ)\n", "intent": "This takes some time\n"}
{"snippet": "def relu(x): return Activation('relu')(x)\ndef dropout(x, p): return Dropout(p)(x) if p else x\ndef bn(x): return BatchNormalization(mode=2, axis=-1)(x)\ndef relu_bn(x): return relu(bn(x))\ndef concat(xs): return merge(xs, mode='concat', concat_axis=-1)\n", "intent": "This should all be familiar.\n"}
{"snippet": "happyModel.fit(X_train, Y_train, epochs=40, batch_size=50)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "m = RandomForestRegressor(n_jobs=-1)\nm.fit(df, y)\nm.score(df,y)\n", "intent": "We now have something we can pass to a random forest!\n"}
{"snippet": "def get_oob(df):\n    m = RandomForestRegressor(n_estimators=30, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)\n    x, _ = split_vals(df, n_trn)\n    m.fit(x, y_train)\n    return m.oob_score_\n", "intent": "Let's try removing some of these related features to see if the model can be simplified without impacting the accuracy.\n"}
{"snippet": "df_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(df_trn2, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\nm.fit(X_train, y_train);\n", "intent": "This next analysis will be a little easier if we use the 1-hot encoded categorical variables, so let's load them up again.\n"}
{"snippet": "net = nn.Sequential(\n    nn.Linear(28*28, 100),\n    nn.ReLU(),\n    nn.Linear(100, 100),\n    nn.ReLU(),\n    nn.Linear(100, 10),\n    nn.LogSoftmax()\n).cuda()\n", "intent": "We will begin with the highest level abstraction: using a neural net defined by PyTorch's Sequential class.  \n"}
{"snippet": "model = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary()) \n", "intent": "This means that our best fit line is:\n$$y = a + b x$$\nwhere $a = -0.363075521319$ and $b = 0.41575542$.\nNext let's use `statsmodels`.\n"}
{"snippet": "l = loss(y_pred, Variable(yt).cuda())\nprint(l)\n", "intent": "We can check the loss:\n"}
{"snippet": "xt, yt = next(dl)\ny_pred = net2(Variable(xt).cuda())\n", "intent": "Now, let's make another set of predictions and check if our loss is lower:\n"}
{"snippet": "for t in range(100):\n    xt, yt = next(dl)\n    y_pred = net2(Variable(xt).cuda())\n    l = loss(y_pred, Variable(yt).cuda())\n    if t % 10 == 0:\n        accuracy = np.mean(to_np(y_pred).argmax(axis=1) == to_np(yt))\n        print(\"loss: \", l.data[0], \"\\t accuracy: \", accuracy)\n    optimizer.zero_grad()\n    l.backward()\n    optimizer.step()\n", "intent": "If we run several iterations in a loop, we should see the loss decrease and the accuracy increase with time.\n"}
{"snippet": "net = nn.Sequential(\n    nn.Linear(28*28, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10)\n).cuda()\n", "intent": "We will begin with the highest level abstraction: using a neural net defined by PyTorch's Sequential class.  \n"}
{"snippet": "def meanshift(data):\n    X = np.copy(data)\n    for it in range(5): X = meanshift_iter(X)\n    return X\n", "intent": "By repeating this a few times, we can make the clusters more accurate.\n"}
{"snippet": "def meanshift(data):\n    X = torch.from_numpy(np.copy(data)).cuda()\n    for it in range(5): X = meanshift_iter(X)\n    return X\n", "intent": "And then we'll use the exact same code as before, but first convert our numpy array to a GPU PyTorch tensor.\n"}
{"snippet": "import pymc3 as pm\nwith pm.Model() as model:\n    parameter = pm.Exponential(\"poisson_param\", 1)\n    data_generator = pm.Poisson(\"data_generator\", parameter)\n", "intent": "In PyMC3, we typically handle all the variables we want in our model within the context of the `Model` object.\n"}
{"snippet": "alpha = 1./20.\nlambda_1, lambda_2 = np.random.exponential(scale=1/alpha, size=2)\nprint(lambda_1, lambda_2)\n", "intent": "2\\. Draw $\\lambda_1$ and $\\lambda_2$ from an $\\text{Exp}(\\alpha)$ distribution:\n"}
{"snippet": "import pymc3 as pm\nimport theano.tensor as tt\nfrom theano.tensor.nlinalg import matrix_inverse, diag, matrix_dot\nprior_mu = np.array([x[0] for x in expert_prior_params.values()])\nprior_std = np.array([x[1] for x in expert_prior_params.values()])\ninit = stock_returns.cov()\nwith pm.Model() as model:\n    cov_matrix = pm.WishartBartlett(\"covariance\", np.diag(prior_std**2), 10, testval = init)\n    mu = pm.Normal(\"returns\", mu=prior_mu, sd=1, shape=4)\n", "intent": "And here let's form our basic model:\n"}
{"snippet": "X = iris[[\"petal_length\", \"setosa\", \"versicolor\", \"virginica\"]]\nX = sm.add_constant(X) \ny = iris[\"petal_width\"]\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary()) \n", "intent": "Now we perform a multilinear regression with the dummy variables added.\n"}
{"snippet": "import theano\nimport theano.tensor as T\nstates = T.matrix(\"states[batch,units]\")\nactions = T.ivector(\"action_ids[batch]\")\ncumulative_rewards = T.vector(\"G[batch] = r + gamma*r' + gamma^2*r'' + ...\")\n", "intent": "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n"}
{"snippet": "import tensorflow as tf\ntf.reset_default_graph()\nsess = tf.InteractiveSession()\n", "intent": "Here's a code for an agent that only uses feedforward layers. Please read it carefully: you'll have to extend it later!\n"}
{"snippet": "from agentnet.learning.generic import get_values_for_actions, get_mask_by_eos\nclass llh_trainer:\n    input_sequence = T.imatrix(\"input sequence [batch,time]\")\n    reference_answers = T.imatrix(\"reference translations [batch, time]\")\n    logprobs_seq = <YOUR CODE>\n    crossentropy = - get_values_for_actions(logprobs_seq,reference_answers)\n    mask = get_mask_by_eos(T.eq(reference_answers, out_voc.eos_ix))\n    loss = T.sum(crossentropy * mask)/T.sum(mask)\n    updates = <YOUR CODE>\n    train_step = theano.function([input_sequence,reference_answers], loss, updates=updates)\n", "intent": "Here we define a function that trains our model through maximizing log-likelihood a.k.a. minimizing crossentropy.\n"}
{"snippet": "get_policy = theano.function([observations], probs, allow_input_downcast=True)\ndef act(obs, sample=True):\n    policy = get_policy([obs])[0]\n    if sample:\n        action = int(np.random.choice(n_actions, p=policy))\n    else:\n        action = int(np.argmax(policy))\n    return action, policy\n", "intent": "In this section, we'll define functions that take actions $ a \\sim \\pi_\\theta(a|s) $ and rollouts $ <s_0,a_0,s_1,a_1,s_2,a_2,...s_n,a_n> $.\n"}
{"snippet": "model.fit(X, y)\n", "intent": "Now it is time to apply our model to data.\nThis can be done with the ``fit()`` method of the model:\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier().fit(X, y)\n", "intent": "This process of fitting a decision tree to our data can be done in Scikit-Learn with the ``DecisionTreeClassifier`` estimator:\n"}
{"snippet": "visualize_classifier(DecisionTreeClassifier(), X, y)\n", "intent": "Now we can examine what the decision tree classification looks like:\n"}
{"snippet": "model = grid.best_estimator_\nmodel.fit(X_train, y_train)\n", "intent": "Let's take the best estimator and re-train it on the full dataset:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\nmodel.summary()\n", "intent": "**Q:** _How hard can it be to build a Multi-Layer Fully-Connected Network with keras?_\n**A:** _It is basically the same, just add more layers!_\n"}
{"snippet": "test_models = [LinearRegression(), Ridge(alpha=10), Lasso(alpha=10)]\nscores = [analyze_performance(my_model) for my_model in test_models]\n", "intent": "Let's try a few degrees with a regularized model.\n"}
{"snippet": "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), \n              metrics=['accuracy'])\nnetwork_history = model.fit(X_train, Y_train, batch_size=128, \n                            epochs=2, verbose=1, validation_data=(X_val, Y_val))\n", "intent": "Try increasing the number of epochs (if you're hardware allows to)\n"}
{"snippet": "def sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n", "intent": "First we need to create a sigmoid function.  The code for this is pretty simple.\n"}
{"snippet": "def cost(theta, X, y):\n    theta = np.matrix(theta)\n    X = np.matrix(X)\n    y = np.matrix(y)\n    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))\n    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))\n    return np.sum(first - second) / (len(X))\n", "intent": "Excellent!  Now we need to write the cost function to evaluate a solution.\n"}
{"snippet": "def costReg(theta, X, y, learningRate):\n    theta = np.matrix(theta)\n    X = np.matrix(X)\n    y = np.matrix(y)\n    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))\n    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))\n    reg = (learningRate / 2 * len(X)) * np.sum(np.power(theta[:,1:theta.shape[1]], 2))\n    return np.sum(first - second) / (len(X)) + reg\n", "intent": "Now we need to modify the cost and gradient functions from part 1 to include the regularization term.  First the cost function:\n"}
{"snippet": "svc.fit(data[['X1', 'X2']], data['y'])\nsvc.score(data[['X1', 'X2']], data['y'])\n", "intent": "For the first experiment we'll use C=1 and see how it performs.\n"}
{"snippet": "svc2 = svm.LinearSVC(C=100, loss='hinge', max_iter=1000)\nsvc2.fit(data[['X1', 'X2']], data['y'])\nsvc2.score(data[['X1', 'X2']], data['y'])\n", "intent": "It appears that it mis-classified the outlier.  Let's see what happens with a larger value of C.\n"}
{"snippet": "print(softmax([10, 2, -3]))\n", "intent": "Make sure that this works one vector at a time (and check that the components sum to one):\n"}
{"snippet": "X = np.array([[10, 2, -3],\n              [-1, 5, -20]])\nprint(softmax(X))\n", "intent": "Note that a naive implementation of softmax might not be able process a batch of activations in a single call:\n"}
{"snippet": "def softmax(X):\n    exp = np.exp(X)\n    return exp / np.sum(exp, axis=-1, keepdims=True)\nprint(\"softmax of a single vector:\")\nprint(softmax([10, 2, -3]))\n", "intent": "Here is a way to implement softmax that works both for an individal vector of activations and for a batch of activation vectors at once:\n"}
{"snippet": "y = df[\"Hired\"]\nX = df[features]\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X,y)\n", "intent": "Now actually construct the decision tree:\n"}
{"snippet": "print(np.sum(softmax(X), axis=1))\n", "intent": "The sum of probabilities for each input vector of logits should some to 1:\n"}
{"snippet": "from keras.layers import Input\nfrom keras.models import Model\nx = Input(shape=[1], name='input')\nembedding = embedding_layer(x)\nmodel = Model(inputs=x, outputs=embedding)\n", "intent": "Let's use it as part of a Keras model:\n"}
{"snippet": "from keras.layers import Input\nfrom keras.models import Model\nx = Input(shape=[1], name='input')\nembedding = embedding_layer(x)\nmodel = Model(input=x, output=embedding)\nmodel.output_shape\n", "intent": "Let's use it as part of a Keras model:\n"}
{"snippet": "x = tf.placeholder(tf.float32, shape=[None, 784])\ny_true = tf.placeholder(tf.float32, shape=[None, 10])\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\ny_pred = tf.matmul(x,W) + b\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true)\nloss = tf.reduce_mean(cross_entropy)\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\ncorrect_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y_true,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n", "intent": "- A logistic regression without taking into account the spatiality of the data\n- Very similar to lab01\n"}
{"snippet": "x = tf.placeholder(tf.float32, shape=[None, 784])\ny_true = tf.placeholder(tf.float32, shape=[None, 10])\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\ny_pred = tf.matmul(x,W) + b\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(y_pred, y_true)\nloss = tf.reduce_mean(cross_entropy)\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\ncorrect_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y_true,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n", "intent": "- A logistic regression without taking into account the spatiality of the data\n- Very similar to lab01\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\ntop_model = Sequential()\ntop_model.add(Dense(1, input_dim=n_features, activation='sigmoid'))\ntop_model.compile(optimizer=Adam(lr=1e-4),\n                  loss='binary_crossentropy', metrics=['accuracy'])\ntop_model.fit(features_train, labels_train,\n              validation_split=0.1, verbose=2, epochs=15)\n", "intent": "Let's define the classification model:\n"}
{"snippet": "history = stupid_model.fit(inputs, [out_cls, out_boxes],\n                           batch_size=10, epochs=10)\n", "intent": "Now check whether the loss decreases and eventually if we are able to overfit on these few examples for debugging purpose.\n"}
{"snippet": "history = model.fit(inputs, [out_cls, out_boxes],\n                    batch_size=10, nb_epoch=10)\n", "intent": "Now check whether the loss decreases and eventually if we are able to overfit on these few examples for debugging purpose.\n"}
{"snippet": "from keras.models import Sequential\nmodel = Sequential([SoftmaxMap(input_shape=(w, h, n_classes))])\nmodel.output_shape\n", "intent": "Let's wrap the `SoftmaxMap` class into a test model to process our test data:\n"}
{"snippet": "from sklearn import svm, datasets\nC = 1.0\nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\n", "intent": "Now we'll use linear SVC to partition our graph into clusters:\n"}
{"snippet": "small_train = slice(0, None, 40)\nmodel.fit(X[small_train], y[small_train], validation_split=0.1,\n          batch_size=128, nb_epoch=1)\n", "intent": "Let's train the model for one epoch on a very small subset of the training set to check that it's well defined:\n"}
{"snippet": "pretrained_embedding_layer = Embedding(\n    MAX_NB_WORDS, EMBEDDING_DIM,\n    weights=[embedding_matrix],\n    input_length=MAX_SEQUENCE_LENGTH,\n)\n", "intent": "Build a layer with pre-trained embeddings:\n"}
{"snippet": "simple_seq2seq = load_model(best_model_fname)\n", "intent": "Let's load the best model found on the validation set at the end of training:\n"}
{"snippet": "net = gluon.nn.Sequential()\nwith net.name_scope():\n    net.add(gluon.nn.Conv2D(channels=20, kernel_size=5, activation='relu'))\n    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))            \n    net.add(gluon.nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n    trl = TRL([10, 3, 3, 10], num_outputs)\n    net.add(trl)\n", "intent": "Here, we define a simple Convolution Neural Network in which, instead of having a flattening + fully connected layer for output, we use a TRL:\n"}
{"snippet": "mu = Variable(torch.zeros(1))   \nsigma = Variable(torch.ones(1)) \nx = dist.normal(mu, sigma)      \nprint(x)\n", "intent": "Let's draw a sample from a unit normal distribution:\n"}
{"snippet": "mu = pyro.param(\"mu\", Variable(torch.zeros(1), requires_grad=True))\nprint(mu)\n", "intent": "We can also declare mu as a named parameter:\n"}
{"snippet": "import torch.nn as nn\nz_dim=20\nhidden_dim=100\nnn_decoder = nn.Sequential(\n    nn.Linear(z_dim, hidden_dim), \n    nn.Softplus(), \n    nn.Linear(hidden_dim, 784), \n    nn.Sigmoid()\n)\n", "intent": "First we define our decoder network\n"}
{"snippet": "from pyro.util import ng_zeros, ng_ones \ndef model(x):\n    batch_size=x.size(0)\n    pyro.module(\"decoder\", nn_decoder)  \n    z = pyro.sample(\"z\", dist.normal,   \n                    ng_zeros(batch_size, z_dim), \n                    ng_ones(batch_size, z_dim))\n    bern_prob = nn_decoder(z)          \n    return pyro.sample(\"x\", dist.bernoulli, bern_prob, obs=x) \n", "intent": "Now we can define our generative model conditioned on the observed mini-batch of images `x`:\n"}
{"snippet": "import torchvision.datasets as dset\nimport torchvision.transforms as transforms\nbatch_size=250\ntrans = transforms.ToTensor()\ntrain_set = dset.MNIST(root='./mnist_data', train=True, \n                       transform=trans, download=True)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_set, \n                                           batch_size=batch_size,\n                                           shuffle=True)\n", "intent": "Let's setup a basic training loop. First we setup the data loader:\n"}
{"snippet": "def conv_layer(prev_layer, layer_num, is_training):\n    strides = 2 if layer_num % 3 == 0 else 1\n    conv_layer = tf.layers.conv2d(prev_layer, layer_num*4, 3, strides, 'same', use_bias=True, activation=tf.nn.relu)\n    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n    return conv_layer\n", "intent": "**Alternate solution that uses a bias and ReLU activation function _before_ batch normalization.**\n"}
{"snippet": "def geom_image_prior(x, step=0):\n    p = Variable(torch.Tensor([0.4]))\n    i = pyro.sample('i{}'.format(step), dist.bernoulli, p)\n    if i.data[0] == 1:\n        return x\n    else:\n        x = x + prior_step(step)  \n        return geom_image_prior(x, step + 1)\n", "intent": "Now we can use `prior_step` to define a recursive prior over images:\n"}
{"snippet": "ix = torch.LongTensor([9906, 1879, 5650,  967, 7420, 7240, 2755, 9390,   42, 5584])\nn_images = len(ix)\nexamples_to_viz = X[ix]\n", "intent": "Let's pick some datapoints from the test set to reconstruct and visualize:\n"}
{"snippet": "x = Variable(torch.ones(2, 2), requires_grad=True)\nprint(x)\n", "intent": "Example taken from [this tutorial](http://pytorch.org/tutorials/beginner/former_torchies/autograd_tutorial.html) in the official documentation.\n"}
{"snippet": "proj_t = np.reshape(proj_operator.todense().A, (l//7,l,l,l))\n", "intent": "dimensions: angles (l//7), positions (l), image for each (l x l)\n"}
{"snippet": "regr = linear_model.LinearRegression()\n", "intent": "Let's start by using the sklearn implementation:\n"}
{"snippet": "n_hidden = 128\nrnn = RNN(n_letters, n_hidden, n_categories)\n", "intent": "With our custom `RNN` class defined, we can create a new instance:\n"}
{"snippet": "import random\ndef random_training_pair():                                                                                                               \n    category = random.choice(all_categories)\n    line = random.choice(category_lines[category])\n    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n    line_tensor = Variable(line_to_tensor(line))\n    return category, line, category_tensor, line_tensor\nfor i in range(10):\n    category, line, category_tensor, line_tensor = random_training_pair()\n    print('category =', category, '/ line =', line)\n", "intent": "We will also want a quick way to get a training example (a name and its language):\n"}
{"snippet": "small_hidden_size = 8\nsmall_n_layers = 2\nencoder_test = EncoderRNN(input_lang.n_words, small_hidden_size, small_n_layers)\ndecoder_test = LuongAttnDecoderRNN('general', small_hidden_size, output_lang.n_words, small_n_layers)\nif USE_CUDA:\n    encoder_test.cuda()\n    decoder_test.cuda()\n", "intent": "Create models with a small size (a good idea for eyeball inspection):\n"}
{"snippet": "from sklearn.linear_model import Perceptron\nppn = Perceptron(n_iter=40, eta0=0.1, random_state=1)\nppn.fit(X_train_std, y_train)\n", "intent": "Redefining the `plot_decision_region` function from chapter 2:\n"}
{"snippet": "def conv_layer(prev_layer, layer_num, is_training):\n    strides = 2 if layer_num % 3 == 0 else 1\n    conv_layer = tf.layers.conv2d(prev_layer, layer_num*4, 3, strides, 'same', use_bias=False, activation=tf.nn.relu)\n    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n    return conv_layer\n", "intent": "**Alternate solution that uses a ReLU activation function _before_ normalization, but no bias.**\n"}
{"snippet": "with tf.Session(graph=g2) as sess:\n    try:\n        sess.run(init_op)\n        print('w2:', sess.run(w2))\n    except tf.errors.FailedPreconditionError as e:\n        print(e)\n", "intent": "Error if a variable is not initialized:\n"}
{"snippet": "g2 = tf.Graph()\nwith tf.Session(graph=g2) as sess:\n    new_saver = tf.train.import_meta_graph(\n        './trained-model.meta')\n    new_saver.restore(sess, './trained-model')\n    y_pred = sess.run('y_hat:0', \n                      feed_dict={'tf_x:0' : x_test})\n", "intent": "Restoring the saved model:\n"}
{"snippet": "from sklearn.linear_model import Perceptron\nppn = Perceptron(n_iter=40, eta0=0.1, random_state=0)\nppn.fit(X_train_std, y_train)\n", "intent": "Redefining the `plot_decision_region` function from chapter 2:\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr = lr.fit(X_train_pca, y_train)\n", "intent": "Training logistic regression classifier using the first 2 principal components.\n"}
{"snippet": "import keras.backend as K\nK.set_image_data_format('channels_first')\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, ZeroPadding2D, BatchNormalization, Input\nfrom keras.layers import Conv2DTranspose, Reshape, Activation, Cropping2D, Flatten\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.activations import relu\nfrom keras.initializers import RandomNormal\nconv_init = RandomNormal(0, 0.02)\ngamma_init = RandomNormal(1., 0.02)\n", "intent": "modifed from https://github.com/martinarjovsky/WassersteinGAN \n"}
{"snippet": "def G(z, w=g_weights):\n    h1 = tf.nn.relu(tf.matmul(z, w['w1']) + w['b1'])\n    return tf.sigmoid(tf.matmul(h1, w['out']) + w['b2'])\ndef D(x, w=d_weights):\n    h1 = tf.nn.relu(tf.matmul(x, w['w1']) + w['b1'])\n    return tf.sigmoid(tf.matmul(h1, w['out']) + w['b2'])\n", "intent": "The models were chosen to be very simple, so just an MLP with 1 hidden layer and 1 output layer.\n"}
{"snippet": "with tf.Session() as sess:\n    result = sess.run(neg_x)\n    print(result)\n", "intent": "You need to summon a session so you can launch the negation op:\n"}
{"snippet": "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n    result = sess.run(neg_op)\n    print(result)\n", "intent": "Now let's use a session with a special argument passed in.\n"}
{"snippet": "import tensorflow as tf\nsess = tf.InteractiveSession()\nraw_data = [1., 2., 8., -1., 0., 5.5, 6., 13]\nspikes = tf.Variable([False] * len(raw_data), name='spikes')\nspikes.initializer.run()\n", "intent": "Create an interactive session and initialize a variable:\n"}
{"snippet": "with train_graph.as_default():\n    saver = tf.train.Saver()\nwith tf.Session(graph=train_graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    embed_mat = sess.run(embedding)\n", "intent": "Restore the trained network if you need to:\n"}
{"snippet": "alpha = tf.constant(0.05)\ncurr_value = tf.placeholder(tf.float32)\nprev_avg = tf.Variable(0.)\nupdate_avg = alpha * curr_value + (1 - alpha) * prev_avg\n", "intent": "The moving average is defined as follows:\n"}
{"snippet": "init = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    for i in range(len(raw_data)):\n        summary_str, curr_avg = sess.run([merged, update_avg], feed_dict={curr_value: raw_data[i]})\n        sess.run(tf.assign(prev_avg, curr_avg))\n        print(raw_data[i], curr_avg)\n        writer.add_summary(summary_str, i)\n", "intent": "Time to compute the moving averages. We'll also run the `merged` op to track how the values change:\n"}
{"snippet": "def model(X, w):\n    return tf.multiply(X, w)\n", "intent": "Define the model as `y = w'*x`\n"}
{"snippet": "w = tf.Variable(0.0, name=\"weights\")\n", "intent": "Set up the weights variable\n"}
{"snippet": "y_model = model(X, w)\ncost = tf.reduce_mean(tf.square(Y-y_model))\n", "intent": "Define the cost function as the mean squared error\n"}
{"snippet": "sess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n", "intent": "Initialize all variables\n"}
{"snippet": "def model(X, w):\n    terms = []\n    for i in range(num_coeffs):\n        term = tf.multiply(w[i], tf.pow(X, i))\n        terms.append(term)\n    return tf.add_n(terms)\n", "intent": "Define our polynomial model\n"}
{"snippet": "w = tf.Variable([0.] * num_coeffs, name=\"parameters\")\ny_model = model(X, w)\n", "intent": "Set up the parameter vector to all zero\n"}
{"snippet": "sess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\nfor epoch in range(training_epochs):\n    for (x, y) in zip(trX, trY):\n        sess.run(train_op, feed_dict={X: x, Y: y})\nw_val = sess.run(w)\nprint(w_val)\n", "intent": "Set up the session and run the learning algorithm just as before\n"}
{"snippet": "def discriminator(x, n_units=128, reuse=False, alpha=0.01):\n    with tf.variable_scope('discriminator', reuse=reuse):\n        h1 = tf.layers.dense(x, n_units, activation=None)\n        h1 = tf.maximum(alpha * h1, h1)\n        logits = tf.layers.dense(h1, 1, activation=None)\n        out = tf.sigmoid(logits)\n        return out, logits\n", "intent": "The discriminator network is almost exactly the same as the generator network, except that we're using a sigmoid output layer.\n"}
{"snippet": "learning_rate = 0.001\ntraining_epochs = 1000\nX = tf.placeholder(\"float\")\nY = tf.placeholder(\"float\")\nw = tf.Variable([0., 0.], name=\"parameters\")\n", "intent": "Define the hyper-parameters, placeholders, and variables:\n"}
{"snippet": "y_model = model(X, w)\ncost = tf.reduce_sum(tf.square(Y-y_model))\n", "intent": "Given a model, define the cost function:\n"}
{"snippet": "sess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n", "intent": "Prepare the session:\n"}
{"snippet": "X = tf.placeholder(tf.float32, shape=(None,), name=\"x\")\nY = tf.placeholder(tf.float32, shape=(None,), name=\"y\")\nw = tf.Variable([0., 0.], name=\"parameter\", trainable=True)\ny_model = tf.sigmoid(w[1] * X + w[0])\ncost = tf.reduce_mean(-Y * tf.log(y_model) - (1 - Y) * tf.log(1 - y_model))\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n", "intent": "Define the placeholders, variables, model, cost function, and training op:\n"}
{"snippet": "X1 = tf.placeholder(tf.float32, shape=(None,), name=\"x1\")\nX2 = tf.placeholder(tf.float32, shape=(None,), name=\"x2\")\nY = tf.placeholder(tf.float32, shape=(None,), name=\"y\")\nw = tf.Variable([0., 0., 0.], name=\"w\", trainable=True)\ny_model = tf.sigmoid(-(w[2] * X2 + w[1] * X1 + w[0]))\ncost = tf.reduce_mean(-tf.log(y_model * Y + (1 - y_model) * (1 - Y)))\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n", "intent": "Define placeholders, variables, model, and the training op:\n"}
{"snippet": "x1_boundary, x2_boundary = [], []\nwith tf.Session() as sess:\n    for x1_test in np.linspace(0, 10, 20):\n        for x2_test in np.linspace(0, 10, 20):\n            z = sess.run(tf.sigmoid(-x2_test*w_val[2] - x1_test*w_val[1] - w_val[0]))\n            if abs(z - 0.5) < 0.05:\n                x1_boundary.append(x1_test)\n                x2_boundary.append(x2_test)\n", "intent": "Here's one hacky, but simple, way to figure out the decision boundary of the classifier: \n"}
{"snippet": "train_size, num_features = xs.shape\nX = tf.placeholder(\"float\", shape=[None, num_features])\nY = tf.placeholder(\"float\", shape=[None, num_labels])\nW = tf.Variable(tf.zeros([num_features, num_labels]))\nb = tf.Variable(tf.zeros([num_labels]))\ny_model = tf.nn.softmax(tf.matmul(X, W) + b)\ncost = -tf.reduce_sum(Y * tf.log(y_model))\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\ncorrect_prediction = tf.equal(tf.argmax(y_model, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n", "intent": "Again, define the placeholders, variables, model, and cost function:\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    X, names = get_dataset(sess)\n    centroids = initial_cluster_centroids(X, k)\n    i, converged = 0, False\n    while not converged and i < max_iterations:\n        i += 1\n        Y = assign_cluster(X, centroids)\n        centroids = sess.run(recompute_centroids(X, Y))\n    print(zip(sess.run(Y), names))\n", "intent": "Open a session, obtain a dataset, and cluster the data:\n"}
{"snippet": "x = tf.reshape(raw_data, shape=[-1, 24, 24, 1])\nW = tf.Variable(tf.random_normal([5, 5, 1, 32]))\nb = tf.Variable(tf.random_normal([32]))\nconv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\nconv_with_b = tf.nn.bias_add(conv, b)\nconv_out = tf.nn.relu(conv_with_b)\nk = 2\nmaxpool = tf.nn.max_pool(conv_out, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n", "intent": "Define the TensorFlow ops:\n"}
{"snippet": "from distutils.version import LooseVersion\nimport warnings\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nassert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\nprint('TensorFlow Version: {}'.format(tf.__version__))\nif not tf.test.gpu_device_name():\n    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\nelse:\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n", "intent": "This will check to make sure you have the correct version of TensorFlow and access to a GPU\n"}
{"snippet": "def conv_layer(x, W, b):\n    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n    conv_with_b = tf.nn.bias_add(conv, b)\n    conv_out = tf.nn.relu(conv_with_b)\n    return conv_out\ndef maxpool_layer(conv, k=2):\n    return tf.nn.max_pool(conv, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n", "intent": "Define helper functions for the convolution and maxpool layers:\n"}
{"snippet": "model_op = model()\ncost = tf.reduce_mean(\n    tf.nn.softmax_cross_entropy_with_logits(logits=model_op, labels=y)\n)\ntrain_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\ncorrect_pred = tf.equal(tf.argmax(model_op, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n", "intent": "Here's the cost function to train the classifier.\n"}
{"snippet": "def make_cell(state_dim):\n    return tf.contrib.rnn.LSTMCell(state_dim)\n", "intent": "Now let's make a helper function to create LSTM cells\n"}
{"snippet": "with tf.variable_scope(\"first_cell\") as scope:\n    cell = make_cell(state_dim=10)\n    outputs, states = tf.nn.dynamic_rnn(cell, input_placeholder, dtype=tf.float32)\n", "intent": "Call the function and extract the cell outputs.\n"}
{"snippet": "multi_cell = make_multi_cell(state_dim=10, num_layers=5)\noutputs5, states5 = tf.nn.dynamic_rnn(multi_cell, input_placeholder, dtype=tf.float32)\n", "intent": "Here's the helper function in action:\n"}
{"snippet": "layer   = model.layers[-4] \nweights = layer.get_weights()\nnew_kernel = np.random.normal(size=weights[0].shape)/(GRID_H*GRID_W)\nnew_bias   = np.random.normal(size=weights[1].shape)/(GRID_H*GRID_W)\nlayer.set_weights([new_kernel, new_bias])\n", "intent": "**Randomize weights of the last layer**\n"}
{"snippet": "layer = model.layers[-4] \nweights = layer.get_weights()\nnew_kernel = np.random.normal(size=weights[0].shape)/(GRID_H*GRID_W)\nnew_bias   = np.random.normal(size=weights[1].shape)/(GRID_H*GRID_W)\nlayer.set_weights([new_kernel, new_bias])\n", "intent": "**Randomize weights of the last layer**\n"}
{"snippet": "print(\"operations\")\noperations = [op.name for op in tf.get_default_graph().get_operations()]\nprint(operations)\nprint\nprint(\"variables\")\nvariables = [var.name for var in tf.all_variables()]\nprint(variables)\n", "intent": "Notice that our weights and operations defined in the `l_1` space are saved in the `l_1` directory of the graph.\n"}
{"snippet": "shape = [5, 5, 5]\ntensor = Variable(tl.tensor(rng.random_sample(shape)), requires_grad=True)\n", "intent": "Define a random tensor which we will try to decompose. We wrap our tensors in Variables so we can backpropagate through them:\n"}
{"snippet": "from distutils.version import LooseVersion\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nassert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\nprint('TensorFlow Version: {}'.format(tf.__version__))\n", "intent": "This will check to make sure you have the correct version of TensorFlow\n"}
{"snippet": "net2 = nn.Sequential()\nnet2.add(nn.Dense(128))\nnet2.add(nn.Dense(10))\nnet2.add(CenteredLayer())\n", "intent": "We can also incorporate this layer into a more complicated network, such as by using ``nn.Sequential()``.\n"}
{"snippet": "dense(nd.ones(shape=(2,10)))\n", "intent": "Now we can run through some dummy data.\n"}
{"snippet": "def get_net():\n    net = gluon.nn.Sequential()\n    with net.name_scope():\n        net.add(gluon.nn.Dense(1))\n    net.initialize()\n    return net\n", "intent": "We define a **basic** linear regression model here. This may be modified to achieve better results on Kaggle. \n"}
{"snippet": "def net(X, drop_prob=0.0):\n    h1_linear = nd.dot(X, W1) + b1\n    h1 = relu(h1_linear)\n    h1 = dropout(h1, drop_prob)\n    h2_linear = nd.dot(h1, W2) + b2\n    h2 = relu(h2_linear)\n    h2 = dropout(h2, drop_prob)\n    yhat_linear = nd.dot(h2, W3) + b3\n    return yhat_linear\n", "intent": "Now we're ready to define our model\n"}
{"snippet": "def net(X):\n    h1_linear = nd.dot(X, W1) + b1\n    h1 = relu(h1_linear)\n    h2_linear = nd.dot(h1, W2) + b2\n    h2 = relu(h2_linear)\n    yhat_linear = nd.dot(h2, W3) + b3\n    return yhat_linear\n", "intent": "Now we're ready to define our model\n"}
{"snippet": "net1 = gluon.nn.Sequential()\nwith net1.name_scope():\n    net1.add(gluon.nn.Dense(128, activation=\"relu\"))\n    net1.add(gluon.nn.Dense(64, activation=\"relu\"))\n    net1.add(gluon.nn.Dense(10))\n", "intent": "Now you might remember that up until now, we've defined neural networks (for example, a multilayer perceptron) like this:\n"}
{"snippet": "net1 = gluon.nn.Sequential()\nwith net1.name_scope():\n    net1.add(gluon.nn.Dense(128, activation=\"relu\"))\n    net1.add(gluon.nn.Dense(64, activation=\"relu\"))\n    net1.add(gluon.nn.Dense(10))\n", "intent": "So Sequential is basically a way of throwing together a Block on the fly. Let's revisit the ``Sequential`` version of our multilayer perceptron.\n"}
{"snippet": "net2 = gluon.nn.Sequential()\nwith net2.name_scope():\n    net2.add(gluon.nn.Dense(128, in_units=784, activation=\"relu\"))\n    net2.add(gluon.nn.Dense(64, in_units=128, activation=\"relu\"))\n    net2.add(gluon.nn.Dense(10, in_units=64))\n", "intent": "If we want to specify the shape manually, that's always an option. We accomplish this by using the ``in_units`` argument when adding each layer.\n"}
{"snippet": "ntokens = len(corpus.dictionary)\nmodel = RNNModel(args_model, ntokens, args_emsize, args_nhid,\n                       args_nlayers, args_dropout, args_tied)\nmodel.collect_params().initialize(mx.init.Xavier(), ctx=context)\ntrainer = gluon.Trainer(model.collect_params(), 'sgd',\n                        {'learning_rate': args_lr, 'momentum': 0, 'wd': 0})\nloss = gluon.loss.SoftmaxCrossEntropyLoss()\n", "intent": "We go on to build the model, initialize model parameters, and configure the optimization algorithms for training the RNN model.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: test_x,\n            labels_: test_y}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"./\", one_hot=False)\nwith tf.Session() as sess:\n    saver1 = tf.train.import_meta_graph('./mlp.meta')\n    saver1.restore(sess, save_path='./mlp')\n    test_acc = sess.run('accuracy:0', feed_dict={'features:0': mnist.test.images,\n                                                 'targets:0': mnist.test.labels})\n    print('Test ACC: %.3f' % test_acc)\n", "intent": "**You can restart and the notebook and the following code cells should execute without any additional code dependencies.**\n"}
{"snippet": "history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=2,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\n", "intent": "Now let's train our network for 20 epochs:\n"}
{"snippet": "smaller_model = models.Sequential()\nsmaller_model.add(layers.Dense(4, activation='relu', input_shape=(100,)))\nsmaller_model.add(layers.Dense(4, activation='relu'))\nsmaller_model.add(layers.Dense(1, activation='sigmoid'))\nsmaller_model.compile(optimizer='rmsprop',\n                      loss='binary_crossentropy',\n                      metrics=['acc'])\n", "intent": "Now let's try to replace it with this smaller network:\n"}
{"snippet": "from keras.layers import Dense\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n                    epochs=2,\n                    batch_size=128,\n                    validation_split=0.2)\n", "intent": "Let's train a simple recurrent network using an `Embedding` layer and a `SimpleRNN` layer:\n"}
{"snippet": "dnn_clf = DNNClassifier(random_state=42)\ndnn_clf.fit(X_train1, y_train1, n_epochs=2, X_valid=X_valid1, y_valid=y_valid1)\n", "intent": "Let's see if we get the exact same accuracy as earlier using this class (without dropout or batch norm):\n"}
{"snippet": "dnn_clf_bn = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n                           n_neurons=90, random_state=42,\n                           batch_norm_momentum=0.95)\ndnn_clf_bn.fit(X_train1, y_train1, n_epochs=2, X_valid=X_valid1, y_valid=y_valid1)\n", "intent": "Good, now let's use the exact same model, but this time with batch normalization:\n"}
{"snippet": "dnn_clf_5_to_9 = DNNClassifier(n_hidden_layers=4, random_state=42)\ndnn_clf_5_to_9.fit(X_train2, y_train2, n_epochs=2, X_valid=X_valid2, y_valid=y_valid2)\n", "intent": "Let's compare that to a DNN trained from scratch:\n"}
{"snippet": "import tensorflow as tf\nreset_graph()\nn_inputs = 28 * 28\nn_hidden1 = 300\nn_hidden2 = 150  \nn_hidden3 = n_hidden1\nn_outputs = n_inputs\nlearning_rate = 0.01\n", "intent": "Using Gaussian noise:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\nmodel.summary()\n", "intent": "Take couple of minutes and try to play with the number of layers and the number of parameters in the layers to get the best results. \n"}
{"snippet": "z = model_lr(X_test_lr[:20])\nprint(\"Label    :\", [label.todense().argmax() for label in Y_test_lr[:20]])\nprint(\"Predicted:\", [z[i,:].argmax() for i in range(len(z))])\n", "intent": "Now we can call it like any Python function:\n"}
{"snippet": "autoencoder.fit(x_train_noisy, x_train,\n                epochs=2,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test_noisy, x_test),\n                callbacks=[TensorBoard(log_dir='/tmp/autoencoder_denoise', \n                                       histogram_freq=0, write_graph=False)])\n", "intent": "Let's train the AutoEncoder for `100` epochs\n"}
{"snippet": "k = T.iscalar(\"k\")\nA = T.vector(\"A\")\nresult, updates = scan(fn=lambda prior_result, A: prior_result * A,\n                              outputs_info=T.ones_like(A),\n                              non_sequences=A,\n                              n_steps=k)\nfinal_result = result[-1]\npower = function(inputs=[A,k], outputs=final_result, updates=updates)\n", "intent": "Often we need to loop (for or while operation), e.g. looping over data elements in a batch. In Theano this can be done using the 'scan' operator.\n"}
{"snippet": "import tensorflow as tf\ntf.logging.set_verbosity(tf.logging.ERROR) \nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=2)]\ndnnc = tf.contrib.learn.DNNClassifier(\n  feature_columns=feature_columns,\n  hidden_units=[],\n  n_classes=2)\ndnnc\n", "intent": "**Run the cell below** to define a neural network.\n"}
{"snippet": "import tensorflow as tf\ntf.logging.set_verbosity(tf.logging.ERROR) \nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=2)]\ndnnc = tf.contrib.learn.DNNClassifier(\n  feature_columns=feature_columns,\n  hidden_units=[10, 10, 10],\n  n_classes=2)\ndnnc\n", "intent": "**Run the cell below** to define a neural network.\n"}
{"snippet": "import tensorflow as tf\nx = tf.placeholder(tf.float32, [None, 784]) \nW = tf.Variable(tf.zeros([784, 10])) \nb = tf.Variable(tf.zeros([10])) \ny = tf.nn.softmax(tf.matmul(x, W) + b)\n", "intent": "In TensorFlow, you'll define a \"graph\" of the calculations.\n"}
{"snippet": "init = tf.global_variables()\nsess = tf.Session()\nsess.run(init)\nfor i in range(1000):\n  batch_xs, batch_ys = mnist.train.next_batch(100)\n  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n", "intent": "Randomly selects 100 samples from the 55K images and calculates the gradient. Repeats 1K times.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=256, verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "parameter_pipe ={\n    'count_vector__ngram_range': ((1,1), (1,2), (1,3)), \n    'count_vector__stop_words': ('english', None),\n    'count_vector__max_features': (None, 1000, 2000, 3000),\n    'clf__max_depth': (10, 20, 30, 50, 70, None), \n    'clf__min_samples_leaf': (1, 5, 10, 15)\n}\ngrid_search_pipe = GridSearchCV(estimator = pipe, param_grid = parameter_pipe, scoring = 'accuracy')\ngrid_search_pipe.fit(X, y)\n", "intent": "Let's also perform a grid search on the former pipeline to find the best parameters to be used for CounterVectorizer and DecisionTreeClassifier\n"}
{"snippet": "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nscore = kfold_cv_rmsle(GBoost, final_train_df, y_train)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n", "intent": "Refer [here](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)\n"}
{"snippet": "data = mx.symbol.Variable('data')\nfc1  = mx.symbol.FullyConnected(data = data, name='fc1', num_hidden=128)\nact1 = mx.symbol.Activation(data = fc1, name='relu1', act_type=\"relu\")\nfc2  = mx.symbol.FullyConnected(data = act1, name = 'fc2', num_hidden = 64)\nact2 = mx.symbol.Activation(data = fc2, name='relu2', act_type=\"relu\")\nfc3  = mx.symbol.FullyConnected(data = act2, name='fc3', num_hidden=10)\nmlp  = mx.symbol.SoftmaxOutput(data = fc3, name = 'softmax')\n", "intent": "Now we can start constructing our network:\n"}
{"snippet": "feature_cols = ['TV', 'radio', 'newspaper', 'IsLarge']\nX = data[feature_cols]\ny = data.sales\nlm = LinearRegression()\nlm.fit(X, y)\nmodel_coef = zip(feature_cols, lm.coef_)\nprint('Model Coefficients:')\nfor values in model_coef:\n    print(values)  \n", "intent": "Let's redo the multiple linear regression and include the IsLarge predictor:\n"}
{"snippet": "train['vtype'] = train.vtype.map({'car':0, 'truck':1})\nfeature_cols = ['year', 'miles', 'doors', 'vtype']\nX = train[feature_cols]\ny = train.price\nfrom sklearn.tree import DecisionTreeRegressor\ntreereg = DecisionTreeRegressor(random_state=1)\ntreereg\n", "intent": "Recap: Before every split, this process is repeated for every feature, and the feature and cutpoint that produces the lowest MSE is chosen.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1e9)\nlogreg.fit(X_train, y_train)\nmodel_coef = zip(feature_cols, logreg.coef_[0])\nprint('Model Coefficients:')\nfor values in model_coef:\n    print(values)  \n", "intent": "Confirm that the coefficients make intuitive sense.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=100)\n", "intent": "[KNeighborsClassifier documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n"}
{"snippet": "from sklearn.pipeline import make_pipeline\npipe = make_pipeline(imp, knn)\n", "intent": "[make_pipeline documentation](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)\n"}
{"snippet": "from sklearn.pipeline import Pipeline\npipe = Pipeline([('imputer', imp), ('kneighborsclassifier', knn)])\n", "intent": "[Pipeline documentation](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n"}
{"snippet": "network_def_path = simple_model(28, 28, 1, num_classes)\n", "intent": "Instantiate an instance of the network.\n"}
{"snippet": "for i in range(5):\n    doc = tf_idf[i]\n    print(np.linalg.norm(doc.todense()))\n", "intent": "We can check that the length (Euclidean norm) of each row is now 1.0, as expected.\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "in sklearn\nEstimator = model\n"}
{"snippet": "data = mx.sym.Variable(\"data\")\nfc1 = mx.sym.FullyConnected(data=data, num_hidden=128, name=\"fc1\")\nbn1 = mx.sym.BatchNorm(data=fc1, name=\"bn1\")\nact1 = mx.sym.Activation(data=bn1, name=\"act1\", act_type=\"tanh\")\nfc2 = mx.sym.FullyConnected(data=act1, name=\"fc2\", num_hidden=10)\nsoftmax = mx.sym.Softmax(data=fc2, name=\"softmax\")\nbatch_size = 100\ndata_shape = (batch_size, 784)\nmx.viz.plot_network(softmax, shape={\"data\":data_shape}, node_attrs={\"shape\":'oval',\"fixedsize\":'false'})\n", "intent": "We will use a very simple 1 hidden layer BatchNorm fully connected MNIST network to demo how to use low level API.\nThe network looks like:\n"}
{"snippet": "mean_knn_n2 = KNeighborsClassifier(n_neighbors=1,\n                              weights='uniform')\naccuracy_crossvalidator(X, y, mean_knn_n2, cv_indices)\n", "intent": "---\nAs you can see the mean cross-validated accuracy is very high with 5 neighbors. \nLet's see what it's like when we use only 1 neighbor:\n"}
{"snippet": "X = affair[['age','religious']].values\nknn_uni_n3 = KNeighborsClassifier(n_neighbors=3, weights='uniform')\nknn_uni_n3.fit(X, y)\n", "intent": "---\nYou should choose **2 predictor variables** to predict had affair vs. not\n"}
{"snippet": "knn_uni_n11 = KNeighborsClassifier(n_neighbors=11, weights='uniform')\nknn_uni_n11.fit(X, y)\n", "intent": "---\nUse the same predictor variables and cv folds.\n"}
{"snippet": "def lasso_coefs(X, Y, alphas):\n    coefs = []\n    lasso_reg = Lasso()\n    for a in alphas:\n        lasso_reg.set_params(alpha=a)\n        lasso_reg.fit(X, Y)\n        coefs.append(lasso_reg.coef_)\n    return coefs\nsgd_reg_alphas = sgd_reg_gs_params['param_grid']['alpha']\nlcoefs = lasso_coefs(Xn, y, sgd_reg_alphas)\n", "intent": "---\nHow you choose to examine the results is up to you. Visualizations are always a good idea, but not entirely neccessary (or easy) in this case.\n"}
{"snippet": "slr = SimpleLinearRegression(fit_intercept=True)\nslr.fit_intercept\n", "intent": "Now, if we instantiate the class, it will assign `fit_intercept` to the class attribute `fit_intercept`, like so:\n"}
{"snippet": "lr_pipe.fit(Xtrain, ytrain)\nlr_pipe.score(Xtest, ytest)\n", "intent": "Fit the pipeline with the training data, then score it on the testing data:\n"}
{"snippet": "lda = models.LdaModel(\n    matutils.Sparse2Corpus(X, documents_columns=False),\n    num_topics  =  3,\n    passes      =  20,\n    id2word     =  vocab\n)\n", "intent": "Finally we initialize and assign our model to a variable object!\n"}
{"snippet": "k = \nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(car1)\n", "intent": "Cluster - Choose K based on plot and variables chosen\n"}
{"snippet": "nb = BernoulliNB()\n", "intent": "---\nThe model should only be built (and cross-validated) on the training data.\nCross-validate the score and compare it to baseline.\n"}
{"snippet": "best_estimator = random_search.best_estimator_                        \nh2o_model      = h2o.get_model(best_estimator._final_estimator._id)    \n", "intent": "It is useful to save constructed models to disk and reload them between H2O sessions. Here's how:\n"}
{"snippet": "g = Graph()\ng.set_as_default()\nA = Variable([[10, 20], [30, 40]])\nb = Variable([1, 1])\nx = Placeholder()\ny = matmul(A,x)\nz = add(y,b)\n", "intent": "** Looks like we did it! **\n"}
{"snippet": "def init_weights(shape):\n    init_random_dist = tf.truncated_normal(shape = shape, stddev = 0.1)\n    return tf.Variable(init_random_dist)\n", "intent": "Function to help intialize random weights for fully connected or convolutional layers, we leave the shape attribute as a parameter for this.\n"}
{"snippet": "def init_bias(shape):\n    init_bias_vals = tf.constant(shape =shape, value = 0.1)\n    return tf.Variable(init_bias_vals)\n", "intent": "Same as init_weights, but for the biases\n"}
{"snippet": "fully_connected_layer_after_dropout = tf.nn.dropout(x = fully_connected_layer_1, \n                                                    keep_prob = hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "cell = tf.contrib.rnn.OutputProjectionWrapper(\n    tf.contrib.rnn.BasicRNNCell(num_units = num_neurons, \n                                activation = tf.nn.relu), \n    output_size = num_outputs)\n", "intent": "____\n____\nPlay around with the various cells in this section, compare how they perform against each other.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./checkpoints/rnn_time_series_model\")\n    zero_seq_seed = [0. for i in range(num_time_steps)]\n    for iteration in range(len(ts_data.x_data) - num_time_steps):\n        X_batch = np.array(zero_seq_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n        y_pred = sess.run(outputs, feed_dict={X: X_batch})\n        zero_seq_seed.append(y_pred[0, -1, 0])\n", "intent": "** Note: Can give wacky results sometimes, like exponential growth**\n"}
{"snippet": "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype = tf.float32)\n", "intent": "** Now pass in the cells variable into tf.nn.dynamic_rnn, along with your first placeholder (X)**\n"}
{"snippet": "import helper\nimages, labels = next(iter(trainloader))\nimg = images[0].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits,\n               dim=1)\nhelper.view_classify(img.view(1, 28, 28),\n                     ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "input_size=1 \noutput_size=1\nhidden_dim=32\nn_layers=1\nrnn = RNN(input_size, output_size, hidden_dim, n_layers)\nprint(rnn)\n", "intent": "---\nNext, we'll instantiate an RNN with some specified hyperparameters. Then train it over a series of steps, and see how it performs.\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "intent": "Finetuning the convnet\n----------------------\nLoad a pretrained model and reset final fully connected layer.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, input_dim=x_train1.shape[1]))\nmodel.add(Activation('tanh'))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train1, y_train1, epochs=1000, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.neighbors import KernelDensity\ndef plot_kde(sample, ax, label=''):\n    xmin, xmax = 0.9 * min(sample), 1.1 * max(sample)\n    xgrid = np.linspace(xmin, xmax, 200)\n    kde = KernelDensity(kernel='gaussian').fit(sample[:, None])\n    log_dens = kde.score_samples(xgrid[:, None])\n    ax.plot(xgrid, np.exp(log_dens), label=label)\n", "intent": "We can also approximate the distribution using a kernel density estimator.  I'll use one from Scikit Learn.\n"}
{"snippet": "m1 = KNeighborsClassifier(1).fit(trainx, trainy)\n", "intent": "Compute accuracy on testing sample using both 1-nn and 5-nn classifier\n"}
{"snippet": "clf = LogisticRegression(class_weight='balanced') \n", "intent": "Notice that the resulting labels are imbalanced.\n"}
{"snippet": "pretrained_file = 'taggers/maxent_treebank_pos_tagger/english.pickle'\naccuracy_treebank_pretrained = pos_tagging_pretrained_model(W_treebank_test, Y_treebank_test, pretrained_file)\nprint('Accuracy on Test Set for TreeBank using pretrained model {0:.2f} '.format(accuracy_treebank_pretrained))\n", "intent": "b) Pre-trained Model Using NLTK Max-Entropy for TreeBank Dataset\n"}
{"snippet": "pretrained_file = 'taggers/maxent_brown_pos_tagger/english.pickle'\naccuracy_brown_pretrained = pos_tagging_pretrained_model(W_brown_test, Y_brown_test, pretrained_file)\nprint('Accuracy on Test Set for Brown Corpus using pretrained model {0:.2f} '.format(accuracy_brown_pretrained))\n", "intent": "(e) Pre-trained Model Using NLTK Max-Entropy for TreeBank Dataset\n"}
{"snippet": "size=10000\nclf.fit(X1[:size], y1[:size])\nprint('training OK')\nX1_test, y1_test = transform_to_dataset(test_sentences_X1)\nperformance1_1 = clf.score(X1_test, y1_test)\nprint(\"Accuracy:\", performance1_1)\n", "intent": "* fit the decision tree for a limited amount (size) of training \n* test data and compare with score function on testdata\n"}
{"snippet": "clf_model_b = MultinomialNB()\n", "intent": "* model b - train - [performance measures]\n* model b - validation - [performance measures]\n* model b - test - [performance measures]\n"}
{"snippet": "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=25)\n", "intent": "Train and evaluate\n^^^^^^^^^^^^^^^^^^\nIt should take around 15-25 min on CPU. On GPU though, it takes less than a\nminute.\n"}
{"snippet": "pretrained_file = 'taggers/maxent_treebank_pos_tagger/english.pickle'\naccuracy_treebank_pretrained = pos_tagging_pretrained_model(W_treebank_test, Y_treebank_test, pretrained_file)\nprint('Accuracy on Test Set for TreeBank using pretrained model',accuracy_treebank_pretrained)\n", "intent": "b) Pre-trained Model Using NLTK Max-Entropy for TreeBank Dataset\n"}
{"snippet": "pretrained_file = '/home/debayan/nltk_data/taggers/maxent_treebank_pos_tagger/PY3/english.pickle'\naccuracy_brown_pretrained = pos_tagging_pretrained_model(W_brown_test, Y_brown_test, pretrained_file)\nprint('Accuracy on Test Set for Brown Corpus using pretrained model',accuracy_brown_pretrained)\n", "intent": "(e) Pre-trained Model Using NLTK Max-Entropy for TreeBank Dataset\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier as RFC\nclf_a = RFC(criterion='entropy', random_state=4222)\nmax_size=10000\nclf_a.fit(vec_train_1[:max_size], y1[:max_size])\ntest_classifier(labels=[\"FAKE\",\"REAL\"], title=\"Configuration 1, model a -- train\", Xt=vec_train_1,yt=y1, clf=clf_a)\ncm_1 = test_classifier(labels=[\"FAKE\",\"REAL\"], title=\"Configuration 1, model a -- test\", Xt=vec_test_1,yt=yt1, clf=clf_a)\n", "intent": "* trying a Random Forest classifier \n"}
{"snippet": "from sklearn.neural_network import MLPClassifier\nclf_b = MLPClassifier(hidden_layer_sizes=(100,), random_state=4222)\nclf_b.fit(vec_train_2, y2)\n", "intent": "* trying a MLP as classifier \n"}
{"snippet": "def model_01(X,y,tX,ty, max_size=10000):\n    model01_clf = train_classifier(X,y,MLPClassifier(hidden_layer_sizes=(100,), learning_rate='adaptive'),max_size=max_size)\n    return test_classifier(clf=model01_clf, tX=tX, ty=ty)\n", "intent": "* train and testing english custom POS tagger model:\n"}
{"snippet": "nn = Sequential()\nnn.add(Embedding(max_fatures, embed_dim, input_length = X.shape[1]))\nnn.add(SimpleRNN(units=3))\nnn.add(Dense(2, activation='softmax'))\nnn.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\nprint(nn.summary())\n", "intent": "<h4>Improve the baseline by inserting RNN cells</h4>\n"}
{"snippet": "with graph.as_default():\n    with tf.name_scope('embedding_layer'):\n        embedding = tf.Variable(tf.random_uniform((n_words + 1, embed_size), -1.0, 1.0), name='embedding') \n        embed = tf.nn.embedding_lookup(embedding, inputs_)\n        tf.summary.histogram('embedding', embedding)\n", "intent": "Adding layer to map the ~74K word vocab to size 300 feature array\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_matrix = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n    embeds = tf.nn.embedding_lookup(embedding_matrix, input_data)\n    return embeds\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from distutils.version import LooseVersion\nimport warnings\nimport tensorflow as tf\ntf.reset_default_graph()\nassert LooseVersion(tf.__version__) in [LooseVersion('1.0.0'), LooseVersion('1.0.1')], 'This project requires TensorFlow version 1.0  You are using {}'.format(tf.__version__)\nprint('TensorFlow Version: {}'.format(tf.__version__))\nif not tf.test.gpu_device_name():\n    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\nelse:\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n", "intent": "This will check to make sure you have the correct version of TensorFlow and access to a GPU\n"}
{"snippet": "x = torch.randn(3)\nx = Variable(x, requires_grad=True)\ny = x * 2\nwhile y.data.norm() < 1000:\n    y = y * 2\nprint(y)\n", "intent": "You can do many crazy things with autograd!\n"}
{"snippet": "RF = RandomForestRegressor(n_estimators = 10000, \n                           max_features = 4,     \n                           min_samples_leaf = 5, \n                           oob_score = True)      \nRF.fit(X,y)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n"}
{"snippet": "for i in [X2,X4]:\n    lm1 = smf.ols(formula='y ~ i', data=BostonData).fit()\n    print(lm1.summary())\n", "intent": "Answer: we will most likely use model 2 or 4. If both of these models are significant, then we use model 4. \n"}
{"snippet": "for i in [X12,X12,X14,X15]:\n    lm1 = smf.ols(formula='y ~ i', data=BostonData).fit()\n    print(lm1.summary())\n", "intent": "Answer: if our goal is prediction, we will most like use models 12, 13, 14 or 15. Let's test to see which one is the best!\n"}
{"snippet": "with tf.Session(graph=graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: test_x,\n            labels_: test_y}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embed_values = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1,1), name=\"word_embedding\")\n    embed_lookup = tf.nn.embedding_lookup(embed_values, input_data)\n    return embed_lookup\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X, y)\n", "intent": "Score and plot your predictions.\n"}
{"snippet": "x = Variable(torch.ones(2, 2), requires_grad = True)\nx  \n", "intent": "$X = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1\\end{bmatrix}$, $Z = 2 * (X + 2) ^ 2$, $out = \\bar{Z}$\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embedding,ids = input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf_iris = KNeighborsClassifier()\nclf_iris.fit(X_ir[:,2:], y_ir)\n", "intent": "As the description of the data says petal length and width are the main indicators. Let's build a model based on them and plot the results.\n"}
{"snippet": "outputs = net(Variable(images))\n", "intent": "Okay, now let us see what the neural network thinks these examples above are:\n"}
{"snippet": "poly15 = PolynomialRegression(15)\npoly15.fit(X_train, y_train)\nplot_regr(X_train, y_train, poly15)\n", "intent": "This is much better, so we might try to go to even higher polynomials.\n"}
{"snippet": "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\ngradients = tf.gradients(loss, [w, u, b])\nclipped_gradients, norm = tf.clip_by_global_norm(gradients, .1)\ntrain_step = optimizer.apply_gradients(zip(clipped_gradients, [u,w,b]))\n", "intent": "<span style=\"color:red\">\\** Note this step is different from previous ones </span>\n"}
{"snippet": "def forewardPass(data):\n    x = sigmoid(np.dot(data, backwardB.getWeights().transpose()) + backwardB.getBias())\n    return x\n", "intent": "foreward pass: $\\mathbb{R}^{n \\times 30} \\rightarrow \\mathbb{R}^n$\n"}
{"snippet": "def forewardMNIST(data):\n    hidden = sigmoid(np.dot(data, backwardMNIST.getWeights(0)) + backwardMNIST.getBias(0))\n    out = softmax(np.dot(hidden, backwardMNIST.getWeights(1)) + backwardMNIST.getBias(1))\n    return out\ndef getClass(data):\n    proba = forewardMNIST(data)\n    return np.argmax(proba, axis=1)\n", "intent": " - foreward pass: $\\mathbb{R}^{n \\times 784} \\rightarrow \\mathbb{R}^{n \\times 10}$\n"}
{"snippet": "conv_layer = tf.nn.max_pool(\n    conv_layer,\n    ksize=[1, 2, 2, 1],\n    strides=[1, 2, 2, 1],\n    padding='SAME')\n", "intent": "TensorFlow provides the tf.nn.max_pool() function to apply max pooling to your convolutional layers.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(params=embedding, ids=input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from distutils.version import LooseVersion\nimport warnings\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nassert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\nprint('TensorFlow Version: {}'.format(tf.__version__)) \nif not tf.test.gpu_device_name():\n    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\nelse:\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n", "intent": "This will check to make sure you have the correct version of TensorFlow and access to a GPU\n"}
{"snippet": "model6_entropy = DecisionTreeClassifier(criterion = \"gini\", min_samples_split=3, random_state=100, max_leaf_nodes=None, max_depth=2, min_samples_leaf=8, splitter='best')\nmodel6_entropy.fit(X_train, y_train)\n", "intent": "- Experiment 1 - Adjusting the DecisionTree depth parameter\n"}
{"snippet": "model12_entropy = DecisionTreeClassifier(criterion = \"gini\", min_samples_split=7, random_state=100, max_leaf_nodes=19, max_depth=6, min_samples_leaf=13, splitter='best')\nmodel12_entropy.fit(X_train, y_train)\n", "intent": "- Experiment 3 - Adjusting the min_samples_split and min_samples_leaf parameters\n"}
{"snippet": "correct = 0\ntotal = 0\nfor data in testloader:\n    images, labels = data\n    outputs = net(Variable(images))\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\nprint('Accuracy of the network on the 10000 test images: %d %%' % (\n    100 * correct / total))\n", "intent": "The results seem pretty good.\nLet us look at how the network performs on the whole dataset.\n"}
{"snippet": "a, b = 1.5, 2.5\ne_out = graph(a,b)\nprint(e_out)\n", "intent": "Now, we can call this function to execute the computation graph given some inputs `a,b`:\n"}
{"snippet": "with ShowAndTellModel(train_dir='/content/datalab/img2txt/train',\n                      vocab_file='/content/datalab/img2txt/vocab.yaml') as m:\n    m.show_and_tell('gs://bradley-sample-notebook-data/chopin_vivaldi.jpg')\n    m.show_and_tell('gs://bradley-sample-notebook-data/vivaldi_chopin_tail.jpg')\n    m.show_and_tell('gs://bradley-sample-notebook-data/vivaldi.jpg')  \n", "intent": "For fun, I would give it a try on pictures of my cats!\n"}
{"snippet": "LSTM_SIZE=128\ndef lstm_model(batch_size, train_data, targets, mode):\n", "intent": "Let's try an LSTM based sequential model and see if it can beat DNN.\n"}
{"snippet": "from sklearn import linear_model\nols = linear_model.LinearRegression()\nols.fit(x, y)\n", "intent": "- Create a LinearRegression instance and use `fit()` function to fit the data.\n"}
{"snippet": "ols.fit(X_train, y_train)\nprint \"R^2 for training set:\",\nprint ols.score(X_train, y_train)\nprint '-'*50\nprint \"R^2 for test set:\",\nprint ols.score(X_test, y_test)\n", "intent": "- Do multiple linear regression with new data set.\n- Report the coefficient of determination from the training and testing sets.\n"}
{"snippet": "from sklearn import linear_model\nlogit_1 = linear_model.LogisticRegression()\n", "intent": "- The implementation of logistic regression in scikit-learn can be accessed from class **`LogisticRegression`**.\n"}
{"snippet": "logit_2 = linear_model.LogisticRegression()\nlogit_2.set_params(C=1e4)\nlogit_2.fit(x_tm2, y_tm)\nprint [logit_2.coef_, logit_2.intercept_]\n", "intent": "- Below we see that the boundary is not affected that much by outliers with logistic regression.\n"}
{"snippet": "logit = linear_model.LogisticRegression(C=1e4)\nlogit.fit(scores, decision)\nlogit.score(scores, decision)\n", "intent": "Build a logistic regression model **`logit`** with the data.\n- What's the score?\n"}
{"snippet": "logit.fit(iris_x, iris_y)\n", "intent": "<p><a name=\"ex2\"></a></p>\n- Agian we first create and fit the logistic regression:\n"}
{"snippet": "for data in rand_loader:\n    if torch.cuda.is_available():\n        input_var = Variable(data.cuda())\n    else:\n        input_var = Variable(data)\n    output = model(input_var)\n    print(\"Outside: input size\", input_var.size(),\n          \"output_size\", output.size())\n", "intent": "Run the Model\n-------------\nNow we can see the sizes of input and output tensors.\n"}
{"snippet": "LDA.fit(iris.data, iris.target)\nLDA.score(iris.data, iris.target)\n", "intent": "- Fit a LDA model with all features of iris dataset, what's your overall accuracy?\n"}
{"snippet": "QDA = discriminant_analysis.QuadraticDiscriminantAnalysis()\nQDA.fit(iris.data, iris.target)\n", "intent": "- Create a QDA model and train it with the iris data.\n"}
{"snippet": "from sklearn import naive_bayes\ngnb = naive_bayes.GaussianNB()\ngnb.fit(iris.data, iris.target)\nprint gnb.score(iris.data, iris.target)\n", "intent": "<p><a name=\"gnb-sklearn\"></a></p>\n**Exercise**\nWe will work on the iris dat. Fit a Gaussian Naive Bayes and print out the accuracy:\n"}
{"snippet": "mnb = naive_bayes.MultinomialNB()\nmnb.fit(x, y)\nprint \"The score of multinomial naive bayes is: %.4f\" %mnb.score(x, y)\n", "intent": "- Create a multinomial naive Bayes model and train it with the data above. What is the accuracy of the model?\n"}
{"snippet": "bnb = naive_bayes.BernoulliNB()\nbnb.fit(iris.data, iris.target)\nprint \"The mean accuracy of Bernoulli Naive Bayes is: \" + str(bnb.score(iris.data, iris.target))\n", "intent": "- Train a BNB model with the data. REport the accuracy.\n"}
{"snippet": "ols.fit(x, y)\nprint \"Coefficeints with Entire dataset: \", ols.coef_\nprint \"Bootstrap estimates: \", np.mean(coefs, 0)\n", "intent": "- The coefficients of the model fitting with the whole data set are:\n"}
{"snippet": "from sklearn import linear_model\nridge = linear_model.Ridge()\n", "intent": "Here is an example showing how the coefficients vary as the parameter $\\alpha$ increases.\n"}
{"snippet": "from sklearn import svm\nsvm_model = svm.SVC(kernel ='poly', C = 1e5, degree=1)\n", "intent": "<p><a name=\"svm-sklearn\"></a></p>\n- In order to implement SVM in python, import **svm** module from sklearn library.\n"}
{"snippet": "svm_model.set_params(degree=3)\nsvm_model.fit(iris.data[index, 0:2], iris.target[index])\nplotModel(svm_model, iris.data[index, 0], iris.data[index, 1], iris.target[index])\npl.xlabel('Sepal Length')\npl.ylabel('Sepal Width')\n", "intent": "This time we set *degree=3*, which result in a cubic boundary:\n"}
{"snippet": "x = torch.Tensor(5, 3)\nprint(x)\n", "intent": "Construct a 5x3 matrix, uninitialized:\n"}
{"snippet": "svm_model.set_params(C=1)\nsvm_model.fit(iris.data[:, 2:4], iris.target) \nsvm_model.score(iris.data[:, 2:4], iris.target) \n", "intent": "Below we change to a different constant `C` (reset `C = 1` from `C=1e5` to decrease the effect of penalty)\n"}
{"snippet": "from sklearn import tree\ntree_model = tree.DecisionTreeClassifier()\n", "intent": "The function **tree.DecisionTreeClassifier** in sklearn can be used to implement decision tree.\n"}
{"snippet": "plotModel(tree_model, iris.data[:, 2], iris.data[:, 3], iris.target)\npl.xlabel('Petal Length')\npl.ylabel('Petal Width')\n", "intent": "We visualize the decision boundary of the tree:\n"}
{"snippet": "tree_model.fit(x_train, y_train)\nprint \"The training error is: %.5f\" %(1-tree_model.score(x_train, y_train))\nprint \"The test     error is: %.5f\" %(1-tree_model.score(x_test, y_test))\n", "intent": "Fit on the training data derictly:\n"}
{"snippet": "np.random.seed(1)\nrandomForest.fit(iris.data[:, 2:4], iris.target) \nrandomForest.score(iris.data[:, 2:4], iris.target) \n", "intent": "We use all the observations and the last two features, \"petal length\" and \"petal width\", in the iris data to build a decision-tree.\n"}
{"snippet": "np.random.seed(1)\ngrid_para_forest = [{\"n_estimators\": [10, 50, 100], \"criterion\": [\"gini\", \"entropy\"], \\\n                    \"min_samples_leaf\": range(1, 10), \"min_samples_split\": np.linspace(2, 30, 15)}]\ngrid_search_forest = gs.GridSearchCV(randomForest, grid_para_forest, scoring='accuracy', cv=5)\ngrid_search_forest.fit(x_train, y_train)\n", "intent": "<p><a name=\"3case3\"></a></p>\nAs for decision tree, we also need to decide several parameters by grid search. It might take longer for this one.\n"}
{"snippet": "from sklearn.cluster import KMeans\nkmeans = KMeans()\n", "intent": "<p><a name=\"kmeans\"></a></p>\nImport the class *KMeans* from the module *cluster* in the sklearn library.\n"}
{"snippet": "kmeans.set_params(n_clusters = 2)\nkmeans.fit(x)\n", "intent": "- Set the parameter *n_clusters* to 2, and fit the model.\n"}
{"snippet": "kmeans.fit(iris.data2)\nlabels = kmeans.labels_\nlabels\n", "intent": "- Cluster the data by kmeans algorithm. Print the labels of the data.\n"}
{"snippet": "result = torch.Tensor(5, 3)\ntorch.add(x, y, out=result)\nprint(result)\n", "intent": "Addition: providing an output tensor as argument\n"}
{"snippet": "pca.set_params(n_components = 3)\npca.fit(data)\n", "intent": "Fit a PCA model on this data set.\n"}
{"snippet": "import sklearn.linear_model as lm\nlogit = lm.LogisticRegression()\ny_train2num = [1 if i == 'email' else 0 for i in y_train]\nlogit.fit(x_train2, y_train2num)\n", "intent": "- We can now build a model using the principal components. In this case we'll make a logistic regression.\n"}
{"snippet": "n_estimators=list(range(100,1100,100))\nlearn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\nhyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\ngd=GridSearchCV(estimator=GradientBoostingClassifier(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)\n", "intent": "We will try to increase it with Hyper-Parameter Tuning\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.get_variable(\"embedding\", shape=[vocab_size, embed_dim], dtype=tf.float32)\n    return tf.nn.embedding_lookup(\n        embeddings,\n        input_data\n    )\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "dtree = DecisionTreeClassifier(max_depth=11, min_samples_leaf=1, class_weight={0:1, 1:7})\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "kmeans.fit(colleges.drop('Private', axis = 1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "scaler.fit(secret.drop('TARGET CLASS', axis = 1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "lr = LogisticRegression()\nlr.fit(X_train, y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn import svm\nmethod = svm.SVC(kernel='rbf', C=100.)\nmethod.fit(x_train, t_train)\n", "intent": "We are going to make a first attempt at classification using a Support Vector classifier with radial basis functions.\n"}
{"snippet": "target = Variable(torch.LongTensor([3]))\nloss_fn = nn.CrossEntropyLoss()  \nerr = loss_fn(out, target)\nerr.backward()\nprint(err)\n", "intent": "Define a dummy target label and compute error using a loss function.\n"}
{"snippet": "import networkx as nx\nG = nx.cycle_graph(10)\nA = nx.adjacency_matrix(G)\nprint(A.todense())\n", "intent": "Creating adjacency matrix\n"}
{"snippet": "from sklearn.ensemble import IsolationForest\nisolation_forest = IsolationForest(max_samples=100, random_state=8451)\niso_fit = isolation_forest.fit(X_train)\noutlier_ratings = iso_fit.decision_function(X_train)\noutlier_ratings[:50]\n", "intent": "We'll use a method called Isolation Forest to find outliers.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nmodel = lin_reg.fit(X=X_train, y=y_train)\nprint('Coef: %s' % str(model.coef_))\nprint('Intercept: %s' % str(model.intercept_))\n", "intent": "For the sake of simplicity, let's just do a linear regression.\n"}
{"snippet": "coefficients_1 = train_model(model_1, param_1, X, y)\ncoefficients_2 = train_model(model_2, param_2, X, y)\n", "intent": "And suppose we train the model using a train_model function like this:\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: test_x,\n            labels_: test_y,\n            keep_prob: 1.0}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "valid_rows = data[~data.bmi.isnull()]\ny = valid_rows['bmi']\nvalid_rows1 = valid_rows.drop(['id','bmi'],axis=1)\nvalid_rows1 = sm.add_constant(valid_rows1)\nm = sm.OLS(y,valid_rows1)\nres = m.fit()\nres.summary()\n", "intent": "The distribution is distorted in comparison with original.\n"}
{"snippet": "model = uncertain_gallup_model(gallup_2012)\nprint model.head()\nmodel = model.join(electoral_votes)\n", "intent": "We construct the model by estimating the probabilities:\n"}
{"snippet": "clf.fit(digits.data[:-10], digits.target[:-10])\n", "intent": "The estimator is trained from the learning set using its ``.fit`` method.\n"}
{"snippet": "from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=2, random_state=0).fit(data.T)\nsegmentation = kmeans.labels_.reshape(im.shape[:-1])\n", "intent": "Then we create a ``KMeans`` estimator for two clusters.\n"}
{"snippet": "import torch\na = torch.FloatTensor(5, 7)\n", "intent": "Tensors\n=======\nTensors behave almost exactly the same way in PyTorch as they do in\nTorch.\nCreate a tensor of size (5 x 7) with uninitialized memory:\n"}
{"snippet": "from clipper_admin.deployers import pytorch as pytorch_deployer\npytorch_deployer.deploy_pytorch_model(\n    clipper_conn,\n    name=\"pytorch-model\",\n    version=1, \n    input_type=\"bytes\", \n    func=predict_torch_model,\n    pytorch_model=model,\n)\n", "intent": "> *Once again, Clipper must download this Docker image from the internet, so this may take a minute. Thanks for your patience.*\n"}
{"snippet": "best_model = get_best_model(make_model, trials, metric=\"mean_accuracy\")\n", "intent": "**Exercise**: Now, let's get the best model from your search process, and check its validation accuracy compared to the first model we created above.\n"}
{"snippet": "import torch\nimport torchvision\nfrom torchvision.datasets import FashionMNIST\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\ndata_transform = transforms.ToTensor()\ntest_data = FashionMNIST(root='./data', train=False,\n                                  download=True, transform=data_transform)\nprint('Test data, number of images: ', len(test_data))\n", "intent": "In this cell, we load in just the **test** dataset from the FashionMNIST class.\n"}
{"snippet": "autoencoder = Model(inp_img, out_img)\n", "intent": "We declare the functional model format, passing both inputs and ouputs. \n"}
{"snippet": "def conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\ndef conv_layer(input, shape):\n    W = weight_variable(shape)\n    b = bias_variable([shape[3]])\n    return tf.nn.relu(conv2d(input, W) + b)\n", "intent": "The next step is to define the layers that we will use in our model. \n"}
{"snippet": "sess = tf.Session()\nouts = sess.run(f)\nsess.close()\nprint('Output is {}'.format(outs))\n", "intent": "~~~python \ntf.Session()\n~~~\n"}
{"snippet": "with tf.Session() as sess:\n    outs = sess.run(f)\nprint('Output is {}'.format(outs))\n", "intent": "~~~python \nwith tf.Session()\n~~~\n"}
{"snippet": "with tf.Graph().as_default():\n    c1 = tf.constant(4,dtype=tf.float64,name='c') \n    with tf.name_scope(\"prefix_name\"):\n        c2 = tf.constant(4,dtype=tf.int32,name='c') \n        c3 = tf.constant(4,dtype=tf.float64,name='c')\nprint(c1.name)\nprint(c2.name)\nprint(c3.name)\n", "intent": "Sometimes when dealing with large complicated graphs, we would like to create some node grouping to make it easier to follow and manage. \n"}
{"snippet": "x = tf.Variable(0, name='x')\ninit = tf.global_variables_initializer()\nprint('pre-run variable:\\n{}'.format(x))\nprint('===============================')\nwith tf.Session() as sess:\n    sess.run(init)\n    val = sess.run(x)\n    print('post-run variable:\\n{}'.format(val))\n", "intent": "Variables are Tensor objects that we use to store and update model parameters.\n"}
{"snippet": "V_data = [1., 2., 3.]\nV = torch.Tensor(V_data)\nprint(V)\nM_data = [[1., 2., 3.], [4., 5., 6]]\nM = torch.Tensor(M_data)\nprint(M)\nT_data = [[[1., 2.], [3., 4.]],\n          [[5., 6.], [7., 8.]]]\nT = torch.Tensor(T_data)\nprint(T)\n", "intent": "Creating Tensors\n~~~~~~~~~~~~~~~~\nTensors can be created from Python lists with the torch.Tensor()\nfunction.\n"}
{"snippet": "x = [vocab.toks2idxs('the most'.split())]\nfor n in range(10):\n    x_fw = Variable(torch.from_numpy(np.array(x))).cuda()\n    logits_fw = net.forward(x_fw, 'forward')\n    logits = nn.functional.log_softmax(logits_fw, 2).cpu().data.numpy()\n    logits = logits[0][-1]\n    p = np.exp(logits) / np.sum(np.exp(logits))\n    new_tok_ind = np.argmax(np.random.multinomial(1, p - 1e-9))\n    x[0].append(new_tok_ind)\nprint(vocab.idxs2toks(x[0]))\n", "intent": "Now try to sample from the model forward direction network given short initial phrase like \"the most\"\n"}
{"snippet": "lr.fit(X_train_level2, y_train_level2)\n", "intent": "Fit a linear regression model to the meta-features. Use the same parameters as in the model above.\n"}
{"snippet": "clf = DecisionTreeClassifier(max_depth=1)\nclf.fit(X_train, y_train)\nprint ('Accuracy for a single decision stump: {}'.format(clf.score(X_test, y_test)))\n", "intent": "The datast is really simple and can be solved with a single decision stump.\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators=500, max_depth=4, n_jobs=-1)\nrf.fit(X_train, y_train)\n", "intent": "**Step 1:** first fit a Random Forest to the data. Set `n_estimators` to a high value.\n"}
{"snippet": "class Seq2SeqModel(object):\n    pass\n", "intent": "Let us use TensorFlow building blocks to specify the network architecture.\n"}
{"snippet": "logreg = LogisticRegression(solver='liblinear')\nC_vals = [0.0001, 0.001, 0.01, 0.1, .15, .25, .275, .33, 0.5, .66, 0.75, 1.0, 2.5, 5.0, 10.0, 100.0, 1000.0]\npenalties = ['l1','l2']\ngs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals},\\\n                  verbose=False, cv=15)\ngs.fit(X, y)\n", "intent": "Then will pass those to GridSearchCV\n"}
{"snippet": "model_file_name = \"locally-trained-xgboost-model\"\nbt._Booster.save_model(model_file_name)\n", "intent": "Note that the model file name must satisfy the regular expression pattern: `^[a-zA-Z0-9](-*[a-zA-Z0-9])*;`. The model file also need to tar-zipped. \n"}
{"snippet": "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'}\ncontainer = containers[boto3.Session().region_name]\n", "intent": "This involves creating a SageMaker model from the model file previously uploaded to S3.\n"}
{"snippet": "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.libsvm')).upload_file('train.libsvm')\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.libsvm')).upload_file('validation.libsvm')\n", "intent": "Now we'll copy the file to S3 for Amazon SageMaker's managed training to pickup.\n"}
{"snippet": "x = torch.Tensor([1., 2., 3.])\ny = torch.Tensor([4., 5., 6.])\nz = x + y\nprint(z)\n", "intent": "Operations with Tensors\n~~~~~~~~~~~~~~~~~~~~~~~\nYou can operate on tensors in the ways you would expect.\n"}
{"snippet": "writer = tf.summary.FileWriter(LOGDIR)\nwriter.add_graph(sess.graph)\ntf.summary.histogram('m', m)\ntf.summary.histogram('b', b)\ntf.summary.scalar('loss', loss)\nsummary_op = tf.summary.merge_all()\n", "intent": "Step 5) Set up TensorBoard\n"}
{"snippet": "biases = tf.Variable(tf.zeros([num_classes]))\n", "intent": "The second variable that must be optimized is called `biases` and is defined as a 1-dimensional tensor (or vector) of length `num_classes`.\n"}
{"snippet": "net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n", "intent": "We now do the max-pooling on the output of the convolutional layer. This was also described in more detail in Tutorial \n"}
{"snippet": "net = tf.layers.conv2d(inputs=net, name='layer_conv2', padding='same',\n                       filters=36, kernel_size=5, activation=tf.nn.relu)\n", "intent": "We now add the second convolutional layer which has 36 filters each with 5x5 pixels, and a ReLU activation function again.\n"}
{"snippet": "net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n", "intent": "The output of the second convolutional layer is also max-pooled for down-sampling the images.\n"}
{"snippet": "net = tf.layers.dense(inputs=net, name='layer_fc1',\n                      units=128, activation=tf.nn.relu)\n", "intent": "We can now add fully-connected layers to the neural network. These are called *dense* layers in the Layers API.\n"}
{"snippet": "y_pred = tf.nn.softmax(logits=logits)\n", "intent": "We use the softmax function to 'squash' the outputs so they are between zero and one, and so they sum to one.\n"}
{"snippet": "def new_fc_layer(input,          \n                 num_inputs,     \n                 num_outputs,    \n                 use_relu=True): \n    weights = new_weights(shape=[num_inputs, num_outputs])\n    biases = new_biases(length=num_outputs)\n    layer = tf.matmul(input, weights) + biases\n    if use_relu:\n        layer = tf.nn.relu(layer)\n    return layer\n", "intent": "The following helper-function creates a fully-connected layer.\n"}
{"snippet": "global_step = tf.Variable(initial_value=0,\n                          name='global_step', trainable=False)\n", "intent": "Create a variable for keeping track of the number of optimization iterations performed.\n"}
{"snippet": "x = torch.randn((2, 2))\ny = torch.randn((2, 2))\nz = x + y  \nvar_x = autograd.Variable(x, requires_grad=True)\nvar_y = autograd.Variable(y, requires_grad=True)\nvar_z = var_x + var_y\nprint(var_z.grad_fn)\nvar_z_data = var_z.data  \nnew_var_z = autograd.Variable(var_z_data)\nprint(new_var_z.grad_fn)\n", "intent": "Understanding what is going on in the block below is crucial for being a\nsuccessful programmer in deep learning.\n"}
{"snippet": "net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n", "intent": "After the convolution we do a max-pooling which is also described in Tutorial \n"}
{"snippet": "net = tf.layers.conv2d(inputs=net, name='layer_conv2', padding='same',\n                       filters=36, kernel_size=5, activation=tf.nn.relu)\n", "intent": "Then we make a second convolutional layer, also with max-pooling.\n"}
{"snippet": "net = tf.contrib.layers.flatten(net)\n", "intent": "The output then needs to be flattened so it can be used in fully-connected (aka. dense) layers.\n"}
{"snippet": "net = tf.layers.dense(inputs=net, name='layer_fc1',\n                      units=128, activation=tf.nn.relu)\n", "intent": "We can now add fully-connected (or dense) layers to the neural network.\n"}
{"snippet": "x = tf.get_variable(name=\"x\", shape=[], dtype=tf.float32, initializer=tf.zeros_initializer)\n", "intent": "To define TensorFlow variables, use the `get_variable()` function as follows:\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Useful in Jupyter Notebooks\n"}
{"snippet": "lap_graph = tf.Graph()\nwith lap_graph.as_default():\n    lap_in = tf.placeholder(np.float32, name='lap_in')\n    lap_out = lap_normalize(lap_in)\nshow_graph(lap_graph)\n", "intent": "We did all this in TensorFlow, so it generated a computation graph that we can inspect.\n"}
{"snippet": "l = tf.Variable(\"local_cpu\")\nl.device\n", "intent": "Each Variable is assigned to a specific device.\n"}
{"snippet": "for ps in param_servers:\n    with tf.device(ps):\n        v = tf.Variable(\"my_var\")\nv.device\n", "intent": "We can enforce the assigned device using the `tf.device` context.\n"}
{"snippet": "def negative_log_likelihood(X, y, w):\n    scores = sigmoid(np.dot(X, w))\n    nll = -np.sum(y*np.log(scores+1e-15) + (1-y)*np.log(1-scores+1e-15))\n    print(y)\n    print(nll)\n    return nll\n", "intent": "As defined in Eq. 33\n"}
{"snippet": "import numpy as np\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\nwith tf.Session(\"grpc://clustered-tensorflow-worker0:2222\", graph=graph) as session:\n    result = session.run(final_result, feed_dict={ input_array: np.ones([1000]) }, options=run_options)\n    print(result)\n", "intent": "We can now run the graph \n"}
{"snippet": "with tf.Session() as sess:\n    get_shape = tf.shape([[[1, 2, 3], [1, 2, 3]],\n                          [[2, 4, 6], [2, 4, 6]],\n                          [[3, 6, 9], [3, 6, 9]],\n                          [[4, 8, 12], [4, 8, 12]]])\n    shape = sess.run(get_shape)\n    print(\"Shape of tensor: \" + str(shape))\n", "intent": "You can use the `tf.shape` Operation to get the shape value of `Tensor` objects:\n"}
{"snippet": "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n    with tf.device(\"/gpu:0\"):\n      result = sess.run(product,feed_dict={matrix1: [[3., 3.]], matrix2: [[6.],[6.]]})\n      print result\n", "intent": "Sessions must be closed to release resources.  We may use the 'with' syntax to close sessions automatically when completed.\n"}
{"snippet": "w = tf.Variable(np.random.normal(), name=\"W\")\n", "intent": "We also create a variable for the weights and note that a NumPy array is convertible to a Tensor.\n"}
{"snippet": "W_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\nkeep_prob = tf.placeholder(\"float\")\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n", "intent": "Regularization / Dropout Layer Avoids Overfitting\n"}
{"snippet": "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n", "intent": "Softmax Layer Produces Class Probabilities\n"}
{"snippet": "sess.close()\nops.reset_default_graph()\nfrom tensorflow.models.rnn import rnn_cell, seq2seq\nsess = tf.InteractiveSession()\nseq_length = 5\nbatch_size = 64\nvocab_size = 7\nembedding_dim = 50\nmemory_dim = 100\n", "intent": "In the next example, we demonstrate an autoencoder which learns a lower-dimensional representation of sequential input data.\n"}
{"snippet": "X_batch = [np.random.choice(vocab_size, size=(seq_length,), replace=False)\n           for _ in range(10)]\nX_batch = np.array(X_batch).T\nfeed_dict = {enc_inp[t]: X_batch[t] for t in range(seq_length)}\ndec_outputs_batch = sess.run(dec_outputs, feed_dict)\nprint(X_batch)\n[logits_t.argmax(axis=1) for logits_t in dec_outputs_batch]\ntmp_def = rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\nshow_graph(tmp_def) \n", "intent": "We can now test our lower dimensional autoencoder by passing data through the embedding to determine if the similar input was recovered.\n"}
{"snippet": "NUM_STEPS = 10000\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\n", "intent": "Next, create a session and initialize the graph variables. We'll also set the number of steps we'll train with.\n"}
{"snippet": "def get_gradient(X, y, w, mini_batch_indices, lmbda):\n    n_batch = mini_batch_indices.shape[0]\n    nll_gradient = np.dot(X[mini_batch_indices].T,sigmoid(np.dot(X[mini_batch_indices], w)) - y[mini_batch_indices])\n    ones = np.ones(w.shape)\n    ones[0] = 0\n    reg_gradient = lmbda * ones * w\n    grad = nll_gradient / n_batch + reg_gradient  \n    return grad\n", "intent": "Make sure that you compute the gradient of the loss function $\\mathcal{L}(\\mathbf{w})$ (not simply the NLL!)\n"}
{"snippet": "age = tf.contrib.layers.real_valued_column(\"age\")\neducation_num = tf.contrib.layers.real_valued_column(\"education_num\")\ncapital_gain = tf.contrib.layers.real_valued_column(\"capital_gain\")\ncapital_loss = tf.contrib.layers.real_valued_column(\"capital_loss\")\nhours_per_week = tf.contrib.layers.real_valued_column(\"hours_per_week\")\n", "intent": "Second, configure the real-valued columns.\n"}
{"snippet": "age_buckets = tf.contrib.layers.bucketized_column(age,\n            boundaries=[ 18, 25, 30, 35, 40, 45, 50, 55, 60, 65 ])\neducation_occupation = tf.contrib.layers.crossed_column([education, occupation], hash_bucket_size=int(1e4))\nage_race_occupation = tf.contrib.layers.crossed_column( [age_buckets, race, occupation], hash_bucket_size=int(1e6))\ncountry_occupation = tf.contrib.layers.crossed_column([native_country, occupation], hash_bucket_size=int(1e4))\n", "intent": "We do a few combined features (feature crosses) here. You can add your own to improve on the model!\n"}
{"snippet": "with tf.Session():\n    a = tf.constant(10)\n    b = tf.constant(20)\n    c=a+b\n    print c\n    print(c.eval())\n", "intent": "Put everything in a session, no need to call session, just use .eval()\n"}
{"snippet": "tf.logging.set_verbosity(tf.logging.INFO)\nshutil.rmtree('taxi_trained', ignore_errors=True) \nmodel = tf.contrib.learn.LinearRegressor(\n      feature_columns=make_feature_cols(), model_dir='taxi_trained')\nmodel.fit(input_fn=make_input_fn(df_train), steps=10);\n", "intent": "<h3> Linear Regression with tf.learn Estimators framework </h3>\n"}
{"snippet": "tf.logging.set_verbosity(tf.logging.INFO)\nshutil.rmtree('taxi_trained', ignore_errors=True) \nmodel = tf.contrib.learn.DNNRegressor(hidden_units=[32, 8, 2],\n      feature_columns=make_feature_cols(), model_dir='taxi_trained')\nmodel.fit(input_fn=make_input_fn(df_train), steps=100);\nprint_rmse(model, 'validation', make_input_fn(df_valid))\n", "intent": "<h3> Deep Neural Network regression </h3>\n"}
{"snippet": "tf.logging.set_verbosity(tf.logging.INFO)\nshutil.rmtree('taxi_trained', ignore_errors=True) \nmodel = tf.contrib.learn.LinearRegressor(\n      feature_columns=feature_cols, model_dir='taxi_trained')\nmodel.fit(input_fn=get_train());\n", "intent": "<h2> Create and train the model </h2>\nNote that we no longer have a num_steps variable.  get_train() specifies a num_epochs.\n"}
{"snippet": "INPUT_COLUMNS = [\n    layers.real_valued_column('pickuplon'),\n    layers.real_valued_column('pickuplat'),\n    layers.real_valued_column('dropofflat'),\n    layers.real_valued_column('dropofflon'),\n    layers.real_valued_column('passengers'),\n]\nfeature_cols = INPUT_COLUMNS\n", "intent": "<h2> Create features out of input data </h2>\nFor now, pass these through.  (same as previous lab)\n"}
{"snippet": "forest_params = {'max_depth': range(10, 21),\n                'max_features': range(5, 105, 10)}\nlocally_best_forest = GridSearchCV (RandomForestClassifier(), cv=3, param_grid=forest_params)\nlocally_best_forest.fit(X_train, y_train)\n", "intent": "Train a random forest **(RandomForestClassifier)**. Tune the maximum depth and maximum number of features for each tree using **GridSearchCV**. \n"}
{"snippet": "session = tf.Session()\n", "intent": "TensorFLow Run Session\n======================\n"}
{"snippet": "import numpy as np\nimport theano\nimport theano.tensor as T\nx = T.vector()\ny = T.vector()\nz = x + x\nz = z * y\nf = theano.function([x, y], z)\nf(np.ones((2,)), np.ones((3,)))\n", "intent": "Here is an example of code with a user error. Don't try to find the error by code inspection, check only the error:\n"}
{"snippet": "model = LogisticRegression(penalty='l1', C=22)\nmodel.fit(X, Y)\nmodel.coef_\n", "intent": "Let's check coefficients now.\n"}
{"snippet": "model=Sequential()   \nmodel.add(Dense(units=64,input_dim=4))  \nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(units=32)) \nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(units=3))\nmodel.add(Activation(\"softmax\"))\n", "intent": "But first : let's do a quick install\nhttps://github.com/fchollet/hualos\n& run the app:\n```\npython api.py\n```\n"}
{"snippet": "model_vgg16_conv = VGG16(weights=None, include_top=False)\nmodel_vgg16_conv.summary()\ninput = Input(shape=(32,32,3),name = 'image_input')  \noutput_vgg16_conv = model_vgg16_conv(input)\nx = Flatten(name='flatten')(output_vgg16_conv)\nx = Dense(4096, activation='relu', name='fc1')(x)\nx = Dense(4096, activation='relu', name='fc2')(x)\nx = Dense(10, activation='softmax', name='predictions')(x)\nmy_model = Model(input=input, output=x)  \nmy_model.summary()\n", "intent": "Lets import the model this time\n"}
{"snippet": "model_2 = Sequential()\nmodel_2.add(Dense(500, activation='relu', input_shape=(784,)))\nmodel_2.add(Dropout(0.2))\nmodel_2.add(Dense(500, activation='relu'))\nmodel_2.add(Dropout(0.2))\nmodel_2.add(Dense(10, activation='softmax'))\n", "intent": "[why is training accuracy lower than val](https://keras.io/getting-started/faq/\n"}
{"snippet": "history=[]\nbatch_size=32\nepochs= 20\nfor i in range(epochs):\n    mod=model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n    model.reset_states()\n    history.append(mod.history)\n", "intent": "https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/\n"}
{"snippet": "import statsmodels.api as sm\nimport patsy\ndf['ones'] = np.ones_like(df['DomesticTotalGross'])\ny, X = patsy.dmatrices('DomesticTotalGross ~ ones', data=df, return_type='dataframe')\nmodel = sm.OLS(y, X)\nres = model.fit()\n", "intent": "The results give the mean of all data points for domestic total gross. The residuals are skewed right.\n"}
{"snippet": "y4, X4 = patsy.dmatrices('DomesticTotalGross ~ Budget + rating_R + Runtime + FridayReleaseDay + log_director_credits', data=movies_df2, return_type=\"dataframe\")\nmodel4 = sm.OLS(y4, X4)\nfit4 = model4.fit()\nfit4.summary()\n", "intent": "Model using budget, R rating, runtime, Friday release day dummy variable, and log(director credits).\n"}
{"snippet": "y4, X4 = patsy.dmatrices('log_DomesticTotalGross ~ Budget + Runtime', data=movies_df2, return_type=\"dataframe\")\nmodel4 = sm.OLS(y4, X4)\nfit4 = model4.fit()\nfit4.summary()\n", "intent": "Budget and Runtime features have the highest p-values in the model.\n"}
{"snippet": "y4, X4 = patsy.dmatrices('log_DomesticTotalGross ~ Budget + Runtime*FridayReleaseDay*rating_R ', data=movies_df2, return_type=\"dataframe\")\nmodel4 = sm.OLS(y4, X4)\nfit4 = model4.fit()\nfit4.summary()\n", "intent": "Adding interaction terms.\n"}
{"snippet": "import theano\nfrom theano import tensor as T\nx = T.vector('x')\nW = T.matrix('W')\nb = T.vector('b')\n", "intent": "This notebook contains the code snippets from the slides, so you can execute them and tinker with those examples.\nTo execute a cell: Ctrl-Enter.\n"}
{"snippet": "zip(list(X.columns),list(model.fit(X,Y).coef_[0]))\n", "intent": "Not as good as Logit but better than KNN\n"}
{"snippet": "logreg.fit(X,y).coef_\n", "intent": "KNN and logistic regression are doing slightly better than the always-PG-13 predictor, and all 3 have about 0.5 accuracy.\n"}
{"snippet": "y,X = patsy.dmatrices('damage~type+construction+operation+months', data = data, return_type = 'dataframe')\nmodel0 = sm.GLM(y,X,family = sm.families.Poisson(sm.families.links.log))\nresults0 = model0.fit()\nprint(results0.summary())\n", "intent": "**Challenge Number 1**\n"}
{"snippet": "y2,X2 = patsy.dmatrices('damage~type+construction+operation', data = data, return_type = 'dataframe')\nmodel1 = sm.GLM(y2,X2,family = sm.families.Poisson(sm.families.links.log), offset = np.log(data['months']))\nresults1 = model1.fit()\nprint(results1.summary())\n", "intent": "**Challenge Number 2**\n"}
{"snippet": "ys= ['averageRating', 'tmtopave', 'tmallave','audiencescore', 'tmtop', 'tmall']\ncs= []\nfor tar in ys:\n    y, X = patsy.dmatrices(tar + ' ~ absmeantime + initialRush  + usaGross', data=Data, return_type=\"dataframe\")\n    model = sm.OLS(y, X)\n    fit = model.fit()\n    i=fit.params\n    cs.append([i[0],i[2]])\n", "intent": "Make 6 part graph on initial analysis\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nmodel = Sequential() \nmodel.add(Dense(256, input_shape=(max_words,), activation='elu')) \nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5)) \nmodel.add(Dense(2, activation='softmax'))\n", "intent": "we will be using Dense fully connected NN.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.get_variable(\n        \"EmbeddingMatrix\",\n        shape=[vocab_size, embed_dim],\n        initializer=tf.random_uniform_initializer(-1., 1.)\n    )\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "import theano, theano.tensor as T\nimport lasagne\nfrom lasagne.layers import *\nEMBEDDING_SIZE = 128    \nLSTM_SIZE  = 256        \nATTN_SIZE  = 256        \nFEATURES,HEIGHT,WIDTH = img_codes.shape[1:]\n", "intent": "Since the image encoder CNN is already applied, the only remaining part is to write a sentence decoder.\n"}
{"snippet": "full_dropout = tf.nn.dropout(full_layer, keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "learning_rate = 0.1\nupdates = [\n    (param_i, param_i - learning_rate * grad_i)\n    for param_i, grad_i in zip(params, grads)\n]\nupdate_model = theano.function([x, mask], cost, updates=updates)\nevaluate_model = theano.function([x, mask], cost)\n", "intent": "We can now compile the function that updates the gradients. We also added a function that computes the cost without updating for monitoring purposes.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Tip: When using Jupyter notebook make sure to call tf.reset_default_graph() at the beginning to clear the symbolic graph before defining new nodes.\n"}
{"snippet": "sess = tf.Session() \n", "intent": "Use the `.run()` method on a Session and evaluate tensors and execute operations.\n"}
{"snippet": "model.fit(X_train, y_train, epochs = 50, batch_size= 32)\n", "intent": "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the fit function.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), maxval=1, minval=-1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "rate = 0.001\nmu = 0\nsigma = 0.1\nmodel = LeNetModel(32,32,1,mu_init=mu,sigma_init=sigma)\nlogits = model.build(filter1=6,filter2=16,layer1=120,layer2=84,kernel1=5,kernel2=5)\nlogits2 = LeNet(x)\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\nloss_operation = tf.reduce_mean(cross_entropy)\noptimizer = tf.train.AdamOptimizer(learning_rate = rate)\ntraining_operation = optimizer.minimize(loss_operation)\n", "intent": "Create a training pipeline that uses the model to classify MNIST data.\nYou do not need to modify this section.\n"}
{"snippet": "dTree = DecisionTreeClassifier(criterion=\"entropy\")\ndTree.fit(trainData.ix[:,0:6], trainData.index)\n", "intent": "Vamos ao que interessa...\n"}
{"snippet": "wknn1 = KNeighborsClassifier(n_neighbors=1,weights='uniform')\nwknn1.fit(trfeatures, trlabels)\nwknn1.score(ttfeatures,ttlabels)\n", "intent": "Ou vamos mudar o k para 1:\n"}
{"snippet": "print(\"K \\t Uniform \\t Distance\")\nfor k in range(1,11):\n    UniformKnnClassifier = KNeighborsClassifier(n_neighbors=k,weights='uniform')\n    UniformKnnClassifier.fit(trfeatures, trlabels)\n    uScore = UniformKnnClassifier.score(ttfeatures, ttlabels)\n    DistanceKnnClassifier = KNeighborsClassifier(n_neighbors=k,weights='distance')\n    DistanceKnnClassifier.fit(trfeatures, trlabels)\n    dScore = DistanceKnnClassifier.score(ttfeatures, ttlabels)\n    print k,\"\\t{:f} \\t{:f}\".format(uScore,dScore) \n", "intent": "Verifique no intervalo de k = 1 a 10, qual o melhor valor de k e o melhor tipo de pesso a ser utilizado \n"}
{"snippet": "d_tree = DecisionTreeClassifier(criterion=\"entropy\")\nd_tree.fit(train_data, train_data.index) \n", "intent": "Vamos ao que interessa...\n"}
{"snippet": "lr = .3\nupdates = [(par, par - lr * gra) for par, gra in zip(parameters, gradient)] \nupdate_model = theano.function([x], cost, updates=updates)\nget_cost = theano.function([x], mse)\npredict = theano.function([x], prediction)\nget_hidden = theano.function([x], hidden)\nget_gradient = theano.function([x], gradient)\n", "intent": "We now compile the function that will update the parameters of the model using gradient descent. \n"}
{"snippet": "model = sm.OLS(y, X).fit() \n", "intent": "Note the difference in argument order:\n"}
{"snippet": "model = LogisticRegression()\nmodel = model.fit(X, y)\nmodel.score(X, y)\n", "intent": "Let's go ahead and run logistic regression on the entire data set, and see how accurate it is!\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(X_train, y_train)\nreg.score(X_test, y_test)\n", "intent": "Train (on the training data) and score (on the test data) a logistic regressor that predicts the correct label for the iris data set.\n"}
{"snippet": "visualize_classifier(DecisionTreeClassifier(max_depth=2), X, y)\n", "intent": "Just because we are so familiar with them now, please run a decision tree with very shallow maximum depth. It will do as bad as you think!\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nvisualize_classifier(KNeighborsClassifier(n_neighbors=5), X, y)\n", "intent": "What kind of classifier that you have seen before do you expect to perform pretty well? Try!\n"}
{"snippet": "visualize_classifier(SVC(), X, y)\nvisualize_classifier(AdaBoostClassifier(SVC(), algorithm=\"SAMME\"), X, y)\n", "intent": "Perfection! This must mean a model like that always gets everything right, doesn't it?\n"}
{"snippet": "mlp = MLPClassifier(hidden_layer_sizes=(200, 50, 50), max_iter=100, alpha=1e-4,\n                    solver='sgd', verbose=50, tol=1e-4, random_state=1,\n                    learning_rate_init=.1 )\n", "intent": "Now train a deep network, with three layers. You should be able to get the $R^2$ close to 98%. Good luck!\n"}
{"snippet": "regressor = LinearRegression()\n", "intent": "These models all work as follows. The first thing you need to is to start an instance of the model object:\n"}
{"snippet": "for i in range(1001):\n    mse_train = update_model(data_train)\n    if i % 100 == 0:\n        mse_val = get_cost(data_val)\n        print 'Epoch {}: train mse: {}    validation mse: {}'.format(i, mse_train, mse_val)\n", "intent": "We can now train the network by supplying this function with our data and calling it repeatedly.\n"}
{"snippet": "with graph.as_default():\n    flat = tf.reshape(inception_out, (-1, 8*144))\n    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n    logits = tf.layers.dense(flat, n_classes)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost)\n    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n", "intent": "Now, flatten and pass to the classifier\n"}
{"snippet": "with graph.as_default():\n    flat = tf.reshape(max_pool_4, (-1, 8*144))\n    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n    logits = tf.layers.dense(flat, n_classes)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost)\n    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n", "intent": "Now, flatten and pass to the classifier\n"}
{"snippet": "with graph.as_default():\n    conv1 = tf.layers.conv1d(inputs=inputs_, filters=18, kernel_size=2, strides=1, \n                             padding='same', activation = tf.nn.relu)\n    n_ch = n_channels *2\n", "intent": "Build Convolutional Layer(s)\nQuestions: \n* Should we use a different activation? Like tf.nn.tanh?\n* Should we use pooling? average or max?\n"}
{"snippet": "with graph.as_default():\n    lstm_in = tf.transpose(conv1, [1,0,2]) \n    lstm_in = tf.reshape(lstm_in, [-1, n_ch]) \n    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None) \n    lstm_in = tf.split(lstm_in, seq_len, 0)\n    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n    initial_state = cell.zero_state(batch_size, tf.float32)\n", "intent": "Now, pass to LSTM cells\n"}
{"snippet": "with graph.as_default():\n    lstm_in = tf.transpose(inputs_, [1,0,2]) \n    lstm_in = tf.reshape(lstm_in, [-1, n_channels]) \n    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None) \n    lstm_in = tf.split(lstm_in, seq_len, 0)\n    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    cell = tf.contrib.rnn.MultiRNNCell([lstm] * lstm_layers)\n    initial_state = cell.zero_state(batch_size, tf.float32)\n", "intent": "Construct inputs to LSTM\n"}
{"snippet": "with graph.as_default():\n    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32,\n                                                     initial_state = initial_state)\n    logits = tf.layers.dense(inputs=outputs[-1], units=n_classes, name='logits')\n    labels = tf.one_hot(indices=indices_, depth=n_classes, dtype=logits.dtype)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost) \n    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n", "intent": "Define forward pass, cost function and optimizer:\n"}
{"snippet": "with graph.as_default():\n    lstm_in = tf.transpose(inputs_, [1,0,2]) \n    lstm_in = tf.reshape(lstm_in, [-1, n_channels]) \n    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None) \n    lstm_in = tf.split(lstm_in, seq_len, 0)\n    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n    initial_state = cell.zero_state(batch_size, tf.float32)\n", "intent": "Construct inputs to LSTM\n"}
{"snippet": "with graph.as_default():\n    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32,\n                                                     initial_state = initial_state)\n    logits = tf.layers.dense(outputs[-1], n_classes, name='logits')\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_))\n    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost) \n    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n", "intent": "Define forward pass, cost function and optimizer:\n"}
{"snippet": "lr = LinearRegression(fit_intercept=True)\nlr.fit(X2, y)\nprint lr.intercept_\nprint lr.coef_\nprint 'Estimated function: y = %.2f + %.2fx0 + %.2fx1' %(lr.intercept_, lr.coef_[0], lr.coef_[1])\n", "intent": "Now let's fit a linear model where the input features are (x, x^2).\n"}
{"snippet": "f = theano.function(inputs=[X, W, b],\n                    outputs=output,\n                    updates=updates)\nX_value = np.arange(-3, 3).reshape(3, 2).astype(theano.config.floatX)\nW_value = np.eye(2).astype(theano.config.floatX)\nb_value = np.arange(2).astype(theano.config.floatX)\nprint(f(X_value, W_value, b_value))\n", "intent": "We can now compile our Theano function and see that it gives the expected results.\n"}
{"snippet": "smf.ols(formula = 'sale_price ~ gross_sq_feet', data = data1.ix[15:70]).fit().summary()\n", "intent": "And that's how we produced lecture example\n"}
{"snippet": "lm=smf.ols('speed~age+C(Gender)+C(hour)',data=data2.ix[comind&demoind]).fit()\nlm.summary()\n", "intent": "Now consider hours of the day\n"}
{"snippet": "lm=smf.ols('speed~age+C(Gender)+C(hour)',data=data2.ix[comind&demoind&(data2.wd==1)]).fit()\nlm.summary()\n", "intent": "One can generally recognize five periods in the day - morning (7-10am, day-11am-5pm, evening-6-10pm, late evening-11pm-1am and night-2am-6am)\n"}
{"snippet": "lm=smf.ols('speed~age+C(Gender)+C(wd)+C(hour)',data=data2.ix[comind&demoind]).fit()\nlm.summary()\n", "intent": "Run a regression using both - wd and hour\n"}
{"snippet": "lm=smf.ols('speed~age+C(Gender)+C(wd)+C(hour)+StartDens+EndDens',data=data2.ix[comind&demoind]).fit()\nlm.summary()\n", "intent": "Finally add density parameters for beginning and the end of the trip\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(train_dtm, y_train)\n", "intent": "Let's build the model with Naive Bayes Now\nhttp://scikit-learn.org/stable/modules/naive_bayes.html\n"}
{"snippet": "fit2 = smf.ols('Y ~ X1 + X2 + X3 + X4 + X6', data=x).fit()\nprint fit2.summary()\n", "intent": "remove feature w/ lowest (abs) t score\n"}
{"snippet": "fit4 = smf.ols('Y ~ X1 + X3 + X6', data=x).fit()\nprint fit4.summary()\n", "intent": "--> increasing bias, decreasing variance\n"}
{"snippet": "fit5 = smf.ols('Y ~ X1 + X3', data=x).fit()\nprint fit5.summary()\n", "intent": "$\\rightarrow$ optimal bias-variance point reached\n"}
{"snippet": "f = theano.function(inputs=[M, s],\n                    outputs=output,\n                    updates=updates)\nM_value = np.arange(9).reshape(3, 3).astype(theano.config.floatX)\ns_value = np.zeros((3, ), dtype=theano.config.floatX)\nprint(f(M_value, s_value))\n", "intent": "We can now compile and test the Theano function :\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1, tf.float32))\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "w=tf.Variable(tf.ones((2,2)),name='weights')\n", "intent": "**Notes for `eval()`**:\n`t.eval()` is a shortcut for calling `tf.get_default_session().run(t)`\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\ntree = DecisionTreeClassifier()\nbag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\n                        random_state=1)\nbag.fit(X, y)\nvisualize_classifier(bag, X, y)\n", "intent": "This type of bagging classification can be done manually using Scikit-Learn's ``BaggingClassifier`` meta-estimator, as shown here:\n"}
{"snippet": "text_clf = build_pipeline()\ntext_clf = text_clf.fit(X_train, y_train)\n", "intent": "We should now be ready to feed these vectors into a classifier. \n"}
{"snippet": "text_clf = build_pipeline()\ntext_clf = text_clf.fit(X_train, y_train)\n", "intent": "We should now be ready to feed these vectors into a classifier\n"}
{"snippet": "from sklearn import svm\nsvc = svm.SVC(kernel='rbf')\nsvc.fit(training_data, labels)  \n", "intent": "Instantiate the classifier here:\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\n", "intent": "Import GaussianNB and instantiate one\n"}
{"snippet": "gnb.fit(data,labels)\n", "intent": "Fit the training data:\n"}
{"snippet": "f = theano.function(inputs=[f_init],\n                    outputs=[next_fibonacci_terms, ratios_between_terms],\n                    updates=updates)\nout = f([1, 1])\nprint(out[0])\nprint(out[1])\n", "intent": "Let's compile our Theano function which will take a vector of consecutive values from the Fibonacci sequence and compute the next 10 values :\n"}
{"snippet": "dtree = tree.DecisionTreeClassifier()\ndtree = dtree.fit(features,labels)\ndtree\n", "intent": "Create a Decision Tree and fit the training data\n"}
{"snippet": "from sklearn import svm\nsvr = svm.SVR()\nsvr.fit(X, y) \n", "intent": "Fit the model using Support Vector Regression\n"}
{"snippet": "y_true = X[:, 0]*X[:, 1] + \\\n    torch.tensor(np.log(X[:, 2])).float() + \\\n    (torch.tensor(np.random.rand(5)).float() * X[:, 3:8]).sum(dim=1) + \\\n    torch.tensor(np.random.normal(0, 0.1, 50)).float()\n", "intent": "Use provided $\\textbf{X}$ and $\\textbf{y}$ values to find gradient of RMSE loss with respect to parameters of your network.\n"}
{"snippet": "def get_random_training_sample():\n    cls = random.choice(unique_classes)\n    word = random.choice(examples_by_class[cls])\n    word_tensor = word_to_tensor(word)\n    class_tensor = class_to_tensor(cls)\n    return word, cls, word_tensor, class_tensor\n", "intent": "As you can see the output is a ``<1 x len(unique_classes)>`` Tensor, where\nevery item is the likelihood of that category (higher is more likely).\n"}
{"snippet": "encoder_test = EncoderRNN(10, 10, 2)\nprint(encoder_test)\nencoder_hidden = encoder_test.init_hidden(1)\nword_input = torch.LongTensor([[1, 2, 3]])\nif USE_CUDA:\n    encoder_test.cuda()\n    word_input = word_input.cuda()\nencoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\nencoder_outputs.shape\n", "intent": "To make sure the Encoder and Decoder model are working (and working together) we'll do a quick test with fake word inputs:\n"}
{"snippet": "x = Variable(torch.ones(2,2), requires_grad=True) \nprint(x)\n", "intent": "Example from official documentation\n"}
{"snippet": "x = torch.autograd.Variable(torch.ones(1,1,32,32), requires_grad=True)\nprint(x.size())\nx = x.view(x.size(0), -1)\nprint(x.size())\n", "intent": "Understanding pytorch tensor reshaping\n"}
{"snippet": "def get_model_i(i=0):\n    val_idxs = get_cv_idxs(n, i, val_pct=0.1)\n    tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_basic, max_zoom=1.05)\n    data = ImageClassifierData.from_csv(path, 'train', f'{path}train.csv', bs, tfms, \n                                        val_idxs=val_idxs, suffix='.jpg', continuous=True)\n    learn = ConvLearnerVGG.pretrained(data, ps=0.0, precompute=False)\n    return learn\n", "intent": "Here's code to do cross-validation:\n"}
{"snippet": "base_model = ResNet50(weights='imagenet', include_top=False)\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\npredictions = Dense(1, activation='sigmoid')(x)\n", "intent": "In Keras you have to manually specify the base model and construct on top the layers you want to add.\n"}
{"snippet": "bkm = bikmeans(5, points)\n", "intent": "Perform clustering (k=5)\n"}
{"snippet": "tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_side_on, crop_type=CropType.NO)\nmd = ImageClassifierData.from_csv(PATH, JPEGS, CSV, tfms=tfms)\n", "intent": "From here on it's jus tlike Dogs vs Cats! We have a CSV file containing a bunch of file names, and for each one: the class and bounding box.\n"}
{"snippet": "learn.fit(lrs/5, 1, cycle_len=2)\n", "intent": "Accuracy isn't improving much - since many images have multiple different objects, it's going to be impossible to be that accurate.\n"}
{"snippet": "learn.fit(lr, 1, cycle_len=3, use_clr=(32,5))\n", "intent": "Now we have something that's printing out our Object Detection Loss, Accuracy, and Detection L1:\n"}
{"snippet": "K.set_value(m_final.optimizer.lr, 1e-4)\nm_final.fit([arr_lr, arr_hr], targ, 16, 2, **pars)\n", "intent": "We use learning rate annealing to get a better fit.\n"}
{"snippet": "top_model = Model(inp, outp)\n", "intent": "We're only interested in the trained part of the model, which does the actual upsampling. *the upsampling model*\n"}
{"snippet": "K.set_value(m_sr.optimizer.lr, 1e-4)\nm_sr.fit([arr_lr, arr_hr], targ, 8, 1, **pars)\n", "intent": "We use learning rate annealing to get a better fit.\n"}
{"snippet": "top_model = Model(inp, outp)\n", "intent": "We're only interested in the trained part of the mdoel, which does the actual upsampling.\n"}
{"snippet": "inp = Input((288, 288, 3))\nref_model = Model(inp, ReflectionPadding2D((40, 10))(inp))\nref_model.compile('adam', 'mse')\n", "intent": "Testing the reflection padding layer:\n"}
{"snippet": "batch_size=16\nvgg = Vgg16()\nmodel = vgg.model\nlast_conv_idx = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\nconv_layers = model.layers[:last_conv_idx + 1]\nconv_model = Sequential(conv_layers)\n", "intent": "**---------------------------------- Now the actual work part ----------------------------------**\n"}
{"snippet": "km = kmeans(5, points)\nprint(\"SSE of clustering: \", km['sse'])\n", "intent": "Perform regular K-means clustering and compute SSE\n"}
{"snippet": "model = Sequential ([\n    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, weights=[emb]),\n    Dropout(0.2),\n    graph,\n    Dropout(0.5),\n    Dense(100, activation='relu'),\n    Dropout(0.7),\n    Dense(1, activation='sigmoid')\n    ])\n", "intent": "We then replace the conv/max-pool layer in our original CNN with the concatenated conv layers.\n"}
{"snippet": "model = Sequential([\n    Embedding(vocab_size, 32, input_length=seq_len, mask_zero=True,\n              W_regularizer=l2(1e-6), dropout=0.2),\n    LSTM(100, consume_less='gpu'),\n    Dense(1, activation='sigmoid')])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "This more complex architecture has given us another boost in accuracy.\nWe haven't covered this bit yet!\n"}
{"snippet": "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=1,\n             validation_data=(conv_val_feat, val_labels))\n", "intent": "Now we can train the model as usual, with pre-computed augmented data.\n"}
{"snippet": "dense_out = Dense(vocab_size, activation='softmax') \n", "intent": "This is the 'blue arrow' from our diagram - the layer operation from hidden to hidden.\n"}
{"snippet": "model = Model([c[0] for c in c_ins], c_out)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n", "intent": "So now we can create our model\n"}
{"snippet": "dense_in = Dense(n_hidden, activation='relu')\ndense_out = Dense(vocab_size, activation='softmax', name='output')\n", "intent": "Now our y dataset looks exactly like our x dataset did before, but everything's shifted over by 1 character.\n"}
{"snippet": "model.fit(x_rnn[:mx], y_rnn[:mx], batch_size=bs, nb_epoch=4, shuffle=False)\n", "intent": "The LSTM model takes much longer to run than the regular RNN because it isn't in parallel: each operation has to be run in order.\n"}
{"snippet": "model.fit(oh_x_rnn, oh_y_rnn, batch_size=64, nb_epoch=8)\n", "intent": "The `86` is the onehotted dimension; classes of characters\n"}
{"snippet": "model = Model([inp, sz_inp], x)\nmodel.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "When we compile the mdoel, we have to specify all the input layers in an array\n"}
{"snippet": "from keras.models import load_model\nmodel.save('my_model.h5')\nmodel2 = load_model('my_model.h5')\n", "intent": "It is really important to be able to reload the model after you've been training it for hours on end (usually).\n"}
{"snippet": "model.optimizer.lr = 1e-5\nmodel.fit(conv_feat, [trn_bbox, trn_labels], batch_size=batch_size, nb_epoch=12,\n             validation_data=(conv_val_feat, [val_bbox, val_labels]))\n", "intent": "The above is after 23 epochs. After 35:\n"}
{"snippet": "def sigmoid(x): return 1 / (1 + np.exp(-x))\ndef sigmoid_d(x):\n    output = sigmoid(x)\n    return output * (1 - output)\ndef relu(x): return np.maximum(0., x)\ndef relu_d(x): return (x > 0.) * 1.\n", "intent": "The key step done automatically above by Theano is gradients calculation.\n---\nWill need to define all functions used:\n"}
{"snippet": "model = Sequential([\n            GRU(n_hidden, return_sequences=True, input_shape=(cs, vocab_size),\n                   activation='relu', inner_init='identity'),\n            TimeDistributed(Dense(vocab_size, activation='softmax')),\n        ])\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam())\nmodel.fit(oh_x_rnn, oh_y_rnn, batch_size = 64, nb_epoch=8)\n", "intent": "---\nGated Recurrent Unit\n"}
{"snippet": "latest_weights_filename = None\nfor epoch in range(no_of_epochs):\n    print \"Running epoch %d\" % epoch\n    vgg.fit(batches, val_batches, nb_epoch=1)\n    latest_weights_filename = 'ft%d.h5' % epoch\n    vgg.model.save_weights(results_path + latest_weights_filename)\nprint \"Completed %s fit operations\" % no_of_epochs\n", "intent": "We're also saving our weights after each epoch.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, input_dim=784))\nmodel.add(Activation('relu'))\n", "intent": "You can also simply add layers via the ```.add()``` method:\n"}
{"snippet": "model.add(Dense(2, activation='softmax'))\n", "intent": "Now we add our final Cat vs Dog layer\n"}
{"snippet": "models = [train_model() for m in xrange(6)]\n", "intent": "Trying the above again, this time having h5py installed.\n"}
{"snippet": "VGG = Vgg16()\nVGG.model.pop()\nfor layer in VGG.model.layers: layer.trainable = False\nVGG.model.add(Dense(10, activation='softmax'))\nVGG.model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "NOTE: I'll want a way to clear GPU memory in the future. Right now all I know is restarting the kernel.\n"}
{"snippet": "m = RandomForestRegressor(n_jobs=-1)\nm.fit(df, y)\nm.score(df, y)\n", "intent": "The R^2 score shown below shows the variance (or mean?) of the data. It shows how much the data varies.\n"}
{"snippet": "import keras.backend as K\ndef create_model(layers, activation):\n    model = Sequential()\n    for i, nodes in enumerate(layers):\n        if i==0:\n            model.add(Activation(activation))\n        else:\n    model.compile(optimizer='adadelta', loss='mse')\n    return model\nmodel = KerasRegressor(build_fn=create_model, verbose=0)    \n", "intent": "What is Cross Validation?\nhttps://stats.stackexchange.com/questions/1826/cross-validation-in-plain-english\n"}
{"snippet": "ms = MeanShift()\nms.fit(X)\nlabels = ms.labels_\ncluster_centers = ms.cluster_centers_\nn_clusters = len(np.unique(labels))\n", "intent": "Let's see how Mean Shift does on this example:\n"}
{"snippet": "from keras.layers import Input, Dense\nfrom keras.models import Model\nencoding_dim = 32  \ninput_img = Input(shape=(784,))\nencoded = Dense(encoding_dim, activation='relu')(input_img)\n", "intent": "See [keras autoencoder tutorial](https://blog.keras.io/building-autoencoders-in-keras.html)\n"}
{"snippet": "x = torch.Tensor(5, 3)\nprint(x)\n", "intent": "Construct a 5x3 matrix, unitialized:\n"}
{"snippet": "outputs = net(Variable(images))\n", "intent": "Okay, now let's see what the neural network thinks these examples above are:\n"}
{"snippet": "correct = 0\ntotal = 0\nfor data in testloader:\n    images, labels = data\n    outputs = net(Variable(images))\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\nprint(f'Accuracy of network on 10,000 test images: {np.round(100*correct/total,2)}%')\n", "intent": "Seems pretty good.\nNow to look at how the network performs on the whole dataset.\n"}
{"snippet": "inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n", "intent": "Remember that you'll have to send the inputs and targets at every step to the GPU too:\n"}
{"snippet": "for data in rand_loader:\n    if torch.cuda.is_available():\n        input_var = Variable(data.cuda())\n    else:\n        input_var = Variable(data)\n    output = model(input_var)\n    print(\"Outside: input size\", input_var.size(), \n          \"output_size\", output.size())\n", "intent": "Now we can see the sizes of input and output tensors.\n"}
{"snippet": "log_softmax(xb@weights+bias)[:2]\n", "intent": "The minibatch activations after the Log Softmax and before heading into Negative Log Likelihood:\n"}
{"snippet": "nll(log_softmax(xb@weights+bias), yb)\n", "intent": "The loss value computed via NLL on the Log Softmax activations:\n"}
{"snippet": "model = XGBRegressor()\nmodel.fit(train_x, train_data['cnt'])\n", "intent": "Keep in mind that you still need to `fit` the model. `cross_val_score` only gets you the cross validation score, nothing more.\n"}
{"snippet": "learn.fit(0.15, 50) \n", "intent": "Looks like [the Keras example](https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py\n"}
{"snippet": "temp = Variable(torch.FloatTensor([1,2]))\ntemp.cpu()\n", "intent": "```\nVariable.cpu(self)\nSource:   \n    def cpu(self):\n        return self.type(getattr(torch, type(self.data).__name__))\n```\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "intent": "Load a pretrained model and reset final fully-connected layer\n"}
{"snippet": "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)\n", "intent": "Should take 15-25 min on CPU; < 1 min on GPU.\n"}
{"snippet": "model(x)[0].shape, x.shape\n", "intent": "Is the problem that Y is not flattened when compared to X in BCE?\n"}
{"snippet": "from sklearn.svm import SVC\nSVC().fit(X, y).score(X, y)\n", "intent": "To illustrate this, let's train a Support Vector Machine naively on the digits dataset:\n"}
{"snippet": "svc = SVC(kernel='rbf').fit(X_train, y_train)\ntrain_score = svc.score(X_train, y_train) \ntrain_score\n", "intent": "Let's retrain a new model on the first subset call the **training set**:\n"}
{"snippet": "svc_2 = SVC(kernel='rbf', C=100, gamma=0.001).fit(X_train, y_train)\nsvc_2\n", "intent": "Let's try again with another parameterization:\n"}
{"snippet": "svc = SVC(C=1, gamma=0.0005)\nfor i, train_size in enumerate(train_sizes):\n    cv = ShuffleSplit(n_samples, n_iter=n_iter, train_size=train_size)\n    for j, (train, test) in enumerate(cv):\n        svc.fit(X[train], y[train])\n        train_scores[i, j] = svc.score(X[train], y[train])\n        test_scores[i, j] = svc.score(X[test], y[test])\n", "intent": "We can now loop over training set sizes and CV iterations:\n"}
{"snippet": "model = Sequential()\nmodel.add(Embedding(len(word2num), 50)) \nmodel.add(LSTM(64))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nmodel.summary()\n", "intent": "This method is no different to the method utilised in the sentiment analysis lesson.\n"}
{"snippet": "MultinomialNB()\n", "intent": "The `MultinomialNB` class is a good baseline classifier for text as it's fast and has few parameters to tweak:\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nparameters = {\n    'vec__max_df': [0.8, 1.0],\n    'vec__ngram_range': [(1, 1), (1, 2)],\n    'vec__use_idf': [True, False],\n}\ngs = GridSearchCV(pipeline, parameters, verbose=2, refit=False)\n_ = gs.fit(twenty_train_small.data, twenty_train_small.target)\n", "intent": "For the grid search, the parameters names are prefixed with the name of the pipeline step using \"__\" as a separator:\n"}
{"snippet": "_ = pipeline.fit(twenty_train_small.data, twenty_train_small.target)\n", "intent": "Let's fit a model on the small dataset and collect info on the fitted components:\n"}
{"snippet": "from copy import copy\ndef average_linear_model(models):\n", "intent": "We can now compute the average linear model:\n"}
{"snippet": "reg_trees = ExtraTreesClassifier(n_estimators=50, max_depth=7, min_samples_split=10)\nreg_trees.fit(X_small_train, y_small_train)\nprint(\"Train score: %0.3f\" % reg_trees.score(X_small_train, y_small_train))\nprint(\"Test score: %0.3f\" % reg_trees.score(X_test, y_test))\n", "intent": "More interesting, an ensemble model composed of regularized trees is not underfitting much less than the individual regularized trees:\n"}
{"snippet": "clf_gini = DecisionTreeClassifier(criterion = \"gini\", \n                                  random_state = 100,\n                                  max_depth=10, \n                                  min_samples_leaf=50,\n                                  min_samples_split=50)\nclf_gini.fit(X_train, y_train)\n", "intent": "**Running the model with best parameters obtained from grid search.**\n"}
{"snippet": "clf_gini = DecisionTreeClassifier(criterion = \"gini\", \n                                  random_state = 100,\n                                  max_depth=3, \n                                  min_samples_leaf=50,\n                                  min_samples_split=50)\nclf_gini.fit(X_train, y_train)\nprint(clf_gini.score(X_test,y_test))\n", "intent": "You can see that this tree is too complex to understand. Let's try reducing the max_depth and see how the tree looks.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\n", "intent": "Let's first fit a random forest model with default hyperparameters.\n"}
{"snippet": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nn_folds = 5\nparameters = {'max_depth': range(2, 20, 5)}\nrf = RandomForestClassifier()\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)\n", "intent": "Let's try to find the optimum values for ```max_depth``` and understand how the value of max_depth impacts the overall accuracy of the ensemble.\n"}
{"snippet": "batch_size = 128\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=15, validation_data=(X_test, y_test))\n", "intent": "I may heave cheated and run the following block 3 times. Good thing about Keras is that it remembers the last learning rate and goes from there.\n"}
{"snippet": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nn_folds = 5\nparameters = {'min_samples_leaf': range(100, 400, 50)}\nrf = RandomForestClassifier()\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)\n", "intent": "Let's now check the optimum value for min samples leaf in our case.\n"}
{"snippet": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nn_folds = 5\nparameters = {'min_samples_split': range(200, 500, 50)}\nrf = RandomForestClassifier()\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)\n", "intent": "Let's now look at the performance of the ensemble as we vary min_samples_split.\n"}
{"snippet": "param_grid = {\n    'max_depth': [4,8,10],\n    'min_samples_leaf': range(100, 400, 200),\n    'min_samples_split': range(200, 500, 200),\n    'n_estimators': [100,200, 300], \n    'max_features': [5, 10]\n}\nrf = RandomForestClassifier()\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1,verbose = 1)\n", "intent": "We can now find the optimal hyperparameters using GridSearchCV.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=10,\n                             min_samples_leaf=100, \n                             min_samples_split=200,\n                             max_features=10,\n                             n_estimators=100)\n", "intent": "**Fitting the final model with the best parameters obtained from grid search.**\n"}
{"snippet": "logm2 = sm.GLM(y_train,(sm.add_constant(X_train2)), family = sm.families.Binomial())\nlogm2.fit().summary()\n", "intent": "Now let's run our model again after dropping highly correlated variables\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\n", "intent": " - We expect this to work very well, giving high performance in accuracy\n"}
{"snippet": "crf = sklearn_crfsuite.CRF(\n    algorithm='lbfgs',\n    c1=0.01,\n    c2=0.1,\n    max_iterations=100,\n    all_possible_transitions=True\n)\ncrf.fit(X_train, y_train)\n", "intent": "Let's now fit a CRF with arbitrary hyperparameters. \n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 2))\nnet.add(Linear(2, 4))\nnet.add(LeakyReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\nprint(net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\nprint(net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "model = Sequential()\nmodel.summary()\n", "intent": "This method is no different to the method utilised in the sentiment analysis lesson.\n"}
{"snippet": "x_cnn_sym = T.matrix()\nx_sentence_sym = T.imatrix()\nmask_sym = T.imatrix()\ny_sentence_sym = T.imatrix()\n", "intent": "Define symbolic variables for the various inputs\n"}
{"snippet": "trainingStartTime = time.time()\nlogisticRegressor = linear_model.LogisticRegression(C=0.1, solver='sag', \n                                                    class_weight={1: 0.46, 0: 1.32})\nlogisticRegressor.fit(X, y)\ntrainingDurationInMinutes = (time.time()-trainingStartTime)/60.0\nprint('full training took %.2f minutes' % (trainingDurationInMinutes))\n", "intent": "Train on the full training data\n-------------------------------\n"}
{"snippet": "from os import system\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\ndf['recipe'] = df['title'].map(lambda t: 1 if 'recipe' in unicode(t).lower() else 0)\nX = df[['image_ratio', 'html_ratio', 'recipe', 'label']].dropna()\ny = X['label']\nX.drop('label', axis=1, inplace=True)\nmodel = DecisionTreeClassifier()\nmodel.fit(X, y)\n", "intent": "Let's build a decision tree model to predict the \"evergreen-ness\" of a given website.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nparameters = {\n    'vec__max_df': [0.8, 1.0],\n    'vec__ngrams_range': [(1, 1), (1, 2)],\n    'vec__use_idf': [True, False],\n}\ngs = GridSearchCV(pipeline, parameters, verbose=2, refit=False)\n_ = gs.fit(twenty_train_small.data, twenty_train_small.target)\n", "intent": "For the grid search, the parameters names are prefixed with the name of the pipeline step using \"__\" as a separator:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.01))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "import numpy as np\nwires_cartesian = np.vstack((wires['wire_rho'] * np.cos(wires['wire_phi']),\n                                  wires['wire_rho'] * np.sin(wires['wire_phi']))).T\nfrom sklearn.neighbors import KDTree\nneighbors_tree = KDTree(wires_cartesian)\n", "intent": "Let's write a routine for finding the closest neighbours of a wire.\n"}
{"snippet": "draw_tree(DecisionTreeClassifier(max_depth=2).fit(iris.data, iris.target))\n", "intent": "Decision tree has paramenters (see lecture).\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\ngrid_search_cv = GridSearchCV(RandomForestClassifier(random_state=42), {\n        'criterion': ('gini', 'entropy'),\n        'max_depth': (1, 5, None),\n        'min_samples_split': (2, 3, 4, 5, 10),\n        'n_estimators': (1, 10, 50, 100)}, scoring='roc_auc', n_jobs=2, cv=4)\ngrid_search_cv.fit(line_features, line_labels)\n", "intent": "Tries all parameters combinations. Is expensive.\n"}
{"snippet": "batch_size = 128\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n", "intent": "I may heave cheated and run the following block 3 times. Good thing about Keras is that it remembers the last learning rate and goes from there.\n"}
{"snippet": "def generate_relu(alpha):\n    def relu(x):\n        return T.switch(x > 0, x, alpha * x)\n    return relu\n", "intent": "A smooth approximation to the rectifier is the analytic function\n$f(x) = \\ln(1 + e^x)$\nwhich is called the softplus function.\n"}
{"snippet": "class MyNeuralNetwork(AbstractNeuralNetworkClassifier):\n    def prepare(self):\n        ...\n        def activation(input):\n            ...\n        return activation\n", "intent": "Here is you daon't need to add `b` parameter, this interface does it and includes additional column in `X`\n"}
{"snippet": "x = T.vector() \ny = T.vector()\nalpha = T.scalar()\nbeta = T.scalar()\ncompiled_expr = theano.function([x, y, alpha, beta], \n                        (T.sum(x * (alpha * y)) + T.sum(y * (beta * x))) ** 2)\n", "intent": "* compute $(<x, \\alpha y> + <\\beta x, y>)^2$\n"}
{"snippet": "p_sig = T.nnet.sigmoid(X_.dot(w))\np_bck = 1 - p_sig\n", "intent": "$p_i = sigmoid(\\sum_j X_{ij}w_j)$\n$loss=\\sum y_i \\log{p} + (1-y_i)\\log{(1 - p)}$\n$-loss \\to min$\n"}
{"snippet": "Y_train_binary = (Y_train > 0)\nY_test_binary = (Y_test > 0)\nbinary_tree = DecisionTreeClassifier().fit(X_train, Y_train_binary)\n", "intent": "Too many of them, select according to your task. For binary classification the most common is ROC AUC.\n"}
{"snippet": "plot_decision_surface(BaggingClassifier(\n    base_estimator=LogisticRegression(C=1e3), n_jobs=-1).fit(X_train, Y_train), X_test, Y_test)\n", "intent": "Why does BaggingClassifier not improve the linear regression quality?\n"}
{"snippet": "plot_embedding(Xrp,y)\n", "intent": "Not super helpful to say the least.\n"}
{"snippet": "model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs=5)\n", "intent": "Keras models follow __Scikit-learn__'s interface of fit/predict with some notable extensions. Let's take a tour.\n"}
{"snippet": "model.save_model('catboost_model.dump')\n", "intent": "4) And its always really handy to be able to dump your model to disk (especially if training took some time)\n"}
{"snippet": "show_graph(tf.get_default_graph().as_graph_def())\n", "intent": "Train the graph above:\n"}
{"snippet": "x_t = tf.placeholder('int32',(None,))\nh_t = tf.Variable(np.zeros([1,rnn_num_units],'float32'))\nnext_probs,next_h = rnn_one_step(x_t,h_t)\n", "intent": "Once we've trained our network a bit, let's get to actually generating stuff. All we need is the `rnn_one_step` function you have written above.\n"}
{"snippet": "from sklearn import mixture\ngmm = mixture.GaussianMixture (n_components=10, covariance_type='full')\ngmm.fit(X)\n", "intent": "We try to solve the same problem with another module: [scikit-learn](http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm.html\n"}
{"snippet": "from statsmodels.api import OLS\nimport numpy\ny = data.macron\nX = numpy.ones((len(y), 2))\nX[:,1] = numpy.arange(0,len(y))\nreg = OLS(y,X)\nresults = reg.fit()\nresults.params\n", "intent": "La fonction [detrend](http://statsmodels.sourceforge.net/stable/generated/statsmodels.tsa.tsatools.detrend.html\n"}
{"snippet": "model = LinearRegression(normalize=True)\nprint(model.normalize)\n", "intent": "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated:\n"}
{"snippet": "from sklearn import svm\nclf = svm.SVC(C=5., gamma=0.001)\nclf.fit(X_train_pca, y_train)\n", "intent": "Now we'll perform support-vector-machine classification on this reduced dataset:\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nconfusion(DecisionTreeClassifier(), X_train, X_test, y_train, y_test)\n", "intent": "Quelques exemples pour tester, quelques exemples pour apprendre. C'est peu.\n"}
{"snippet": "model = uncertain_gallup_model(gallup_2012)\nmodel = model.join(electoral_votes)\nmodel.head()\n", "intent": "We construct the model by estimating the probabilities:\n"}
{"snippet": "images, labels = next(iter(trainloader))\nimg = images[13].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "naive_bayes.fit(training_data, y_train)\nbag_mod.fit(training_data, y_train)\nrf_mod.fit(training_data, y_train)\nada_mod.fit(training_data, y_train)\nsvm_mod.fit(training_data, y_train)\n", "intent": "> **Step 1**: Now, fit each of the above models to the appropriate data.  Answer the following question to assure that you fit the models correctly.\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Step 2:** \"Instantiate\" the \"estimator\"\n- \"Estimator\" is scikit-learn's term for model\n- \"Instantiate\" means \"make an instance of\"\n"}
{"snippet": "tree_mod.fit(X_train, y_train)\nrf_mod.fit(X_train, y_train)\nada_mod.fit(X_train, y_train)\nreg_mod.fit(X_train, y_train)\n", "intent": "> **Step 4:** Fit each of your instantiated models on the training data.\n"}
{"snippet": "train_features = train_data[:, 1:]\ntrain_target = train_data[:, 0]\nclf = clf.fit(train_features, train_target)\nscore = clf.score(train_features, train_target)\nprint (\"Mean accuracy of Random Forest = %.2f %%\" % (score*100))\n", "intent": "Fit the training data and create the decision trees:\n"}
{"snippet": "from sklearn import svm\nX = np.array(channel[['x','y']])\ny = np.array(channel[\"Channel\"])\nC = 1.0\nlin_svc = svm.SVC(kernel='linear', C=C).fit(X,y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\npoly9_svc = svm.SVC(kernel='poly', degree=9, C=C).fit(X, y)\n", "intent": "Use Support Vector Machine (SVM), a frontier which best segregates the two classes.\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm_inertia = []\nn = int(input(\"Enter Starting Cluster: \"))\nn1 = int(input(\"Enter Ending Cluster: \"))\nfor i in range(n,n1):\n    km = KMeans(n_clusters=i)\n    km.fit(matrix[matrix.columns[2:]])\n    print (i, km.inertia_)\n    km_inertia.append(km.inertia_)\n", "intent": "**Find optimal K-value**\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=12, init='k-means++', n_init=10)\n", "intent": "Fit the model with the settled number of clusters. \n"}
{"snippet": "model = LogisticRegression()\nmodel = model.fit(X, Y)\nmodel.score(X, Y)\n", "intent": "Let's go ahead and run logistic regression on the entire data set, and see how accurate it is!\n"}
{"snippet": "model_details = model.fit(images_train, class_train,\n                    batch_size = 128,\n                    epochs = NUM_EPOCH, \n                    validation_data= (images_test, class_test),\n                    callbacks=[checkpoint],\n                    verbose=1)\n", "intent": "Fit the model on the data provided\n"}
{"snippet": "from sklearn.linear_model import SGDClassifier\nmy_sgd_model = SGDClassifier(n_iter=100, alpha=0.01)\n", "intent": "- Stochastic Gradient Descent  \nhttp://scikit-learn.org/stable/modules/sgd.html\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal([vocab_size, embed_dim], stddev=0.1), \n                            dtype=tf.float32, name=\"embedding\")\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "knn.fit(X, y)\n", "intent": "**Step 3:** Fit the model with data (aka \"model training\")\n- Model is learning the relationship between X and y\n- Occurs in-place\n"}
{"snippet": "formula = \"HHDelivPurDiff ~ PurRatecardHHImps + Operator\"\nmodel = sm.ols(formula, data=matchedData).fit()\nprint model.summary()\n", "intent": "Linear Regression\n=================\nTry and explain the difference in delivered impressions and purchased impressions through linear regression\n"}
{"snippet": "from sklearn import tree\nfrom sklearn.externals.six import StringIO\ndt = tree.DecisionTreeClassifier()\ndt = dt.fit(X, matchedData.HHBinaryDeliv)\nwith open(\"decision tree.dot\", 'w') as f:\n    f = tree.export_graphviz(dt, out_file=f)\n", "intent": "Decision Tree Output\n====================\n"}
{"snippet": "model = sm.ols(formula = ' sl ~ yd + yr + sx +rk', data=data).fit()\n", "intent": "___\nsex _beta_ changed likely because it is correlated to the other variables\n"}
{"snippet": "model = sm.ols(formula = ' sl ~ yd + yr + sx + rk + sx*rk', data=data).fit()\n", "intent": "___\n**Intercept**: reflects 0 year, female, assistant, 0 years from degree\n"}
{"snippet": "model = LinearRegression()\nX = pd.get_dummies(data.rk)\nmodel.fit(X, data.sl)\n", "intent": "___\nshould drop one of the dummy columns, otherwise will get a nonsense intercept (can't turn all of them off)\n"}
{"snippet": "ridge = Ridge(normalize=True)  \ncoefs = []\nfor a in alphas:\n    ridge.set_params(alpha=a)\n    ridge.fit(X, y)\n    coefs.append(ridge.coef_)\nnp.shape(coefs)\ncoefs[0]\n", "intent": "- Try different values of alpha to compare the affect on the weights\n"}
{"snippet": "ridge = Ridge(normalize=True)  \ncoefs = []\nfor a in alphas:\n    ridge.set_params(alpha=a)\n    ridge.fit(X, y)\n    coefs.append(ridge.coef_)\nnp.shape(coefs)\n", "intent": "- Try different values of alpha to compare the affect on the weights\n"}
{"snippet": "import statsmodels.api as sm\nX = np.array(xs).transpose()\nX = sm.add_constant(X)\nmod = sm.OLS(ys, X)\nres = mod.fit()\nres.summary()\n", "intent": "This generated a fit of $y = 3 x + 1$. Let's see what a linear regression yields.\n"}
{"snippet": "lm = linear_model.LinearRegression()\nX = np.vander(xs, 4)\ny = ys\nprint X, y\n", "intent": "Now we fit a model to the data. If we try to fit a size degree polynomial to the data we should obtain a very overfitted model.\n"}
{"snippet": "grid.fit(X, y)\n", "intent": "- You can set **`n_jobs = -1`** to run computations in parallel (if supported by your computer and OS)\n"}
{"snippet": "from sklearn.cluster import KMeans\nkmeans = KMeans(2)\nkmeans\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "with pm.Model() as logistic_model:\n    pm.glm.glm('bush ~ female + black', df, family=pm.glm.families.Binomial())\n    trace_logistic_model = pm.sample(2000, pm.NUTS(), progressbar=True)\n", "intent": "We'll start by doing a simple model of female and black voters on Bush voteshare:\n"}
{"snippet": "bnb = nb.BernoulliNB()\ngnb = nb.GaussianNB()\nmnb = nb.MultinomialNB()\n", "intent": "Instantiate the Naive Bayes predictor from scikit-learn with the training data. \n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogit = LogisticRegression()\n", "intent": "Define a logistic regression and train it with the feature and target set\n"}
{"snippet": "fit_eval_model(logit)\n", "intent": "Produce the accuracy score of the logistic regression from the test set\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim]))\n    embed = tf.nn.embedding_lookup(embedding,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "x = torch.arange(10, 50, dtype=torch.float).view(4, 10)\nxlen = torch.LongTensor([4, 8, 1, 10])\nm = torch.arange(x.size(1)).unsqueeze(0).expand(x.size())\nmask = xlen.unsqueeze(1).expand(x.size()) <= m\nx[mask] = float('-inf')\nx\n", "intent": "Set unmasked elements to zero.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)\n", "intent": "Now its time to train our model!\n**Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.**\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\ngrid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "rand = RandomizedSearchCV(knn, param_dist, cv=10, scoring='accuracy', n_iter=10, random_state=5)\nrand.fit(X, y)\nrand.grid_scores_\n", "intent": "- **Important:** Specify a continuous distribution (rather than a list of values) for any continous parameters\n"}
{"snippet": "writer = tf.summary.FileWriter(LOGDIR)\nwriter.add_graph(sess.graph)\ntf.summary.histogram('m', m)\ntf.summary.histogram('b', b)\ntf.summary.scalar('loss', loss)\nsummary_op = tf.summary.merge_all()\n", "intent": "Later, we'll get this for free.\n"}
{"snippet": "tf.reset_default_graph()\nsess = tf.Session()\n", "intent": "We will now use a linear model to classify handwritten digits from the MNIST dataset.\n"}
{"snippet": "x, y = get_input_fn(TRAIN_INPUT, 1)()\nwith tf.Session() as s:\n    print(s.run(x))\n    print(s.run(y))\n", "intent": "Testing the input function\n"}
{"snippet": "tf.reset_default_graph()\nhidden1_units = 128\nlearning_rate = 0.005\ninput_dimension = train_data.shape[1]   \noutput_dimension = train_labels.shape[1]  \noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\nimages_column = tf.contrib.layers.real_valued_column(\"images\")\ncanned_model = _\n", "intent": "Instead of defining our own DNN classifier, TensorFlow supplies a number of *canned* estimators that can save a lot of work.\n"}
{"snippet": "logreg = linear_model.LogisticRegression(solver='newton-cg')\nX = cr['Hours Researched'].values.reshape(-1,1)\nY = cr['Hired']\nlogreg.fit(X,Y)\n", "intent": "We call our instance `logreg`, and then feed it our data:\n"}
{"snippet": "lgrg = linear_model.LogisticRegression(solver='newton-cg')\nX = loans[['Credit Score','Loan Request Amount']]\nY = loans['Approval']\nlgrg.fit(X,Y)\n", "intent": "We feed the data to the `linear_model.LogisticRegression()` the same as above, but now our `X` consists of multiple columns.\n"}
{"snippet": "loans_model(650,20)\n", "intent": "The prediction is that the loan will be approved, and we can determine the probability of approval with:\n"}
{"snippet": "ftr = tree.DecisionTreeClassifier()\nftr.fit(fr, fruits['Category'])\n", "intent": "Again, we make an instance of the `DecisionTreeClassifier()` class:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(1000,)))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "import tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n", "intent": "First we start with loading TensorFlow and reseting the computational graph.\n"}
{"snippet": "import xgboost\ngbtree = xgboost.XGBClassifier(n_estimators=300, max_depth=5)\ngbtree.fit(encoded_train, labels_train)\n", "intent": "This time, we use gradient boosted trees as the model, using the [xgboost](https://github.com/dmlc/xgboost) package.\n"}
{"snippet": "import statsmodels.api as sm\nX = np.array(xs).transpose()\nX = sm.add_constant(X)\nmod = sm.OLS(ys, X)\nres = mod.fit()\nprint res.summary()\n", "intent": "This generated a fit of $y = 4 x + 1$. Let's see what a linear regression yields.\n"}
{"snippet": "neigh = KNeighborsClassifier(n_neighbors=3)\n", "intent": "Tell Phyton that we will use kNN Classifier with a k value of 3\n"}
{"snippet": "neigh.fit(X, y) \n", "intent": "Fit the kNN Classifier to the X & y data\n"}
{"snippet": "km = KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "km.fit(df.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "scaler.fit(df.iloc[:,:-1])\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "KNN = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "KNN.fit(X_train,y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Get graph handle with the tf.Session()\n"}
{"snippet": "nb = MultinomialNB()\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "grid=GridSearchCV(SVC(),param_grid,verbose=1)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=1)\nlr.fit(features_train, target_train)\n", "intent": "Let's start with a simple model from sklearn, namely `LogisticRegression`:\n"}
{"snippet": "new_docs = ['He watches basketball and baseball', 'Julie likes to play basketball', 'Jane loves to play baseball']\nnew_term_freq_matrix = tfidf_vectorizer.transform(new_docs)\nprint tfidf_vectorizer.vocabulary_\nprint new_term_freq_matrix.todense()\n", "intent": "That was easy! Next, we can fit new observations into this vocabulary space like so:\n"}
{"snippet": "X = torch.IntTensor(3, 2).fill_(0)\nprint(\"X=>\", X.type())\nY = X.float()\nprint(\"Y=>\", Y.type())\n", "intent": "Q3. Convert the data type of X to float32.\n"}
{"snippet": "X = torch.LongTensor(3, 2).fill_(0)\nprint(\"X=>\", X.type())\nY = X.int()\nprint(\"Y=>\", Y.type())\n", "intent": "Q4. Convert the data type of X to int32.\n"}
{"snippet": "X = torch.arange(1, 22).resize_(3, 7)\nprint(X)\ny = torch.LongTensor([0, 2, 4, 5])\nprint(X.index_select(1, y))\n", "intent": "Q12. Return the columns of indices y.\n"}
{"snippet": "x = torch.Tensor([1, 4])\ny = torch.Tensor([2, 5])\nz = torch.Tensor([3, 6])\nO = torch.stack((x, y, z), 0)\nprint(O)\nassert np.array_equal(O.numpy(), np.stack([x.numpy(), y.numpy(), z.numpy()], 0))\n", "intent": "Q17. Stack x, y, and z vertically.\n"}
{"snippet": "X = torch.Tensor([[1, 2, 3], [4, 5, 6]])\nY = torch.Tensor([[7, 8, 9], [10, 11, 12]])\nZ = torch.cat((X, Y), 0)\nprint(Z)\nassert np.array_equal(Z.numpy(), np.concatenate((X.numpy(), Y.numpy()), 0))\n", "intent": "Q19. Concatenate X and Y along the first dimension.\n"}
{"snippet": "zero_var = tf.Variable(tf.zeros([row_dim, col_dim]))\nones_var = tf.Variable(tf.ones([row_dim, col_dim]))\n", "intent": "Here are variables initialized to contain all zeros or ones.\n"}
{"snippet": "X = torch.Tensor([[0,1,7,0,0],[3,0,0,2,19]])\ny = X.nonzero()\nprint(y)\nassert np.count_nonzero(X.numpy()) == y.size(0)\n", "intent": "Q23. Get the indices of all nonzero elements in X.\n"}
{"snippet": "means = torch.Tensor([1, 2, 3])\nstd = torch.Tensor([0.1, 0., -0.1])\nX = torch.normal(means=means, std=std)\nprint(X)\n", "intent": "Q3. Create a tensor of random numbers drawn from the given mean and standard deviation.\n"}
{"snippet": "means = torch.Tensor([1, 2, 3])\nstd = torch.Tensor([0.1, 0., -0.1])\nX = ...\nprint(X)\n", "intent": "Q3. Create a tensor of random numbers drawn from the given mean and standard deviation.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(X.sum(dim=0, keepdim=True))\n", "intent": "Q5. Sum the elements of X along the first dimension, retaining it.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(X.prod())\n", "intent": "Q6. Return the product of all elements in X.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(X.cumsum(dim=1))\n", "intent": "Q7. Return the cumulative sum of all elements along the second axis in X.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(X.cumprod(dim=1))\n", "intent": "Q8. Return the cumulative product of all elements along the second axis in X.\n"}
{"snippet": "x = torch.Tensor([1., 2., 3.])\ny = x.exp()\nprint(y)\nassert np.array_equal(y.numpy(), np.exp(x.numpy()))\n", "intent": "Q9. Compute $e^x$, element-wise.\n"}
{"snippet": "x = torch.Tensor([1, np.e, np.e**2])\ny = x.log()\nprint(y)\nassert np.allclose(y.numpy(), np.log(x.numpy()))\n", "intent": "Q10. Compute logarithms of x element-wise.\n"}
{"snippet": "zero_similar = tf.Variable(tf.zeros_like(zero_var))\nones_similar = tf.Variable(tf.ones_like(ones_var))\nsess.run(ones_similar.initializer)\nsess.run(zero_similar.initializer)\nprint(sess.run(ones_similar))\nprint(sess.run(zero_similar))\n", "intent": "If the shape of a tensor depends on the shape of another tensor, then we can use the TensorFlow built-in functions `ones_like()` or `zeros_like()`.\n"}
{"snippet": "x = torch.Tensor([1, 2, 3])\ny = torch.Tensor([-1, -2, -3])\nz = y.sub(x)\nprint(z)\nassert np.array_equal(z.numpy(), np.subtract(y.numpy(), x.numpy()))\n", "intent": "Q12. Subtract y from x element-wise.\n"}
{"snippet": "x = torch.Tensor([3, 4, 5])\ny = torch.Tensor([1, 0, -1])\nx_y = x.mul(y)\nprint(x_y)\nassert np.array_equal(x_y.numpy(), np.multiply(x.numpy(), y.numpy()))\n", "intent": "Q13. Multiply x by y element-wise.\n"}
{"snippet": "x = torch.FloatTensor([3., 4., 5.])\ny = torch.FloatTensor([1., 2., 3.])\nz = x.div(y)\nprint(z)\nassert np.allclose(z.numpy(), np.true_divide(x.numpy(), y.numpy()))\n", "intent": "Q14. Divide x by y element-wise.\n"}
{"snippet": "x = torch.Tensor([1, -1])\nnegx = x.neg()\nprint(negx)\nassert np.array_equal(negx.numpy(), np.negative(x.numpy()))\n", "intent": "Q15. Compute numerical negative value of x, element-wise.\n"}
{"snippet": "x = torch.Tensor([1., 2., .2])\ny = x.reciprocal()\nprint(y)\nassert np.array_equal(y.numpy(), np.reciprocal(x.numpy()))\n", "intent": "Q16. Compute the reciprocal of x, element-wise.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = torch.Tensor([[1, 2], [1, 2]])\nX_Y = X.pow(Y)\nprint(X_Y)\nassert np.array_equal(X_Y.numpy(), np.power(X.numpy(), Y.numpy()))\n", "intent": "Q17. Compute $X^Y$, element-wise.\n"}
{"snippet": "x = torch.Tensor([-3, -2, -1, 1, 2, 3])\ny = 2.\nz = x.remainder(y)\nprint(z)\nassert np.array_equal(z.numpy(), np.remainder(x.numpy(), y))\n", "intent": "Q18. Compute the remainder of x / y element-wise.\n"}
{"snippet": "x = torch.Tensor([1., 2.5, -3.2])\ny = x.frac()\nprint(y)\nassert np.allclose(y.numpy(), np.modf(x.numpy())[0])\n", "intent": "Q19. Compute the fractional portion of each element of x.\n"}
{"snippet": "X = torch.Tensor( [[-1, -2, -3], [0, 1, 2]] )\nY = X.eq(0)\nprint(Y)\nassert np.allclose(Y.numpy(), np.equal(X.numpy(), 0))\n", "intent": "Q21. Return 1 if an element of X is 0, otherwise 0.\n"}
{"snippet": "fill_var = tf.Variable(tf.fill([row_dim, col_dim], -1))\nsess.run(fill_var.initializer)\nprint(sess.run(fill_var))\n", "intent": "Here is how we fill a tensor with a constant.\n"}
{"snippet": "x = torch.Tensor([1., 4., 9.])\ny = x.sqrt()\nprint(y)\nassert np.array_equal(y.numpy(), np.sqrt(x.numpy()))\n", "intent": "Q27. Compute square root of x element-wise.\n"}
{"snippet": "x = torch.Tensor([1., 4., 9.])\ny = x.rsqrt()\nprint(y)\nassert np.allclose(y.numpy(), np.reciprocal(np.sqrt(x.numpy())))\n", "intent": "Q28. Compute the reciprocal of the square root of x, element-wise.\n"}
{"snippet": "X = torch.Tensor([[1, -1], [3, -3]])\nY = X.abs()\nprint(Y)\nassert np.array_equal(Y.numpy(), np.abs(X.numpy()))\n", "intent": "Q29. Compute the absolute value of X.\n"}
{"snippet": "x = torch.Tensor([1, 3, 0, -1, -3])\ny = x.sign()\nprint(y)\nassert np.array_equal(y.numpy(), np.sign(x.numpy()))\n", "intent": "Q30. Compute an element-wise indication of the sign of x, element-wise.\n"}
{"snippet": "x = torch.FloatTensor([1.2, 0.7, -1.3, 0.1])\ny = x.sigmoid()\nprint(y)\nassert np.allclose(y.numpy(), 1. / (1 + np.exp(-x.numpy())))\n", "intent": "Q31. Compute the sigmoid of x, elemet-wise.\n"}
{"snippet": "X = torch.Tensor([1,2,3,4])\nY = torch.Tensor([10,10,10,10])\nZ = X.lerp(Y, .9)\nprint(Z)\nassert np.allclose(Z.numpy(), X.numpy() + (Y.numpy()-X.numpy())*.9)\n", "intent": "Q32. Interpolate X and Y linearly with a weight of .9 on Y.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(...)\n", "intent": "Q5. Sum the elements of X along the first dimension, retaining it.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(...)\n", "intent": "Q6. Return the product of all elements in X.\n"}
{"snippet": "X = torch.Tensor(\n    [[1, 2, 3, 4],\n     [5, 6, 7, 8]])\nprint(...)\n", "intent": "Q7. Return the cumulative sum of all elements along the second axis in X.\n"}
{"snippet": "const_var = tf.Variable(tf.constant([8, 6, 7, 5, 3, 0, 9]))\nconst_fill_var = tf.Variable(tf.constant(-1, shape=[row_dim, col_dim]))\nsess.run(const_var.initializer)\nsess.run(const_fill_var.initializer)\nprint(sess.run(const_var))\nprint(sess.run(const_fill_var))\n", "intent": "We can also create a variable from an array or list of constants.\n"}
{"snippet": "x = torch.Tensor([1., 2., 3.])\ny = ...\nprint(y)\n", "intent": "Q9. Compute $e^x$, element-wise.\n"}
{"snippet": "x = torch.Tensor([1, np.e, np.e**2])\ny = ...\nprint(y)\n", "intent": "Q10. Compute logarithms of x element-wise.\n"}
{"snippet": "x = torch.Tensor([1, 2, 3])\ny = torch.Tensor([-1, -2, -3])\nz = ...\nprint(z)\n", "intent": "Q11. Add x and y element-wise.\n"}
{"snippet": "x = torch.Tensor([1, 2, 3])\ny = torch.Tensor([-1, -2, -3])\nz = ...\nprint(z)\n", "intent": "Q12. Subtract y from x element-wise.\n"}
{"snippet": "x = torch.Tensor([3, 4, 5])\ny = torch.Tensor([1, 0, -1])\nx_y = ...\nprint(x_y)\n", "intent": "Q13. Multiply x by y element-wise.\n"}
{"snippet": "x = torch.FloatTensor([3., 4., 5.])\ny = torch.FloatTensor([1., 2., 3.])\nz = ...\nprint(z)\n", "intent": "Q14. Divide x by y element-wise.\n"}
{"snippet": "x = torch.Tensor([1, -1])\nnegx = ...\nprint(negx)\n", "intent": "Q15. Compute numerical negative value of x, element-wise.\n"}
{"snippet": "x = torch.Tensor([1., 2., .2])\ny = ...\nprint(y)\n", "intent": "Q16. Compute the reciprocal of x, element-wise.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = torch.Tensor([[1, 2], [1, 2]])\nX_Y = ...\nprint(X_Y)\n", "intent": "Q17. Compute $X^Y$, element-wise.\n"}
{"snippet": "import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n", "intent": "We introduce how to use placeholders in TensorFlow.\nFirst we import the necessary libraries and reset the graph session.\n"}
{"snippet": "x = torch.Tensor([1., 2.5, -3.2])\ny = ...\nprint(y)\n", "intent": "Q19. Compute the fractional portion of each element of x.\n"}
{"snippet": "X = torch.Tensor( [[-1, -2, -3], [0, 1, 2]] )\nY = ...\nprint(Y)\n", "intent": "Q21. Return 1 if an element of X is 0, otherwise 0.\n"}
{"snippet": "X = torch.Tensor( [[-1, -2, -3], [0, 1, 2]] )\nY = ...\nprint(Y)\n", "intent": "Q22. Return 0 if an element of X is 0, otherwise 1.\n"}
{"snippet": "x = torch.Tensor([1., 4., 9.])\ny = ...\nprint(y)\n", "intent": "Q27. Compute square root of x element-wise.\n"}
{"snippet": "x = torch.Tensor([1., 4., 9.])\ny = ...\nprint(y)\n", "intent": "Q28. Compute the reciprocal of the square root of x, element-wise.\n"}
{"snippet": "X = torch.Tensor([[1, -1], [3, -3]])\nY = ...\nprint(Y)\n", "intent": "Q29. Compute the absolute value of X.\n"}
{"snippet": "x = torch.Tensor([1, 3, 0, -1, -3])\ny = ...\nprint(y)\n", "intent": "Q30. Compute an element-wise indication of the sign of x, element-wise.\n"}
{"snippet": "x = torch.FloatTensor([1.2, 0.7, -1.3, 0.1])\ny = ...\nprint(y)\n", "intent": "Q31. Compute the sigmoid of x, elemet-wise.\n"}
{"snippet": "X = torch.Tensor([1,2,3,4])\nY = torch.Tensor([10,10,10,10])\nZ = ...\nprint(Z)\n", "intent": "Q32. Interpolate X and Y linearly with a weight of .9 on Y.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Start graph session\n"}
{"snippet": "x = torch.Tensor([1, 2])\nY = torch.Tensor([[0, 0], [1, 1]])\nz = x.matmul(Y)\nprint(z)\nassert torch.equal(z, x.unsqueeze(0).mm(Y).squeeze()) is True\n", "intent": "Q2. Compute the product of vector x and matrix Y.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\ny = torch.Tensor([3, 4])\nz = X.matmul(y)\nprint(z)\nassert torch.equal(z, X.mv(y)) is True\n", "intent": "Q3. Compute a matrix-vector product of matrix X and vector y.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = torch.Tensor([[0, 0], [1, 1]])\nZ = X.matmul(Y)\nprint(Z)\nassert torch.equal(Z, X.mm(Y)) is True\n", "intent": "Q4. Compute a matrix multiplication of matrix X and Y.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\ny = torch.Tensor([3, 4])\nm = torch.ones(2)\nprint(m.addmv(X, y))\n", "intent": "Q7.Express the below computation as a single line.<br>\n`m + torch.mv(X, y)`\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = torch.Tensor([[0, 0], [1, 1]])\nM = torch.ones(2, 2)\nZ = M.addmm(X, Y)\nprint(Z)\n", "intent": "Q8.Express the below computation as a single line.<br/>\n`M + torch.mm(X, Y)`\n"}
{"snippet": "X = torch.Tensor([[2, 0, 0], [0, 3, 4], [0, 4, 9]])\ne, v = X.eig(eigenvectors=True)\nprint(\"eigen values=\", e)\nprint(\"eigen vectors=\", v)\n_e, _v = np.linalg.eig(X.numpy())\nassert np.allclose(e.numpy()[:, 0], _e)\n", "intent": "Q14. Compute the eigenvalues and right eigenvectors of X.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = X.inverse()\nprint(Y)\nassert np.allclose(Y.numpy(), np.linalg.inv(X.numpy()))\n", "intent": "Q17. Compute the inverse of X.\n"}
{"snippet": "x = torch.Tensor([1, 2])\ny = torch.Tensor([3, 4])\nz = ...\nprint(z)\n", "intent": "Q1. Compute the inner product of two vectors x and y.\n"}
{"snippet": "x = torch.Tensor([1, 2])\nY = torch.Tensor([[0, 0], [1, 1]])\nz = ...\nprint(z)\n", "intent": "Q2. Compute the product of vector x and matrix Y.\n"}
{"snippet": "import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\n", "intent": "This function introduces various ways to create\nmatrices and how to use them in TensorFlow\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = torch.Tensor([[0, 0], [1, 1]])\nZ = ...\nprint(Z)\n", "intent": "Q4. Compute a matrix multiplication of matrix X and Y.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\ny = torch.Tensor([3, 4])\nm = torch.ones(2)\nZ = ...\nprint(Z)\n", "intent": "Q7.Express the below computation as a single line.<br>\n`m + torch.mv(X, y)`\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = torch.Tensor([[0, 0], [1, 1]])\nM = torch.ones(2, 2)\nZ = ...\nprint(Z)\n", "intent": "Q8.Express the below computation as a single line.<br/>\n`M + torch.mm(X, Y)`\n"}
{"snippet": "X = torch.Tensor([[2, 0, 0], [0, 3, 4], [0, 4, 9]])\ne, v = ...\nprint(\"eigen values=\", e)\nprint(\"eigen vectors=\", v)\n", "intent": "Q14. Compute the eigenvalues and right eigenvectors of X.\n"}
{"snippet": "X = torch.Tensor([[1, 2], [3, 4]])\nY = ...\nprint(Y)\n", "intent": "Q17. Compute the inverse of X.\n"}
{"snippet": "X = Variable(torch.ones(3, 2))\nprint(X)\n", "intent": "Q0. Create a variable `X` of the size (3, 2), filled with 1's.\n"}
{"snippet": "X = Variable(torch.randn(3, 3))\nY = X.data\nprint(Y)\n", "intent": "Q1. Get the tensor of Variable X.\n"}
{"snippet": "X = Variable(torch.randn(3, 3))\nY = ...\nprint(Y)\n", "intent": "Q1. Get the tensor of Variable X.\n"}
{"snippet": "torch.manual_seed(3)\nx = Variable(Tensor([1, 2, 3]).unsqueeze(1))\nW = Variable(Tensor(3, 3))\nb = Variable(Tensor(3, 1))\ntorch.set_printoptions(precision=1)\nprint(x, W, b)\n", "intent": "The goal is to derive the gradients for W, x, and b for this function:\n$$ f(W, x, b) = \\sum\\sigma(Wx + b) $$\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Start a graph session\n"}
{"snippet": "hist = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test), verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    n_vocab = vocab_size\n    n_embedding = embed_dim \n    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "print('Checking the Training on a Single Batch...')\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(epochs):\n        batch_i = 1\n        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n", "intent": "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches.\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingRegressor\nregressor=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)\nregressor.fit(X_train, y_train)\n", "intent": "9.GradientBoostingRegressor\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\nmodel=GradientBoostingClassifier(learning_rate=0.1, min_samples_split=100,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10)\nperformTimeSeriesCV(X_train, y_train, 10,\n                    GradientBoostingClassifier,\n                    {'min_samples_split':100,'min_samples_leaf':50,'max_depth':8,'max_features':'sqrt','subsample':0.8,'random_state':10},\n                    {'n_estimators':[20,30,40,50,60,70,80],'learning_rate':[0.1]})\n", "intent": "5.2 Fix learning rate and number of estimators for tuning tree-based parameters\n"}
{"snippet": "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nmodel=QuadraticDiscriminantAnalysis()\nmodel.fit(X_train, y_train)\n", "intent": "6 QuadraticDiscriminantAnalysis\n"}
{"snippet": "X = iris[[\"petal_length\"]]\ny = iris[\"petal_width\"]\nmodel = linear_model.LinearRegression()\nresults = model.fit(X, y)\nprint (results.intercept_, results.coef_)\n", "intent": "Now let's use scikit-learn to find the best fit line.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nclfs = {'lr': LogisticRegression(random_state=0),\n        'mlp': MLPClassifier(random_state=0),\n        'dt': DecisionTreeClassifier(random_state=0),\n        'rf': RandomForestClassifier(random_state=0),\n        'svc': SVC(random_state=0)}\n", "intent": "In the dictionary:\n- the key is the acronym of the classifier\n- the value is the classifier (with random_state=0)\n"}
{"snippet": "D = tf.convert_to_tensor(np.array([[1., 2., 3.], [-3., -7., -1.], [0., 5., -2.]]))\nprint(sess.run(D))\n", "intent": "Create matrix from np array:\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=, random_state=0)\n", "intent": "Hint: Consider the number of class labels (in the data) when deciding the value of n_clusters for KMeans\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state=0)\nimportances = \n", "intent": "1. train the random forest classifier\n2. get the feature importances\n3. plot the bar chart of importances (in descending order)\n"}
{"snippet": "from sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.cluster import KMeans\nX_train, X_test, y_train, y_test = your_friends_diligent_work()\nkm = KMeans(n_clusters=3, random_state=0)\nprint(precision_recall_fscore_support(y_test_pred, y_test, average='micro'))\n", "intent": "1. You did not use any other packages\n2. The prediction accuracy on y_test is over 75%\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3, random_state=0)\n", "intent": "Hint: Consider the number of class labels (in the data) when deciding the value of n_clusters for KMeans\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state=0)\nrfc.fit(X_train_std, y_train)\nimportances = rfc.feature_importances_\n", "intent": "1. train the random forest classifier\n2. get the feature importances\n3. plot the bar chart of importances (in descending order)\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nclfs = {'lr': LogisticRegression(random_state=0),\n        'mlp': MLPClassifier(random_state=0),\n        'dt': DecisionTreeClassifier(random_state=0),\n        'rf': RandomForestClassifier(random_state=0),\n        'svc': SVC(random_state=0, probability=True)}\n", "intent": "In the dictionary:\n- the key is the acronym of the classifier\n- the value is the classifier (with random_state=0)\n"}
{"snippet": "vect.fit(X_train)\n", "intent": "Note : We can also perform tfidf transformation.\n"}
{"snippet": "prediction = dict()\nfrom sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(X_train_df,y_train)\n", "intent": "Generally, Naive Bayes works well on text data. Multinomail Naive bayes is best suited for classification with discrete features. \n"}
{"snippet": "clf.fit(X_scaled, y_scaled)\n", "intent": "Depending on how computationally heavy your algorithm is, this could take a while.\n"}
{"snippet": "print(sess.run(tf.nn.tanh([-1., 0., 1.])))\ny_tanh = sess.run(tf.nn.tanh(x_vals))\n", "intent": "Hyper Tangent activation\n"}
{"snippet": "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)\n", "intent": "Let's now fit the model and run it for one epoch.\n"}
{"snippet": "model = LinearRegression(normalize=True, fit_intercept=False)\nprint model.normalize\nprint model\n", "intent": "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated, and have suitable default values:\n"}
{"snippet": "model = LinearRegression(normalize=True, fit_intercept=False) \nprint model.normalize\nprint model\n", "intent": "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated, and have suitable default values:\n"}
{"snippet": "from sklearn import linear_model\nmodel = linear_model.Perceptron()\nmodel.fit(X, y)\nplot_decision_boundary(model, X, plot_boundary=True)\nscatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)\n", "intent": "The `Perceptron` is a simple algorithm suitable for large scale learning.\n"}
{"snippet": "classifier = svm.SVC(kernel='rbf', gamma=gammas[np.argmax(scores)])\nclassifier.fit(X, y)\nX_test_norm = scaler.transform(X_test)\nprint classifier.score(X_test_norm, y_test)\n", "intent": "Check the score of the best model in the test set.\n"}
{"snippet": "classifier = svm.SVC(kernel='rbf', gamma=gammas[np.argmax(scores)])\nclassifier.fit(U, y)\nX_test_norm = scaler.transform(X_test)\nU_test = pca.transform(X_test_norm)\nprint classifier.score(U_test, y_test)\n", "intent": "Check the score of the best model that uses PCA in the test set.\n"}
{"snippet": "classifier = svm.SVC(kernel='rbf', gamma=gammas[np.argmax(scores)])  \nclassifier.fit(X, y)\nX_test_norm = X_test/255.*2-1 \nprint classifier.score(X_test_norm, y_test)\n", "intent": "Check the score of the best model in the test set.\n"}
{"snippet": "W_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n", "intent": "Now that the image size is reduced to 7x7, we add a [fully-connected layer](https://en.wikipedia.org/wiki/Convolutional_neural_network\n"}
{"snippet": "W_fc2 = weight_variable([1024, labels_count])\nb_fc2 = bias_variable([labels_count])\ny = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n", "intent": "Finally, we add a softmax layer, the same one if we use just a  simple [softmax regression](https://en.wikipedia.org/wiki/Softmax_function).\n"}
{"snippet": "print(sess.run(tf.nn.softsign([-1., 0., 1.])))\ny_softsign = sess.run(tf.nn.softsign(x_vals))\n", "intent": "Softsign activation\n"}
{"snippet": "model_2 = ensemble.RandomForestClassifier(n_estimators=500, max_depth=12, random_state=0)\nmodel_2.fit(X_train2, y_train2)\n", "intent": "Provo il modello sul nuovo training set:\n"}
{"snippet": "model_5 = ensemble.RandomForestClassifier(n_estimators=500, random_state=0)\n", "intent": "Rieseguo il modello sui dati bilanciati:\n"}
{"snippet": "x = Variable(torch.ones(batch_size,1),requires_grad=True)\nout = model(x)\nout = torch.sum(out)\n", "intent": "- Forward_Pre_Hook is called Before Forward\n- Farward_Hook is called After Forward\n"}
{"snippet": "from keras.models import Sequential\nmodel = Sequential()\n", "intent": "Create a sequential model\n"}
{"snippet": "from keras.layers import Dense, Activation\nmodel.add(Dense(64, input_shape=(784, )))\nmodel.add(Activation('relu'))\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n", "intent": "Define its structure.\n"}
{"snippet": "lm.fit(X, bos.PRICE)\nlm.score(X, bos.PRICE)\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "lm = LinearRegression()\nlm.fit(X[['PTRATIO']], bos.PRICE)\n", "intent": "***\nTry fitting a linear regression model using only the 'PTRATIO' (pupil-teacher ratio by town)\nCalculate the mean squared error. \n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,verbose=2)\ngrid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "ols = sklearn.linear_model.LinearRegression()\ncolumns = ['dwelling_type_Condo', 'dwelling_type_Multi-Family', 'dwelling_type_Residential', 'dwelling_type_Unkown']\nols.fit(sac_w_dummies[columns],sac_w_dummies.price)\nzip(columns,ols.coef_)\n", "intent": "If we attempt a linear regression using all 4 dummy variables, there are an infinite number of options for the zero-intercept point. \n"}
{"snippet": "print(sess.run(tf.nn.softplus([-1., 0., 1.])))\ny_softplus = sess.run(tf.nn.softplus(x_vals))\n", "intent": "Softplus activation\n"}
{"snippet": "logreg = LogisticRegression()\nlogreg.fit(train_x, train_y)\n", "intent": "Fit a logistic regression on the training data.\n"}
{"snippet": "k = 5\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(dn1)\n", "intent": "Cluster the Data - We are going to use 5 clusters based off of the above scatterplot\n"}
{"snippet": "kmeans = KMeans(n_clusters=2)\nclusters = kmeans.fit(x)\n", "intent": "Set up the k-means clustering analysis. Use the graph from above to derive \"k\"\n"}
{"snippet": "dbscn = DBSCAN(eps = .5, min_samples = 5).fit(X)  \n", "intent": "Let's set up the DBSCAN from scikit using an epsilon of .5 and a minimum sample number of 5\n"}
{"snippet": "dbscn = DBSCAN(eps = 0.5, min_samples=5).fit(X_standard)\n", "intent": "Let's set up the DBSCAN from scikit using an epsilon of .5 and a minimum sample number of 5\n"}
{"snippet": "import scipy\ndata_dense = data_features.todense()\n", "intent": "Construct your LDA with the pieces above by fitting the model to your feature set and target\n"}
{"snippet": "x_cols = np.array(df)\nks = [n for n in range(2, 11)] \ny = np.empty(0)\nfor k in ks:\n    kmeans = KMeans(n_clusters=k, random_state=41).fit(x_cols)\n    y = np.append(y, kmeans.inertia_)  \n", "intent": "Low value of SS, because we want to minimize the distances to the centroids.\n"}
{"snippet": "fisher_c34 = LinearDiscriminantAnalysis(solver='eigen',n_components=2).fit(x,y_3_4)\nfisher_c34.coef_\n", "intent": "**Fitting Fisher projection by discriminating classes 3 and 4**\n"}
{"snippet": "from keras.utils.np_utils import to_categorical\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nnb_train = 10_000\nmodel.fit(X[:nb_train], to_categorical(y[:nb_train]), validation_split=.1, epochs=5)\n", "intent": "It's time to compile your model and fit it on MNSIT.\n- Compile your model\n- Fit it on MNIST\n"}
{"snippet": "print(sess.run(tf.nn.elu([-1., 0., 1.])))\ny_elu = sess.run(tf.nn.elu(x_vals))\n", "intent": "Exponential linear activation\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Flatten, Dense\nfrom keras.constraints import nonneg\nmodel = Sequential()\nmodel.add(Flatten(input_shape=X.shape[1:]))\nmodel.add(Dense(units=10, activation='sigmoid'))\nmodel.summary()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n", "intent": "The following code creates a multi-label logistic regression classifier to predict images with multiple MNIST digits.\n"}
{"snippet": "model100.fit(X100,y100,batch_size=100,epochs=50)\n", "intent": "- Fit Your RNN Model on `X100` and `Y100`\n- Run your model for `model.fit(..., batch_size=100, epochs=50)`\n"}
{"snippet": "model.fit(X1, Y1, batch_size=1, shuffle=False, epochs=50)\n", "intent": "- Fit your RNN model on `X1` and `Y1`\n- Fit your model with `model.fit(..., batch_size=1, shuffle=False, epochs=50)`\n"}
{"snippet": "model.fit(X100, Y100, batch_size=100, epochs=50)\n", "intent": "- Fit Your RNN Model on `X100` and `Y100`\n- Run your model for `model.fit(..., batch_size=100, epochs=50)`\n"}
{"snippet": "import tensorflow as tf\nX = tf.placeholder(dtype=tf.float32, shape=[1, 13], name='X')\ny = tf.placeholder(dtype=tf.float32, shape=[1, 1], name='y')\nW = tf.Variable(initial_value=tf.zeros(shape=[13, 1]), name='W')\nb = tf.Variable(initial_value=tf.zeros(shape=[1]), name='b')\nz = tf.matmul(X, W, name='z')\ny_pred = tf.add(z, b, name='y_pred')\nr = tf.subtract(y_pred, y, name='r')\nloss = tf.square(r, name='losses')\nX, y, W, b, z, y_pred, r, loss\n", "intent": "Notice here we specify the *shape* of each input tensor as a matrix as opposed to a scalar.\n"}
{"snippet": "A = tf.Variable(tf.random_normal(shape=[1]))\n", "intent": "We now create the variable for our computational graph, `A`.\n"}
{"snippet": "from support import show_graph\nshow_graph(sess.graph)\n", "intent": "1. TensorBoard\n2. Keras Layers\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation=\"relu\", input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes,activation='softmax'))\nmodel.summary()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, batch_size=32,\n          epochs=50,\n          validation_data=(x_test, y_test), \n          verbose=2)\nprint(x_train.shape)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "reg = fit_model(X_train, y_train)\nprint(\"Parameter 'max_depth' is {} for the optimal model.\".format(reg.get_params()['max_depth']))\n", "intent": "* What maximum depth does the optimal model have? How does this result compare to our previous guess?  \n"}
{"snippet": "oob_forest = RandomForestClassifier(oob_score = True, n_estimators = 100)\noob_forest.fit(X, y)\noob_forest.oob_score_\n", "intent": "**Warning:** Out of bag error can only be used with ensemble classifiers\n"}
{"snippet": "def DNN_model(X):\n    Y = X * tf.Variable(tf.ones([]), name=\"dummy1\") \n    Yout = tf.layers.dense(Y, 1, activation=None) \n    return Yout\n", "intent": "<a name=\"assignment1\"></a>\n<div class=\"alert alert-block alert-info\">\n**Assignment \n</div>\n"}
{"snippet": "tf.reset_default_graph() \nfeatures, labels, dataset_init_op, evaldataset_init_op = datasets(NB_EPOCHS)\nYout, loss, eval_metrics, train_op = model_fn(features, labels, linear_model)\n", "intent": "<a name=\"instantiate\"></a>\n"}
{"snippet": "sess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n", "intent": "This resets all neuron weights and biases to initial random values\n"}
{"snippet": "Hzero = np.zeros([BATCHSIZE, RNN_CELLSIZE * N_LAYERS])\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run([init])\n", "intent": "This resets all neuron weights and biases to initial random values\n"}
{"snippet": "predictions = []\nfor i in range(len(x_vals)):\n    x_val = [x_vals[i]]\n    prediction = sess.run(tf.round(tf.sigmoid(my_output)), feed_dict={x_data: x_val})\n    predictions.append(prediction[0])\naccuracy = sum(x==y for x,y in zip(predictions, y_vals))/100.\nprint('Ending Accuracy = ' + str(np.round(accuracy, 2)))\n", "intent": "Now we can also see how well we did at predicting the data by creating an accuracy function and evaluating them on the known targets.\n"}
{"snippet": "def rnn_loop(char_rnn, batch_ix):\n    batch_size, max_length = batch_ix.size()\n    hid_state = char_rnn.initial_state(batch_size)\n    logprobs = []\n    for x_t in batch_ix.transpose(0,1):\n        hid_state, logp_next = char_rnn(x_t, hid_state)  \n        logprobs.append(logp_next)\n    return torch.stack(logprobs, dim=1)\n", "intent": "Once we've defined a single RNN step, we can apply it in a loop to get predictions on each step.\n"}
{"snippet": "background_filter = cb.CatBoostClassifier(iterations=NUM_EPOCH, depth=4, learning_rate=0.1, loss_function='Logloss', custom_loss=['Accuracy'], random_seed=123, logging_level='Silent')\nbackground_filter.fit(X_filter, Y_filter, verbose=False);\n", "intent": "Background filter training\n"}
{"snippet": "class Tensor(object):\n    def __init__(self, data):\n        self.data=data\n    def add(self, more):\n        self.data+=more\n    def __repr__(self):\n        return 'data: '+str(self.data)\n", "intent": "https://keras-cn.readthedocs.io/en/latest/getting_started/functional_API/\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), dtype=tf.float32)\n    embedding = tf.nn.embedding_lookup(embedding, input_data)\n    return embedding\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegressionCV\nmodel_cv = LogisticRegressionCV(10)\nmodel_cv.fit(X_train, y_train)\n", "intent": "(y_true == y_predict).mean()\n"}
{"snippet": "logreg.fit(X_train, y_train)\nzip(feature_cols, logreg.coef_[0])\n", "intent": "Confirm that the coefficients make intuitive sense.\n"}
{"snippet": "model = XGBClassifier()\nmodel.fit(X_train, y_train)\n", "intent": "Let's finally try XGBoost. The hyperparameters are the same, some important ones being ```subsample```, ```learning_rate```, ```max_depth``` etc.\n"}
{"snippet": "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 2000, print_loss = True)\n", "intent": "Now, let's call the function L_layer_model on the dataset we have created.This will take 10-20 mins to run.\n"}
{"snippet": "nn_model.fit(train_set_x, train_set_y, epochs=10, batch_size=10)\n", "intent": "Now, to fit the model on the training input and training target dataset, we run the following command using a minibatch of size 10 and 10 epochs.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "We start a computational graph session.\n"}
{"snippet": "clf = RandomForestClassifier(random_state=42)\nparameters = {'n_estimators':[100, 200, 400, 600],\n              'max_depth':[100, 200, 400, 600],\n              'min_samples_split':[2, 5]}\nscorer = make_scorer(fbeta_score, beta=0.5)\ngrid_obj = GridSearchCV(estimator=clf,\n                        param_grid=parameters,\n                        scoring=scorer)\ngrid_fit = grid_obj.fit(X_t, y_t)\n", "intent": "Grid search is used to find best parameters.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "from keras.callbacks import EarlyStopping\nmy_callbacks = [EarlyStopping(monitor='val_acc', patience=5, mode='max')]\nhist = model.fit(x_train, y_train,\n          batch_size=32,\n          epochs=20,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "images, labels = next(iter(trainloader))\nimg = images[1].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "bag_mod.fit(training_data, y_train)\nrf_mod.fit(training_data, y_train)\nada_mod.fit(training_data, y_train)\nsvm_mod.fit(training_data, y_train)\nprint('fit done')\n", "intent": "> **Step 1**: Now, fit each of the above models to the appropriate data.  Answer the following question to assure that you fit the models correctly.\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\nmodel_GB = GradientBoostingClassifier(learning_rate=0.1,n_estimators=400,random_state = 42)\nmodel_GB.fit(X_train, y_train)\n", "intent": "Used values from Udacity project 1\n"}
{"snippet": "with train_graph.as_default():\n    saver = tf.train.Saver()\nwith tf.Session(graph=train_graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('../../../Data/For_emmbedding/checkpoints'))\n    embed_mat = sess.run(embedding)\n", "intent": "Restore the trained network if you need to:\n"}
{"snippet": "def conv_block(x, num_filters, filter_size, stride=(2,2), mode='same', act=True):\n    x = Convolution2D(num_filters, filter_size, filter_size, subsample=stride, border_mode=mode)(x)\n    x = BatchNormalization(mode=2)(x)\n    return Activation('relu')(x) if act else x\ndef res_block(initial_input, num_filters=64):\n    x = conv_block(initial_input, num_filters, 3, (1,1))\n    x = conv_block(x, num_filters, 3, (1,1), act=False)\n    return merge([x, initial_input], mode='sum')\n", "intent": "ConvBlock  \nResBlock\n"}
{"snippet": "def up_block(x, num_filters, size):\n    x = keras.layers.UpSampling2D()(x)\n    x = Convolution2D(num_filters, size, size, border_mode='same')(x)\n    x = BatchNormalization(mode=2)(x)\n    return Activation('relu')(x)\n", "intent": "Deconvolution / Transposed Conv / Fractionally Strident Convs\n"}
{"snippet": "A = tf.Variable(tf.random_normal(shape=[1]))\nmy_output = tf.multiply(x_data, A)\n", "intent": "We create the one variable in the graph, `A`.  We then create the model operation, which is just the multiplication of the input data and `A`.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim),-1,1))\n    print(\"get_embed input:\",input_data)\n    print(\"get_embed embedding:\",embedding)\n    return tf.nn.embedding_lookup(embedding,input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "import pickle\nA = pickle.load(file('matrix_sparse.pkl', 'r')).todense()\nnum_measurements = np.sum(np.abs(A)/2)\nnum_edges = np.sum(A != 0)/2\nprint('There are %s edges.'%(num_edges))\nprint('There are %s measurements.'%(num_measurements))\n", "intent": "1.\nHow many measurements are in the matrix?\n"}
{"snippet": "import networkx as nx\nG = nx.Graph(A)\nprint('Size of components in A:')\nfor i, component in enumerate(nx.connected_components(G)):\n    print('  Component %s contains %s nodes.'%(i, len(component)))\n", "intent": "2.\nIs the graph connected?\nAlmost but no.\n"}
{"snippet": "X_r = pca.fit(X).transform(X)\nprint(X_r)\n", "intent": "We then fit `X` to get the model and then transform the original data of four variables (`X`) to two variables (components).\n"}
{"snippet": "pca.fit(faces)\n", "intent": "Fit the faces data:\n"}
{"snippet": "model_top = Sequential()\nmodel_top.add(Flatten(input_shape=train_data.shape[1:]))\nmodel_top.add(Dense(256, activation='relu'))\nmodel_top.add(Dropout(0.5))\nmodel_top.add(Dense(1, activation='sigmoid'))\nmodel_top.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n", "intent": "And define and train the custom fully connected neural network :\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    return tf.nn.embedding_lookup(tf.Variable(tf.random_uniform((vocab_size, embed_dim), -0.1, 0.1)), input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "biases = tf.Variable(tf.zeros([num_classes]), name='biases')\nprint biases\n", "intent": "The second variable that must be optimized is called `biases` and is defined as a 1-dimensional tensor (or vector) of length `num_classes`.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded_input = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embedded_input = tf.nn.embedding_lookup(embedded_input, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "ops.reset_default_graph()\nsess = tf.Session()\n", "intent": "------------------\nWe start by resetting the computational graph\n"}
{"snippet": "inp = Input(shape=(SEQ_LEN,))\nemb = Embedding(VOCAB_SIZE, EMBEDDING_LEN_50, input_length=SEQ_LEN)(inp)\nx = Convolution1D(filters=32,kernel_size=4,padding='same')(emb)\nx = MaxPooling1D(pool_size=(2))(x)\nlstm = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(x)\npreds = Dense(2, activation = 'softmax')(lstm)\nconv_lstm_model = Model(inputs=inp, outputs=preds)\nconv_lstm_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\nconv_lstm_model.summary()\n", "intent": "We can also work with convolution and LSTM layers in a model.\n"}
{"snippet": "inp = Input(shape=(SEQ_LEN,))\nemb = Embedding(VOCAB_SIZE, EMBEDDING_LEN_50, input_length=SEQ_LEN)(inp)\nlstm = GRU(100)(emb)\npreds = Dense(2, activation = 'softmax')(lstm)\nsimple_gru_model = Model(inputs=inp, outputs=preds)\nsimple_gru_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\nsimple_gru_model.summary()\n", "intent": "Creating a GRU based model is as simpel as simply replacing the LSTM layer above with the GRU layer\n"}
{"snippet": "def dense_model():\n    model = Sequential()\n    model.add(BatchNormalization(axis=1, input_shape=(28,28,1)))\n    model.add(Flatten())\n    model.add(Dense(1024, activation='relu'))\n    model.add(Dense(10, activation = 'softmax'))\n    model.compile(optimizer = Adam(), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    model.summary()\n    return model\n", "intent": "Now let us create a model which has a single dense layer in it\n"}
{"snippet": "def dense_model_dropout():\n    model = Sequential()\n    model.add(BatchNormalization(axis=1, input_shape=(28,28,1)))\n    model.add(Flatten())\n    model.add(Dense(1024, activation='relu'))\n    model.add(Dropout(0.6))\n    model.add(Dense(10, activation = 'softmax'))\n    model.compile(optimizer = Adam(), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    model.summary()\n    return model\n", "intent": "Since our above model was overfitting, we can add Dropout to it in order to add regularization to our deep neural network\n"}
{"snippet": "outputs = [] \nfor i in [1,2,3]:\n    outputs.append(get_output_layers_of_vgg(i))\nvgg_content = Model(inputs=vgg_inp, outputs=outputs)\n", "intent": "We need to compare thee outputs among some conv layers. Here we are selecting the conv2 layers of blocks 1,2 and 3.\n"}
{"snippet": "K.set_value(super_res_model.optimizer.lr, 1e-4)\nsuper_res_model.fit(x=[arr_lr, arr_hr], y=targ, batch_size=32, epochs=1)\n", "intent": "Running one more epoch ...\n"}
{"snippet": "def copy_weights(from_layers, to_layers):\n    for from_layer,to_layer in zip(from_layers,to_layers):\n        to_layer.set_weights(from_layer.get_weights())\n", "intent": "Now let us copy the weights of our previously trained model, to this new model which takes high res images as input.\n"}
{"snippet": "inp = Input(shape=(g_inp_dim,), name=\"input_G\")\nx = Dense(200, activation='relu')(inp)\nx = Dense(400, activation='relu')(x)\noutp = Dense(784, activation='sigmoid')(x)\nMLP_G = Model(inputs=inp, outputs=outp)\n", "intent": "The generator has 784 as the output shape because we want the generator to create a 28x28 image.\n"}
{"snippet": "MLP_C = Sequential([MLP_G,MLP_D])\nMLP_C.compile(optimizer='adam', loss='binary_crossentropy')\n", "intent": "Now, for the generative flow, we need to combined both G and D in a sequence since the output of G should be fed into D.\n"}
{"snippet": "A = tf.Variable(tf.random_normal(shape=[1,1]))\nmy_output = tf.matmul(x_data, A)\n", "intent": "We create the one variable in the graph, `A`.  We then create the model operation, which is just the multiplication of the input data and `A`.\n"}
{"snippet": "def fit_single_model():\n    new_model = fc_model()\n    new_model.optimizer.lr = 1e-3\n    new_model.fit(train_features_to_dense, train_labels, epochs=10, \n             batch_size=batch_size*4, validation_data=(valid_features_to_dense, valid_labels))\n    new_model.optimizer.lr = 1e-6\n    new_model.fit(train_features_to_dense, train_labels, epochs=15, \n             batch_size=batch_size*4, validation_data=(valid_features_to_dense, valid_labels))\n    return new_model\n", "intent": "Lets create an ensemble to get a further boost to our predictions\n"}
{"snippet": "new_model.optimizer.lr = 1e-6\nnew_model.fit(train_features_to_dense, train_labels, epochs=4, \n             batch_size=batch_size*4, validation_data=(valid_features_to_dense, valid_labels))\n", "intent": "Running for a few more epochs ...\n"}
{"snippet": "new_model.optimizer.lr = 1e-6\nnew_model.fit(train_features_to_dense, train_labels, epochs=4, \n             batch_size=batch_size*8, validation_data=(valid_features_to_dense, valid_labels))\n", "intent": "A few more epochs ...\n"}
{"snippet": "layer_model = Model(inputs=base_vgg_model.input, outputs=conv_layer_output)\n", "intent": "Now we want to snip our VGG model such that given an input image, we can get the outputs of this conv layer.\n"}
{"snippet": "mul_layer_model = Model(base_vgg_model.input, mul_conv_layer_outputs)\n", "intent": "Creating a model which gives the output of the above multiple conv layers.\n"}
{"snippet": "top_model = Model(inp, outp)\n", "intent": "We are only interested in the trained part of the model.\n"}
{"snippet": "hist = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_test, y_test), verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nclf= DecisionTreeClassifier(random_state=1)\nclf.fit(X_train, y_train)\n", "intent": "We start with creating a DecisionTreeClassifier and fit it to your training data.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeding = tf.Variable(tf.random_normal((vocab_size,embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embeding,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Then we start a computational graph session.\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators = 250, bootstrap = True, oob_score = True, max_depth = 50)\nrf.fit(X_train, y_train)\n", "intent": "Fitting a DTC with maximum depth of 6\n"}
{"snippet": "dtc = DecisionTreeClassifier(max_depth=6)\ndtc.fit(X_train, y_train)\n", "intent": "Fitting a DTC with maximum depth of 6\n"}
{"snippet": "model = Sequential()\n", "intent": "** Working on Model **\n"}
{"snippet": "weights = tf.Variable(tf.random_normal([2, 1], 0, 0.1))\nprint weights\n", "intent": "The objective is minimizing L2 loss\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    n_vocab = vocab_size\n    n_embedding = embed_dim \n    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n    embed_input = tf.nn.embedding_lookup(embedding, input_data)\n    return embed_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = sm.Logit(Y_train, X_train)\nresult = model.fit()\nresult.summary2()\n", "intent": "We can see the results by using result.summary2() function.\n"}
{"snippet": "logistic_reg = LogisticRegression()\nlogistic_reg.fit(titanic_data_numeric, titanic_label)\nprint(logistic_reg.score(titanic_data_numeric, titanic_label))\n", "intent": "- Define model\n- Fit titanic_data_numeric, titanic_label into this model\n"}
{"snippet": "happyModel.fit(x = X_train, y = Y_train, epochs=5, batch_size=32)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "def sigmoid(t):\n    return 1/(1 + np.e**-t)\n", "intent": "First, we define the sigmoid function:\n"}
{"snippet": "A = tf.Variable(tf.random_normal(shape=[1, 1]))\nb = tf.Variable(tf.random_normal(shape=[1, 1]))\n", "intent": "We are going to be doing a linear model, so we will need to create two variables, `A` (slope) and `b` (intercept).\n"}
{"snippet": "model = lm.LinearRegression(fit_intercept = True)\nmodel.fit(small_data[[\"sqft\"]], small_data[[\"price\"]])\nprint(\"Intercept:\", model.intercept_[0])\nprint(\"Slope:\", model.coef_[0,0])\n", "intent": "Your friend fits a linear model of sale price on home square footage with an intercept as shown below:\n"}
{"snippet": "mean0 = [-1,-1]  \nmean1 = [1,1] \ncov0 = [[.5, .28], [.28, .5]] \ncov1 = [[1, -.8], [-.8, 1]] \nmixture = util.GaussianMixture(mean0,cov0,mean1,cov1)\nmX,mY = mixture.sample(500,0.5,plot=True)\n", "intent": "*Exercise: Use Sklearn to fit a logistic regression model on the gaussian mixture data.*\n"}
{"snippet": "x_train = [1, 2, 3]\ny_train = [1, 2, 3]\nW = tf.Variable(tf.random_normal([1]), name = 'weight')\nb = tf.Variable(tf.random_normal([1]), name = 'bias')\nhypothesis = x_train * W + b\n", "intent": "$$ H(x) = Wx + b $$\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    emnedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embedded_imput = tf.nn.embedding_lookup(emnedding, input_data)\n    return embedded_imput\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, epochs=10, batch_size=50)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "import torch.nn as nn\ninput = Variable(torch.randn(4, 5), requires_grad=True)\ntarget = Variable(torch.randn(4, 5))\n", "intent": "Question: What is mean squared error? What are the inputs? What's the output?\n"}
{"snippet": "input = Variable(torch.randn(4, 5), requires_grad=True)\ntarget = Variable(torch.LongTensor(4).random_(5))\nprint(input)\nprint(target)\n", "intent": "Question: What is cross entropy loss? What are the inputs? What's the output?\n"}
{"snippet": "import torch.nn as nn\ninput = Variable(torch.randn(4, 5), requires_grad=True)\ntarget = Variable(torch.randn(4, 5))\n", "intent": "Question: What is mean square error? What are the inputs? What's the output?\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embedded = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Start a graph session:\n"}
{"snippet": "fit_1 =  sm.GLS(df.wage, X1).fit()\nfit_2 =  sm.GLS(df.wage, X2).fit()\nfit_3 =  sm.GLS(df.wage, X3).fit()\nfit_4 =  sm.GLS(df.wage, X4).fit()\nfit_5 =  sm.GLS(df.wage, X5).fit()\nsm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1)\n", "intent": "Selecting a suitable degree for the polynomial of age.\n"}
{"snippet": "svm3 = SVC(C=1, kernel='rbf', gamma=2)\nsvm3.fit(X_train, y_train)\n", "intent": "Comparing the ROC curves of two models on train/test data. One model is more flexible than the other.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embed = tf.Variable(tf.random_uniform((vocab_size, embed_dim),\n                                         -1, 1))\n    embedded = tf.nn.embedding_lookup(embed, input_data)\n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "hidden5_out = tf.get_default_graph().get_tensor_by_name(\"hidden5_out:0\")\nprint('done')\n", "intent": "3.\nLet's start by getting a handle on the output of the last frozen layer:\n"}
{"snippet": "dnn_clf_5_to_9 =  DNNClassifier(n_hidden_layers=4, random_state=42)\ndnn_clf_5_to_9.fit(X_train2, y_train2, n_epochs=1000, X_valid=X_valid2, y_valid=y_valid2)\n", "intent": "Let's compare that to a DNN trained from scratch:\n"}
{"snippet": "LR = LinearRegression()\nLR.fit(X_train_scaled, y_train)\n", "intent": "Now train a simple linear regression on scaled data:\n"}
{"snippet": "lr_pca = LinearRegression()\nlr_pca.fit(pca_features_train, y_train)\nplotModelResults(lr_pca,pca_features_train, pca_features_test, plot_intervals=True);\n", "intent": "Now train linear regression on pca features and plot its forecast.\n"}
{"snippet": "ridge.fit(X_train, y_train)\n", "intent": "MAE is ~48.5 which corresponds to ~1.16  error in \n"}
{"snippet": "db2 = DBSCAN(eps=1, min_samples=10)   \ndb2.fit(iris_data_scaled)\ndb2.labels_    \n", "intent": "What happens as we alter either $\\epsilon$ or min_samples?\nLet's first alter min_samples:\n"}
{"snippet": "A = tf.Variable(tf.random_normal(shape=[1,1]))\nmy_output = tf.matmul(x_data, A)\n", "intent": "We create the model variable `A` and the multiplication operation.\n"}
{"snippet": "lasso_all = Lasso().fit(pf_3_data,housing_data[housing_target])\nprint(\"Number of features in the model: \",len(lasso_all.coef_))\nnon_zero_features_mask = np.abs(lasso_all.coef_)>0.0001\nprint(non_zero_features_mask)\nprint(\"Number of non-zero features in the model: \",np.sum(non_zero_features_mask.astype(int)))\nprint(\"Fraction of total features used: \",float(np.sum(non_zero_features_mask.astype(int)))/len(lasso_all.coef_))\n", "intent": "Wow! That worked very well! We now have a much better model! Let's examine how many of the coefficients are non-zero when we fit on the full dataset:\n"}
{"snippet": "scaler.fit(df.drop('Class',axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n                                            hidden_units=[10, 20, 10],\n                                            n_classes=2,\n                                            model_dir=\"./output\")\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "model.fit(data.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "nn = MLPRegressor(max_iter=1000, random_state = seed)\n", "intent": "Define a neural network ([MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n"}
{"snippet": "_, mse_valid_lr = train_model(lr, df_train_X, df_train_y, df_valid_X, df_valid_y)\nprint 'Mean squared error on validation set using linear regression: ', '%.2f' % mse_valid_lr\n", "intent": "Obtain its performance on the validation set. Take a moment to think whether you need to use the original or standardised training / validation sets.\n"}
{"snippet": "mse_train_dt, mse_valid_dt = train_model(dt, df_train_X, df_train_y, df_valid_X, df_valid_y)\nprint 'Mean squared error on validation set using decision tree: ', '%.2f' % mse_valid_dt\n", "intent": "Obtain its performance on the validation set. Take a moment to think whether you need to use the original or standardised training / validation sets.\n"}
{"snippet": "nn_models = [nn1, nn2, nn3, nn4, nn5]\nfor i in range(len(nn_models)):\n    nn = nn_models[i]\n    _, mse_valid_nn = train_model(nn, df_train_X_std, df_train_y, df_valid_X_std, df_valid_y)\n    print 'Mean squared error on validation set using neural network nn' + str(i + 1), ': ', '%.2f' % mse_valid_nn\n", "intent": "Let's obtain the performance of each model on the validation set.\n"}
{"snippet": "model = DecisionTreeRegressor(random_state=seed)\n", "intent": "Run the following to define a new decision tree.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Create a graph session:\n"}
{"snippet": "from pytorchlearning import helper\nimages, labels = next(iter(trainloader))\nimg = images[0].view(1, 784)\nwith torch.no_grad():\n    logps = model(img)\nps = torch.exp(logps)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "t2= torch.Tensor(t)\nprint(t2)\nt3= torch.IntTensor([[1,21],[2,22]])\nprint(t3)b\n", "intent": "Ways of creating a tensor\n"}
{"snippet": "scaler.fit(df.drop(\"TARGET CLASS\",axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "knn = KNeighborsClassifier(5).fit(X_train, y_train)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n"}
{"snippet": "k = range(2, 10)\nparams = {'n_neighbors': k }\ngv = GridSearchCV(\n    estimator = KNeighborsClassifier(),\n    param_grid = params,\n    cv = kf\n)\ngv.fit(X, y)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n"}
{"snippet": "model2 = LogisticRegression()\nmodel2.fit(college[factors], college['admit'])\n", "intent": "Observe that the regularization serves to shrink the coefficient parameters\n"}
{"snippet": "estimator = KMeans(n_clusters=2)\nX = df[[\"x\", \"y\"]]\nestimator.fit(X)\nlabels = estimator.labels_\nprint(labels)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "from gensim.models.ldamodel import LdaModel\nfrom gensim.matutils import Sparse2Corpus\ncorpus = Sparse2Corpus(docs, documents_columns = False)\nnum_topics = 15\nlda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics)\n", "intent": "https://radimrehurek.com/gensim/\n"}
{"snippet": "dt = fit_model(tree.DecisionTreeClassifier(max_depth=3), X, y)\n", "intent": "[Example of a visualization](http://scikit-learn.org/stable/_images/iris.svg)  \n[sklearn reference](http://scikit-learn.org/stable/modules/tree.html)\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Next we start a graph session.\n"}
{"snippet": "knn = KNeighborsClassifier()\nparams = {\"n_neighbors\": range(2,20,2)}\nknn_grid = GridSearchCV(knn, params, n_jobs=-1, verbose=True, cv=cv)\nknn_grid.fit(X,y)\n", "intent": "Using grid search to tune the KNN model to see if it performed better than the logistic regression:\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(X,y)\n", "intent": "Building the model and evaluating mean performance:\n"}
{"snippet": "dt = DecisionTreeClassifier()\nparams = {\"max_depth\": [3,5,7,10,20],\"max_features\":[0.1, 0.3, 0.7, 1], \\\n         \"min_samples_split\":[2,3,5]}\ndt_grid = GridSearchCV(dt, params, n_jobs=-1, verbose=True, cv=cv)\ndt_grid.fit(X,y)\n", "intent": "Using grid search to tune the DT model to see if it performed better than the logistic regression:\n"}
{"snippet": "model.fit(X_train, y_train,\n          validation_data=(X_val, y_val), epochs=5);\n", "intent": "Keras models follow __Scikit-learn__'s interface of fit/predict with some notable extensions. Let's take a tour.\n"}
{"snippet": "class ReLU(Layer):\n    def __init__(self):\n        return np.maximum(0,input)\n    def backward(self,input,grad_output):\n", "intent": "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\n"}
{"snippet": "network = []\nnetwork.append(Dense(X_train.shape[1],100))\nnetwork.append(ReLU())\nnetwork.append(Dense(100,200))\nnetwork.append(ReLU())\nnetwork.append(Dense(200,10))\n", "intent": "We'll define network as a list of layers, each applied on top of previous one. In this setting, computing predictions and training becomes trivial.\n"}
{"snippet": "weights = tf.Variable(dtype= tf.float32, initial_value=tf.zeros([X.shape[1],1]), name=\"weight\")\nb = tf.Variable(dtype= tf.float32, initial_value=0, name=\"bias\")\n", "intent": "__Your code goes here.__ For the training and testing scaffolding to work, please stick to the names in comments.\n"}
{"snippet": "encoder,decoder = build_pca_autoencoder(img_shape,code_size=32)\ninp = L.Input(img_shape)\ncode = encoder(inp)\nreconstruction = decoder(code)\nautoencoder = keras.models.Model(inp,reconstruction)\nautoencoder.compile('adamax','mse')\n", "intent": "Meld them together into one model\n"}
{"snippet": "x_t = tf.placeholder('int32',(1,))\nh_t = tf.Variable(np.zeros([1,rnn_num_units],'float32'))\nnext_probs,next_h = rnn_one_step(x_t,h_t)\n", "intent": "Once we've trained our network a bit, let's get to actually generating stuff. All we need is the `rnn_one_step` function you have written above.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Next we create a graph session\n"}
{"snippet": "ops.reset_default_graph()\nsess = tf.Session()\n", "intent": "$$sigmoid(x) = {1 \\over{1 + e^x}}$$\n$$ReLU(x) = max(0, x)$$\n"}
{"snippet": "for i in range(1001):\n    mse_train = update_model(data_train)\n    if i % 100 == 0:\n        mse_val = get_cost(data_val)\n        print('Epoch {}: train mse: {}    validation mse: {}'.format(i, mse_train, mse_val))\n", "intent": "We can now train the network by supplying this function with our data and calling it repeatedly.\n"}
{"snippet": "input = Variable(torch.randn(1, 1, 32, 32))\nout = net(input)\nprint(out)\n", "intent": "The input to the forward is an ``autograd.Variable``, and so is the output.\n"}
{"snippet": "result = torch.Tensor(5, 3)\ntorch.add(x, y, out=result)\nprint(result)\n", "intent": "Addition: giving an output tensor\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n", "intent": "Finetuning the convnet\n----------------------\nLoad a pretrained model and reset final fully connected layer.\n"}
{"snippet": "print(\"Fitting the classifier to the training set\")\nt0 = time()\nparam_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\nclf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\nclf = clf.fit(X_train_pca, y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(\"Best estimator found by grid search:\")\nprint(clf.best_estimator_)\n", "intent": "Train a SVM classification model\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, input_dim=x_train.shape[1]))\nmodel.add(Activation('softmax'))\nmodel.add(Dense(2))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=500, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "neigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(data, target)\n", "intent": "Set kNN's k value to 3 and fit data\n"}
{"snippet": "sess = tf.Session()\n", "intent": "We create a graph session.\n"}
{"snippet": "_ = linear_regressor.fit(\n    input_fn=training_input_fn,\n    steps=100\n)\n", "intent": "Calling `fit()` on the feature column and targets will train the model.\n"}
{"snippet": "_, adagrad_training_losses, adagrad_validation_losses = train_nn_regression_model(\n    optimizer=tf.train.AdagradOptimizer(learning_rate=0.5),\n    steps=500,\n    batch_size=100,\n    hidden_units=[10, 10],\n    training_examples=normalized_training_examples,\n    training_targets=training_targets,\n    validation_examples=normalized_validation_examples,\n    validation_targets=validation_targets)\n", "intent": "First, let's try Adagrad.\n"}
{"snippet": "_, adam_training_losses, adam_validation_losses = train_nn_regression_model(\n    optimizer=tf.train.AdamOptimizer(learning_rate=0.009),\n    steps=500,\n    batch_size=100,\n    hidden_units=[10, 10],\n    training_examples=normalized_training_examples,\n    training_targets=training_targets,\n    validation_examples=normalized_validation_examples,\n    validation_targets=validation_targets)\n", "intent": "Now let's try Adam.\n"}
{"snippet": "_ = train_linear_classification_model(\n    learning_rate=0.03,\n    steps=1000,\n    batch_size=100,\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\n", "intent": "Here is a set of parameters that should attain roughly 0.9 accuracy.\n"}
{"snippet": "calibration_data = train_model(\n    learning_rate=0.05,\n    steps=500,\n    batch_size=5,\n    input_feature=\"rooms_per_person\")\n", "intent": "To verify that clipping worked, let's train again and print the calibration data once more.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn_params = {\n    'n_neighbors':range(5,20),\n    'weights':['distance','uniform']\n}\nknn_gs = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, verbose=1)\nknn_gs.fit(X_train, y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "lr_params = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\ngs_lrap = GridSearchCV(LogisticRegression(), lr_params, scoring = \"average_precision\", cv=5)\ngs_lrap = gs_lrap.fit(X, y)\nprint('Grid Search Best Score: %.4f' % gs_lrap.best_score_)\nprint('Grid Search Best Parameter for C: ')\nprint gs_lrap.best_params_\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "from sklearn.svm import SVC\nsvc_params = {\n    'C':np.logspace(-5,5),\n    'kernel':['linear']\n}\nsvc_gs = GridSearchCV(SVC(), svc_params, cv=3, verbose=1)\nsvc_gs.fit(X_train, y_train)\n", "intent": "Below is my Support Vector Machine score, it didn't improve.\n"}
{"snippet": "print('Gradient Boost with 1200 trees, learning rate of .9')\ngb3 = GradientBoostingRegressor(learning_rate=.9,n_estimators=1200)\nstage_score_plot(gb3, X_train, y_train, X_test, y_test, .9)\n", "intent": "- Train error goes to zero around 1,200 iterations (this is our optimal parameter then). Next adjust our learning rate.\n"}
{"snippet": "x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nA = tf.Variable(tf.random_normal(shape=[1,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\n", "intent": "We now initialize the placeholders and variables in the model.\n"}
{"snippet": "print('Gradient Boost with 1200 trees, learning rate of .1, max depth of 1')\ngb7 = GradientBoostingRegressor(learning_rate=.1,n_estimators=1200,max_depth=1)\nstage_score_plot(gb7, X_train, y_train, X_test, y_test, .1)\n", "intent": "- Alright, bse MSE around .45. Next, adjust max depth.\n"}
{"snippet": "print('Gradient Boost with 1200 trees, learning rate of .1, max depth of 10')\ngb8 = GradientBoostingRegressor(learning_rate=.1,n_estimators=1200,max_depth=10)\nstage_score_plot(gb8, X_train, y_train, X_test, y_test, .1)\n", "intent": "- It keeps improving, let's try a much higher max depth.\n"}
{"snippet": "random_search_gb_ensemble= RandomizedSearchCV(estimator=GradientBoostingRegressor(),param_distributions=gradient_boost_parameters,cv=3,n_iter=100,verbose=1)\nrandom_search_gb_ensemble.fit(final_predictions_df[['rd', 'myside', 'opptside', 'mypush', 'mypull', 'opptpush', 'opptpull',\n       'mychoice', 'mychoicecard', 'GLM_scaled_train', 'GB_train', 'RF_train',\n       'XGB_train', 'KNN_train']],final_predictions_df['actual_training'])\n", "intent": "- For our GB ensemble, find the best parameters for this training split.\n"}
{"snippet": "def ridge(X, y, d2):\n    return dot(dot(inv(dot(X.T, X) + d2*eye(X.shape[1])), X.T), y)\n", "intent": "The estimator for the ridge regression model is:\n$$\\hat{\\beta}^{ridge} = (X'X + \\alpha I)^{-1}X'y$$\n"}
{"snippet": "for i in [.1,1,10,100]:\n    svm_four = SVC(C=i,gamma=3)\n    svm_four.fit(rbf_df,rbf_df_labels)\n    print(' C = {}, Gamma = 3'.format(i))\n    decision_boundary(svm_four,np.array(rbf_df),np.array(rbf_df_labels))\n", "intent": "> Plot C from [1E-1,1,10,100] holding gamma constant at 3. What do you notice?\n"}
{"snippet": "svm_five = SVC(C=1.0,gamma=250)\nsvm_five.fit(rbf_df,rbf_df_labels)\ndecision_boundary(svm_five,np.array(rbf_df),np.array(rbf_df_labels))\n", "intent": "> This may take a while, but plot gamma at 250. What do you notice?\n"}
{"snippet": "svm_iris = SVC(kernel='rbf').fit(data_iris[:,:2],labels_iris)\n", "intent": "Now, run an SVM on the third and fourth column, and use the function below to plot the boundary. What do you notice?\n"}
{"snippet": "svm_iris = SVC(kernel='linear').fit(data_iris[:,:2],labels_iris)\ndecision_boundary(svm_iris,np.array(data_iris[:,:2]),np.array(labels_iris))\n", "intent": "- Which kernel can you use to correctly classify the last point?\n"}
{"snippet": "svm_iris = SVC(kernel='sigmoid').fit(data_iris[:,:2],labels_iris)\ndecision_boundary(svm_iris,np.array(data_iris[:,:2]),np.array(labels_iris))\n", "intent": "- Poly kernel get the last point correct.\n"}
{"snippet": "x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nA = tf.Variable(tf.random_normal(shape=[1,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n", "intent": "Now we can initialize placeholders, model variables, and model operations.\n"}
{"snippet": "multiNB_tf = MultinomialNB()\n", "intent": "Now fit a new NB with the tf-idf vectorized data\n"}
{"snippet": "from sklearn.svm import SVC\nlb_view = client.load_balanced_view()\nmodel = SVC()\nsvc_params = {\n    'C': np.logspace(-1, 2, 4),\n    'gamma': np.logspace(-4, 0, 5),\n}\nall_parameters, all_tasks = grid_search(\n   lb_view, model, digits_split_filenames, svc_params)\n", "intent": "Let's try on the digits dataset that we splitted previously as memmapable files:\n"}
{"snippet": "from implicit.als import AlternatingLeastSquares\nfrom scipy import sparse\nmodel = AlternatingLeastSquares(factors=20)\nuser_item_train = sparse.csr_matrix(train_data_matrix)\nmodel.fit(user_item_train.T)\nrecommendations = model.recommend(user_normalizer.lookup[10370],user_item_train,\\\n                      N=25,\\\n                filter_items=list(user_item_train[user_normalizer.lookup[125981]].indices)) \nrecommendations = [x[0] for x in recommendations[:20]]\n", "intent": "* There exist many packages that implement this algorithm, for example `implicit`\n"}
{"snippet": "model.fit(x_train, y_train,\n                    batch_size=64,\n                    epochs=30,\n                    verbose=1,\n                    initial_epoch=20,\n                    callbacks=callbacks_list,\n                    validation_data=(x_test, y_test))\n", "intent": "Train for 10 epochs from 20 to 30\n"}
{"snippet": "model.fit(x_train, y_train,\n                    batch_size=64,\n                    epochs=40,\n                    verbose=1,\n                    initial_epoch=30,\n                    callbacks=callbacks_list,\n                    validation_data=(x_test, y_test))\n", "intent": "Train for 10 epochs from 30 to 40\n"}
{"snippet": "model.fit(x_train, y_train,\n                    batch_size=64,\n                    epochs=50,\n                    verbose=1,\n                    initial_epoch=40,\n                    callbacks=callbacks_list,\n                    validation_data=(x_test, y_test))\n", "intent": "Train for 10 epochs from 40 to 50\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(n1, input_dim=n_features, activation='sigmoid'))\nmodel.add(Dense(n2, activation='sigmoid'))\nmodel.add(Dense(n_classes, activation='sigmoid'))\n", "intent": "The layers definitions can be also written in a more compact manner:\n"}
{"snippet": "def tanh(inputs):\n    values = [ np.tanh(x) for x in inputs]\n    return values\ny_tanh = tanh(x)\nline_graph(x, y_tanh, \"x\", \"tanh(x)\")\n", "intent": "- Hyperbolic tangent\n$$\\phi(x) = \\tanh(x)$$\n"}
{"snippet": "def ReLU(inputs):    \n    values = [x if x>0 else 0 for x in inputs]\n    return values\ny_relu = ReLU(x)\nline_graph(x, y_relu, \"x\", \"tanh(x)\")\n", "intent": "- Rectified Linear Unit fuction (ReLU)\n$$\\phi(x) = x I_{x \\ge 0}$$\n"}
{"snippet": "x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nA = tf.Variable(tf.random_normal(shape=[1,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n", "intent": "Same as before, we initialize the placeholders, variables, and model operations.\n"}
{"snippet": "model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))\n", "intent": "For training set ~8000, 20 epochs: \n"}
{"snippet": "model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))\n", "intent": "For training set 10000, 20 epochs: \n"}
{"snippet": "model_8_pretrained_layers = vgg16_model(img_rows, img_cols, channel, num_classes)\nmodel_8_pretrained_layers.summary()\n", "intent": "Let's train 50 epochs of model withou \n"}
{"snippet": "classifier = svm.SVC(gamma=0.0005)\nclassifier.fit( training_data, training_labels )  \n", "intent": "create a classifier (in this case a Support Vector Machine Classifier)\n"}
{"snippet": "with tf.Session() as sess:\n    print \"Addition with variables: %i\" % sess.run(add, feed_dict={a: 2, b: 3})\n    print \"Multiplication with variables: %i\" % sess.run(mul, feed_dict={a: 2, b: 3})\n", "intent": "In this case we need to use the `feed_dict` argument to pass the actual values to our graph.\n"}
{"snippet": "g = tf.get_default_graph()\n[op.name for op in g.get_operations()]\n", "intent": "What does the graph look like for this network?\n"}
{"snippet": "convolved = tf.nn.conv2d(img_4d, z_4d, strides=[1, 1, 1, 1], padding='SAME')\nconvolved\n", "intent": "The convolution operation is performed in tensorflow using the `nn.conv2d` function\n"}
{"snippet": "sess = tf.Session(graph=g)\nsess.close()\n", "intent": "We could also explicitly tell the session which graph we want to manage:\n"}
{"snippet": "convolved = tf.nn.conv2d(img_4d, z_4d,\n                         strides=[1, 1, 1, 1],\n                         padding='SAME')\nconvolved\n", "intent": "The convolution operation is performed in tensorflow using the `nn.conv2d` function\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Start a computational graph session:\n"}
{"snippet": "regr = LinearRegression()\n", "intent": "$$\ny = \\alpha + \\beta_0 x_0 + \\beta_1 x_1 + ...\n$$\n"}
{"snippet": "lm.fit(X,Y)\n", "intent": "Fit the linear model using highway-mpg.\n"}
{"snippet": " lm.fit(Z, df['price'])\n", "intent": "Fit the linear model using the four above-mentioned variables.\n"}
{"snippet": "pipe=Pipeline(Input)\npipe\n", "intent": "we input the list as an argument to the pipeline constructor \n"}
{"snippet": "pipe.fit(Z,y)\n", "intent": "We can normalize the data,  perform a transform and fit the model simultaneously. \n"}
{"snippet": "lm.fit(X, Y)\nlm.score(X, Y)\n", "intent": "Let's calculate the R^2\n"}
{"snippet": "lm.fit(Z, df['price'])\nlm.score(Z, df['price'])\n", "intent": "Let's calculate the R^2\n"}
{"snippet": "lre=LinearRegression()\n", "intent": " We create a Linear Regression object:\n"}
{"snippet": "lre.fit(x_train[['horsepower']],y_train)\n", "intent": "we fit the model using the feature horsepower \n"}
{"snippet": "batch_size = 125\nx_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nA = tf.Variable(tf.random_normal(shape=[1,1]))\nb = tf.Variable(tf.random_normal(shape=[1,1]))\nmodel_output = tf.add(tf.matmul(x_data, A), b)\n", "intent": "Next we declare the batch size, model placeholders, model variables, and model operations.\n"}
{"snippet": "RigeModel=Ridge(alpha=0.1)\n", "intent": "Let's create a Ridge regression object, setting the regularization parameter to 0.1 \n"}
{"snippet": "RigeModel.fit(x_train_pr,y_train)\n", "intent": " Like regular regression, you can fit the model using the method **fit**.\n"}
{"snippet": "Rsqu_test=[]\nRsqu_train=[]\ndummy1=[]\nALFA=5000*np.array(range(0,10000))\nfor alfa in ALFA:\n    RigeModel=Ridge(alpha=alfa) \n    RigeModel.fit(x_train_pr,y_train)\n    Rsqu_test.append(RigeModel.score(x_test_pr,y_test))\n    Rsqu_train.append(RigeModel.score(x_train_pr,y_train))\n", "intent": " We select the value of Alfa that minimizes the test error, for example, we can use a for loop. \n"}
{"snippet": "RR=Ridge()\nRR\n", "intent": "Create a ridge regions object:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(64, activation='sigmoid', input_shape=(1000,)))\nmodel.add(Dropout(.5))\nmodel.add(Dense(32, activation='sigmoid'))\nmodel.add(Dropout(.8))\nmodel.add(Dense(num_classes, activation='softmax'))\nadam = keras.optimizers.Adam(lr=0.0005)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "history = model.fit(x_train, y_train, epochs=10, batch_size=100, verbose=0, validation_split=.1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "Conv2D(filters, kernel_size, strides, padding, activation='relu', input_shape)\n", "intent": "Then, we can create a convolutional layer by using the following format:\n"}
{"snippet": "Conv2D(64, (2,2), activation='relu')\n", "intent": "If we look up code online, it is also common to see convolutional layers in Keras in this format:\n"}
{"snippet": "MaxPooling2D(pool_size, strides, padding)\n", "intent": "Then, we can create a convolutional layer by using the following format:\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Start a computational graph.\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import MaxPooling2D\nmodel = Sequential()\nmodel.add(MaxPooling2D(pool_size=2, strides=2, input_shape=(100, 100, 15)))\nmodel.summary()\n", "intent": "We can change the arguments in the code below and check how the shape of the max pooling layer changes.\n"}
{"snippet": "from keras.preprocessing.image import ImageDataGenerator\ndatagen_train = ImageDataGenerator(\n    width_shift_range=0.1,  \n    height_shift_range=0.1,  \n    horizontal_flip=True) \ndatagen_train.fit(x_train)\n", "intent": "Feel free to modify this part to use different setting for data [augmentation](https://keras.io/preprocessing/image/).\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nmodel = Sequential()\nmodel.add(Flatten(input_shape = x_train.shape[1:])) \nmodel.add(Dense(1000, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\n", "intent": "The first model we will train is a regular vanilla neural network.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(64, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "import torch\nsample_caption = torch.Tensor(sample_caption).long()\nprint(sample_caption)\n", "intent": "Finally, in **`line 6`**, we convert the list of integers to a PyTorch tensor and cast it to [long type](http://pytorch.org/docs/master/tensors.html\n"}
{"snippet": "with tf.Session() as sess:\n    c = sess.run(1.5*b)\nprint(b)\n", "intent": "Now, let's keep exploring and add some intermediate computations:\n"}
{"snippet": "W2 = tf.get_variable('W2', shape=[1], use_resource=True)\nprint(W2)\n", "intent": "Finally, we can initialize an eager variable using the standard `get_variable` method, by specifying a flag:\n"}
{"snippet": "hid = tf.layers.Dense(units=10, activation=tf.nn.relu)\ndrop = tf.layers.Dropout()\nout = tf.layers.Dense(units=3, activation=None)\ndef nn_model(x, training=False):\n  return out(drop(hid(x), training=training))\n", "intent": "We can stack multiple layers to create more complicated models. For example, a neural network with one hidden layer and dropout in the middle:\n"}
{"snippet": "net = SingleHiddenLayerNetwork()\nopt = tf.train.AdamOptimizer(learning_rate=0.01)\nfor epoch in range(50):\n  for (xb, yb) in tfe.Iterator(train_dataset.shuffle(1000).batch(32)):\n    opt.apply_gradients(loss_grad(net, xb, yb))\n  if epoch % 10 == 0:\n    print(\"Epoch %d: Loss on training set : %f\" %\n              (epoch, loss(net, X_train, y_train).numpy()))\n", "intent": "Using the first syntax, the optimization cycle is now a trivial matter:\n"}
{"snippet": "batch_size = 100\nx_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nA = tf.Variable(tf.random_normal(shape=[2, 1]))\nb = tf.Variable(tf.random_normal(shape=[1, 1]))\n", "intent": "Set model parameters, placeholders, and coefficients.\n"}
{"snippet": "opt.apply_gradients(loss_grad(net, xb, yb), global_step=tf.train.get_or_create_global_step()) \ntf.train.get_global_step()\n", "intent": "Note that the variable is currently set to 0. If we want to update it correctly, we need to provide the global step during the optimization cycle:\n"}
{"snippet": "model.fit(train_images, train_labels, epochs=5)\n", "intent": "Next, we will train the model by using the [fit method](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n"}
{"snippet": "model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(10000,)))\nmodel.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\nmodel.add(tf.keras.layers.Dense(LABEL_DIMENSIONS, activation=tf.nn.softmax))\noptimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])\nmodel.summary()\n", "intent": "Our model is similar to the previous notebook, modified to work for a multiclass classification problem.\n"}
{"snippet": "lreg = LinearRegression()\n", "intent": "Next, we create a LinearRegression object, afterwards, type lm. then press tab to see the list of methods availble on this object.\n"}
{"snippet": "lreg.fit(X_multi,Y_target)\n", "intent": "Finally, we're ready to pass the X and Y using the linear regression object.\n"}
{"snippet": "kn = KNeighborsClassifier(n_neighbors=3,weights='uniform')\nX = affair[['education','religious']]\ny = affair['affair']\nmodel = kn.fit(X,y)\nscore = kn.score(X,y)\nprint score\n", "intent": "---\nYou should choose **2 predictor variables** to predict had affair vs. not\n"}
{"snippet": "penalty = gelr.best_params_['penalty']\nC = gelr.best_params_['C']\nelr = LogisticRegression(penalty = penalty, C=C)\nelr_model = elr.fit(Xen,ye)\n", "intent": "Wow! That was a much better score than I expected.\n"}
{"snippet": "def relu(input):\n    return max(input, 0)\n", "intent": "The 'ReLU' Activation Function outputs the maximum of (input, 0) as a linear number. \n"}
{"snippet": "node_0_input = (input_data * weights['node_0']).sum()\nnode_0_output = relu(node_0_input)\nnode_1_input = (input_data * weights['node_1']).sum()\nnode_1_output = relu(node_1_input)\nhidden_layer_outputs = np.array([node_0_output, node_1_output])\noutput = (hidden_layer_outputs * weights['output']).sum()\nprint(output)\n", "intent": "Rewriting the example above, replacing the tanh with the ReLU\n"}
{"snippet": "batch_size = 350\nx_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\nprediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)\nb = tf.Variable(tf.random_normal(shape=[1,batch_size]))\n", "intent": "We declare the batch size (large for SVMs), create the placeholders, and declare the $b$ variable for the SVM model.\n"}
{"snippet": "init = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n", "intent": "Now we have to initialize the Variable in Tensorflow\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim)))\n    embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n", "intent": "Implement two helper function to initialize the RELU Nodes with small positive randomn values\n"}
{"snippet": "def conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                         strides=[1, 2, 2, 1], padding='SAME')\n", "intent": "Implement Convolution and Pooling Settings\n"}
{"snippet": "W_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n", "intent": "We add a 64 feature convolutional layer for each 5x5 patch of input data\n"}
{"snippet": "W_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n", "intent": "Here we add a softmax function to scale the values on each of the 10 classes to a value between 0 and 1.\n"}
{"snippet": "def conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                         strides=[1, 2, 3, 1], padding='SAME')\n", "intent": "Implement Convolution and Pooling Settings\n"}
{"snippet": "def get_weights(n_features, n_labels):\n    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\ndef get_biases(n_labels):\n    return tf.Variable(tf.zeros(n_labels))\ndef linear(input, w, b):\n    return tf.add(tf.matmul(input, w), b)\n", "intent": "Define function to use\n"}
{"snippet": "x = tf.constant(10)\ny = tf.constant(2)\nz = tf.subtract(tf.divide(x,y), tf.cast(tf.constant(1), tf.float64))\nwith tf.Session() as sess:\n    output = sess.run(z)\n    print(output)\n", "intent": "Other example with more operation\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Create a graph session\n"}
{"snippet": "hidden_layer = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\nhidden_layer = tf.nn.relu(hidden_layer)\noutput_layer = tf.add(tf.matmul(hidden_layer, weights['output_layer']), biases['output_layer'])\n", "intent": "* Build Network with tensorflow\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(training_epochs):\n        total_batch = int(mnist.train.num_examples/batch_size)\n        for i in range(total_batch):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            sess.run(optimizer, feed_dict={x:batch_x, y:batch_y })\n", "intent": "* Launch the session\n"}
{"snippet": "tf.reset_default_graph()\nweights = tf.Variable(tf.truncated_normal([2,3]))\nbias = tf.Variable(tf.truncated_normal([3]))\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, save_file)\n    print(\"Weights: \")\n    print(sess.run(weights))\n    print(\"Bias: \\n\") \n    print(sess.run(bias))\n", "intent": "> Dont nedd to call the ***sess.run(tf.global_variables_initializer())***\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1e9)\n", "intent": "Logistic regression can do what we just did:\n"}
{"snippet": "X = df.drop('Class', axis=1)\ny = df['Class']\nscaler.fit(X)\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "kmeans.fit(df.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "clf = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "clf.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid=param_grid, verbose=5)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Start a computational graph session.\n"}
{"snippet": "lr_model = xl.LRModel(task='reg', epoch=10, lr=0.1)\nlr_model.fit(tmp_X_train, tmp_y_train)\n", "intent": "Let's now try the linear method in xlearn\n"}
{"snippet": "lr_model = xl.create_linear()\nlr_model.setTrain(\"trainfm.txt\")\nparam = {'task':'reg', 'lr':0.1, 'epoch': 10}\nlr_model.cv(param)\n", "intent": "At this point I thought I will use these methods and optimize + cross validate, given that the library comes with a convenient `.cv` method\n"}
{"snippet": "knn.fit(X_train,y_train)\n", "intent": "** Ajuste este modelo KNN aos dados de treinamento. **\n"}
{"snippet": "nb.fit(X_train,y_train)\n", "intent": "** Agora ajuste nb usando os dados de treinamento. **\n"}
{"snippet": "nb.fit(X_train, y_train)\n", "intent": "** Agora ajuste nb usando os dados de treinamento. **\n"}
{"snippet": "lm.fit(X_train, y_train)\n", "intent": "** Treine lm nos dados de treinamento. **\n"}
{"snippet": "lm.fit(X_train,y_train)\n", "intent": "** Treine lm nos dados de treinamento. **\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\ngrid.fit(X_train,y_train)\n", "intent": "** Crie um objeto GridSearchCV e ajuste-o aos dados de treinamento. **\n"}
{"snippet": "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']),\\\n    biases['hidden_layer'])\nlayer_1 = tf.nn.relu(layer_1)\nlogits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n", "intent": "<img src=\"images/multi-layer.png\" style=\"height:150px\">\n"}
{"snippet": "x_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)\ny_target = tf.placeholder(shape=[3, None], dtype=tf.float32)\nprediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)\nb = tf.Variable(tf.random_normal(shape=[3,batch_size]))\n", "intent": "Initialize placeholders and create the variables for multiclass SVM\n"}
{"snippet": "kmeans = KMeans(n_clusters=4).fit(df_nonull[features])\ndf_nonull['cluster'] = kmeans.labels_\ncmap = {'0': 'r', '2': 'g', '1': 'b','3':'y' }\ndf_nonull['ccluster'] = df_nonull.cluster.apply(lambda x: cmap[str(x)])\ndf_nonull.plot('Age', 'Pclass', kind='scatter', c=df_nonull.ccluster)\n", "intent": "Observations: Looks like there is an elbow at 4?? What does the plot of 4 clusters look like...\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier()\nforest.fit(xtrain, ytrain)\nprint(forest.score(xtrain, ytrain))\nprint(forest.score(xtest, ytest))\n", "intent": "Interestingly, the training accuracy declined but the test accuracy improved, leading to a better model with less overfititng.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(criterion='gini', class_weight='balanced')\nparam_grid = {'n_estimators' : [40, 50, 60], 'min_samples_split' : [2, 3, 4], \n              'max_depth' : [4, 7, 10]}\nrf_cv = GridSearchCV(rf, param_grid, cv = 5)\nrf_cv.fit(X_train, y_train)\nPrint out the best model\nprint('Best RF Params: {}'.format(rf_cv.best_params_))\nprint('Best RF Score : %f' % rf_cv.best_score_)\n", "intent": "I will us scikit-learn's Random Forest Classifier to build a prediction model for active status.\n"}
{"snippet": "def linear_model(img):\n  X = tf.reshape(img, [-1, HEIGHT*WIDTH]) \n  W = tf.Variable(tf.truncated_normal([HEIGHT*WIDTH, NCLASSES], stddev=0.1))\n  b = tf.Variable(tf.truncated_normal([NCLASSES], stddev=0.1))\n  ylogits = tf.matmul(X, W) + b\n  return ylogits, NCLASSES\n", "intent": "Let's start with a very simple linear classifier. All our models will have this basic interface -- they will take an image and return logits.\n"}
{"snippet": "def linear_model(img, mode):\n  X = tf.reshape(img, [-1, HEIGHT*WIDTH]) \n  W = tf.Variable(tf.truncated_normal([HEIGHT*WIDTH, NCLASSES], stddev=0.1))\n  b = tf.Variable(tf.truncated_normal([NCLASSES], stddev=0.1))\n  ylogits = tf.matmul(X, W) + b\n  return ylogits, NCLASSES\n", "intent": "A simple low-level matrix multiplication\n"}
{"snippet": "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n", "intent": "Without limiting the depth, the DT will be evolved until perfect accuracy.\nBut not really useful &rarr; Over-training\nBetter approach:\n"}
{"snippet": "from sklearn.manifold import Isomap\niso = Isomap(n_components=2)\niso.fit(digits.data)\nXI_2D = iso.transform(digits.data)\n", "intent": "Some digits are nicely isolated, others less so\nThink about it, which digits tend to look similar?\n***\nAlternative method for dimension reduction:\n"}
{"snippet": "xy_pca = pca.fit(xy)\n", "intent": "What does the following code do?\n"}
{"snippet": "model = LinearRegression()\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "import tensorflow as tf\nsess = tf.Session()\n", "intent": "This notebook illustrates how to use the Levenstein distance (edit distance) in TensorFlow.\nGet required libarary and start tensorflow session.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "grid_search = GridSearchCV(SVC(), param_grid, verbose=2)\ngrid_search.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nm = LogisticRegression()\nm.fit(Xtrain, ytrain)\n", "intent": "Create a Machine Learning model using logistic regression and fit it with the training data:\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nm = RandomForestClassifier()\n", "intent": "Let's try a different model: The Random Forest (an **ensemble of decision trees**)\n"}
{"snippet": "m1 = RandomForestClassifier(max_depth=2)\nm2 = RandomForestClassifier(max_depth=3)\nm3 = RandomForestClassifier(max_depth=10)\n", "intent": "Compare how the following parameters affect prediction quality:\n"}
{"snippet": "m.fit(Xtrain, ytrain)\nm.score(Xtrain, ytrain)\n", "intent": "Fit the Random Forest model to the training data yourself and evaluate it on the test set.\n"}
{"snippet": "m = RandomForestClassifier(max_depth=2)\nm.fit(Xtrain, ytrain)\nm.score(Xtrain, ytrain), m.score(Xtest, ytest)\n", "intent": "Limiting the complexity of a model is called **regularization**\n"}
{"snippet": "ad_all_ols = sm.ols(formula=\"Sales ~ TV + Radio + Newspaper\", data=advert).fit()\nad_all_ols.summary()\n", "intent": "**Model:**\n$$\nSales = \\beta_0 + \\beta_1 * TV + \\beta_2*Radio + \\beta_3*Newspaper. \n$$\n"}
{"snippet": "x = tf.Variable(3.0)\ny = tf.Variable(4.0)\nf = x + 2*y*y + 2\ngrads = tf.gradients(f,[x,y])\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer()) \n    print([g.eval() for g in grads])\n", "intent": "TensorFlow can automatically compute the derivative of functions using [```gradients```](https://www.tensorflow.org/api_docs/python/tf/gradients). \n"}
{"snippet": "sess = tf.Session()\n", "intent": "We start a computation graph session.\n"}
{"snippet": "x = tf.Variable(3.0, trainable=True)\ny = tf.Variable(2.0, trainable=True)\nf = x*x + 100*y*y\nopt = tf.train.MomentumOptimizer(learning_rate=1e-2,momentum=.5).minimize(f)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(1000):\n        if i%100 == 0: print(sess.run([x,y,f]))\n        sess.run(opt)\n", "intent": "+ [```MomentumOptimizer```](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer)\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, save_path)\n    X_new_scaled = mnist.test.images\n    Z = logits.eval(feed_dict={X: X_new_scaled})\n    y_pred = np.argmax(Z, axis=1)\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(mnist.test.labels,y_pred))\n", "intent": "We can also print the confusion matrix using [```confusion_matrix```](https://www.tensorflow.org/api_docs/python/tf/confusion_matrix). \n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1)\nlogreg.fit(features_train, target_train)\n", "intent": "Let's start with a simple model from sklearn, namely `LogisticRegression`:\n"}
{"snippet": "def flatten(array):\n    out = []\n    for item in array:\n        out += item\n    return out\n", "intent": "Regroup indices such that the distribution is more less equalized\n"}
{"snippet": "def linear(x, alpha, beta):\n    return f\n", "intent": "In the cell below, define the linear function\n$$f = \\alpha + \\beta\\hspace{1pt}x$$\n"}
{"snippet": "def linear(x, alpha, beta):\n    return alpha + (beta * x)\n", "intent": "In the cell below, define the linear function\n$$f = \\alpha + \\beta\\hspace{1pt}x$$\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=5,\n                           weights='uniform',\n                           p=2,\n                           metric='minkowski')\nknn.fit(X,y)\n", "intent": "Build a logit model and fit\n"}
{"snippet": "model = LogisticRegression()\nmodel.fit(X, y)\n", "intent": "Then let's initialize a logistic regression model:\n"}
{"snippet": "db = DBSCAN(eps=0.5, min_samples=5).fit(x)\ncore_samples = db.core_sample_indices_\nlabels = db.labels_\nprint core_samples\nprint labels\n", "intent": "Next, we'll find the labels calculated by DBSCAN\n"}
{"snippet": "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\nprediction = tf.sigmoid(model_output)\nmy_opt = tf.train.GradientDescentOptimizer(0.001)\ntrain_step = my_opt.minimize(loss)\ninit = tf.global_variables_initializer()\nsess.run(init)\n", "intent": "Now we declare our loss function (which has the sigmoid built in), prediction operations, optimizer, and initialize the variables.\n"}
{"snippet": "session = tf.Session()\nsession.run(tf.global_variables_initializer())\n", "intent": "<h6> Feel free to change the values of \"m\" and \"c\" in future to check how the initial position of line changes </h6>\n"}
{"snippet": "W_fc1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))\nb_fc1 = tf.Variable(tf.constant(0.1, shape=[1024])) \n", "intent": "Composition of the feature map from the last layer (7x7) multiplied by the number of feature maps (64); 1027 outputs to Softmax layer\n"}
{"snippet": "y_CNN= tf.nn.softmax(fc)\ny_CNN\n", "intent": "__softmax__ allows us to interpret the outputs of __fcl4__ as probabilities. So, __y_conv__ is a tensor of probablities.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Lets start with a new session:\n"}
{"snippet": "cells = []\nfor _ in range(num_layers):\n    cell = tf.contrib.rnn.LSTMCell(LSTM_CELL_SIZE)\n    cells.append(cell)\nstacked_lstm = tf.contrib.rnn.MultiRNNCell(cells)\n", "intent": "Lets create the stacked LSTM cell:\n"}
{"snippet": "data = tf.placeholder(tf.float32, [None, None, input_dim])\noutput, state = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)\n", "intent": "Now we can create the RNN:\n"}
{"snippet": "set_rf_samples(6000)\nm = RandomForestRegressor(n_estimators=7000,max_features=0.15,n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\n", "intent": "1) 40 Trees is as good as we get\n"}
{"snippet": "learn.fit(0.005,5)\n", "intent": "0.1 - e-1\n0.01 - e-2\n0.001 - e-3\n"}
{"snippet": "for k in range(1, 30)[::-1]:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(wine_df, y)\n    print(k, knn.score(wine_df, y))\n", "intent": "Using the best features, explore what the best value of `k` is for this dataset. \n"}
{"snippet": "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\nnce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n                                               stddev=1.0 / np.sqrt(embedding_size)))\nnce_biases = tf.Variable(tf.zeros([vocabulary_size]))\nx_inputs = tf.placeholder(tf.int32, shape=[batch_size])\ny_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\nvalid_dataset = tf.constant(valid_examples, dtype=tf.int32)\nembed = tf.nn.embedding_lookup(embeddings, x_inputs)\n", "intent": "Next we define our model and placeholders.\n"}
{"snippet": "knn5 = KNeighborsClassifier(n_neighbors=5, weights='uniform')\nscores = accuracy_crossvalidator(Xs, y, knn5, cv_indices)\n", "intent": "<a id='cv-knn5'></a>\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nclf_dtc = DecisionTreeClassifier()\nclf_dtc.fit(X_train, y_train)\n", "intent": "Let's fit a decision tree classifier.\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nclf_gnb = GaussianNB()\nclf_gnb.fit(X, y)\n", "intent": "Let's fit a Gaussian naive Bayes classifier.\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, verbose = 2)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "dtree = DecisionTreeClassifier()\ndtree.fit(X_train, y_train)\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "reg = LinearRegression()\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "reg.fit(X_train, y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=2,\n         validation_data=(x_test,y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "pipeline = Pipeline(screen=universe)\npipeline.add(\n    mean_reversion_5day_sector_neutral_smoothed(5, universe, sector),\n    'Mean_Reversion_5Day_Sector_Neutral_Smoothed')\nengine.run_pipeline(pipeline, factor_start_date, universe_end_date)\n", "intent": "Let's see what some of the smoothed data looks like.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Next we start a computational graph session.\n"}
{"snippet": "from IPython.display import display\nfrom sklearn.tree import DecisionTreeClassifier\nclf_random_state = 0\nsimple_clf = DecisionTreeClassifier(\n    max_depth=3,\n    criterion='entropy',\n    random_state=clf_random_state)\nsimple_clf.fit(X_train, y_train)\ndisplay(project_helper.plot_tree_classifier(simple_clf, feature_names=features))\nproject_helper.rank_features_by_importance(simple_clf.feature_importances_, features)\n", "intent": "Let's see how a single tree would look using our data.\n"}
{"snippet": "train_score = []\nvalid_score = []\noob_score = []\nfor n_trees in tqdm(n_trees_l, desc='Training Models', unit='Model'):\n    clf = bagging_classifier(n_trees, 0.2, 1.0, clf_parameters)\n    clf.fit(X_train, y_train)\n    train_score.append(clf.score(X_train, y_train.values))\n    valid_score.append(clf.score(X_valid, y_valid.values))\n    oob_score.append(clf.oob_score_)\n", "intent": "With the bagging classifier built, lets train a new model and look at the results.\n"}
{"snippet": "p.show_graph(format='png')\n", "intent": "Note that you can right-click the image and view in a separate window if it's too small.\n"}
{"snippet": "x = np.linspace(min(acc), max(acc))\ne = np.empty((samps,len(x)))\nfor sim_dataset in range(samps):\n    acc = ppc['acc'][sim_dataset,1,:].flatten()\n    ecdf = sm.distributions.empirical_distribution.ECDF(acc)\n    e[sim_dataset,:] = ecdf(x)\n", "intent": "Empirical CDF generated from Posterior\n"}
{"snippet": "import statsmodels.api as sm\nX = np.array(xs).transpose()\nX = sm.add_constant(X)\nmod = sm.OLS(ys, X) \nres = mod.fit()\nprint res.summary()\n", "intent": "This generated a fit of $y = 3 x + 1$. Let's see what a linear regression yields.\n"}
{"snippet": "import sklearn.linear_model as lm\nmodel = lm.LogisticRegression()\nfeatures = ['CRS_DEP_TIME', 'dow_1', 'dow_2', 'dow_3', 'dow_4', 'dow_5', 'dow_6']\nX = df[features]\ny = df['DEP_DEL15']\nmodel.fit(X, y)\n", "intent": "Run a logistic regression model to predict delays:\n"}
{"snippet": "k = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(dfn)\n", "intent": "Cluster the Data - We are going to use 5 clusters based off of the above scatterplot\n"}
{"snippet": "kmeans = KMeans(n_clusters=6)\nclusters = kmeans.fit(X_scale)\n", "intent": "Set up the k-means clustering analysis. Use the graph from above to derive \"k\"\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(500, activation = 'relu', input_shape = (1000,)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation = 'sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer = 'adam',\n              metrics = ['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "logistic_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=tf.cast(log_y_target, tf.float32)))\nprediction = tf.round(tf.sigmoid(model_output))\npredictions_correct = tf.cast(tf.equal(prediction, tf.cast(log_y_target, tf.float32)), tf.float32)\naccuracy = tf.reduce_mean(predictions_correct)\nlogistic_opt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\nlogistic_train_step = logistic_opt.minimize(logistic_loss, var_list=[A, b])\ninit = tf.global_variables_initializer()\nsess.run(init)\n", "intent": "Loss function, Prediction function, Optimization function and variable initializer.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nprint('Accuracy of Logistic regression classifier on training set: {:.2f}'\n     .format(logreg.score(X_train, y_train)))\nprint('Accuracy of Logistic regression classifier on test set: {:.2f}'\n     .format(logreg.score(X_test, y_test)))\n", "intent": "Logistic Regression Model Accuracy\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier().fit(X_train, y_train)\nprint('Accuracy of Random Forest classifier on training set: {:.2f}'\n     .format(clf.score(X_train, y_train)))\nprint('Accuracy of Random Forest classifier on test set: {:.2f}'\n     .format(clf.score(X_test, y_test)))\n", "intent": "Random Forest Model Accuracy\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nprint('Accuracy of K-NN classifier on training set: {:.2f}'\n     .format(knn.score(X_train, y_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'\n     .format(knn.score(X_test, y_test)))\n", "intent": "K Nearest Neighbors Model Accuracy\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nlasso = Lasso()\nalphas = [0.5, 1, 4, 10, 50, 100]\nparam_grid = [\n    {'alpha': alphas}\n]\ngrid_search = GridSearchCV(lasso, param_grid, cv = 5, scoring = 'neg_mean_squared_error')\ngrid_search.fit(housing_prep, y)\n", "intent": "<b>CROSS-VALIDATION / GRIDSEARCH </B>\n"}
{"snippet": "tf.reset_default_graph()\nweights = tf.Variable(tf.truncated_normal([2, 3]))\nbias = tf.Variable(tf.truncated_normal([3]))\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, './model.ckpt')\n    print('Weight:')\n    print(sess.run(weights))\n    print('Bias:')\n    print(sess.run(bias))\n", "intent": "<font size=3>\nNow that the Tensor Variables are saved, let's load them back into a new model.\n"}
{"snippet": "def build_output(lstm_output, in_size, out_size):\n    seq_output = tf.concat(lstm_output, axis=1)\n    x = tf.reshape(seq_output, [-1,in_size]) \n    with tf.variable_scope('softmax'):\n        softmax_w = tf.Variable(tf.truncated_normal([in_size,out_size], stddev=0.1, dtype=tf.float32))\n        softmax_b = tf.Variable(tf.zeros(out_size))\n    logits = tf.matmul(x, softmax_w) + softmax_b\n    out = tf.nn.softmax(logits, name='prediction')\n    return out, logits\n", "intent": "<font size=5>If you have some questions about **LSTM structure**, check discussion here </font> \n"}
{"snippet": "vr_by_f1_features_to_test = []\nvr_by_f1_features_test_results = {}\nfor i, feature in tqdm(enumerate(vr_by_f1_performant_features)):\n    vr_by_f1_features_to_test.append(feature)\n    vr_by_f1_features_test_results[feature] = run_model(LogisticRegression(), 'logit', 100,\n                                                        adult_train_df[vr_by_f1_features_to_test],\n                                                        adult_train_target)\n", "intent": "Add one feature at a time.\n"}
{"snippet": "gspipe.fit(X_train, y_train.ravel())\n", "intent": "This will take a while, and you may see some errors...\n"}
{"snippet": "losses = []\ntest_beta_1 = list(range(-5,5))\ntest_beta_2 = list(range(-5,5))\nfor beta_2 in test_beta_2:\n    for beta_1 in test_beta_1:\n        ptron = Perceptron((beta_1, beta_2), 0.5)\n        losses.append(ptron.loss(xs, ys))\n", "intent": "Let's change our weights and collect the scores.\n"}
{"snippet": "prediction = tf.nn.softmax(model_output)\ntest_prediction = tf.nn.softmax(test_model_output)\ndef get_accuracy(logits, targets):\n    batch_predictions = np.argmax(logits, axis=1)\n    num_correct = np.sum(np.equal(batch_predictions, targets))\n    return(100. * num_correct/batch_predictions.shape[0])\n", "intent": "We also create a prediction and accuracy function for evaluation on the train and test set.\n"}
{"snippet": "sess=tf.Session()\n", "intent": "create a tensorflow session\n"}
{"snippet": "tree=DecisionTreeClassifier()\ntree.fit(X_train,y_train)\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "kmeans=KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS',axis=1))  \n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "KNN=KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "lm=LinearRegression()\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "param_grid={'C':[0.1,1,10,100],'gamma':[1, 0.1, 0.01, 0.001]}\ngrid_model=GridSearchCV(SVC(),param_grid,verbose=2)\n", "intent": "**Create a dictionary called param_grid and fill out some parameters for C and gamma.**\n"}
{"snippet": "grid_model.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embeded = tf.nn.embedding_lookup(embedding, input_data)\n    return embeded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "initial = tf.random_normal(shape) * 0.256\nimage = tf.Variable(initial)\nvgg_net = vgg_network(network_weights, image)\n", "intent": "Make Combined Image\n"}
{"snippet": "model = KMeans(n_clusters=3)\n", "intent": "**Step 3:** Create a `KMeans` model called `model` with `3` clusters.\n"}
{"snippet": "kmeans = KMeans(n_clusters=4)\n", "intent": "**Step 6:** Create an instance of `KMeans` with `4` clusters called `kmeans`.\n"}
{"snippet": "pipeline.fit(samples)\n", "intent": "**Step 3:** Fit the pipeline to the fish measurements `samples`.\n"}
{"snippet": "kmeans = KMeans(n_clusters=14)\n", "intent": "**Step 4:** Create an instance of `KMeans` called `kmeans` with `14` clusters.\n"}
{"snippet": "pipeline = make_pipeline(normalizer, kmeans)\n", "intent": "**Step 5:** Using `make_pipeline()`, create a pipeline called `pipeline` that chains `normalizer` and `kmeans`.\n"}
{"snippet": "pipeline.fit(movements)\n", "intent": "**Step 6:** Fit the pipeline to the `movements` array.\n"}
{"snippet": "model2 = Models.deep_cnn_model(flag_BN=True,flag_STN=True)\nhistory2,evaluate2= train_model_(model2,\"BN_STN_001\",flag_earlystop=False,flag_reduceLR=False, flag_lrsched=False,flag_tensorboard=True)\n", "intent": "* Train Accuracy : 99.54 \n* Valid Accuracy : 99.27\n"}
{"snippet": "model4 = Models.deep_cnn_model(flag_BN=True,flag_STN=True)\nhistory4,evaluate4= train_model_(model4,\"BN_STN_lrsched\",flag_earlystop=False,flag_reduceLR=False, flag_lrsched=True,flag_tensorboard=True)\n", "intent": "lr = (lr*(0.1 ** int(epoch / 4)))\n* Train Accuracy : 99.52 \n* Valid Accuracy : 99.57\n"}
{"snippet": "model5 = Models.deep_cnn_model(flag_BN=True,flag_STN=True)\nhistory5,evaluate5= train_model_(model5,\"BN_STN_lrsched_2\",flag_earlystop=False,flag_reduceLR=False, flag_lrsched=True,flag_tensorboard=True)\n", "intent": " return lr * ( 0.5 ** int(epoch / 5))\n* Train Accuracy = 99.87\n* Valid Accuracy = 99.57\n"}
{"snippet": "lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n                        training_seq_len, vocab_size)\nwith tf.variable_scope(tf.get_variable_scope(), reuse=True):\n    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n                                 training_seq_len, vocab_size, infer_sample=True)\n", "intent": "In order to use the same model (with the same trained variables), we need to share the variable scope between the trained model and the test model.\n"}
{"snippet": "svc_model = model_performance(SVC(probability=True))\n", "intent": "In this section we'll evaluate the performance of a support vector machine.\n"}
{"snippet": "parameters = {'C' : [0.001, 0.1, 1, 10, 100], 'penalty' : ['l1', 'l2']}\nmodel = model_performance(GridSearchCV(LogisticRegression(random_state=42), param_grid=parameters, cv=skf), cv=True)\n", "intent": "Without any optimization, we can predict patients who will convert to AD with 89% accuracy. Now, the classifier will be optimized.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nmodel_eval(RandomForestClassifier())\n", "intent": "Using the out of the box classifier works better on non TD-IDF data.\n"}
{"snippet": "reg = RandomForestRegressor(random_state = 0, max_features = 0.85, min_samples_leaf = 9,n_estimators = 685, n_jobs = -1)\nreg.fit(X_train, y_train)\n", "intent": "Fitting model on training set\n"}
{"snippet": "regressor = LinearRegression(fit_intercept = True)\nregressor.fit(X_train, y_train)\n", "intent": "Fitting model on training set\n"}
{"snippet": "log = model.fit(X_train, y_train,nb_epoch=30, batch_size=32, verbose=2,validation_data=(X_test, y_test))\n", "intent": "Finally, I proceed to train the model with a simple one line command:\n"}
{"snippet": "class DQNAgent(RLDebugger):\n    def __init__(self, observation_space, action_space):\n        self.learning_rate = ??? \n    def build_model(self):\n        model = Sequential()\n        model.add(Dense(???, input_dim=self.state_size, activation=???))\n        model.add(Dense(self.action_size, activation=???))\n        model.compile(loss=???, optimizer=Adam(lr=self.learning_rate))\n        model.summary()\n        return model\n", "intent": "  - Skeleton provided, implemented with [Keras](https://keras.io/)\n"}
{"snippet": "history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\n", "intent": "Train the network for 20 epochs with a batch size of 512. Save the object as `history`.\n"}
{"snippet": "model_a = sm.OLS.from_formula(\"SalePrice ~ scale(OverallQual) + scale(OverallCond) + scale(GrLivArea) + scale(I(GrLivArea**2)) + scale(I(GrLivArea**3)) + scale(KitchenQual) + scale(I(KitchenQual**2)) + scale(I(KitchenQual**3)) + scale(GarageCars) + scale(BsmtQual) + scale(YearBuilt) + C(Neighborhood) + C(MSZoning)\", data=df)\nresult_a = model_a.fit()\nprint(result_a.summary())\n", "intent": "$$ y = OQ + OC + GA + GA^2 + GA^3 + KQ +KQ^2 +KQ^3 + GC + BQ + YB + Category $$\n"}
{"snippet": "lstm_model = LSTM_Model(rnn_size, num_layers, batch_size, learning_rate,\n                        training_seq_len, vocab_size)\nwith tf.variable_scope(tf.get_variable_scope(), reuse=True):\n    test_lstm_model = LSTM_Model(rnn_size,num_layers, batch_size, learning_rate,\n                                 training_seq_len, vocab_size, infer_sample=True)\n", "intent": "Initialize the LSTM Model\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_dim=1000))\nmodel.add(Dropout(.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=200, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "rm , predictions, mse, score = regression_model(['exchange_vol_usd_log'])\nprint_stats('Exchange Volume in USD', mse, score) \nrm , predictions, mse, score = regression_model(['ma_price_7'])\nprint_stats('Seven-day moving average of bitcoin price', mse, score)\nrm , predictions, mse, score = regression_model(['ma_price_3'])\nprint_stats('Three-day moving average of bitcoin price', mse, score)\nrm , predictions, mse, score = regression_model(['price_usd'])\nprint_stats('Daily price of bitcoin', mse, score)\ndraw_scatterplot('price_usd', rm, predictions)\n", "intent": "Try out different models parameters.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nk_range = range(1, 51, 2)\nparam_grid = {'n_neighbors' : k_range}\ngrid = GridSearchCV(knn, param_grid, n_jobs=4)\ngrid.fit(X, y)\nprint grid.grid_scores_\nprint grid.best_score_\nprint grid.best_estimator_\nprint grid.best_params_\n", "intent": "Use grid search to find the optimal value of k.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=4, n_neighbors=7, p=2,\n           weights='uniform')\nknn.fit(X, y)\n", "intent": "With k=7, we get accuracy of 35%. Random accuracy would have been 2.6%.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nparam_grid = {'max_depth': np.arange(3, 10)}\ngrid = GridSearchCV(dtc, param_grid, n_jobs=2)\ngrid.fit(X, y)\nprint grid.grid_scores_\nprint grid.best_score_\nprint grid.best_estimator_\nprint grid.best_params_\n", "intent": "Grid search to optimize depth of the tree.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nk_range = range(1, 51, 2)\nparam_grid = {'n_neighbors' : k_range}\ngrid = GridSearchCV(knn, param_grid, n_jobs=2)\ngrid.fit(X, y)\nprint grid.grid_scores_\nprint grid.best_score_\nprint grid.best_estimator_\nprint grid.best_params_\n", "intent": "Use grid search to find the optimal value of k.\n"}
{"snippet": "clf = SVC()\nclf.fit(X_train, y_train)\n", "intent": "Other classification algorithms: http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n"}
{"snippet": "features = df.drop('Class', axis=1)\nscaler.fit(features)\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Now we start a computational graph session.\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, verbose=3)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)    \n", "intent": "- \"Estimator\" is scikit-learn's term for model\n- \"Instantiate\" means \"make an instance of\"\n"}
{"snippet": "knn.fit(X,y)  \n", "intent": "- Model is learning the relationship between X and y\n- Occurs in-place so need to assign the result to another object\n"}
{"snippet": "images, labels = next(iter(trainloader))\nprint(labels[0])\nimg = images[0].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "model.fit(x_train, y_train,\n          epochs=10, \n          batch_size=32,\n          validation_data = (x_test,y_test),\n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "x1 = tf.Variable(1)\nx.graph is tf.get_default_graph()\n", "intent": "Any node you create is automatically added to the default graph:\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing.data, housing.target.reshape(-1, 1))\nprint(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])\n", "intent": "And with scikit learn\n"}
{"snippet": "c_values = np.arange(0.01,10,0.01)\nlogreg_parameters = {'penalty':['l1','l2'], 'C': c_values}\nmodel_GS1 = GridSearchCV(LogisticRegression(),logreg_parameters,cv=5)\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "svm_clf = SVC(decision_function_shape=\"ovr\")\nsvm_clf.fit(X_train[:7500], np.ravel(y_train[:7500]))\n", "intent": "Now I will test SVC.\n"}
{"snippet": "x_graph_input = tf.placeholder(tf.float32, [None])\ny_graph_input = tf.placeholder(tf.float32, [None])\nm = tf.Variable(tf.random_normal([1], dtype=tf.float32), name='Slope')\noutput = tf.multiply(m, x_graph_input, name='Batch_Multiplication')\nresiduals = output - y_graph_input\nl1_loss = tf.reduce_mean(tf.abs(residuals), name=\"L1_Loss\")\nmy_optim = tf.train.GradientDescentOptimizer(0.01)\ntrain_step = my_optim.minimize(l1_loss)\n", "intent": "Now we declare the placeholders, variables, model operations, loss, and optimization function.\n"}
{"snippet": "tf.reset_default_graph()\nbatch_size = 32\nwith tf.device('/cpu:0'):\n    images, _, _ = data_provider.provide_data('train', batch_size, MNIST_DATA_DIR)\nimgs_to_visualize = tfgan.eval.image_reshaper(images[:20,...], num_cols=10)\nvisualize_digits(imgs_to_visualize)\n", "intent": "<a id='unconditional_input'></a>\n"}
{"snippet": "tf.reset_default_graph()\nbatch_size = 32\nwith tf.device('/cpu:0'):\n    images, one_hot_labels, _ = data_provider.provide_data('train', batch_size, MNIST_DATA_DIR)\nimgs_to_visualize = tfgan.eval.image_reshaper(images[:20,...], num_cols=10)\nvisualize_digits(imgs_to_visualize)\n", "intent": "<a id='conditional_input'></a>\n"}
{"snippet": "tf.reset_default_graph()\nbatch_size = 32\nwith tf.device('/cpu:0'):\n    images, _, _ = data_provider.provide_data('train', batch_size, MNIST_DATA_DIR)\nimgs_to_visualize = tfgan.eval.image_reshaper(images[:20,...], num_cols=10)\nvisualize_digits(imgs_to_visualize)\n", "intent": "<a id='infogan_input'></a>\nThis is the same as the unconditional case (we don't need labels).\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1),dtype=tf.float32)\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def k_means(df):\n    x1=np.array(df[\"t_summer\"])\n    x2=np.array(df[\"t_winter\"])\n    x = np.vstack((x1, x2)).T\n    kmeans=KMeans(init=\"k-means++\").fit(x)\n    l=kmeans.labels_\n    k=set(l)\n    c=kmeans.cluster_centers_\n    return k,c\n", "intent": "Write a function to perform k-means clustering for the given dataset.\nCreate a fucntion k_means()\nReturn\nOptimum value of k\nCluster centers\n"}
{"snippet": "if USE_PRETRAINED:\n    with open('pretrained/xgb_grid_search_eta_50.pkl', 'rb') as f:\n        grid = pickle.load(f)\nelse:      \n    xgb_param_grid = {'eta':[0.5,0.4,0.3,0.2,0.1,0.075,0.05,0.04,0.03]}\n    grid = GridSearchCV(XGBoostRegressor(num_boost_round=50, gamma=0.2, max_depth=8, min_child_weight=6,\n                                         colsample_bytree=0.6, subsample=0.9,nthread=10),\n                        param_grid=xgb_param_grid, cv=5, scoring=mae_scorer)\n    grid.fit(train_x, train_y.values)\n", "intent": "First, we plot different learning rates for a simpler model (50 trees):\n"}
{"snippet": "if USE_PRETRAINED:\n    with open('pretrained/xgb_grid_search_eta_100.pkl', 'rb') as f:\n        grid = pickle.load(f)\nelse:\n    xgb_param_grid = {'eta':[0.5,0.4,0.3,0.2,0.1,0.075,0.05,0.04,0.03]}\n    grid = GridSearchCV(XGBoostRegressor(num_boost_round=100, gamma=0.2, max_depth=8, min_child_weight=6,\n                                         colsample_bytree=0.6, subsample=0.9,nthread=10),\n                        param_grid=xgb_param_grid, cv=5, scoring=mae_scorer)\n    grid.fit(train_x, train_y.values)\n", "intent": "Now, replicate the process for `num_boost_round=100`. We effectively double the amount of trees, let's see how `eta` should be changed.\n"}
{"snippet": "def mlp_model():\n    model = Sequential()\n    model.add(Dense(1024, input_dim=xtr.shape[1]))\n    model.add(Activation('relu'))\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dense(1))\n    model.compile(loss='mae', optimizer='adam')\n    return model\n", "intent": "We initialize a wide3-layer model with 1024-512 units in hidden layers:\n"}
{"snippet": "mlp = mlp_model()\nif USE_PRETRAINED:\n    with open('pretrained/mlp_v4_hist.pkl', 'rb') as f:\n        hist = pickle.load(f)\nelse:\n    sys.stdout = open('mlp_v4_out.txt', 'w')\n    fit = mlp.fit(xtr, ytr, batch_size=128, validation_data=(xte,yte),\n                  nb_epoch=40, verbose=1)\n    hist = fit.history\n", "intent": "We train the model and visualize the results:\n"}
{"snippet": "tf.reset_default_graph()\nx=tf.placeholder(tf.float32,shape=(None,n_steps,n_inputs),name=\"X\")\nbasic_cell=tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\noutputs,states=tf.nn.dynamic_rnn(basic_cell,x,dtype=tf.float32)\ninit=tf.global_variables_initializer()\n", "intent": "* Using Dynamic RNN to let tensorflow take care of stacking and unstacking of the inputs and outputs \n"}
{"snippet": "if USE_PRETRAINED:\n    with open('pretrained/xgb_grid_search_eta_100.pkl', 'rb') as f:\n        grid = pickle.load(f)\nelse:\n    xgb_param_grid = {'eta':[0.5,0.4,0.3,0.2,0.1,0.075,0.05,0.04,0.03]}\n    grid = GridSearchCV(XGBoostRegressor(num_boost_round=100, gamma=0.2, max_depth=8, min_child_weight=6,\n                                         colsample_bytree=0.6, subsample=0.9),\n                        param_grid=xgb_param_grid, cv=5, scoring=mae_scorer)\n    grid.fit(train_x, train_y.values)\n", "intent": "Now, replicate the process for `num_boost_round=100`. We effectively double the amount of trees, let's see how `eta` should be changed.\n"}
{"snippet": "from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = ExtraTreesClassifier(n_estimators=200)\nclf = clf.fit(train, targets)\n", "intent": "Tree-based estimators can be used to compute feature importances, which in turn can be used to discard irrelevant features.\n"}
{"snippet": "model = RandomForestClassifier(n_estimators=100)\nmodel.fit( train_X , train_y )\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))\nplot_model_var_imp(model, train_X, train_y)b\n", "intent": "Try a random forest model by running the cell below. \n"}
{"snippet": "model = SVC()\nmodel.fit( train_X , train_y )\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))\n", "intent": "Try a Support Vector Machines model by running the cell below. \n"}
{"snippet": "model = GradientBoostingClassifier()\nmodel.fit( train_X , train_y )\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))\nplot_model_var_imp(model, train_X, train_y)\n", "intent": "Try a Gradient Boosting Classifier model by running the cell below. \n"}
{"snippet": "model = KNeighborsClassifier(n_neighbors = 3)\nmodel.fit( train_X , train_y )\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))\n", "intent": "Try a k-nearest neighbors model by running the cell below. \n"}
{"snippet": "model = GaussianNB()\nmodel.fit( train_X , train_y )\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))\n", "intent": "Try a Gaussian Naive Bayes model by running the cell below. \n"}
{"snippet": "model = LogisticRegression()\nmodel.fit( train_X , train_y )\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))\n", "intent": "Try a Logistic Regression model by running the cell below. \n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nfrom sklearn.cross_validation import KFold\npredictors = [\"Pclass\", \"Sex\", \"Age\",\"SibSp\", \"Parch\", \"Fare\",\n              \"Embarked\",\"NlengthD\", \"FsizeD\", \"Title\",\"Deck\"]\ntarget=\"Survived\"\nalg = LinearRegression()\nkf = KFold(titanic.shape[0], n_folds=3, random_state=1)\npredictions = []\n", "intent": "*Linear Regression*\n-------------------\n"}
{"snippet": "clf = RandomForestRegressor(max_depth=2, random_state=0)\nltr = PointWiseLTRModel(clf)\nltr._train(train_X, train_y)\n", "intent": "Set `max_depth` roughly to the square root of the number features\n"}
{"snippet": "lm.fit(X, bos.PRICE)\nlm\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "model = SVC()\n", "intent": "Now we create a Support Vector Classification model for the data.\n"}
{"snippet": "model.fit(X_train,Y_train)\n", "intent": "Now we fit our model using the training data set:\n"}
{"snippet": "lm = LinearRegression()\nlm.fit(train_data, train_labels)\n", "intent": "Build a linear regression model for all the points of the complete cases.\n"}
{"snippet": "analyze_single_model(\"pickles/net-base.pickle\")\n", "intent": "This is the same model as above, but run for a maximum 10,000 epochs (it did hit early stopping around 9000).\n"}
{"snippet": "pipeline.fit(X_train,y_train)\n", "intent": "Excellent! Now fit the pipeline just like you would a regular classifier on the training set\n"}
{"snippet": "clf_grid = GridSearchCV(pipeline, param_grid)\nclf_grid.fit(X_train,y_train)\n", "intent": "Great, now instantiate the grid search object and fit it on the training set \n"}
{"snippet": "svm = LinearSVC(C=0.1)\n", "intent": "1) Instantiate an object and set the parameters\n"}
{"snippet": "arr = np.arange(24).reshape(2,3,4)\nprint(\"In NumPy:\", arr.shape,arr,sep=\"\\n\")\nshape_op = tf.shape(arr)\nprint(\"In TensorFlow:\", shape_op, sep=\"\\n\")\nshape = tf.Session().run(shape_op)\nprint(\"Shape of tensor: \" + str(shape))\n", "intent": "You can use the `tf.shape` Operation to get the shape value of `Tensor` objects:\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "**Creating an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "grad_graph = tf.Graph()\nwith grad_graph.as_default():\n    a = tf.Variable(3.0)\n    b = tf.square(a) \n    opt = tf.train.GradientDescentOptimizer(0.05)\n    grads = opt.compute_gradients(b, [a])\n    init = tf.global_variables_initializer()\nwith tf.Session(graph=grad_graph) as session:\n    session.run(init)\n    print(session.run(grads))\n", "intent": "The below code gets the gradient of the `tf.square()` Operation, which we expect to return `2*input`, as the derivative of $x^2$ is $2x$\n"}
{"snippet": "old_alex_graph = tf.Graph()\nwith old_alex_graph.as_default():\n    saver = tf.train.import_meta_graph(\"saved_models/alexnet.meta\")\n", "intent": "And here is how we can bring that saved model back.  We'll play with a \"reloaded\" model more next week\n"}
{"snippet": "with var_graph.as_default(), tf.variable_scope('my_var_scope'):\n    try:\n        w_again = tf.get_variable('w')\n        b_again = tf.get_variable('b')\n    except ValueError as e:\n        print(str(e).splitlines()[0]) \n", "intent": "Note that if we are within the same variable scope, we _must_ set `reuse` to `True`. If we don't, TensorFlow will complain at us:\n"}
{"snippet": "with var_graph.as_default(), tf.variable_scope('my_var_scope') as var_scope:\n    var_scope.reuse_variables()\n    w_again = tf.get_variable('w')\n    b_again = tf.get_variable('b')\nprint(w_again, b_again, sep=\"\\n\")\nprint(w is w_again and b is b_again)  \n", "intent": "As an alternative to passing `reuse` into the `variable_scope` parameter, we can set it after the fact by using the `variable_scope.\n"}
{"snippet": "with var_graph.as_default(), tf.variable_scope('my_var_scope'):\n    curr_scope = tf.get_variable_scope()\n    curr_scope.reuse_variables()\n    w_again = tf.get_variable('w')\n    b_again = tf.get_variable('b')\n", "intent": "You can get the current variable scope with `tf.get_variable_scope()`; similar to `tf.get_default_graph()`:\n"}
{"snippet": "var_graph = tf.Graph()\nwith var_graph.as_default(), tf.variable_scope('my_var_scope') as scope:\n    w_init = tf.truncated_normal_initializer()\n    b_init = tf.zeros_initializer()\n    w = tf.get_variable('w', shape=[10, 10], initializer=w_init)\n    b = tf.get_variable('b', shape=[10], initializer=b_init)\nprint(w,b,sep=\"\\n\")\n", "intent": "Here's the first example we used above:\n"}
{"snippet": "var_graph = tf.Graph()\nwith var_graph.as_default():\n    init1 = tf.truncated_normal_initializer()\n    init2 = tf.zeros_initializer()\n    with tf.variable_scope('var_scope_1', initializer=init1) as var_scope_1:\n        w1 = tf.get_variable('w1', shape=[10, 10])\n        with tf.variable_scope('var_scope_2', initializer=init2) as var_scope_2:\n            w2 = tf.get_variable('w2', shape=[200])\n            w3 = tf.get_variable('w3', shape=[300,10,10])\n", "intent": "These default parameters can be nested, too.\n"}
{"snippet": "h = activation(torch.matmul(features, W1) + B1)\noutput = activation(torch.mm(h, W2) + B2)\nprint(output)\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "import helper\nimages, labels = next(iter(trainloader))\nimg = images[0].view(1, 784)\nwith torch.no_grad():\n    logps = model(img)\nps = torch.exp(logps)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "from pymc3.backends import SQLite\nniter = 2000\nwith pm.Model() as sqlie3_save_demo:\n    p = pm.Beta('p', alpha=2, beta=2)\n    y = pm.Binomial('y', n=n, p=p, observed=heads)\n    db = SQLite('trace.db')\n    trace = pm.sample(niter, trace=db)\n", "intent": "If you are fitting a large complex model that may not fit in memory, you can use the SQLite3 backend to save the trace incremnetally to disk.\n"}
{"snippet": "calibration_data = train_model(\n    learning_rate=0.05,\n    steps=500,\n    batch_size=5,\n    input_feature=\"rooms_per_person\")\n", "intent": "To verify that clipping worked, let's train again and print the calibration data once more:\n"}
{"snippet": "model.pop()\nfor layer in model.layers: layer.trainable=False\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "Re-create and load our modified VGG model with binary dependent (i.e. dogs v cats)\n"}
{"snippet": "def fit_model(model, batches, val_batches, nb_epoch=1):\n    model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=nb_epoch, \n                        validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n", "intent": "We'll define a simple function for fitting models, just to save a little typing...\n"}
{"snippet": "K.set_value(m_final.optimizer.lr, 1e-4)\nm_final.fit([arr_lr, arr_hr], targ, 16, 2, **parms)\n", "intent": "We use learning rate annealing to get a better fit.\n"}
{"snippet": "top_model = Model(inp, outp)\ntop_model.save_weights(dpath+'top_final.h5')\n", "intent": "We are only interested in the trained part of the model, which does the actual upsampling.\n"}
{"snippet": "linear = LinearRegression()\nlinear.fit(X_train, y_train_hot)\n", "intent": "First, let's do a basic linear regression.\n"}
{"snippet": "from keras.layers import Activation\nmodel.add(Activation('relu'))\n", "intent": "Now let's add an Activation function to our network after our first fully connected layer.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=100)\n", "intent": "Now we have to actually train our model on the data. This is really easy in Keras, in fact it only takes one line of code.\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(..., ...) \n", "intent": "Now let's choose a model. For this we're going ot choose a Gaussian naive Bayes model with model.fit()\n"}
{"snippet": "with pm.Model() as prior_context:\n    sigma = pm.Gamma('gamma', alpha=2.0, beta=1.0)\n    mu = pm.Normal('mu', mu=0, sd=sigma)\n    trace = pm.sample(niter, step=pm.Metropolis())\n", "intent": "Just omit the `observed=` argument.\n"}
{"snippet": "clf = tree.DecisionTreeClassifier(criterion = 'entropy')\nclf.fit(train_features, train_labels)\n", "intent": "Create a basic decision tree which minimizes entropy and fit it to the training data.\n"}
{"snippet": "parameters = {\"min_samples_split\": [2, 10],\n              \"max_depth\": [None, 2, 5, 10],\n              \"min_samples_leaf\": [1, 5, 10],\n              \"max_leaf_nodes\": [None, 5, 10, 20],\n              }\ngridsearch = GridSearchCV(clf, parameters)\ngridsearch.fit(train_features, train_labels)\n", "intent": "We will now use Grid Search to find a good set of hyperparameters which attempt to regualize the tree.\n"}
{"snippet": "svc2 = svm.LinearSVC(C=1000, loss='hinge', max_iter=5000)\nsvc2.fit(data[['X1', 'X2']], data['y'])\nsvc2.score(data[['X1', 'X2']], data['y'])\n", "intent": "It appears that it mis-classified the outlier.  Let's see what happens with a larger value of C.\n"}
{"snippet": "clf = KNeighborsClassifier().fit(X_train, y_train)\n", "intent": "Train and test a standard K Nearest Neighbors classifier on the original data\n"}
{"snippet": "cov = EmpiricalCovariance().fit(X_train)\ncov = cov.covariance_\n", "intent": "To determine how many principle components to use for PCA, let's look at the eigenvalues of the covariance matrix.\n"}
{"snippet": "X_train_pca = pca.transform(X_train)\nclf_pca = KNeighborsClassifier().fit(X_train_pca, y_train)\n", "intent": "Train and test another default K Nearest Neighbors classifier, but now use the data with reduced dimensions.\n"}
{"snippet": "model_yis = model_linear(xis_true, slope_ml, intercept_ml)\nobject_chi2s = 0.5*((yis_noisy - model_yis) / sigma_yis)**2\n", "intent": "Let's visualize the $\\chi^2$ for individual objects\n"}
{"snippet": "mrcnn = model.run_graph([image], [\n    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n    (\"probs\", model.keras_model.get_layer(\"mrcnn_class\").output),\n    (\"deltas\", model.keras_model.get_layer(\"mrcnn_bbox\").output),\n    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n])\n", "intent": "Run the classifier heads on proposals to generate class propbabilities and bounding box regressions.\n"}
{"snippet": "activations = model.run_graph([image], [\n    (\"input_image\",        model.keras_model.get_layer(\"input_image\").output),\n    (\"res2c_out\",          model.keras_model.get_layer(\"res2c_out\").output),\n    (\"res3c_out\",          model.keras_model.get_layer(\"res3c_out\").output),\n    (\"res4w_out\",          model.keras_model.get_layer(\"res4w_out\").output),  \n    (\"rpn_bbox\",           model.keras_model.get_layer(\"rpn_bbox\").output),\n    (\"roi\",                model.keras_model.get_layer(\"ROI\").output),\n])\n", "intent": "In some cases it helps to look at the output from different layers and visualize them to catch issues and odd patterns.\n"}
{"snippet": "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_idx\")\nindexed = indexer.fit(output).transform(output)\n", "intent": "Convert laebl to numeric index\n"}
{"snippet": "activations = model.run_graph([image], [\n    (\"input_image\",        model.keras_model.get_layer(\"input_image\").output),\n    (\"res2c_out\",          model.keras_model.get_layer(\"res2c_out\").output),\n    (\"res3c_out\",          model.keras_model.get_layer(\"res3c_out\").output),\n    (\"rpn_bbox\",           model.keras_model.get_layer(\"rpn_bbox\").output),\n    (\"roi\",                model.keras_model.get_layer(\"ROI\").output),\n])\n", "intent": "In some cases it helps to look at the output from different layers and visualize them to catch issues and odd patterns.\n"}
{"snippet": "model.fit(X_train, Y_train, epochs = 25, batch_size = 32)\n", "intent": "Run the following cell to train your model on 2 epochs with a batch size of 32. On a CPU it should take you around 5min per epoch. \n"}
{"snippet": "testGraph = readGraph(\"input/celegans_metabolic.graph\", Format.METIS)\nprint(\"Loaded graph with\", testGraph.numberOfEdges(), \"edges.\")\n", "intent": "3) Load an actual network that should be analyzed\n"}
{"snippet": "learn.fit(lrs/10,1,wds=wd, cycle_len=5,use_clr=(20,5))\n", "intent": "Training at this image size increases the dice score to 89.3.\n"}
{"snippet": "learn.fit(lrs/10,1,wds=wd, cycle_len=5,use_clr=(20,5))\n", "intent": "Training on full size images brings the dice score to 89.99\n"}
{"snippet": "learn.fit(lr/5,1,wds=wd, cycle_len=5,use_clr=(20,5))\n", "intent": "This brings our dice score to 91.0\n"}
{"snippet": "learner.fit(3e-3, 1, wds=1e-6)\n", "intent": "Now we begin training\n"}
{"snippet": "trn_tfms, val_tfms = tfms_from_model(arch, sz)\nmodel=learn.model\nmodel.eval()\nacts = plot_cnn_visuals(PATH, filename, model, transformer=val_tfms, total_layers=9,\n                    resize=(512,512), animation_interval=250, folder_suffix='_dog1')\n", "intent": "There's a good dog. Now we input the image filename and our model into the plot_cnn_visuals function\n"}
{"snippet": "tfms = tfms_from_model(arch, sz, aug_tfms=augs, crop_type=CropType.NO)\ndata = ImageClassifierData.from_csv(PATH, 'train_crop', f'{PATH}train_int.csv', test_name='test',\n                                      val_idxs=val_idxs, tfms=tfms, bs=bs)\n", "intent": "Here we set up our transformer to do our image augmentation and our dataloader to pass minibatches to the model.\n"}
{"snippet": "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_idx\")\nindexed = indexer.fit(output).transform(output)\n", "intent": "Convert label to numeric index\n"}
{"snippet": "learn.data.valid_ds.ds.classes[np.argmax(to_np(F.softmax(preds[500], dim=0)))]\n", "intent": "The index of the largest value in the vector corresponds to the class prediction\n"}
{"snippet": "class FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048, dropout = 0.2):\n        super().__init__() \n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n    def forward(self, x):\n        x = self.dropout(F.relu(self.linear_1(x)))\n        x = self.linear_2(x)\n        return x\n", "intent": "This is a standard feed forward network that will be used in the encoder and decoder layers.\n"}
{"snippet": "en_vecs = ft.load_model(str((PATH/'wiki.en.bin')))\nfr_vecs = ft.load_model(str((PATH/'wiki.fr.bin')))\n", "intent": "Now we add in fastText word vectors for English and French words in our vocabulary that are represented in fastText\n"}
{"snippet": "learner.fit(1e-3, 1, wds=1e-5, cycle_len=8)\n", "intent": "There doesn't seem to be an obvious effect.\nLets see where we can take this thing\n"}
{"snippet": "kmodel = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "kmodel.fit(X_train,y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "tf.reset_default_graph()\ndef generator(z, reuse=None):\n    with tf.variable_scope('gen', reuse=reuse):\n        alpha = 0.1\n        hidden1 = tf.layers.dense(inputs=z, units=128)\n        hidden1 = tf.maximum(alpha*hidden1, hidden1)\n        hidden2 = tf.layers.dense(inputs=hidden1, units=128)\n        hidden2 = tf.maximum(alpha*hidden2, hidden2)\n        output = tf.layers.dense(hidden2, units=784, activation=tf.nn.tanh)\n        return output\n", "intent": "- takes inputs z\n- applies leaky relu activation function\n- outputs result as it's a generator\n"}
{"snippet": "model2_lasso = lm.Lasso(fit_intercept=False).fit(X, y)\n", "intent": "Fit the model (by default, $\\alpha$ = 1).\n"}
{"snippet": "gs = ms.GridSearchCV(estimator=lm.Lasso(fit_intercept=False),\n                     param_grid={'alpha': np.logspace(-10, 10, 21)},\n                     scoring='neg_mean_squared_error',\n                     cv=ten_fold_cv)\ngs.fit(X, y)\n", "intent": "Determine 'optimal' value of $\\alpha$ using grid search with cross-validation.\n"}
{"snippet": "with pm.Model() as prior_context:\n    sigma = pm.Gamma('sigma', alpha=2.0, beta=1.0)\n    mu = pm.Normal('mu', mu=0, sd=sigma)\n    trace = pm.sample(niter, step=pm.Metropolis())\n", "intent": "Just omit the `observed=` argument.\n"}
{"snippet": "gs = ms.GridSearchCV(estimator=lm.Ridge(fit_intercept=False),\n                     param_grid={'alpha': np.logspace(-10, 10, 21)},\n                     scoring='neg_mean_squared_error',\n                     cv=ten_fold_cv)\ngs.fit(X, y)\n", "intent": "Determine 'optimal' value of $\\alpha$ using grid search with cross-validation.\n"}
{"snippet": "X = whites[['density', 'sulphates']]\ny = whites['quality']\nmodel1 = lm.LinearRegression().fit(X, y)\n", "intent": "Fit a linear regression model for 'quality' using 'density' and 'sulphates' as predictors.\n"}
{"snippet": "gs = ms.GridSearchCV(estimator=lm.Lasso(),\n                     param_grid={'alpha': np.logspace(-10, 10, 21)},\n                     scoring='neg_mean_squared_error',\n                     cv=five_fold_cv)\ngs.fit(X, y)\n", "intent": "Using `GridSearchCV`, tune a linear regression model with LASSO regularisation (include all predictors).\n"}
{"snippet": "Cs = np.logspace(-4, 4, 10)\ngs = ms.GridSearchCV(\n    estimator=lm.LogisticRegression(),\n    param_grid={'C': Cs},\n    scoring='roc_auc',\n    cv=five_fold_cv\n)\ngs.fit(X, y)\n", "intent": "Determine 'optimal' value of `C` by cross-validation using AUC scoring\n(`scikit-learn` uses $L_{2}$ regularisation by default).\n"}
{"snippet": "X = whites.drop(['quality', 'good_quality'], axis=1)\ny = whites.good_quality\nmodel1 = lm.LogisticRegression(C=1e50)\nmodel1.fit(X, y)\n", "intent": "Fit a logistic regression model for 'good_quality' using all predictors.\n"}
{"snippet": "model2 = lm.LogisticRegressionCV(Cs=10, cv=ten_fold_cv, scoring='roc_auc')\nmodel2.fit(X, y)\n", "intent": "Determine 'optimal' value of `C` by cross-validation using AUC scoring and $L_{2}$ regularisation.\n"}
{"snippet": "spls.set_params(\n    pls__n_components=3\n)\nspls.fit(boroughs, feelings)\n", "intent": "Train a PLS regression model with three components.\n"}
{"snippet": "gs = ms.GridSearchCV(\n    estimator=spls,\n    param_grid={\n        'pls__n_components': np.arange(1, 10)\n    },\n    scoring='neg_mean_squared_error',\n    cv=three_fold_cv\n)\ngs.fit(boroughs, feelings)\n", "intent": "Determine 'optimal' number of components.\n"}
{"snippet": "tree1 = tree.DecisionTreeClassifier()\ntree1.fit(X, y)\n", "intent": "Train a decision tree.\n"}
{"snippet": "qw = Normal(mu=tf.Variable(tf.random_normal([p])),\n            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([p]))))\nqb = Normal(mu=tf.Variable(tf.random_normal([1])),\n            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))\n", "intent": "We fit a fully factroized variational model by minimizing the Kullback-Leibler divergence.\n"}
{"snippet": "rf1 = ensemble.RandomForestClassifier(n_estimators=20)\nrf1.fit(X, y)\n", "intent": "Train a random forest with 20 decision trees.\n"}
{"snippet": "X = whites.drop('quality', axis=1)\ny = whites.quality\ntree1 = tree.DecisionTreeRegressor(max_depth=2, min_samples_leaf=50)\ntree1.fit(X, y)\n", "intent": "Train a decision tree for 'quality' limiting the depth to 3, and the minimum number of samples per leaf to 50.\n"}
{"snippet": "rf1 = ensemble.RandomForestRegressor(n_estimators=20)\nrf1.fit(X, y)\n", "intent": "Train a random forest with 20 decision trees.\n"}
{"snippet": "rf2 = ensemble.RandomForestClassifier(n_estimators=20)\nrf2.fit(X_tfidf, y)\n", "intent": "Train a random forest with 20 decision trees.\n"}
{"snippet": "lda = LdaModel(corpus=corpus, num_topics=10, id2word=id2word)\n", "intent": "Fit LDA model with 10 topics.\n"}
{"snippet": "gtb1 = ensemble.GradientBoostingClassifier(n_estimators=20)\ngtb1.fit(X, y)\n", "intent": "Train a gradient tree boosting classifier with 20 decision trees.\n"}
{"snippet": "ssvc.set_params(\n    svc__kernel='linear'\n)\nssvc.fit(X, y)\n", "intent": "Train a support vector classifier with linear (= no) kernel.\n"}
{"snippet": "ssvc.set_params(\n    svc__kernel='rbf'\n)\nssvc.fit(X, y)\n", "intent": "Train using the Radial Basis Function (RBF) kernel.\n"}
{"snippet": "gs = ms.GridSearchCV(\n    estimator=ssvc,\n    param_grid={\n        'svc__C': [1e-15, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n        'svc__kernel': ['linear', 'rbf']\n    },\n    scoring='roc_auc',\n    cv=ten_fold_cv\n)\ngs.fit(X, y)\n", "intent": "Determine 'optimal' kernel and value of `C` by cross-validation using AUC scoring.\n"}
{"snippet": "import sklearn.linear_model\nlinear_model = sklearn.linear_model.LinearRegression()\nlinear_model.fit(x_data.reshape(-1, 1), y_data, sample_weight=weights)\nfit = dict(m=linear_model.coef_[0], b=linear_model.intercept_)\nprint fit\n", "intent": "scikit-learn provides a convenient class for (weighted) linear least squares:\n"}
{"snippet": "nn.add(Dense(input_dim=X.shape[1], units=5, activation='sigmoid'))\n", "intent": "Input layer feeding into hidden layer with 5 neurons (sigmoid activation):\n"}
{"snippet": "nn.add(Dense(units=1, activation='sigmoid'))\n", "intent": "Hidden layer feeding into a single output neuron (sigmoid activation):\n"}
{"snippet": "gs = ms.GridSearchCV(\n    estimator=ssvc,\n    param_grid={\n        'svr__C': [1e-15, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n        'svr__kernel': ['linear', 'rbf']\n    },\n    scoring='neg_mean_squared_error',\n    cv=split\n)\ngs.fit(X, y)\n", "intent": "Determine 'optimal' kernel and value of `C` by cross-validation.\n"}
{"snippet": "regressor = LinearRegression()\nregressor.fit(X_train, y_train)\n", "intent": "Reference: http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\n"}
{"snippet": "humidity_classifier = DecisionTreeClassifier(max_leaf_nodes=10, random_state=0)\nclf = humidity_classifier.fit(X_train, y_train)\nclf\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nFit on Train Set\n<br><br></p>\n"}
{"snippet": "kmeans = KMeans(n_clusters=2)\nclusters = kmeans.fit(X)\n", "intent": "Set up the k-means clustering analysis\n"}
{"snippet": "logreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\nmodel = GridSearchCV(LogisticRegression(),logreg_parameters, cv=5)\nmodel.fit(x,y)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "logreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\nmodel = GridSearchCV(LogisticRegression(),logreg_parameters, scoring = 'average_precision', cv=5)\nmodel.fit(x,y)\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "from sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nX = tb[['1Q15_nSold_x_Price']]\ny = tb['2015_sales']\nlm = LinearRegression()\nlm.fit(X, y)\nlm.coef_ , lm.intercept_\n", "intent": "Using scikit-learn or statsmodels, build the necessary models for your scenario. Evaluate model fit.\n"}
{"snippet": "nsample=2000\nwith pm.Model() as model:\n    start = pm.find_MAP() \n    step = pm.NUTS(scaling=start) \n    trace = pm.sample(nsample, step, start=start, progressbar=True, njobs=4) \nlines = {var:trace[var].mean() for var in trace.varnames}\npm.traceplot(trace, lines=lines)\n", "intent": "Repeat the earlier analysis and learn the variance of the data\n"}
{"snippet": "model_knn = KNeighborsClassifier()\nkNN_grid = {\"n_neighbors\": np.arange(1,11), \"weights\": [\"uniform\", \"distance\"]}\nselector_knn = GridSearchCV(model_knn, kNN_grid, cv = 5)\nselector_knn.fit(X_train2, y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "logreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\ngs = grid_search.GridSearchCV(logit, logreg_parameters, cv=5)\ngs_model = gs.fit(X_train, y_train) \n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nlogreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\nmodel2 = LogisticRegression()\nselector2 = GridSearchCV(model2, logreg_parameters, cv = 5)\nselector2.fit(X_train_best, y_train)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier as kNN\nmodel3 = kNN()\nkNN_grid = {\"n_neighbors\": np.arange(1,11), \"weights\": [\"uniform\", \"distance\"]}\nselector3 = GridSearchCV(model3, kNN_grid, cv = 5)\nselector3.fit(X_train_best, y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "logreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\nmodel4 = LogisticRegression()\nselector4 = GridSearchCV(model4, logreg_parameters, cv = 5, scoring = \"average_precision\")\nselector4.fit(X_train_best, y_train)\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nlogreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\ngs = GridSearchCV(estimator = LogisticRegression(), param_grid = logreg_parameters, cv = 5)\ngs_model = gs.fit(X, y)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn_parameters = {\n    'n_neighbors': np.arange(5, 10),\n    'weights': ['uniform', 'distance']\n}\nknn_gridsearch = GridSearchCV(estimator = KNeighborsClassifier(), param_grid = knn_parameters, cv = 5)\nknn_gridsearch.fit(X, y)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "import statsmodels.api as sm\nX = np.array(xs).transpose()\nX = sm.add_constant(X)\nmod = sm.OLS(ys, X)\nres = mod.fit()\nres.summary()\n", "intent": "This generated a fit of $y = 2 x + 0.2420$. Let's see what a linear regression yields.\n"}
{"snippet": "X = data[[\"petal width (cm)\", \"sepal width (cm)\"]]\ny = target\nkmean_cluster = cluster.KMeans(n_clusters = 3, n_init = 20)\nkmean_cluster.fit(X, y)\n", "intent": "Run the clustering analysis using scikit-learn.\n*Hint*: Estimate the number of clusters, k, based on your visual examination of the distributions.\n"}
{"snippet": "import theano.tensor as T \ndef logp_signoise(yobs, is_outlier, yest_in, sigma_y_in, yest_out, sigma_y_out):\n    pdfs_in = T.exp(-(yobs - yest_in + 1e-4)**2 / (2 * sigma_y_in**2)) \n    pdfs_in /= T.sqrt(2 * np.pi * sigma_y_in**2)\n    logL_in = T.sum(T.log(pdfs_in) * (1 - is_outlier))\n    pdfs_out = T.exp(-(yobs - yest_out + 1e-4)**2 / (2 * (sigma_y_in**2 + sigma_y_out**2))) \n    pdfs_out /= T.sqrt(2 * np.pi * (sigma_y_in**2 + sigma_y_out**2))\n    logL_out = T.sum(T.log(pdfs_out) * is_outlier)\n    return logL_in + logL_out\n", "intent": "Assume the data are drawn from two Gaussians error distribution (one for the function and the other for the outliers)\n"}
{"snippet": "logreg = linear_model.LogisticRegressionCV(cv = 5)\nlogreg.fit(X, y)\n", "intent": "Define a logistic regression and train it with the feature and target set\n"}
{"snippet": "gs_logreg = linear_model.LogisticRegression(C = 2.5, penalty = 'l2', solver = 'liblinear')\ngs_logreg.fit(cat_X,y)\n", "intent": "The best model has C = 100.0 and uses L1 regularization penalty.\n"}
{"snippet": "gs_logreg = linear_model.LogisticRegression(C = 100.0, penalty = 'l1', solver = 'liblinear')\ngs_logreg.fit(X,y)\n", "intent": "The best model has C = 100.0 and uses L1 regularization penalty.\n"}
{"snippet": "formula1 = 'prglngth ~ agepreg'\nmodel = smf.ols(formula1, data=newdf)\nresults = model.fit()\nprint results.summary()\n", "intent": "I started by comparing the dependent variable, pregnancy length, with age pregnant\n"}
{"snippet": "formula1 = 'prglngth ~ race'\nmodel = smf.ols(formula1, data=newdf)\nresults = model.fit()\nprint results.summary()\n", "intent": "I then compared the dependent variable, pregnancy length, with race\n"}
{"snippet": "formula1 = 'prglngth ~ birthord'\nmodel = smf.ols(formula1, data=newdf)\nresults = model.fit()\nprint results.summary()\n", "intent": "I then compared the dependent variable, pregnancy length, with birth order of the child\n"}
{"snippet": "formula1 = 'prglngth ~ basewgt'\nmodel = smf.ols(formula1, data=newdf)\nresults = model.fit()\nprint results.summary()\n", "intent": "I then compared the dependent variable, pregnancy length, with respondents weight\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nforest_model_with_num_words = RandomForestClassifier()\nforest_model_with_num_words.fit(X_train, y_train)\nforest_model_with_num_words.score(X_test, y_test)\n", "intent": "Let's see how good we can do with a Random Forest.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegressionCV\nmodel = LogisticRegressionCV(Cs=[math.e**v for v in range(-5,5)],\n                             multi_class='multinomial',\n                             solver='newton-cg')\nmodel.fit(X_train, y_train)\nprint \"With tuning\", model.score(X_test, y_test)\n", "intent": "As a final step, let's tune the amount of regularization for our logistic regression model (since it seems to be working the best).\n"}
{"snippet": "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('flatten_8').output)\nencoder.summary()\n", "intent": "This will allow us to encode the images and look at what the encoding results in. \n"}
{"snippet": "knn.fit(X, y)\n", "intent": "so now that we have an instance of this classifier, let's use it to train the model\n"}
{"snippet": "logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n", "intent": "> __Note:__ Why 97 and 53? 150 * .35 = 52.5 or rounded to 53\nNow let's use the data and perform the training and testing activities\n"}
{"snippet": "a = tensor(-1.,1)\n", "intent": "Suppose we believe `a = (-1.0,1.0)` then we can compute `y_hat` which is our *prediction* and then compute our error.\n"}
{"snippet": "lm.fit(x_train, y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "dtr = tree.DecisionTreeRegressor(max_depth=4)\n", "intent": "Train a DecisionTreeRegressor.\n"}
{"snippet": "clf = tree.DecisionTreeRegressor(max_depth=2)\nclf = clf.fit(X_train, y_train)\n", "intent": "Train a DecisionTreeRegressor.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32,input_dim = x_train.shape[1]))\nmodel.add(Activation('relu'))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dense(8))\nmodel.add(Dropout(0.5))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(2))\nmodel.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train,y_train,epochs=30,verbose = 0,batch_size = 50)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "results = model.fit()\n", "intent": "Now we actually fit the model. `fit` actually runs the model and returns a model-specific Results class that we can work with.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier( \nknn_clf.fit( \n", "intent": "**Problem 3b**\nTrain a $k=7$ nearest neighbors machine learning model on the Titanic training set.\n"}
{"snippet": "tf.reset_default_graph()\nm1 = tf.constant([[3., 3.]], name='M1')\nm2 = tf.placeholder('float32', shape=...) \nproduct = 10*tf.matmul(m1,m2)\n", "intent": "Instead of the matrix m2 use a placeholder to feed-in values. You must specify the shape of the m2 matrix (rows, columns). \n"}
{"snippet": "sess = tf.Session()\nval_m2 = np.array([[2.],[3.]])\nres = sess.run(fetches=..., feed_dict=...) \nprint(res)\nsess.close()\n", "intent": "Provide the correct feeds (inputs) and fetches (outputs of the computational graph)\n"}
{"snippet": "tf.reset_default_graph()\nm1 = tf.constant([[3., 3.]], name='M1')\nm2 = tf.placeholder('float32', shape=...) \nproduct = 10*tf.matmul(m1,m2)\n", "intent": "Instead of the matrix m2 use a placefolder to feed-in values. You must specify the shape of the m2 matrix (rows, columns). \n"}
{"snippet": "tf.reset_default_graph()\nm1 = tf.constant([[3., 3.]], name='M1')\nm2 = tf.placeholder('float32', shape=(2,1)) \nproduct = 10*tf.matmul(m1,m2)\n", "intent": "Instead of the matrix m2 use a placefolder to feed-in values. You must specify the shape of the m2 matrix (rows, columns). \n"}
{"snippet": "with tf.Session() as sess: \n    sess.run(init_op) \n    res = sess.run(loss, feed_dict={x:x_data, y:y_data, a:, b:}) \n    writer = tf.summary.FileWriter(\"/tmp/linreg\", sess.graph) \n    writer.close()\n    print(\"Loss {} \".format(res))\n", "intent": "Now feed your optimal parameters from above through the tensorflow graph and compare the \"loss\" with the RSS \n"}
{"snippet": "with tf.Session() as sess: \n    sess.run(init_op) \n    res = sess.run(loss, feed_dict={x:x_data, y:y_data, a:1.7, b:1.55}) \n    writer = tf.summary.FileWriter(\"/tmp/linreg\", sess.graph) \n    writer.close()\n    print(\"Loss {} (a=1.7, b=1.55)\".format(res))\n", "intent": "Now feed your optimal parameters from above through the tensorflow graph and compare the \"loss\" with the RSS \n"}
{"snippet": "model_1Dconv_w_d = Sequential()\nks = 5\nmodel_1Dconv_w_d.add(Convolution1D())\nmodel_1Dconv_w_d.add(Convolution1D())\nmodel_1Dconv_w_d.add(Convolution1D())\nmodel_1Dconv_w_d.add(Convolution1D())\nmodel_1Dconv_w_d.add(Dense(1))\nmodel_1Dconv_w_d.add(Lambda(slice, arguments={'slice_length':look_ahead}))\nmodel_1Dconv_w_d.compile(optimizer='adam', loss='mean_squared_error')\nmodel_1Dconv_w_d.summary()\n", "intent": "Here we define a Neural network with 1D convolutions and \"causal\" padding, this time with dilation rate, so we are able to look back longer in time.\n"}
{"snippet": "model_simple_RNN = Sequential()\nmodel_simple_RNN.add(SimpleRNN(12,return_sequences=True,input_shape=(128,1)))\nmodel_simple_RNN.add((Dense(1)))\nmodel_simple_RNN.add(Lambda(slice, arguments={'slice_length':look_ahead}))\nmodel_simple_RNN.summary()\nmodel_simple_RNN.compile(optimizer='adam', loss='mean_squared_error')\n", "intent": "Now, let's use a RNN cell to see if we are able to learn the data generating process. We will use a hidden state size of 12.\n"}
{"snippet": "model_LSTM = Sequential()\nmodel_LSTM.add()\nmodel_LSTM.add((Dense(1)))\nmodel_LSTM.add(Lambda(slice, arguments={'slice_length':look_ahead}))\nmodel_LSTM.summary()\nmodel_LSTM.compile(optimizer='adam', loss='mean_squared_error')\n", "intent": "Let's use a more complex LSTM cell to and see if it works better than the RNN cell,  we again use a hidden state size of 12.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier(n_neighbors=7)\nknn_clf.fit(X, y)\n", "intent": "**Problem 3b**\nTrain a $k=7$ nearest neighbors machine learning model on the Titanic training set.\n"}
{"snippet": "model_LSTM = Sequential()\nmodel_LSTM.add(LSTM(12,return_sequences=True,input_shape=(128,1)))\nmodel_LSTM.add((Dense(1)))\nmodel_LSTM.add(Lambda(slice, arguments={'slice_length':look_ahead}))\nmodel_LSTM.summary()\nmodel_LSTM.compile(optimizer='adam', loss='mean_squared_error')\n", "intent": "Let's use a more complex LSTM cell to and see if it works better than the RNN cell,  we again use a hidden state size of 12.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=300, random_state=36)  \nclassifier.fit(X_train, y_train)\n", "intent": "let's train a radomforest on the bag of words features of the train set\n"}
{"snippet": "mus = np.zeros(shape=(50,100))\nfor i in range(0,50):\n    y_hat = model(x_test)\n    mu = y_hat.mean()\n    mus[i] = mu[:,0]\n", "intent": "Now, we average over many runs of the network. This is the same as taking the mean of the predictive distribution. \n"}
{"snippet": "qw = Normal(mu=tf.Variable(tf.random_normal([D])),\n            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D]))))\nqb = Normal(mu=tf.Variable(tf.random_normal([1])),\n            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))\n", "intent": "Perform variational inference. Define the variational model to be a fully factorized normal across the weights.\n"}
{"snippet": "qw_mu = tf.Variable(tf.random_normal([D]))\nqw_sigma = tf.nn.softplus(tf.Variable(tf.random_normal([D])))\nqb_mu = tf.Variable(tf.random_normal([]) + 10)\nqb_sigma = tf.nn.softplus(tf.Variable(tf.random_normal([])))\nqw = ?\nqb = ?\n", "intent": "Let's define the variational parameters and then the variational distribution. Let's use Gaussians.\n"}
{"snippet": "S = 50\nrs = np.random.RandomState(0)\ninputs = np.linspace(-5, 3, num=400, dtype=np.float32)\nx_in = tf.expand_dims(inputs, 1)\nmus = []\nfor s in range(S):\n    mus += [tf.sigmoid(ed.dot(x_in, ?) + ?)]\nmus = tf.stack(mus)\n", "intent": "We want to sample from the variational distribution. Can you fill in the question marks below?\n"}
{"snippet": "from sklearn.pipeline import make_pipeline\npoly_model = make_pipeline(PolynomialFeatures(7),\n                           LinearRegression())\n", "intent": "**Now, John decides to use the same technique to create a 7th-degree polynomial model for the non-linear data that he generated  earlier **\n"}
{"snippet": "clf = GridSearchCV(SVR(), param_grid=param_grid)\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n", "intent": "This is much faster, but might result in worse hyperparameters and therefore worse results.\n"}
{"snippet": "text_clf = text_clf.fit(X_train, y_train)\n", "intent": "Fitting the model to the train set\n"}
{"snippet": "indexer = StringIndexer(inputCol=\"raw_label\", outputCol=\"label\")\nindexed = indexer.fit(output).transform(output)\n", "intent": "Convert label to numeric index\n"}
{"snippet": "model = svm.SVC(kernel=\"linear\")\n", "intent": "sklearn provides multiple SVM implementations\n"}
{"snippet": "model.fit(X, y)\n", "intent": "After loading and preparing the data and creating an SVM object, the model is now ready to learn.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    params = tf.Variable(tf.random_uniform(shape=(vocab_size, embed_dim), minval=-1, maxval=1))\n    embed = tf.nn.embedding_lookup(params, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def sigmoid(self, z):\n", "intent": "Implement functions to calculate sigmoid and it's derivative\n"}
{"snippet": "def softmax(self, scores):\n", "intent": "Implement the softmax function, cross entropy loss, and its derivative\n"}
{"snippet": "early = EarlyStopping(patience=3)\nmodel.fit(x_train, y_train, batch_size=128, epochs=150, validation_split=.2, verbose = 2, callbacks=[early])\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=1000.0, random_state=0)\nlr.fit(X_train_std, y_train)\nplot_decision_regions(X_combined_std, y_combined,\n                      classifier=lr, test_idx=test_idx,\n                      xlabel = 'petal length [standardized]', \n                      ylabel='petal width [standardized]')\n", "intent": "The code is quite simple.\n"}
{"snippet": "svm = SVC(kernel='rbf', random_state=0, gamma=0.10, C=10.0)\nsvm.fit(X_xor, y_xor)\nplot_decision_regions(X_xor, y_xor,\n                      classifier=svm)\n", "intent": "This is the classification result using a rbf (radial basis function) kernel\nNotice the non-linear decision boundaries\n"}
{"snippet": "import numpy as np\nx = T.matrix(name='x') \nx_sum = T.sum(x, axis=0)\ncalc_sum = theano.function(inputs=[x], outputs=x_sum)\nary = [[1, 2, 3], [1, 2, 3]]\nprint('Column sum:', calc_sum(ary))\nary = np.array(ary, dtype=theano.config.floatX)\nprint('Column sum:', calc_sum(ary))\n", "intent": "This is an example code to work with tensors.\nCreate a $2 \\times 3$ tensor, and calculate its column sum.\n"}
{"snippet": "x = tf.placeholder(tf.float32, shape=(3,3))\ny = tf.matmul(x, x)\ndata = np.random.rand(3, 3)\nwith tf.Session() as s1:\n    print(s1.run(y, feed_dict={x: data})) \n", "intent": "Placeholders are used to feed in data when the data flow graph is run.\n"}
{"snippet": "IrisTree = model.fit(X, Y)\n", "intent": "Once we have an instance of the model we can fit it with the `fit` model by providing the inputs and target:\n"}
{"snippet": "regdict = {}\nfor k in [1, 2, 4, 6, 8, 10, 15]:\n    knnreg = KNeighborsRegressor(n_neighbors=k)\n    knnreg.fit(X_train, y_train)\n    regdict[k] = knnreg \n", "intent": "Lets vary the number of neighbors and see what we get.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nsimp_reg = LinearRegression().fit(xtrain.reshape(-1,1), ytrain)\nbeta0_sreg = simp_reg.intercept_\nbeta1_sreg = simp_reg.coef_[0]\nprint(\"(beta0, beta1) = ({0:8.6f}, {1:8.6f})\".format(beta0_sreg, beta1_sreg))\n", "intent": "To start, let's fit the old classic, linear regression.\n"}
{"snippet": "simp_reg = LinearRegression() \nsimp_reg.fit(xtrain.reshape(-1,1), ytrain) \nbeta0_sreg = simp_reg.intercept_\nbeta1_sreg = simp_reg.coef_[0]\ndfResults['OLS'][:] = [beta0_sreg, beta1_sreg]\ndfResults\n", "intent": "We start with simple linear regression to get the ball rolling.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ndef cv_optimize_ridge(x: np.ndarray, y: np.ndarray, list_of_lambdas: list, n_folds: int =4):\n    est = Ridge()\n    parameters = {'alpha': list_of_lambdas}\n    gs = GridSearchCV(est, param_grid=parameters, cv=n_folds, scoring=\"neg_mean_squared_error\")\n    gs.fit(x, y)\n    return gs\n", "intent": "Here is a helper function that we will use to get the best Ridge regression.\n"}
{"snippet": "from sklearn.linear_model import RidgeCV\nridgeCV_object = RidgeCV(alphas=(1e-8, 1e-4, 1e-2, 1.0, 10.0), cv=5)\nridgeCV_object.fit(Xtrain, ytrain)\nprint(\"Best model searched:\\nalpha = {}\\nintercept = {}\\nbetas = {}, \".format(ridgeCV_object.alpha_,\n                                                                            ridgeCV_object.intercept_,\n                                                                            ridgeCV_object.coef_\n                                                                            )\n     )\n", "intent": "Some sklearn models have built-in, automated cross validation to tune their hyper parameters. \n"}
{"snippet": "red_model = knn_pipeline.fit(x_train.drop('red', axis=1), x_train['red'])\nred_model.score(x_test.drop('red', axis=1), x_test['red'])\n", "intent": "It's easy to run the whole modelling process on new data:\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=24,\n          epochs=36,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "with tf.Session() as sess:\n    output = sess.run(hello_constant)\n    print(output)\n", "intent": "* An environment for running a graph. In charge of allocating the operations to GPU(s) and/or CPU(s).\nContinuing our example:\n"}
{"snippet": "N = df.shape[0]\nX = tf.placeholder(tf.float32, (N, 5))\ny = tf.placeholder(tf.float32, (N, 1))\nW = tf.Variable(tf.random_normal((5,1)))\nb = tf.Variable(tf.random_normal((1,)))\nyhat = tf.matmul(X, W) + b\n", "intent": "Reset the data flow graph.\n"}
{"snippet": "import tensorflow as tf\ndef run():\n    output = None\n    logit_data = [2.0, 1.0, 0.1]\n    logits = tf.placeholder(tf.float32)\n    softmax = tf.nn.softmax(logits)    \n    with tf.Session() as sess:\n        output = sess.run(softmax, feed_dict={logits: logit_data})\n    return output\n", "intent": "That's some elegant Numpy code.\n"}
{"snippet": "model.fit(X_train, y_train,\n          validation_data=(X_val, y_val), epochs=30);\n", "intent": "Keras models follow __Scikit-learn__'s interface of fit/predict with some notable extensions. Let's take a tour.\n"}
{"snippet": "class ReLU(Layer):\n    def __init__(self):\n        return np.maximum(0, input)\n    def backward(self, input, grad_output):\n", "intent": "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\n"}
{"snippet": "small_number = 1e-3\nweights = tf.Variable(initial_value=np.ones([X.shape[1], 1])*small_number,\n                      name=\"weights\", dtype='float32')\nb = tf.Variable(initial_value=small_number, name=\"bias\", dtype='float32')\n", "intent": "__Your code goes here.__ For the training and testing scaffolding to work, please stick to the names in comments.\n"}
{"snippet": "s = reset_tf_session()\nencoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=32)\ninp = L.Input(IMG_SHAPE)\ncode = encoder(inp)\nreconstruction = decoder(code)\nautoencoder = keras.models.Model(inputs=inp, outputs=reconstruction)\nautoencoder.compile(optimizer=\"adamax\", loss='mse')\n", "intent": "Convolutional autoencoder training. This will take **1 hour**. You're aiming at ~0.0056 validation MSE and ~0.0054 training MSE.\n"}
{"snippet": "model = keras.models.Sequential()\nmodel.add(L.InputLayer([None],dtype='int32'))\nmodel.add(L.Embedding(len(all_words),50))\nmodel.add(L.Bidirectional(L.LSTM(64,return_sequences=True), merge_mode='concat'))\nstepwise_dense = L.Dense(len(all_tags),activation='softmax')\nstepwise_dense = L.TimeDistributed(stepwise_dense)\nmodel.add(stepwise_dense)\nmodel.compile('adam','categorical_crossentropy')\nmodel.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n                    callbacks=[EvaluateAccuracy()], epochs=5,)\n", "intent": "**Conclusion:** With an accuracy of 0.95621 it appeared that the model suffered from a higher batch number\n"}
{"snippet": "model = keras.models.Sequential()\nmodel.add(L.InputLayer([None],dtype='int32'))\nmodel.add(L.Embedding(len(all_words),50))\nmodel.add(L.Bidirectional(L.GRU(64,return_sequences=True), merge_mode='concat'))\nstepwise_dense = L.Dense(len(all_tags),activation='softmax')\nstepwise_dense = L.TimeDistributed(stepwise_dense)\nmodel.add(stepwise_dense)\nmodel.compile('adam','categorical_crossentropy')\nmodel.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n                    callbacks=[EvaluateAccuracy()], epochs=5,)\n", "intent": "**Conclusion:** This is marginally better with an accuracy of 0.96562, however, it was timeconsuming\n"}
{"snippet": "model = keras.models.Sequential()\nmodel.add(L.InputLayer([None],dtype='int32'))\nmodel.add(L.Embedding(len(all_words),50))\nmodel.add(L.Bidirectional(L.GRU(128,return_sequences=True), merge_mode='concat'))\nstepwise_dense = L.Dense(len(all_tags),activation='softmax')\nstepwise_dense = L.TimeDistributed(stepwise_dense)\nmodel.add(stepwise_dense)\nmodel.compile('adam','categorical_crossentropy')\nmodel.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n                    callbacks=[EvaluateAccuracy()], epochs=10,)\n", "intent": "**Conclusion:** This is marginally better with an accuracy of 0.96601, however, and was slightly faster than LSTM\n"}
{"snippet": "model = keras.models.Sequential()\nmodel.add(L.InputLayer([None],dtype='int32'))\nmodel.add(L.Embedding(len(all_words),50))\nmodel.add(L.Bidirectional(L.GRU(64,recurrent_dropout=0.2,return_sequences=True), merge_mode='concat'))\nstepwise_dense = L.Dense(len(all_tags),activation='softmax')\nstepwise_dense = L.TimeDistributed(stepwise_dense)\nmodel.add(stepwise_dense)\nmodel.compile('adam','categorical_crossentropy')\nmodel.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n                    callbacks=[EvaluateAccuracy()], epochs=10,)\n", "intent": "**Conclusion:** Oh dear...this was both time-consuming, and it overfitted!\n"}
{"snippet": "from distutils.version import LooseVersion\nimport warnings\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nassert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\nprint('TensorFlow Version: {}'.format(tf.__version__))\n", "intent": "This will check to make sure you have the correct version of TensorFlow and access to a GPU\n"}
{"snippet": "with pm.Model() as logistic_model:\n    pm.glm.GLM.from_formula('income_more_50K ~ age + educ', data=data, family=pm.glm.families.Binomial())\n    map_estimate = pm.find_MAP()\n    print(map_estimate)\n", "intent": "Sumbit MAP estimations of corresponding coefficients:\n"}
{"snippet": "with pm.Model() as logistic_model:\n    pm.glm.GLM.from_formula('income_more_50K ~ sex + age + age_squared + educ + hours', \n                            data=data, \n                            family=pm.glm.families.Binomial())\n    step = pm.NUTS()\n    iter_sample = pm.iter_sample(2 * samples, step, start=map_estimate)\nanim = animation.FuncAnimation(fig, animate, init_func=init,\n                               frames=samples, interval=5, blit=True)\nHTML(anim.to_html5_video())\n", "intent": "Now rerun the animation providing the NUTS sampling method as the step argument.\n"}
{"snippet": "lenet_pow_2_k_4_lr_red_callback_non_aug_high_do_model = lenet_pow_2_k_4(imgs_train.shape[1:], dropout=0.5)\nlenet_pow_2_k_4_lr_red_callback_non_aug_high_do_model = compile_model(lenet_pow_2_k_4_lr_red_callback_non_aug_high_do_model)\nmodel, _ =\\\n    fit_model(lenet_pow_2_k_4_lr_red_callback_non_aug_high_do_model, \n              'lenet_pow_2_k_4_lr_red_callback_non_aug_high_do')\n", "intent": "> **NOTE**: It would be most appropriate to visualize on a test set as we can overfit to the validation set by trying out different architectures\n"}
{"snippet": "tf.reset_default_graph()\nwith tf.Session() as sess:\n    is_training = tf.placeholder(tf.bool, name='is_training')\n    with tf.variable_scope(\"G\") as scope:\n        z = tf.placeholder(tf.float32, [None, Z_DIM], name='z')\n        G = generator(z, is_training)\n    writer = tf.summary.FileWriter('logs', sess.graph)\n    writer.close()\n", "intent": "Tensorboard can be accessed at http://localhost:6060\n"}
{"snippet": "tf.reset_default_graph()\nis_training = tf.placeholder(tf.bool, name='is_training')\nwith tf.variable_scope(\"G\") as scope:\n    z = tf.placeholder(tf.float32, [None, Z_DIM], name='z')\n    G = generator(z, is_training)\nwith tf.variable_scope('D') as scope:\n    images = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS])\n    D_real, D_real_logits = discriminator(images, is_training)\n    scope.reuse_variables()\n    D_fake, D_fake_logits = discriminator(G, is_training)\n", "intent": "Now let's define generator and discriminator.\n"}
{"snippet": "class Seq2SeqModel(object):\n    pass\n", "intent": "**NOTE**: We will just declare the class here, and add functions to it as we progress\n"}
{"snippet": "from sklearn.svm import LinearSVC\nlinsvc2 = LinearSVC()\n", "intent": "6 - Fit a linear SVM to the data for various values of the C parameter, reporting your testing and training accuracies.  Comment on your results.\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nNB = MultinomialNB()\nNB.fit(X_spam_train, y_spam_train)\n", "intent": "I would use a multinomial classifier, since we are dealing with counts:\n"}
{"snippet": "KNN_sonar = KNeighborsClassifier()\nKNN_sonar.fit(X_sonar_train, y_sonar_train)\nprint('Train accuracy: {:.5f}'.format(KNN_sonar.score(X_sonar_train, y_sonar_train)))\nprint('Test accuracy: {:.5f}'.format(KNN_sonar.score(X_sonar_test, y_sonar_test)))\n", "intent": "4 - Fit a KNN classifier using the default settings and report the training and testing accuracies.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    word_embeddings = tf.Variable(tf.random_normal([vocab_size,embed_dim], stddev=0.1))\n    embedding_lookup = tf.nn.embedding_lookup(word_embeddings, input_data)\n    return embedding_lookup\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "for n_features in np.arange(1, 10):\n    treeClass.set_params(max_features=n_features)\n    treeClass.fit(bcw_train, bcw_target_train)\n    print('Training accuracy for {} features: {}'.format(n_features, treeClass.score(bcw_train, bcw_target_train)))\n    print('Testing accuracy for {} features: {}'.format(n_features, treeClass.score(bcw_test, bcw_target_test)))\n    print('\\n')\n", "intent": "3 - Now try different values of the `max_features` parameter and report the training and testing errors.  Comment on your results.\n"}
{"snippet": "treeClass = DecisionTreeClassifier(random_state=59)\nfor min_samples in np.arange(50, 700, 50):\n    treeClass.set_params(min_samples_split=min_samples)\n    treeClass.fit(bcw_train, bcw_target_train)\n    print('Training accuracy for {} samples: {}'.format(min_samples, treeClass.score(bcw_train, bcw_target_train)))\n    print('Testing accuracy for {} samples: {}'.format(min_samples, treeClass.score(bcw_test, bcw_target_test)))\n    print('\\n')\n", "intent": "4 - Now try different settings for the `min_samples_split` parameter.  Comment on your results.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier()\nKNN.fit(bcw_train, bcw_target_train)\nprint(KNN.score(bcw_train, bcw_target_train))\nprint(KNN.score(bcw_test, bcw_target_test))\n", "intent": "6 - Fit two other classification models of your choice to the data and comment on your results.\n"}
{"snippet": "for n_features in np.arange(1, 10):\n    forest.set_params(max_features=n_features)\n    forest.fit(bcw_train, bcw_target_train)\n    print('Training accuracy for {} features: {}'.format(n_features, forest.score(bcw_train, bcw_target_train)))\n    print('Testing accuracy for {} features: {}'.format(n_features, forest.score(bcw_test, bcw_target_test)))\n    print('\\n')\n", "intent": "2 - Now try different values of the `max_features` parameter and report the training and testing errors.  Comment on your results.\n"}
{"snippet": "forest = RandomForestClassifier(random_state=59)\nfor n_estimators in [3, 10, 30, 100, 300, 1000]:\n    forest.set_params(n_estimators=n_estimators)\n    forest.fit(bcw_train, bcw_target_train)\n    print('Training accuracy for {} estimators: {}'.format(n_estimators, forest.score(bcw_train, bcw_target_train)))\n    print('Testing accuracy for {} estimators: {}'.format(n_estimators, forest.score(bcw_test, bcw_target_test)))\n    print('\\n')\n", "intent": "3 - Now try out a few different values of the `n_estimators` parameter.  Comment on your results.\n"}
{"snippet": "forest = RandomForestClassifier(random_state=59)\nfor min_samples in 2**np.arange(1, 10):\n    forest.set_params(min_samples_split=min_samples)\n    forest.fit(bcw_train, bcw_target_train)\n    print('Training accuracy for {} samples split: {}'.format(min_samples, forest.score(bcw_train, bcw_target_train)))\n    print('Testing accuracy for {} samples split: {}'.format(min_samples, forest.score(bcw_test, bcw_target_test)))\n    print('\\n')\n", "intent": "4 - Now try a few different values for the `min_samples_split` parameter.  Then build a final model and comment on your results.\n"}
{"snippet": "forest.set_params(min_samples_split=8, n_estimators=300, max_features=2)\nforest.fit(bcw_train, bcw_target_train)\nprint('Training accuracy: {}'.format(forest.score(bcw_train, bcw_target_train)))\nprint('Testing accuracy: {}'.format(forest.score(bcw_test, bcw_target_test)))\n", "intent": "It seems that values between 4 and 64 give the better generalization, with the best result for 16.\n"}
{"snippet": "forest.set_params(min_samples_split=8, n_estimators=300, max_features=2)\nforest.fit(bcw_train_red, bcw_target_train)\nprint('Training accuracy: {}'.format(forest.score(bcw_train_red, bcw_target_train)))\nprint('Testing accuracy: {}'.format(forest.score(bcw_test_red, bcw_target_test)))\n", "intent": "The result is slightly worse on the training set but the same on the testing set. Let's try the best model from before:\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3)\nkm.fit(iris_train.iloc[:, :4])\nprint(km.labels_)\nprint(km.cluster_centers_)\n", "intent": "1 - Fit the training data using k-means clustering (without labels) using a number a clusters determined by the label values\n"}
{"snippet": "save_model_path = './image_classification'\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(epochs):\n        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_size):\n            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n        print('Epoch {:>2}:  '.format(epoch + 1), end='')\n        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n    saver = tf.train.Saver()\n    save_path = saver.save(sess, save_model_path)\n", "intent": "Now that you have your model built and your hyperparameters defined, let's train it!\n"}
{"snippet": "lr.fit(pm2_regr.dropna(thresh=5).iloc[:, :-1], pm2_regr.dropna(thresh=5).iloc[:, -1])\nlr.score(pm2_regr.dropna(thresh=5).iloc[:, :-1], pm2_regr.dropna(thresh=5).iloc[:, -1])\n", "intent": "Dropping all row with at least 3 NA gets me an error because I have nans in some rows:\n"}
{"snippet": "lr.fit(pm2_enc.iloc[:, :-1], pm2_enc.iloc[:, -1])\nlr.score(pm2_enc.iloc[:, :-1], pm2_enc.iloc[:, -1])\n", "intent": "2 - Perform a multilinear regression, using the classified data, removing the `NA` values.  Comment on your results.\n"}
{"snippet": "gnb.fit(Xtrain_norm, ytrain)\nprint(gnb.score(Xtrain_norm, ytrain))\nprint(gnb.score(Xtest_norm, ytest))\n", "intent": "Normalized dataset:\n"}
{"snippet": "gnb.fit(Xtrain_std, ytrain)\nprint(gnb.score(Xtrain_std, ytrain))\nprint(gnb.score(Xtest_std, ytest))\n", "intent": "Standardized dataset:\n"}
{"snippet": "lda = LinearDiscriminantAnalysis()\nlda.fit(X, y)\n", "intent": "2 - Determine a reasonable number of components for dimensionality reduction on the data.  Comment on your results.\n"}
{"snippet": "lr.fit(Xtrain, ytrain)\nprint('Training accuracy: {:.5f}'.format(lr.score(Xtrain, ytrain)))\nprint('Test accuracy: {:.5f}'.format(lr.score(Xtest, ytest)))\n", "intent": "4 - Perform part (3) again, but this time with the full data set and comment on your results.\n"}
{"snippet": "param_grid = [{'n_neighbors':neigh_values}]\ngs = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, n_jobs=2)\ngs.fit(Xtrain, ytrain)\nprint(gs.best_params_)\nprint(gs.best_score_)\n", "intent": "I'm going to tune each model by itself and then do the ensemble learning with the tuned models:\n"}
{"snippet": "ada = AdaBoostClassifier(n_estimators=50, algorithm='SAMME.R')\nsvm.set_params(probability=True);\n", "intent": "4 - Repeat part (3), but this time using AdaBoost.\n"}
{"snippet": "lr = LogisticRegression()\nboost_lr = AdaBoostClassifier(lr)\n", "intent": "9 - Repeat part (7) using another classification model of your choice, performing either bagging or boosting.  Comment on your results.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "rte = RandomTreesEmbedding()\nrte.fit(X, y)\nX_trans = rte.transform(X)\n", "intent": "2 - Repeat part (1), but this time transforming the data using a Totally Random Trees hash prior to fitting.\n"}
{"snippet": "pipe.set_params(knn__n_neighbors=7)\npipe.fit(Xtrain, ytrain)\npipe.score(Xtest, ytest)\n", "intent": "Seven is the number of neighbors with the best CV score:\n"}
{"snippet": "cols_1 = ['bb', 'bpf', 'double', 'dp', 'e', 'h', 'ha', 'hra', 'ppf', 'ra', 'sb', 'sf', 'triple']\nX_kfold_1 = team_train[cols_1]\ny_kfold_1 = team_train.r.values.reshape(-1, 1)\nOLS_kfold = LinearRegression()\nk_fold(OLS_kfold, X_kfold_1, y_kfold_1, 5)\n", "intent": "All of this is just to verify that all this kfold CV stuff makes sense (and that my functions are somewhat correct):\n"}
{"snippet": "lassocv = LassoCV(cv=10)\nlassocv.fit(X_multvar, y_multvar.ravel())\nprint(lassocv.alpha_)\nprint(lassocv.score(X_multvar, y_multvar))\n", "intent": "$R^2$ is better but RMSE is not, something isn't right...\n"}
{"snippet": "tf.get_default_graph()\n", "intent": "Where is this graph? If you specify nothing, you are working on the \"default\" one.\n"}
{"snippet": "reset_graph()\nx = tf.Variable(3, name=\"x\")   \ny = tf.Variable(4, name=\"y\")\nf = x*x*y + y + 2\n", "intent": "Try one more (in the default Graph).\n"}
{"snippet": "reset_graph()\na = tf.constant(3)\nb = tf.constant(5)\ns = a + b\nwith tf.Session() as sess:\n    result = s.eval()\nprint(result)\n", "intent": ".. but actually inizialisation was unnecessary, e.g. if I do not do it, it still works:\n"}
{"snippet": "reset_graph()\nw = tf.constant(3)\nx = w + 2\ny = x + 5\nz = x * 3\nwith tf.Session() as sess:\n    print(y.eval())  \n    print(z.eval())  \n", "intent": "When you evaluate a node, TF automatically determines the set of nodes that it depends on and it evaluates these nodes first.\n"}
{"snippet": "import numpy as np\nfrom IPython.display import display, HTML\ndef strip_consts(graph_def, max_const_size=32):\n    graph_def = graph_def or tf.get_default_graph()\n    if hasattr(graph_def, 'as_graph_def'):\n        graph_def = graph_def.as_graph_def()\n    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n    display(HTML(iframe))\n", "intent": "Display your graph (not mandatory to understand all code in the next cell.. just run it.. NOTE it works only on Chrome..)\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), mean=0, stddev=0.1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "reset_graph()\nmy_third_graph = tf.get_default_graph()\nx = tf.Variable(3, name=\"x\")\ny = tf.Variable(4, name=\"y\")\nf = x*x*y + y + 2\ninit = tf.global_variables_initializer() \nwith tf.Session() as sess:\n    init.run()\n    result = f.eval()\nprint(result)\n", "intent": "And re-try the same, as a third graph, with global variable initialisation:\n"}
{"snippet": "history = model.fit(x=X_train, y=Y_train, batch_size=15, epochs=50,\n                   validation_data = (X_val, Y_val), shuffle=True)\n", "intent": "Fit the model. Modify the epochs/batch_size as needed. \n"}
{"snippet": "opt = keras.optimizers.SGD(lr=0.05,momentum=.9,nesterov=True,clipnorm=0.5)\nSegModel2.compile(loss=dice_coef,optimizer=opt)\n", "intent": "Now, everything else is just like the previous segmentation model. Let's try it out and see how it works!\n"}
{"snippet": "opt = keras.optimizers.Adam()\nRegModel.compile(loss=loss,optimizer=opt)\n", "intent": "Finally, add an optimizer and compile the model\n"}
{"snippet": "model = Model(inputs=img_input, outputs=x)\n", "intent": "We define our model, define the input(s) and output(s). \n"}
{"snippet": "feature_cols = ['TV', 'Radio', 'Newspaper']\nX = data[feature_cols]\ny = data.Sales\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X, y)\nlm.intercept_, lm.coef_\n", "intent": "Let's redo some of the Statsmodels code above in scikit-learn:\n"}
{"snippet": "feature_cols = ['TV', 'Radio', 'Newspaper', 'IsLarge']\nX = data[feature_cols]\ny = data.Sales\nlm = LinearRegression()\nlm.fit(X, y)\ndict(zip(feature_cols, lm.coef_))\n", "intent": "Let's redo the multiple linear regression and include the **IsLarge** predictor:\n"}
{"snippet": "X_train_2 = sm.add_constant(X_train) \nest1 = sm.OLS(Y_train, X_train_2)\nest12 = est1.fit()\nprint(est12.summary())\n", "intent": "Treinar o novo modelo\n"}
{"snippet": "kmeans.fit(df_user_elo7.values)\nlabels = kmeans.labels_\n", "intent": "Agora podemos encontrar os clusters.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_normal((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "kmeans.fit(X)\n", "intent": "Treine o modelo K-Means.\n"}
{"snippet": "kmeans = KMeans(n_clusters=5)\n", "intent": "Agora podemos iniciar o algoritmo de clustering.\n"}
{"snippet": "kmeans.fit(X)\n", "intent": "Treina o modelo K-Means.\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(column_name='')]\nclassifier = learn.DNNClassifier(n_classes=2, \n                                 hidden_units=[10, 20, 10], \n                                 feature_columns=feature_columns, \n                                 model_dir='./output')\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "kmeans.fit(data.drop(['Unnamed: 0', 'Private'], axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "kmeans = KMeans(n_clusters=4)\nkmeans.fit(data[0])\n", "intent": "Let's now plot the the result of applying KMeans (clusters=4) and original\n"}
{"snippet": "model = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "model.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "model = KNeighborsClassifier(n_neighbors=4)\n", "intent": "Looks like K = 4 could be a good option\n"}
{"snippet": "sess.run(tf.nn.softmax([1., 2.]))\n", "intent": "Btw, this is how I calculated the softmax values in the post:\n"}
{"snippet": "def calculate_mses(alphas, Model):\n    mses = []\n    for a in alphas:\n        model = ...\n        model.fit(...)\n        y_pred = ...\n        mses.append(...)\n    return mses\n", "intent": "Write a function that automates the process of calculating MSE for various alpha (and can be used on different models).\n"}
{"snippet": "lm_fit = first_lm.fit(xsample, ysample)\nprint('The intercept is',lm_fit.intercept_,'and the slope is', lm_fit.coef_[0])\n", "intent": "Then to fit the model to data we do the following:\n"}
{"snippet": "lm_fit = first_lm.fit(xsample,ysample)\nprint('The intercept is',lm_fit.intercept_,'and the slope is', lm_fit.coef_[0])\n", "intent": "Then to fit the model to data we do the following:\n"}
{"snippet": "predictors_all = df_all.loc[:,'WRF+DOMINO':'total_14000']\npredictors_all_const = sm.add_constant(predictors_all)\nest_all = sm.OLS(output, predictors_all_const)\nest_all_fit = est_all.fit()\nprint(est_all_fit.summary())\n", "intent": "Now let's try estimating a model with **all** the predictors embedded:\n"}
{"snippet": "predictors_less = df_all.loc[:,'WRF+DOMINO':'Resident_100']\npredictors_less_const = sm.add_constant(predictors_less)\nest_less = sm.OLS(output, predictors_less_const)\nest_less_fit = est_less.fit()\nprint(est_less_fit.summary())\n", "intent": "Now let's look at what happens if we drop some of the predictors\n"}
{"snippet": "X_all = df_all.loc[:,'WRF+DOMINO':'total_14000']\nX_all_const = sm.add_constant(X_all)\nest_all = sm.OLS(Y, X_all_const)\nresult_all = est_all.fit()\nresult_all.aic\n", "intent": "Now let's try estimating a model with **all** the predictors embedded:\n"}
{"snippet": "X_base = df_all[['WRF+DOMINO', 'Impervious_6000', 'Major_800', 'total_100', 'Major_100', 'Major_200', 'Elevation_truncated_km', 'Distance_to_coast_km', 'Population_800', 'total_800']]\nX_base_const = sm.add_constant(X_base)\nest_base = sm.OLS(Y, X_base_const)\nresults_base = est_base.fit()\nresults_base.aic\n", "intent": "And now a model that is close to (but not exactly the same as) Novotny's\n"}
{"snippet": "m = df_all.loc[:,'Population_800']\nto_add = pd.Series( np.where( m > 0, np.log(m), 0))\nX_base_poplog = X_base.assign(pop_log = to_add.values)\nX_base_poplog_const = sm.add_constant(X_base_poplog)\nest_base_poplog = sm.OLS(Y, X_base_poplog_const)\nresults_base_poplog = est_base_poplog.fit()\nresults_base_poplog.aic\n", "intent": "And now how about taking the log?\n"}
{"snippet": "X_base = df_all[['WRF+DOMINO', 'Impervious_6000', 'Major_800', 'total_100', 'Major_100', 'Major_200', 'Elevation_truncated_km', 'Distance_to_coast_km']]\nX_base_const = sm.add_constant(X_base)\nest_base = sm.OLS(Y, X_base_const)\nresults_base = est_base.fit()\nresults_base.aic\n", "intent": "And now a model that is close to (but not exactly the same as) Novotny's\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\ntrain_cols2 = data.columns[1:-1]\nprint train_cols2\nlm = LogisticRegression(C=1000000000)\ny = data[['admit']]\nx = data[train_cols2]\nlm.fit(x,y)\nprint lm.coef_\nprint lm.intercept_\n", "intent": "Answer: For a 1 point increase in GPA, the odds of being admitted increase by 118% (or, in other words, the odds are 2.18 x)\n"}
{"snippet": "neigh.fit(X, y) \n", "intent": "...and fit the data!\n"}
{"snippet": "a = tf.placeholder(dtype=tf.int32, shape=(None,))  \nb = tf.placeholder(dtype=tf.int32, shape=(None,))\nc = tf.add(a, b)\nwith tf.Session() as sess:\n  result = sess.run(c, feed_dict={\n      a: [3, 4, 5],\n      b: [-1, 2, 3]\n    })\n  print (result)\n", "intent": "<h2> Using a feed_dict </h2>\nSame graph, but without hardcoding inputs at build stage\n"}
{"snippet": "from sklearn.tree import DecisionTreeRegressor\nmodelDTR = MultiOutputRegressor(DecisionTreeRegressor(random_state=0)).fit(X,y)\n", "intent": "Second, let's train a DecisionTreeRegressor and see how it performes \n"}
{"snippet": "vk_session = vk.Session() \nvk_api = vk.API(vk_session)\n", "intent": "Starting new vk session in order to parse data\n"}
{"snippet": "scaler.fit(df.drop(\"Class\",axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "kmean = KMeans(n_clusters=2,n_jobs=-1)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "kmean.fit(df.drop(\"Private\",axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "knn = KNeighborsClassifier(n_jobs=-1,n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "lm.fit(X_train,Y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nclf_dt = DecisionTreeClassifier().fit(scaled_X_train,y_train)              \n", "intent": "Q10. Instantiate and train a DecisionTreeClassifier on the given data\n"}
{"snippet": "X = bikes.loc[:,['season_num', 'is_holiday', 'is_workingday', 'weather', 'temp_celsius',\n       'atemp', 'humidity_percent', 'windspeed_knots']]\ny = bikes.loc[:, 'num_total_users']\nlr_all.fit(X,y)\n", "intent": "- Train the model instance using our new feature matrix $X$ and the same target variable $y$.\n"}
{"snippet": "lr_temp_atemp = LinearRegression()\nX = bikes.loc[:,['temp_celsius', 'atemp']]\nlr_temp_atemp.fit(X,y)\n", "intent": "**Exercise.**\n- Create another `LinearRegression` instance that is fit using `temp_celsius` and `atemp_celsius`, and print the coefficients.\n"}
{"snippet": "lr_atemp = LinearRegression()\nX = bikes.loc[:,['atemp']]\nlr_atemp.fit(X,y)\n", "intent": "- Create another `LinearRegression` instance that is fit using `atemp_celsius` only, and print the coefficients.\n"}
{"snippet": "lr_all = LinearRegression()\n", "intent": "- Make a new instance of the LinearRegression class. Call it lr_all to distinguish it from our last model.\n"}
{"snippet": "lr_all.fit(X, y)\n", "intent": "- Train the model instance using our new feature matrix $X$ and the same target variable $y$.\n"}
{"snippet": "feature_cols = ['temp_celsius', 'atemp_celsius']\nX = bikes.loc[:, feature_cols]\ny = bikes.loc[:, 'num_total_users']\nlr_temp_atemp = LinearRegression()\nlr_temp_atemp.fit(X, y)\nprint(lr_temp_atemp.coef_)\n", "intent": "**Exercise.**\n- Create another `LinearRegression` instance that is fit using `temp_celsius` and `atemp_celsius`, and print the coefficients.\n"}
{"snippet": "feature_cols = ['atemp_celsius']\nX = bikes[feature_cols]\ny = bikes.num_total_users\nlr_atemp = LinearRegression()\nlr_atemp.fit(X, y)\nprint(lr_atemp.coef_)\n", "intent": "- Create another `LinearRegression` instance that is fit using `atemp_celsius` only, and print the coefficients.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=1, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "a_fit = cluster.KMeans(n_clusters=2).fit(a_data)\n", "intent": "For the [KMeans algorithm](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nclf_nb = GaussianNB().fit(scaled_X_train,y_train)                \n", "intent": "Q11. Instantiate and train a GaussianNB on the given data\n"}
{"snippet": "c_fit = cluster.KMeans(n_clusters=2).fit(c_data)\nplot_clusters(c_data, c_fit)\n", "intent": "The fit results look reasonable for (b), although the sharp dividing line between the two clusters looks artificial.\n"}
{"snippet": "fit = compare_sizes(cluster.KMeans(n_clusters=10, random_state=1).fit(df_iso))\n", "intent": "Look for 10 clusters instead of 3 (again using `random_state=1`). One cluster should be clearly different from the others.\n"}
{"snippet": "fit = compare_sizes(cluster.KMeans(n_clusters=10, random_state=123).fit(df_iso))\n", "intent": "Here is one example where the cluster of large sizes disappears:\n"}
{"snippet": "model_selection.cross_validate(\n    svm.LinearSVR(random_state=123),\n    df_iso_normed, e_scaled, cv=3, return_train_score=False)\n", "intent": "Cross validation works the same for classification and regression, but with different score functions:\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\nAdB = AdaBoostClassifier(n_estimators=100)\n", "intent": "Compare to AdaBoost\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators = 100, max_features=1)\n", "intent": "Limit the number of classifiers considered at each step of RF:\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators = 10, max_features=1,oob_score=True)\n", "intent": "Play around with different parameters for the RF\n"}
{"snippet": "lm = LinearRegression(n_jobs=4)\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,verbose=3)\ngrid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor().fit(X_train,y_train)\n", "intent": "Q10. Instantiate any Regressor, such as DecisionTreeRegressor, and train it on the training data.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=128)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n", "intent": "As before, here we'll train the network. Instead of flattening the images though, we can pass them in as 28x28x1 arrays.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_matrix = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -0.1, 0.1))\n    embed = tf.nn.embedding_lookup(embedding_matrix, input_data)\n    return embed\ntest_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "import statsmodels.formula.api as smf\nlm = smf.ols(formula='y ~ X', data=CreditData).fit()\nlm.summary()\n", "intent": "Second Step, find the p-values of your estimates. You have a few variables try to show your p-values along side the names of the variables.\n"}
{"snippet": "RF = RandomForestRegressor(n_estimators = 10000, \n                           max_features = 4,     \n                           min_samples_leaf = 10, \n                           oob_score = True,    \n                           random_state = 1)    \nRF.fit(X,y)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n"}
{"snippet": "lm_stats = smf.ols(formula='sleep ~ age + yrsmarr', data=sleep).fit()\nprint (lm_stats.summary())\n", "intent": "    What are the features with coefficients greater than 0\n---\n"}
{"snippet": "optimal_lasso = LassoCV(n_alphas=300, cv=10, verbose=1)\noptimal_lasso.fit(X_train, y_train)\nprint optimal_lasso.alpha_\n", "intent": "**Now implement a Lasso Regression**\n"}
{"snippet": "l1_ratios = np.linspace(0.01, 1.0, 50)\noptimal_enet = ElasticNetCV(l1_ratio=l1_ratios, n_alphas=300, cv=5, verbose=1)\noptimal_enet.fit(X_train, y_train)\nprint optimal_enet.alpha_\nprint optimal_enet.l1_ratio_\n", "intent": "**Now implement Elastic Net Regression**\n"}
{"snippet": "from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\nparams = {\"max_depth\": [3,5,10,20],\n          \"max_features\": [None, \"auto\"],\n          \"min_samples_leaf\": [1, 3, 5, 7, 10],\n          \"min_samples_split\": [2, 5, 7],\n           \"criterion\" : ['mse']\n         }\nfrom sklearn.grid_search import GridSearchCV\ndtr_gs = GridSearchCV(dtr, params, n_jobs=-1, cv=5, verbose=1)\n", "intent": "---\nInclude a gridsearch \n"}
{"snippet": "model.add(Dense(128, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.25))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.50))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(.25))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "tf.reset_default_graph()\ngraph = tf.get_default_graph()\nsess = tf.Session()\ntensor_a = tf.constant([[4,5,6],[1,3,5],[3,1,3]],shape=[3,3], dtype =tf.float32 ,name='tensor_b')\ntensor_b = tf.constant([[4.,3,5],[12,3,45],[63,41,3]],shape=[3,3], dtype =tf.float32 ,name='tensor_b')\ntensor_matrix_mul = tf.matmul(tensor_a,tensor_b)\n", "intent": "<img src=\"./diagram5g.png\" style=\"width:800px;\">\nmulti dimensional array/matrix\n"}
{"snippet": "tf.reset_default_graph()\ngraph = tf.get_default_graph()\nsess = tf.Session()\ntensor_a = tf.constant([[4,5,6],[1,3,5],[3,1,3]],shape=[3,3], dtype =tf.float32 ,name='tensor_b')\ntensor_b = tf.constant([[4.,3,5],[12,3,45],[63,41,3]],shape=[3,3], dtype =tf.float32 ,name='tensor_b')\ntensor_matrix_mul = tf.matmul(tensor_a,tensor_b)\n", "intent": "<img src=\"./diagram5g.png\" style=\"width:800px;\">\nmultidimensional array/matrix\n"}
{"snippet": "tf.reset_default_graph()\nsess = tf.Session()\ngraph = tf.get_default_graph()\ntensor_a = tf.random_normal(shape=[3,3], mean=0.0,stddev=1.0, dtype=tf.float32,name='tensor_a')\ntensor_b = tf.constant([[4.,3,5],[12,3,45],[.63,.41,3]],shape=[3,3], dtype =tf.float32 ,name='tensor_b')\ntensor_matrix_mul = tf.matmul(tensor_a,tensor_b)\n", "intent": "<img src=\"./diagram5c.png\" style=\"width:800px;\">\ninput_a = tf.constant(4.,shape=[3,2], dtype =tf.float32 ,name='a')\n"}
{"snippet": "def discriminator(x):\n    Dis_h1 = tf.nn.relu(tf.matmul(x, Dis_W1) + Dis_b1)\n    Dis_logits = tf.matmul(Dis_h1, Dis_W2) + Dis_b2\n    Dis_prob = tf.nn.sigmoid(Dis_logits)\n    return Dis_prob, Dis_logits\n", "intent": "This returns a prob result and the logits level that was used to derive that probability\n"}
{"snippet": "def conv_block(x, filters, size, stride=(2,2), mode='same', act=True):\n    x = Convolution2D(filters, size, size, subsample=stride, border_mode=mode)(x)\n    x = BatchNormalization(mode=2)(x)\n    return Activation('relu')(x) if act else x\ndef res_block(ip, nf=64):\n    x = conv_block(ip, nf, 3, (1,1))\n    x = conv_block(x, nf, 3, (1,1), act=False)\n    return merge([x, ip], mode='sum')\n", "intent": "ConvBlock  \nResBlock\n"}
{"snippet": "def deconv_block(x, filters, size, shape, stride=(2,2)):\n    x = Deconvolution2D(filters, size, size, subsample=stride, \n        border_mode='same', output_shape=(None,)+shape)(x)\n    x = BatchNormalization(mode=2)(x)\n    return Activation('relu')(x)\ndef up_block(x, filters, size):\n    x = keras.layers.UpSampling2D()(x)\n    x = Convolution2D(filters, size, size, border_mode='same')(x)\n    x = BatchNormalization(mode=2)(x)\n    return Activation('relu')(x)\n", "intent": "Deconvolution / Transposed Conv / Fractionally Strident Convs\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(8, activation='sigmoid', input_dim=3000))\nmodel.add(Dropout(.3))\nmodel.add(Dense(8, activation='sigmoid'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=100)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "kmeans.fit(Universities.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "print('\\nTraining...')\nmodel.fit(x_train, y_train, epochs=1000, batch_size=50, verbose=0)\nprint('...Done!')\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "knn.fit(X_train,Y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "Now its time to train our model on our training data!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embedded_input = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "X = df[['disp', 'wt']]\nk=3\nkmeans = KMeans(n_clusters=k)\nkmeans.fit(X)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nbagg2 = BaggingClassifier(logreg)\n", "intent": "**Logistic Regression and Bagging**\n"}
{"snippet": "k = 5\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(dn1)\n", "intent": "Cluster the Data to our our target groups. **BONUS**: Perform multiple clustering tests on various indicator variables\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(feature_train, target_train)\n", "intent": "Define a logistic regression and train it with the feature and target set\n"}
{"snippet": "kmeans = cluster.KMeans(n_clusters=3)\nkmeans.fit(data)\n", "intent": "Run the clustering analysis using scikit-learn.\n*Hint*: Estimate the number of clusters, k, based on your visual examination of the distributions.\n"}
{"snippet": "import sklearn.linear_model\nols = sklearn.linear_model.LinearRegression()\ncolumns = ['dwelling_type_Condo', 'dwelling_type_Multi-Family', 'dwelling_type_Residential', 'dwelling_type_Unkown']\nols.fit(sacramento_with_dummies[columns], sacramento.price)\nzip(columns, ols.coef_)\n", "intent": "If we attempt a linear regressionusing all 4 dummy\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1., 1.))\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "dbscan = DBSCAN(eps=0.5, min_samples=5).fit(X_standard)\n", "intent": "Let's set up the DBSCAN from scikit using an epsilon of .5 and a minimum sample number of 5\n"}
{"snippet": "kmeans = KMeans(n_clusters=2)\nkmeans.fit(new_X)\n", "intent": "Set up the k-means clustering analysis. Use the graph from above to derive \"k\"\n"}
{"snippet": "logreg_params = {\n                 'penalty':['l2'],\n                 'C':np.logspace(-5,1,50),\n                 'solver':['liblinear', 'newton-cg', 'lbfgs', 'sag']\n                 }\ngrid = GridSearchCV(logreg, logreg_params, cv=5)\ngrid.fit(X, y)\nprint grid.best_params_\nprint grid.best_score_\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "knc = KNeighborsClassifier()\nknc_params = {'n_neighbors': range(3, 10),\n              'weights': ['uniform', 'distance']}\ngrid = GridSearchCV(knc, knc_params, cv=5)\ngrid.fit(X, y)\nprint grid.best_params_\nprint grid.best_score_\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "grid2 = GridSearchCV(logreg, logreg_params, cv=5, scoring='average_precision')\ngrid2.fit(X, y)\nprint grid2.best_params_\nprint grid2.best_score_\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "import numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nK = 4\nkmeans = KMeans(n_clusters=K).fit(reduced_data)\n", "intent": "There is no clear elbow. ploted values till k = 50. \nTried various options for algo=, and init=\nIts always a near smooth curve. Have taken K=4\n"}
{"snippet": "regr = linear_model.LinearRegression(fit_intercept=True)\nregr.fit(x.reshape((len(x), 1)), y)   \nprint 'Actual: m = 3, b = -2'\nprint 'Fit: m = %.2f, b = %.2f' % (regr.coef_[0], regr.intercept_)\n", "intent": "Of course, `scikit-learn` has linear regression built in, with a simple interface.\n"}
{"snippet": "from pyspark.mllib.regression import LabeledPoint\nlp1 = LabeledPoint(0, dfv)\nlp2 = LabeledPoint(1, sfv)\nlp3 = LabeledPoint(2, Vectors.dense([4.5, 1.0]))\nprint(lp1)\nprint(lp2)\nprint(lp3)\n", "intent": "Then there are labeled feature vectors, represented by the type **`LabeledPoint`**:\n"}
{"snippet": "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat=\"wordcount\")\nspam_RF = RandomForestClassifier(n_estimators=200, criterion='entropy')\nspam_RF.fit(Xtrain,ytrain);\n", "intent": "Let's keep this last classifier and identify which are the misclassified emails.\n"}
{"snippet": "dtc=DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "Xtrain = X[:1000,:]\nytrain = y[:1000]\nXtest = X[1000:,:]\nytest = y[1000:]\ndigits_svc = svm.SVC(kernel='rbf', gamma=1e-3)\ndigits_svc.fit(Xtrain,ytrain);\n", "intent": "Let's identify which are the misclassified images (and find the confusion matrix by the way).\n"}
{"snippet": "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\nspam_nbc = MultinomialNB()\nspam_nbc.fit(Xtrain,ytrain);\n", "intent": "Let's identify which are the misclassified emails (and find the confusion matrix by the way).\n"}
{"snippet": "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\nspam_GP = GaussianProcessClassifier()\nspam_GP.fit(Xtrain.toarray(),ytrain)\n", "intent": "Let's keep this last classifier and identify which are the misclassified emails.\n"}
{"snippet": "from keras.models import Sequential\nmodel = Sequential()\n", "intent": "Let's declare a [feed-forward neural network](https://keras.io/getting-started/sequential-model-guide/).\n"}
{"snippet": "from keras.layers import Activation\nmodel.add(Activation('softmax'))\n", "intent": "Let's indicate that the [activation function](https://keras.io/activations/) for this output layer is a softmax.\n"}
{"snippet": "from keras.utils import plot_model\nplot_model(model, to_file='model.png')\n", "intent": "Let's save a picture of our model.\n"}
{"snippet": "batch_size = 100\ntransform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) ])\ntrain_dataset = dsets.MNIST(root='./data/', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\ntrain_iterator = iter(train_loader)\n", "intent": "As suggested above, we will practise on Mnist database. Basically, we will learn to our computer how to write figures.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Start a TF session (we could actually do that after defining the graph).\n"}
{"snippet": "x = tf.placeholder(tf.float32, shape=[None, 1])\ny_true = tf.placeholder(tf.float32, shape=[None, 1])\nhidden_layer1 = tf.layers.dense(x, units=20, activation=tf.nn.relu)\nhidden_layer2 = tf.layers.dense(hidden_layer1, units=20, activation=tf.nn.relu)\ny_pred = tf.layers.dense(hidden_layer2, units=1)\n", "intent": "Define the neural network. 2 hidden layers with 20 units each, ReLU activation functions.\n"}
{"snippet": "kms=KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\nupdate_weights = optimizer.apply_gradients(zip((grad_W, grad_b),(W,b)))\n", "intent": "We still want to use Tensorflow's `GradientDescentOptimizer` to perform the update operation.\n"}
{"snippet": "iris_dt2 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth=2)\niris_dt2.fit(iris.data, iris.target)\ndisp_iris_tree('iris_dt2',iris_dt2)\n", "intent": "Let's try to limit the depth of the tree to preserve the generalization error.\n"}
{"snippet": "from sklearn.ensemble import AdaBoostClassifier\nboosted_forest = AdaBoostClassifier(tree.DecisionTreeClassifier(criterion='entropy',max_depth=3), n_estimators=100)\nboosted_forest.fit(X,y)\nplot_decision_boundary(boosted_forest,X,y)\nprint(\"Training score:\", boosted_forest.score(X,y))\nprint(\"Testing score: \", boosted_forest.score(Xtest,ytest))\n", "intent": "Scikit-learn provides an [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n"}
{"snippet": "hold_prob = tf.placeholder(tf.float32)\nfull_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "clfdt=DecisionTreeClassifier()\nclfdt, Xtrain, ytrain, Xtest, ytest  = do_classify(clfdt, {\"max_depth\": range(1,10,1)}, dfchurn, colswewant_cont+colswewant_cat, 'Churn?', \"True.\", reuse_split=reuse_split)\n", "intent": "We train a simple decision tree classifier.\n"}
{"snippet": "model = LinearRegression(normalize=True)\nprint model.normalize\n", "intent": "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated:\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB(alpha=0.1)\nclf\n", "intent": "We can now train a classifier, for instance a Multinomial Naive Bayesian classifier which is a fast baseline for text classification tasks:\n"}
{"snippet": "pipeline.fit(twenty_train_subset.data, twenty_train_subset.target)\nprint(\"Train score:\")\nprint(pipeline.score(twenty_train_subset.data, twenty_train_subset.target))\nprint(\"Test score:\")\nprint(pipeline.score(twenty_test_subset.data, twenty_test_subset.target))\n", "intent": "Such a pipeline can then be used to evaluate the performance on the test set:\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\ngrid = GridSearchCV(estimator=pipeline, param_grid=dict(svc__C=[1e-2, 1, 1e2]))\ngrid.fit(X, y)\nprint grid.best_estimator_.named_steps['svc']\n", "intent": "Now we can proceed to run the grid search:\n"}
{"snippet": "kms.fit(df.drop(\"Private\",axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "modelW0 = sm.OLS(y, X)\nresultsW0 = modelW0.fit()\nprint(resultsW0.summary())\n", "intent": "Now let's fit a linear regression model with an intercept. \n"}
{"snippet": "import pymc3 as pm\nwith pm.Model() as model:\n    parameter = pm.Exponential(\"poisson_param\", 1.0)\n    data_generator = pm.Poisson(\"data_generator\", parameter)\n", "intent": "In PyMC3, we typically handle all the variables we want in our model within the context of the `Model` object.\n"}
{"snippet": "from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nreg_DT = DecisionTreeRegressor(random_state=42)\nreg_LR = LinearRegression()\nreg_DT.fit(X_train, y_train)\nreg_LR.fit(X_train, y_train)\n", "intent": "** Ejercicio 7: Importa de la libreria sklearn los modelos elegidos anteriormente y entrenalos**\n"}
{"snippet": "cifar_model = Sequential()\n", "intent": "**(b)** Initialize a Sequential model\n"}
{"snippet": "cifar_model.add(MaxPooling2D(2,2))\n", "intent": "**(f)** Add another ``Conv2D`` layer identical to the others except with 64 filters instead of 32. Add another ``relu`` activation layer.\n"}
{"snippet": "clf = DecisionTreeClassifier(criterion = \"entropy\", \n                                  random_state = 0,\n                                  max_depth=10, \n                                  min_samples_leaf=1\n                                 )\nclf.fit(train_xs, train_ys)\nprint(clf.score(train_xs, train_ys))\nprint(clf.score(validation_xs,validation_ys))\n", "intent": "Why is a single decision tree so prone to overfitting?\n"}
{"snippet": "clf = RandomForestClassifier(criterion='entropy',max_depth=10,min_samples_leaf=1,random_state=0)\nrandom_forest = clf.fit(train_xs, train_ys)\nprint(random_forest.score(train_xs,train_ys))\nprint(random_forest.score(validation_xs,validation_ys))\n", "intent": "How does a random forest classifier prevent overfitting better than a single decision tree?\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(Xtrain, ytrain)\n", "intent": "Now let's choose a model. For this we're going ot choose a Gaussian naive Bayes model with model.fit()\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(784,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])\n", "intent": "Building a network in Keras is incredibly straight forward. Just instantiate a `Sequential` object and then use `.add` to add layers to you network.\n"}
{"snippet": "ss.fit(df.drop(\"TARGET CLASS\",axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "model2 = nn.Sequential()\nmodel2.add(nn.Linear(784, 50), 'Linear1')\nmodel2.add(nn.Sigmoid(50, 50))\nmodel2.add(nn.Linear(50, 10), 'Linear2')\nmodel2.add(nn.CrossEntropyCriterion())\nevaluation_results = main(model2, X_train, y_train, X_test, y_test, X_val, y_val, \n                          learning_rate=1e-3, learning_rate_evolution=None, \n                          n_epoch=10, layers_to_check_gradients = ['Linear1', 'Linear2'])\n", "intent": "1. Linear (784, 50)\n2. Sigmoid (50, 50)\n3. Linear (50, 10)\n4. Cross-Entropy ( )\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\n    return embedding\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('.'))\n    test_accuracy, gotten_wrong_list = evaluate_on_test_data(X_test_norm, y_test)\n    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n", "intent": "* We Run the test Images through convnet and also retrive indexes of images **gotten wrong** by the network.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('.'))\n    predictions = predict_on_custom_data(X_test_custom)\n    print(predictions)\n", "intent": "* Run the images through hte Conv Net Classifier and get predictions\n"}
{"snippet": "top_predictions = tf.nn.top_k(tf.nn.softmax(logits), k=3)\ndef top_predict_on_custom_data(X_data):\n    sess = tf.get_default_session()\n    pred = sess.run(top_predictions, feed_dict={x:X_data, train:-1})\n    return pred\n", "intent": "* Function for getting **top 3 predictions** and thier **softmax** probabilities\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('.'))\n    predictions = top_predict_on_custom_data(X_test_custom)\n", "intent": "* Get **Top 3** predictions for all the images\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(222, activation='relu', input_dim=x_train.shape[1]))\nmodel.add(Dropout(.4))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\nkeras.optimizers.Adagrad(lr=0.1, epsilon=1e-08, decay=0.01)\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=12, batch_size=32, verbose=1, shuffle=True)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "scaler.fit(bank.drop(['Class'], axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "knn=KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "kmeans.fit(X = college.drop(['Private'], axis = 1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS', axis = 1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors = 1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "mnb.fit(msg_train, label_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, verbose = 1)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(64, activation='linear', input_dim=x_train.shape[1]))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=50, verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from keras.layers.normalization import BatchNormalization\nmodel_batch = Sequential()\nmodel_batch.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_batch.add(BatchNormalization())\nmodel_batch.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_batch.add(BatchNormalization())\nmodel_batch.add(Dense(output_dim, activation='softmax'))\nmodel_batch.summary()\n", "intent": "<h2> MLP + Batch-Norm on hidden Layers + AdamOptimizer </2>\n"}
{"snippet": "from keras.layers import Dropout\nmodel_drop = Sequential()\nmodel_drop.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\nmodel_drop.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\nmodel_drop.add(Dense(output_dim, activation='softmax'))\nmodel_drop.summary()\n", "intent": "<h2> 5. MLP + Dropout + AdamOptimizer </h2>\n"}
{"snippet": "lr.fit(X_train,y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "model_sigmoid = Sequential()\nmodel_sigmoid.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))\nmodel_sigmoid.add(Dense(128, activation='sigmoid'))\nmodel_sigmoid.add(Dense(output_dim, activation='softmax'))\nmodel_sigmoid.summary()\nmodel_sigmoid.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_sigmoid.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n", "intent": "<h2>MLP + Sigmoid activation + ADAM </h2>\n"}
{"snippet": "model_relu = Sequential()\nmodel_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu.add(Dense(output_dim, activation='softmax'))\nmodel_relu.summary()\n", "intent": "<h2> MLP + ReLU +SGD </h2>\n"}
{"snippet": "model_relu = Sequential()\nmodel_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu.add(Dense(output_dim, activation='softmax'))\nprint(model_relu.summary())\nmodel_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n", "intent": "<h2> MLP + ReLU + ADAM </h2>\n"}
{"snippet": "from keras.optimizers import Adam,RMSprop,SGD\ndef best_hyperparameters(activ):\n    model = Sequential()\n    model.add(Dense(512, activation=activ, input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\n    model.add(Dense(128, activation=activ, kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\n    model.add(Dense(output_dim, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n    return model\n", "intent": "<h2> Hyper-parameter tuning of Keras models using Sklearn </h2>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>1D ConvNet on Sentiment Analysis</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Encoder-Decoder with Attention</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>FastText with Gensim</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>GloVe-Yelp-Comments-Classification</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>LSTM and GRU on Sentiment Analysis</H1></u></center>\n"}
{"snippet": "grid=GridSearchCV(SVC(),param_grid,verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Word Embeddings on Sentiment Analysis</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Word2Vec-CBOW-Keras</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Word2Vec-Skipgram-Keras</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Developing a Chatbot</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Emotion Recognition with LSTM and Attention</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Neural Machine Translation-Optimization</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Neural Machine Translation with Seq2Seq</H1></u></center>\n"}
{"snippet": "import tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = True\nsess = tf.Session(config=config)\nset_session(sess)\n", "intent": "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n<br><br>\n<center><u><H1>Text Generation</H1></u></center>\n"}
{"snippet": "from keras.models import load_model\nclassifier = load_model('/home/deeplearningcv/DeepLearningCV/Trained Models/simpsons_little_vgg.h5')\n", "intent": "If we just trained our classifer, we an use model instead.\n"}
{"snippet": "x = tf.placeholder(tf.float32)\ntrue_label = tf.placeholder(tf.float32)\nW = tf.Variable([.3], tf.float32)\nb = tf.Variable([-.3], tf.float32)\nlinear_model = W * x + b \nsquared_diff = tf.square(linear_model - true_label)\nloss = tf.reduce_sum(squared_diff)\n", "intent": " - cross-entropy loss, l2 normalization loss, hinge loss, etc\n"}
{"snippet": "clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y, cv_x_responseCoding, cv_y, clf)\n", "intent": "<h3>4.2.2. Testing the model with best hyper paramters</h3>\n"}
{"snippet": "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n", "intent": "<h4>4.3.1.2. Testing the model with best hyper paramters</h4>\n"}
{"snippet": "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n", "intent": "<h4>4.3.2.2. Testing model with best hyper parameters</h4>\n"}
{"snippet": "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42,class_weight='balanced')\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)\n", "intent": "<h3>4.4.2. Testing model with best hyper parameters</h3>\n"}
{"snippet": "clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)\n", "intent": "<h3>4.5.2. Testing model with best hyper parameters (One Hot Encoding)</h3>\n"}
{"snippet": "clf = RandomForestClassifier(max_depth=max_depth[int(best_alpha%4)], n_estimators=alpha[int(best_alpha/4)], criterion='gini', max_features='auto',random_state=42)\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y,cv_x_responseCoding,cv_y, clf)\n", "intent": "<h3>4.5.4. Testing model with best hyper parameters (Response Coding)</h3>\n"}
{"snippet": "X = uscensus[\"population\"].as_matrix()[1:-1].reshape((-1,1))\ny = uscensus[\"relgrowth\"].as_matrix()[1:-1]\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression().fit(X,y)\nm = lr.coef_ [0]\nk_val  = lr.intercept_\nn_val  = -k_val/m\nprint \"N\", n_val\nprint \"k\", k_val\n", "intent": "use a linear regression to fit a line to the data and get $k$, and $N$ which are the intercept points with each axis. recall from above that $m=-k/N$\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    w = tf.get_variable('embedding_weight', shape= [vocab_size, embed_dim])\n    return tf.nn.embedding_lookup(w, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.cluster import DBSCAN\ndbscn = DBSCAN(eps = 3, min_samples = 3)\ndbscn.fit(Xs)\n", "intent": "Remember to pass an `eps` and `min_samples` of your choice.\n"}
{"snippet": "node1 = tf.placeholder(tf.float32)\nnode2 = \nwith tf.Session() as sess:\n    inp = np.random.randn(1,10)\n    print(\"softmax using tensorflow: \\n\", sess.run(node2, {node1:inp}))\n    print(\"softmax using numpy: \\n\", np.exp(inp) / np.sum(np.exp(inp), axis=1))\n", "intent": "  - softmax$(x_i)=\\frac{\\exp^{x_i}}{\\sum_{j=1}^{K}\\exp^{x_i}}$\n"}
{"snippet": "pipe = Pipeline([\n    ('fu', fu),\n    ('lr', LogisticRegression())\n])\n", "intent": "Create a pipeline with two components:\n1. The `FeatureUnion` you set up in the previous step\n2. The `LogisticRegression` class from `sklearn`\n"}
{"snippet": "pipe = Pipeline([\n    ('fu', fu),\n    ('lr', LogisticRegression())\n])\n", "intent": "Create a pipeline with two components:\n1. The `FeatureUnion` you set up in the previous step\n2. The `LogisticRegression` from `sklearn`\n"}
{"snippet": "ridge = Ridge(alpha=optimal_ridge_alpha)\nridge.fit(x_train_ss, y_train)\nridge_train_score = ridge.score(x_train_ss, y_train)\nridge_test_score = ridge.score(x_test_ss, y_test)\nprint ('Train Score with Ridge: ', ridge_train_score)\nprint ('Test Score with Ridge: ', ridge_test_score)\n", "intent": "---\nIs it better than the Linear regression? If so, why might this be?\n"}
{"snippet": "lasso = Lasso(alpha = optimal_lasso_alpha)\nlasso_fit = lasso.fit(x_train_ss, y_train)\nlasso_train_score = lasso.score(x_train_ss, y_train)\nlasso_test_score = lasso.score(x_test_ss, y_test)\nprint ('Train  Score with Lasso: ', lasso_train_score)\nprint ('Test Score with Lasso: ', lasso_test_score)\n", "intent": "---\nIs it better than the Linear regression? Is it better than Ridge? What do the differences in results imply about the issues with the dataset?\n"}
{"snippet": "elastic_net = ElasticNet(alpha=optimal_en_alpha, l1_ratio=optimal_en_l1_ratio)\nelastic_net.fit(x_train_ss, y_train)\nelastic_train_score = elastic_net.score(x_train_ss, y_train)\nelastic_test_score = elastic_net.score(x_test_ss, y_test)\n", "intent": "---\nHow does it compare to the Ridge and Lasso regularized regressions?\n"}
{"snippet": "knn5 = KNeighborsClassifier()\nskf_5_scores = stratified_cross_val(X_stdized.values, y.values.ravel(), np_cv_indices, knn5)\n", "intent": "<a id='cv-knn5'></a>\n"}
{"snippet": "y = admit.admit.values\nX = admit[['gpa']].values\nlinmod = LinearRegression()\nlinmod.fit(X, y)\nprint 'Intercept:', linmod.intercept_\nprint 'Coef(s):', linmod.coef_\n", "intent": "<a id='pred-admit'></a>\n---\nLet's try predicting the `admit` binary indicator using just `gpa` with a Linear Regression to see what goes wrong.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nimport patsy\ny, X = patsy.dmatrices('weight_lb ~ height_in -1', data=baseball, return_type='dataframe')\ny = y.values.ravel()\nlinreg = LinearRegression()\nlinreg.fit(X, y)\nprint 'Intercept:', linreg.intercept_\nprint 'Height coef:', linreg.coef_\n", "intent": "**Construct a linear regression predicting weight from height. Interpret the value of the intercept and the coefficient from this model.**\n"}
{"snippet": "baseball['height_ctr'] = baseball.height_in - baseball.height_in.mean()\ny, X = patsy.dmatrices('weight_lb ~ height_ctr -1', data=baseball, return_type='dataframe')\ny = y.values.ravel()\nlinreg = LinearRegression()\nlinreg.fit(X, y)\nprint 'Intercept:', linreg.intercept_\nprint 'Height centered coef:', linreg.coef_\n", "intent": "**Center the height variable and re-run the regression with the centered height. Interpret the new intercept and coefficient.**\n"}
{"snippet": "x = tf.placeholder(tf.float32)\ntrue_label = tf.placeholder(tf.float32) \nW = tf.Variable([.3], tf.float32)\nb = tf.Variable([-.3], tf.float32)\nlinear_model = W * x + b \nl1_loss = \n", "intent": "  - $y_i$: ground-truth label\n  - $f(x_i)$: predicted label\n  - $L = \\sum_i{|y_i - f(x_i)|}$\n"}
{"snippet": "X = temp_df[['RM','LSTAT']]\ny = temp_df[['MEDV']]\nmlr_model = lm.fit(X,y)\n", "intent": "**Print out the coefficients from this MLR model and interpret them.**\n"}
{"snippet": "studB_days = students['B']['days']\nstudB_mor = students['B']['morale']\nBmod = LinearRegression()\nBmod.fit(studB_days[:, np.newaxis], studB_mor)\n", "intent": "**Now we fit a new model to student B's data:**\n"}
{"snippet": "logreg_cv = LogisticRegressionCV(Cs=100, cv=5, penalty='l1', scoring='accuracy', solver='liblinear')\nlogreg_cv.fit(X_train, y_train)\n", "intent": "**Gridsearch hyperparameters for the training data.**\n"}
{"snippet": "logreg_1 = LogisticRegression(C=best_C['non-crime'], penalty='l1', solver='liblinear', multi_class = 'ovr')\nlogreg_2 = LogisticRegression(C=best_C['non-violent'], penalty='l1', solver='liblinear', multi_class = 'ovr')\nlogreg_3 = LogisticRegression(C=best_C['violent'], penalty='l1', solver='liblinear', multi_class = 'ovr')\nlogreg_1.fit(X_train, y_train)\n", "intent": "**Build three logisitic regression models using the best parameters for each target class.**\n"}
{"snippet": "clf = svm.SVC(kernel='rbf')\ngamma_range = np.logspace(-5, 2, 10)\nC_range = np.logspace(-3, 2, 10)\nparam_grid = dict(gamma=gamma_range, C=C_range)\ngaus_grid = GridSearchCV(clf, param_grid, cv=10, scoring='accuracy', verbose=1)\ngaus_grid.fit(iris_X, iris_y)\n", "intent": "- Gaussian\n- Linear\n- Poly of degree 3\n"}
{"snippet": "k_wine = KMeans(n_clusters=2)\nk_wine.fit(Xs)\n", "intent": "**Fit a KMeans model with K=2 and extract the predicted labels.**\n"}
{"snippet": "from sklearn.ensemble import BaggingClassifier\nclf = DecisionTreeClassifier(max_depth=15)\nbagger = BaggingClassifier(clf, max_samples=1.0)\nvisualize_tree(bagger, X, y, boundaries=False);\n", "intent": "<a id=\"lets-see-it-now-with-our-tree-example\"></a>\n"}
{"snippet": "quote_fit = quote_clf.fit(X_train, y_train)\n", "intent": "Let's try that again, shall we? We call the pipeline just as we would its component parts.\n"}
{"snippet": "feature_set = data.iloc[:, :-1]\ntarget = data.iloc[:, -1]\nclassifier1 = naive_bayes.MultinomialNB().fit(feature_set, target)\n", "intent": "> Check: what do you think is going on with this dataset?\n> Which sklearn NB implementation should we use?\n"}
{"snippet": "node1 = tf.placeholder(tf.float32)\nnode2 = tf.div(tf.exp(node1), tf.reduce_sum(tf.exp(node1)))\nwith tf.Session() as sess:\n    inp = np.random.randn(1,10)\n    print \"softmax using tensorflow: \\n\", sess.run(node2, {node1:inp})\n    print \"softmax using numpy: \\n\", np.exp(inp) / np.sum(np.exp(inp), axis=1)\n", "intent": "  - softmax$(x_i)=\\frac{\\exp^{x_i}}{\\sum_{j=1}^{K}\\exp^{x_i}}$\n"}
{"snippet": "W = tf.Variable(tf.zeros([13, 1]))\nb = tf.Variable(tf.zeros([1]))\n", "intent": "<a id=\"create-weights-to-use-in-our-model\"></a>\n"}
{"snippet": "def multilayer_perceptron(x, weights, biases):\n    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n    return tf.matmul(layer_2, weights['out']) + biases['out']\n", "intent": "**The ReLu function**\n"}
{"snippet": "dbscan = DBSCAN(eps=1.0, n_jobs=-1)\n", "intent": "This is a bad cluster, we are going to try and find another technique\n"}
{"snippet": "from sklearn.linear_model import LogisticRegressionCV\nlogreg = LogisticRegressionCV()\n", "intent": "We now have a train and test set \n"}
{"snippet": "knn5 = KNeighborsClassifier(n_neighbors=5,\n                            weights='uniform')\nscores = accuracy_crossvalidator(Xs, y, knn5, \n                                 cv_indices)\n", "intent": "<a id='cv-knn5'></a>\n"}
{"snippet": "y = admit.admit.values\nX = admit[['gpa']].values\nlinmod = LinearRegression()\nlinmod.fit(X, y)\nprint('Intercept:', linmod.intercept_)\nprint('Coef(s):', linmod.coef_)\n", "intent": "<a id='pred-admit'></a>\n---\nLet's try predicting the `admit` binary indicator using just `gpa` with a Linear Regression to see what goes wrong.\n"}
{"snippet": "lasso = Lasso()\nlasso.fit(X, y)\n", "intent": "Next, we will fit a `Lasso` model predicting `y` with all of the features in `X`:\n"}
{"snippet": "rfe.fit(X, y)\n", "intent": "Next, we fit `rfe` to our `X` and `y` datasets:\n"}
{"snippet": "selectkbest.fit(X, y)\n", "intent": "Now, fit this to the `X` and `y` objects we created before:\n"}
{"snippet": "batch_size = 32\nmodel.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 2)\n", "intent": "- Now we will train the network! Since we don't have too much time, we will train for 5 epochs. Feel free to train for longer :)\n"}
{"snippet": "ag = AgglomerativeClustering(n_clusters=2)\nag.fit(X)\npredicted_labels = ag.labels_\npredicted_labels\n", "intent": "Next we'll ask `AgglomerativeClustering` to return back two clusters for Iris:\n"}
{"snippet": "model1 = Pipeline(modeling_steps)\nmodel1\n", "intent": "Next, we'll instantiate a Pipeline object, passing in the steps:\n"}
{"snippet": "model1.fit(X_train, y_train)\n", "intent": "Finally, we'll fit it to the data we have!\n"}
{"snippet": "petal_width_pipe = make_pipeline(\n    FeatureExtractor('petal_length'),\n    PolynomialFeatures(2, include_bias=False)\n)\n", "intent": "And a second for the `petal_width` feature:\n"}
{"snippet": "fu = make_union(\n    petal_length_pipe,\n    petal_width_pipe\n)\nfu.fit(iris)\nfu.transform(iris)[0:5, :]\n", "intent": "Just like `Pipeline`, `FeatureUnion` also has a function that removes some of the boilerplate code (`make_union()`):\n"}
{"snippet": "petal_width_pipe = make_pipeline(\n    FeatureExtractor('petal_width'),\n    PolynomialFeatures(2, include_bias=False)\n)\n", "intent": "And a second for the `petal_width` feature:\n"}
{"snippet": "random_state = 1\nclf = DecisionTreeClassifier(max_depth=20)\nrng = np.random.RandomState(random_state)\ni = np.arange(len(y))\nrng.shuffle(i)\nvisualize_tree(clf, X[i[:250]], y[i[:250]], boundaries=False,\n               xlim=(X[:, 0].min(), X[:, 0].max()),\n               ylim=(X[:, 1].min(), X[:, 1].max()))\n", "intent": "<a id=\"lets-see-what-the-boundaries-look-like-for-different-seeds\"></a>\n"}
{"snippet": "from sklearn.ensemble import BaggingClassifier\nclf = DecisionTreeClassifier(max_depth=15)\nbagger = BaggingClassifier(clf, max_samples=1.0, n_estimators=20)\nvisualize_tree(bagger, X, y, boundaries=False);\n", "intent": "<a id=\"lets-see-it-now-with-our-tree-example\"></a>\n"}
{"snippet": "lr = LinearRegression()\nlr.fit(X_train, y_train)\nprint(lr.score(X_train, y_train))\n", "intent": "Fit a linear regression to the **training** set (`X_train` and `y_train`) and look at the $R^2$ score **on the training set**\n"}
{"snippet": "start = time()\nnnGrid = GridSearchCV(neighbors.KNeighborsClassifier(), tuned_parameters,cv=5,n_jobs=3)\nnnGrid.fit(features34, activity34)\nprint(str(time() - start)+ \" sec\")\n", "intent": "Nous utilisant maintenant 3 coeurs (il y a en 4 sur mon mac) et nous comparons les temps de calcul :\n"}
{"snippet": "toyregr_skl = linear_model.LinearRegression()\nresults_skl = toyregr_skl.fit(x_train,y_train)\nbeta0_skl = results_skl.intercept_\nbeta1_skl = results_skl.coef_[0]\nprint(\"(beta0, beta1) = (%f, %f)\" %(beta0_skl, beta1_skl))\n", "intent": "Below is the code for sklearn.\n"}
{"snippet": "lol = [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0]\nfitmodel = cv_optimize_ridge(Xtrain, ytrain, lol, n_folds=4)\n", "intent": "> **EXERCISE:** Use the function above to fit the model on the training set with 4-fold cross validation.  Save the fit as the variable `fitmodel`.\n"}
{"snippet": "clf = LogisticRegression(C=100000)\nclf.fit(set1['Xtrain'], set1['ytrain'])\nclf.score(set1['Xtest'], set1['ytest'])\n", "intent": "> YOUR TURN HERE: Carry out an unregularized logistic regression and calculate the score on the `set1` test set.\n"}
{"snippet": "gb_cv.fit(Xtrain, ytrain)\n", "intent": "This will take some time! We've made a smaller grid, but things are still slow\n"}
{"snippet": "from statsmodels.api import OLS\nregr = LinearRegression()\nregr.fit(x_train, y_train)\nprint(regr.coef_)\n", "intent": "We will use the training/testing dataset as before and create our linear regression objects.\n"}
{"snippet": "import statsmodels.api as sm\nX_train_gene_10 = sm.add_constant(X_train[\"Gene 10\"])\nX_test_gene_10 = sm.add_constant(X_test[\"Gene 10\"])\nols = OLS(endog=y_train, exog=X_train_gene_10).fit()\nols.summary()\n", "intent": "Let's use one single predictor variable, `Gene 10` and fit a linear regression model to it. \n"}
{"snippet": "lda = LDA()\nlda.fit(X_train_2, y_train_2)\nqda = QDA()\nqda.fit(X_train_2, y_train_2)\n", "intent": "We can then compare our results to that given by LDA and QDA:\n"}
{"snippet": "knn_2 = KNN(n_neighbors=2)\nknn_2.fit(X_train_2, y_train_2)\nknn_5 = KNN(n_neighbors=5)\nknn_5.fit(X_train_2, y_train_2)\n", "intent": "Finally, we cover the fitting of k-Nearest Neighbors for $k = 2,5$:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=x_train.shape[1]))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(num_classes, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model = LinearRegression(normalize=True)\nprint(model.normalize)\n", "intent": "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated, and have suitable default values:\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, epochs=5, batch_size=32)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model_cluster_1.fit(X_trainW, y_trainW)\nmodel_cluster_2.fit(X_trainM, y_trainM)\n", "intent": "Fit the model and predictions:\n"}
{"snippet": "lm.fit(X_train,y_train) \n", "intent": "**Fit the model on to the instantiated object itself**\n"}
{"snippet": "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\nlearn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\nlearn.clip=25.\nlearn.metrics = [accuracy]\n", "intent": "Lets use the RNN_Learner just as before.\n"}
{"snippet": "cle3 = learn.fit(lr, 1, cycle_len=12, use_clr=(20,10), stepper=Seq2SeqStepper)\n", "intent": "At least 11 epochs as before then it was \"cheating\" by getting the right value.\n"}
{"snippet": "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')  \ncb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\nfit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)\n", "intent": "updating the learning rate with a cosine annealing for the optimizer\n"}
{"snippet": "learn.fit(0.01, 1)\n", "intent": "Now train only the (-4) layer.\n"}
{"snippet": "learn.fit(lrs/5, 1, cycle_len=2)\n", "intent": "Accuracy isn't improving much - since many images have multiple different objects, it's going to be very hard (impossible?) to be more accurate.\n"}
{"snippet": "head_reg4 = nn.Sequential(Flatten(), nn.Linear(25088,4))\nlearn = ConvLearner.pretrained(f_model, md, custom_head=head_reg4)\nlearn.opt_fn = optim.Adam\nlearn.crit = nn.L1Loss()\n", "intent": "Normally the previous layer has $7*7*512=2508$ in ResNet34, so flatten that out into a single vector of length 2508\n"}
{"snippet": "model = LogisticRegression()\nmodel = model.fit(xData, yData)\nmodel.score(xData, yData)\n", "intent": "Let's go ahead and run logistic regression on the entire data set, and see how accurate it is!\n"}
{"snippet": "m = RandomForestRegressor(n_estimators=1, max_depth=1, bootstrap=False)\nm.fit(x_samp, y_samp)\ndraw_tree(m.estimators_[0], x_samp, precision=2)\n", "intent": "Lets do it first using the sckit library to later compare our results.\n"}
{"snippet": "m = RandomForestRegressor(n_estimators=1, max_depth=2, bootstrap=False)\nm.fit(x_samp, y_samp)\ndraw_tree(m.estimators_[0], x_samp, precision=2)\n", "intent": "First (again) do it with the library of sckit to later be able to compare our results.\n"}
{"snippet": "vxmb = Variable(xmb.cuda())\nvxmb\n", "intent": "`Variable(self, /, *args, **kwargs)`  Wraps a tensor and records the operations applied to it. \n(Wrapping a tensor in a variable)\n"}
{"snippet": "xt, yt = next(dl)   \ny_pred = net2(Variable(xt).cuda())  \n", "intent": "First, we will do a **forward pass**, which means computing the predicted y by passing x to the model.\n"}
{"snippet": "m = CharLoopModel(vocab_size, n_fac).cuda() \nopt = optim.Adam(m.parameters(), 1e-2)\n", "intent": "This now is a deeper network, 8 chars vs 2 before. \nAs DNNs get deeper, they become harder to train.\n"}
{"snippet": "learn.fit(lr, 2, cycle_len=1)\n", "intent": "log_preds,y = learn.TTA()\npreds = np.mean(np.exp(log_preds),0)\naccuracy_np(preds,y)\n"}
{"snippet": "m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(x, y);\nm.oob_score_\n", "intent": "Let's now see if we can still predict whether something's in the validation set... \n"}
{"snippet": "m = RandomForestRegressor(n_estimators=1, max_depth=2, bootstrap=False)\nm.fit(x_samp, y_samp)\n", "intent": "First (again) do it with the library of scikit to later be able to compare our results.\n"}
{"snippet": "m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False)\nm.fit(x_samp, y_samp)\n", "intent": "We create a tree ensemble again, this time maximum depth of 3.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    return tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1024, activation='relu', input_dim = 1000))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='sigmoid'))\nmodel.summary()\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics =['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train,y_train, batch_size =16, epochs =5, validation_data=(x_test,y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "import numpy as np\nregr = LinearRegression()\nregr.fit(X_train, y_train)\nprint('Coefficients: \\n', regr.coef_)\n", "intent": "d) Train model on training data, and make predictions on testing data\n"}
{"snippet": "def negative_log_likelihood(X, y, w):\n    scores = sigmoid(np.dot(X, w))\n    nll = -np.sum(y*np.log(scores+1e-15) + (1-y)*np.log(1-scores+1e-15))\n    return nll\n", "intent": "As defined in Eq. 33\n"}
{"snippet": "model = MODEL(out_dim)\n", "intent": "let's create our model, which will take as input a dummy matrix to get a list of output\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X, y)\n", "intent": "**Build scikit-learn model**\n"}
{"snippet": "classifier = LogisticRegression()\n", "intent": "All models in scikit-learn have a very consistent interface.\nFirst, we instantiate the estimator object.\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "This time we set a parameter of the KNeighborsClassifier to tell it we only want to look at one nearest neighbor:\n"}
{"snippet": "knn.fit(X_train, y_train)\n", "intent": "We fit the model with out training data\n"}
{"snippet": "from sklearn import tree\nclf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,min_samples_leaf=5)\nclf = clf.fit(X_train,y_train)\n", "intent": "Fit a decision tree with the data.\n"}
{"snippet": "pca.fit(X_blob)\n", "intent": "Then we fit the PCA model with our data. As PCA is an unsupervised algorithm, there is no output ``y``.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf\n", "intent": "We can now train a classifier, for instance a logistic regression classifier which is a fast baseline for text classification tasks:\n"}
{"snippet": "grid.fit(X_train, y_train)\nprint(grid.best_params_)\nprint(grid.best_score_)\n", "intent": "I installed lightGBM for computing with GPU, check how incredibly faster the computations are:\n"}
{"snippet": "import tensorflow as tf\nA = tf.constant(1234)\nB = tf.constant([123, 456, 789])\nC = tf.constant([[12, 34], [56, 78]])\nwith tf.Session() as sess:\n    output = sess.run(A)\n    print(A)\n    print(B)\n    print(C)\n", "intent": "** Tensorflow - Hello world! **\n"}
{"snippet": "SRN_result = model.fit(X_train, y_train,\n          batch_size=32,\n          epochs=5,\n          validation_data=(X_test, y_test))\n", "intent": "2\\. Fit the model with the training set with 5 epochs and batch size 32.\n"}
{"snippet": "from keras.layers import LSTM\nmodel_L = Sequential()\nmodel_L.add(Embedding(10000, 64))\nmodel_L.add(LSTM(64))\nmodel_L.add(Dense(1, activation='sigmoid'))\nmodel_L.summary()\n", "intent": "i) [2 point] Now built a LSTM model by replacing the simple RNN layter in the above model with a LSTM layer. Print a summary of the LSTM model.\n"}
{"snippet": "from keras.layers import LSTM\nlstm_model = Sequential()\nlstm_model.add(Embedding(max_words, 64, input_length=max_len))\nlstm_model.add(LSTM(64))\nlstm_model.add(Dense(1, activation='sigmoid'))\nlstm_model.summary()\n", "intent": "i) [1 point] Now built a LSTM model by replacing the simple RNN layter in the above model with a LSTM layer. Print a summary of the LSTM model.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nresult_k = knn.fit(x, y) \n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "gs_knn_p = GridSearchCV(knn, knn_params, cv=5, scoring = 'average_precision')\nresult_k_p = gs_knn_p.fit(x_train,y_train)\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=10,random_state=33)\nclf = clf.fit(X_train,y_train)\nloo_cv(X_train,y_train,clf)\n", "intent": "Try to improve performance using Random Forests\n"}
{"snippet": "_X = np.array([[1,2,3], [4,5,6]])\nX = tf.convert_to_tensor(_X)\nout = tf.ones_like(X)\nprint(out.eval())\nassert np.allclose(out.eval(), np.ones_like(_X))\n", "intent": "Q4. Let X be a tensor of [[1,2,3], [4,5,6]]. <br />Create a tensor of the same shape and dtype as X with all elements set to one.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "<img src=\"figs/fig3.png\",width=500>\n"}
{"snippet": "_x = np.array([1, 2, 3])\n_y = np.array([-1, -2, -3])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q1. Add x and y element-wise.\n"}
{"snippet": "_x = np.array([3, 4, 5])\n_y = np.array(3)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q2. Subtract y from x element-wise.\n"}
{"snippet": "_x = np.array([3, 4, 5])\n_y = np.array([1, 0, -1])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q3. Multiply x by y element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q4. Multiply x by 5 element-wise.\n"}
{"snippet": "_x = np.array([10, 20, 30], np.int32)\n_y = np.array([2, 3, 5], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.div(x, y)\nout2 = tf.truediv(x, y)\nprint(np.array_equal(out1.eval(), out2.eval()))\nprint(out1.eval(), out1.eval().dtype) \nprint(out2.eval(), out2.eval().dtype)\n", "intent": "Q5. Predict the result of this.\n"}
{"snippet": "_x = np.array([10, 20, 30], np.int32)\n_y = np.array([2, 3, 7], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q6. Get the remainder of x / y element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3], np.int32)\n_y = np.array([4, 5, 6], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q7. Compute the pairwise cross product of x and y.\n"}
{"snippet": "clf_dt=tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,min_samples_leaf=5)\nclf_dt.fit(X_train,y_train)\nmeasure_performance(X_test,y_test,clf_dt)\n", "intent": "To evaluate performance on future data, evaluate on the training set and test on the evaluation set\n"}
{"snippet": "_X = np.array([[1, -1], [3, -3]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q9. Compute the absolute value of X element-wise.\n"}
{"snippet": "_x = np.array([1, -1])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q10. Compute numerical negative value of x, elemet-wise.\n"}
{"snippet": "_x = np.array([1, 3, 0, -1, -3])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q11. Compute an element-wise indication of the sign of x, element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 2/10])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q12. Compute the reciprocal of x, element-wise.\n"}
{"snippet": "_x = np.array([1, 2, -1])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q13. Compute the square of x, element-wise.\n"}
{"snippet": "_x = np.array([2.1, 1.5, 2.5, 2.9, -2.1, -2.5, -2.9])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q14. Predict the results of this, paying attention to the difference among the family functions.\n"}
{"snippet": "_x = np.array([1, 4, 9], dtype=np.float32)\nx = tf.convert_to_tensor(_x)\n", "intent": "Q15. Compute square root of x element-wise.\n"}
{"snippet": "_x = np.array([1., 4., 9.])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q16. Compute the reciprocal of square root of x element-wise.\n"}
{"snippet": "_x = np.array([[1, 2], [3, 4]])\n_y = np.array([[1, 2], [1, 2]])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q17. Compute $x^y$, element-wise.\n"}
{"snippet": "from datetime import datetime\na = datetime.now()\nclf = ExtraTreesClassifier(n_estimators=100, random_state=10).fit(x_train, y_train)\nprint \"High Number of Trees Timing:\", datetime.now() - a\na = datetime.now()\nclf = AdaBoostClassifier(n_estimators=100, random_state=10).fit(x_train, y_train)\nprint \"AdaBoost Timing:            \", datetime.now() - a\n", "intent": "Try timing the above on your machine (you can use datetimes to get a simple approach here)\n"}
{"snippet": "_x = np.array([1, np.e, np.e**2])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q18. Compute natural logarithm of x element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 2])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q19. Compute the max of x and y element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 2])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q20. Compute the min of x and y element-wise.\n"}
{"snippet": "_x = np.array([-np.pi, np.pi, np.pi/2])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q21. Compuete the sine, cosine, and tangent of x, element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 1])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q22. Compute (x - y)(x - y) element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3])\n_y = np.array([-1, -2, -3])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.add(x, y)\nout2 = x + y \nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval()) \n_out = np.add(_x, _y)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q1. Add x and y element-wise.\n"}
{"snippet": "_x = np.array([3, 4, 5])\n_y = np.array(3)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.subtract(x, y)\nout2 = x - y \nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval()) \n_out = np.subtract(_x, _y)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q2. Subtract y from x element-wise.\n"}
{"snippet": "_x = np.array([3, 4, 5])\n_y = np.array([1, 0, -1])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.multiply(x, y)\nout2 = x * y \nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval()) \n_out = np.multiply(_x, _y)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q3. Multiply x by y element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3])\nx = tf.convert_to_tensor(_x)\nout1 = tf.scalar_mul(5, x)\nout2 = x * 5\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n_out = _x * 5\nassert np.array_equal(out1.eval(), _out)\n", "intent": "Q4. Multiply x by 5 element-wise.\n"}
{"snippet": "clf = MultinomialNB().fit(xtrain, ytrain)\nprint \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\nclf = BernoulliNB().fit(xtrain, ytrain)\nprint \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\n", "intent": "Finally we use scikit learn's Naive Bayes models to fit the training, then we score them on accuracy.\n"}
{"snippet": "_x = np.array([1, 2, 3], np.int32)\n_y = np.array([4, 5, 6], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.cross(x, y)\nprint(out1.eval())\n_out = np.cross(_x, _y)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q7. Compute the pairwise cross product of x and y.\n"}
{"snippet": "_x = np.array([1, 2, 3], np.int32)\n_y = np.array([4, 5, 6], np.int32)\n_z = np.array([7, 8, 9], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nz = tf.convert_to_tensor(_y)\nout1 = tf.add_n([x, y, z])\nout2 = x + y + z\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n", "intent": "Q8. Add x, y, and z element-wise.\n"}
{"snippet": "_X = np.array([[1, -1], [3, -3]])\nX = tf.convert_to_tensor(_X)\nout = tf.abs(X)\nprint(out.eval())\n_out = np.abs(_X)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q9. Compute the absolute value of X element-wise.\n"}
{"snippet": "_x = np.array([1, -1])\nx = tf.convert_to_tensor(_x)\nout1 = tf.negative(x)\nout2 = -x\nprint(out.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n_out = np.negative(_x)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q10. Compute numerical negative value of x, elemet-wise.\n"}
{"snippet": "_x = np.array([1, 3, 0, -1, -3])\nx = tf.convert_to_tensor(_x)\nout = tf.sign(x)\nprint(out.eval())\n_out = np.sign(_x)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q11. Compute an element-wise indication of the sign of x, element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 2/10])\nx = tf.convert_to_tensor(_x)\nout1 = tf.reciprocal(x)\nout2 = 1/x\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n_out = np.reciprocal(_x)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q12. Compute the reciprocal of x, element-wise.\n"}
{"snippet": "_x = np.array([1, 2, -1])\nx = tf.convert_to_tensor(_x)\nout1 = tf.square(x)\nout2 = x * x\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n_out = np.square(_x)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q13. Compute the square of x, element-wise.\n"}
{"snippet": "_x = np.array([1, 4, 9], dtype=np.float32)\nx = tf.convert_to_tensor(_x)\nout = tf.sqrt(x)\nprint(out.eval())\n_out = np.sqrt(_x)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q15. Compute square root of x element-wise.\n"}
{"snippet": "_x = np.array([1., 4., 9.])\nx = tf.convert_to_tensor(_x)\nout1 = tf.rsqrt(x)\nout2 = tf.reciprocal(tf.sqrt(x))\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n", "intent": "Q16. Compute the reciprocal of square root of x element-wise.\n"}
{"snippet": "from sklearn import cluster\nclf = cluster.KMeans(init='k-means++', n_clusters=10, random_state=42)\nclf.fit(X_train)\nprint clf.labels_.shape\nprint clf.labels_[1:10]\nprint_digits(images_train, clf.labels_, max_n=10)\n", "intent": "Train a KMeans classifier, show the clusters. \n"}
{"snippet": "_x = np.array([1., 2., 3.], np.float32)\nx = tf.convert_to_tensor(_x)\nout1 = tf.exp(x)\nout2 = tf.pow(np.e, x) \nprint(out1.eval())\nassert np.allclose(out1.eval(), out2.eval())\n_out = np.exp(_x)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q17. Compute $e^x$, element-wise.\n"}
{"snippet": "_x = np.array([1, np.e, np.e**2])\nx = tf.convert_to_tensor(_x)\nout = tf.log(x)\nprint(out.eval())\n_out = np.log(_x)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q18. Compute natural logarithm of x element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 2])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.maximum(x, y)\nout2 = tf.where(x > y, x, y)\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n_out = np.maximum(_x, _y)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q19. Compute the max of x and y element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 2])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.minimum(x, y)\nout2 = tf.where(x < y, x, y)\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n_out = np.minimum(_x, _y)\nassert np.array_equal(out1.eval(), _out) \n", "intent": "Q20. Compute the min of x and y element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 1])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.squared_difference(x, y)\nout2 = tf.square(tf.subtract(x, y))\nprint(out1.eval())\nassert np.array_equal(out1.eval(), out2.eval())\n", "intent": "Q22. Compute (x - y)(x - y) element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3, 4])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q1. Create a diagonal tensor with the diagonal values of x.\n"}
{"snippet": "_X = np.array(\n[[1, 0, 0, 0],\n [0, 2, 0, 0],\n [0, 0, 3, 0],\n [0, 0, 0, 4]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q2. Extract the diagonal of X.\n"}
{"snippet": "_X = np.random.rand(2,3,4)\nX = tf.convert_to_tensor(_X)\n", "intent": "Q3. Permutate the dimensions of x such that the new tensor has shape (3, 4, 2).\n"}
{"snippet": "_X = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\nX = tf.convert_to_tensor(_X)\ndiagonal_tensor = tf.matrix_diag(X)\ndiagonal_part = tf.matrix_diag_part(diagonal_tensor)\nprint(\"diagonal_tensor =\\n\", diagonal_tensor.eval())\nprint(\"diagonal_part =\\n\", diagonal_part.eval())\n", "intent": "Q5. Predict the result of this.\n"}
{"snippet": "from sklearn.cluster import KMeans\nkmeans = KMeans(3, random_state=8)\nY_hat = kmeans.fit(X).labels_\n", "intent": "Normally, you do not know the information in `Y`, however.\nYou could try to recover it from the data alone.\nThis is what the kMeans algorithm does. \n"}
{"snippet": "_X = np.array([[1, 2, 3], [4, 5, 6]])\n_Y = np.array([[1, 1], [2, 2], [3, 3]])\nX = tf.convert_to_tensor(_X)\nY = tf.convert_to_tensor(_Y)\n", "intent": "Q7. Multiply X by Y.\n"}
{"snippet": "_X = np.arange(1, 13, dtype=np.int32).reshape((2, 2, 3))\n_Y = np.arange(13, 25, dtype=np.int32).reshape((2, 3, 2))\nX = tf.convert_to_tensor(_X)\nY = tf.convert_to_tensor(_Y)\n", "intent": "Q8. Multiply X and Y. The first axis represents batches.\n"}
{"snippet": "_X = np.arange(1, 5, dtype=np.float32).reshape((2, 2))\nX = tf.convert_to_tensor(_X)\n", "intent": "Q9. Compute the determinant of X.\n"}
{"snippet": "_X = np.arange(1, 5, dtype=np.float64).reshape((2, 2))\nX = tf.convert_to_tensor(_X)\n", "intent": "Q10. Compute the inverse of X.\n"}
{"snippet": "_X = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]], np.float32)\nX = tf.convert_to_tensor(_X)\n", "intent": "Q11. Get the lower-trianglular in the Cholesky decomposition of X.\n"}
{"snippet": "_X = np.diag((1, 2, 3))\nX = tf.convert_to_tensor(_X, tf.float32)\n", "intent": "Q12. Compute the eigenvalues and eigenvectors of X.\n"}
{"snippet": "_X = np.array(\n[[1, 0, 0, 0, 2], \n [0, 0, 3, 0, 0], \n [0, 0, 0, 0, 0], \n [0, 2, 0, 0, 0]], dtype=np.float32)\nX = tf.convert_to_tensor(_X)\n", "intent": "Q13. Compute the singular values of X.\n"}
{"snippet": "_X = np.array([[0, 1, 0],\n              [1, 1, 0]])\nX = tf.convert_to_tensor(_X)\nouts = [tf.count_nonzero(X),\n        tf.count_nonzero(X, axis=0),\n        tf.count_nonzero(X, axis=1, keep_dims=True),\n        ]\nfor out in outs:\n    print(\"->\", out.eval())\n", "intent": "Q16. Predict the results of these.\n"}
{"snippet": "_x = np.array([1, 2, 3, 4])\nx = tf.convert_to_tensor(_x)\nout = tf.diag(x)\nprint(out.eval())\n_out = np.diag(_x)\nassert np.array_equal(out.eval(), _out)\n", "intent": "Q1. Create a diagonal tensor with the diagonal values of x.\n"}
{"snippet": "from datetime import datetime\na = datetime.now()\nclf1 = ExtraTreesClassifier(n_estimators=100, random_state=10).fit(x_train, y_train)\nprint \"High Number of Trees Timing:\", datetime.now() - a\na = datetime.now()\nclf2 = AdaBoostClassifier(n_estimators=100, random_state=10).fit(x_train, y_train)\nprint \"AdaBoost Timing:            \", datetime.now() - a\n", "intent": "Try timing the above on your machine (you can use datetimes to get a simple approach here)\n"}
{"snippet": "_X = np.random.rand(2,3,4)\nX = tf.convert_to_tensor(_X)\nout = tf.transpose(X, [1, 2, 0])\nprint(out.get_shape())\n_out = np.transpose(_X, [1, 2, 0])\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q3. Permutate the dimensions of x such that the new tensor has shape (3, 4, 2).\n"}
{"snippet": "_X= np.random.rand(1, 2, 3, 4)\nX = tf.convert_to_tensor(_X)\nout1 = tf.matrix_transpose(X)\nout2 = tf.transpose(X, [0, 1, 3, 2])\nprint(out1.eval().shape)\nassert np.array_equal(out1.eval(), out2.eval())\n", "intent": "Q6. Transpose the last two dimensions of x.\n"}
{"snippet": "_X = np.array([[1, 2, 3], [4, 5, 6]])\n_Y = np.array([[1, 1], [2, 2], [3, 3]])\nX = tf.convert_to_tensor(_X)\nY = tf.convert_to_tensor(_Y)\nout = tf.matmul(X, Y)\nprint(out.eval())\n_out = np.dot(_X, _Y)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q7. Multiply X by Y.\n"}
{"snippet": "_X = np.arange(1, 13, dtype=np.int32).reshape((2, 2, 3))\n_Y = np.arange(13, 25, dtype=np.int32).reshape((2, 3, 2))\nX = tf.convert_to_tensor(_X)\nY = tf.convert_to_tensor(_Y)\nout = tf.matmul(X, Y)\nprint(out.eval())\n", "intent": "Q8. Multiply X and Y. The first axis represents batches.\n"}
{"snippet": "_X = np.arange(1, 5, dtype=np.float32).reshape((2, 2))\nX = tf.convert_to_tensor(_X)\nout = tf.matrix_determinant(X)\nprint(out.eval())\n", "intent": "Q9. Compute the determinant of X.\n"}
{"snippet": "_X = np.arange(1, 5, dtype=np.float64).reshape((2, 2))\nX = tf.convert_to_tensor(_X)\nout = tf.matrix_inverse(X)\nprint(out.eval())\n_out = np.linalg.inv(_X)\nassert np.allclose(out.eval(), _out)\n", "intent": "Q10. Compute the inverse of X.\n"}
{"snippet": "_X = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]], np.float32)\nX = tf.convert_to_tensor(_X)\nout = tf.cholesky(X)\nprint(out.eval())\n_out = np.linalg.cholesky(_X)\nassert np.array_equal(out.eval(), _out)\n", "intent": "Q11. Get the lower-trianglular in the Cholesky decomposition of X.\n"}
{"snippet": "_X = np.diag((1, 2, 3))\nX = tf.convert_to_tensor(_X, tf.float32)\neigenvals, eigenvecs = tf.self_adjoint_eig(X)\nprint(\"eigentvalues =\\n\", eigenvals.eval())\nprint(\"eigenvectors =\\n\", eigenvecs.eval())\n_eigenvals, _eigenvecs = np.linalg.eig(_X)\nassert np.allclose(eigenvals.eval(), _eigenvals)\nassert np.allclose(eigenvecs.eval(), _eigenvecs)\n", "intent": "Q12. Compute the eigenvalues and eigenvectors of X.\n"}
{"snippet": "_X = np.array(\n[[1, 0, 0, 0, 2], \n [0, 0, 3, 0, 0], \n [0, 0, 0, 0, 0], \n [0, 2, 0, 0, 0]], dtype=np.float32)\nX = tf.convert_to_tensor(_X)\nout = tf.svd(X, compute_uv=False)\nprint(out.eval())\n_out = np.linalg.svd(_X, compute_uv=False)\nassert np.allclose(out.eval(), _out)\n", "intent": "Q13. Compute the singular values of X.\n"}
{"snippet": "clf_dt = tree.DecisionTreeClassifier(max_depth=10)\nclf_dt.fit(X_train, y_train)\nclf_dt.score(X_test, y_test)\n", "intent": "Here 4 classifiers are used to train the data, namely, *Decission Tree*, *AdaBoost*, *Gradient Boosting* and  *Random Forest*.\n"}
{"snippet": "_X = np.array([[1,2,3], [4,5,6]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q2. Compute the cumulative product of X along the second axis.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [-1,-2,-3,-4], \n     [-10,-20,-30,-40],\n     [10,20,30,40]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q3. Compute the sum along the first two elements and \nthe last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [1,1/2,1/3,1/4], \n     [1,2,3,4],\n     [-1,-1,-1,-1]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q4. Compute the product along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,4,5,7], \n     [2,3,6,8], \n     [1,2,3,4],\n     [-1,-2,-3,-4]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q5. Compute the minimum along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,4,5,7], \n     [2,3,6,8], \n     [1,2,3,4],\n     [-1,-2,-3,-4]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q6. Compute the maximum along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [5,6,7,8], \n     [-1,-2,-3,-4],\n     [-5,-6,-7,-8]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q7. Compute the mean along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [-1,-2,-3,-4], \n     [-10,-20,-30,-40],\n     [10,20,30,40]])\nX = tf.convert_to_tensor(_X)\n", "intent": "Q8. Compute the sum along the second and fourth and \nthe first and third elements of X separately in the order.\n"}
{"snippet": "_X = np.random.permutation(10).reshape((2, 5))\nprint(\"_X =\", _X)\nX = tf.convert_to_tensor(_X)\n", "intent": "Q9. Get the indices of maximum and minimum values of X along the second axis.\n"}
{"snippet": "_x = np.array([0, 1, 2, 5, 0])\n_y = np.array([0, 1, 4])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n", "intent": "Q10. Find the unique elements of x that are not present in y.\n"}
{"snippet": "def run_prob_cv(X, y, clf_class, roc=False, **kwargs):\n    kf = KFold(len(y), n_folds=5, shuffle=True)\n    y_prob = np.zeros((len(y),2))\n    for train_index, test_index in kf:\n        X_train, X_test = X[train_index], X[test_index]\n        y_train = y[train_index]\n        clf = clf_class(**kwargs)\n        clf.fit(X_train,y_train)\n        y_prob[test_index] = clf.predict_prob(X_test)\n    return y_prob\n", "intent": "RandomForestClassifier, KNeighborsClassifier and LogisticRegression have predict_prob() function \n"}
{"snippet": "_x = np.array([1, 2, 6, 4, 2, 3, 2])\nx = tf.convert_to_tensor(_x)\n", "intent": "Q12. Get unique elements and their indices from x.\n"}
{"snippet": "hypothesis = tf.SparseTensor(\n    [[0, 0],[0, 1],[0, 2],[0, 4]],\n    [\"a\", \"b\", \"c\", \"a\"],\n    (1, 5)) \ntruth = tf.SparseTensor(\n    [[0, 0],[0, 2],[0, 4]],\n    [\"a\", \"c\", \"b\"],\n    (1, 6))\n", "intent": "Q13. Compute the edit distance between hypothesis and truth.\n"}
{"snippet": "_X = np.array([[1,2,3], [4,5,6]])\nX = tf.convert_to_tensor(_X)\nout = tf.cumsum(X, axis=1)\nprint(out.eval())\n_out = np.cumsum(_X, axis=1)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q1. Compute the cumulative sum of X along the second axis.\n"}
{"snippet": "_X = np.array([[1,2,3], [4,5,6]])\nX = tf.convert_to_tensor(_X)\nout = tf.cumprod(X, axis=1)\nprint(out.eval())\n_out = np.cumprod(_X, axis=1)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q2. Compute the cumulative product of X along the second axis.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [-1,-2,-3,-4], \n     [-10,-20,-30,-40],\n     [10,20,30,40]])\nX = tf.convert_to_tensor(_X)\nout = tf.segment_sum(X, [0, 0, 1, 1])\nprint(out.eval())\n", "intent": "Q3. Compute the sum along the first two elements and \nthe last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [1,1/2,1/3,1/4], \n     [1,2,3,4],\n     [-1,-1,-1,-1]])\nX = tf.convert_to_tensor(_X)\nout = tf.segment_prod(X, [0, 0, 1, 1])\nprint(out.eval())\n", "intent": "Q4. Compute the product along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,4,5,7], \n     [2,3,6,8], \n     [1,2,3,4],\n     [-1,-2,-3,-4]])\nX = tf.convert_to_tensor(_X)\nout = tf.segment_min(X, [0, 0, 1, 1])\nprint(out.eval())\n", "intent": "Q5. Compute the minimum along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,4,5,7], \n     [2,3,6,8], \n     [1,2,3,4],\n     [-1,-2,-3,-4]])\nX = tf.convert_to_tensor(_X)\nout = tf.segment_max(X, [0, 0, 1, 1])\nprint(out.eval())\n", "intent": "Q6. Compute the maximum along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [5,6,7,8], \n     [-1,-2,-3,-4],\n     [-5,-6,-7,-8]])\nX = tf.convert_to_tensor(_X)\nout = tf.segment_mean(X, [0, 0, 1, 1])\nprint(out.eval())\n", "intent": "Q7. Compute the mean along the first two elements and the last two elements of X separately.\n"}
{"snippet": "session = tf.Session()\n", "intent": "Once the TensorFlow graph has been created, we have to create a TensorFlow session which is used to execute the graph.\n"}
{"snippet": "_x = np.array([0, 1, 2, 5, 0])\n_y = np.array([0, 1, 4])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout = tf.setdiff1d(x, y)[0]\nprint(out.eval())\n_out = np.setdiff1d(_x, _y)\nassert np.array_equal(out.eval(), _out)\n", "intent": "Q10. Find the unique elements of x that are not present in y.\n"}
{"snippet": "_X = np.arange(1, 10).reshape(3, 3)\nX = tf.convert_to_tensor(_X)\nout = tf.where(X < 4, X, X*10)\nprint(out.eval())\n_out = np.where(_X < 4, _X, _X*10)\nassert np.array_equal(out.eval(), _out) \n", "intent": "Q11. Return the elements of X, if X < 4, otherwise X*10.\n"}
{"snippet": "_x = np.array([1, 2, 6, 4, 2, 3, 2])\nx = tf.convert_to_tensor(_x)\nout, indices = tf.unique(x)\nprint(out.eval())\nprint(indices.eval())\n_out, _indices = np.unique(_x, return_inverse=True)\nprint(\"sorted unique elements =\", _out)\nprint(\"indices =\", _indices)\n", "intent": "Q12. Get unique elements and their indices from x.\n"}
{"snippet": "_x = np.array([[1, 2, 4, 8], [2, 4, 6, 8]], dtype=np.float32)\nx = tf.convert_to_tensor(_x)\nout = ...\nwith tf.Session() as sess:\n    _out = sess.run(out)\n    print(_out)    \n    assert np.allclose(np.sum(_out, axis=-1), 1)\n", "intent": "Q3. Apply `softmax` to x.\n"}
{"snippet": "_x = np.array([[1, 2, 4, 8], [2, 4, 6, 8]], dtype=np.float32)\nprint(\"_x =\\n\" , _x)\nx = tf.convert_to_tensor(_x)\nout = ...\nwith tf.Session() as sess:\n    _out = sess.run(out)\n    print(\"_out =\\n\", _out) \n", "intent": "Q4. Apply `dropout` with keep_prob=.5 to x.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Q6. Apply 2 kernels of width-height (2, 2), stride 1, and same padding to x.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Q7. Apply 3 kernels of width-height (2, 2), stride 1, dilation_rate 2 and valid padding to x.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Q8. Apply 4 kernels of width-height (3, 3), stride 2, and same padding to x.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Q9. Apply 4 times of kernels of width-height (3, 3), stride 2, and same padding to x, depth-wise.\n"}
{"snippet": "net = MLP(3, .25, outtype=\"linear\")\nnet.fit(train, train_target, 100)\n", "intent": "* We don't know how many hidden neurons, we'll need, so let's try 3.\n* We will run this for 100 iterations\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Q11. Apply conv2d transpose with 5 kernels of width-height (3, 3), stride 2, and same padding to x.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Q12. Apply conv2d transpose with 5 kernels of width-height (3, 3), stride 2, and valid padding to x.\n"}
{"snippet": "_x = np.array([[1, 2, 4, 8], [2, 4, 6, 8]], dtype=np.float32)\nx = tf.convert_to_tensor(_x)\nout = tf.nn.softmax(x, dim=-1)\nwith tf.Session() as sess:\n    _out = sess.run(out)\n    print(_out)    \n    assert np.allclose(np.sum(_out, axis=-1), 1)\n", "intent": "Q3. Apply `softmax` to x.\n"}
{"snippet": "_x = np.array([[1, 2, 4, 8], [2, 4, 6, 8]], dtype=np.float32)\nprint(\"_x =\\n\" , _x)\nx = tf.convert_to_tensor(_x)\nout = tf.nn.dropout(x, keep_prob=0.5)\nwith tf.Session() as sess:\n    _out = sess.run(out)\n    print(\"_out =\\n\", _out) \n", "intent": "Q4. Apply `dropout` with keep_prob=.5 to x.\n"}
{"snippet": "x = tf.random_normal([8, 10])\nout = tf.contrib.layers.fully_connected(inputs=x, num_outputs=2, \n                                        activation_fn=tf.nn.sigmoid,\n                                        weights_initializer=tf.contrib.layers.xavier_initializer())\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(out))\n", "intent": "Q5. Apply a fully connected layer to x with 2 outputs and then an sigmoid function.\n"}
{"snippet": "_x = np.arange(1, 11)\nepsilon = 1e-12\nx = tf.convert_to_tensor(_x, tf.float32)\n", "intent": "Q1. Apply `l2_normalize` to `x`.\n"}
{"snippet": "_x = np.arange(1, 11)\nx = tf.convert_to_tensor(_x, tf.float32)\n", "intent": "Q2. Calculate the mean and variance of `x` based on the sufficient statistics.\n"}
{"snippet": "tf.reset_default_graph()\n_x = np.arange(1, 11)\nx = tf.convert_to_tensor(_x, tf.float32)\n", "intent": "Q3. Calculate the mean and variance of `x`.\n"}
{"snippet": "tf.reset_default_graph()\nx = tf.constant([1, 1, 2, 2, 2, 3], tf.float32)\nmean, variance = tf.nn.moments(x, [0])\nwith tf.Session() as sess:\n    print(sess.run([mean, variance]))\nunique_x, _, counts = tf.unique_with_counts(x)\nmean, variance = ...\nwith tf.Session() as sess:\n    print(sess.run([mean, variance]))\n", "intent": "Q4. Calculate the mean and variance of `x` using `unique_x` and `counts`.\n"}
{"snippet": "model = iris_clf.fit(data)\n", "intent": "* The Pipeline object behaves like a classifier\n"}
{"snippet": "tf.reset_default_graph()\nlogits = tf.random_normal(shape=[2, 5, 10])\nlabels = tf.convert_to_tensor(np.random.randint(0, 10, size=[2, 5]), tf.int32)\noutput = tf.nn....\nwith tf.Session() as sess:\n    print(sess.run(output))\n", "intent": "Q7. Compute softmax cross entropy between logits and labels. Note that the rank of them is not the same.\n"}
{"snippet": "logits = tf.random_normal(shape=[2, 5, 10])\nlabels = tf.convert_to_tensor(np.random.randint(0, 10, size=[2, 5]), tf.int32)\nlabels = tf.one_hot(labels, depth=10)\noutput = tf.nn....\nwith tf.Session() as sess:\n    print(sess.run(output))\n", "intent": "Q8. Compute softmax cross entropy between logits and labels.\n"}
{"snippet": "tf.reset_default_graph()\nx = tf.constant([0, 2, 1, 3, 4], tf.int32)\nembedding = tf.constant([0, 0.1, 0.2, 0.3, 0.4], tf.float32)\noutput = tf.nn....\nwith tf.Session() as sess:\n    print(sess.run(output))\n", "intent": "Q9. Map tensor `x` to the embedding.\n"}
{"snippet": "_x = np.arange(1, 11)\nepsilon = 1e-12\nx = tf.convert_to_tensor(_x, tf.float32)\noutput = tf.nn.l2_normalize(x, dim=0, epsilon=epsilon)\nwith tf.Session() as sess:\n    _output = sess.run(output)\n    assert np.allclose(_output, _x / np.sqrt(np.maximum(np.sum(_x**2), epsilon)))\nprint(_output)\n", "intent": "Q1. Apply `l2_normalize` to `x`.\n"}
{"snippet": "_x = np.arange(1, 11)\nx = tf.convert_to_tensor(_x, tf.float32)\ncounts_, sum_, sum_of_squares_, _ = tf.nn.sufficient_statistics(x, [0])\nmean, variance = tf.nn.normalize_moments(counts_, sum_, sum_of_squares_, shift=None)\nwith tf.Session() as sess:\n    _mean, _variance = sess.run([mean, variance])\nprint(_mean, _variance)\n", "intent": "Q2. Calculate the mean and variance of `x` based on the sufficient statistics.\n"}
{"snippet": "tf.reset_default_graph()\n_x = np.arange(1, 11)\nx = tf.convert_to_tensor(_x, tf.float32)\noutput = tf.nn.moments(x, [0])\nwith tf.Session() as sess:\n    _mean, _variance = sess.run(output)\nprint(_mean, _variance)\n", "intent": "Q3. Calculate the mean and variance of `x`.\n"}
{"snippet": "tf.reset_default_graph()\nx = tf.constant([1, 1, 2, 2, 2, 3], tf.float32)\nmean, variance = tf.nn.moments(x, [0])\nwith tf.Session() as sess:\n    print(sess.run([mean, variance]))\nunique_x, _, counts = tf.unique_with_counts(x)\nmean, variance = tf.nn.weighted_moments(unique_x, [0], counts)\nwith tf.Session() as sess:\n    print(sess.run([mean, variance]))\n", "intent": "Q4. Calculate the mean and variance of `x` using `unique_x` and `counts`.\n"}
{"snippet": "tf.reset_default_graph()\nlogits = tf.random_normal(shape=[2, 5, 10])\nlabels = tf.convert_to_tensor(np.random.randint(0, 10, size=[2, 5]), tf.int32)\noutput = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\nwith tf.Session() as sess:\n    print(sess.run(output))\n", "intent": "Q7. Compute softmax cross entropy between logits and labels. Note that the rank of them is not the same.\n"}
{"snippet": "logits = tf.random_normal(shape=[2, 5, 10])\nlabels = tf.convert_to_tensor(np.random.randint(0, 10, size=[2, 5]), tf.int32)\nlabels = tf.one_hot(labels, depth=10)\noutput = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\nwith tf.Session() as sess:\n    print(sess.run(output))\n", "intent": "Q8. Compute softmax cross entropy between logits and labels.\n"}
{"snippet": "y = np.zeros(len(epochs.events), dtype=int)\ny[epochs.events[:, 2] == 3] = 1\ncv = StratifiedKFold(y=y)  \ngat = GeneralizationAcrossTime(predict_mode='cross-validation', n_jobs=1,\n                               cv=cv, scorer=roc_auc_score)\ngat.fit(epochs, y=y)\ngat.score(epochs)\ngat.plot()\ngat.plot_diagonal()\n", "intent": "Generalization Across Time\n--------------------------\nHere we'll use a stratified cross-validation scheme.\n"}
{"snippet": "x = tf.constant([[1, 0, 0, 0],\n                 [0, 0, 2, 0],\n                 [0, 0, 0, 0]], dtype=tf.int32)\nsp = tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\nprint(sp.eval())\n", "intent": "Q1. Convert tensor x into a `SparseTensor`.\n"}
{"snippet": "def dense_to_sparse(tensor):\n    indices = tf.where(tf.not_equal(tensor, 0))\n    return tf.SparseTensor(indices=indices,\n                           values=tf.gather_nd(tensor, indices) - 1,  \n                           dense_shape=tf.to_int64(tf.shape(tensor)))\nprint(dense_to_sparse(x).eval())\n", "intent": "Q3. Let's write a custom function that converts a SparseTensor to Tensor. Complete it.\n"}
{"snippet": "output = tf.sparse_to_dense(sparse_indices=[[0, 0], [1, 2]], sparse_values=[1, 2], output_shape=[3, 4])\nprint(output.eval())\nprint(\"Check if this is identical with x:\\n\", x.eval())\n", "intent": "Q4. Convert the SparseTensor `sp` to a Tensor using `tf.sparse_to_dense`.\n"}
{"snippet": "output = tf.sparse_tensor_to_dense(s)\nprint(output.eval())\nprint(\"Check if this is identical with x:\\n\", x.eval())\n", "intent": "Q5. Convert the SparseTensor `sp` to a Tensor using `tf.sparse_tensor_to_dense`.\n"}
{"snippet": "def dense_to_sparse(tensor):\n    indices = tf.where(tf.not_equal(tensor, 0))\n    return tf.SparseTensor(indices=indices,\n                           values=...,  \n                           dense_shape=tf.to_int64(tf.shape(tensor)))\nprint(dense_to_sparse(x).eval())\n", "intent": "Q3. Let's write a custom function that converts a SparseTensor to Tensor. Complete it.\n"}
{"snippet": "_X = np.arange(1, 1*2*3*4 + 1).reshape((1, 2, 3, 4))\nX = tf.convert_to_tensor(_X)\n", "intent": "Q23. Given X below, reverse the last dimension.\n"}
{"snippet": "_X = np.ones((1, 2, 3))\nX = tf.convert_to_tensor(_X)\n", "intent": "Q24. Given X below, permute its dimensions such that the new tensor has shape (3, 1, 2).\n"}
{"snippet": "_X = np.arange(1, 10).reshape((3, 3))\nX = tf.convert_to_tensor(_X)\n", "intent": "Q25. Given X, below, get the first, and third rows.\n"}
{"snippet": "_X = np.arange(1, 10).reshape((3, 3))\nX = tf.convert_to_tensor(_X)\n", "intent": "Q26. Given X below, get the elements 5 and 7.\n"}
{"snippet": "time_decod.fit(X, y)\npatterns = get_coef(time_decod, 'patterns_', inverse_transform=True)\nstc = stcs[0]  \nvertices = [stc.lh_vertno, np.array([], int)]  \nstc_feat = mne.SourceEstimate(np.abs(patterns), vertices=vertices,\n                              tmin=stc.tmin, tstep=stc.tstep, subject='sample')\nbrain = stc_feat.plot(views=['lat'], transparent=True,\n                      initial_time=0.1, time_unit='s')\n", "intent": "To investigate weights, we need to retrieve the patterns of a fitted model\n"}
{"snippet": "_X = np.arange(1, 7).reshape((2, 3))\nX = tf.convert_to_tensor(_X)\nout = tf.tile(X, [1, 3])\nprint(out.eval())\nassert np.allclose(out.eval(), np.tile(_X, [1, 3]))\n", "intent": "Q17. Lex X be a tensor<br/>\n[[ 1 2 3]<br/>\n [ 4 5 6].<br/>\nCreate a tensor looking like <br/>\n[[ 1 2 3 1 2 3 1 2 3 ]<br/>\n [ 4 5 6 4 5 6 4 5 6 ]].\n"}
{"snippet": "_X = np.arange(1, 7).reshape((2, 3))\nX = tf.convert_to_tensor(_X)\nout = tf.pad(X, [[2, 0], [0, 3]])\nprint(out.eval())\nassert np.allclose(out.eval(), np.pad(_X, [[2, 0], [0, 3]], 'constant', constant_values=[0, 0]))\n", "intent": "Q18. Lex X be a tensor <br/>\n[[ 1 2 3]<br/>\n [ 4 5 6].<br/>\nPad 2 * 0's before the first dimension, 3 * 0's after the second dimension.\n"}
{"snippet": "_X = np.arange(1, 1*2*3*4 + 1).reshape((1, 2, 3, 4))\nX = tf.convert_to_tensor(_X)\nout = tf.reverse(X, [-1]) \nprint(out.eval())\nassert np.allclose(out.eval(), _X[:, :, :, ::-1])\n", "intent": "Q23. Given X below, reverse the last dimension.\n"}
{"snippet": "_X = np.ones((1, 2, 3))\nX = tf.convert_to_tensor(_X)\nout = tf.transpose(X, [2, 0, 1])\nprint(out.eval().shape)\nassert np.allclose(out.eval(), np.transpose(_X))\n", "intent": "Q24. Given X below, permute its dimensions such that the new tensor has shape (3, 1, 2).\n"}
{"snippet": "_X = np.arange(1, 10).reshape((3, 3))\nX = tf.convert_to_tensor(_X)\nout1 = tf.gather(X, [0, 2])\nout2 = tf.gather_nd(X, [[0], [2]])\nassert np.allclose(out1.eval(), out2.eval())\nprint(out1.eval())\nassert np.allclose(out1.eval(), _X[[0, 2]])\n", "intent": "Q25. Given X, below, get the first, and third rows.\n"}
{"snippet": "_X = np.arange(1, 10).reshape((3, 3))\nX = tf.convert_to_tensor(_X)\nout = tf.gather_nd(X, [[1, 1], [2, 0]])\nprint(out.eval())\nassert np.allclose(out.eval(), _X[[1, 2], [1, 0]])\n", "intent": "Q26. Given X below, get the elements 5 and 7.\n"}
{"snippet": "_x = np.array([0, 1, 2, 3])\n_y = np.array([True, False, False, True])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout = tf.boolean_mask(x, y)\nprint(out.eval())\nassert np.allclose(out.eval(), _x[_y])\n", "intent": "Q30. Let x be a tensor [0, 1, 2, 3] and y be a tensor [True, False, False, True].<br/>\nApply mask y to x.\n"}
{"snippet": "w = tf.Variable(1.0, name=\"Weight\")\nassign_op = ...\nwith tf.Session() as sess:\n    sess.run(w.initializer)\n    for _ in range(10):\n        print(sess.run(w), \"=>\", end=\"\")\n        sess.run(assign_op)\n", "intent": "Q1. Complete this code.\n"}
{"snippet": "w1 = tf.Variable(1.0)\nw2 = tf.Variable(2.0)\nw3 = tf.Variable(3.0)\nout = w1 + w2 + w3\ninit_op = ...\nwith tf.Session() as sess:\n    sess.run(init_op) \n    print(sess.run(out))\n", "intent": "Q2. Complete this code.\n"}
{"snippet": "spoc.fit(X, y)\nlayout = read_layout('CTF151.lay')\nspoc.plot_patterns(meg_epochs.info, layout=layout)\n", "intent": "Plot the contributions to the detected components (i.e., the forward model)\n"}
{"snippet": "g = tf.Graph()\nwith g.as_default():\n    W = tf.Variable([[0,1],[2,3]], name=\"Weight\", dtype=tf.float32)\n    print(\"Q5.\", ...)\n    print(\"Q6.\", ...)\n    print(\"Q7.\", ...)\n    print(\"Q8.\", ...)\n    print(\"Q9.\", ...)\n    print(\"Q10.\", ...)\n", "intent": "Q5-8. Complete this code.\n"}
{"snippet": "tf.reset_default_graph()\nw1 = tf.Variable(1.0, name=\"weight1\")\nw2 = tf.Variable(2.0, name=\"weight2\", trainable=False)\nw3 = tf.Variable(3.0, name=\"weight3\")\nwith tf.Session() as sess:\n    sess.run(...)\n    for v in tf.global_variables():\n        print(\"global variable =>\", ...)\n    for v in tf.trainable_variables():\n        print(\"trainable_variable =>\", ...)\n", "intent": "Q11-15. Complete this code.\n"}
{"snippet": "g = tf.Graph()\nwith g.as_default():\n    with tf.variable_scope(\"foo\"):\n        v = tf.get_variable(\"vv\", [1,])  \n    ...\nassert v1 == v    \n", "intent": "Q16. Complete this code.\n"}
{"snippet": "with tf.variable_scope(\"foo\"):\n    with tf.variable_scope(\"bar\"):\n        v = tf.get_variable(\"vv\", [1])\n        print(\"v.name =\", v.name)\n", "intent": "Q17. Predict the result of this code.\n"}
{"snippet": "value = [0, 1, 2, 3, 4, 5, 6, 7]\ninit = ...\ntf.reset_default_graph()\nx = tf.get_variable('x', shape=[2, 4], initializer=init)\nwith tf.Session() as sess:\n    sess.run(x.initializer)\n    print(\"x =\\n\", sess.run(x))\n", "intent": "Q18. Complete this code.\n"}
{"snippet": "init = ...\ntf.reset_default_graph()\nx = tf.get_variable('x', shape=[10, 1000], initializer=init)\nwith tf.Session():\n    x.initializer.run()\n    _x = x.eval()\n    print(\"Make sure the mean\", np.mean(_x), \"is close to 0\" )\n    print(\"Make sure the standard deviation\", np.std(_x), \"is close to 2\" )\n", "intent": "Q19. Complete this code.\n"}
{"snippet": "tf.reset_default_graph()\nprint(\"Of course, there're no variables since we reset the graph. See\", tf.global_variables())\nwith tf.Session() as sess:\n    new_saver = ...\n    new_saver.restore(sess, 'model/my-model-10000')\n    for v in tf.global_variables():\n        print(\"Now we have a variable\", v.name)\n", "intent": "Q22. Complete the code. Make sure you've done questions 14-15.\n"}
{"snippet": "w = tf.Variable(1.0, name=\"weight\")\nwith tf.Session() as sess:\n    sess.run(w.initializer)\n    print(sess.run(w))\n", "intent": "Q0. Create a variable `w` with an initial value of 1.0 and name `weight`.\nThen, print out the value of `w`.\n"}
{"snippet": "w = tf.Variable(1.0, name=\"Weight\")\nassign_op = w.assign(w + 1.0)\nwith tf.Session() as sess:\n    sess.run(w.initializer)\n    for _ in range(10):\n        print(sess.run(w), \"=>\", end=\"\")\n        sess.run(assign_op)\n", "intent": "Q1. Complete this code.\n"}
{"snippet": "time_decod.fit(X, y)\npatterns = get_coef(time_decod, 'patterns_', inverse_transform=True)\nstc = stcs[0]  \nvertices = [stc.lh_vertno, np.array([], int)]  \nstc_feat = mne.SourceEstimate(np.abs(patterns), vertices=vertices,\n                              tmin=stc.tmin, tstep=stc.tstep, subject='sample')\nbrain = stc_feat.plot(views=['lat'], transparent=True,\n                      initial_time=0.1, time_unit='s',\n                      subjects_dir=subjects_dir)\n", "intent": "To investigate weights, we need to retrieve the patterns of a fitted model\n"}
{"snippet": "V = tf.Variable(tf.truncated_normal([1, 10]))\nW = tf.Variable(V.initialized_value() * 2.0)\ninit_op = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init_op) \n    _V, _W = sess.run([V, W])\n    print(_V)\n    print(_W)\n    assert np.array_equiv(_V * 2.0, _W)\n", "intent": "Q3-4. Complete this code.\n"}
{"snippet": "g = tf.Graph()\nwith g.as_default():\n    W = tf.Variable([[0,1],[2,3]], name=\"Weight\", dtype=tf.float32)\n    print(\"Q5.\", W.name)\n    print(\"Q6.\", W.op.name)\n    print(\"Q7.\", W.dtype)\n    print(\"Q8.\", W.get_shape().as_list())\n    print(\"Q9.\", W.get_shape().ndims)\n    print(\"Q10.\", W.graph == g)\n", "intent": "Q5-8. Complete this code.\n"}
{"snippet": "tf.reset_default_graph()\nw1 = tf.Variable(1.0, name=\"weight1\")\nw2 = tf.Variable(2.0, name=\"weight2\", trainable=False)\nw3 = tf.Variable(3.0, name=\"weight3\")\nwith tf.Session() as sess:\n    sess.run(tf.variables_initializer([w1, w2]))\n    for v in tf.global_variables():\n        print(\"global variable =>\", v.name)\n    for v in tf.trainable_variables():\n        print(\"trainable_variable =>\", v.name)\n", "intent": "Q11-15. Complete this code.\n"}
{"snippet": "g = tf.Graph()\nwith g.as_default():\n    with tf.variable_scope(\"foo\"):\n        v = tf.get_variable(\"vv\", [1,])  \n    with tf.variable_scope(\"foo\", reuse=True):\n        v1 = tf.get_variable(\"vv\")  \nassert v1 == v    \n", "intent": "Q16. Complete this code.\n"}
{"snippet": "value = [0, 1, 2, 3, 4, 5, 6, 7]\ninit = tf.constant_initializer(value)\ntf.reset_default_graph()\nx = tf.get_variable('x', shape=[2, 4], initializer=init)\nwith tf.Session() as sess:\n    sess.run(x.initializer)\n    print(\"x =\\n\", sess.run(x))\n", "intent": "Q18. Complete this code.\n"}
{"snippet": "init = tf.random_normal_initializer(mean=0, stddev=2)\ntf.reset_default_graph()\nx = tf.get_variable('x', shape=[10, 1000], initializer=init)\nwith tf.Session():\n    x.initializer.run()\n    _x = x.eval()\n    print(\"Make sure the mean\", np.mean(_x), \"is close to 0\" )\n    print(\"Make sure the standard deviation\", np.std(_x), \"is close to 2\" )\n", "intent": "Q19. Complete this code.\n"}
{"snippet": "tf.reset_default_graph()\nprint(\"Of course, there're no variables since we reset the graph. See\", tf.global_variables())\nwith tf.Session() as sess:\n    new_saver = tf.train.import_meta_graph('model/my-model-10000.meta')\n    new_saver.restore(sess, 'model/my-model-10000')\n    for v in tf.global_variables():\n        print(\"Now we have a variable\", v.name)\n", "intent": "Q22. Complete the code. Make sure you've done questions 14-15.\n"}
{"snippet": "weightVar = tf.ones(shape=[2,3], dtype=tf.int32)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    result = sess.run(weightVar)\n    print(\"result=\\n{}\".format(result))\n", "intent": "Q0. Create a variable w with an initial value of 1.0 and name weight. Then, print out the value of w.\n"}
{"snippet": "with tf.Session() as sess:\n    zeros = tf.zeros(shape=[2,3])\n    print(sess.run(zeros))\n    assert np.allclose(sess.run(zeros), np.zeros([2, 3]))\n", "intent": "Q1. Create a tensor with shape [2, 3] with all elements set to zero. It should pass the assertion\n"}
{"snippet": "import statsmodels.formula.api as smf\nlm = smf.ols(formula='Sales ~ TV', data=data).fit()\nlm.params\n", "intent": "Let's use **Statsmodels** to estimate the model coefficients for the advertising data:\n"}
{"snippet": "with tf.Session() as sess:\n    _X = np.array([[1,3,5], [4,6,8]])\n    out = tf.constant(_X, dtype=tf.float32) \n    print(sess.run(out))\n    assert np.allclose(sess.run(out), np.array([[1, 3, 5], [4, 6, 8]], dtype=np.float32))\n", "intent": "Q4. Create a constant tensor of [[1, 3, 5], [4, 6, 8]], with dtype=float32\n"}
{"snippet": "with tf.Session() as sess:\n    _X = np.linspace(5., 10., 50)\n    out = tf.convert_to_tensor(_X)\n    print(sess.run(out))\n    assert np.allclose(sess.run(out), np.linspace(5., 10., 50))\n", "intent": "Q5. Create a 1-D tensor of 50 evenly spaced numberts between 5 and 10 (inclusive). *Hint: Look up the equivalent of np linspace for tf*\n"}
{"snippet": "with tf.Session() as sess:\n    X = tf.random_normal([3, 2], mean=0,  stddev=0.01)\n    print(sess.run(X))\n", "intent": "Q6. Create a random tensor of the shape [3, 2], with elements from a normal distribution of mean=0, standard deviation=2.\n"}
{"snippet": "with tf.Session() as sess:\n    _X = np.array([[1, 2], [3, 4], [5, 6]])\n    X = tf.convert_to_tensor(_X)\n    out = tf.random_shuffle(X)\n    print(sess.run(out))\n", "intent": "Q7. Randomly shuffle the data in matrix X using tf.random_shuffle()\n"}
{"snippet": "with tf.Session() as sess:\n    _x = tf.random_uniform([], -1, 1)\n    _y = tf.random_uniform([], -1, 1)\n    out = \n    print(sess.run(_x),sess.run(_y), sess.run(_out))\n", "intent": "Q8. Let x and y be random 0-D tensors. Return x + y if x < y and x - y otherwise. Use tf.cond()\n"}
{"snippet": "_x = np.linspace(-10., 10., 1000)\nx = tf.convert_to_tensor(_x)\n", "intent": "Q9. Apply relu, elu, and softplus to x. Then observe the plot\nThe package tf.nn contains all of these methods. \n"}
{"snippet": "_x = np.linspace(-10., 10., 1000)\nx = tf.convert_to_tensor(_x)\n", "intent": "Q2. Apply sigmoid and tanh to x using tf.nn.sigmoid() and tf.nn.tanh()\n"}
{"snippet": "_x = np.array([[1, 2, 4, 8], [2, 4, 6, 8]], dtype=np.float32)\nx = tf.convert_to_tensor(_x)\nout = tf.nn.softmax(x) \nwith tf.Session() as sess:\n    _out = sess.run(out)\n    print(_out)    \n    assert np.allclose(np.sum(_out, axis=-1), 1)\n", "intent": "Q3. Apply softmax to x. Assertion should pass if you did this correctly\n"}
{"snippet": "x = tf.random_normal([8, 10])\nout = tf.contrib.layers.fully_connected(inputs=x, num_outputs=5) \nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(out))\n", "intent": "Q4. Apply a fully connected layer to x with 2 outputs and then an sigmoid function. We provide the prepared contrib fully_connected layer\n"}
{"snippet": "feature_cols = ['TV', 'Radio', 'Newspaper']\nX = data[feature_cols]\ny = data.Sales\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X, y)\nprint lm.intercept_\nprint lm.coef_\n", "intent": "Let's redo some of the Statsmodels code above in scikit-learn:\n"}
{"snippet": "import tensorflow as tf\nx = tf.Variable(3, name='x') \ny = tf.Variable(4, name='y')\nf = x*x*y + y + 2\n", "intent": "The following code creates the graph represented above. \n"}
{"snippet": "x1 = tf.Variable(1)\nx1.graph is tf.get_default_graph()\n", "intent": "Each graph seems to be related to an execution plan.\n"}
{"snippet": "kernel = 1* RBF(length_scale=[0.1], length_scale_bounds=(0.00001, 10000)) \\\n        + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e10))\ngp = GaussianProcessRegressor(kernel=kernel,\n                              n_restarts_optimizer=10,random_state=999)\n", "intent": "The above plot shows a skewed data towards the right indicating a noisy dataset.\nAdd a noise kernel to adjust for the noise in the data.\n"}
{"snippet": "callback=[keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0009, patience=2, verbose=0, mode='auto'),\n          keras.callbacks.ModelCheckpoint(filepath='imdb.model.best.hdf5',verbose=1, save_best_only=True)\n         ]\nmodel_history=model.fit(x_train, y_train, epochs=100, batch_size=500, verbose=2, validation_split=0.2, callbacks=callback)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def sigmoid(x):\n", "intent": "$$\n\\sigma = \\frac{1}{1 + e^{-x}}\n$$\n"}
{"snippet": "skipgram = Sequential()\nskipgram.add(Embedding(input_dim=vocab_size, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\nskipgram.add(Reshape((dim,)))\nskipgram.add(Dense(input_dim=dim, units=vocab_size, activation='softmax'))\nSVG(model_to_dot(skipgram, show_shapes=True).create(prog='dot', format='svg'))\n", "intent": "- Lastly, we create the (shallow) network!\n"}
{"snippet": "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\nrun_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n", "intent": "Why do we have 121 parameters?  Does that make sense?\nLet's fit our model for 200 epochs.\n"}
{"snippet": "batch_size = 32\nopt = keras.optimizers.rmsprop(lr=0.0005, decay=1e-6)\nmodel_1.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\nmodel_1.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=15,\n              validation_data=(x_test, y_test),\n              shuffle=True)\n", "intent": "We still have 181K parameters, even though this is a \"small\" model.\n"}
{"snippet": "batch_size = 32\nopt = keras.optimizers.rmsprop(lr=0.0005, decay=1e-6)\nmodel_1.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n", "intent": "We still have 181K parameters, even though this is a \"small\" model.\n"}
{"snippet": "feature_cols = ['TV', 'Radio', 'Newspaper', 'IsLarge']\nX = data[feature_cols]\ny = data.Sales\nlm = LinearRegression()\nlm.fit(X, y)\nzip(feature_cols, lm.coef_)\n", "intent": "Let's redo the multiple linear regression and include the **IsLarge** predictor:\n"}
{"snippet": "model = fit_SimpleRNN(train_X, train_y, cell_units=10, epochs=10)\n", "intent": "Great, now let's use this function to fit a very simple baseline model.\n"}
{"snippet": "KNN = KNeighborsClassifier(n_neighbors=5)\n", "intent": "Create a N-Nearest Neighbors classifier of 5 neighbors. (hint: KNeighborsClassifier?)\n"}
{"snippet": "knn = KNN.fit(X_train, y_train)\n", "intent": "Fit the model to the training set. (hint: knn.fit?)\n"}
{"snippet": "from sklearn.ensemble import VotingClassifier\nestimators = [('LR_L2', LR_L2), ('GBC', GV_GBC)]\nVC = VotingClassifier(estimators, voting='soft')\nVC = VC.fit(X_train, y_train)\n", "intent": "And now the stacked model.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression().fit(X_new, Y_new)\n", "intent": " Repeat Model building with new training data after removing higly correlated columns\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    return tf.contrib.layers.embed_sequence(input_data, vocab_size=vocab_size, embed_dim= embed_dim)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "new_docs = ['He watches basketball and baseball', 'Julie likes to play basketball', 'Jane loves to play baseball']\nnew_term_freq_matrix = tfidf_vectorizer.transform(new_docs)\nprint tfidf_vectorizer.vocabulary_\nprint new_term_freq_matrix.todense()\n", "intent": "And we can fit new observations into this vocabulary space like so:\n"}
{"snippet": "lr = LinearRegression().fit(response_design, motor_data)\n", "intent": "**(4)** Fit a linear model with the response design matrix as the independent variables and the voxel data as dependent variables.\n"}
{"snippet": "fake_linear_x5,fake_linear_y5 = create_fake_linear(100, 1, 0, .3)\nfake_linear_x6,fake_linear_y6 = create_fake_linear(100, 2, 0, .3)\nfake_linear_x7,fake_linear_y7 = create_fake_linear(100, .3, 0, .3)\n", "intent": "And now we'll create some fake linear data that has noise.\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators=100, random_state=17)\nrf.fit(X_train, y_train)\n", "intent": "Train a random forest **(RandomForestClassifier)**. Set the number of trees to 100 and use **random_state = 17**.\n"}
{"snippet": "fake_linear_x8, fake_linear_y8 = create_fake_linear(100, 1.5, 5, .5)\nfake_linear_x9, fake_linear_y9 = create_fake_linear(100, 5, -.4, .5)\nfake_linear_x10, fake_linear_y10 = create_fake_linear(100, -2, 2, .5)\n", "intent": "Again we did perfect because there was no noise in this data. Let's add some noise and see how we do.\n"}
{"snippet": "reg_model1 = LinearRegression()\nreg_model1.fit(fake_linear_x10_2D, fake_linear_y10_2D)\n", "intent": "Now we'll create a linear regression object, and use it to fit the fake x and y 2-D data.\n"}
{"snippet": "reg_model2 = LinearRegression()\nreg_model2.fit(fake_linear_x9.reshape(-1,1), fake_linear_y9.reshape(-1,1))\n", "intent": "1\\. Fit a linear regression model to the `fake_linear_x9` and `fake_linear_y9`. Print out the intercept and coefficient.\n"}
{"snippet": "reg_model_faces = LinearRegression()\nreg_model_faces.fit(faces_response_vec, data_faces)\n", "intent": "Now let's apply linear models to some real fMRI data. We'll fit the faces response vector to our FFA voxel and see how we do.\n"}
{"snippet": "reg_model_full = LinearRegression()\nreg_model_full.fit(designmat_full, data_faces_2D)\n", "intent": "Now we'll fit a multiple regression model in the same way.\n"}
{"snippet": "reg_model_simple1 = LinearRegression()\nreg_model_simple1.fit(fake_multi_x1.reshape(-1,1), fake_multiple_y1.reshape(-1,1))\nreg_model_simple2 = LinearRegression()\nreg_model_simple2.fit(fake_multi_x2.reshape(-1,1), fake_multiple_y1.reshape(-1,1))\nprint('Real Coefficient 1: %.03f, Simple Estimated Coefficient 1: %.03f' % (coef_real1, reg_model_simple1.coef_[0][0]))\nprint('Real Coefficient 2: %.03f, Simple Estimated Coefficient 2: %.03f' % (coef_real2, reg_model_simple2.coef_[0][0]))\n", "intent": "Now let's use 2 simple linear models to estimate the two coefficeints separately.\n"}
{"snippet": "designmat_fake_multiple = np.stack((fake_multi_x1, fake_multi_x2), axis=1)\nreg_model_multi1 = LinearRegression()\nreg_model_multi1.fit(designmat_fake_multiple, fake_multiple_y1.reshape(-1,1))\nprint('Real Coefficient 1: %.03f, Simple Estimated Coefficient 1: %.03f' % (coef_real1, reg_model_multi1.coef_[0][0]))\nprint('Real Coefficient 2: %.03f, Simple Estimated Coefficient 2: %.03f' % (coef_real2, reg_model_multi1.coef_[0][1]))\n", "intent": "That's not so great. Now let's see what happens when we put both independent variables in the model.\n"}
{"snippet": "reg_model_full = LinearRegression()\nreg_model_full.fit(designmat_full, five_voxels)\n", "intent": "Now we'll fit a multiple regression model in the same way.\n"}
{"snippet": "reg_model_simple1 = LinearRegression()\nreg_model_simple1.fit(fake_multi_x1.reshape(-1,1), fake_multiple_y1.reshape(-1,1))\nreg_model_simple2 = LinearRegression()\nreg_model_simple2.fit(fake_multi_x2.reshape(-1,1), fake_multiple_y1.reshape(-1,1))\nprint('Real Coefficient 1: %.03f, Simple Estimated Coefficient 1: %.03f' % (coef_real1, reg_model_simple1.coef_[0][0]))\nprint('Real Coefficient 2: %.03f, Simple Estimated Coefficient 2: %.03f' % (coef_real2, reg_model_simple2.coef_[0][0]))\n", "intent": "Now let's use 2 simple linear models to estimate the two coefficients separately.\n"}
{"snippet": "lr = LogisticRegression(random_state=5, class_weight='balanced')\n", "intent": "Now, we will create a `LogisticRegression` model and use `class_weight='balanced'` to make up for our unbalanced classes.\n"}
{"snippet": "fake_noise_sd = 5\nfake_x2, fake_y2 = create_fake_linear(fake_n, fake_slope, fake_intercept, fake_noise_sd)\n", "intent": "1\\. Run the cell below to make some new fake data that has more noise. \n"}
{"snippet": "fake_x, fake_y = create_fake_linear(fake_n, fake_slope, fake_intercept, fake_noise_sd)\n", "intent": "Generate some fake data using the values above\n"}
{"snippet": "model_fake = LinearRegression()\nmodel_fake.fit(fake_x_train, fake_y_train)\nprint('Original Data')\nprint('The estimated weight is: %.04f' % (model_fake.coef_[0][0]))\n", "intent": "Fit the model on the fake training data\n"}
{"snippet": "model_fake_outlier = LinearRegression()\nmodel_fake_outlier.fit(fake_x_train, fake_y_train_outlier)\nprint('\\nOutlier Training Data')\nprint('The estimated weight is: %.04f' % (model_fake_outlier.coef_[0][0]))\n", "intent": "Fit the model on the fake training data that has an outlier\n"}
{"snippet": "model_gradient = LinearRegression()\nmodel_gradient.fit(smoothed[:1260].reshape(1260, -1), data_train)\n", "intent": "It doesn't really look like much! But you can see roughly where the seal was.\nNow we'll use this image as an encoding model!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded_sequence = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev = 0.05))\n    embedded_sequence = tf.nn.embedding_lookup(embedded_sequence, input_data)\n    return embedded_sequence\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "svc_linear = LinearSVC()\nsvc_linear.fit(X_train, y_train)\nprint('Linear SVC classification accuracy on training set: {:.3f}'.format(svc_linear.score(X_train, y_train)))\nprint('Linear SVC classification accuracy on test set: {:.3f}'.format(svc_linear.score(X_test, y_test)))\n", "intent": "Train a [`LinearSVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n"}
{"snippet": "svc_rbf = SVC(kernel='rbf')\nsvc_rbf.fit(X_train, y_train)\nsvc_poly = SVC(kernel='poly', degree=2)\nsvc_poly.fit(X_train, y_train)\nprint('RBF SVC classification accuracy on training set: {:.3f}'.format(svc_rbf.score(X_train, y_train)))\nprint('RBF SVC classification accuracy on test set: {:.3f}'.format(svc_rbf.score(X_test, y_test)))\nprint('Poly SVC classification accuracy on training set: {:.3f}'.format(svc_poly.score(X_train, y_train)))\nprint('Poly SVC classification accuracy on test set: {:.3f}'.format(svc_poly.score(X_test, y_test)))\n", "intent": "By using the [`SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"}
{"snippet": "train_sizes = np.linspace(0.05, 1, 20)\nN_train, val_train, val_test = learning_curve(SVC(kernel='linear', C=1E-6),\n                                              X_proj, y, train_sizes)\n", "intent": "These scores are statistically equivalent: it seems that our projection is not losing significant information relevant to the classification!\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42, \n                            class_weight='balanced')\n", "intent": "Initialize Random Forest with 100 trees and balance target classes:\n"}
{"snippet": "def forward_propagation(self, x):\n    T = len(x)\n    s = np.zeros((T + 1, self.hidden_dim))\n    s[-1] = np.zeros(self.hidden_dim)\n    o = np.zeros((T, self.word_dim))\n    for t in np.arange(T):\n        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n        o[t] = softmax(self.V.dot(s[t]))\n    return [o, s]\nRNNNumpy.forward_propagation = forward_propagation\n", "intent": "Next, let's implement the forward propagation (predicting word probabilities) defined by our equations above:\n"}
{"snippet": "model.fit(x_train, y_train, epochs=1, batch_size=100, validation_split=0.1, shuffle=True)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "with train_graph.as_default():\n    saver = tf.train.Saver()\nwith tf.Session(graph=train_graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints_large'))\n    embed_mat = sess.run(embedding)\n", "intent": "Restore the trained network if you need to:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1), name='embedding')\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = tflearn.DNN(net, tensorboard_verbose=1)\nmodel.fit({'input': X}, {'target': Y}, n_epoch=20,\n           validation_set=({'input': testX}, {'target': testY}),\n           snapshot_step=100, show_metric=True, run_id='convnet_mnist')\n", "intent": "Train the network and use the test data as the validation set. \n"}
{"snippet": "base_model = applications.ResNet50(weights = \"imagenet\", include_top=False, input_shape = (224, 224, 3))\nx = base_model.output\nx = Flatten()(x)\nx = Dense(15000, activation=\"softmax\")(x)\nmodel_ResNet50 = Model(input = base_model.input, output = x)\nfor layer in base_model.layers:\n    layer.trainable = False\nmodel_ResNet50.summary()\n", "intent": "ResNet-50, Inception, and Xception.\n"}
{"snippet": "train_dataset = dsets.MNIST(root='../data', \n                            train=True, \n                            transform=transforms.ToTensor(),  \n                            download=True)\ntest_dataset = dsets.MNIST(root='../data', \n                           train=False, \n                           transform=transforms.ToTensor())\n", "intent": "Now, we'll load the MNIST data. First time we may have to download the data, which can take a while.\n"}
{"snippet": "train_dataset = dsets.MNIST(root='./data',\n                           train=True,\n                           transform=transforms.ToTensor(),\n                           download=True)\ntest_dataset = dsets.MNIST(root='./data',\n                           train=False,\n                           transform=transforms.ToTensor())\n", "intent": "Now, we'll load the MNIST data\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "intent": "Loading a pretrained model and reseting final fully connected layer.\n"}
{"snippet": "linreg = LinearRegression()\nlinreg.fit(X_train_scaled, y_train);\n", "intent": "**Train a simple linear regression model (Ordinary Least Squares).**\n"}
{"snippet": "from sklearn import manifold\niso = manifold.Isomap(n_neighbors=6, n_components=2)\niso.fit(X)\nmanifold_data = iso.transform(X)\nmanifold_data.shape\n", "intent": "**Excerise 3:** Apply ISOMAP on the data\n"}
{"snippet": "iso = manifold.Isomap(n_neighbors=6, n_components=2)\niso.fit(X)\nmanifold_data = iso.transform(X)\n", "intent": "**Excerise 2:** Apply ISOMAP on the data\n"}
{"snippet": "lle_data, _ = manifold.locally_linear_embedding(X, n_neighbors=12, n_components=2)\n", "intent": "**Excerise 3:** Apply LLE on the data\n"}
{"snippet": "lm = smf.ols(formula='sale_price ~ gross_sq_feet -1', data = REsample1).fit()\nprint(lm.summary())\n", "intent": "This is already pretty consistend with what python will give us if we take the entire training sample at once\n"}
{"snippet": "model.fit(X_train, y_train)\n", "intent": "We can use the `%%timeit` magic function to time how long a cell takes.\n"}
{"snippet": "model = KNeighborsClassifier(2)\nmodel.fit(X_r_train, y_r_train)\n", "intent": "Pick your favorite algorithm and time how long it takes to train\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nmodel_lr = LogisticRegression()\n", "intent": "$$ \\frac{1}{2m} \\sum_i (h(x_i) - y_i)^2 \\text{ mit } h(x) = m*x + t$$\n$$  $$\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.truncated_normal((vocab_size, embed_dim),\n                                               stddev=0.1))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "m1 = ols('PRICE ~ PTRATIO', bos).fit()\nprint(m1.summary())\n", "intent": "The scatterplot between price and ptratio shows no clear relationship\n"}
{"snippet": "lasso1 = Lasso(alpha=0.01, random_state=17)\nlasso1.fit(X_train_scaled, y_train)\n", "intent": "**Train a LASSO model with $\\alpha = 0.01$ (weak regularization) and scaled data. Again, set random_state=17.**\n"}
{"snippet": "cv_lin.fit(X_train_scaled, y_train.values.ravel())\n", "intent": "Gaussian SVC yields 29% accuracy, which is poor, I will try linear SVC to see if that improves prediction accuracy\n"}
{"snippet": "knn_params = {'n_neighbors': (range(1,100)), 'weights': ['uniform','distance'], 'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'] \n}\ngrid = GridSearchCV(neighbors.KNeighborsClassifier(), knn_params)\ngrid.fit(X_norm,y)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "ada = AdaBoostRegressor() \nparam_grid2 = { \n    'n_estimators': [5,50,100,200,300],\n    'learning_rate': [0.5, 1.0, 1.5]\n}\nCV_ada = GridSearchCV(estimator=ada, param_grid=param_grid2, cv= 5)\nCV_ada.fit(X_train2, y_train2)\nprint CV_ada.best_params_\n", "intent": "(base_estimator=None, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n"}
{"snippet": "knn = NearestNeighbors(n_neighbors=3)\nknn.fit(trainNorm[['zIncome', 'zLot_Size']])\ndistances, indices = knn.kneighbors(newHouseholdNorm)\nprint(trainNorm.iloc[indices[0], :])  \n", "intent": "Use k-nearest neighbour\n"}
{"snippet": "regressor.fit(X_train, y_train)\n", "intent": "`fit` learns the model parameters.\n"}
{"snippet": "from keras.layers import Dropout, Flatten, Dense\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(n_classes))\nmodel.add(Activation('softmax'))\n", "intent": "Then we can add dropout and our dense and output (softmax) layers.\n"}
{"snippet": "model = Sequential()\nmodel.add(LSTM(512, return_sequences=True, input_shape=(max_len, len(chars))))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(512, return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(len(chars)))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n", "intent": "Now we'll define our RNN. Keras makes this trivial:\n"}
{"snippet": "epochs = 10\nmodel.fit(X, y, batch_size=128, nb_epoch=epochs)\n", "intent": "Now that we have our training data, we can start training. Keras also makes this easy:\n"}
{"snippet": "model = build_model(one_hot=False, bidirectional=True)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit_generator(generator=generate_batches(batch_size, one_hot=False), samples_per_epoch=n_examples, nb_epoch=n_epochs, verbose=1)\n", "intent": "And we can try the bidirectional variations, e.g.\n"}
{"snippet": "alphas = np.logspace(-6, 2, 200)\nlasso_cv = LassoCV(random_state=17, cv=5, alphas=alphas)\nlasso_cv.fit(X_train_scaled, y_train)\n", "intent": "**Train LassoCV with random_state=17 to choose the best value of $\\alpha$ in 5-fold cross-validation.**\n"}
{"snippet": "hidden_linout = linear(X, hidden_layer_weights, hidden_layer_biases)\nhidden_output = activation(hidden_linout)\nprint('hidden output')\nprint(hidden_output)\n", "intent": "Now we can do a forward pass with our inputs $X$ to see what the predicted outputs are.\nFirst, we'll pass the input through the hidden layer:\n"}
{"snippet": "from scipy.spatial.distance import cdist\nignore_n_most_common = 50\ndef get_closest(word):\n    embedding = get_embedding(word)\n    distances = cdist(embedding, embeddings)[0]\n    distances = list(enumerate(distances))\n    distances = sorted(distances, key=lambda d: d[1])\n    for idx, dist in distances[1:]:\n        if idx > ignore_n_most_common:\n            return reverse_word_index[idx]\n", "intent": "Then we can define a function to get a most similar word for an input word:\n"}
{"snippet": "X = theano.shared(value=np.asarray([[0, 1], [1, 0], [0, 0], [1, 1]]), name='X')\ny = theano.shared(value=np.asarray([[0], [0], [1], [1]]), name='y')\nprint('X: {}\\ny: {}'.format(X.get_value(), y.get_value()))\n", "intent": "Initialize our input data `X` and output data `y`\n"}
{"snippet": "def layer(*shape):\n    assert len(shape) == 2\n    mag = 4. * np.sqrt(6. / sum(shape))\n    W_value = np.asarray(rng.uniform(low=-mag, high=mag, size=shape), dtype=theano.config.floatX)\n    b_value = np.asarray(np.zeros(shape[1], dtype=theano.config.floatX), dtype=theano.config.floatX)\n    W = theano.shared(value=W_value, name='W_{}'.format(shape), borrow=True, strict=False)\n    b = theano.shared(value=b_value, name='b_{}'.format(shape), borrow=True, strict=False)\n    return W, b\n", "intent": "A helper method for generating the matrices (as Theano shared variables) for a single layer can be written as follows.\n"}
{"snippet": "input_layer = Input(shape=(2,))\nhidden_layer = Dense(5, activation='relu')(input_layer)\noutput_layer = Dense(1, activation='sigmoid')(hidden_layer)\n", "intent": "Next, let's define our model, the same way we defined it in the Theano example.\n"}
{"snippet": "def get_weights(n_in, n_out):\n    mag = 4. * np.sqrt(6. / (n_in + n_out))\n    W_value = np.asarray(rng.uniform(low=-mag, high=mag, size=(n_in, n_out)), dtype=dtype)\n    W = theano.shared(value=W_value, name='W_%d_%d' % (n_in, n_out), borrow=True, strict=False)\n    return W\ndef get_bias(n_out):\n    b_value = np.asarray(np.zeros((n_out,), dtype=dtype), dtype=theano.config.floatX)\n    b = theano.shared(value=b_value, name='b_%d' % n_out, borrow=True, strict=False)\n    return b\n", "intent": "Next, let's make some helper methods for generating weights as shared variables, like we did with the XOR example.\n"}
{"snippet": "print(X_train[0].shape)\nmodel = Sequential()\nmodel.add(Conv2D(filters=18, kernel_size=(3,3), strides=1, padding='same', activation='relu', input_shape=X_train[0].shape))\nmodel.add(MaxPool2D(pool_size=3, padding='valid'))\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(train_labels.shape[1], activation='softmax'))\nprint(model.summary())  \n", "intent": "Design your network here using Keras\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(64, input_shape=X_train[0].shape, activation='relu'))   \nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(train_labels[0].shape[0], activation='softmax'))   \nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\nmodel.summary()\n", "intent": "Design your network here using Keras\n"}
{"snippet": "for layer in seq_model.layers:\n    layer.trainable = False\nseq_model.summary()\nseq_model.add( Flatten(name=\"my_flatten\") )\nseq_model.add( Dense(8, activation=\"relu\", name=\"predictions\") )\nseq_model.summary()\n", "intent": "Freeze the layers that remain in our model.\n"}
{"snippet": "forest = RandomForestRegressor(random_state=17)\nforest.fit(X_train_scaled, y_train)\n", "intent": "**Train a Random Forest with out-of-the-box parameters, setting only random_state to be 17.**\n"}
{"snippet": "feature_columns = learn.infer_real_valued_columns_from_input(data)\nclassifier = learn.LinearClassifier(feature_columns=feature_columns, n_classes=10)\nclassifier.fit(data, labels, batch_size=100, steps=1000)\n", "intent": "Our goal here is to get about 90% accuracy with this simple classifier.\n"}
{"snippet": "regr = LogisticRegression()\nX_test = test_dataset.reshape(test_dataset.shape[0], 28 * 28)\ny_test = test_labels\n", "intent": "I have already used scikit-learn in a previous MOOC. It is a great tool, very easy to use!\n"}
{"snippet": "regr2 = LogisticRegression(solver='sag')\nsample_size = len(train_dataset)\nX_train = train_dataset[:sample_size].reshape(sample_size, 784)\ny_train = train_labels[:sample_size]\nregr2.score(X_test, y_test)\n", "intent": "To train the model on all the data, we have to use another solver. SAG is the faster one.\n"}
{"snippet": "clf = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=1000)\n", "intent": "Now, we can initialize the RandomForestClassifier and fit a model to our training data. Use `n_estimators=100` to use 100 different trees.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train,y_train)\n", "intent": "Now its time to train our model!\n**Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.**\n"}
{"snippet": "X = college_data.drop(['Unnamed: 0', 'Private'],axis=1)\nkm.fit(X)\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "k = 1\nknn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "logreg = linear_model.LogisticRegression()\nlogreg.fit(X_train, Y_train)\n", "intent": "We will train the classifier the exact same way we did previously with the example.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel\n", "intent": "First, we create the model using the Sklearn LinearRegression model.\n"}
{"snippet": "forest_params = {'max_depth': list(range(10, 25)), \n                  'max_features': list(range(6,12))}\nlocally_best_forest = GridSearchCV(RandomForestRegressor(n_jobs=-1, random_state=17), \n                                 forest_params, \n                                 scoring='neg_mean_squared_error',  \n                                 n_jobs=-1, cv=5,\n                                  verbose=True)\nlocally_best_forest.fit(X_train_scaled, y_train)\n", "intent": "**Tune the `max_features` and `max_depth` hyperparameters with GridSearchCV and again check mean cross-validation MSE and MSE on holdout set.**\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X, y)\nscore = model.score(X, y)\nprint(f\"R2 Score: {score}\")\n", "intent": "Luckily, we can just supply our n-dimensional features and sklearn will fit the model using all of our features.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier.verbose=2\nclassifier\n", "intent": "Create a Logistic Regression Model\n"}
{"snippet": "voicemodel = LogisticRegression()\nvoicemodel\n", "intent": "Create a Logistic Regression Model\n"}
{"snippet": "voicemodel.fit(X_train,y_train)\n", "intent": "Fit (train) or model using the training data\n"}
{"snippet": "ctb.fit(X_train_part, y_train_part,\n        cat_features=categ_feat_idx)\n", "intent": "**Train Catboost without setting hyperparameters, passing only the indexes of categorical features.**\n"}
{"snippet": "input_X = T.tensor4()\ntarget_y = T.matrix(dtype='int64')\nweights = T.vector(\"weights\", dtype='float32') \nlearning_rate = T.scalar(name='learning_rate') \n", "intent": "https://github.com/Lasagne/Recipes/blob/master/modelzoo/Unet.py\n"}
{"snippet": "z_sample = T.matrix()\ninput_sample = InputLayer(input_shape, input_var=z_sample)\nl_dec_sample = DenseLayer(input_sample, HU_decoder, W=l_dec.W, b=l_dec.b)\nl_out_sample = DenseLayer(l_dec_sample, image_h * image_w * 3, W=l_out.W, b=l_out.b, nonlinearity=sigmoid)\ngenerated_x = lasagne.layers.get_output(l_out_sample)\ngen_fn = theano.function([z_sample], generated_x)\n", "intent": "This task requires deeper Lasagne knowledge. You need to perform inference from $z$, reconstruct an image given some random $z$ representations.\n"}
{"snippet": "mu_x = lasagne.layers.get_output(l_dec_mu)\nsigma_x = lasagne.layers.get_output(l_dec_logsigma)\nmu_z, sigma_z = lasagne.layers.get_output(l_enc_mu), lasagne.layers.get_output(l_enc_logsigma)\nloss_kl = - KL(mu_z, sigma_z)\nloss_ll = - log_likelihood(input_X, mu_x, sigma_x)\nloss = loss_kl + loss_ll\nparams = lasagne.layers.get_all_params(l_dec_mu, trainable=True) + [l_dec_logsigma.W, l_dec_logsigma.b]\nupdates = lasagne.updates.adam(loss, params)\ntrain_fn = theano.function([input_X], [loss_kl, loss_ll], updates=updates, allow_input_downcast=True)\ntest_fn = theano.function([input_X], lasagne.layers.get_output(l_dec_mu, deterministic=True), allow_input_downcast=True)\n", "intent": "Now build the loss and training function:\n"}
{"snippet": "import theano\nimport theano.tensor as T\nstates = T.matrix(\"states[batch,units]\")\nactions = T.ivector(\"action_ids[batch]\")\ncumulative_rewards = T.vector(\"R[batch] = r + gamma*r' + gamma^2*r'' + ...\")\n", "intent": "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n"}
{"snippet": "get_policy = theano.function([observations],probs,allow_input_downcast=True)\ndef act(obs, sample=True):\n    policy = get_policy([obs])[0]\n    if sample:\n        action = int(np.random.choice(n_actions,p=policy))\n    else:\n        action = int(np.argmax(policy))\n    return action, policy\n", "intent": "In this section, we'll define functions that take actions $ a \\sim \\pi_\\theta(a|s) $ and rollouts $ <s_0,a_0,s_1,a_1,s_2,a_2,...s_n,a_n> $.\n"}
{"snippet": "xgb_param_grid = {}\nxgb_model = train_model(\n    xgb.XGBClassifier(),\n    xgb_param_grid,\n    train_predictors, \n    train_outcome\n)\n", "intent": "Gradient boosting algorithms surprisingly did very poor job when there are not many features\n"}
{"snippet": "dt_param_grid = {}\ndt_model = train_model(\n    DecisionTreeClassifier(max_depth=9, min_samples_split=2, min_samples_leaf=3, criterion=\"entropy\"),\n    dt_param_grid,\n    train_predictors, \n    train_outcome\n)\n", "intent": "So far, this model gives us the best score: **1426**\n"}
{"snippet": "print('Checking the Training on a Single Batch...')\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(epochs):\n        batch_i = 1\n        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n", "intent": "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch.\n"}
{"snippet": "if 'my_vgg' in globals():\n    print('\"vgg\" object already exists.  Will not create again.')\nelse:\n    with tf.Session() as sess:\n        input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\n        vgg = vgg16.Vgg16()\n        vgg.build(input_)\n", "intent": "Below, feel free to choose images and see how the trained classifier predicts the flowers in them.\n"}
{"snippet": "ctb.fit(X_train, y_train,\n        cat_features=categ_feat_idx)\n", "intent": "**Train on the whole train set, make prediction on the test set. We got 0.73008 in the competition.**\n"}
{"snippet": "history = model2.fit([train_matrix,train_matrix_feature_index], \n          [train_labels],\n          nb_epoch=50,\n          verbose=2,\n          batch_size = 128,\n         shuffle = True,\n         validation_split=0.2)\n", "intent": "**Observation **\nSolution is converaging better. More epochs would result in better solution\n"}
{"snippet": "g = Graph()\n", "intent": "z = Ax + b\nA = 10\nb = 1\nz = 10x + 1\n"}
{"snippet": "kmean_clustering = KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "kmean_clustering.fit(df.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "KNN_model = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "KNN_model.fit(X_train,y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "sess = tf.Session()\nval_m2 = np.array([[2],[3]])\nres = sess.run(fetches=[product], feed_dict={m2: val_m2}) \nprint(res)\nsess.close()\n", "intent": "Provide the correct feeds (inputs) and fetches (outputs of the computational graph)\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(init_op)\n    res_val = sess.run(out, feed_dict={x:X[0:2]})\nres_val\n", "intent": "Since we fixed the random seed, you should you should get a result like:\n"}
{"snippet": "tf.reset_default_graph()\nX_ = tf.placeholder(name='X', shape=(None, 2), dtype='float32')\nW = tf.get_variable('W', shape=(2,1))\nX_rec_ = tf.matmul(tf.matmul(X_, W), tf.transpose(W))\nloss_ = tf.reduce_sum((X_rec_ - X_)**2)\n", "intent": "Calculate the PCA in tensorflow, by reducing the reconstruction error as shown above using TensorFlow.\n"}
{"snippet": "param_grid={'n_estimators': [500],\n            'learning_rate': np.logspace(-4, 0, 100),\n            'max_depth':  [10],\n            'num_leaves': [72],\n            'reg_lambda': [0.0010722672220103231],\n            'random_state': [17]}\ngs=RandomizedSearchCV(model_gb, param_grid, n_iter = 20, scoring='neg_mean_squared_error', fit_params=None, \n                n_jobs=-1, cv=kf, verbose=1, random_state=17)\ngs.fit(X_trees,y)\n", "intent": "Let's fix n_estimators=500, it is big enough but is not to computationally intensive yet, and find the best value of the learning_rate\n"}
{"snippet": "scaler.fit(bnote.drop('Class',axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "feature_columns = [contrib.layers.real_valued_column(\"\", dimension=1)]\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "lm = LinearRegression()\nlm\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "happyModel.fit(X_train, Y_train, epochs=10, batch_size=16)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model.fit([Xoh, s0, c0], outputs, epochs=5, batch_size=100)\n", "intent": "Let's now fit the model and run it for one epoch.\n"}
{"snippet": "print (\"Training started\")\nmodel.fit(x_train, y_train, epochs=30, batch_size=512, verbose=2)\nprint (\"Training completed\")\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "body_reg = linear_model.LinearRegression()\nbody_reg.fit(xVals, yVals)\n", "intent": "First we build a regression model using `sklearn`, and then we fit the model to our dependent variables, the brain weight.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, activation='relu', input_shape=(x_train.shape[1],)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss = 'mean_squared_error', optimizer='rmsprop', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "warnings.filterwarnings('ignore')\nmlog.fit(X_train,  y_train)\n", "intent": "Let's try to predict\n"}
{"snippet": "history = model.fit(Xsmall, ysmall, batch_size=500, epochs=40,verbose = 1)\nmodel.save_weights('model_weights/best_RNN_small_textdata_weights.hdf5')\n", "intent": "Now lets fit our model!\n"}
{"snippet": "model2 = models.Sequential()\nmodel2.add(layers.Dense(10, activation=\"relu\", input_dim=X.shape[1]))\nmodel2.add(layers.Dense(3, activation=\"softmax\"))\nmodel2.compile(optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"])\nmodel2.fit(X, y0, epochs=500, validation_split=0.2)\n", "intent": "Now let's fit the same model but this time encoding the data as integers\n"}
{"snippet": "model_features = models.Sequential()\nmodel_features.add(layers.Dense(1, input_dim=2, activation=\"sigmoid\"))\nmodel_features.compile(optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"])\nmodel_features.fit(X_features, y, epochs=500, validation_split=0.2)\n", "intent": "Now let's fit a logistic regression.\n"}
{"snippet": "nn_tanh = models.Sequential()\nnn_tanh.add(layers.Dense(5, input_dim=2, activation=\"tanh\"))\nnn_tanh.add(layers.Dense(5, activation=\"tanh\"))\nnn_tanh.add(layers.Dense(1, activation=\"sigmoid\"))\n", "intent": "Alternatively we could use a tanh activation function.\n"}
{"snippet": "from keras import layers\nfrom keras import models\nmodel = models.Sequential()\nmodel.add(layers.Dense(128, activation=\"relu\", input_dim=28 * 28))\nmodel.add(layers.Dense(32, activation=\"relu\"))\nmodel.add(layers.Dense(10, activation=\"softmax\"))\n", "intent": "We set up the same model as Chapter 6.5.\n"}
{"snippet": "model.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nhist_MNIST = model.fit(train_images, train_labels, epochs=20, batch_size=64,\n                       callbacks = callbacks_list,\n                       validation_data=(val_images, val_labels))\n", "intent": "Now we run the model using early stopping.\n"}
{"snippet": "ninputs = x_train.shape[1]\nlayer_sizes = [5, 3, 1] \nbiases = [tfe.Variable(np.random.normal(scale=0.05, size=n), dtype=tf.float32) for n in layer_sizes]\nweight_dims = zip([ninputs]+layer_sizes[0:-1], layer_sizes) \nweights = [tfe.Variable(np.random.normal(scale=0.05, size=s), dtype=tf.float32) for s in weight_dims]\nvariables = biases+weights\n", "intent": "Next we create the variables we are going to use. Their shapes depend on the architecture of our network so we also specify this.\n"}
{"snippet": "sess = tf.Session()\nsess.run(init)\n", "intent": "Next we start a Tensorflow session and initialise the variables.\n"}
{"snippet": "regr = LinearRegression()\nregr.fit(X_train, y_train)\n", "intent": "d) Train model on training data, and make predictions on testing data\n"}
{"snippet": "mlog2.fit(X_train,  y_train)\n", "intent": "Let's check this out\n"}
{"snippet": "from sklearn import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nclf =\n", "intent": "<h3>Train a Decision Tree Classifier</h3>\n"}
{"snippet": "from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(features, labels)\n", "intent": "<h3>Train a Decision Tree Classifier</h3>\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nclf.fit(X_train, Y_train)\n", "intent": "<h3>Train the Decision Tree Model</h3>\n"}
{"snippet": "A_days = students['A']['days']\nA_days = A_days[:, np.newaxis]\nA_morale = students['A']['morale']\nA_model = LinearRegression()\nA_model.fit(A_days, A_morale)\nprint A_model.intercept_, A_model.coef_\n", "intent": "---\nI decide to measure the morale with a linear regression, predicting it from the number of days. \nMy model is:\n"}
{"snippet": "mean_knn_n2 = KNeighborsClassifier(\n    n_neighbors=1,       \n    weights='uniform'    \n)\naccuracy_crossvalidator(X, y, mean_knn_n2, cv_indices)\n", "intent": "---\nAs you can see the mean cross-validated accuracy is very high with 5 neighbors. \nLet's see what it's like when we use only 1 neighbor:\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nX = affair[['age','religious']]\ny = affair.nbaffairs.values\nknn_uniform_n3 = KNeighborsClassifier(n_neighbors=50, weights='uniform')\nknn_uniform_n3.fit(X, y)\n", "intent": "---\nYou should choose **2 predictor variables** to predict had affair vs. not\n"}
{"snippet": "pipe.fit(X_test, y_test)\n", "intent": "Fit the pipeline with the training data, then score it on the testing data:\n"}
{"snippet": "def cluster_plotter(eps=1.0, min_samples=5):\n    dbkk = DBS(eps=eps, min_samples=min_samples)\n    dbkk.fit(X)\n    plot_clusters(X, dbkk.cluster_labels)\n", "intent": "---\nDon't pass `X` in to the function. We will just use the \"global\" X defined in the jupyter notebook earlier.\n"}
{"snippet": "graph = PredictedValuesGraph( table51 )\n", "intent": "N.B. Fractals gone from the top 50\n"}
{"snippet": "rf = RandomForestRegressor(n_estimators=300, random_state=42).fit(X_ohe_train_rf, to_log(y_train_2))\n", "intent": "Now, let's try RandomForest with default parameters on the same features (but without polynomial features):\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embedding,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_normal([vocab_size, embed_dim], stddev=0.1), name='embeddings')\n    embed = tf.nn.embedding_lookup(embeddings, input_data, name='embed')\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier().fit(train_data[['Sex', 'Age', 'Fare']], train_data.Survived)\ntree.score(test_data[['Sex', 'Age', 'Fare']], test_data.Survived)\n", "intent": "We'll use a simple Decision Tree model to predict if a passenger would survive the Titanic by making use of its gender, age and fare.\n"}
{"snippet": "lasso_reg = Lasso(alpha=1.0)\nlasso_reg.fit(X, y)\n", "intent": "$$\\frac{1}{2n} ||y - Xw||^2_2 + \\alpha ||w||_1$$\n"}
{"snippet": "LR = linear_model.LogisticRegression()\n", "intent": "Things tried but not used\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "> Note: this is only compatible with `contrib` feature_columns\n"}
{"snippet": "with tf.Session() as sess:\n    print(tf.nn.embedding_lookup(word_vector, first_sentence).eval().shape)\n", "intent": "The 10 x 50 output should contain the 50 dimensional word vectors for each of the 10 words in the sequence. \n"}
{"snippet": "for data in rand_loader:\n    input = data.to(device)\n    output = model(input)\n    print(\"Outside: input size\", input.size(),\n          \"output_size\", output.size())\n", "intent": "Run the Model\n-------------\nNow we can see the sizes of input and output tensors.\n"}
{"snippet": "nnr = MLPRegressor(max_iter = 1000)\nparameters = {'hidden_layer_sizes':[5, 100, [5, 10], [100, 50]], \n              'alpha':[1e-1, 1e-3, 1e-5], \n            'batch_size':[50, 500, 2000]}\ngcnn = GridSearchCV(nnr, parameters, cv=4)\ngcnn.fit(...)\nresults = gcnn.cv_results_\n...\n...\n", "intent": "Do a grid search on different architecture of layers (number and sizes), batch sizes, and regularizing strength and find the top 10 models.\n"}
{"snippet": "rf_best = RandomForestRegressor(n_estimators=300, max_depth=best_max_depth, random_state=42)\n", "intent": "Finnaly, let's try to fit RandomForestRegressor with best **max_depth** value, found due to cross-validation. We'll use the same datasets.\n"}
{"snippet": "mlpc = MLPClassifier(max_iter = 1000)\nparameters = {'hidden_layer_sizes':[5, 100, [5, 10], [100, 50]], \n              'alpha':[1e-1, 1e-3, 1e-5], \n            'batch_size':[50, 500, 2000]}\ngcnnc = GridSearchCV(mlpc, parameters, cv=4)\ngcnnc.fit(...)\nresults = gcnnc.cv_results_\n...\n...\n", "intent": "Do a grid search on hidden layers, batch and regularization to get the best model\n"}
{"snippet": "model.add(Dense(num_classes, activation='softmax'))\n", "intent": "Lastly, we need to add a soft-max layer since we have a multi-class output.\n"}
{"snippet": "nnr = MLPRegressor(max_iter = 1000)\nparameters = {'hidden_layer_sizes':[5, 100, [5, 10], [100, 50]], \n              'alpha':[1e-1, 1e-3, 1e-5], \n            'batch_size':[50, 500, 2000]}\ngcnn = GridSearchCV(nnr, parameters, cv=4)\ngcnn.fit(*trainn)\nresults = gcnn.cv_results_\n", "intent": "Do a grid search on different architecture of layers (number and sizes), batch sizes, and regularizing strength and find the top 10 models.\n"}
{"snippet": "mlpc = MLPClassifier([30, 10], max_iter = 1000)\nparameters = {'activation':['relu', 'logistic', 'identity']}\ngcnnc = GridSearchCV(mlpc, parameters, cv=4)\ngcnnc.fit(*traincn)\nresults = gcnnc.cv_results_\nprint(results['mean_test_score'])\n", "intent": "*First, Lets confirm which activation function works the best for classification. Is the difference as significant as for the regression problem?*\n"}
{"snippet": "mlpc = MLPClassifier(max_iter = 1000)\nparameters = {'hidden_layer_sizes':[5, 100, [5, 10], [100, 50]], \n              'alpha':[1e-1, 1e-3, 1e-5], \n            'batch_size':[50, 500, 2000]}\ngcnnc = GridSearchCV(mlpc, parameters, cv=4)\ngcnnc.fit(*traincn)\nresults = gcnnc.cv_results_\n", "intent": "Do a grid search on hidden layers, batch and regularization to get the best model\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding=tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)    \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "train_model(\n    learning_rate=0.0002,\n    steps=100,\n    batch_size=32, input_feature = 'population'\n)\n", "intent": "See if you can do any better by replacing the `total_rooms` feature with the `population` feature.\nDon't take more than 5 minutes on this portion.\n"}
{"snippet": "train_model(\n    learning_rate=0.00002,\n    steps=1000,\n    batch_size=5,\n    input_feature=\"population\"\n)\n", "intent": "Click below for one possible solution.\n"}
{"snippet": "california_housing_dataframe[\"rooms_per_person\"] = (\n    california_housing_dataframe[\"total_rooms\"] / california_housing_dataframe[\"population\"])\ncalibration_data = train_model(\n    learning_rate=0.05,\n    steps=500,\n    batch_size=5,\n    input_feature=\"rooms_per_person\")\n", "intent": "Click below for a solution.\n"}
{"snippet": "grid.fit(X_train.iloc[:500000,:], y_train.iloc[:500000])\n", "intent": "There we take only 500 000 teams out of 1 500 000. As we will see further (on learning curve), it's enough to find best params.\n"}
{"snippet": "from sklearn.cluster import KMeans\nimport numpy as np\nx_cols = df_table.values\nx_list = list(range(2,11))\nSS_list = [KMeans(n_clusters=k).fit(x_cols).inertia_ for k in x_list]\n", "intent": "Lower values of $SS$ ought to represent better clusterings as we don't want points that are excessively far from their respective cluster centroids.\n"}
{"snippet": "lm = LinearRegression(fit_intercept = False)\nlm.fit(X, bos.PRICE)\n", "intent": "**Your turn:** How would you change the model to not fit an intercept term? Would you recommend not having an intercept?\n"}
{"snippet": "h = activation(torch.mm(features, W1) + B1)\noutput = activation(torch.mm(h, W2) + B2)\nprint(output)\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "import helper\nimages, labels = next(iter(trainloader))\nimg = images[0].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, input_dim=1000))\nmodel.add(Activation('softmax'))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('sigmoid'))\nmodel.summary()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, nb_epoch=1000, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(490, activation='relu', input_dim=1000))\nmodel.add(Dense(num_classes, activation='sigmoid'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(2, input_dim=x_train.shape[1]))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "start = time() \nmodel.fit(x_train, y_train, epochs=5, batch_size=10, verbose=2)\nprint(time() - start)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "cat_boost = CatBoostClassifier(random_seed=17, iterations=10)\ncat_boost.fit(X_train, y_train, verbose=False, plot=True);\n", "intent": "Try a CatBoostClassifier.\n"}
{"snippet": "model_breast = KNeighborsClassifier(n_neighbors=6)\nmodel_breast.fit(X_train, y_train)\n", "intent": "Create a KNN classifier with six neighbors and train it with the appropriate data.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n", "intent": "Then we can create a linear model and train it on the right dataset.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\n", "intent": "Now let's use a decision tree on a real dataset.\n"}
{"snippet": "svc_minmax = SVC(C=100, gamma=\"auto\")\nsvc_minmax.fit(X_train_scaled, y_train)\nprint(\"Accuracy =\", svc_minmax.score(X_test_scaled, y_test))\n", "intent": "Train again a SVC model, but this time train it on the scaled data. Do you see any improvements ?\n"}
{"snippet": "def feed_forward(theta, X):\n    t1, t2 = deserialize(theta)  \n    m = X.shape[0]\n    a1 = X  \n    z2 = a1 @ t1.T  \n    a2 = np.insert(sigmoid(z2), 0, np.ones(m), axis=1)  \n    z3 = a2 @ t2.T  \n    h = sigmoid(z3)  \n    return a1, z2, a2, z3, h  \n", "intent": "> (400 + 1) -> (25 + 1) -> (10)\n<img style=\"float: left;\" src=\"../img/nn_model.png\">\n"}
{"snippet": "def forward_propagate(X, theta1, theta2):\n    m = X.shape[0]\n    a1 = np.insert(X, 0, values=np.ones(m), axis=1)\n    z2 = a1 * theta1.T\n    a2 = np.insert(sigmoid(z2), 0, values=np.ones(m), axis=1)\n    z3 = a2 * theta2.T\n    h = sigmoid(z3)\n    return a1, z2, a2, z3, h\n", "intent": "> (400 + 1) -> (25 + 1) -> (10)\n<img style=\"float: left;\" src=\"../img/nn_model.png\">\n"}
{"snippet": "svc1 = sklearn.svm.LinearSVC(C=1, loss='hinge')\nsvc1.fit(data[['X1', 'X2']], data['y'])\nsvc1.score(data[['X1', 'X2']], data['y'])\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n"}
{"snippet": "svc100 = sklearn.svm.LinearSVC(C=100, loss='hinge')\nsvc100.fit(data[['X1', 'X2']], data['y'])\nsvc100.score(data[['X1', 'X2']], data['y'])\n", "intent": "with large C, you try to overfit the data, so the left hand side edge case now is categorized right\n"}
{"snippet": "parameters = {'C': candidate, 'gamma': candidate}\nsvc = svm.SVC()\nclf = GridSearchCV(svc, parameters, n_jobs=-1)\nclf.fit(training[['X1', 'X2']], training['y'])\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html\n"}
{"snippet": "n_estimators = np.arange(250, 400, 50)\nmax_depth = np.arange(15, 26, 3)\nmax_features = np.arange(5, 21, 5)\nparams = {'n_estimators': n_estimators,\n          'max_depth': max_depth,\n          'max_features': max_features}\ncv_rf = GridSearchCV(rf, param_grid=params, cv=skf, scoring='roc_auc', n_jobs=-1)\ncv_rf.fit(X_train_RF, y_train)\n", "intent": "Now we can find more precision parameters.\n"}
{"snippet": "with tf.Session(graph=detection_graph) as sess:\n    image_tensor = sess.graph.get_tensor_by_name('image_tensor:0')\n    detection_boxes = sess.graph.get_tensor_by_name('detection_boxes:0')\n    detection_scores = sess.graph.get_tensor_by_name('detection_scores:0')\n    detection_classes = sess.graph.get_tensor_by_name('detection_classes:0')\n    new_clip = clip.fl_image(pipeline)\n    new_clip.write_videofile('result.mp4')\n", "intent": "**[Sample solution](./exercise-solutions/e5.py)**\n"}
{"snippet": "MEDV_to_RM = smf.ols('MEDV ~ RM', data=boston_housing_df)\nres = MEDV_to_RM.fit()\nres.summary()\n", "intent": "Perform a regression on your first feature.\n"}
{"snippet": "MEDV_to_LSTAT = smf.ols('MEDV ~ LSTAT', data=boston_housing_df)\nres = MEDV_to_LSTAT.fit()\nres.summary()\n", "intent": "Plot the predicted values versus the true values.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=17, weights='distance')\n", "intent": "---\nIt's up to you what predictors you want to use and how you want to parameterize the model.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(penalty='l2', C=1)\n", "intent": "- What is the mean accuracy?\n- What is the standard deviation of accuracy?\n"}
{"snippet": "w_opt = minimize(ridge(1e-5), w)\nlen(w_opt.x[w_opt.x==0])\n", "intent": "1e-05 0.100575155982\n"}
{"snippet": "clf2 = svm.SVC(kernel=\"poly\", degree=3, probability=True)\nclf2.fit(X, Y)\n", "intent": "The shape of the dividing lines between classes depend on the kernel used.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.01))\n    embedding = tf.nn.embedding_lookup(embedding, input_data)\n    return embedding\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "results = smf.logit('Yes ~ balance + student', data=default).fit()\n", "intent": "more than 1 predictor\n"}
{"snippet": "lr=LogisticRegression(C=100, random_state=2018, penalty='l1')\nlr.fit(X_train_B, y_train)\n", "intent": "Let's make a prediction on the hold-out set for Logistic Regression model.\n"}
{"snippet": "mod1 = smf.ols('wage ~ age', data=wage).fit()\nmod2 = smf.ols('wage ~ age + np.power(age, 2)', data=wage).fit()\nmod3 = smf.ols('wage ~ age + np.power(age, 2) + np.power(age, 3)', data=wage).fit()\nmod4 = smf.ols('wage ~ age + np.power(age, 2) + np.power(age, 3) + np.power(age, 4)', data=wage).fit()\nmod5 = smf.ols('wage ~ age + np.power(age, 2) + np.power(age, 3) + np.power(age, 4) + np.power(age, 5)', data=wage).fit()\n", "intent": "Models must be nested here, meaning that mod2 must be a superset of mod1\n"}
{"snippet": "from distutils.version import LooseVersion as Version\nfrom sklearn import __version__ as sklearn_version\nif Version(sklearn_version) < '0.18':\n    clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\nelse:\n    clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\ndoc_stream = stream_docs(path='movie_data.csv')\n", "intent": "**Note**\n- You can replace `Perceptron(n_iter, ...)` by `Perceptron(max_iter, ...)` in scikit-learn >= 0.19.\n"}
{"snippet": "classifier = LogisticRegression()\n", "intent": "Next, we instantiate the estimator object.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n", "intent": "Next, we use the learning algorithm implemented in `LinearRegression` to **fit a regression model to the training data**:\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf\n", "intent": "We can now train a classifier, for instance a logistic regression classifier, which is a fast baseline for text classification tasks:\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinear_regression = LinearRegression().fit(X_train, y_train)\nprint(\"R^2 on training set: %f\" % linear_regression.score(X_train, y_train))\nprint(\"R^2 on test set: %f\" % linear_regression.score(X_test, y_test))\n", "intent": "$$ \\text{min}_{w, b} \\sum_i || w^\\mathsf{T}x_i + b  - y_i||^2 $$\n"}
{"snippet": "from sklearn.ensemble import IsolationForest\niforest = IsolationForest(contamination=0.05)\niforest = iforest.fit(X_5)\n", "intent": "1. Let's use IsolationForest to find the top 5% most abnormal images.\n2. Let's plot them !\n"}
{"snippet": "logit.fit(X_train, y_train)\n", "intent": "Train the same logistic regression.\n"}
{"snippet": "lr = LogisticRegression(random_state=5, class_weight='balanced')\n", "intent": "Now, we will create a LogisticRegression model and use class_weight='balanced' to make up for our unbalanced classes.\n"}
{"snippet": "rf = RandomForestClassifier(random_state=2018, max_depth=8, max_features=8, n_estimators=200)\nrf.fit(X_train_B, y_train)\n", "intent": "The result on cross-calidation is capmarable and is about *0.848*.  \nLet's make a prediction on the hold-out set for Fandom Forest model.\n"}
{"snippet": "rf_grid_search = GridSearchCV(rf, parameters, n_jobs=-1, scoring='roc_auc', cv=skf, verbose=True)\nrf_grid_search = rf_grid_search.fit(X, y)\nprint(rf_grid_search.best_score_ - grid_search.best_score_)\n", "intent": "**Answer:** 1.\n**Solution:**\n"}
{"snippet": "bg = BaggingClassifier(LogisticRegression(class_weight='balanced'),\n                       n_estimators=100, n_jobs=-1, random_state=42)\nr_grid_search = RandomizedSearchCV(bg, parameters, n_jobs=-1, \n                                   scoring='roc_auc', cv=skf, n_iter=20, random_state=1,\n                                   verbose=True)\nr_grid_search = r_grid_search.fit(X, y)\n", "intent": "**Answer:** 1.\n**Solution:**\n"}
{"snippet": "sgd_reg = SGDRegressor()\nsgd_reg.fit(X_train_scaled, y_train)\n", "intent": "Train created `SGDRegressor` with `(X_train_scaled, y_train)` data. Leave default parameter values for now.\n"}
{"snippet": "rf = RandomForestClassifier()\nrf.fit(X_train,y_train)\n", "intent": "- http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size+1, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    print(\"vocab_size:\", vocab_size)\n    print(\"embed.shape:\", embed.shape)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "X_full = np.concatenate((X_train_scaled_dimred, X_test_scaled_dimred), axis=0)\nY_full = np.concatenate((Y_train, Y_test), axis=0)\nsvc_full = svm.SVC(C=100)\nsvc_full.fit(X_full, Y_full)\n", "intent": "The model seems pretty good. So, lets train on the entire training set to improve accuracy.\n"}
{"snippet": "clf = RandomForestClassifier(criterion='gini', n_estimators=50, max_depth=10)\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n", "intent": "<p> a. Train a random forest classifier on your training set using the split function assigned to your group.</p>\n"}
{"snippet": "timesteps = 28 \nnum_hidden = 128 \ndef RNN(x, weights, biases):\n    x = tf.unstack(x, timesteps, 1)\n    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n    return tf.matmul(outputs[-1], weights) + biases\nfor time_step in range(timesteps):\n    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n", "intent": "A) LSTM \nB) GRU\nC) Multi Layer\nD) Multi Layer with Dropout\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "<b>Step2:</b> Instantiate the estimator\n- Estimator is scikit-learn's term for model\n- Instantiate means, make an instance of\n"}
{"snippet": "weight={}\nfor i in [.1,.3,.5,.7,.9,1.1,1.3,1.5,1.7,1.9,2.1]:\n  class_weight={0:1,1:1.3+i}\n  params = {'n_estimators':range(140,190,10),'max_depth':[1,2],'min_samples_split':[0.4]} \n  rfc=RandomForestClassifier(random_state=42,class_weight=class_weight)\n  rfc_grid = GridSearchCV(rfc,param_grid=params,cv=2)\n  rfc_grid.fit(X_train,y_train) \n  rfc=rfc_grid.best_estimator_ \n  weight[1.3+i]=rfc_grid.best_score_ \n", "intent": "Since we have an imbalanced dataset with respect to target variable, we need to assign weights to the class with lower number of records.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X, y)\n", "intent": "<b>Using a different value for K</b>\n"}
{"snippet": "logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n", "intent": "<b>1. Logistic Regression</b>\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinear = LinearRegression()\nlinear.fit(X_train, y_train)\n", "intent": "<b>1. Linear Regression</b>\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=13, weights='uniform')\nknn.fit(X, y)\n", "intent": "<b>Using the best parameters to make predictions</b>\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X, y)\n", "intent": "Implementing Linear Regression on the Data\n"}
{"snippet": "lm_for_impute = LinearRegression() \n", "intent": "Then, we create model for regression imputation.\n"}
{"snippet": "rfr = RandomForestRegressor(n_estimators=100)\nrfr.fit(X_binarized, Y)\nsorted([i for i in zip(X_binarized.columns, rfr.feature_importances_)], key=lambda x:x[1], reverse=True)\n", "intent": "We can query the model to ask what amino acids are most important. To do so, call the `model.feature_importances_` attribute.\n"}
{"snippet": "def fit_model(model, X, Y):\n    return \nrf_mdl, rf_preds = fit_model(RandomForestRegressor, X_binarized, Y)\n", "intent": "Here, I will show you how to output the predictions of the machine learning model to disk as a CSV file.\n**Live Coding Together**\n"}
{"snippet": "print(loss_func(model(xb), yb), accuracy(model(xb), yb))\n", "intent": "Note that we no longer call ``log_softmax`` in the ``model`` function. Let's\nconfirm that our loss and accuracy are the same as before:\n"}
{"snippet": "class_weight={0:1,1:2.0} \nparams = {'n_estimators':range(140,160,3),'max_depth':[1,2,3],'min_samples_split':[0.4,.7]}\nrfc=RandomForestClassifier(random_state=42,class_weight=class_weight)\nrfc_grid = GridSearchCV(rfc,param_grid=params,cv=5)\nrfc_grid.fit(X_train,y_train)\nrfc=rfc_grid.best_estimator_ \n", "intent": "We are getting maximum accuracy at class weight of 2 for class 1.\n"}
{"snippet": "model = Mnist_Logistic()\nprint(loss_func(model(xb), yb))\n", "intent": "We instantiate our model and calculate the loss in the same way as before:\n"}
{"snippet": "fit()\nprint(loss_func(model(xb), yb))\n", "intent": "We are still able to use our same ``fit`` method as before.\n"}
{"snippet": "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\nmodel, opt = get_model()\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)\n", "intent": "Now, our whole process of obtaining the data loaders and fitting the\nmodel can be run in 3 lines of code:\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\nmodel_ft = model_ft.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "intent": "Finetuning the convnet\n----------------------\nLoad a pretrained model and reset final fully connected layer.\n"}
{"snippet": "scaler.fit(data.drop('Class', axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "kmeans.fit(df.drop('Private', axis=1))   \n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "from sklearn.svm import SVC\nsvc_model = SVC()\nsvc_model.fit(X_train, y_train)\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "param_grid = {'C':[0.1,1,10,100,1000], 'gamma':[1,0.1,0.01,0.001,0.0001]}\ngrid = GridSearchCV(SVC(), param_grid, verbose=3)\ngrid.fit(X_train, y_train)\n", "intent": "**Create a dictionary called param_grid and fill out some parameters for C and gamma.**\n"}
{"snippet": "sess = tf.Session()\nfeed_dict = { a: 5, b: 3 }\noutput = sess.run(out, feed_dict=feed_dict)\nprint(output)\n", "intent": "* Start a `tf.Session` to launch the graph\n* Setup any necessary input values\n* Use `Session.run()` to compute values from the graph\n"}
{"snippet": "weight={} \nfor i in [.1,.3,.5,.7,.9,1.1,1.3,1.5]:\n  class_weight={0:1,1:.3+i}\n  params = {'C':[0.01,.1,5,1,10,15]} \n  log=LogisticRegression(class_weight=class_weight)\n  log_grid = GridSearchCV(log,param_grid=params,cv=5)\n  log_grid.fit(X_train,y_train)\n  log=log_grid.best_estimator_\n  weight[.3+i]=log_grid.best_score_ \n", "intent": "We apply the same method as we did before for logistic regression too.\n"}
{"snippet": "model = LinearRegression(fit_intercept=True)\nmodel\n", "intent": "```\nmodel = LinearRegression(fit_intercept=True)\nmodel```\n"}
{"snippet": "model.fit(X, y)\n", "intent": "```\nmodel.fit(X, y)```\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X, y);\n", "intent": "```\nfrom sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X, y);```\n"}
{"snippet": "model_vectorizer.fit(train.data)\nvectors = model_vectorizer.transform(train.data)\n", "intent": "```\nmodel_vectorizer.fit(train.data)\nvectors = model_vectorizer.transform(train.data)```\n"}
{"snippet": "model_classifier.fit(vectors, train.target)\n", "intent": "```\nmodel_classifier.fit(vectors, train.target)```\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier().fit(X, y)\n", "intent": "scikit-learn includes DecisionTreeClassifier that is a class capable of performing multi-class classification on a dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='train',\n                input_to_softmax=final_model(), \n                model_path='model_end.h5')\n", "intent": "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='validation',\n                input_to_softmax=final_model(), \n                model_path='model_end.h5')\n", "intent": "Use the next code cell to visualize the model's prediction for the first example in the validation dataset.\n"}
{"snippet": "from sklearn import svm, datasets\nfrom sklearn.preprocessing import scale\nC = 1.0\nsvc = svm.SVC(kernel='linear', C=C).fit(scale(X), scale(y))\n", "intent": "Now we'll use linear SVC to partition our graph into clusters:\n"}
{"snippet": "pr_lr=LogisticRegression(class_weight='balanced')\n", "intent": "Let's at first train a basic LogReg without tuning hyperparametes, creating new features to establish sort of baseline:\n"}
{"snippet": "knn = KNeighborsClassifier()\nparam_dict = {'n_neighbors':range(1, 31), 'weights':['uniform', 'distance']}\ngsknn = GridSearchCV(knn, param_dict, scoring='accuracy')\ngsknn.fit(X, y)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "ridge(sample, 0)\n", "intent": "Let us run this with slpha = 0, which is OLS\n"}
{"snippet": "kidiq_X = kidiq.as_matrix([\"mom_iq\"])\nkidiq_Y = kidiq.as_matrix([\"kid_score\"])\nlr = linear_model.LinearRegression()\nlr.fit(kidiq_X,kidiq_Y)\n", "intent": "Some Pandas/Python practice: we extract the numpy arrays from the dataframe, and use linear_model from scikit-learn to perform the same fit.\n"}
{"snippet": "kidiq_X = kidiq.as_matrix([\"mom_iq\",\"mom_hs\"])\nkidiq_Y = kidiq.as_matrix([\"kid_score\"])\nlr = linear_model.LinearRegression()\nlr.fit(kidiq_X,kidiq_Y)\nprint lr.coef_, lr.intercept_\n", "intent": "Again the R^2 value is not a final test. It is merely another interpretation of the goodness of fit, that can help provide some guidance.\n"}
{"snippet": "R = 0.005 \nC = 1/R  \nsvc = svm.SVC(kernel='linear', C=C).fit(X_tr, y_tr)\npoly_svc3 = svm.SVC(kernel='poly', degree=3, C=C).fit(X_tr, y_tr)\npoly_svc5 = svm.SVC(kernel='poly', degree=5, C=C).fit(X_tr, y_tr)\npoly_svc13 = svm.SVC(kernel='poly', degree=13, C=C).fit(X_tr, y_tr)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X_tr, y_tr)\nsig_svc = svm.SVC(kernel='sigmoid', gamma=0.7, C=C).fit(X_tr, y_tr)\n", "intent": "Next, we train SVMs with several different kernels and regularization parameters.\n"}
{"snippet": "trainAA=trainA.copy(deep=True)\ntrainAA.drop(\"class\", axis=1, inplace=True)\ngnb = GaussianNB()\ngnb.fit(trainAA, trainA[\"class\"])\ngnb.score(trainAA, trainA[\"class\"])\n", "intent": "Fit a Gaussian Naive Bayes model to the original dataset A (including the outliers). Display the classification accuracy on the training dataset.\n"}
{"snippet": "kmeans = KMeans(n_clusters=5, random_state=0)\nkmeans.fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "reg = Lasso(alpha=1.e-2, fit_intercept=False)\nreg.fit(X_train20, y_train20)\nreg.coef_\n", "intent": "[sklearn.linear_model.Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html\n"}
{"snippet": "scaler.fit(df.drop('Class', axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "xg=XGBClassifier()\nxg.fit(polfeats_train,y_train_logreg)\n", "intent": "I'll fit XGboost Classifier on those 820 features to see which features were the most important from its prespective.\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=6)\ngrid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(2000, input_dim=x_train.shape[1]))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(500))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(2))\nmodel.add(Activation('sigmoid'))\nmodel.summary()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"RMSprop\", metrics = [\"accuracy\"])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.svm import SVC\nfrom sklearn.grid_search import GridSearchCV\nsvc = SVC(kernel='rbf', class_weight='balanced')\nparam_grid = {'C': [1, 5, 10, 50],\n              'gamma': [0.0001, 0.0005, 0.001, 0.005]}\ngrid = GridSearchCV(svc, param_grid)\nprint('Best parameters:', grid.best_params_)\n", "intent": "Select appropriate hyperparameters C and gamma, and run SVM on the original training dataset.\n"}
{"snippet": "svc = SVC(kernel='rbf', class_weight='balanced')\nparam_grid = {'C': [1, 5, 10, 50],\n              'gamma': [0.0001, 0.0005, 0.001, 0.005]}\ngrid = GridSearchCV(svc, param_grid)\nprint('Best parameters:', grid.best_params_)\n", "intent": "Select appropriate hyperparameters C and gamma, and run SVM on the dataset after PCA.\n"}
{"snippet": "lr = LogisticRegression(penalty= 'l2', C = 1/best_lambda)\nerror_test, error_train = Kfold_cross_validation(5, x_train.T, y_train, lr)\nprint(\"training error (lambda = \" + str(int(best_lambda)) + \"): \" + str(round(error_train,4)))\nprint(\"CV error: (lambda = \" + str(int(best_lambda)) + \"): \" + str(round(error_test,4)))\n", "intent": "Print the training error and CV error for the fitted $Logistic \\  Regression$ with $\\lambda = 100$\n"}
{"snippet": "lr = LinearRegression()\nrf = RandomForestRegressor()\nab = AdaBoostRegressor()\ndt = DecisionTreeRegressor()\n", "intent": "> **Step 3:** Now that you have imported the 4 models that can be used for regression problems, instantate each below.\n"}
{"snippet": "lr.fit(X_train, y_train)\nrf.fit(X_train, y_train)\nab.fit(X_train, y_train)\ndt.fit(X_train, y_train)\n", "intent": "> **Step 4:** Fit each of your instantiated models on the training data.\n"}
{"snippet": "fit_info = model.fit(X_train, Y_train,\n                    epochs=5,\n                    batch_size=32,\n                   validation_data=(X_Valid,Y_Valid))\n", "intent": "2\\. Fit the model with the training set with 5 epochs and batch size 32.\n"}
{"snippet": "xgbclf.fit(X_train_tb_fin,y_train_tb_fin)\n", "intent": "**XGBoost Clasiffier**\n"}
{"snippet": "knn.fit(X_train, y_train)\n", "intent": "** Ajuste este modelo KNN aos dados de treinamento. **\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\ngrid.fit(X_train, y_train)\ngrid.best_params_\n", "intent": "** Crie um objeto GridSearchCV e ajuste-o aos dados de treinamento. **\n"}
{"snippet": "bag_clf = BaggingClassifier(base_estimator=clf, n_estimators=500,\n                           bootstrap=True, oob_score=True, n_jobs=-1,\n                           random_state=42)\n", "intent": "Use out of bag samples to estimate the generalization accuracy\n"}
{"snippet": "from sklearn.pipeline import make_pipeline\npoly_model = make_pipeline(PolynomialFeatures(7),\n                           LinearRegression())\n", "intent": "This new, higher-dimensional data representation can then be plugged into a linear regression.\nLet's make a 7th-degree polynomial model in this way:\n"}
{"snippet": "x_init = x.copy()\nx = x.reshape(-1, 1)\nclf = GaussianMixture(n_components=5, max_iter=500, random_state=3).fit(x)\n", "intent": "Gaussian mixture models will allow us to approximate this density:\n"}
{"snippet": "lr = LogisticRegression(C=1)\nlr.fit(features_train, target_train)\n", "intent": "Let's start with a simple model from sklearn, namely `LogisticRegression`:\n"}
{"snippet": "km = KMeans(n_clusters=3, random_state=1)\n", "intent": "Do KMeans with 3 clusters\n"}
{"snippet": "clf = tree.DecisionTreeClassifier(random_state=0)\n", "intent": "Create a new Decision Tree classifier using default parameters:\n"}
{"snippet": "clf.fit(df[iris.feature_names], df.species)\n", "intent": "Fit the classifier with the iris data:\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nc_values = np.linspace(10, 15, 5)\nlogit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_values},\n                                           scoring='f1', n_jobs=-1, cv=skf, verbose=1)\nlogit_grid_searcher.fit(X_train_full, train_df['target'])\nprint(logit_grid_searcher.best_score_, logit_grid_searcher.best_params_)\n", "intent": "Now, let's find optimal parametr C for logregression using GridSearch\n"}
{"snippet": "import tensorflow as tf\na = tf.constant([1.0,2.0], name = \"a\")\nb = tf.constant([1.0,2.0], name = \"b\")\nc = a + b\nwith tf.Session() as sess:\n    sess.run(c)\nprint(c)\n", "intent": "Please refere to my notes in the directory of JiaxuanLi-book2017\nA test code for your environment:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\n", "intent": "In order to improve the performances of our prediction is possible to add hidden layers between the input layer and the output layer.\n"}
{"snippet": "model2 = Model(inputs=inputs, outputs=outputs)\n", "intent": "Create a new instance of the Keras Functional Model. We give it the inputs and outputs of the Convolutional Neural Network that we constructed above.\n"}
{"snippet": "model3 = load_model(path_model)\n", "intent": "Loading the model is then just a single function-call, as it should be.\n"}
{"snippet": "new_model = Sequential()\nnew_model.add(conv_model)\nnew_model.add(Flatten())\nnew_model.add(Dense(1024, activation='relu'))\nnew_model.add(Dropout(0.5))\nnew_model.add(Dense(num_classes, activation='softmax'))\n", "intent": "We can then use Keras to build a new model on top of this.\n"}
{"snippet": "model = load_model(path_best_model)\n", "intent": "We can now use the best model on the test-set. It is very easy to reload the model using Keras.\n"}
{"snippet": "model = Sequential()\n", "intent": "We are now ready to create the Recurrent Neural Network (RNN). We will use the Keras API for this because of its simplicity. See Tutorial \n"}
{"snippet": "model.add(GRU(units=8, return_sequences=True))\n", "intent": "This adds the second GRU with 8 output units. This will be followed by another GRU so it must also return sequences.\n"}
{"snippet": "model.add(Dense(1, activation='sigmoid'))\n", "intent": "Add a fully-connected / dense layer which computes a value between 0.0 and 1.0 that will be used as the classification output.\n"}
{"snippet": "x_test = tfidf.transform(test_df['clean_text'])\nX_test_full = hstack([x_test, test_df[feats].values])\nlogit.fit(X_train_full, train_df['target'])\n", "intent": "It's time to make predict for test set!\n"}
{"snippet": "def connect_encoder():\n    net = encoder_input\n    net = encoder_embedding(net)\n    net = encoder_gru1(net)\n    net = encoder_gru2(net)\n    net = encoder_gru3(net)\n    encoder_output = net\n    return encoder_output\n", "intent": "This helper-function connects all the layers of the encoder.\n"}
{"snippet": "decoder_output = connect_decoder(initial_state=decoder_initial_state)\nmodel_decoder = Model(inputs=[decoder_input, decoder_initial_state],\n                      outputs=[decoder_output])\n", "intent": "Then we create a model for just the decoder alone. This allows us to directly input the initial state for the decoder's GRU units.\n"}
{"snippet": "def flatten(captions_listlist):\n    captions_list = [caption\n                     for captions_list in captions_listlist\n                     for caption in captions_list]\n    return captions_list\n", "intent": "This helper-function converts a list-of-list to a flattened list of captions.\n"}
{"snippet": "captions_train_flat = flatten(captions_train_marked)\n", "intent": "Now use the function to convert all the marked captions from the training set.\n"}
{"snippet": "decoder_embedding = Embedding(input_dim=num_words,\n                              output_dim=embedding_size,\n                              name='decoder_embedding')\n", "intent": "This is the embedding-layer which converts sequences of integer-tokens to sequences of vectors.\n"}
{"snippet": "A = tf.placeholder(tf.float32, shape=(None, 3))\nB = A + 5\nwith tf.Session() as sess:\n    B_val_1 = B.eval(\n        feed_dict={A: [[1, 2, 3]]})\n    B_val_2 = B.eval(\n        feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\nprint(B_val_1, \"\\n\", B_val_2)\n", "intent": "* Goal: modify previous code for Minibatch gradient descent\n* Best practice: *placeholder* nodes (no computation, just data output)\n"}
{"snippet": "import tensorflow as tf\ntf.reset_default_graph()\nn_inputs = 28*28  \nn_hidden1 = 300\nn_hidden2 = 100\nn_outputs = 10\nlearning_rate = 0.01\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\ny = tf.placeholder(tf.int64,   shape=(None),           name=\"y\")\n", "intent": "* Use **mini-batch gradient descent** on MNIST dataset\n* Specify \n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, save_path) \n    X_new_scaled = mnist.test.images[:20]\n    Z = logits.eval(feed_dict={X: X_new_scaled})\n    print(np.argmax(Z, axis=1))\n    print(mnist.test.labels[:20])\n", "intent": "* Now trained - you can use the NN to predict.\n"}
{"snippet": "import tensorflow as tf\nwith tf.device(\"/cpu:0\"):\n    a,b = tf.Variable(3.0), tf.Variable(4.0)\nc = a*b\nc\n", "intent": "* Mostly up to you. To pin devices to specific device, use a device() function. Below: a,b pinned to cpu\n"}
{"snippet": "parameters = {'n_estimators':[40, 50, 60, 80, 100, 150, 200, 300], 'max_depth':[3, 4, 5, 6, 7, 8], 'min_child_weight': [1,3,5,7,9]}\nxgb = XGBClassifier(random_state=33, n_jobs=4)\nclf = GridSearchCV(xgb, parameters, scoring='roc_auc', cv=kf)\nclf.fit(X_train, y_train)\nprint('Best parameters: ', clf.best_params_)\n", "intent": "Whoah! New features gave a noticeable increase in quality! Let's tune hyperparameters via GridSearchCV.\n"}
{"snippet": "with tf.device(\"/gpu:0\"):\n    i = tf.Variable(3)\nconfig = tf.ConfigProto()\nconfig.allow_soft_placement = True\nsess = tf.Session(config=config)\ntest = sess.run(i.initializer) \nprint(test)\n", "intent": "* To allow TF to \"fall back\" to a CPU instead, use *allow_soft_placement=True*.\n"}
{"snippet": "optimizer = tf.train.GradientDescentOptimizer(0.1)\ntrain = optimizer.minimize(error)\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\n", "intent": "Like in the previous exercise, we will declare a gradient descent optimizer and run the initialization step.\n"}
{"snippet": "def bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n", "intent": "Do the same but for the bias term:\n"}
{"snippet": "sess = tf.Session()\n", "intent": "Let's get the easy part out of the way, we create a session just like in any other TensorFlow-based code\n"}
{"snippet": "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n", "intent": "Now we can define the first convolution layer. This is a convolution using 32 5x5 filters.\n"}
{"snippet": "W_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n", "intent": "We do pretty much the same thing (convolution + maxpool), adding another 2 layers, but with 64 5x5 filters for the convolution piece:\n"}
{"snippet": "fitted_models = {}\nfor name, pipeline in pipelines.items():\n    model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1)\n    model.fit(X_train, y_train)\n    fitted_models[name] = model\n    print(name, 'has been fitted.')\n", "intent": "* Create fitted models dictionary that have been tuned using CV\n"}
{"snippet": "STEPS = 1000\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer()) \n    for step in range(STEPS):\n        sess.run(fetches = optimizer, feed_dict = {LEARNING_RATE: 0.02})\n        if step % 100 == 0:\n            print(\"STEP: {} MSE: {}\".format(step, sess.run(fetches = loss_mse)))\n    print(\"STEP: {} MSE: {}\".format(STEPS, sess.run(loss_mse)))\n    print(\"w0:{}\".format(round(float(sess.run(w0)), 4)))\n    print(\"w1:{}\".format(round(float(sess.run(w1)), 4)))\n", "intent": "Note our results are identical to what we found in Eager mode.\n"}
{"snippet": "def linear_model(img):\n    X = tf.reshape(tensor = img, shape = [-1, HEIGHT * WIDTH]) \n    W = tf.get_variable(name = \"W\", shape = [HEIGHT * WIDTH, NCLASSES], initializer = tf.truncated_normal_initializer(stddev = 0.1, seed = 1))\n    b = tf.get_variable(name = \"b\", shape = [NCLASSES], initializer = tf.zeros_initializer)\n    ylogits = tf.matmul(a = X, b = W) + b\n    return ylogits, NCLASSES\n", "intent": "Let's start with a very simple linear classifier. All our models will have this basic interface -- they will take an image and return logits.\n"}
{"snippet": "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\ntree_params = {\n    'n_estimators' : [250, 300, 350],\n    'max_depth' : [None, 10, 20],\n    'max_features' : ['sqrt', 'log2', 50]\n}\ngcv = GridSearchCV(random_forest, tree_params, scoring='neg_log_loss', cv=skf, verbose=1)\ngcv.fit(X_crossvalid, y_crossvalid)\n", "intent": " Using 3 splits as one of the most frequently used amount, shuffle samples in random order. \n"}
{"snippet": "def linear_model():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.InputLayer(input_shape = [HEIGHT, WIDTH], name = \"image\"))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(units = NCLASSES, activation = tf.nn.softmax, name = \"probabilities\"))\n    return model\n", "intent": "Let's start with a very simple linear classifier. All our models will have this basic interface -- they will take an image and return probabilities.\n"}
{"snippet": "def linear_model():\n    model = tf.keras.models.Sequential()\n    return model\n", "intent": "Let's start with a very simple linear classifier. All our models will have this basic interface -- they will take an image and return probabilities.\n"}
{"snippet": "def try_out():\n    with tf.Session() as sess:\n        fn = read_dataset(\n            mode = tf.estimator.ModeKeys.EVAL, \n            args = {\"input_path\": \"data\", \"batch_size\": 4, \"nitems\": 5668, \"nusers\": 82802})\n        feats, _ = fn()\n        print(feats[\"input_rows\"].eval())\n        print(feats[\"input_rows\"].eval())\ntry_out()\n", "intent": "This code is helpful in developing the input function. You don't need it in production.\n"}
{"snippet": "vect.fit(X_train)\nX_train_dtm = vect.transform(X_train)\nX_train_dtm\n", "intent": "Use CountVectorizer to create **document-term matrices** from X_train and X_test.\n"}
{"snippet": "X1 = tf.convert_to_tensor(_X1)\nX1\n", "intent": "Or you can convert them to a tensor like this.\n"}
{"snippet": "kde = KernelDensity(6.7858, kernel='gaussian').fit(X)\n", "intent": "So let us see the result\n"}
{"snippet": "g = gp.fit(x[:, np.newaxis], y)\n", "intent": "The `alpha` option provides a way to give the uncertainty on $y$ to the procedure.\n"}
{"snippet": "reg = AdaBoostRegressor(n_estimators=100)\n", "intent": "I will first run in default mode where the regressor is a Decision tree regressor. \n"}
{"snippet": "knn = KNeighborsRegressor(n_neighbors=3, weights='distance')\nknn_boost = AdaBoostRegressor(knn, n_estimators=100)\n", "intent": "We can also run this with a k-nearest neighbours regressor.\n"}
{"snippet": "random_forest_balanced = RandomForestClassifier(\n    random_state=17, n_estimators=350, max_features=150, min_samples_split=64)\n", "intent": "Fitting estimators on the whole train sets, evaluating on hold-out set.\n"}
{"snippet": "N = range(1, 11)\nmodels = [None for i in N]\nfor i, n in enumerate(N):\n    models[i] = GaussianMixture(n).fit(X)\n", "intent": "We then fit this dataset using a different number of components. I store each model fit in a list since I will need them later\n"}
{"snippet": "pca.fit(f)\n", "intent": "Do the PCA - you can experiment with subtracting off the mean, here I do not.\n"}
{"snippet": "def lrelu(x, leak=0.2):\n        f1 = 0.5 * (1 + leak)\n        f2 = 0.5 * (1 - leak)\n        return f1 * x + f2 * abs(x)\n", "intent": "This is the relu function suggested by my reviewer to help make the model more memory efficient.\n"}
{"snippet": "def process_single_frame(img):\n    rectangles = mult_sliding_windows(img) \n    heatmap_img = np.zeros_like(img[:,:,0])\n    heatmap_img = add_heat(heatmap_img, rectangles)\n    heatmap_img = apply_threshold(heatmap_img, 1)\n    labels = label(heatmap_img)\n    draw_img, rects = draw_labeled_bboxes(np.copy(img), labels)\n    return draw_img\n", "intent": "&nbsp;\n---\nRun your pipeline on a video stream with recurring detections frame by frame that reject outliers and follow detected vehicles.\n"}
{"snippet": "classifier.add(Dense(units = 6, kernel_initializer='uniform', activation ='relu'))\n", "intent": "- for a more complex models it might be useful to create a second hidden layer\n- the 2nd hidden layer, does not need the input_dim argument\n"}
{"snippet": "classifier.fit(X_train, y_train, batch_size=10, nb_epoch = 130)\n", "intent": "- the batch size & the Epoch\n"}
{"snippet": "model = LogisticRegression()\npredictor = ['Gender']\noutcome = 'Loan_Status'\nclassification_model(model, df, predictor, outcome)\n", "intent": "Let's start with fitting simpler model and gradually increasing model complexity just to understand overfitting and underfitting\n"}
{"snippet": "y = df[\"Hired\"]\nprint y\nX = df[features]\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X,y)\n", "intent": "Now actually construct the decision tree:\n"}
{"snippet": "duplicate_questions_model.fit(x=[padded_question_1s, padded_question_2s], y=labels, \n                              batch_size=batch_size, epochs=4, validation_split=0.1)\n", "intent": "Now, we can finally pass in our input arrays and output labels and watch the model train!\n"}
{"snippet": "lasso = LassoCV(cv =tscv, max_iter=10000)\nlasso.fit(X_train_scaled, y_train)\n", "intent": "Lets create our next model - Lasso Regression\n"}
{"snippet": "kmeans.fit(col.drop('Private', axis =1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "    lm = LinearRegression()\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "grid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\npd.Series(list(zip(feature_cols, logreg.coef_[0])))\n", "intent": "Confirm that the coefficients make intuitive sense.\n"}
{"snippet": "rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nprint(\"Accuracy: %0.3f\" % rf.score(X_train, y_train))\nprint(\"Accuracy: %0.3f\" % rf.score(X_test, y_test))\n", "intent": "(4) Repeat step 3 but with a Random Forest\n"}
{"snippet": "lm = Lasso(alpha=0.00001, normalize=True)\nlm.fit(X,y)\n", "intent": "**Fit the LASSO regression**\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X, y)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n"}
{"snippet": "feature_cols = ['season_2', 'season_3', 'season_4', 'humidity']\nX = bikes[feature_cols]\ny = bikes.total\nlinreg = LinearRegression()\nlinreg.fit(X, y)\nprint(pd.Series(list(zip(feature_cols, linreg.coef_))))\n", "intent": "- Let's see what happens if we exclude temperature\n"}
{"snippet": "regr = linear_model.LinearRegression()\nregr.fit(X_tr,y_tr)\n", "intent": "To fit the linear model, we first create a regression object and the fit the data with regression object.\n"}
{"snippet": "lr.fit(pca_features_train, y_train)\n", "intent": "Lets run the Linear Regression model once again to see if there are any improvements since last time\n"}
{"snippet": "regressor = LinearRegression()\nregressor.fit(X_train, y_train)\nprint regressor.intercept_\nprint regressor.coef_\n", "intent": "Add the code to instantiate and train (fit) the linear regression model.\n"}
{"snippet": "quadratic_regressor = LinearRegression()\nquadratic_regressor.fit(X_train_quadratic, y_train)\n", "intent": "Now fit another regressor on the expanded features.\n"}
{"snippet": "degree_7_regressor = LinearRegression()\ndegree_7_regressor.fit(X_train_degree_7, y_train)\n", "intent": "Now instantiate and train the model.\n"}
{"snippet": "starttime = datetime.datetime.now()\ngrid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=3, scoring='f1')\ngrid_search.fit(X_train_gs, y_train_gs)\nprint 'Best score:', grid_search.best_score_\nprint 'Best parameters set:'\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print '\\t', param_name, best_parameters[param_name]\nendtime = datetime.datetime.now()\nprint '\\nRun time (secs): ', timestamp(endtime) - timestamp(starttime)\n", "intent": "Start grid search and display results\n"}
{"snippet": "pipeline2 = Pipeline([\n    ('clf', RandomForestClassifier(bootstrap = False,criterion='entropy',n_jobs=-1))\n])\nn_features = int(math.ceil(math.sqrt(X_train_gs.shape[1])))\nparameters2 = {\n    'clf__n_estimators': (50, n_features),\n    'clf__max_depth': (50,100),\n    'clf__min_samples_split': (1, 2),\n    'clf__min_samples_leaf': (1, 10)\n}\n", "intent": "Set up a pipeline and the hyper-parameters\n"}
{"snippet": "starttime = datetime.datetime.now()\ngrid_search2 = GridSearchCV(pipeline2, parameters2, n_jobs=1, verbose=3, scoring='f1')\ngrid_search2.fit(X_train_gs_v, y_train_gs)\nprint 'Best score:', grid_search2.best_score_\nprint 'Best parameters set:'\nbest_parameters2 = grid_search2.best_estimator_.get_params()\nfor param_name in sorted(parameters2.keys()):\n    print '\\t', param_name, best_parameters2[param_name]\nendtime = datetime.datetime.now()\nprint '\\nRun time (secs): ', timestamp(endtime) - timestamp(starttime)\n", "intent": "Start grid search and display results\n"}
{"snippet": "predicted_classes = []\nwith tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('.'))\n    TopKV2 = sess.run(tf.nn.top_k(tf.nn.softmax(logits), 5), feed_dict={x: X_test_n})\n", "intent": "**Answer:**\nFrom the possibility distribution shown below, we can conclude that our classifier is very confident with the output it gives.\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, batch_size=16, epochs=20)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model.fit(X_train, Y_train, epochs = 20, batch_size = 32)\n", "intent": "Run the following cell to train your model on 2 epochs with a batch size of 32. On a CPU it should take you around 5min per epoch. \n"}
{"snippet": "from sklearn.linear_model import RidgeCV\nridge = RidgeCV(cv=tscv)\nridge.fit(X_train_scaled, y_train)\nplotModelResults(ridge, X_train_scaled, X_test_scaled, y_train, y_test, plot_intervals=True, plot_anomalies=True)\nplotCoefficients(ridge)\n", "intent": "Not Bad at all!\nLasso and Ridge turned out to quite close and already are superstars\nLets see the plots for Ridge\n"}
{"snippet": "sc.fit(imagedf.drop(axis=1,columns='Class'))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('lrm', lrm_best),\n('svc', SVMC_best)], voting='soft', n_jobs=4)\nvotingC = votingC.fit(X, y)\n", "intent": "Ensembling more than one model\n"}
{"snippet": "votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('lrm', lrm_best),\n('GBC', GBC_best)], voting='soft', n_jobs=4)\nvotingC = votingC.fit(X, y)\n", "intent": "Ensembling more than one model\n"}
{"snippet": "km.fit(cdf.drop(labels='Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlrm = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "knn.fit(X_Train,y_Train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "lm.fit(X_Train,y_Train)\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(X)\nlabels = ms.labels_\ncluster_centers = ms.cluster_centers_\nlabels_unique = np.unique(labels)\nn_clusters_ = len(labels_unique)\nprint(\"number of estimated clusters : %d\" % n_clusters_)\n", "intent": "Use sklearn.cluster.MeanShift method to perform clustering\n"}
{"snippet": "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nscore = cv_evaluation(KRR)\nprint('Kernel Ridge Score: {:.4f}'.format(score.mean()))\n", "intent": "* **Kernel Ridge Regression**:\n"}
{"snippet": "from sklearn.linear_model import Lasso\nLasso = Lasso(max_iter=10000)\nLasso.fit(pca_features_train, y_train)\nfrom sklearn.linear_model import Ridge\nridge = Ridge(max_iter=10000, random_state=17)\nridge.fit(pca_features_train, y_train)\n", "intent": "Now Lets see how Lasso and Ridge are performing on PCA transformed data\n"}
{"snippet": "grid_sgdlogreg.fit(X_train, y_train)\n", "intent": "Conduct the grid search and train the final model on the whole dataset:\n"}
{"snippet": "idx = 0\ntarget_y = 6\nX_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\nX_fooling = make_fooling_image(X_tensor[idx:idx+1], target_y, model)\nscores = model(Variable(X_fooling))\nassert target_y == scores.data.max(1)[1][0], 'The model is not fooled!'\n", "intent": "Run the following cell to generate a fooling image:\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\ntorch.cuda.set_device(7)\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "intent": "Finetuning the convnet\n----------------------\nLoad a pretrained model and reset final fully connected layer.\n"}
{"snippet": "ols_5 = ols('PRICE ~ RM + CRIM +PTRATIO + DIS + LSTAT',bos).fit()\nprint(ols_5.summary())\n", "intent": "using CRIM, RM, PTRATIO, DIS, and LSTAT\n"}
{"snippet": "model_NB.fit(X_text_train_tfidf, np.asarray(yelp_data_final_update['Review_category_useful_notuseful']))\n", "intent": "Use the features trained by tfidf vectorizer for the Naive Bayes Model: Useful vs not useful\n"}
{"snippet": "model_NB.fit(X_text_train_tfidf, np.asarray(yelp_data_final_update['Review_category_cool_notcool']))\n", "intent": "Use the features trained by tfidf vectorizer for the Naive Bayes Model: cool vs not cool\n"}
{"snippet": "model_NB.fit(X_text_train_tfidf, np.asarray(yelp_data_final_update['Review_category_funny_notfunny']))\n", "intent": "Use the features trained by tfidf vectorizer for the Naive Bayes Model: funny vs not funny\n"}
{"snippet": "model_random_forest_10trees.fit(X_text_train_tfidf, yelp_data_final_update['Review_category_useful_notuseful'])\n", "intent": "Random forest useful vs not useful:\n"}
{"snippet": "model_random_forest_100trees.fit(X_text_train_tfidf, yelp_data_final_update['Review_category_funny_notfunny'])\n", "intent": "Random Forest for funny versus not funny:\n"}
{"snippet": "m = Prophet()\nm.fit(train_df);\n", "intent": "Fitting the model and Creating Future Dataframes including the history\n"}
{"snippet": "model_NB.fit(X_text_train_tfidf, yelp_data_final_update['Review_category_coolest_funniest_mostuseful'])\n", "intent": "Use the features trained by teh tfidf vectorizer to train the Naive Bayes Model:\n"}
{"snippet": "model_NB.fit(X_text_train_tfidf, np.asarray(yelp_data_final_update['Review_category_cool_notcool_binary']))\n", "intent": "Use the features trained by tfidf vectorizer for the Naive Bayes Model: cool vs not cool\n"}
{"snippet": "model_random_forest_10trees.fit(X_text_train_tfidf, yelp_data_final_update['Review_category_useful_notuseful_binary'])\n", "intent": "Random forest useful vs not useful:\n"}
{"snippet": "model_random_forest_10trees.fit(X_text_train_tfidf, yelp_data_final_update['Review_category_funny_notfunny_binary'])\n", "intent": "Random Forest for funny versus not funny:\n"}
{"snippet": "model_random_forest_10trees.fit(X_text_train_tfidf, yelp_data_final_update['Review_category_cool_notcool_binary'])\n", "intent": "Random Forest for cool not cool:\n"}
{"snippet": "model_logistic_regression=LogisticRegression()\n", "intent": "Use logistic Regression to classify the reivews into useful vs not useful:\n"}
{"snippet": "model_logistic_regression=LogisticRegression()\n", "intent": "Use logistic Regression:\n"}
{"snippet": "diabetes_X_train = diabetes.data[:-20]\ndiabetes_X_test = diabetes.data[-20:]\ndiabetes_y_train = diabetes.target[:-20]\ndiabetes_y_test = diabetes.target[-20:]\nregr = LinearRegression()\nregr.fit(diabetes_X_train, diabetes_y_train)\n", "intent": "We ran linear regression using only one feature. Now we will run linear regression with all the features in the dataset.\n"}
{"snippet": "from sklearn import neighbors\nknn = neighbors.KNeighborsClassifier(n_neighbors=10)\nknn.fit(X,y) \n", "intent": "Let us choose a model and fit the training data:\n"}
{"snippet": "model_ridge = Pipeline(\n    steps=[\n        ('preprocessing', preprocessing),\n        ('ridge', Ridge(random_state=RANDOM_SEED))\n    ]\n)\ncv = GridSearchCV(model_ridge, param_grid={}, scoring='neg_mean_absolute_error', cv=TimeSeriesSplit(n_splits=5),\n                 return_train_score=True, verbose=3)\ncv.fit(X_train, y_train)\n", "intent": "Let's try Ridge from sklearn.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nclf = LinearRegression()\nclf.fit(data.data, data.target)\n", "intent": "Let's make predictions\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X,Y)\n", "intent": "Fitting of a Logistic Regression\n"}
{"snippet": "lr_model3 = LinearRegression()\nlr_model3.fit(X_train,y_train)\n", "intent": "Next, let us try using all of the variables (in the reduced selection)\n"}
{"snippet": "km = KMeans(n_clusters=20)\n", "intent": "Try creating a KMeans clusterer with 20 classes (obviously 10 would be ideal, but let's try 20 first).  Fit the model to the digits data.\n"}
{"snippet": "lsi = models.LsiModel(tfidf_corpus, id2word=id2word, num_topics=300)\n", "intent": "- Now let's build an LSI space!\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=3, metric=smp.cosine_distances)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)\n", "intent": "- And now classification:\n"}
{"snippet": "left_branch = Sequential()\nleft_branch.add(Dense(32, input_dim=784))\nright_branch = Sequential()\nright_branch.add(Dense(32, input_dim=784))\nmerged = Merge([left_branch, right_branch], mode='concat')\nfinal_model = Sequential()\nfinal_model.add(merged)\nfinal_model.add(Dense(10, activation='softmax'))\nSVG(model_to_dot(final_model, show_shapes=True).create(prog='dot', format='svg'))\n", "intent": "- Merge multiple `Sequential` models into a single layer\n- A number of options for merging outputs: `concat`, `sum`, `ave`, etc\n- Like so:\n"}
{"snippet": "skipgram = Sequential()\nskipgram.add(Embedding(input_dim=V, input_length=1, embeddings_initializer=\"glorot_uniform\", output_dim=100))\nskipgram.add(Reshape((dim, )))\nskipgram.add(Dense(input_dim=dim, units=V, activation='softmax'))\nSVG(model_to_dot(skipgram, show_shapes=True).create(prog='dot', format='svg'))\n", "intent": "- Lastly, we create the (shallow) network!\n"}
{"snippet": "estimator = LogisticRegression()\nplot_learning_curve(estimator, \"Logistic Learning Curve\",cancer.iloc[:,cancer.columns != 'survival_status'], cancer['survival_status']);\n", "intent": "Nodes are the highest indicator followed by age and year operated\n"}
{"snippet": "clf = SVC(class_weight='balanced', gamma='scale', random_state=17)\nparams = {\n    'C': [0.01, 0.1, 1, 10],\n    'kernel': ('linear', 'poly', 'rbf', 'sigmoid')\n}\ngscv = GridSearchCV(clf, params, scoring='recall', cv=cv)\ngscv.fit(X_train, y_train)\ngscv.best_params_\n", "intent": "We choose SVM classifier with balanced weights. Our scoring metrics is recall. We are tuning the regularization parameter `C` and the `kernel` type.\n"}
{"snippet": "rfmodel4=RandomForestRegressor(n_estimators = 2000, min_samples_leaf=5,  n_jobs=-1, max_features=5) \nrfmodel4.fit(X5_train,y5_train)\n", "intent": "**Random Forests again!!**\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: test_x,\n            labels_: test_y,\n            keep_prob:1.0}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "sess = tf.Session()\nval_m2 = np.array([[2],[3]])\nres = sess.run(fetches=..., feed_dict=...) \nprint(res)\nsess.close()\n", "intent": "Provide the correct feeds (inputs) and fetches (outputs of the computational graph)\n"}
{"snippet": "name = 'fc_autoencoder'\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=(784),activation='relu',name=\"bottleneck\"))\nmodel.add(Dense(784,activation='sigmoid',name=\"reconstruction\"))\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adadelta')\n", "intent": "We use a fc NN with a bottleneck architecture and the objective to reconstruct the input image.\n"}
{"snippet": "from keras.models import Model\nmodel_bottleneck = Model(inputs=model.input, outputs=model.get_layer('bottleneck').output)\n", "intent": "Extracting the bottleneck features and predicting the reconstruction\n"}
{"snippet": "from keras.models import Model\nmodel_bottleneck = Model(inputs=model.input, outputs=model.get_layer('conv2d_3').output)\n", "intent": "Extracting the bottleneck features and predicting the reconstruction\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "lm2 = LinearRegression(fit_intercept=False)\nlm2.fit(X, bos.PRICE)\n", "intent": "I would recommend not having an intercept as we are saying then that expected y given x = 0 is zero but we almost never can know that for sure.\n"}
{"snippet": "clf = SVC(kernel='linear', class_weight='balanced', gamma='scale', random_state=17)\nparams = {\n    'C': [0.005, 0.01, 0.02]\n}\ngscv = GridSearchCV(clf, params, scoring='recall', cv=cv)\ngscv.fit(X_train, y_train)\ngscv.best_params_\n", "intent": "Our best parameters are: linear kernel with C = 0.01. Now let's make one more cross-validation with closer range of the parameter `C`.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(wine_mag_train, wine_abv_train)\n", "intent": "Then, we fit the model to our data.\n"}
{"snippet": "clf = Perceptron(n_iter=10)\nclf.fit(X, y)\n", "intent": "*How do we fit the perceptron model?* [perceptron definition](\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ngs = GridSearchCV(estimator=pipe_lasso, \n                  param_grid={'lasso__alpha':lambdas}, \n                  scoring='neg_mean_squared_error', \n                  cv=10)\ngs = gs.fit(X_train, y_train)\ngs.best_params_\n", "intent": "*How does this look in Python?*\n"}
{"snippet": "x = tf.nn.softmax([2.0, 1.0, 0.2])\n", "intent": "TensorFlow Softmax\nWe're using TensorFlow to build neural networks and, appropriately, there's a function for calculating softmax.\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, save_file)\n    test_accuracy = sess.run(\n        accuracy,\n        feed_dict={features: mnist.test.images, labels: mnist.test.labels})\nprint('Test Accuracy: {}'.format(test_accuracy))\n", "intent": "Load a Trained Model\nLet's load the weights and bias from memory, then check the test accuracy.\n"}
{"snippet": "with tf.Session() as sess:\n  result = sess.run([product])\n  print(result)\n", "intent": "If you do not specify a graph, the default graph is used.\nTo launch your graph you need a `Session` object.\n"}
{"snippet": "with tf.Session() as sess:\n  with tf.device(\"/cpu:0\"):\n    matrix1 = tf.constant([[3., 3.]])\n    matrix2 = tf.constant([[2.],[2.]])\n    product = tf.matmul(matrix1, matrix2)\n    result = sess.run(product)\n    print(result)\n", "intent": "You can tell the session which device to use for doing a computation.\n"}
{"snippet": "input1 = tf.constant([3.0])\ninput2 = tf.constant([2.0])\ninput3 = tf.constant([5.0])\nintermed = tf.add(input2, input3)\nmul = tf.mul(input1, intermed)\nwith tf.Session() as sess:\n  result = sess.run([mul, intermed])\n  print(result)\n", "intent": "`Session.run()` returns the results of the defined operations.\n"}
{"snippet": "x = tf.placeholder(tf.int32)\nwith tf.Session() as sess:\n    output = sess.run(x, feed_dict={x : 23})\n    output = sess.run(x, feed_dict={x : \"5\"}) \n    output = sess.run(x, feed_dict={x : 12.34}) \nprint(output)\n", "intent": "https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops/placeholders\nBasic structure: tf.placeholder(dtype, shape=None, name=None)\n"}
{"snippet": "clf = SVC(C=0.01, kernel='linear', class_weight='balanced', gamma='scale', random_state=17)\nclf.fit(X_train, y_train)\n", "intent": "Still C = 0.01 is the best, so we use it to train the whole train dataset.\n"}
{"snippet": "hidden_inputs = tf.add(tf.matmul(features,weights_hidden),biases_hidden)\nhidden_outputs = tf.add(tf.matmul(hidden_inputs,weights_out),biases_out)\nprediction_hidden = tf.nn.relu(hidden_outputs)\n", "intent": "$$ x_h = x.W_{x -> h} + b_{h}$$\n$$ x_h = relu (x_h) $$\n$$ x_o = x.W_{h -> o} + b_{o}$$\n"}
{"snippet": "logits = tf.nn.softmax(hidden_outputs)\n", "intent": "$$ z_i = \\frac{e^{x_{o,i}}}{\\sum_i{e^{x_{o,i}} }}$$\nwhere i is the output from the i-th neuron in the softmax layer\n"}
{"snippet": "writer.add_graph(sess.graph)\n", "intent": "tensorboard --logdir=[PATH TO LOGDIR]\n"}
{"snippet": "model.fit(x_train, y_train, epochs=100, batch_size=1000, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "clf.fit(x_train,y_train)\n", "intent": "Now its time to train our model on our training data!\n** Import LinearRegression from sklearn.linear_model **\n"}
{"snippet": "clf = LogisticRegression()\nclf.fit(X_train,y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "model.fit(np.expand_dims(X_train, -1), y_train, epochs=5)\n", "intent": "Train the network and use the test data as the validation set. \n"}
{"snippet": "with tf.name_scope('loss'):\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_logits))\nwith tf.name_scope('train'):\n    train_step = tf.train.AdamOptimizer(LR, name='adam_optimizer').minimize(loss)\nwith tf.name_scope('accuracy'):\n    corrects = tf.equal(tf.argmax(tf.nn.softmax(y_logits),1), tf.argmax(y_test, 1))\n    acc = tf.reduce_mean(tf.cast(corrects, tf.float32))\n", "intent": "Set up our loss and training algorithms. We'll use `Adam` here, which is a tweak on the way that stochastic gradient descent works.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, './saved_model/cnn_mnist.ckpt')\n    preds = sess.run(tf.nn.softmax(y_logits), feed_dict={X: X_test})\n    pred_val = tf.argmax(preds, 1).eval()\n", "intent": "Collect the predictions for the test set.\n"}
{"snippet": "logit = LogisticRegression(solver='lbfgs', n_jobs=-1, random_state=7)\nlogit.fit(X_train, y_train)\n", "intent": "**The next step is to train Logistic Regression.**\n"}
{"snippet": "from sklearn import tree\nX = df.drop('Play', axis=1)\ny = df['Play']\nclf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\nclf.fit(X, y)\n", "intent": "Now we use the decision tree classifier in sklearn to build our model.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)\nclf.fit(X_train, y_train)\n", "intent": "2. import knn classifier, k = 3\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_weights = tf.Variable(tf.truncated_normal([vocab_size, embed_dim], stddev=0.1), name=\"embedding\")\n    embedding = tf.nn.embedding_lookup(embedding_weights, input_data)\n    return embedding\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1024, activation = 'relu', input_shape=(1000,)))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=50, batch_size=1000, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.metrics import roc_auc_score\nparams = {'n_neighbors': arange(1, 100, 1)}\ngrid_searcher = GridSearchCV(KNN(),\\\n                             params, cv=8, scoring='roc_auc', n_jobs=3)\nprint grid_searcher.fit(X_train, y_train)\nprint grid_searcher.best_score_\nprint grid_searcher.best_estimator_\n", "intent": "Use 5-fold crossvalidation for finding optimal K in KNN\n"}
{"snippet": "params = {'C': [0.1, 1, 10]}\ngrid_searcher = GridSearchCV(SVC(probability=True), \\\n                            params, \\\n                            cv = 8, n_jobs = -1)\nprint grid_searcher.fit(Xtrain, ytrain)\nprint grid_searcher.decision_function\nprint grid_searcher.best_score_\nprint grid_searcher.best_estimator_\n", "intent": "GridSearchCV for SVC and then mix it with RandomForestClassifier and ExtraTreesClassifier.\n"}
{"snippet": "regr = Lasso(alpha=alpha_best)\nregr.fit(X_train, y_train)\nregr.coef_\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim = x_train.shape[1]))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "lasso_cv = LassoCV(alphas=alphas, \n                   cv=3, random_state=17)\nlasso_cv.fit(X, y)\n", "intent": "**Now let's find the best value of $\\alpha$ during cross-validation.**\n"}
{"snippet": "a1 = activation(torch.mm(features, W1) + B1)\na2 = activation(torch.mm(a1, W2) + B2)\na2\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1000, input_dim = x_train.shape[1], activation = 'relu'))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.summary()\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "def classify(model, img):\n    img = img.cuda()\n    img = Variable(img, volatile=True)\n    output = model(img)\n    return output.data.max(1)[1]\n", "intent": "This function conducts inference on a single image\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "callback = [ EarlyStopping( monitor = 'val_loss', patience = 2, verbose = 1 ) ]\nmodel.fit( X_train, y_train, shuffle = True,\n           nb_epoch = 10, batch_size = 256,\n           validation_data = (X_val, y_val),\n           callbacks = callback )\n", "intent": "```python\nfrom keras.utils.visualize_util import plot\nplot( model, to_file = 'model.png', show_shapes = True, show_layer_names = True )\n```\n"}
{"snippet": "X = T.matrix('X') \ny = T.lvector('y') \n", "intent": "The first thing we need to is define our computations using Theano. We start by defining our input data matrix `X` and our training labels `y`:\n"}
{"snippet": "forward_prop = theano.function([X], y_hat)\ncalculate_loss = theano.function([X, y], loss)\npredict = theano.function([X], prediction)\nforward_prop([[1,2]])\n", "intent": "We saw how we can evaluate a Theano expression by creating a [Theano function](http://deeplearning.net/software/theano/library/compile/function.html\n"}
{"snippet": "import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nnp.random.seed(7)\n", "intent": "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nte5 = TextExplainer(clf=DecisionTreeClassifier(max_depth=2), random_state=0)\nte5.fit(doc, pipe.predict_proba)\nprint(te5.metrics_)\nte5.show_weights()\n", "intent": "it uses words as features and doesn't take word position in account\n"}
{"snippet": "from xgboost import XGBRegressor \nxgb = XGBRegressor()\nxgb.fit(X_train_scaled, y_train);\n", "intent": "Why shouldn't we try XGBoost now?\n<img src=\"https://habrastorage.org/files/754/a9a/26e/754a9a26e59648de9fe2487241a27c43.jpg\"/>\n"}
{"snippet": "weight, params = [], []\nfor c in np.arange( -4, 6 ) :\n    lr = LogisticRegression( penalty = \"l1\", C = 10 ** c, random_state = 0 )\n    lr.fit( x_train_std, y_train )\n    weight.append(lr.coef_[1])\n    params.append(10 ** c)\n", "intent": "Plotting the regularization Path, weight coefficient of the different features for different regularization.\n"}
{"snippet": "knn.fit( x_train_std, y_train )\nprint( \"Training Accuracy\", knn.score( x_train_std, y_train ) )\nprint( \"Testing Accuracy\", knn.score( x_test_std, y_test ) )\n", "intent": "Compare performance. Using less feature to obtain a higher accuracy and reduce overfitting.\n"}
{"snippet": "clf = gs.best_estimator_\nclf.fit( x_train, y_train )\nprint( 'Test accuracy: %.3f' % clf.score( x_test, y_test ) )\n", "intent": "Then use the `best_estimator` to estimate the performance of the best selected model.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nparams = { 'dt__max_depth': [1, 2],\n           'lr__clf__C': [0.001, 0.1, 100.0] }\ngrid = GridSearchCV( estimator = clf_vc, \n                     param_grid = params, \n                     cv = 10, scoring = 'roc_auc' )\ngrid.fit( X_train, y_train )\nfor params, mean_score, scores in grid.grid_scores_:\n    print( \"%0.3f+/-%0.2f %r\"\n           % ( mean_score, scores.std() / 2, params ) ) \n", "intent": "We can also use it with grid search to obtain the best set of hyperparameters.\n"}
{"snippet": "import theano\nimport theano.tensor as T\nimport numpy\nx = T.fvector('x')\nW = theano.shared(numpy.asarray([0.2, 0.7]), 'W')\ny = (x * W).sum()\nf = theano.function([x], y)\noutput = f([1.0, 1.0])\nprint(output)\n", "intent": "http://www.marekrei.com/blog/theano-tutorial/\nhttps://github.com/bbc/theano-bpr\n"}
{"snippet": "model.fit(x_train, y_train, \n          batch_size=32,\n          epochs=10,\n          validation_data=(x_test, y_test),\n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "lstm_mpc = Sequential()\nlstm_mpc.add(embedding_layer)\nlstm_mpc.add(LSTM(128, dropout=.2, recurrent_dropout=.2, return_sequences=True))\nlstm_mpc.add(LSTM(128, dropout=.2, recurrent_dropout=.2))\nlstm_mpc.add(Dense(num_classes, activation='sigmoid'))\nlstm_mpc.compile(loss='categorical_crossentropy', \n              optimizer='rmsprop', \n              metrics=['accuracy'])\nprint(lstm_mpc.summary())\n", "intent": "You have free reign to provide additional analyses.\nAnother Idea: Try to create a RNN for generating novel text. \n"}
{"snippet": "kmeans.fit(college.drop('Private', axis = 1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "scaler.fit(KNN_data.drop('TARGET CLASS', axis = 1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "isof = IsolationForest(random_state=77,n_jobs=4,contamination=0.05)\n", "intent": "We will test this implementation of IF on our 2-d dataset.\n"}
{"snippet": "from sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X_train,y_train)\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n", "intent": "**Create a dictionary called param_grid and fill out some parameters for C and gamma.**\n"}
{"snippet": "kmeans_iter1 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n                     algorithm=\"full\", max_iter=1, random_state=1)\nkmeans_iter2 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n                     algorithm=\"full\", max_iter=2, random_state=1)\nkmeans_iter3 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n                     algorithm=\"full\", max_iter=3, random_state=1)\nkmeans_iter1.fit(X)\nkmeans_iter2.fit(X)\nkmeans_iter3.fit(X)\n", "intent": "Let's run the K-Means algorithm for 1, 2 and 3 iterations, to see how the centroids move around:\n"}
{"snippet": "KMeans()\n", "intent": "To set the initialization to K-Means++, simply set `init=\"k-means++\"` (this is actually the default):\n"}
{"snippet": "gms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\n", "intent": "Let's train Gaussian Mixture models with various values of $k$ and measure their BIC:\n"}
{"snippet": "min_bic = np.infty\nfor k in range(1, 11):\n    for covariance_type in (\"full\", \"tied\", \"spherical\", \"diag\"):\n        bic = GaussianMixture(n_components=k, n_init=10,\n                              covariance_type=covariance_type,\n                              random_state=42).fit(X).bic(X)\n        if bic < min_bic:\n            min_bic = bic\n            best_k = k\n            best_covariance_type = covariance_type\n", "intent": "Let's search for best combination of values for both the number of clusters and the `covariance_type` hyperparameter:\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\n", "intent": "The integer passed in the verbose argument will allow the output to be more or less detailed.\n"}
{"snippet": "grid.fit(X_train,y_train)\n", "intent": "Calling `grid.fit()` may take lots of time. This underscores the importance of `verbose = n` to display output.\n"}
{"snippet": "dropout_layer = tf.nn.dropout(full_layer, keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "from sklearn.externals import joblib\nfrom dask.distributed import Client\nclient = Client()\nparameters = {'clf__C': (0.1, 1, 10, 100)}\ngrid_search = GridSearchCV(classifier, parameters, scoring ='roc_auc', cv=skf)\nt_start = time.time()\nwith joblib.parallel_backend('dask'):\n    grid_search.fit(X_text, y_text)\nt_end = time.time()\nprint('Elapsed time for grid_search with joblib replace (s):', round((t_end - t_start)))    \n", "intent": "In this approach all we need to do is replace joblib to dask distributed. We need to initialize distributed client, and change backend\n"}
{"snippet": "slr = SimpleLinearRegression(fit_intercept=True)\nslr.fit_intercept\n", "intent": "**Now, if we instantiate the class, it will assign `fit_intercept` to the class attribute `fit_intercept`. Try it out:**\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nslr = LinearRegression()\nslr.fit(X_train, y_train)\n", "intent": "<a id='fit-on-train'></a>\nUsing the training `X` and training `y`, we can fit a linear regression with sklearn's `LinearRegression`.\n"}
{"snippet": "knn5 = KNeighborsClassifier(n_neighbors=5, weights='uniform')\nscores = accuracy_crossvalidator(xs, y, knn5, cv_indices)\n", "intent": "<a id='cv-knn5'></a>\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(Xs, y)\n", "intent": "**10.E Fit a `KNeighborsClassifier` with the best number of neighbors.**\n"}
{"snippet": "logreg_cv = LogisticRegressionCV(Cs=100, cv=5, penalty='l1', scoring='accuracy', solver='liblinear')\nlogreg_cv.fit(x_train, y_train)\n", "intent": "**Gridsearch hyperparameters for the training data.**\n"}
{"snippet": "param_test6 = {\n    'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n}\ngsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n                                        min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n                       param_grid = param_test6, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch6.fit(train[predictors],train[target])\n", "intent": "Try regularization:\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nparam_grid = {'n_neighbors': np.arange(1, int(k+(k/2)), 1),\n              'weights': [\"uniform\", \"distance\"]}\nknn_r = GridSearchCV(neighbors.KNeighborsRegressor(), param_grid)\nknn_r.fit(X_train, y_train)\n", "intent": "A more thorough analysis allows for checking multiple values for any parameter, let's look for the best model by looking at a range of values for `k`\n"}
{"snippet": "tfidf = models.TfidfModel(corpus)\ncorpus_tfidf = tfidf[corpus]\n", "intent": "For LSI we first create a tfidf similar to above with clustering:\n"}
{"snippet": "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=6)\ncorpus_lsi = lsi[corpus_tfidf]\nlsi.print_topics(6)\n", "intent": "We then build the model:\n"}
{"snippet": "', '.join(PolynomialFeatures().fit(df_train).get_feature_names())\n", "intent": "Note that with its default settings PolynomialFeatures will add 15 new features:\n"}
{"snippet": "kmeans = cluster.KMeans(n_clusters=4)\nkmeans.fit(X)\n", "intent": "- Select a number of clusters of your choice based on your visual analysis above.\n"}
{"snippet": "def cluster_plotter(eps=1.0, min_samples=5):\n    db = DBSCAN(eps=eps, min_samples=min_samples)\n    db.fit(X)\n    plot_clusters(X, db.point_cluster_labels)\n", "intent": "---\n<a id='plot-fit'></a>\nDon't pass `X` in to the function. We will just use the \"global\" X defined in the jupyter notebook earlier.\n"}
{"snippet": "dbscn = DBSCAN(eps =3, min_samples = 3).fit(X)\n", "intent": "Remember to pass an `eps` and `min_samples` of your choice.\n"}
{"snippet": "dbscn = DBSCAN(eps = .5, min_samples = 3).fit(X)\n", "intent": "**9.3 Evaluate DBSCAN visually, with silhouette, and with the metrics against the true `y`.**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1000, activation='relu'))\nmodel.add(Dropout(.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "random_forest_default_model = RandomForestClassifier()\nrandom_forest_default_model.fit(X_train, y_train)\n", "intent": "First, model with default parameters will be trained to check accuracy and AUROC.\n"}
{"snippet": "random_forrest_optimized_gsearch2 = GridSearchCV(RandomForestClassifier(bootstrap=True),\n                                                 {'n_estimators' : [110, 120, 130, 150],\n                                                  'max_features' : ['auto', 'log2', 'sqrt']},\n                                                 scoring='roc_auc',\n                                                 verbose=1,\n                                                 n_jobs=2)\nrandom_forrest_optimized_gsearch2.fit(train, train_labels)\ndisplay(random_forrest_optimized_gsearch2.best_params_)\nprint(\"Best score {}\".format(random_forrest_optimized_gsearch2.best_score_))\n", "intent": "Bootstrap value will be taken from previous search.\n"}
{"snippet": "solution_current = solution_best = np.random.binomial(1, 0.5, ncols).astype(bool)\nsolution_vars = predictors[predictors.columns[solution_current]]\ng = LinearRegression().fit(X=solution_vars, y=logsalary)\naic_best = aic(g, solution_vars, logsalary)\naic_values.append(aic_best)\n", "intent": "Initialize the annealing run and temperature schedule:\n"}
{"snippet": "def get_phrase_embedding(phrase):\n    vector = np.zeros([model.vector_size], dtype='float32')\n    word_count = 0\n    for word in phrase.split():\n        if word in model.vocab:\n            vector += model.get_vector(word)\n            word_count += 1\n    if word_count:\n        vector /= word_count\n    return vector\n", "intent": "Our categories mingled, but we can notice that years, days, languages are stays apart from authors cloud.\n"}
{"snippet": "step = theano.shared(10., name='step')\ntrain = theano.function(\n          inputs=[x, y],\n          outputs=[prediction, xent],\n          updates=((w, w - step * gw), (b, b - step * gb), (step, step * 0.99)))\npredict = theano.function(inputs=[x], outputs=prediction)\n", "intent": "Compile Theano functions:\n"}
{"snippet": "with disaster_model:\n    early_mean = pm.Exponential('early_mean', lam=1)\n    late_mean = pm.Exponential('late_mean', lam=1)\n", "intent": "Similarly, the rate parameters can automatically be given exponential priors:\n"}
{"snippet": "from pymc3.glm import glm\nwith pm.Model() as model:\n    glm('y ~ x', data)\n    fit = pm.advi(n=50000)\n", "intent": "The model can then be very concisely specified in one line of code.\n"}
{"snippet": "with pm.Model() as model:\n    alpha = pm.Gamma('alpha', 1., 1.)\n    beta = pm.Beta('beta', 1., alpha, shape=K)\n    w = pm.Deterministic('w', beta * T.concatenate([[1], T.extra_ops.cumprod(1 - beta)[:-1]])) \n    component = pm.Categorical('component', w, shape=N)\n    tau = pm.Gamma('tau', 1., 1., shape=K)\n    mu = pm.Normal('mu', 0, tau = tau, shape=K)\n    obs = pm.Normal('obs', mu[component], tau = tau[component],\n                    observed=old_faithful_df.std_waiting.values)\n", "intent": "We can implement this in `pymc3` in the following way:\n"}
{"snippet": "N = sunspot_df.shape[0]\nK = 30\nwith pm.Model() as model:\n    alpha = pm.Gamma('alpha', 1., 1.)\n    beta = pm.Beta('beta', 1, alpha, shape=K)\n    w = pm.Deterministic('w', beta * T.concatenate([[1], T.extra_ops.cumprod(1 - beta[:-1])]))\n    component = pm.Categorical('component', w, shape=N)\n    mu = pm.Uniform('mu', 0., 300., shape=K)\n    obs = pm.Poisson('obs', mu[component], observed=sunspot_df['sunspot.year'])\n", "intent": "The model is specified as:\n"}
{"snippet": "import numpy as np\nn = 1000\nboot_samples = np.empty((n, len(lrmod.coef_[0])))\nfor i in np.arange(n):\n    boot_ind = np.random.randint(0, len(X0), len(X0))\n    y_i, X_i = y.values[boot_ind], X0.values[boot_ind]\n    lrmod_i = LogisticRegression(C=1000)\n    lrmod_i.fit(X_i, y_i)\n    boot_samples[i] = lrmod_i.coef_[0]\n", "intent": "We can bootstrap some confidence intervals:\n"}
{"snippet": "km_wine = KMeans(n_clusters=3, random_state=rng)\nkm_wine.fit(X_pca)\n", "intent": "We can now create a `KMeans` object with `k=3`, and fit the data with it.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nfit = LinearRegression().fit(y=bodyfat['fat'], X=bodyfat[subsets[3]])\n", "intent": "We can use scikit-learn to estimate an ordinary least squares (OLS) model for a given set of predictors.\n"}
{"snippet": "from sklearn.linear_model import RidgeCV\nplot_learning_curve(LinearRegression())\nplot_learning_curve(Ridge())\nplot_learning_curve(RidgeCV())\n", "intent": "scikit-learn's `RidgeCV` class automatically tunes the L2 penalty using Generalized Cross-Validation.\n"}
{"snippet": "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n                          meta_classifier=LogisticRegression())\ncompare(sclf)\n", "intent": "To get more information look https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/\n"}
{"snippet": "rf = RandomForestClassifier(n_jobs=4, criterion='entropy')\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\nindices = np.argsort(importances)[::-1]\nprint(\"Feature ranking:\")\nfor f in range(X.shape[1]):\n    print(\"%d. %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n", "intent": "`RandomForestClassifier` uses the Gini impurity index by default; one may instead use the entropy information gain as a criterion.\n"}
{"snippet": "est = GradientBoostingRegressor(n_estimators=3000, max_depth=6, learning_rate=0.04,\n                                loss='huber', random_state=0)\nest.fit(X_train, y_train)\n", "intent": "Lets fit a gradient boosteed regression tree (GBRT) model to this dataset and inspect the model:\n"}
{"snippet": "z1 = X.dot(W1) + b1\na1 = T.tanh(z1)\nz2 = a1.dot(W2) + b2\ny_hat = T.nnet.softmax(z2) \nloss_reg = 1./num_examples * alpha/2 * (T.sum(T.sqr(W1)) + T.sum(T.sqr(W2))) \nloss = T.nnet.categorical_crossentropy(y_hat, y).mean() + loss_reg\nprediction = T.argmax(y_hat, axis=1)\n", "intent": "Our definition of forward propagation in Theano is identical to our pure Python implementation, except that we now define Theano expressions. \n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix,roc_auc_score\nsplit_ind = 0.7 * len(Y)\nX_train = X[:split_ind]\nY_train = Y[:split_ind]\nX_test = X[split_ind:]\nY_test = Y[split_ind:]\nn_est_lim = 1000\ngbc = GradientBoostingClassifier(n_estimators = n_est_lim, max_depth = 2)\ngbc.fit(X_train, Y_train)\n", "intent": "<p>\nNow let's compare training and test error as a function of the \n<p>\n"}
{"snippet": "def best_threshold():\n    maximum_ig = 0\n    maximum_threshold = 0\n    for threshold in X['number_pets']:\n        ig = information_gain(X['number_pets'], threshold, np.array(Y))\n        if ig > maximum_ig:\n            maximum_ig = ig\n            maximum_threshold = threshold\n    return \"The maximum IG = %.3f and it occured by splitting on %.4f.\" % (maximum_ig, maximum_threshold)\nprint best_threshold()\n", "intent": "To be more precise, we can iterate through all values and find the best split.\n"}
{"snippet": "decision_tree = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\")\ndecision_tree.fit(X, Y)\nDecision_Tree_Image(decision_tree, X.columns)\n", "intent": "Let's see how we can do this with just sklearn!\n"}
{"snippet": "decision_tree = DecisionTreeClassifier(max_depth=2, criterion=\"entropy\")\ndecision_tree.fit(X, Y)\nDecision_Tree_Image(decision_tree, X.columns)\n", "intent": "Let's add one more level to our decision tree.\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix,roc_auc_score\nsplit_ind = int(0.7 * len(Y))\nX_train = X[:split_ind]\nY_train = Y[:split_ind]\nX_test = X[split_ind:]\nY_test = Y[split_ind:]\nn_est_lim = 1000\ngbc = GradientBoostingClassifier(n_estimators = n_est_lim, max_depth = 2)\ngbc.fit(X_train, Y_train)\n", "intent": "<p>\nNow let's compare training and test error as a function of the \n<p>\n"}
{"snippet": "batch_size = 128\nz_dim = 100\nlearning_rate = 0.0002\nbeta1 = 0.3\nepochs = 2\nmnist_dataset = helper.Dataset('mnist', glob(os.path.join(data_dir, 'mnist/*.jpg')))\nwith tf.Graph().as_default():\n    train(epochs, batch_size, z_dim, learning_rate, beta1, mnist_dataset.get_batches,\n          mnist_dataset.shape, mnist_dataset.image_mode)\n", "intent": "It seems that 0.3/0.2 is a good value for beta1. What about learning rate? we will try 0.0002, 0.0004 and 0.0008 \n"}
{"snippet": "scvclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3], \n                            meta_classifier=LogisticRegression())\ncompare(scvclf)\n", "intent": "To get more information look https://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/\n"}
{"snippet": "happyModel.fit(x = X_train, y = Y_train, epochs = 40, batch_size = 16)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid=param_grid, refit=True, verbose=2)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "history = sa_model.fit(x_train, y_train, epochs=30, verbose=0, validation_split=0.33)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], minval=-1., maxval=1.))\n    embedded_sequence = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded_sequence\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "print(x_train[0].shape)\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=1000))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(2))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=40, batch_size=1000, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=32,\n          epochs=20,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "library(cluster)\nset.seed(1)\nisGoodCol <- function(col){\n   sum(is.na(col)) == 0 && is.numeric(col) \n}\ngoodCols <- sapply(nba, isGoodCol)\nclusters <- kmeans(nba[,goodCols], centers=5)\nlabels <- clusters$cluster\n", "intent": "Let's do a cluster plot. To do this, we need to remove columns which do not contain numeric values.\n"}
{"snippet": "from sklearn import svm\nclf = svm.SVC(gamma=0.001, C=100.)\n", "intent": "5b. Use the [scikit-learn SVM classifier](http://scikit-learn.org/stable/modules/svm.html\n"}
{"snippet": "sl = SuperLearner(folds=5, random_state=seed, verbose=2)\nsl.add([clf1, clf2, clf3])\nsl.add_meta(LogisticRegression())\ncompare(sl)\n", "intent": "To get more information follow the link http://ml-ensemble.com/info/start/ensembles.html\n"}
{"snippet": "def max_pool_2by2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1], padding='SAME')\n", "intent": "We are going to wrap in the same way as for the convolution\n"}
{"snippet": "conv_2_flat = tf.reshape(conv_2_pooling, [-1, 7^7^64])\nfull_layer_one = tf.nn.relu(normal_full_layer(conv_2_flat, 1024))\n", "intent": "We flatten back the matrix to a 1d vector, because now we want to use it as the input of a DNN\n"}
{"snippet": "hold_prob = tf.placeholder(tf.float32) \nfull_one_dropout = tf.nn.dropout(full_layer_one, keep_prob=hold_prob)\n", "intent": "We can create Dropout to avoid Overfitting\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlm_1 = LinearRegression()\n", "intent": "<big> Now we build the model\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1)\nlogreg.fit(features_train, target_train)\n", "intent": "---\nLet's start with a simple model from sklearn, namely `LogisticRegression`:\n"}
{"snippet": "plot_learning_curve(LinearSVC(C=10.0), \"LinearSVC(C=10.0) Features: 11&14\",\n                    X[:, [11, 14]], y, ylim=(0.8, 1.0),\n                    train_sizes=np.linspace(.05, 0.2, 5))\n", "intent": " * **decrease the number of features** (we know from our visualizations that features 11 and 14 are most informative)\n"}
{"snippet": "plot_learning_curve(LinearSVC(C=0.1), \"LinearSVC(C=0.1)\", \n                    X, y, ylim=(0.8, 1.0),\n                    train_sizes=np.linspace(.05, 0.2, 5))\n", "intent": " * **increase regularization of classifier** (decrease parameter C of Linear SVC)\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nest = GridSearchCV(LinearSVC(), \n                   param_grid={\"C\": [0.001, 0.01, 0.1, 1.0, 10.0]})\nplot_learning_curve(est, \"LinearSVC(C=AUTO)\", \n                    X, y, ylim=(0.8, 1.0),\n                    train_sizes=np.linspace(.05, 0.2, 5))\nprint \"Chosen parameter on 100 datapoints: %s\" % est.fit(X[:500], y[:500]).best_params_\n", "intent": "This already helped a bit. We can also select the regularization of the classifier automatically using a grid-search based on cross-validation:\n"}
{"snippet": "est = LinearSVC(C=0.1, penalty='l1', dual=False)\nest.fit(X[:150], y[:150])  \nprint \"Coefficients learned: %s\" % est.coef_\nprint \"Non-zero coefficients: %s\" % np.nonzero(est.coef_)[1]\n", "intent": "This also looks quite well. Let's investigate the coefficients learned:\n"}
{"snippet": "sub = Subsemble(partitions=3, random_state=seed, verbose=2, shuffle=True)\nsub.add([clf1, clf2, clf3])\nsub.add_meta(SVC())\ncompare(sub)\n", "intent": "To get more information follow the link http://ml-ensemble.com/info/start/ensembles.html\n"}
{"snippet": "from sklearn.svm import SVC\nplot_learning_curve(SVC(C=2.5, kernel=\"rbf\", gamma=1.0),\n                    \"SVC(C=2.5, kernel='rbf', gamma=1.0)\",\n                    X, y, ylim=(0.5, 1.1), \n                    train_sizes=np.linspace(.1, 1.0, 5))\n", "intent": " * **use more a complex model** (reduced regularization and/or non-linear kernel)\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm_microbiome = KMeans(n_clusters=2, random_state=123)\nkm_microbiome.fit(X_pca)\n", "intent": "We can now create a `KMeans` object with `k=2`, and fit the data with it.\n"}
{"snippet": "km_microbiome = KMeans(n_clusters=2, random_state=rng)\nkm_microbiome.fit(X_pca)\n", "intent": "We can now create a `KMeans` object with `k=2`, and fit the data with it.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded_input = tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def selu(z,\n         scale=1.0507009873554804934193349852946,\n         alpha=1.6732632423543772848170429916717):\n    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))\n", "intent": "The `tf.nn.selu()` function was added in TensorFlow 1.4. For earlier versions, you can use the following implementation:\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./my_model_final.ckpt\")\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, \"Validation accuracy:\", accuracy_val)\n    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    \n", "intent": "Actually, let's test this for real!\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./my_model_final.ckpt\")\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, \"Validation accuracy:\", accuracy_val)\n    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    \n", "intent": "And continue training:\n"}
{"snippet": "with tf.Session() as sess:\n    init.run()\n    saver.restore(sess, \"./my_model_final.ckpt\")\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n        print(epoch, \"Validation accuracy:\", accuracy_val)\n    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")\n", "intent": "And we can train this new model:\n"}
{"snippet": "reset_graph()\nn_inputs = 28 * 28  \nn_hidden1 = 300\nn_outputs = 10\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\ny = tf.placeholder(tf.int32, shape=(None), name=\"y\")\nwith tf.name_scope(\"dnn\"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")\n", "intent": "Let's implement $\\ell_1$ regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity):\n"}
{"snippet": "be = BlendEnsemble(test_size=0.7, random_state=seed, verbose=2, shuffle=True)\nbe.add([clf1, clf2, clf3])\nbe.add_meta(LogisticRegression())\ncompare(be)\n", "intent": "To get more information follow the link http://ml-ensemble.com/info/start/ensembles.html\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "The Data API, introduced in TensorFlow 1.4, makes reading data efficiently much easier.\n"}
{"snippet": "with tf.Session() as sess:\n    try:\n        while True:\n            print(next_element.eval())\n    except tf.errors.OutOfRangeError:\n        print(\"Done\")\n", "intent": "Let's repeatedly evaluate `next_element` to go through the dataset. When there are not more elements, we get an `OutOfRangeError`:\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "The `interleave()` method is powerful but a bit tricky to grasp at first. The easiest way to understand it is to look at an example:\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "- Reset Tensorflow graph\n"}
{"snippet": "linear_regression_model = LinearRegression(iterations, normalize=True, verbose=True)\nlinear_regression_model.fit(X, y)\n", "intent": "Fitting to generated data\n"}
{"snippet": "linear_regression = linear_model.LinearRegression(normalize=True)\n", "intent": "Creating Linear Regression model with feature scaling already embedded in model\n"}
{"snippet": "linear_regression.fit(X, y)\n", "intent": "Fitting to generated data\n"}
{"snippet": "tf.reset_default_graph()\nwith tf.name_scope(\"io-scope\"):\n    inputs_ = tf.placeholder(tf.float32, [None, DATA_FEATURES], name=\"inputs\")\n    targets_ = tf.placeholder(tf.float32, [None, DATA_LABELS], name=\"targets\")\nwith tf.name_scope(\"lr-scope\"):\n    learning_rate_ = tf.placeholder(tf.float32, None, name=\"learning_rate\")\nwith tf.name_scope(\"weights-scope\"):\n    weights = tf.Variable(tf.random_normal([DATA_FEATURES, 1]), name=\"weights\")\nwith tf.name_scope(\"bias-scope\"):\n    bias = tf.Variable(tf.random_normal([DATA_LABELS]), name=\"bias\")\n", "intent": "Model implementation\n"}
{"snippet": "polynomial_regression_model = PolynomialRegression(DATA_DEGREE, iterations, normalize=True, verbose=True)\npolynomial_regression_model.fit(X, y)\n", "intent": "Fitting to generated data\n"}
{"snippet": "se = SequentialEnsemble(random_state=seed, shuffle=True)\nse.add('blend',\n             [clf1, clf2])\nse.add('stack', [clf1, clf3])\nse.add_meta(SVC())\ncompare(se)\n", "intent": "To get more information follow the link http://ml-ensemble.com/info/start/ensembles.html\n"}
{"snippet": "regression.fit(X_pow, y)\n", "intent": "Fitting to generated data\n"}
{"snippet": "linear_regression_model = RidgeRegression(iterations, normalize=True, verbose=True)\nlinear_regression_model.fit(X, y)\n", "intent": "L2 Regularization set to 0 - reducing RidgeRegression to LinearRegression\n"}
{"snippet": "ridge_regression_model = RidgeRegression(iterations, regularization_factor=10,normalize=True, verbose=True)\nridge_regression_model.fit(X, y)\n", "intent": "L2 Regularization set to 10 - using RidgeRegression\n"}
{"snippet": "model2, history2, score2 = build_and_train_model(\n    X_train_input, y_train_input, X_val_input, y_val_input, initializer=RandomUniform(minval=0.0, maxval=1.0, seed=RANDOM_SEED))\nmodel3, history3, score3 = build_and_train_model(\n    X_train_input, y_train_input, X_val_input, y_val_input, initializer=RandomUniform(minval=-1.0, maxval=1.0, seed=RANDOM_SEED))\n", "intent": "Przetestujmy zakresy: niesymetryczny [0.0, 1.0) oraz symetryczny [-1.0, 1.0).\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators=100, random_state=17)\nrf.fit(X_train, y_train) \n", "intent": "Train a random forest **(RandomForestClassifier)**. Set the number of trees to 100 and use **random_state = 17**.\n"}
{"snippet": "forest_params = {'max_depth': range(10, 21),\n                'max_features': range(5, 105, 20)}\nlocally_best_forest = GridSearchCV(rf, forest_params, n_jobs=-1) \nlocally_best_forest.fit(X_train, y_train) \n", "intent": "Train a random forest **(RandomForestClassifier)**. Tune the maximum depth and maximum number of features for each tree using **GridSearchCV**. \n"}
{"snippet": "tree_params = {'max_depth': range(2,11)}\nlocally_best_tree = GridSearchCV(DecisionTreeClassifier(random_state=17),\n                                 tree_params, cv=5)                  \nlocally_best_tree.fit(X_train, y_train)\n", "intent": "Train a decision tree **(DecisionTreeClassifier, random_state = 17).** Find optimal maximum depth using 5-fold cross-validation **(GridSearchCV)**.\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators=100, random_state = 17)\nrf.fit(X_train, y_train)\n", "intent": "Train a random forest **(RandomForestClassifier)**, set number of trees to 100 and **random_state = 17**.\n"}
{"snippet": "forest_params = {'max_depth': range(10, 21),\n                'max_features': range(5, 105, 20)}\nlocally_best_forest = GridSearchCV(RandomForestClassifier(random_state=17,\n                                                         n_jobs=-1),\n                                 forest_params, cv=3,\n                                  verbose=1)\nlocally_best_forest.fit(X_train, y_train)\n", "intent": "Train a random forest **(RandomForestClassifier)**. Tune maximum depth and maximum number of features for each tree using **GridSearchCV**. \n"}
{"snippet": "ridge.fit(X_train, y_train);\n", "intent": "Finally, train the model on the full accessible training set, make predictions for the test set and form a submission file. \n"}
{"snippet": "ridge = Ridge(alpha=1.0)\nridge.fit(X_train_sparse, y_train);\n", "intent": "**Train the same Ridge with all available data, make predictions for the test set and form a submission file.**\n"}
{"snippet": "x = torch.tensor([5.5, 3])\nprint(x)\n", "intent": "Construct a tensor directly from data:\n"}
{"snippet": "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=25)\n", "intent": "It should take around 15-25 min on CPU. On GPU though, it takes less than a\nminute.\n"}
{"snippet": "conv2 = tf.layers.conv2d(inputs=pool1 , filters=64, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\npool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n", "intent": "** Create the next convolutional and pooling layers.  The last two dimensions of the convo_2 layer should be 32,64 **\n"}
{"snippet": "logits = tf.layers.dense(inputs=dropout, units=10)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    parameter = tf.Variable(tf.random_uniform(shape=[vocab_size,embed_dim],minval=-1,maxval=1),dtype=tf.float32)\n    return tf.nn.embedding_lookup(params=parameter,ids=input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_dim=1000))\nmodel.add(Dropout(.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "hist = model.fit(x_train, y_train, epochs=10, batch_size=50, validation_data=(x_test, y_test), verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100,criter)\nrfc.fit(X_train, y_train)\n", "intent": "Now let's compare the decision tree model to a random forest.\n"}
{"snippet": "g = Graph()\n", "intent": "$$ z = Ax + b $$\nWith A=10 and b=1\n$$ z = 10x + 1 $$\nJust need a placeholder for x and then once x is filled in we can solve it!\n"}
{"snippet": "dt = DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "km.fit(df.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "scaler.fit(df.drop(['TARGET CLASS'], axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "nb = MultinomialNB().fit(X_train, y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "grid_search = GridSearchCV(estimator=SVC(), param_grid=param_grid, verbose=3)\ngrid_search.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n    result = sess.run(negMatrix)\n", "intent": "3. See where each operation is mapped to:\n"}
{"snippet": "W = tf.Variable(tf.zeros([4, 3]), name=\"weights\")\nb = tf.Variable(tf.zeros([3], name=\"bias\"))\n", "intent": "The variable initialization code:\n"}
{"snippet": "def inference(X):\n    return tf.nn.softmax(combine_inputs(X))\n", "intent": "Tensorflow contains an embedded implementation of the softmax function:\n"}
{"snippet": "def sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n", "intent": "Defining the sigmoid function:\n"}
{"snippet": "g = Graph()\ng.set_as_default()\nA = Variable([[10,20],[30,40]])\nb = Variable([1,1])\nx = Placeholder()\ny = matmul(A,x)\nz = add(y,b)\n", "intent": "** Looks like we did it! **\n"}
{"snippet": "X = df_iris[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\ny = iris.target\nclf = tree.DecisionTreeClassifier()\nclf.fit(X, y)\n", "intent": "- Check out the code above for an example\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\ny=df['y'].values\nX=df['x'].values\nlr = LinearRegression()\nlr.fit(df[['x']], df[['y']])\n", "intent": "+ Simpson's paradox! \n"}
{"snippet": "from sklearn.linear_model import LinearRegression, Ridge\nrid = Ridge(normalize=True)\nrid.fit(X_train, y_train)\n", "intent": "+ What model you select is up to you\n+ check out sklearn documentation!\n"}
{"snippet": "def fully_connected(prev_layer, num_units):\n    layer = tf.layer.dense(prev_layer, num_units, activation=tf.nn.relu)\n    return layer\n", "intent": "def fully_connected(prev_layer, num_units):\n    layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n    return layer\n"}
{"snippet": "param_grid = [\n        {'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n         'feature_selection__k': list(range(1, len(feature_importances) + 1))}\n]\ngrid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n                                scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\ngrid_search_prep.fit(housing, housing_labels)\n", "intent": "Question: Automatically explore some preparation options using `GridSearchCV`.\n"}
{"snippet": "config = tf.ConfigProto(intra_op_parallelism_threads=1,\n                        inter_op_parallelism_threads=1)\nwith tf.Session(config=config) as sess:\n    pass\n", "intent": "You should make sure TensorFlow runs your ops on a single thread:\n"}
{"snippet": "tf.reset_default_graph()\ntf.set_random_seed(42)\nrnd = tf.random_uniform(shape=[])\nwith tf.Session() as sess:\n    print(rnd.eval())\n    print(rnd.eval())\nprint()\nwith tf.Session() as sess:\n    print(rnd.eval())\n    print(rnd.eval())\n", "intent": "Every time you reset the graph, you need to set the seed again:\n"}
{"snippet": "GB = nx.Graph()\nGB.add_nodes_from(df_all.asin.values)\n", "intent": "Build the graph as we did above.\n"}
{"snippet": "GB = nx.Graph()\nGB.add_nodes_from(df_all_new.asin.values)\nb_len_list = [] \nr_len_list = [] \nfor i in range(len(df_all_new)):\n    for j in range(i+1, len(df_all_new)):\n        b_len_list.append(len(set(df_all_new.iloc[i]['also_bought'])&set(df_all_new.iloc[j]['also_bought'])))\n        r_len_list.append(len(set(df_review_chosen.iloc[i]['reviewerID'])&set(df_review_chosen.iloc[j]['reviewerID'])))\n", "intent": "Build the graph as we did above.\n"}
{"snippet": "print(tf.get_default_graph())\n", "intent": "___\nWhen you start TF, a default Graph is created, you can create additional graphs easily:\n"}
{"snippet": "compute_kmeans(pp_G, 'conference', 2, nx.to_numpy_matrix(pp_G),np.linspace(100, 1e4, 100, dtype=int))\n", "intent": "Number of clusters : 2 (EASTERN and WESTERN Conferences)\n"}
{"snippet": "_ , dbscan_labels = apply_dbscan(nx.to_numpy_matrix(pp_G), 10)\nset(dbscan_labels) \n", "intent": "K-means technique gives results slightly better then random labelling.\n"}
{"snippet": "compute_kmeans(pp_G, 'division', 6, nx.to_numpy_matrix(pp_G), np.linspace(10, 1e4, 100, dtype=int))\n", "intent": "Number of clusters : 6\n"}
{"snippet": "compute_kmeans(pp_G, 'team', 30, nx.to_numpy_matrix(pp_G), np.linspace(10, 1e4, 100, dtype=int))\n", "intent": "Number of clusters : 30\n"}
{"snippet": "def create_team_graph(team_name, full_graph, node_color, show = True):\n", "intent": "Create individual team graphs in order to build the perfect graph:\n"}
{"snippet": "pygsp_text = utils.load_text(\"books/pygsp.txt\")\noccs_pygsp, words_map_pygsp = graph.text_to_graph(pygsp_text, undirected=True, ignore_punct=True, ignore_stopwords=True, self_links=False, nlinks=nlinks, return_words_map=True)\nwords_map_inv_pygsp = utils.inverse_dict(words_map_pygsp)\nG_pygsp = graph.np_to_nx(occs_pygsp, words_map_pygsp)\npos_pygsp, partition_pygsp, betweenness_scaled_pygsp = graph.communities(G_pygsp, draw=True, cmap=cmap)\n", "intent": "We have decided to analyze the description of your (our ?) favorite library. What do you think of the result ?\n"}
{"snippet": "Ggenre = graphs.Graph(GenreW)\nGgenre.compute_laplacian('normalized')\n", "intent": "Computation of the normalized Laplacian of this weighted graph.\n"}
{"snippet": "GText = graphs.Graph(NormTextW)\nGText.compute_laplacian('normalized')\n", "intent": "Computation of the normalized Laplacian of the graph.\n"}
{"snippet": "G = graphs.Graph(WTot)\nG.compute_laplacian('normalized')\n", "intent": "Let us first compute the normalized Laplacian of the graph and its eigenvalues:\n"}
{"snippet": "graph_one = tf.get_default_graph()\ngraph_two = tf.Graph()\n", "intent": "Setting a graph as the default:\n"}
{"snippet": "logreg = linear_model.LogisticRegression(solver ='liblinear', class_weight ='balanced')\nlogreg.fit(features_mat_perso, label)\npos_important_fts  = []\nneg_important_fts = []\nfor i in range(0, len(kept_countries)):\n    pos_important_fts.append(user_sampled_data.columns[logreg.coef_[i].argsort()[-2:][::-1]])\n    neg_important_fts.append(user_sampled_data.columns[logreg.coef_[i].argsort()[:2]])\n", "intent": "Recall that a 'personnal info' feature is 'high' if the user strongly agrees with the statement. It is low if the user disagrees with the statement. \n"}
{"snippet": "laplacian = np.diag(degrees) - adj.todense()\nlaplacian = sparse.csr_matrix(laplacian)\n", "intent": "We define the graph laplacian using the formula $L = D- A$ where $D$ is the diagonal matrix containing the degrees and $A$ is the adjacency matrix.\n"}
{"snippet": "class NeuralNetwork(torch.nn.Sequential):\n    def __init__(self, hidden_layer_size):\n        self.name = 'NN'\n        self.num_classes = 2\n        super().__init__(\n            torch.nn.Linear(2, hidden_layer_size),\n            torch.nn.ReLU(),\n            torch.nn.Linear(hidden_layer_size, self.num_classes),\n        )\n    def init_params(self, train_data):\n", "intent": "We will create a simple 2 layer neural network using the default functions provided by PyTorch\n"}
{"snippet": "model_sk.fit(X_train, y_train)\n", "intent": "Fitting model on prepared data\n"}
{"snippet": "knn_sk = KNeighborsClassifier(n_jobs=-1)\n", "intent": "Let's use sklearn KNN for parameter selection\n"}
{"snippet": "model_knn_weighted_sklearn = KNeighborsClassifier(n_jobs=-1, weights=\"distance\", p=1, n_neighbors=3)\n", "intent": "In this section you should not code anything again. Just see.\n"}
{"snippet": "model_lr = LogisticRegression(C = 1e8, \n                              max_iter=200, \n                              solver=\"sag\", \n                              multi_class=\"multinomial\")\n", "intent": "Let's take Sklearn implementation of logistic ragression and build a baseline model\n"}
{"snippet": "plot_model(dummy_predictor(), X_train, y_train, (X_valid, y_valid))\n", "intent": "Plot the decision surface\n"}
{"snippet": "model_mlp = MLPHomegrownSoftmax(layer_config=[2, 100, 3], random_state=21)\n", "intent": "Now let's check our implementation.\nFirst of all, create a model:\n"}
{"snippet": "def init_weights(shape):\n    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(init_random_dist)\n", "intent": "Function to help intialize random weights for fully connected or convolutional layers, we leave the shape attribute as a parameter for this.\n"}
{"snippet": "model_sk = LinearRegression()\n", "intent": "Here we use Scikit-learn Linear Regression model as a baseline.\n"}
{"snippet": "model_2l_ext = MLPHomegrownMSEMulti(layer_config=[13, 13, 10 , 1], random_state=21, verbose=False)\nmodel_2l_ext.fit(X_train, y_train[:, None], max_iter=40000, alpha=0.006)\n", "intent": "As one can notice this 2-layer configuration worked worse than 1-layer. Let's try to investigate the architectures further.\n"}
{"snippet": "model_2l_regl2 = MLPHomegrownMSEMultiReg(layer_config=[13, 100, 30 , 1], random_state=21, verbose=False, \n                                         l2_reg=0.05)\nmodel_2l_regl2.fit(X_train, y_train[:, None], max_iter=40000, alpha=0.005)\n", "intent": "Let's try the \"deep\" 100-30 architecture with L2 regularization which was prone to overfitting the most.\n"}
{"snippet": "model_mlp_multi_regl2 = MLPHomegrownSoftmaxMultiReg(layer_config=[2, 40, 40, 40, 3], \n                                                    random_state=21, verbose=True, \n                                                    l2_reg=0.00005)\nmodel_mlp_multi_regl2.fit(*get_data(random_state=21), max_iter=10000, alpha=5e-1)\n", "intent": "Create model with the same configuration as before (for Spirals) and train it using L2 regularization\n"}
{"snippet": "plot_model(model_mlp_multi_regl2, *get_data(random_state=21), get_data(n_samples=50, random_state=12))\n", "intent": "One can notice that all the weight matrices have smaller L2 norm and thus hopefully less prone to the overfitting.\n"}
{"snippet": "model_2l_regl1 = MLPHomegrownMSEMultiReg(layer_config=[13, 100, 30 , 1], random_state=21, verbose=False, \n                                         l1_reg=0.01)\nmodel_2l_regl1.fit(X_train, y_train[:, None], max_iter=40000, alpha=0.005)\n", "intent": "Let's try the previous Boston architecture with L1 regularization\n"}
{"snippet": "model_mlp_regl1 = MLPHomegrownSoftmaxMultiReg(layer_config=[2, 100, 3], \n                                                    random_state=21, verbose=True, \n                                                    l1_reg=0.0001)\nmodel_mlp_regl1.fit(*get_data(random_state=21), max_iter=20000, alpha=5e-1)\n", "intent": "Create model with the 1-layer configuration as before (for Spirals) and train it using L1 regularization\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\nprint(net)\n", "intent": " Define a **logistic regression** for debugging.\n"}
{"snippet": "net = ModelTorch()\nnet.add(Linear(X.shape[1], 64))\nnet.add(ELU())\nnet.add(Linear(64, y.shape[1]))\nnet.add(SoftMax())\nnet.compile(ClassNLLCriterion(), nesterov, {\"learning_rate\": 0.05, \"momentum\": 0.6})\nnp.random.seed(21)\nhist_no_BN = net.fit(X, y, iteration_hist=True, epochs=5)\n", "intent": "No Batch Normalization\n"}
{"snippet": "def init_bias(shape):\n    init_bias_vals = tf.constant(0.1, shape=shape)\n    return tf.Variable(init_bias_vals)\n", "intent": "Same as init_weights, but for the biases\n"}
{"snippet": "classification_layers = [\n    Dense(units=5, activation=\"softmax\")\n]\n", "intent": "And only one layer as a classifier\n"}
{"snippet": "model = load_model(filepath=\"chkpt\")\n", "intent": "Load the best model\n"}
{"snippet": "model = load_model(filepath=\"chkpt\", compile=False)\n", "intent": "Load previously trained model\n"}
{"snippet": "sess = K.get_session()\nfor i in range(extractor_len):\n    model.layers[i].trainable = False\nfor i in range(extractor_len, len(model.layers)):\n    l = model.layers[i]\n    new_weights = l.kernel_initializer(l.get_weights()[0].shape).eval(session = sess)\n    new_bias = l.bias_initializer(l.get_weights()[1].shape).eval(session = sess)\n    l.set_weights([new_weights, new_bias])\n", "intent": "Freeze all the layers of the feature extractor part and reinitialize layers of classification part\n"}
{"snippet": "hist = model.fit(X_train_g5_reduced, y_train_g5_reduced, \n                 batch_size=16, \n                 epochs=150, \n                 verbose=0, \n                 validation_data=(X_valid_g5, y_valid_g5))\n", "intent": "And finally fine-tune model only on small amount of data\n"}
{"snippet": "model_cached = load_model(filepath=\"chkpt\", compile=True)\n", "intent": "Load previously trained model\n"}
{"snippet": "feature_extractor_model = Model(inputs=model_cached.input,\n                                outputs=model_cached.layers[extractor_len - 1].output)\n", "intent": "Create a model that takes an output from the intermediate layer\n"}
{"snippet": "model_classification = Sequential([Dense(units=5, \n                                         activation=\"softmax\", \n                                         input_shape=(X_train_g5_reduced_features.shape[1], ))])\nmodel_classification.compile(loss=\"categorical_crossentropy\",\n                             optimizer=\"adam\",\n                             metrics=[\"accuracy\"])\n", "intent": "Build and compile a new model with only classification layers\n"}
{"snippet": "classification_layer = Dense(units=10, \n                             activation=\"softmax\")(dnn_a)\n", "intent": "Now let's build simple softmax classification layer on top of the DNN A branch.\n"}
{"snippet": "def convolutional_layer(input_x, shape):\n    W = init_weights(shape)\n    b = init_bias([shape[3]])\n    return tf.nn.relu(conv2d(input_x, W) + b)\n", "intent": "Using the conv2d function, we'll return an actual convolutional layer here that uses an ReLu activation.\n"}
{"snippet": "model_sk = LinearRegression()\n", "intent": "Here we use very simple Linear Regression model.\n"}
{"snippet": "num_plot = 50 \nplot_embedding(np.squeeze(X_train[:num_plot]), emb[:num_plot], y_train[:num_plot])\n", "intent": "To make the plot, we input the image data itself, the embedding data we fit with tSNE and class labels for the the data points\n"}
{"snippet": "num_plot = 500 \nplot_embedding(np.squeeze(X_train[:num_plot]), emb[:num_plot], y_train[:num_plot])\n", "intent": "To make the plot, we input the image data itself, the embedding data we fit with tSNE, and class labels for the the data points\n"}
{"snippet": "output = Dense(n_classes, activation=\"softmax\")(averaged)\n", "intent": "Multinomial Logistic Regression classifier on top of averaged vectors\n"}
{"snippet": "model_lin_no_mask = Model(inputs=[sequence_input], outputs=[output])\n", "intent": "Build and compile model\n"}
{"snippet": "model_lin_masked = Model(inputs=[sequence_input], outputs=[output])\n", "intent": "Build and compile model\n"}
{"snippet": "start_lin_masked = time.time()\ntrace = trace_callback()\nhistory_lin_masked = model_lin_masked.fit(X_train, y_train,\n                                          epochs=100, \n                                          batch_size=32,\n                                          validation_data=(X_valid, y_valid), \n                                          verbose=0, \n                                          callbacks=[trace])\nend_lin_masked = time.time()\n", "intent": "Train model. Here we use simple callback (defined before) to trace how many epochs have been done (default output is too long for 100 epochs)\n"}
{"snippet": "fc = Dense(64, activation=\"relu\")(averaged)\noutput = Dense(46, activation=\"softmax\")(fc)\n", "intent": "Additional FC layers + Softmax Classifier\n"}
{"snippet": "model_fc = Model(inputs=[sequence_input], outputs=[output])\n", "intent": "Build and compile model\n"}
{"snippet": "full_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "embedded_sequences = Embedding(input_dim=number_of_words + 1,\n                               output_dim=EMBEDDING_DIM,\n                               input_length=MAX_SEQUENCE_LENGTH,\n                               trainable=True,\n                               mask_zero=True, \n                               name=\"Embedding\")(sequence_input)\n", "intent": "Embed each word of input sentence. Here we do not want to use pre-trained embedding. Now we do want to use masking ($\\text{mask_zero=True}$)\n"}
{"snippet": "model_no_emb = Model(inputs=[sequence_input], outputs=[output])\n", "intent": "Build and compile model\n"}
{"snippet": "start_no_emb = time.time()\ntrace = trace_callback()\nhistory_no_emb = model_no_emb.fit(X_train, y_train,\n                                 epochs=100, \n                                 batch_size=32,\n                                 validation_data=(X_valid, y_valid), \n                                 verbose=0, \n                                 callbacks=[trace])\nend_no_emb = time.time()\n", "intent": "Train model. Here we use simple callback (defined before) to trace how many epochs have been done (default output is too long for 100 epochs)\n"}
{"snippet": "model_dense = Sequential()\n", "intent": "First of all, let's build MLP model and see how it performs\n"}
{"snippet": "model_cnn = Sequential()\n", "intent": "Now it's time to build the model step-by-step\n"}
{"snippet": "model_cnn.add(Convolution2D(filters=filters, \n                            kernel_size=kernel_size,\n                            padding=\"valid\"))\nmodel_cnn.add(Activation('relu'))\n", "intent": "Let's stack one more Convolution layer on top of that:\n"}
{"snippet": "model_cnn.add(Dropout(0.5))\nmodel_cnn.add(Dense(128, activation=\"relu\"))\nmodel_cnn.add(Dropout(0.5))\nmodel_cnn.add(Dense(nb_classes, activation=\"softmax\"))\n", "intent": "Now let's add FC part with the [dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) to avoid overfitting.\n"}
{"snippet": "model_dense = Sequential()\n", "intent": "First of all, let\"s build MLP model and see how it performs\n"}
{"snippet": "hid = Dense(units=32, activation=\"relu\")(hid)\nhid = Dropout(rate=0.5)(hid)\nout = Dense(nb_classes, activation=\"softmax\")(hid)\n", "intent": "Let\"s stack fully-conected classifier on top of that:\n"}
{"snippet": "cell = tf.contrib.rnn.OutputProjectionWrapper(\n    tf.contrib.rnn.BasicRNNCell(num_units=num_neurons, activation=tf.nn.relu),\n    output_size=num_outputs)\n", "intent": "____\n____\nPlay around with the various cells in this section, compare how they perform against each other.\n"}
{"snippet": "cnn = Convolution2D(filters=32, \n                    kernel_size=(3, 3),\n                    padding=\"valid\", \n                    activation=\"relu\")(cnn)\ncnn = Convolution2D(filters=32, \n                    kernel_size=(3, 3),\n                    padding=\"valid\", \n                    activation=\"relu\")(cnn)\ncnn = MaxPooling2D(pool_size=(2, 2))(cnn)\n", "intent": "Let\"s apply the same block one more time to further reduce dimensionality\n"}
{"snippet": "rnn = Bidirectional(LSTM(units=8, return_sequences=True), merge_mode=\"concat\")(cnn)\nrnn = LSTM(units=16, return_sequences=False)(rnn)\n", "intent": "Now let\"s come back to our pixelwise LSTM idea\n"}
{"snippet": "hid = Dense(units=32, activation=\"relu\")(rnn)\nhid = Dropout(rate=0.5)(hid)\nout = Dense(nb_classes, activation=\"softmax\")(hid)\n", "intent": "Let\"s stack fully-conected classifier on top of that:\n"}
{"snippet": "out = TimeDistributed(Dense(units=len(tags_vocabulary) + 1, \n                            activation='softmax'), \n                      name=\"Softmax\")(dense)\n", "intent": "And the final layer is usual softmax $\\text{Dense}$ layer (with the same $\\text{TimeDistributed}$ wrapper)\n"}
{"snippet": "model = Model(inputs=[sequence_input], \n              outputs=[out])\n", "intent": "Let's finally build all the pieces together and create a Keras model\n"}
{"snippet": "hist = model.fit(X_train, y_train, \n                 validation_data=(X_test, y_test), \n                 batch_size=25, epochs=20, verbose=1, \n                 class_weight={0: np.sqrt(ironic_num) * 1. / (np.sqrt(ironic_num) + np.sqrt(regular_num)), \n                               1: np.sqrt(regular_num) * 1. / (np.sqrt(ironic_num) + np.sqrt(regular_num))})\n", "intent": "Due to unbalanced classes in our problem we use the same reweighting technique as we did in CNN.\n"}
{"snippet": "cnn = Convolution2D(filters=setting.filters, \n                    kernel_size=setting.kernel_size, \n                    padding=\"same\",\n                    activation=\"relu\", \n                    name=\"conv2\")(cnn)\ncnn = MaxPooling2D(pool_size=setting.pool_size_2, \n                   name=\"pool2\")(cnn)\n", "intent": "Let's apply the same block again to make our model deeper and more representative\n"}
{"snippet": "Model(inputs=[input_data], outputs=[out]).summary()\n", "intent": "That's the main part of our model. Let's compile it and see the summary\n"}
{"snippet": "model = Model(inputs=[input_data, input_labels, input_length, label_length], outputs=[loss_out])\n", "intent": "Now we can define our final model\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./rnn_time_series_model\")\n    zero_seq_seed = [0. for i in range(num_time_steps)]\n    for iteration in range(len(ts_data.x_data) - num_time_steps):\n        X_batch = np.array(zero_seq_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n        y_pred = sess.run(outputs, feed_dict={X: X_batch})\n        zero_seq_seed.append(y_pred[0, -1, 0])\n", "intent": "** Note: Can give wacky results sometimes, like exponential growth**\n"}
{"snippet": "sess = tf.Session()\nsess.run(init)\n", "intent": "Create TF session in which we're going to do all the stuff and run initialization process\n"}
{"snippet": "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n", "intent": "Now it's time to create operator that will write summary logs.\n"}
{"snippet": "model = Model(inputs=[q_input, a_input], outputs=[loss_layer])\n", "intent": "Now we are ready to build a model\n"}
{"snippet": "nn.fit(a_train_encoded)\n", "intent": "Fit nearest neighbors model on encoded train answers\n"}
{"snippet": "h = activation(torch.mm(features, W1) + B1)\ny = activation(torch.mm(h, W2) + B2)\ny\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "test_rnn = RNN(input_size=1, output_size=1, hidden_dim=10, n_layers=2)\ntime_steps = np.linspace(0, np.pi, seq_length)\ndata = np.sin(time_steps)\ndata.resize((seq_length, 1))\ntest_input = torch.Tensor(data).unsqueeze(0) \nprint('Input size: ', test_input.size())\ntest_out, test_h = test_rnn(test_input, None)\nprint('Output size: ', test_out.size())\nprint('Hidden state size: ', test_h.size())\n", "intent": "As a check that your model is working as expected, test out how it responds to input data.\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, epochs=40, batch_size=64)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model = Sequential()\n", "intent": "- define model\n- compile model\n- fit model\n"}
{"snippet": "def create_model(learn_rate = learning_rate):\n    model = Sequential()\n    model.add(Dense(output_dim , input_dim = input_dim, kernel_initializer='normal')) \n    optimizer = Adam(lr=learn_rate)\n    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n    return model\n", "intent": "(We will just pick Adam for this run.)\n"}
{"snippet": "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n", "intent": "** Now pass in the cells variable into tf.nn.dynamic_rnn, along with your first placeholder (X)**\n"}
{"snippet": "x = Input(batch_shape=(batch_size, original_dim)) \nh = Dense(intermediate_dim, activation='relu')(x) \nz_mean = Dense(latent_dim)(h)                     \nz_log_var = Dense(latent_dim)(h)                  \n", "intent": "First, an encoder network turns the input samples x into two parameters in a latent space, which we will note z_mean and z_log_sigma.\n"}
{"snippet": "decoder_h = Dense(intermediate_dim, activation='relu')  \nh_decoded = decoder_h(z)\ndecoder_mean = Dense(original_dim, activation='sigmoid') \nx_decoded_mean = decoder_mean(h_decoded)\nh_decoded.shape, x_decoded_mean.shape\n", "intent": "Finally, a decoder network maps these latent space points back to the original input data.\n"}
{"snippet": "g = Graph()\ng.set_as_default()\n", "intent": "$$ z = Ax + b $$\n- A = 10\n- b = 1\n- Evaluate x = 10\n"}
{"snippet": "g = Graph()\ng.set_as_default()\n", "intent": "$$ z = Ax + b $$\n- A = \n\\begin{bmatrix}\n    10 & 20 \\\\\n    30 & 40\n\\end{bmatrix}\n- b = \n\\begin{bmatrix}\n    1 & 1\n\\end{bmatrix}\n- Evaluate x = 10\n"}
{"snippet": "g = Graph()\ng.set_as_default()\n", "intent": "$$z=w^Tx+b$$\n$$a=sigmoid(z)$$\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "```python\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\n```\n"}
{"snippet": "num_neurons = 3\nX_t0_ph = tf.placeholder(tf.float32,[None,2])\nX_t1_ph = tf.placeholder(tf.float32,[None,2])\nWx = tf.Variable(tf.random_normal(shape=[2,num_neurons]))\nWy = tf.Variable(tf.random_normal(shape=[num_neurons,num_neurons]))\nb = tf.Variable(tf.zeros([1,num_neurons]))\ny_t0_pred = tf.tanh(tf.matmul(X_t0_ph,Wx) + b)\ny_t1_pred = tf.tanh(tf.matmul(y_t0_pred,Wy) + tf.matmul(X_t1_ph,Wx) + b)\n", "intent": "$$y_t = tanh(W_x^Tx_t + b)$$\n$$y_{t+1}=tanh(W_y^Ty_t + W_x^Tx_{t+1} + b)$$\n"}
{"snippet": "x_t = tf.placeholder(tf.int32, (1,))\nh_t = tf.Variable(np.zeros([1, rnn_num_units], np.float32))  \nnext_probs, next_h = rnn_one_step(x_t, h_t)\n", "intent": "Once we've trained our network a bit, let's get to actually generating stuff. All we need is the `rnn_one_step` function you have written above.\n"}
{"snippet": "model = LinearRegression(normalize=True)\nprint model.normalize\n", "intent": "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated, and have suitable default values:\n"}
{"snippet": "regressor.fit(X['train'], y['train'], monitors=[validation_monitor], logdir=LOG_DIR)\n", "intent": "- fit: fitting using training data\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\ngrid.fit(X_train_scaled, y_train)\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\nprint(\"Best set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\nprint(\"Best parameters: \", grid.best_params_)\n", "intent": "* Find the best parameters for *SVC* using *GridSearch* $\\to$ Just add the grid parameters.\n"}
{"snippet": "param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n               'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\nprint(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(grid.best_params_))\n", "intent": "Pipelines and grid search are combined in the same way that any estimator.\n"}
{"snippet": "fc1 = tf.layers.dense(images, HIDDEN1_SIZE, activation=tf.nn.relu, name=\"fc1\")\nfc2 = tf.layers.dense(fc1, HIDDEN2_SIZE, activation=tf.nn.relu, name=\"fc2\")\ndropped = tf.nn.dropout(fc2, keep_prob=0.9, name=\"dropout1\")\ny = tf.layers.dense(dropped, NUM_CLASSES, name=\"output\")\n", "intent": "Up until this point, everything is effectively identical to authoring the deep neural networks with the low-level APIs. But from here on, we diverge.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogistic = LogisticRegression(C = 10000, fit_intercept=False)\nlogistic.fit(mX,mY)\n", "intent": "*Exercise: Use Sklearn to fit a logistic regression model on the gaussian mixture data.*\n"}
{"snippet": "X2 = np.vstack((np.ones((150,2)),-1*np.ones((150,2)))) + 0.3*np.random.normal(size=(300,2))\nY2 = np.hstack((np.ones(150),-1*np.ones(150)))\nfrom sklearn import svm\nsvm_instance = svm.SVC(kernel='linear')\nsvm_instance.fit(X2,Y2)\nutil.plot_svm(X2, Y2, svm_instance)\n", "intent": "*Exercise: Use Scikit.learn linear svm object to find the maximum margin hyperplane separating the dataset generated below.* \n"}
{"snippet": "X, truth = tut.load_2d_hard()\nlabels, centres = kmeans(X, 3)\ntut.plot_2d_clusters(X, labels, centres)\n", "intent": "**5)** Run K-means on the following dataset with 3 clusters and visualise the results\n"}
{"snippet": "plotKmeans(X,y)\n", "intent": "Plotting with the original labels\n"}
{"snippet": "batch_size = 128\nepochs = 2\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\n", "intent": "Now, we can fit the model.  This should take about 10-15 seconds per epoch on a commodity GPU, or about 2-3 minutes for 12 epochs.\n"}
{"snippet": "model = LinearRegression().fit(X, y)\n", "intent": "Now we can re-train the model on the entire training data\n"}
{"snippet": "lm.fit(X, bos.PRICE)\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)\n", "intent": "Train and evaluate\n^^^^^^^^^^^^^^^^^^\nIt should take around 15-25 min on CPU. On GPU though, it takes less than a\nminute.\n"}
{"snippet": "X_train_pre = np.array([grayscale(x)[:, :, np.newaxis] for x in X_train])\nX_validation_pre = np.array([grayscale(x)[:, :, np.newaxis] for x in X_validation])\nsign_pipeline(X_train_pre, y_train, X_validation_pre, y_validation, input_shape=(None, 32, 32, 1), n_classes=43, \\\n                  conv1_shape=(5, 5, 1, 6), conv2_shape=(5, 5, 6, 16), fc1_shape=(400, 120), fc2_shape=(120, 43), \\\n                  epochs=20, batch_size=128, learning_rate=0.001, save_path='checkpoint/network02.ckpt')\n", "intent": "**Apply grayscale and run the network again**\n"}
{"snippet": "X_train_pre = np.array([normalize(x, method=2) for x in X_train])\nX_validation_pre = np.array([normalize(x, method=2) for x in X_validation])\nsign_pipeline(X_train_pre, y_train, X_validation_pre, y_validation, input_shape=(None, 32, 32, 3), n_classes=43, \\\n                  conv1_shape=(5, 5, 3, 6), conv2_shape=(5, 5, 6, 16), fc1_shape=(400, 120), fc2_shape=(120, 43), \\\n                  epochs=20, batch_size=128, learning_rate=0.001, save_path='checkpoint/network03.ckpt')\n", "intent": "**Apply normalization and run the network again with more epochs**\n"}
{"snippet": "X_train_pre = np.array([augment_image(x) for x in X_train])\nsign_pipeline(X_train_pre, y_train, X_validation, y_validation, input_shape=(None, 32, 32, 3), n_classes=43, \\\n                  conv1_shape=(5, 5, 3, 6), conv2_shape=(5, 5, 6, 16), fc1_shape=(400, 120), fc2_shape=(120, 43), \\\n                  epochs=20, batch_size=128, learning_rate=0.001, save_path='checkpoint/network04.ckpt')\n", "intent": "**Apply augmentation and run the network**\n"}
{"snippet": "X_train_pre = np.array([normalize(grayscale(x), method=2)[:,:,np.newaxis] for x in X_train])\nX_validation_pre = np.array([normalize(grayscale(x), method=2)[:,:,np.newaxis] for x in X_validation])\nsign_pipeline(X_train_pre, y_train, X_validation_pre, y_validation, input_shape=(None, 32, 32, 1), n_classes=43, \\\n                  conv1_shape=(5, 5, 1, 6), conv2_shape=(5, 5, 6, 16), fc1_shape=(400, 120), fc2_shape=(120, 43), \\\n                  epochs=20, batch_size=128, learning_rate=0.001, save_path='checkpoint/network05.ckpt')\n", "intent": "**Apply both grayscale and normalization and run the network again with epochs=20**\n"}
{"snippet": "X_train_pre = np.array([normalize(grayscale(x), method=2)[:,:,np.newaxis] for x in X_train])\nX_validation_pre = np.array([normalize(grayscale(x), method=2)[:,:,np.newaxis] for x in X_validation])\nsign_pipeline(X_train_pre, y_train, X_validation_pre, y_validation, input_shape=(None, 32, 32, 1), n_classes=43, \\\n                  conv1_shape=(5, 5, 1, 12), conv2_shape=(5, 5, 12, 32), fc1_shape=(800, 240), fc2_shape=(240, 43), \\\n                  epochs=20, batch_size=128, learning_rate=0.001, save_path='checkpoint/network07.ckpt')\n", "intent": "**Apply normalization and grayscale pre-processing and double network settings**\n"}
{"snippet": "sign_pipeline(X_train_pre, y_train, X_validation_pre, y_validation, input_shape=(None, 32, 32, 1), n_classes=43, \\\n                  conv1_shape=(5, 5, 1, 12), conv2_shape=(5, 5, 12, 32), fc1_shape=(800, 240), fc2_shape=(240, 43), \\\n                  epochs=50, batch_size=128, learning_rate=0.0005, save_path='checkpoint/network08.ckpt')\n", "intent": "**Increase Number of Epochs, Descrease Learning rate with the double network settings**\n"}
{"snippet": "theta_avp,theta_hist = structure_perceptron.estimate_perceptron(training_set,\n                                                       features.word_feats,\n                                                       tagger_base.classifier_tagger,\n                                                       20,\n                                                       all_tags)\n", "intent": "The cell below takes 30 seconds to run on my laptop.\n"}
{"snippet": "theta_avp_ja,theta_hist_ja =\\\nstructure_perceptron.estimate_perceptron(training_set_ja,\n                                         features.word_feats,\n                                         tagger_base.classifier_tagger,\n                                         20,\n                                         all_tags_ja)\n", "intent": "The cell below takes approximately 40 seconds to run on my laptop.\n"}
{"snippet": "lm_no_intercept = LinearRegression(fit_intercept=False)\nlm_no_intercept.fit(X, bos.PRICE)\nprint \"Using Intercept: \",lm.score(X, bos.PRICE)\nprint \"Not using Intercept: \",lm_no_intercept.score(X, bos.PRICE)\n", "intent": "The no intercept model can be created by passing the fit_intercept option as False as shown below\n"}
{"snippet": "tagger_base.apply_tagging_model(constants.JA_TEST_FILE_HIDDEN,\n                               tagger_base.classifier_tagger,\n                               features.word_suff_feats,\n                               theta_suff_avp_ja,\n                               all_tags,\n                               'avp-words-suff-te.ja.preds')\n", "intent": "10% better on Japanese! Why might that be?\n"}
{"snippet": "theta_neighbor_avp_ja,_ =\\\nstructure_perceptron.estimate_perceptron(training_set_ja,\n                                         features.word_neighbor_feats,\n                                         tagger_base.classifier_tagger,\n                                         20,\n                                         all_tags_ja)\n", "intent": "Even better for English than the suffix features! Let's try Japanese.\nThe code below takes 60 seconds to run on my laptop.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier() \nrf.fit(features, labels) \n", "intent": "You can then create a random forest and fit it to the examples by using the following:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[10,20,10], n_classes=2)\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "kmeans.fit(cd.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "lm.fit(X, bos.PRICE)\nlm.coef_\nlm.intercept_\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "W = tf.Variable(tf.random_normal([3,3,1,32],stddev=0.01))\nW\n", "intent": "---\nTEST for understanding Convolution Network\n"}
{"snippet": "L = tf.nn.conv2d(X, W, strides=[1,1,1,1], padding='SAME')\nL\n", "intent": "* [3,3,1,32] / [3,3] filter(kernel) size / [1] input_channels / [32] output channels\n"}
{"snippet": "m_new = ols('PRICE ~ np.log(CRIM) + RM + PTRATIO + AGE + DIS', bos).fit()\nprint(m_new.summary())\n", "intent": "Adding two more variables - to the model and running the regression again,\n"}
{"snippet": "y = df[\"Hired\"]\nX = df[features]\nclf = tree.DecisionTreeClassifier()  \nclf = clf.fit(X,y)\n", "intent": "Now actually construct the decision tree:\n"}
{"snippet": "actualDemo = BernoulliNB(alpha=0.001) \nactualDemo.fit(bow_counts_train_x,np.array(train_label_y))\n", "intent": "<b>as the hyper parameter value increasing accuracy is decreasing.</b>\n"}
{"snippet": "actualDemotfidf = BernoulliNB(alpha=0.001) \nactualDemotfidf.fit(tfidf_train_x,np.array(train_label_y))\n", "intent": "<b>in starting with alpha value accuracy is very high but suddenly with the alpha increasing accuracy decreased and then become nearly stable.</b>\n"}
{"snippet": "Multinomialtfidf = MultinomialNB(alpha=0.001) \nMultinomialtfidf.fit(tfidf_train_x,np.array(train_label_y))\n", "intent": "<b>As the alpha increases accuracy decreases and after one level it became stable.</b>\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ndef Classification_model_gridsearchCV(model,param_grid,train_X,train_y, test_X, test_y):\n    clf = GridSearchCV(model,param_grid,cv=10,scoring=\"accuracy\")\n    clf.fit(train_X,train_y)\n    print(\"The best parameter found on development set is :\")\n    print(clf.best_params_)\n    print (clf.score(test_X,test_y))\n", "intent": "Use GridSearchCV to find the best parameters:\n"}
{"snippet": "lm = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=data).fit()\nprint(lm.pvalues)\n", "intent": "We could try a model with all features, and only keep features in the model if they have **small p-values**:\n"}
{"snippet": "from sklearn import linear_model\nreg = linear_model.LinearRegression()\nreg.fit (X[:,1:], y)\nprint(\"Regression coefficiants:\",reg.coef_)\nprint(\"Regression intercept:\",reg.intercept_)\n", "intent": "Just for completion, we can compare our hard coded result, to `scikit learn`'s ordinary least squares implementation of linear regression:\n"}
{"snippet": "from sklearn import linear_model\nlogreg = linear_model.LogisticRegression()\nlogreg.fit(X[:,1:], y)\nprint(\"Regression coefficiants:\",logreg.coef_)\nprint(\"Regression intercept:\",logreg.intercept_)\n", "intent": "Lets compare to `scikit`'s implementation:\n"}
{"snippet": "from sklearn import linear_model\nreg = linear_model.LinearRegression()\nreg.fit(X[:,1:], y)\nprint(\"Regression coefficiants:\",reg.coef_)\nprint(\"Regression intercept:\",reg.intercept_)\n", "intent": "Just for completion, we can compare our hard coded result, to `scikit learn`'s ordinary least squares implementation of linear regression:\n"}
{"snippet": "from nltk.classify.scikitlearn import SklearnClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nbernoulli = SklearnClassifier(BernoulliNB())\nbernoulli.train(trainData)\nprint(nltk.classify.accuracy(bernoulli, testData))\n", "intent": "Nltk also has functions that allow us to call other machine learning libraries, including scikit-learn, using wrapper classes.\n"}
{"snippet": "shared_weights = theano.shared(np.zeros(64))\ninput_X = T.matrix()\ninput_y = T.ivector()\n", "intent": "As we see, on the last iteration the train and test auc reach 1.0. Now let's try to initialize the weights to zeros.\n"}
{"snippet": "import lasagne\nimport theano\nimport theano.tensor as T\ninput_X = T.tensor4(\"X\")\ninput_shape = [None,3,32,32]\ntarget_y = T.vector(\"target Y integer\",dtype='int32')\n", "intent": "* lasagne is a library for neural network building and training\n* it's a low-level library with almost seamless integration with theano\n"}
{"snippet": "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\ndense_1 = lasagne.layers.DenseLayer(input_layer,num_units=100,\n                                   nonlinearity = lasagne.nonlinearities.sigmoid,\n                                   name = \"hidden_dense_layer\")\ndense_output = lasagne.layers.DenseLayer(dense_1,num_units = 10,\n                                        nonlinearity = lasagne.nonlinearities.softmax,\n                                        name='output')\n", "intent": "Defining network architecture\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nkfold = StratifiedKFold(n_splits=6, random_state=42, shuffle=True)\ngrid_searcher = GridSearchCV(RandomForestClassifier(\n                                random_state=42, n_estimators=20), \n                             {'criterion': ['gini', 'entropy'],\n                              'max_depth': [1, 3, 5, 7, 10, None]},\n                              cv=kfold, n_jobs=3)\ngrid_searcher.fit(X, y)\nprint 'best score:', grid_searcher.best_score_, \\\ngrid_searcher.best_params_\n", "intent": "<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], minval=-1, maxval=1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def random_forest_classifier_trainer(X,y):\n    n_estimators = [100]\n    min_samples_split = [2]\n    min_samples_leaf = [1]\n    parameters = {'n_estimators': n_estimators, 'min_samples_leaf': min_samples_leaf,\n                  'min_samples_split': min_samples_split}\n    classifier = GridSearchCV(RandomForestClassifier(verbose=1,n_jobs=-1), cv=4, param_grid=parameters)\n    classifier.fit(X, y)\n    return classifier\n", "intent": "**RandomForestClassifier:**\n"}
{"snippet": "def svm_classifier_trainer(X,y):\n    parameters = {'C': [10,15,20,25],'random_state':[2016]}\n    svc_classifier = GridSearchCV(SVC(kernel = 'rbf'), cv=4, param_grid=parameters)\n    svc_classifier.fit(X, y)\n    return svc_classifier\n", "intent": "**SVM Classifier:**\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\ndef naive_bayes_trainer(X,y):\n    naive_bayes_classifier = MultinomialNB()\n    naive_bayes_classifier.fit(X, y)\n    return naive_bayes_classifier\n", "intent": "**Naive Bayes Classifier:**\n"}
{"snippet": "classifier = RandomForestRegressor(n_estimators= 35, \n                                   max_depth= 19,\n                                   min_samples_leaf= 9,\n                                   criterion='mse', \n                                   random_state=seed)\n", "intent": "Best score=0.5309 (EI)\nExpected Improvement (EI) best parameters:\n- n_estimators= 35  \n- max_depth= 19\n- min_samples_leaf= 9 \n"}
{"snippet": "from sklearn.svm import LinearSVC\nsvm = SklearnClassifier(LinearSVC())\nsvm.train(trainData)\nprint(nltk.classify.accuracy(svm, testData))\n", "intent": "We can use any of the learning algorithms implemented by scikit-learn ([decision trees](http://scikit-learn.org/stable/modules/tree.html\n"}
{"snippet": "model.fit(X, y, epochs=200, verbose=0)\n", "intent": "We can now fit the model on the training dataset.\n"}
{"snippet": "model = Sequential()\nmodel.add(LSTM(100, activation='relu', \n               input_shape=(n_steps_in, n_features)))\nmodel.add(RepeatVector(n_steps_out))\nmodel.add(LSTM(100, activation='relu', \n               return_sequences=True))\nmodel.add(TimeDistributed(Dense(1)))\nmodel.compile(optimizer='adam', loss='mse')\n", "intent": "We define an Encoder-Decoder LSTM model for multi-step time series forecasting.\n"}
{"snippet": "model = Sequential()\nmodel.add(LSTM(100, activation='relu', \n               return_sequences=True, \n               input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(100, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer='adam', loss='mse')\n", "intent": "We definea Stacked LSTM with vector output (or an encoder-decoder model could be used).\n"}
{"snippet": "model = Sequential()\nmodel.add(LSTM(200, activation='relu', input_shape=(n_steps_in, n_features)))\nmodel.add(RepeatVector(n_steps_out))\nmodel.add(LSTM(200, activation='relu', return_sequences=True))\nmodel.add(TimeDistributed(Dense(n_features)))\nmodel.compile(optimizer='adam', loss='mse')\n", "intent": "We define a Encoder-Decoder model, but we could have used a Vector Output or Encoder-Decoder LSTM to model.\n"}
{"snippet": "visible = Input(shape=(10,))\nhidden1 = Dense(10, activation='relu')(visible) \nhidden2 = Dense(20, activation='relu')(hidden1) \nhidden3 = Dense(10, activation='relu')(hidden2) \noutput = Dense(1, activation='sigmoid')(hidden3) \nmodel = Model(inputs=visible, outputs=output)\n", "intent": "We define a Multilayer Perceptron model for binary classification.\n"}
{"snippet": "def collate(samples):\n    graphs, labels = map(list, zip(*samples))\n    batched_graph = dgl.batch(graphs)\n    return batched_graph, torch.tensor(labels)\n", "intent": "Define a function that form a mini-batch from a given list of graph and label pairs.\n"}
{"snippet": "def init_weights(shape):\n    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(init_random_dist)\n", "intent": "1. Conv2D\n2. Regular functions that come along with it.\n"}
{"snippet": "from keras.layers import Dropout, Flatten, Dense\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(n_classes))\nmodel.add(Activation('softmax'))\n", "intent": "Then we can add dropout and our fully-connected (\"Dense\") and output (softmax) layers.\n"}
{"snippet": "epochs = 10\nmodel.fit(X, y, batch_size=128, epochs=epochs)\n", "intent": "Now that we have our training data, we can start training. Keras also makes this easy:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(\n    tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "loss_func(model(xb), yb)\n", "intent": "Let's double-check that our loss has gone down:\n"}
{"snippet": "model = Mnist_Logistic()\nloss_func(model(xb), yb)\n", "intent": "We instantiate our model and calculate the loss in the same way as before:\n"}
{"snippet": "def get_model():\n    model = Mnist_Logistic()\n    return model, optim.SGD(model.parameters(), lr=lr)\n", "intent": "We'll define a little function to create our model and optimizer so we can reuse it in the future.\n"}
{"snippet": "train_dl,valid_dl = get_data(train_ds, valid_ds, bs)\nmodel,opt = get_model()\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)\n", "intent": "Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:\n"}
{"snippet": "model = nn.Sequential(\n    Lambda(preprocess),\n    nn.Conv2d(1,  16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n    nn.AvgPool2d(4),\n    Lambda(lambda x: x.view(x.size(0),-1))\n)\n", "intent": "The model created with `Sequential` is simply:\n"}
{"snippet": "model = nn.Sequential(\n    ResizeBatch(1,28,28),\n    conv2d_relu(1,  16), \n    conv2d_relu(16, 16),\n    conv2d_relu(16, 10),\n    PoolFlatten()\n)\n", "intent": "Using our newly defined layers and functions, we can instead now define the same networks as:\n"}
{"snippet": "loss_fn(model(x_valid[0:bs]), y_valid[0:bs])\n", "intent": "**Test our loss function**\nWe try out our loss function on one batch of X features and y targets to make sure it's working correctly.\n"}
{"snippet": "loss_func(model(xb), yb)\n", "intent": "Note that we no longer call `log_softmax` in the `model` function. Let's confirm that our loss is the same as before:\n"}
{"snippet": "def apply_threshold(heatmap,threshold):\n    heatmap[heatmap<= threshold] = 0\n    return heatmap\nprint('Done with applying threshold for the heatmap!')\n", "intent": "The threshold function is aiming to eliminate the false positive in case.\n"}
{"snippet": "clf=AdaBoostClassifier()\nclf.fit(x_train,y_train)\nimp_features=clf.feature_importances_\nfeature_plot(imp_features,x_test)\nclassification_metric(y_test,pred)\n", "intent": "Models with the best results are:\n1. AdaBoostClassifier\n2. GradientBoostingClassifier\n3. SVC\n"}
{"snippet": "classifier = Sequential()\n", "intent": "Definimos nuestro red neuronal\n"}
{"snippet": "classifier.add(Dense(units=6,kernel_initializer='uniform',activation='relu',input_dim=11))\nclassifier.add(Dense(units=6,kernel_initializer='uniform',activation='relu'))\n", "intent": "Creamos dos hidden layers\n"}
{"snippet": "classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n", "intent": "Creamos el output layer\n"}
{"snippet": "def build_classifier():\n    classifier = Sequential()\n    classifier.add(Dense(units=6,kernel_initializer='uniform',activation='relu',input_dim=11))\n    classifier.add(Dropout(p=0.1))\n    classifier.add(Dense(units=6,kernel_initializer='uniform',activation='relu'))\n    classifier.add(Dropout(p=0.1))\n    classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\n", "intent": "Funcion que crea la red\n"}
{"snippet": "kmeans.fit(data.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "from keras import optimizers\nopt = optimizers.RMSprop(lr=0.001)\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=1000))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, validation_split=0.2, batch_size=32, epochs=4, verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(640, input_dim=1000))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), name=\"embedding\")\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf=RandomForestClassifier()\nclf.fit(x_train,y_train)\nimp_features=clf.feature_importances_\nfeature_plot(imp_features,x_test)\nclassification_metric(y_test,pred)\n", "intent": "Models with the best results are:\n1. RandomForestClassifier\n2. DecisionTreeClassifier\n3. GradientBoostingClassifier\n"}
{"snippet": "lm1_ex1 = smf.ols(formula = 'y~x1+x2',data=ex1).fit()\nprint(lm1_ex1.params)\n", "intent": "* (c) Verify your answer by implementing the OLS regression function with python *statsmodels* module.\n"}
{"snippet": "lm2_ex2 = smf.ols(formula = 'TotalIncome~TotalPop+PrivateOnly+BothPublicAndPrivate',data=ex2).fit()\n", "intent": "**Since the p-value of PublicOnly is much higher than others,I would try to remove it.**\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: test_x,\n            labels_: test_y}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "get_predictions(index=0, \n                partition='train',\n                input_to_softmax=final_model(), \n                model_path='results/model_end.h5')\n", "intent": "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='validation',\n                input_to_softmax=final_model(), \n                model_path='results/model_end.h5')\n", "intent": "Use the next code cell to visualize the model's prediction for the first example in the validation dataset.\n"}
{"snippet": "kmeans.fit(df.drop(['Private','Unnamed: 0'],axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_prime(x):\n    return sigmoid(x) * (1-sigmoid(x))\ndef error_formula(y, output):\n    return - y*np.log(output) - (1 - y) * np.log(1-output)\ndef error_term_formula(y, output):\n    return (y - output) * output * (1 - output)\n", "intent": "The following function trains the 2-layer neural network. First, we'll write some helper functions.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(train_features_encoded, train_target)\nprint('Model fitted')\n", "intent": "As you can see, it created a new variable for each `weather` value. We can now fit our model to the encoded data using Scikit-learn estimators.\n"}
{"snippet": "scaler.fit(bank.drop('Class',axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nX = [[0, 0], [1, 1]]\nY = [0, 1]\nclf = RandomForestClassifier(n_estimators=10)\nclf = clf.fit(X, Y)\n", "intent": "Let's train a simple random forest model and deploy it in the Predictive Service\n"}
{"snippet": "import graphlab as gl\nmodel = gl.load_model('pattern_mining_model.gl')\nmodel\n", "intent": "Let's train a simple pattern mining model\n<img src=\"images/left.png\"></img>\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1000, activation=\"relu\"))\nmodel.add(Dense(2, activation='softmax'))\nmodel.summary()\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "ada=AdaBoostRegressor()\nparams_b={'base_estimator':[knn],'learning_rate':[1.0,0.9,0.8],'n_estimators':[5,10,30,50],'random_state':[50]}\nGS_b=GridSearchCV(ada, params_b);\nGS_b.fit(X_train_pca,y_train);\nprint GS_b.best_estimator_\n", "intent": "Tune the AdaBoost parameters\n"}
{"snippet": "t_knn=GS_a.best_estimator_\nstart=time()\nt_knn.fit(X_pca_pd,y_train)\nend=time()\n", "intent": "Training the tuned model\n"}
{"snippet": "ada_neigh=GS_b.best_estimator_\nstart=time()\nada_neigh.fit(X_pca_pd,y_train)\nend=time()\n", "intent": "Train the AdaBoosting tuned K-Nearest Neighbor model\n"}
{"snippet": "m = ols('PRICE ~ PTRATIO',bos).fit()\nprint(m.summary())\n", "intent": "- $F-statistic$ is 175,and not significant\n"}
{"snippet": "modules = list(model_ft.children())[:-1]\nresnet18_feature = nn.Sequential(*modules)\ninputs, classes = next(iter(dataloaders['val']))\nprint(classes)\n", "intent": "Now extract 512-features as the network output after removing the last fc layer.\n"}
{"snippet": "from keras import backend as K\ncfg = K.tf.ConfigProto()\ncfg.gpu_options.allow_growth = True\nK.set_session(K.tf.Session(config=cfg))\n", "intent": "Build a VGG19 and extract the last layer.\n"}
{"snippet": "k_means = KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = torch.nn.Linear(num_ftrs, 2)\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "intent": "Let's borrow pretrained ResNet18 from PyTorch, reset final fully connected layer. \n"}
{"snippet": "final_model = Sequential([\n        Merge([image_model, caption_model], mode='concat', concat_axis=1),\n        Bidirectional(LSTM(256, return_sequences=False)),\n        Dense(vocab_size),\n        Activation('softmax')\n    ])\n", "intent": "Merging the models and creating a softmax classifier\n"}
{"snippet": "config = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\nsess.run(tf.global_variables_initializer())\n_, _, varLoss, losses, varAcc, acc = run_model(sess,y_out,mean_loss,train_data,train_labels,1,64,100,train_step,False, xVal=eval_data, yVal=eval_labels)\n", "intent": "Train the model without batch normalization on mnist dataset, as the dataset is simple, one epoch is enough to overfit the data\n"}
{"snippet": "tf.reset_default_graph()\nX = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.int64, [None])\nis_training = tf.placeholder(tf.bool)\ny_out= cnn_model(X,y,is_training)\n", "intent": "Here we conduct experiment on a normally used way of regularization: L2 Loss on parameters. This is also known as weight decay.\n"}
{"snippet": "model_inject2.fit([X1train, X2train], ytrain, epochs=6, verbose=2, callbacks=[checkpoint], \n          validation_data=([X1test, X2test], ytest))\n", "intent": "We further retrain the model on Flickr8k dataset (on a different machine) with default ADAM optimizer, and use standard BLEU evaluation. \n"}
{"snippet": "modelCVa = LogisticRegressionCV(cv=5)\nresults5 = modelCVa.fit(X ,y)\nresults5.coef_\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n"}
{"snippet": "model = LogisticRegression()\nresults1 = model.fit(X, y)\nresults1.coef_\n", "intent": "Logistic Regression WITHOUT Cross Validation\n"}
{"snippet": "class Sigmoid(Node):\n    def __init__(self, node):\n        Node.__init__(self, [node])\n    def _sigmoid(self, x):\n        return 1. / (1. + np.exp(-x)) \n    def forward(self):\n        input_value = self.inbound_nodes[0].value\n        self.value = self._sigmoid(input_value)\n", "intent": "Here's how I implemented the sigmoid function.\n"}
{"snippet": "X, y = Input(), Input()\nW1, b1 = Input(), Input()\nW2, b2 = Input(), Input()\nl1 = Linear(X, W1, b1)\ns = Sigmoid(l1)\nl2 = Linear(s, W2, b2)\ncost = MSE(l2, y)\n", "intent": "Writing this out in MiniFlow, it would look like:\n"}
{"snippet": "k_means.fit(college.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "x = tf.nn.softmax([2.0, 1.0, 0.2])\n", "intent": "We're using TensorFlow to build neural networks and, appropriately, there's a function for calculating softmax.\n"}
{"snippet": "import tensorflow as tf\ndef run():\n    output = None\n    logit_data = [2.0, 1.0, 0.1]\n    logits = tf.placeholder(tf.float32)\n    softmax = tf.nn.softmax(logits)\n    with tf.Session() as sess:\n        output = sess.run(softmax, feed_dict={logits: logit_data})\n    return output\n", "intent": "Use the softmax function in the quiz below to return the softmax of the logits.\n"}
{"snippet": "rfr = RandomForestRegressor().fit(sensorReadings, lable)\n", "intent": "Create two classifiers used to evaluate a subset of attributes.\n"}
{"snippet": "reload(bilstm);\ntorch.manual_seed(765);\nembedding_dim=30\nhidden_dim=30\nmodel = bilstm.BiLSTM(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim)\n", "intent": "- Once, you have defined the parameters of the model: check if you have done it right using the unit test.\n"}
{"snippet": "lstm_feats = model(sentence)\nprint (lstm_feats[0][0:5])\n", "intent": "- this calls the `forward()` function on the model, which returns the tag_scores for each tag for each particular token in the sentence\n"}
{"snippet": "reload(bilstm);\ntorch.manual_seed(765);\nloss = torch.nn.CrossEntropyLoss()\nmodel, losses, accuracies = bilstm.train_model(loss, model, X_tr,Y_tr, word_to_ix, tag_to_ix, \n                                        X_dv, Y_dv, num_its=10, status_frequency=2, \n                                        optim_args = {'lr':0.1,'momentum':0}, param_file = 'best.params')\n", "intent": "- Train the model for now\n"}
{"snippet": "reload(bilstm);\ntorch.manual_seed(765);\nembedding_dim=64\nhidden_dim=30\nmodel = None\nloss = None\ndel model\ndel loss\nmodel = bilstm.BiLSTM(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim, word_embeddings)\n", "intent": "- Now, all we need to do is send in the word_embeddings when initializing your model\n"}
{"snippet": "theta_perc,theta_perc_history = perceptron.estimate_perceptron(x_tr_pruned,y_tr,20)\n", "intent": "For reference, here is the running time on a relatively modern consumer-grade machine:\n"}
{"snippet": "reload(logreg)\ntorch.manual_seed(765)\nmodel = logreg.build_bakeoff(X_tr_var,Y_tr)\nmodel.add_module('log_softmax',torch.nn.LogSoftmax(dim=1))\nloss = torch.nn.MultiMarginLoss() \nmodel,losses,accuracies = logreg.train_model(loss,model,X_tr_var,Y_tr_var,\n                                             Y_dv_var=Y_dv_var,X_dv_var = X_dv_var,\n                                             num_its=1000,status_frequency=35, optim_args = {'lr':0.1,'weight_decay':0.05},)\n", "intent": "See if these features help!\n"}
{"snippet": "fitted = scaler.fit(data.drop('TARGET CLASS',axis=1))\nfitted\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "def try_out():\n  with tf.Session() as sess:\n    fn = read_dataset(tf.estimator.ModeKeys.EVAL, \n                    {'input_path': 'data', 'batch_size': 8, 'nitems': 5668, 'nusers': 82802})\n    feats, _ = fn()\n    print(feats['input_rows'].eval())\ntry_out()\n", "intent": "This code is helpful in developing the input function. You don't need it in production.\n"}
{"snippet": "simple_model = LogisticRegression().fit(train_matrix_word_subset, train_data['sentiment'])\nsimple_model\n", "intent": "We will now build a classifier with **word_count_subset** as the feature and **sentiment** as the target. \n"}
{"snippet": "neigh = KNeighborsClassifier(n_neighbors=3)\n", "intent": "We will use kNN Classifier with a k value of 3\n"}
{"snippet": "features = ['CRS_DEP_TIME', 'DAY_OF_WEEK', \"DAY_OF_MONTH\" ]\nX = df[features]\ny = df['DEP_DEL15']\nmodel.fit(X, y)\n", "intent": "First, let's run a model with slightly different inputs\n"}
{"snippet": "from sklearn.cluster import KMeans\ncluster_model = KMeans(n_clusters = 5)\ncluster_model.fit(X)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "k = 5\nkmeans = cluster.KMeans(n_clusters=k, n_jobs=-1)\nkmeans.fit(dn1)\n", "intent": "Cluster the Data - We are going to use 5 clusters based off of the above scatterplot\n"}
{"snippet": "model = AgglomerativeClustering(linkage='ward',\n                                            n_clusters=3)\n", "intent": "Note that Agglomerative Clustering is This method is most appropriate for quantitative variables, and not binary variables.\n"}
{"snippet": "model_ft=model_ft\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model_ft = nn.DataParallel(model_ft)\nmodel_ft.to(device)\nmodel_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)\n", "intent": "Train and evaluate\n^^^^^^^^^^^^^^^^^^\nIt should take around 15-25 min on CPU. On GPU though, it takes less than a\nminute.\n"}
{"snippet": "max_depth = 7\nmodel = DecisionTreeRegressor(max_depth=max_depth)\nmodel.fit(x.reshape(-1, 1), y)\n", "intent": "We'll create a decision tree to predict the points. We won't make it that deep because we want to visualize the prediction.\n"}
{"snippet": "scaler.fit(df.drop(\"Class\", axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "X_insects = insects[['continent', 'latitude', 'sex']]\ny_insects = insects['wingsize']\ninsects_regression.fit(X_insects, y_insects)\n", "intent": "2\\. Use the `fit` method on the regression object to train the model using your data.\n"}
{"snippet": "X_wells_names = np.array(['arsenic', 'dist', 'assoc', 'educ'])\nX_wells = wells[X_wells_names]\ny_wells = wells['switch']\nwells_regression.fit(X_wells, y_wells)\n", "intent": "We can use the `fit` method of the `LogisticRegression` object to train the model using our data.\n"}
{"snippet": "standardizer.fit(X_wells)\n", "intent": "2\\. Use the `fit` method.  Behind the scenes this computes and memorizes the mean and standard deviation of all the columns in the dataset.\n"}
{"snippet": "X = np.array([[1], [2], [3], [4]])\nP = PolynomialExpansion(3)\nP.fit(X)\nP.transform(X)\n", "intent": "Let's test this out on a simple example.\n"}
{"snippet": "wells_pipeline.fit(X_wells, y_wells)\n", "intent": "This is now a pipline of considerable complexity.  Even so, using it is exactly the same as any of the simpler pipelines that we constructed earlier.\n"}
{"snippet": "lm.fit(X, bos.PRICE)\nlm.coef_\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, input_dim=1000))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(32))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(2))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, nb_epoch=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Activation\nlogreg = Sequential()\nlogreg.add(Flatten(input_shape=(28, 28, 1)))\nlogreg.add(Dense(10))\nlogreg.add(Activation('softmax'))\nlogreg.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n", "intent": "<img src=\"../data/images/logreg.png\" width=\"800\">\n<img src=\"../data/images/sigmoid.png\" width=\"400\">\n"}
{"snippet": "kmeans.fit(data.drop(\"Private\", axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "import helper_udacity\nimages, labels = next(iter(trainloader))\nimg = images[0].view(1, 784)\nwith torch.no_grad():\n    logps = model(img)\nps = torch.exp(logps)\nhelper_udacity.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "with tf.Session() as sess:\n    outs = sess.run(f)\n    sess.close()\nprint(\"outs ={}\".format(outs))\n", "intent": "Once, we have finished our graph the next step is to run the computations that it represents, to do this we must create and run a session. \n"}
{"snippet": "a =  tf.constant(5)\nprint(a.graph is g)\nprint(a.graph is tf.get_default_graph())\n", "intent": "Look that we have two graphs with different identification, also the graph _g_ is not associated to the default graph. Let's see the associations.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "from sklearn.ensemble import VotingClassifier\nvector_mach = SVC()\ndecision = DecisionTreeClassifier(max_depth=4)\nvc = VotingClassifier([('svc',vector_mach),('dt',decision)])\nover_cross(vc,canc)\n", "intent": "Next, we need to upgrade sklearn to v0.17\n"}
{"snippet": "mod = SVC(kernel='rbf')\nscores = over_cross(mod,canc)\nnp.mean(scores)\n", "intent": "**8) Train an SVM using the RBF kernel. Is this model better or worse?**\n"}
{"snippet": "bag_o_words.todense().shape\n", "intent": "Note that this is a sparse matrix! You can convert to dense matrices with `.todense()` (named for clarity, clearly)\n"}
{"snippet": "dtc = DecisionTreeClassifier(max_depth=30).fit(X_train, y_train)\ndtc.score(X_test,y_test)\n", "intent": "Let's try with a decisiontree\n"}
{"snippet": "fit_grid = gs_clf.fit(X_train,y_train)\n", "intent": "What's going on here? Note: if you have a windows machine, keep n_jobs=1\n"}
{"snippet": "scaler.fit(df.drop([\"TARGET CLASS\"], axis=1))\nscaler\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "def discrim(X, w, w2, w3, wy):    \n    h = relu(abstract_conv.conv2d(X,w,subsample=(2, 2), border_mode=(2, 2)),alpha=0.2)\n    h2 = relu(batchnorm(abstract_conv.conv2d(h,w2,subsample=(2, 2), border_mode=(2, 2))),alpha=0.2)\n    h2 = T.flatten(h2, 2)    \n    h3 = relu(batchnorm(T.dot(h2,w3)),alpha=0.2)\n    y = sigmoid(T.dot(h3,wy))\n    return y\n", "intent": "**Problem 5 (1pt)** Implement discriminator by plugging in the hidden variables and weights. Hint: if the sizes mismatched, theano will complain.\n"}
{"snippet": "def gen(Z, w,  w2, w3, wx):\n    h = relu(batchnorm(...))\n    h2 = relu(batchnorm(...))\n    h2 = h2.reshape((h2.shape[0], ngf*2, 7, 7))\n    h3 = relu(batchnorm(trconv(..., ..., output_shape=(None,None,7,7),filter_size=(5,5),subsample=(2, 2), border_mode=(2, 2))))\n    x = sigmoid(trconv(..., ..., output_shape=(None, None, 14, 14),\n                                   filter_size=(5, 5), subsample=(2, 2), border_mode=(2, 2)))\n    return x\n", "intent": "**Problem 4 (1pt)** Implement generator by plugging in the hidden variables and weights. Hint: if the sizes mismatched, theano will complain.\n"}
{"snippet": "def discrim(X, w, w2, w3, wy):    \n    h = relu(conv2d(... subsample=(2, 2), border_mode=(2, 2)),alpha=0.2)\n    h2 = relu(batchnorm(conv2d(... subsample=(2, 2), border_mode=(2, 2))),alpha=0.2)\n    h2 = T.flatten(h2, 2)    \n    h3 = relu(batchnorm(T.dot(...)),alpha=0.2)\n    y = sigmoid(T.dot(...))\n    return y\n", "intent": "**Problem 5 (1pt)** Implement discriminator by plugging in the hidden variables and weights. Hint: if the sizes mismatched, theano will complain.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def train_model(model, epochs=10):\n    model.fit(x_train, y_train, epochs=epochs, batch_size=100, verbose=0, validation_split=0.2)\n    return model\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn import grid_search\nalphas = np.logspace(-10, 10, 21)\ngs = grid_search.GridSearchCV(\n    estimator=linear_model.Ridge(),\n    param_grid={'alpha': alphas},\n    scoring='mean_squared_error')\ngs.fit(modeldata, y)\n", "intent": "**4.** Do 2 grid searches to find out the optimal alpha for Lasso and Ridge. Again calculate the MSE and R2\n"}
{"snippet": "lm = linear_model.LogisticRegression() \nY = data['admit']\nX = data[train_cols]\nmodel = lm.fit(X,Y)\nscore = model.score(X,Y)\nprint \"The score for this model is: \" + str(score) + \" out of 1\"\n", "intent": "First let's run the model using all the predictors and see what scores we get.\n"}
{"snippet": "lm2 = linear_model.LogisticRegression() \nY2 = data['admit']\nX2 = data[train_cols]\nmodel2 = lm2.fit(X2,Y2)\nscore2 = model2.score(X2,Y2)\nprint \"The score for this model is: \" + str(score2) + \" out of 1\"\n", "intent": "Let's take another look at cross-validation and other scoring in sklearn.\n"}
{"snippet": "X.todense()\n", "intent": "Convert the matrix from the vectorizer to a dense matrix, then sum by column to get the counts per term.\n"}
{"snippet": "knn.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "svc_params = {'kernel': ['rbf', 'linear'], 'gamma': [0.01, 0.001, 0.0001], 'C': [1, 10, 100]}\nclf = GridSearchCV(estimator=SVC(), param_grid=svc_params, cv=3, n_jobs=-1, \n                   scoring='accuracy', verbose=10)\nclf.fit(cnn_codes_train, y_train)\n", "intent": "Unfortunately I had to limit train data to 20% because of my computer's weak parameters.\n"}
{"snippet": "mod = Sequential()\nmod.add(Dense(32, activation='sigmoid', input_dim=x_train.shape[1]))\nmod.add(Dropout(0.2))\nmod.add(Dense(64, activation='relu'))\nmod.add(Dropout(0.2))\nmod.add(Dense(64, activation='sigmoid'))\nmod.add(Dropout(0.2))\nmod.add(Dense(num_classes, activation='sigmoid'))\nmod.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "mod.fit(x_train, y_train,\n          batch_size=32,\n          epochs=30,\n          validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "import tensorflow as tf\nhello_constant = tf.constant(\"Hello, World!\")\nint_constant   = tf.constant(1234)\narray_constant = tf.constant([[10,20], [30,40]])\nwith tf.Session() as sess:\n    output = sess.run(hello_constant)\n    print(output)\n", "intent": "Vincent Vanhoucke, Principal Scientist at Google Brain, introduces you to deep learning and Tensorflow, Google's deep learning framework.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed_input = tf.nn.embedding_lookup(embedding, input_data)\n    return embed_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "W_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n", "intent": "In order to build a deep network, we **stack several layers of this type**. The second layer will have 64 features for each 5x5 patch.\n"}
{"snippet": "W_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n", "intent": "Finally, we add a layer, just like for the one layer softmax regression above.\n"}
{"snippet": "tr = DecisionTreeRegressor()\n", "intent": "Train and examine the performance of a `DecisionTreeRegressor` and `LinearRegression` model here.  Which does better?  Visualize your results.\n"}
{"snippet": "seed = 0 \nrng = np.random.RandomState(seed)\nrng.seed(seed)\nkmeans = KMeans(n_clusters=5, random_state=rng)  \nkmeans.fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "lm.fit(X_train, y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "model.fit(x_train, y_train, epochs=25, batch_size=50, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "batch_size = 128\nn_epochs = 1\nmodel.fit(X_train, y_train,\n          batch_size=batch_size,\n          nb_epoch=n_epochs,\n          validation_data=(X_test, y_test))\n", "intent": "(try more epochs, 30secs to 1min per epoch aprox)\n"}
{"snippet": "model.set_weights(init_weights)\nh = model.fit(X_train, y_train, nb_epoch=15, batch_size=1, validation_split=.15, verbose=0)\n", "intent": "entrenamos el modelo con los datos de entrenamiento. Observe how after a few iterations we get little improvement\n"}
{"snippet": "fg_W1 = theano.function(inputs=[], outputs=g_W1, givens={tX: X,ty: y})\n", "intent": "get compiled evaluable function from symbolic gradient\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n", "intent": "Let's check random forests.\n"}
{"snippet": "lm = smf.ols(formula='sale_price ~ gross_sq_feet -1', data = REsample1).fit()\nprint(lm.summary())\n", "intent": "This is already pretty consistent with what python will give us if we take the entire training sample at once\n"}
{"snippet": "import tensorflow as tf\ntf.reset_default_graph()\n", "intent": "$$y = softmax(Wx+b)$$\n"}
{"snippet": "sess = tf.Session()\nsess.run(init)\n", "intent": "We can now launch the model in a Session, and run the operation that initializes the variables:\n"}
{"snippet": "def KMeanDict(data,nClusters):\n    kmLabels = KMeans(n_clusters=nClusters, random_state=None).fit(data).labels_\n    classDict = {label: data[label==kmLabels] for label in np.unique(kmLabels)}    \n    for i in classDict:\n        classDict[i] = np.matrix(classDict[i])\n    return classDict\n", "intent": "We will now define a helper function which will allow us to perform kMeans and return the results in a form easy to work with. \n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "y = tf.nn.softmax(tf.matmul(x,W) + b)\n", "intent": "<center><h2>Predicted Class and Cost Function</h2></center>\n"}
{"snippet": "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n", "intent": "Convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool.\n"}
{"snippet": "W_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n", "intent": "Add a softmax layer, just like for the one layer softmax regression above.\n"}
{"snippet": "X = news_A.drop('class',axis=1)\ny = news_A['class']\nclf2 = GaussianNB()\nclf2.fit(X,y)\nca2 = clf2.score(X,y)\nprint(\"The classification accuracy without cleaning is {}\".format(ca2))\n", "intent": "Fit a Gaussian Naive Bayes model to the original dataset A (including the outliers). Display the classification accuracy on the training dataset.\n"}
{"snippet": "svc = LinearSVC()\nsvc.fit(X_train,y_train)\nprint('The accuracy of training data is {:.3f}'.format(svc.score(X_train,y_train)))\nprint('The accuracy of test data is {:.3f}'.format(svc.score(X_test,y_test)))\n", "intent": "Train a [`LinearSVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n"}
{"snippet": "svc1 = SVC(kernel='rbf')\nsvc2 = SVC(kernel='poly')\nsvc1.fit(X_train,y_train)\nsvc2.fit(X_train,y_train)\nprint('The accuracy of training data with rbf is {:.3f}'.format(svc1.score(X_train,y_train)))\nprint('The accuracy of test data with rbf is {:.3f}'.format(svc1.score(X_test,y_test)))\nprint('The accuracy of training data with poly is {:.3f}'.format(svc2.score(X_train,y_train)))\nprint('The accuracy of test data with poly is {:.3f}'.format(svc2.score(X_test,y_test)))\n", "intent": "By using the [`SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"}
{"snippet": "from sklearn.dummy import DummyClassifier\ndc1 = DummyClassifier(strategy='most_frequent')\ndc1.fit(X_tr,y_tr)\nprint('The dummy classifier has accuracy: {:.3f} on the validation set.'.format(dc1.score(X_val,y_val)))\n", "intent": "I chose to return the most frequent class as output for my dummy classifier as simply guessing non-person will get an accuracy of 50%.\n"}
{"snippet": "kmeans = KMeans(n_clusters=5,random_state=1337)\nkmeans.fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "model.fit(X_train,y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "nb.fit(X_train, y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nmodel = SVC()\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "def sigmoid(x):\n    return 1/(1+exp(-x))\n", "intent": "The sigmoid converts values into range [0,1]\n"}
{"snippet": "def flatten(data):\n    data = None\n    return data\n", "intent": "---\nThe data has to be flattened after the last convolutional layer and before the first fully connected layer. \n"}
{"snippet": "def flatten(data):\n    data = tf.contrib.layers.flatten(inputs=data)\n    return data\n", "intent": "---\nThe data has to be flattened after the last convolutional layer and before the first fully connected layer. \n"}
{"snippet": "def fullyConnectedLayer(data, hiddenLayerSize, useActivationFunc=False):\n    ny = data.get_shape().as_list()[1]\n    nx = hiddenLayerSize\n    W = tf.get_variable(name='W', shape=(ny, nx), initializer=tf.contrib.layers.xavier_initializer())\n    z = tf.matmul(data, W, name='matmul')\n    if useActivationFunc==True:\n        a = tf.tanh(z, name='activation_function')\n    else:\n        a = z\n    return a\n", "intent": "---\nImplement a fully connected layer using tf.matmul()\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(x_train, y_train_class1)\nclf.score(x_train, y_train)   \n", "intent": "First, try simple random forest.\n"}
{"snippet": "from sklearn import linear_model\nmodel = linear_model.Perceptron() \nmodel.fit(X, y)\nplot_decision_boundary(model, X, plot_boundary=True)\nscatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)\n", "intent": "The `Perceptron` is a simple algorithm suitable for large scale learning.\n"}
{"snippet": "classifier = svm.SVC(kernel='rbf', gamma=gammas[np.argmax(scores)])\nclassifier.fit(X, y) \nX_test_norm = scaler.transform(X_test) \nprint classifier.score(X_test_norm, y_test)\n", "intent": "Check the score of the best model in the test set.\n"}
{"snippet": "import numpy as np\nrnd_data = np.random.multivariate_normal([0, 0], [[3, 4], [5, 6]], 1000)\nwhitened_data, whitening_matrix = whiten(rnd_data)\nassert(np.allclose(np.identity(2), np.cov(whitened_data)))\nprint(\"Passed\")\n", "intent": "See if the function returns whitened data\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, verbose=2)\ngrid.fit(X_test, y_test)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "sfa_dims = 2\nrnd_data = np.random.multivariate_normal([0, 0], [[3, 4], [5, 6]], 1000)\nsfa_homemade = slow_feature_analysis(rnd_data, sfa_dims)\nassert(np.allclose(np.identity(2), np.cov(sfa_homemade.T)))\nprint(\"Passed\")\n", "intent": "Identity covariance test:\n"}
{"snippet": "rnd_data = np.random.multivariate_normal([0, 0], [[3, 4], [5, 6]], 1000)\nprint(rnd_data.shape)\nwhitened_data, whitening_matrix = whiten(rnd_data)\nassert(np.allclose(np.identity(2), np.cov(whitened_data)))\nprint(\"Passed\")\n", "intent": "Identity covariance test:\n"}
{"snippet": "import numpy as np\nrnd_data = np.random.multivariate_normal([0, 0], [[3, 4], [5, 6]], 1000)\nprint(rnd_data.shape)\nwhitened_data, whitening_matrix = whiten(rnd_data)\nassert(np.allclose(np.identity(2), np.cov(whitened_data)))\nprint(\"Passed\")\n", "intent": "See if the function returns whitened data\n"}
{"snippet": "data = df_cp[['Population', 'Violent_crime', 'Property_crime']]\nmodel=lg.fit(data[['Population', 'Violent_crime']],data['Property_crime'])\nprint (\"Regression formula: y = {0}x + {1}\".format(model.coef_, model.intercept_))\n", "intent": "<h1> Model2:  Linear for 2 variables: Population + violent </h1>\n"}
{"snippet": "X = df_cp.drop('Property_crime',axis=1)\ny = df_cp.Property_crime\nmodel=lg.fit(X,y)\nprint (\"Regression formula: y = {0}x + {1}\".format(model.coef_, model.intercept_))\n", "intent": "<h1>Model3: Linear regression for all variables in table</h1>\n"}
{"snippet": "X = data[['amount','fico_avr']]\ny = data['interest']\nlg=linear_model.LinearRegression()\nmodel1=lg.fit(X,y)\nprint (\"formula: y = {0}x + {1}\".format(model1.coef_, model1.intercept_))\n", "intent": "We will use linear regression function to model the interest rate of borrower\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(min_samples_split=20, random_state=0,n_estimators=10)\nrf.fit(X, y)\n", "intent": "<h1> Random Forest </h1>\n"}
{"snippet": "rfc= RandomForestClassifier()\nmodel = rfc.fit(data[data.columns[0:-2]],data.Activity)\nmodel\n", "intent": "<h1> Classification Model </h1>\n"}
{"snippet": "rf_op = RandomForestRegressor(bootstrap=True, compute_importances=None,\n           criterion='mse', max_depth=12, max_features='auto',\n           max_leaf_nodes=None, min_density=None, min_samples_leaf=2,\n           min_samples_split=2, n_estimators=1000, n_jobs=-1,\n           oob_score=False, random_state=None, verbose=0)\nrf_op=rf_op.fit(x_data, y_data)\n", "intent": "RandomForestRegressor is trained using the best parameters.\n"}
{"snippet": "weights = {\n    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n}\nbiases = {\n    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n", "intent": "The variable n_hidden_layer determines the size of the hidden layer in the neural network. This is also known as the width of a layer.\n"}
{"snippet": "n_clusters = 5\nestimator = KMeans(n_clusters=n_clusters)\nestimator.fit(X_rolling)\n", "intent": "We have already completed the code to fit the estimator using sklearn. You will see how simple it is!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embedded = tf.nn.embedding_lookup(embedding, input_data)  \n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "opt_tree_model = DecisionTreeRegressor(max_depth=np.argmin(cv_test_errors))\nopt_tree_model.fit(X = X_train.values.reshape(-1,1), y = y_train.values.reshape(-1,1))\nprint(opt_tree_model.get_params)\ndot_data = StringIO() \nexport_graphviz(opt_tree_model, out_file=dot_data) \ngraph = pydot.graph_from_dot_data(dot_data.getvalue()) \ndisplay.Image(graph.create_png())\n", "intent": "It's hard to tell exactly which depth is optimal from the graph, so we'll use `np.argmin` to choose the lowest value from the `cv_test_errors`.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Random init is too complicated\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim),-1,1))\n    embed_layer = tf.nn.embedding_lookup(embedding, input_data)\n    return embed_layer\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "mod = Sequential()\nmod.add(Dense(10, input_dim=784,activation='softmax'))\nopti = keras.optimizers.RMSprop(lr=0.001, rho=0.95, epsilon=1e-08, decay=0.0)\nmod.compile(optimizer = opti , loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "* By varying the number of hidden layers, see if you can improve the score on the test data.\n"}
{"snippet": "model1=Sequential()\nmodel1.add(Dense(64,input_dim=3072,activation='relu'))\nmodel1.add(Dense(10,activation='softmax'))\nmodel1.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\nmodel1.summary()\n", "intent": "* Example of a standard network with one hidden layer\n"}
{"snippet": "model = Sequential()\nmodel.add(Conv2D(16, (3, 3), padding='same',input_shape=train_X.shape[1:],activation='relu'))\nmodel.add(Conv2D(16, (3, 3),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(num_classes,activation='softmax'))\n", "intent": "Modification of Ayman's and Iliana's model\n"}
{"snippet": "W = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.nn.softmax(tf.matmul(x, W) + b)\n", "intent": "We define our linear model for the score function after introducing two of parameters, **W** and **b**.\n"}
{"snippet": "from keras import regularizers\nmodel = Sequential()\nmodel.add(Dense(256, activation='relu', input_shape=(1000,), activity_regularizer=regularizers.l1(0.01)))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model = make_model(y=0.)\nmcmc = pm.MCMC(model)\nmcmc.use_step_method(ps.RandomWalk, model['theta'])\nsmc = ps.SMC(mcmc, num_particles=100, num_mcmc=1, verbose=4, ess_reduction=0.9)\nsmc.initialize(0.)\nsmc.move_to(1.)\n", "intent": "Now we initialize the model and run PySMC.\n"}
{"snippet": "scaler.fit(X_train)\n", "intent": "2) Fit using only the data.\n"}
{"snippet": "pca.fit(X_train)\n", "intent": "2) Fit to training data\n"}
{"snippet": "grid_search.fit(X_train, y_train)\n", "intent": "A GridSearchCV object behaves just like a normal classifier.\n"}
{"snippet": "clf = LogisticRegression()\nclf = clf.fit(X_train, y_train)\n", "intent": "Now will use the Logistic Regression class from the scikit-learn library to create a classifier object and train it on our data.\n"}
{"snippet": "clf = MultinomialNB()\nclf = clf.fit(X_train, y_train)\n", "intent": "Similarly fiddle around with the hyper parameters of this model. You can find the documentation on the scikit-learn website.\n"}
{"snippet": "regr = lm.LinearRegression()\nregr.fit(car_data.horsepower.values.reshape(len(car_data),1), car_data.mpg)\nseaborn.regplot(x=\"horsepower\", y=\"mpg\", data=car_data)\nprint(\"Pearson correlation: %.3f, Regresion coefficient: %.3f\" % (car_data.horsepower.corr(car_data.mpg), regr.coef_[0]))\n", "intent": "**Pozor** sklon regresnej ciary nehovori o sile korelacie. Len o smere.\n"}
{"snippet": "dtree=DecisionTreeClassifier()\ndtree.fit(X_train,y_train)\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "kmeans=KMeans(n_clusters=2)\nkmeans.fit(data.drop('Private',axis=1))\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "model.fit(x_train, y_train, epochs=50, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(X_train,y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "nb=MultinomialNB()\nnb.fit(X_train,y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "grid=GridSearchCV(SVC(),param_grid,verbose=3)\ngrid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=5)\n", "intent": "<a id=\"tuning-a-knn-model\"></a>\n---\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nfeatures = ['al']\nX = glass.loc[:, features]\ny = glass.loc[:, 'ri']\nlinreg.fit(X,y)\n", "intent": "- Instantiate and fit a linear regression model predicting `ri` from `al` (and an intercept).\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nfeature_cols = ['al']\nX = glass.loc[:, feature_cols]\ny = glass.loc[:, 'ri']\nlinreg.fit(X, y)\n", "intent": "- Instantiate and fit a linear regression model predicting `ri` from `al` (and an intercept).\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nX = mammals.loc[:, [\"body\"]]\npf = PolynomialFeatures(degree=3, include_bias=False)\npf.fit(X)\npf.transform(X)\n", "intent": "**sklearn has a \"transformer\" that generates polynomial terms**\n"}
{"snippet": "X = boston.loc[:,[\"DIS\"]]\ny = boston.loc[:,\"MEDV\"]\nlrb = LinearRegression()\nlrb.fit(X,y)\n", "intent": "- Create a linear regression model for MEDV against DIS with no higher-order polynomial terms.\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\npf = PolynomialFeatures(degree=7, include_bias=False)\npf.fit(X)\nX7 = pf.transform(X)\nlr_boston7 = LinearRegression()\nlr_boston7.fit(X7, y)\n", "intent": "- Create a linear regression model for y against X polynomial terms up to and including degree seven.\n"}
{"snippet": "model.fit(Xsmall, ysmall, batch_size=500, epochs=40,verbose = 1)\nmodel.save_weights('model_weights/best_RNN_small_textdata_weights.hdf5')\n", "intent": "Now lets fit our model!\n"}
{"snippet": "X = boston.loc[:, ['DIS']]\ny = boston.loc[:, 'MEDV']\nlr_boston1 = LinearRegression()\nlr_boston1.fit(X, y)\n", "intent": "- Create a linear regression model for MEDV against DIS with no higher-order polynomial terms.\n"}
{"snippet": "pf = PolynomialFeatures(degree=7, include_bias=False)\npf.fit(X)\nX7 = pf.transform(X)\nlr_boston7 = LinearRegression()\nlr_boston7.fit(X7, y)\n", "intent": "- Create a linear regression model for y against X polynomial terms up to and including degree seven.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\nmodel.fit(X_train, Y_train, validation_data = (X_val, Y_val), nb_epoch=20, \n          batch_size=128, verbose=True, validation_split=0.15, \n          callbacks=[best_model, early_stop]) \n", "intent": "So, how hard can it be to build a Multi-Layer percepton with keras?\nIt is baiscly the same, just add more layers!\n"}
{"snippet": "from keras.optimizers import SGD\nfrom keras.preprocessing.text import one_hot, text_to_word_sequence, base_filter\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.preprocessing import sequence\nimport tensorflow as tf\ntf.python.control_flow_ops = tf\n", "intent": "_source: http://colah.github.io/posts/2015-08-Understanding-LSTMs_\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(256, activation='relu', input_shape=x_train[0].shape))\nmodel.add(Dense(2, activation='softmax'))\nopti = optimizers.SGD(lr=0.01, momentum=0.3, decay=0.0, nesterov=False)\nmodel.compile(loss='categorical_crossentropy', optimizer=opti, metrics=['accuracy'])\nmodel.summary()\nprint(x_train.shape)\nprint(y_train.shape)\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "startTime = time.time()\ntotal_time = timeit.timeit('model.fit(x_train, y_train, epochs=200, batch_size=100, verbose=2)', number=1)\nelapsedTime = time.time() - startTime\nprint(elapsedTime)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "one_hot.fit(sport_encoded.reshape(-1,1))\none_hot_sport = one_hot.transform(sport_encoded.reshape(-1, 1))\n", "intent": "Next, we need to fit our data to the transformer. Let's do that here:\n"}
{"snippet": "km = KMeans(n_clusters=3, random_state=42)\n", "intent": "You guessed it! We're using sklearn.\n"}
{"snippet": "km = KMeans(n_clusters=3)\nkm.fit(iris_features_df_scaled)\n", "intent": "You guessed it! We're using sklearn.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X, y)\n", "intent": "In order to **build a model**, the features must be **numeric**, and every observation must have the **same features in the same order**.\n"}
{"snippet": "knc = KNeighborsClassifier(n_neighbors=29)\nknc.fit(X_train_scaled, y_train)\n", "intent": "Finally, fit a KNeighborsClassifier with our optimal k.\n"}
{"snippet": "ks = list(range(1,31))\ngs_train_accuracies = []\ngs_test_accuracies = []\nfor k in ks:\n    gs = GridSearchCV(KNeighborsClassifier(n_neighbors=k), param_grid={}, cv=5)\n    gs.fit(X_train_scaled, y_train)\n    train_score = gs.score(X_train_scaled, y_train)\n    test_score = gs.score(X_test_scaled, y_test)\n    gs_train_accuracies.append(train_score)\n    gs_test_accuracies.append(test_score)\n", "intent": "Plot your train and test scores against your different values of K. Which K should you choose? Does CV do anything to help the train or test score?\n"}
{"snippet": "gs_knc = GridSearchCV(KNeighborsClassifier(), param_grid=params, cv=5, n_jobs=-1)\n", "intent": "Instantiate GridSearchCV with KNeighborsClassifier, your param_grid, and 5-fold cross-validation.\n"}
{"snippet": "gs_knc.fit(X_train_scaled, y_train)\n", "intent": "Fit your grid search object to your scaled X_train and y_train\n"}
{"snippet": "Ridge().get_params().keys()\n", "intent": "---\nIs it better than the Linear regression? If so, why might this be?\n"}
{"snippet": "y = admissions.admit.values\nX = admissions[['gpa']].values\nlinmod = LinearRegression()\nlinmod.fit(X, y)\nprint 'Intercept:', linmod.intercept_\nprint 'Coef(s):', linmod.coef_\n", "intent": "<a id='pred-admit'></a>\n---\nLet's try predicting the `admit` binary indicator using just `gpa` with a Linear Regression to see what goes wrong.\n"}
{"snippet": "gspipe.fit(features_dummies, target)\n", "intent": "This will take a while...\n"}
{"snippet": "pipe = r.pipeline()\n", "intent": "Now, let's use `redis pipeline` to load our lookup.\n"}
{"snippet": "def make_prediction(title):\n    vec = nlp(title).vector\n    model = pickle.loads(r.get('model'))\n    distances , indices = model.kneighbors(vec.reshape(1,-1))\n    indices = indices[0]\n    pipe = r.pipeline()\n    for index in indices:\n        pipe.get(bytes(str(index), 'utf-8'))\n    return list(zip(pipe.execute(), distances[0]))\n", "intent": "What stuff do we need for our script?\n* redis (r)\n* nlp (English())\n* pickle\n* argparser\n"}
{"snippet": "from sklearn.pipeline import make_pipeline\npipe = make_pipeline(vect, nb)\n", "intent": "[make_pipeline documentation](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)\n"}
{"snippet": "KNN = KNeighborsClassifier()\ngs = GridSearchCV(KNN, param_grid={'n_neighbors':range(3,21,2)}) \ngs.fit(latent_semantic_analysis, news_df['group_num'])\n", "intent": "We used KNN here, but we can use RandomForest, LogisticRegression, XGBoost (Gradient Descent Trees), Gausean, Naive Bayes, any classifiers. \n"}
{"snippet": "encoder.fit(titanic_df['embarked'])\ntitanic_embarked_encoded = encoder.transform(titanic_df['embarked'])\ntitanic_embarked_encoded\n", "intent": "1. Create a new `LabelEncoder`. \n1. Use it to label encode the `embarked` column.\n1. Save the label encoded vector back to the `titanic_df`.\n"}
{"snippet": "rf.fit(X_train, y_train)\ngdbr.fit(X_train, y_train)\nabr.fit(X_train, y_train)\n", "intent": "better means lowest mse, higher r squared; the magnitude of differences in this sample is way too small to use.\n"}
{"snippet": "C = [1E-1,1,10,100]\ngamma = 250\nclassifiers = []\nfor C in C:\n    clf = SVC(C=C, gamma=gamma)\n    clf.fit(X, y)\n    decision_boundary(clf,X,y)\n", "intent": "No visible changes the C value increases.\n"}
{"snippet": "clf = SVC(kernel = 'linear')\nclf.fit(X, y)\ndecision_boundary(clf,X,y)\n", "intent": "we have two different groups , but one group has misclassified one data point\n"}
{"snippet": "logreg = LogisticRegression()\nlogreg.fit(X_small,y_small)\ndecision_boundary(logreg,X_small,y_small)\n", "intent": "The class counts look more obvious in this case , rather distinct and more seperated\n"}
{"snippet": "clf = SVC()\nclf.fit(X_small, y_small)\ndecision_boundary(clf,X_small,y_small)\n", "intent": "Two points are misclassified\n"}
{"snippet": "clf = SVC(kernel = 'sigmoid')\nclf.fit(X_small, y_small)\ndecision_boundary(clf,X_small,y_small)\n", "intent": "the classification is better using SVM and the decision boundaries make them more distinct, the poly kernel serves as the best decision boundary\n"}
{"snippet": "X = df.values\nkm = KMeans(n_clusters=3).fit(X)\nkm.labels_\n", "intent": "There are some dense blobs inside, but it's rather hard to tell.\n"}
{"snippet": "from sklearn import metrics\ncuisine_similarity = []\nfor idx in range(cuisine_dtm.shape[0]):\n    similarity = metrics.pairwise.linear_kernel(cuisine_dtm[idx, :], cuisine_dtm).flatten()\n    cuisine_similarity.append(similarity)\n", "intent": "[How to calculate document similarity](http://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity/12128777\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(2000, activation='relu', input_dim=x_train.shape[1]))\nmodel.add(Dense(500, activation='relu'))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.summary()\noptimizer = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=85, batch_size=2000, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "logreg = LogisticRegression()\nlogreg\n", "intent": "Is your model predicting well? If not, how would you improve it?\n"}
{"snippet": "estimator = GridSearchCV(knn2, search_parameters, cv=5, verbose=1, n_jobs=4)\nresults = estimator.fit(trainX, trainY)\n", "intent": "+ Now use 10-fold cross-validation to choose the most efficient `k`\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(64, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "history = model.fit(x_train, y_train, epochs=5, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "ratings_true = ratings_mat[user_id, items_rated_by_this_user].todense()\nratings_pred = pred_ratings[items_rated_by_this_user]\nfor i,j in zip(np.array(ratings_true).squeeze(),ratings_pred):\n    print i, j\nerr_one_user = ratings_true-ratings_pred\nprint 'Mean Absolute Error:', abs(err_one_user).mean()\n", "intent": "We have checked total error for all users, here we want to check error for each rated item of picked user.\n"}
{"snippet": "clf_A.fit(nnn, y)\n", "intent": "This is an overfitting due to we don't have properly formatted features. (Supervised learning failed). \n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nclf= DecisionTreeClassifier(random_state=1)\nclf.fit(training_inputs, training_classes)\n", "intent": "Now create a DecisionTreeClassifier and fit it to your training data.\n"}
{"snippet": "import numpy as np\nfrom sklearn import linear_model\nX = np.array([[-1, -1], [-2, -1], [-2, -1], [-2, -1], [-2, -1], [2, 1]])\nY = np.array([1, 3, 3, 3, 3, 2])\nclf = linear_model.SGDClassifier(max_iter=15, tol=None)\nclf.fit(X, Y)\n", "intent": "                          *** Implement Incremental/Online Learning using SGDClassifier **\n"}
{"snippet": "results = smf.ols('Sales ~ Price + US', data=carseats).fit()\nresults.summary()\n", "intent": "d) Reject null for US and Price\n"}
{"snippet": "estimates = defaultdict(list)\nfor predictor in list(boston_df.drop('CRIM',axis=1)):\n    X = sm.add_constant(boston_df[predictor])\n    estimate=sm.OLS(boston_df['CRIM'],X).fit()\n    estimates[predictor] = estimate\n    print(\"{0:8s}: [beta_0, beta_1] = [{1:5.2f},{2:6.2f}]:    p-value= {3:10.6f}\".format(predictor, estimate.params[0], estimate.params[1], estimate.pvalues[1]))\n", "intent": "We will regress 'CRIM' response onto each of the predictors and save the coeffecients and the p-values from each regression. \n"}
{"snippet": "X_train = df_X_train[\"Lag2\"].values.reshape(-1,1)\ny_train = df_X_train[\"Direction2\"].values\nknn_20 = KNeighborsClassifier(n_neighbors= 20)\nknn_20.fit(X_train, y_train)\n", "intent": "Using only 1 Nearest Neighbor, the model performs at chance level.\n"}
{"snippet": "X_train = training_df[[\"Lag1\", \"Lag2\"]].values\nY_train = training_df[\"Direction2\"].values\nlda_model = LDA(solver=\"lsqr\")\nlda_model.fit(X_train, Y_train)\nprint('Class Priors =', lda_model.priors_)\nprint('Class Means =', lda_model.means_)\nprint('Class Coefficients =', lda_model.coef_)\n", "intent": "Predict the market direction using Lag1 and Lag2 as predictors using scikit learn LDA module.\n"}
{"snippet": "from sklearn import svm, datasets\nC = 1.0 \nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\n", "intent": "Now we'll use linear SVC to partition our graph into clusters:\n"}
{"snippet": "logreg = linear_model.LogisticRegression(solver = \"newton-cg\", multi_class = 'multinomial')\n", "intent": "Next, let's try multinomial logistic regression! Follow the same steps as with Naive Bayes, and plot the confusion matrix.\n"}
{"snippet": "svm = svm.LinearSVC()\nsvm_model = svm.fit(X_train, y_train)\n", "intent": "Now do the same for a Support Vector Machine.\n"}
{"snippet": "ada_reg = AdaBoostRegressor(base_estimator=None,  \n                                    n_estimators=50,  \n                                    learning_rate=1.0,  \n                                    random_state=10, \n                                    loss='linear')  \nada_model = ada_reg.fit(X_train, y_train)\nprint(ada_model.score(X_train, y_train))\nprint(ada_model.score(X_val, y_val))\n", "intent": "Using an Ada Boost Regressor is nearly identical to using the other regressors covered today.\n"}
{"snippet": "from sklearn import svm\nC = 1.0\nsvc = svm.SVC(kernel='linear', C=C)\n", "intent": "Next try using svm.SVC with a linear kernel. How does it compare to the decision tree?\n"}
{"snippet": "model.add(\n    Dense(num_classes, kernel_initializer='normal', activation='softmax')\n)\n", "intent": "Add in the output layer to our model.\n"}
{"snippet": "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n", "intent": "Let's now fit our model on the training dataset. We fit our model over 10 epochs and update it every 200 images. It might take a few minutes.\n"}
{"snippet": "import numpy\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras import backend as K\nK.set_image_dim_ordering('th')\n", "intent": "Keras also provides useful methods to create a CNN.\n"}
{"snippet": "def CNN_model():\n    model = Sequential()\n    model.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n", "intent": "Let's define a function that creates our CNN. Read through the comments to understand what each line does.\n"}
{"snippet": "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n", "intent": "Let's now fit our model on the training datasets. We fit our model over 10 epochs and update it every 200 images. It might take a few minutes.\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(init)\n    epochs = 100\n    for i in range(epochs):\n        sess.run(train)\n    final_slope, final_intercept = sess.run([m, b])\n", "intent": "- **Create Session and Run!**\n"}
{"snippet": "def get_oob(df):\n    m = RandomForestRegressor(n_estimators=30, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)\n    x, _ = split_vals(df, n_trn)\n    m.fit(x, y_train)\n    return m.oob_score_\n", "intent": "- Let's try removing some of these related features to see if the model can be simplified without impacting the accuracy.\n"}
{"snippet": "model.fit(X_train_indices, Y_train_oh, epochs = 100, batch_size = 32, shuffle=True)\n", "intent": "Fit the Keras model on `X_train_indices` and `Y_train_oh`. We will use `epochs = 50` and `batch_size = 32`.\n"}
{"snippet": "vgg_model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n", "intent": "Now lets add your first pooling layer. Pooling reduces the width and height of the input by aggregating adjacent cells together.\n"}
{"snippet": "def batch_lrelu(x, alpha=0.1, batch_normalization=True, is_train=True):\n    if batch_normalization:\n        temp = tf.layers.batch_normalization(x, training=is_train)\n        return tf.maximum(alpha * x, x)\n    else:\n        return tf.maximum(alpha * x, x) \n", "intent": "Each convolution goes throught the same process of batch_normalization and then a leaky relu\n"}
{"snippet": "predictions = Dense(2, activation='softmax')(fc1)\n", "intent": "Finally, for the output, we need a softmax layer with 2 neurons (two classes: 0/1). \n"}
{"snippet": "model.fit(XTrain_s_batch, yTrain_batch, epochs=30) \n", "intent": "Fit the model on the batched training data with 100 epochs, you should see the accuracy on the test set increase dramatically.\n"}
{"snippet": "inputs = Input(shape=(None, 17)) \nlstm = LSTM(512, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n            bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n            recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n            kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, \n            recurrent_dropout=0.0, implementation=1, return_sequences=True, return_state=False, \n            go_backwards=False, stateful=False, unroll=False)(inputs)\n", "intent": "Let's use as a first layer a 512 LSTM without regularisation\n"}
{"snippet": "predictions = TimeDistributed(Dense(17, activation='linear'))(lstm)\nmodel_lstm512 = Model(inputs=inputs, outputs=predictions)\nmodel_lstm512.compile(optimizer='rmsprop',\n              loss='mean_squared_error',\n              metrics=['accuracy'])\n", "intent": "The only thing which we need to change is the output dimensions of the network from 1 to 17. \n"}
{"snippet": "model_lstm512.fit(X_batch, y_batch, epochs=500)\n", "intent": "Let's turn the magic on, train for 500 epochs, more may be better but may crash your computer.\n"}
{"snippet": "rnn_model.fit(XTrain_s_batch, yTrain_batch, epochs=15)\n", "intent": "Now we're good to fit this for a few epochs and check the performances. \n"}
{"snippet": "hist = model.fit(x_train, y_train, batch_size = 128, epochs = 8, validation_split = 0.2, verbose = 0, callbacks=[plot_losses], shuffle = True)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-softmax'))\n    feed = {x: test_x,\n            y: test_y}\n    out = sess.run(tf.nn.softmax(logits), feed_dict=feed)\n    out_y =sess.run(tf.argmax(out, axis=1))\n    label_y = sess.run(tf.argmax(test_y, axis=1))\n    test_los,test_acc = sess.run([cost,accuracy], feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "load the trained model and run it\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-softmax'))\n    feed = {x: test_x,\n            y: test_y}\n    out = sess.run(tf.nn.softmax(logits), feed_dict=feed)\n    out_test_y =sess.run(tf.argmax(out, axis=1))\n    label_test_y = sess.run(tf.argmax(test_y, axis=1))\n    test_los,test_acc = sess.run([cost,accuracy], feed_dict=feed)\n    print(\"Test accuracy: {:.4f}%\".format(test_acc*100))\n", "intent": "load the trained model and run it\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_normal([vocab_size, embed_dim], -1 ,1))\n    return tf.nn.embedding_lookup(embeddings, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-softmax'))\n    feed = {x: test_x,\n            y: test_y}\n    out = sess.run(tf.nn.softmax(logits), feed_dict=feed)\n    out_test_y =sess.run(tf.argmax(out, axis=1))\n    label_test_y = sess.run(tf.argmax(test_y, axis=1))\n    test_los,test_acc = sess.run([cost,accuracy], feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc*100))\n", "intent": "load the trained model and run it\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-softmax'))\n    feed = {x: train_x,\n            y: train_y}\n    out = sess.run(tf.nn.softmax(logits), feed_dict=feed)\n    out_train_y =sess.run(tf.argmax(out, axis=1))\n    label_train_y = sess.run(tf.argmax(train_y, axis=1))\n    test_los,test_acc = sess.run([cost,accuracy], feed_dict=feed)\n    print(\"Train accuracy: {:.4f}\".format(test_acc))\n", "intent": "show the lowest confidence for correct output\nshow the highest confidence for wrong output\nBUT keep the number of correct prediction at 95%\n"}
{"snippet": "for i in tf.get_default_graph().get_operations():\n    print(i.name)\n", "intent": "https://stackoverflow.com/questions/35336648/list-of-tensor-names-in-graph-in-tensorflow\n"}
{"snippet": "import functools\ndef get_embed(input_data, vocab_size, embed_dim):\n    input_data_list = input_data.get_shape().as_list()\n    input_dim = functools.reduce(lambda x,y: x*y, input_data_list)\n    embedding = tf.Variable(tf.truncated_normal([vocab_size, embed_dim], stddev = np.sqrt(1.0/input_dim)))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "kmeans(<FILL IN>)\n", "intent": "Test your algorithm on the dataset\n"}
{"snippet": "kmeans(rdd, N_CENTERS, 20)\n", "intent": "Test your algorithm on the dataset\n"}
{"snippet": "print(x4.todense())\n", "intent": "Let's print it and see what it looks like \n"}
{"snippet": "simple_model = Sequential()\nsimple_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape,padding='same'))\nsimple_model.add(MaxPooling2D((2, 2),padding='same'))\nsimple_model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\nsimple_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nsimple_model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))\nsimple_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nsimple_model.add(Flatten())\nsimple_model.add(Dense(128, activation='relu'))\nsimple_model.add(Dense(num_classes, activation='softmax'))\n", "intent": "Finally, we initialize a model and start adding layers \n"}
{"snippet": "simple_train = simple_model.fit(X_train, y_train_onehot, batch_size=batch_size, epochs=num_epochs, verbose=1,\n                                validation_data=(X_valid, y_valid_onehot))\n", "intent": "**Part B**: Now we're ready to actually train the model! You might want to grab a magazine or something because this'll take a minute!   \n"}
{"snippet": "def FCBlock(model):\n    model.add(Dense(4096, activation='relu'))\n    model.add(Dropout(0.5))\n", "intent": "...and here's the fully-connected definition.\n"}
{"snippet": "import scipy\nscipy.stats.norm.fit(house.SalePrice)\n", "intent": "That doesn't look promising.\n"}
{"snippet": "import fitter\nf = fitter.Fitter(house.SalePrice)\nf.fit()\nf.summary()\n", "intent": "Pull out the big guns -- what distribution might it be?\n"}
{"snippet": "f = fitter.Fitter(house.LogPrice)\nf.fit()\nf.summary()\n", "intent": "Yeah! That looks normal. Let's double check..\n"}
{"snippet": "import sklearn.linear_model\ngrid = sklearn.model_selection.GridSearchCV(\n    sklearn.linear_model.Lasso(normalize=True),\n    param_grid = {'alpha': np.arange(0.0001, 0.0010, 0.00001)\n                 }\n)\ngrid.fit(numeric_house, house.LogPrice)\ngrid.best_score_\n", "intent": "So, how does this prediction model work?\n"}
{"snippet": "log_sci_model = LogisticRegression()\n", "intent": "Instantiate a logistic regression model as:\n"}
{"snippet": "from sklearn import metrics, cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfeatures = ['Pclass', 'Imputed_Age', 'SibSp', 'Parch', 'Fare', 'C', 'Q', 'female']\nlog_sci_model = LogisticRegression()\nlog_sci_model = log_sci_model.fit(train_data[features], train_data['Survived'])\nlog_sci_model.score(train_data[features], train_data['Survived'])\n", "intent": "Train the model with 'Fare':\n"}
{"snippet": "from sklearn import metrics, cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfeatures = ['Fare']\nlog_sci_model = LogisticRegression()\nlog_sci_model = log_sci_model.fit(train_data[features], train_data['Survived'])\nlog_sci_model.score(train_data[features], train_data['Survived'])\n", "intent": "Train the model with 'Fare':\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded_input = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.01))\n    embedded_input = tf.nn.embedding_lookup(embedded_input, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "cells = tf.contrib.rnn.MultiRNNCell([create_single_cell(lstm_size, keep_prob) for _ in range(number_of_layers)])\n", "intent": "Stack the LSTM cell in to Mutliple layers\n"}
{"snippet": "lm = Sequential([ Dense(2, activation='softmax', input_shape=(1000,)) ])\nlm.compile(optimizer=RMSprop(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "Now we can define our linear model, just like we did earlier:\n"}
{"snippet": "with tf.variable_scope('softmax_variables'):\n    softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n    softmax_b = tf.Variable(tf.zeros(num_classes))\n", "intent": "The shape of the weights will be Number of hidden units in a LSTM cell and the size of class/unique characters.\n"}
{"snippet": "logits = tf.nn.softmax(prediction, name='predictions')\n", "intent": "Apply the softmax function for output logits from each LSTM\n"}
{"snippet": "from time import time\nfrom sklearn.linear_model import LogisticRegression\ntic = time()\nlogisticRegr = LogisticRegression(solver = 'lbfgs')\nlogisticRegr.fit(train_img, train_label)\nscore = logisticRegr.score(test_img, test_label)\ntoc = time()\nprint('The total time is %s seconds ' % (toc-tic))\nprint('The classification accuracy is %s ' % score)\n", "intent": "Hint: Use LogisticRegression from sklearn.linear_model. To increase speed, change the default solver to 'lbfgs'\n"}
{"snippet": "iris_model = KNeighborsClassifier(5).fit(iris_features_train, iris_target_train)\n", "intent": "Train the KNN classifier\n"}
{"snippet": "def cross_validate(features, target, classifier, k_fold=10, r_state=None) :\n    k_fold_indices = KFold(k_fold,shuffle=True, random_state=r_state)\n    k_score_total = 0\n    for train_indices, test_indices in k_fold_indices.split(features, target):\n        model = classifier.fit(features[train_indices],target[train_indices])\n        k_score = model.score(features[test_indices],target[test_indices])\n        k_score_total += k_score\n    return k_score_total/k_fold\n", "intent": "Function for calculating cross-validation\n"}
{"snippet": "cross_validate(features, target, KNeighborsClassifier(5), 10, 0)\n", "intent": "Test the cross_validate function with different numbers of neighbors\n"}
{"snippet": "titanic_features_normalized = titanic_data[['Age_norm', 'Sex', 'Pclass_1', 'Pclass_2', 'Pclass_3']].values\nknn_accuracy_with_neighbors_normalized = [(neighbor, cross_validate(titanic_features_normalized, titanic_target, KNeighborsClassifier(neighbor, weights='distance'), 10, 0)) for neighbor in range(1, 70)]\nzipped_data_normalized = list(zip(*knn_accuracy_with_neighbors_normalized))\nprint(zipped_data_normalized[1])\npercentages = list(zipped_data_normalized[1])\nprint(sum(percentages)/len(percentages))\n", "intent": "Create new features using normalized data\n"}
{"snippet": "random_forest_model = RandomForestClassifier(random_state=0)\n", "intent": "Create an instance of a random forest classifier.  Random state is used to set random number generator for reproducible results\n"}
{"snippet": "model.add(layers.Dense(units=84, activation='relu'))\n", "intent": "The sixth layer is a fully connected layer (F6) with 84 units.\n"}
{"snippet": "lm = Sequential([ Dense(2, activation='softmax', input_shape=(1000,)) ])\n", "intent": "Do you remember how we defined our linear model? Here it is again for reference:\n"}
{"snippet": "for k in range(1, 21):\n    knn = KNeighborsClassifier(n_neighbors = k)\n    x,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\n    knn.fit(x_train,y_train)\n    print('KNN (K=' + str(k) + ') accuracy is: ',knn.score(x_test,y_test)) \n", "intent": "Let's look at how changing the number of neigbors affects the accuracy of our prediction. \n"}
{"snippet": "for k in range(1, 20):\n    knn = KNeighborsClassifier(n_neighbors = k)\n    x,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\n    knn.fit(x_train,y_train)\n    print('KNN (K=' + str(k) + ') accuracy is: ',knn.score(x_train,y_train)) \n", "intent": "Now, let's do what we said we weren't going to do and test with our training data instead of our testing data. \n"}
{"snippet": "estimator.fit(trainingImages, trainingValues)\n", "intent": "Now we fit the `estimator` just like we would any other model, but now it's doing a fit for each pair of values in the grid search\n"}
{"snippet": "estimator.fit(X_train, y_train)\n", "intent": "Now we fit the `estimator` just like we would any other model, but now it's doing a fit for each pair of values in the grid search\n"}
{"snippet": "from sklearn import linear_model\ncls = linear_model.LinearRegression()\ncls.fit(X, y)\n", "intent": "**Sklearn Method for Part 2**\n"}
{"snippet": "from sklearn import linear_model\nmodel = linear_model.LogisticRegression(penalty='l2', C=1.0)\nmodel.fit(X2, y2)\nmodel.coef_\n", "intent": "**Sklearn Regularized Logistic Regression Solution**\n"}
{"snippet": "def sigmoidGradient(z):\n    return np.multiply(sigmoid(z), (1 - sigmoid(z)))\nsigmoidGradient(0)\n", "intent": "**2. Backpropagation**\n"}
{"snippet": "clf = sklearn.svm.SVC(C=1000, kernel='linear')\nclf.fit(df[['X1','X2']], df['y'])\ngraph_svm_decision_boundary(clf)\n", "intent": "Large C (No Regularization)\n"}
{"snippet": "clf = sklearn.svm.SVC(C=1, kernel='linear')\nclf.fit(df[['X1','X2']], df['y'])\ngraph_svm_decision_boundary(clf)\n", "intent": "Small C (Lots of Regularization)\n"}
{"snippet": "model.add(Dense(4096, activation='relu'))\n", "intent": "And do you remember the definition of a fully connected layer in the original VGG?:\n"}
{"snippet": "modelW0 = sm.OLS(y, X)\nresultsW0 = modelW0.fit()\nprint (resultsW0.summary())\n", "intent": "Now let's fit a linear regression model with an intercept. \n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim],-0.01,0.01),dtype=tf.float32)\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn import linear_model\nX = df_Features\ny = df_Target\nlg = linear_model.LogisticRegression(penalty=\"l2\", dual=False)\nlg.fit(X,y)\nlg.score(X,y)\n", "intent": "Build a logit model and fit\n"}
{"snippet": "k = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(dn)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "k = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(newDF)\n", "intent": "Cluster the Data to our our target groups. **BONUS**: Perform multiple clustering tests on various indicator variables\n"}
{"snippet": "k = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(X)\n", "intent": "Run the clustering analysis using scikit-learn.\n*Hint*: Estimate the number of clusters, k, based on your visual examination of the distributions.\n"}
{"snippet": "dbscn = DBSCAN(eps = 3.2, min_samples = 3, random_state=5).fit(X)  \n", "intent": "Let's set up the DBSCAN from scikit using an epsilon of .5 and a minimum sample number of 5\n"}
{"snippet": "db2 = DBSCAN(eps=.2, min_samples=3, random_state=5).fit(X)\ndb2\n", "intent": "Let's set up the DBSCAN from scikit using an epsilon of .5 and a minimum sample number of 5\n"}
{"snippet": "W = 0.01 * np.random.randn(784,10)\nb = np.zeros((1,10))\nX_in = Input()\nW_in = Input()\nb_in = Input()\ny_in = Input()\nf = Linear(X_in, W_in, b_in)\nf = CrossEntropyWithSoftmax(f, y_in)\n", "intent": "You should hit above 90% accuracy on even a 1-layer neural network.\n"}
{"snippet": "def fit_model(model, batches, val_batches, nb_epoch=1):\n    model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=nb_epoch, \n                        validation_data=val_batches, nb_val_samples=val_batches.N)\n", "intent": "We'll define a simple function for fitting models, just to save a little typing...\n"}
{"snippet": "model.add(Convolution2D(filters=32, kernel_size=3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.summary()\n", "intent": "- Add the second convolutional block to the model\n- Call `keras.models.Model.summary()` to verify the dimensions are correct\n"}
{"snippet": "model.add(Convolution2D(filters=64, kernel_size=3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2))\n", "intent": "- Add the second convolutional block to the model\n- Call `keras.models.Model.summary()` to verify the dimensions are correct\n"}
{"snippet": "model.add(Flatten())\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n", "intent": "- Add the first bottleneck block to the model\n- Call `keras.models.Model.summary()` to verify the dimensions are correct\n"}
{"snippet": "model.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\nmodel.summary()\n", "intent": "- Add the second bottleneck block to the model\n- Call `keras.models.Model.summary()` to verify the dimensions are correct\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=42)\ndtc.fit(X_train, y_train);\n", "intent": "[sklearn.tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n"}
{"snippet": "def leaky_relu(x,alpha=0.1,name='leaky_relu'):\n    return tf.maximum(alpha*x,x,name=name)\n", "intent": "Implement a leaky relu function\n"}
{"snippet": "wf_array.todense()\n", "intent": "The sparse arrays can be converted to dense with the `todense` method. The dense version can be read by pandas and converted into a dataframe.\n"}
{"snippet": "knn_classifier = KNeighborsClassifier(n_neighbors = 3, weights = 'distance')\nknn_classifier.fit(wine_topics, wine_df['rating'])\n", "intent": "We can now use our topics as features\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\ndtc = DecisionTreeClassifier(max_depth = 3, \n                             min_samples_leaf = 10) \n", "intent": "What about a different method?\n"}
{"snippet": "fc_model.fit(trn_features, trn_labels, nb_epoch=8, \n             batch_size=batch_size, validation_data=(val_features, val_labels))\n", "intent": "And fit the model in the usual way:\n"}
{"snippet": "model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Flatten(input_shape=(64, 64,)))\nmodel.add(MyLinearLayer(len(labels)))\nmodel.compile(\n    optimizer=tf.train.AdamOptimizer(),\n    loss='categorical_crossentropy',\n    metrics=['accuracy'])\nmodel.summary()\n", "intent": "**Model definition**\nWe can now define our TF model using `tf.keras.Sequential`.\n"}
{"snippet": "model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=1)\n", "intent": "**Training the model**\n"}
{"snippet": "model.save('./tmp.h5')\nloaded_model = tf.keras.models.load_model('./tmp.h5', dict(MyLinearLayer=MyLinearLayer))\n", "intent": "**Saving and loading a (trained) model**\n"}
{"snippet": "treereg = DecisionTreeRegressor(max_depth=6, random_state=1)\ntreereg.fit(X,y)\n", "intent": "Ok, looks like 9 is the right max depth for this model\n"}
{"snippet": "treereg = DecisionTreeRegressor(max_depth=9, random_state=1)\ntreereg.fit(X,y)\n", "intent": "Ok, looks like 9 is the right max depth for this model\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr_all = LinearRegression()\n", "intent": "- Make a new instance of the LinearRegression class. Call it lr_all to distinguish it from our last model.\n"}
{"snippet": "X = bikes.iloc[:, 0:8]\ny = bikes.loc[:,'num_total_users']\nlr_all.fit(X,y)\n", "intent": "- Train the model instance using our new feature matrix $X$ and the same target variable $y$.\n"}
{"snippet": "lr_temp_atemp = LinearRegression()\nX = bikes.loc[:,['temp_celsius','atemp']]\nlr_temp_atemp.fit(X,y)\n", "intent": "**Exercise.**\n- Create another `LinearRegression` instance that is fit using `temp_celsius` and `atemp`, and print the coefficients.\n"}
{"snippet": "lr_temp_atemp = LinearRegression()\nX = bikes.loc[:, ['temp_celsius', 'atemp']]\nlr_temp_atemp.fit(X, y)\n", "intent": "**Exercise.**\n- Create another `LinearRegression` instance that is fit using `temp_celsius` and `atemp`, and print the coefficients.\n"}
{"snippet": "def embedding_input(name, n_in, n_out, reg):\n    inp = Input(shape=(1,), dtype='int64', name=name)\n    return inp, Embedding(n_in, n_out, input_length=1, W_regularizer=l2(reg))(inp)\n", "intent": "Previously we didnt add bias terms which is very crucial. Let us try with bias\n"}
{"snippet": "knn =  KNeighborsClassifier(n_neighbors=5)\n", "intent": "<a id=\"tuning-a-knn-model\"></a>\n---\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg\nX = glass.loc[:, ['al']]\ny = glass.loc[:, 'ri']\nlinreg.fit(X,y)\n", "intent": "- Instantiate and fit a linear regression model predicting `ri` from `al` (and an intercept).\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nX = mammals.loc[:, ['body']]\npf = PolynomialFeatures(degree=3, include_bias = False)\npf.fit(X)\npf.transform(X)\n", "intent": "**sklearn has a \"transformer\" that generates polynomial terms**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "logistic_regression = LogisticRegression()\nclassification_model(logistic_regression)\n", "intent": "<img src=\"Images/logistic_regression.jpeg\" height=\"742\" width=\"742\" align=\"middle\" > \n* Slightly good result \n"}
{"snippet": "decision_trees = DecisionTreeClassifier()\nclassification_model(decision_trees)\n", "intent": "<img src=\"Images/decision_trees.png\" height=\"742\" width=\"742\" align=\"middle\" > \nOverfitting\n"}
{"snippet": "random_forests = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\nclassification_model(random_forests)\n", "intent": "<img src=\"Images/random_forests.jpg\" height=\"742\" width=\"742\" align=\"middle\" > \nGood classifier\n"}
{"snippet": "model.fit(\n    X_train,\n    y_train,\n    batch_size=512,\n    nb_epoch=30,\n    validation_split=0.1,\n    verbose=1)\n", "intent": "Adjust hyperparameters and train the model.\n"}
{"snippet": "resultados_lrgs = [lrgs.fit(x[train_indices], y[train_indices]).score(x[test_indices],y[test_indices]) for train_indices, test_indices in kf_total]\n", "intent": "Com isso podemos procurar nosso modelo \"tunado\":\n"}
{"snippet": "user_in = Input(shape=(1,), dtype='int64', name='user_in')\nu = Embedding(n_users, n_factors, input_length=1, W_regularizer=l2(1e-4))(user_in)\nmovie_in = Input(shape=(1,), dtype='int64', name='movie_in')\nm = Embedding(n_movies, n_factors, input_length=1, W_regularizer=l2(1e-4))(movie_in)\n", "intent": "The most basic model is a dot product of a movie embedding and a user embedding. Let's see how well that works:\n"}
{"snippet": "gs = RandomizedSearchCV(pipe, param_grid, n_jobs = 7, n_iter = 10)\ngs.fit(X, y)\nprint \"Best CV score\", gs.best_score_\nprint gs.best_params_\n", "intent": "We can instead randomly sample from the parameter space with __RandomizedSearchCV__:\n"}
{"snippet": "nf = 22 \nlsi = models.LsiModel(corpus, id2word=dictionary, num_topics=nf, extra_samples=150,)\nindex = similarities.MatrixSimilarity(lsi[corpus], num_features=nf) \n", "intent": "define Latent Semantic Index of Documents\n"}
{"snippet": "post_profile.todense()\n", "intent": "And converting the sparce vector to a dense one, we can see we are really saving memory here, as most of the values are zeroes.\n"}
{"snippet": "def get_post_profile_sorted_features(post_id):\n    profile_values = get_post_profile_values(post_id)\n    tfidf_values = profile_values.todense().flatten().tolist()[0]\n    tfidf_feature_values = zip(tfidf_feature_names, tfidf_values)\n    sorted_tfidf_feature_values = sorted(tfidf_feature_values, key=lambda c: c[1], reverse=True)\n    return filter(lambda p: p[1] > 0, sorted_tfidf_feature_values)\npost_features = get_post_profile_sorted_features('z121itaiezmtzbp5j04cg5owalnmwjqab20')\npost_features\n", "intent": "It's possible to retrieve the most \"important\" words for a post\n"}
{"snippet": "def get_global_relevant_words():\n    tfidf_global_feature_values = tfidf_sparce_matrix.mean(axis=0).flatten().tolist()[0]\n    feature_values = zip(tfidf_feature_names, tfidf_global_feature_values)\n    sorted_feature_values = sorted(feature_values, key=lambda c: c[1], reverse=True)\n    return sorted_feature_values\nglobal_relevant_words = get_global_relevant_words()[:200]\nglobal_relevant_words[:20]\n", "intent": "Finally, we average TF-IDF vectors of all posts, and get the most \"important\" terms for the company professionals.\n"}
{"snippet": "regr3 = linear_model.Ridge(alpha = 0.6)\nregr3.fit(X_train, y_train)\nprint('Coefficients: ', regr3.coef_[0][0])\nprint('Variance score: %.2f' % regr3.score(X_test, y_test))\n", "intent": "Fit the ridge regressor on all the attributes and calculate the fits for a given $\\alpha$\n"}
{"snippet": "param_grid = [{'n_estimators': [25, 50, 100, 200],\n               'max_depth': [16, 32],\n               'min_samples_split' : [2, 4],\n               'min_samples_leaf' : [1],\n               'bootstrap' : [True]}]\nt0 = time()\nrfr = grid_search.GridSearchCV(ensemble.RandomForestRegressor(),\n                               param_grid, cv=5, verbose=0)\nrfr.fit(boston_train, bostony_train)\nprint('Done in %0.3f[s]' %(time() - t0))\n", "intent": "In this example we use a Random Forest Regressor and we choose the best estimator based on Cross Validation score.\n"}
{"snippet": "gbr = ensemble.GradientBoostingRegressor()\nparams = {'n_estimators':[100, 200, 500],\n          'max_depth':[4, 6],\n          'learning_rate':[0.1, 0.01],\n          'subsample':[0.5]}\nt0 = time()\ngrid = grid_search.GridSearchCV(gbr, params, n_jobs=-1)\ngrid.fit(boston_train, bostony_train)\nprint('Done in %0.3f[s]' %(time() - t0))\n", "intent": "In this example we use a Gradient Boosting Regressor Tree and we choose the best estimator based on Cross Validation score.\n"}
{"snippet": "forest = ensemble.RandomForestRegressor(n_estimators=500, max_features=0.3)\nforest.fit(Cal_train, Caly_train)\n", "intent": "In the following example, we use scikit-learn's RandomForestRegressors to asses variable importance. \n"}
{"snippet": "model = Sequential([\n        BatchNormalization(axis=1, input_shape=(3,224,224)),\n        Flatten(),\n        Dense(10, activation='softmax')\n    ])\nmodel.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n                 nb_val_samples=val_batches.nb_sample)\n", "intent": "Our hypothesis was correct. It's nearly always predicting class 1 or 6, with very high confidence. So let's try a lower learning rate:\n"}
{"snippet": "regr3 = linear_model.Ridge(alpha = 0.6)\nregr3.fit(X_train, y_train)\nprint 'Coefficients: ', regr3.coef_\nprint ('Variance score: %.2f' % regr3.score(X_test, y_test))\n", "intent": "Fit the ridge regressor on all the attributes and calculate the fits for a given $\\alpha$\n"}
{"snippet": "param_grid = [{'n_estimators': [25, 50, 100, 200],\n               'max_depth': [16, 32],\n               'min_samples_split' : [2, 4],\n               'min_samples_leaf' : [1],\n               'bootstrap' : [True]}]\nt0 = time()\nrfr = grid_search.GridSearchCV(ensemble.RandomForestRegressor(),\n                               param_grid, cv=5, verbose=0)\nrfr.fit(boston_train, bostony_train)\nprint 'Done in %0.3f[s]' %(time() - t0)\n", "intent": "In this example we use a Random Forest Regressor and we choose the best estimator based on Cross Validation score.\n"}
{"snippet": "gbr = ensemble.GradientBoostingRegressor()\nparams = {'n_estimators':[100, 200, 500],\n          'max_depth':[4, 6],\n          'learning_rate':[0.1, 0.01],\n          'subsample':[0.5]}\nt0 = time()\ngrid = grid_search.GridSearchCV(gbr, params, n_jobs=-1)\ngrid.fit(boston_train, bostony_train)\nprint 'Done in %0.3f[s]' %(time() - t0)\n", "intent": "In this example we use a Gradient Boosting Regressor Tree and we choose the best estimator based on Cross Validation score.\n"}
{"snippet": "batch_size =  \nepochs = \nlr = \nmomentum = \ntrain_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\ndev_loader = torch.utils.data.DataLoader(dev, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)\nmodel = Model()\noptimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum) \n", "intent": "Set your batch size, learning rate, number of epochs etc. Experiment with various hyper parameters.\n"}
{"snippet": "batch_size =  32\nepochs = 25\nlr = 1e-2\nmomentum=.9\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\ndev_loader = torch.utils.data.DataLoader(dev, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)\nmodel = Model()\noptimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n", "intent": "Set your batch size, learning rate, number of epochs etc. Experiment with various hyper parameters.\n"}
{"snippet": "b = torch.Tensor([[1,2,3],[4,5,6]])\nprint(b)\nprint(b.size())\n", "intent": "We can also take an array, and convert it to a tensor.\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS',axis = 1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "mod = KNeighborsClassifier(n_neighbors = 1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "mod.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=1, \n             validation_data=(conv_val_feat, val_labels))\n", "intent": "Now we can train the model as usual, with pre-computed augmented data.\n"}
{"snippet": "msno.matrix(players.sample(500),\n            figsize=(16, 7),\n            width_ratios=(15, 1))\n", "intent": "https://github.com/ResidentMario/missingno\n"}
{"snippet": "msno.matrix(players.sample(500),\n            figsize=(16, 7),\n            width_ratios=(15, 1))\n", "intent": "We've removed 468 players from the table who had no skintone rating.\nLet's look again at the missing data in this table. \n"}
{"snippet": "players.head()\n", "intent": "This looks correlated enough to me -- let's combine the rater's skintone ratings into a new column that is the average rating. \n"}
{"snippet": "sns.distplot(players.skintone, kde=False);\n", "intent": "What is the skintone distribution?\n"}
{"snippet": "sns.distplot(players.height.dropna());\n", "intent": "It's possible that size might correspond with redcards. \n"}
{"snippet": "clean_players.head()\n", "intent": "There are a couple of ways to do this -- set operations and joins are two ways demonstrated below: \n"}
{"snippet": "model = LeNet.build(width=32, height=32, depth=1, classes=2)\nopt = SGD(lr=0.01)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\nmodel.fit(X_train, Y_train, batch_size=10, nb_epoch=300,verbose=1)\n", "intent": "We build the neural network and fit it on the training set\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.5))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def sigmoid_gradient(z):\n    return np.multiply(sigmoid(z), (1 - sigmoid(z)))\n", "intent": "The first thing we need is a function that computes the gradient of the sigmoid function we created earlier.\n"}
{"snippet": "conv1 = Sequential([\n    Embedding(vocab_size, 32, input_length=seq_len, dropout=0.2),\n    Dropout(0.2),\n    Convolution1D(64, 5, border_mode='same', activation='relu'),\n    Dropout(0.2),\n    MaxPooling1D(),\n    Flatten(),\n    Dense(100, activation='relu'),\n    Dropout(0.7),\n    Dense(1, activation='sigmoid')])\n", "intent": "A CNN is likely to work better, since it's designed to take advantage of ordered data. We'll need to use a 1D CNN, since a sequence of words is 1D.\n"}
{"snippet": "svc = svm.SVC(C=100, gamma=10, probability=True)\nsvc\n", "intent": "For this data set we'll build a SVM classifier using the built-in RBF kernel and examine its accuracy on the training data.  \n"}
{"snippet": "x = torch.Tensor([1, 2, 3])\ny = torch.Tensor([4, 5, 6])\nprint(x)\nprint(y)\nw = torch.matmul(x, y)\nprint(w)\n", "intent": "Simple mathematical operations: <b>Addition, Multiplication</b>\n"}
{"snippet": "var_x = autograd.Variable(x, requires_grad=True)\nvar_y = autograd.Variable(y, requires_grad=True)\nvar_z = var_x + var_y\nprint(var_z.grad_fn)\n", "intent": "Now we wrap the torch tensors in <code>autograd.Variable</code>. The <code>var_z</code> contains the information for backpropagation:\n"}
{"snippet": "var_z_data = var_z.data\nnew_var_z = autograd.Variable(var_z_data)\nprint(new_var_z.grad_fn)\n", "intent": "But what happens if we extract the wrapped tensor object out of <code>var_z</code> and re-wrap the tensor in a new <code>autograd.Variable</code>?\n"}
{"snippet": "if torch.cuda.is_available():\n    a = torch.LongTensor(10).fill_(3).cuda()\n    print(type(a))\n    b = a.cpu()\n", "intent": "CUDA\n----\nCheck wether GPU accelaration with **CUDA** is available\n"}
{"snippet": "from keras.layers import Flatten\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras import backend as K\nK.set_image_dim_ordering('th')\n", "intent": "So, let's use Keras to change our ANN from an MLP to an CNN network, and see if we perform better ...\n"}
{"snippet": "w_layer1 = np.random.rand(4)\ndef neuron(x):\n    return sigmoid(x.dot(w_layer1))\n", "intent": "Next, let's create a function, which computed the logistic function for a single datapoint:\n"}
{"snippet": "w_layer2 = np.random.rand(4,4)\ndef layer2(x):\n    return sigmoid(x.dot(w_layer2))\n", "intent": "To create a 2nd layer, we only have to change the weight matrix:\n"}
{"snippet": "def sigmoid(x):\n    return 1/(1+np.exp(-x))\nw_layer1 = np.random.rand(4,4)\ndef layer1(x):\n    return sigmoid(x.dot(w_layer1))\n", "intent": "As a next step, let's vectorize our implementation:\n"}
{"snippet": "graph_in = Input ((vocab_size, 50))\nconvs = [ ] \nfor fsz in range (3, 6): \n    x = Convolution1D(64, fsz, border_mode='same', activation=\"relu\")(graph_in)\n    x = MaxPooling1D()(x) \n    x = Flatten()(x) \n    convs.append(x)\nout = Merge(mode=\"concat\")(convs) \ngraph = Model(graph_in, out) \n", "intent": "We use the functional API to create multiple conv layers of different sizes, and then concatenate them.\n"}
{"snippet": "hidden_out = tf.add(tf.matmul(x, W1), b1)\nhidden_out = tf.nn.relu(hidden_out)\n", "intent": "Next, we have to setup node inputs and activation functions of the hidden layer nodes:\n"}
{"snippet": "params_grid = {'n_estimators': [50, 100, 300, 500], 'max_depth': [2,3,4]}\ngsearch = GridSearchCV(estimator = gbr, param_grid = params_grid,\n                       scoring='neg_mean_absolute_error', n_jobs=4,iid=False, cv=5)\ngsearch.fit(X_train, y_train)\ngsearch.grid_scores_, gsearch.best_params_, gsearch.best_score_\n", "intent": "Lista de scoring functions:\nhttp://scikit-learn.org/stable/modules/model_evaluation.html\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_table = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding_table, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_table = tf.Variable(tf.random_uniform((vocab_size + 1, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding_table, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "my_tree = tree.DecisionTreeClassifier(criterion=\"entropy\")\nmy_tree.fit(X_train,y_train)\n", "intent": "Train a decision tree\n"}
{"snippet": "my_tree = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=2)\nmy_tree = my_tree.fit(X_train,y_train)\n", "intent": "Train a decision tree, limiting its depth to 2\n"}
{"snippet": "param_grid = [\n {'n_estimators': list(range(100, 501, 50)), 'max_features': list(range(1, 10, 2)), 'min_samples_split': [20] }\n]\nmy_tuned_model = GridSearchCV(ensemble.RandomForestClassifier(), param_grid, cv=5)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "param_grid = [\n {}\n]\nmy_tuned_model = GridSearchCV(linear_model.LogisticRegression(), param_grid, cv=5)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "param_grid = [\n {}\n]\nmy_tuned_model = GridSearchCV(neighbors.KNeighborsClassifier(), param_grid, cv=5)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "model = Sequential ([\n    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, weights=[emb]),\n    Dropout (0.2),\n    graph,\n    Dropout (0.5),\n    Dense (100, activation=\"relu\"),\n    Dropout (0.7),\n    Dense (1, activation='sigmoid')\n    ])\n", "intent": "We then replace the conv/max-pool layer in our original CNN with the concatenated conv layers.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(input_dim=784, units=512))\nmodel.add(Activation(\"sigmoid\"))\nmodel.add(Dense(units=207))\nmodel.add(Activation(\"sigmoid\"))\nmodel.add(Dense(units=102))\nmodel.add(Activation(\"sigmoid\"))\nmodel.add(Dense(units=10))\nmodel.add(Activation(\"softmax\"))\n", "intent": "Specfiy the structure of the neural network model\n"}
{"snippet": "def create_model(optimiser = \"rmsprop\", hidden_units = 512):\n    model = Sequential()\n    model.add(Dense(input_dim=784, units=hidden_units))\n    model.add(Activation(\"sigmoid\"))\n    model.add(Dense(units=10))\n    model.add(Activation(\"softmax\"))\n    model.compile(optimizer=optimiser,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n    return model\n", "intent": "Specfiy the structure of the neural network model and training parameters in a function\n"}
{"snippet": "tf_size_factor = tf.Variable(np.random.randn(), name=\"size_factor\")\ntf_price_offset = tf.Variable(np.random.randn(), name=\"price_offset\")\n", "intent": "Define the variables holding size_factor and price_offset <br>\ny = size_factor * x + price_offset\n"}
{"snippet": "W = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n", "intent": "variable weight and biases\n"}
{"snippet": "def weight_variable(shape):\n    initial = tf.truncated_normal(shape=shape, stddev=0.1)\n    return tf.Variable(initial)\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n", "intent": "We are using ReLU activation function so we will set the Weight and biases to some random number otherwise ReLU will give activation function 0.\n"}
{"snippet": "def conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\"SAME\")\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n", "intent": "we use convolution and then pooling to avoid overfitting\n"}
{"snippet": "W_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1)+ b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n", "intent": "Define layers in the CNN\n"}
{"snippet": "keep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n", "intent": "Remember the keep_prob is placeholder, it will be hyperparameter and will get as a training input\n"}
{"snippet": "model = Model(inputs, output)\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n", "intent": "Instantiate the model and compile the model\n"}
{"snippet": "model = Sequential([\n    Embedding(vocab_size, 32, input_length=seq_len, mask_zero=True,\n              W_regularizer=l2(1e-6), dropout=0.2),\n    LSTM(100, consume_less='gpu'),\n    Dense(1, activation='sigmoid')])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "We haven't covered this bit yet!\n"}
{"snippet": "model = LinearRegression()\nx = df[[\"TV\"]]\nmodel.fit(x,df[\"Sales\"])\n", "intent": "Will will try building a simple linear model to predict Sales based on the TV budget spend:\n"}
{"snippet": "x = df[[\"Newspaper\"]]\nmodel = LinearRegression()\nmodel.fit(x,df[\"Sales\"])\n", "intent": "We can repeat the same process using a different features, such as newspaper budget spend:\n"}
{"snippet": "model = LinearRegression()\nmodel.fit(x,df[\"Sales\"])\n", "intent": "Now use all 3 input variables to fit linear regression model:\n"}
{"snippet": "model = KNeighborsClassifier(n_neighbors=3)\nmodel.fit(dataset_train, target_train)\nprint(model)\n", "intent": "Next, we will fit a k-nearest neighbor model to the data using $k=3$ nearest neighbours:\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=3)\nmodel.fit(data_train, target_train)\nprint(model)\n", "intent": "Now we will build a KNN classifier ($k=3$) as we have seen previously. Note that we only use the training data to build the model:\n"}
{"snippet": "model = GridSearchCV(LogisticRegressionCV(solver='liblinear'), {'cv':np.arange(2, 17, 1), 'Cs':np.arange(1, 12, 1), 'penalty': ['l1']},\n                    n_jobs=-1, verbose=1)\nmodel.fit(X, y)\nmodel.best_estimator_\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "k = 2\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(df2)\n", "intent": "Cluster the Data to our our target groups. **BONUS**: Perform multiple clustering tests on various indicator variables\n"}
{"snippet": "model = naive_bayes.MultinomialNB()\nmodel.fit(X_train, y_train)\n", "intent": "Instantiate the Naive Bayes predictor from scikit-learn with the training data. \n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n", "intent": "Define a logistic regression and train it with the feature and target set\n"}
{"snippet": "model = simple_gallup_model(gallup_2012)\nmodel\n", "intent": "Now, we run the simulation with this model, and plot it.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "logreg.fit(X_training, y_training.ravel())\n", "intent": "<hr />\n<b>Then we need to fit the training data to the model</b>\n"}
{"snippet": "from sklearn import tree\nmodel = tree.DecisionTreeClassifier()\n", "intent": "Lets initialize our model so we can take a look at it's attributes.\n"}
{"snippet": "from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = ExtraTreesClassifier(n_estimators=400)\nclf = clf.fit(train, targets)\n", "intent": "<hr/>\n<img style=\"width:400px;\" src=\"http://labs.centerforgov.org/Analytics-Training/images/data_prep.png\" />\n<hr/>\n"}
{"snippet": "grid_search = GridSearchCV(forest, param_grid=parameter_grid, cv=cross_validation)\ngrid_search.fit(train_new, targets)\nprint('Best score : {}'.format(grid_search.best_score_))\nprint('Best parameters : {}'.format(grid_search.best_params_))\n", "intent": "<hr/>\n<img style=\"width:450px;\" src=\"https://thinkr.fr/wp-content/uploads/random-forest.jpg\" />\n<hr/>\n"}
{"snippet": "from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = ExtraTreesClassifier(n_estimators=400)\nclf = clf.fit(train, targets)\n", "intent": "<img style=\"width:400px;\" src=\"http://labs.centerforgov.org/Analytics-Training/images/data_prep.png\" />\n<br/><br/><hr/><br/><br/>\n"}
{"snippet": "grid_search = GridSearchCV(forest, param_grid=parameter_grid, cv=cross_validation)\ngrid_search.fit(train_new, targets)\nprint('Best score : {}'.format(grid_search.best_score_))\nprint('Best parameters : {}'.format(grid_search.best_params_))\n", "intent": "<br/><br/><hr/><br/><br/>\n<img style=\"width:400px;\" src=\"https://thinkr.fr/wp-content/uploads/random-forest.jpg\" />\n"}
{"snippet": "X = T.vector()\n", "intent": "Numpy style indexing\n===========\n"}
{"snippet": "cnn_layers = googlenet.build_model()\ncnn_input_var = cnn_layers['input'].input_var\ncnn_feature_layer = cnn_layers['loss3/classifier']\ncnn_output_layer = cnn_layers['prob']\nget_cnn_features = theano.function([cnn_input_var], lasagne.layers.get_output(cnn_feature_layer))\n", "intent": "Build the model and select layers we need - the features are taken from the final network layer, before the softmax nonlinearity.\n"}
{"snippet": "model = uncertain_gallup_model(gallup_2012)\nmodel = model.join(electoral_votes)\n", "intent": "We construct the model by estimating the probabilities:\n"}
{"snippet": "knn_final = KNeighborsRegressor(n_neighbors=10, weights='distance', metric='euclidean', n_jobs=-1)\nknn_final.fit(X, y)\n", "intent": "If you are happy with your model we can re-train it using all observations, and then use it to make predictions.\n"}
{"snippet": "model = applications.VGG16(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\nfor layer in model.layers[:5]:\n    layer.trainable = False\nx = model.output\nx = Flatten()(x)\nx = Dense(1024, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\nx = Dense(1024, activation=\"relu\")(x)\npredictions = Dense(2, activation=\"softmax\")(x)\nmodel_final = Model(input = model.input, output = predictions)\n", "intent": "Now we will invoke the VGG16 pretrained model, not including the top flattening layers.\n"}
{"snippet": "model.fit(X['train'], y['train'], batch_size=512, epochs=10, validation_split=0.08)\n", "intent": "Now it's time to run the model and adjust the weights. The model fitter will use 8% of the dataset values as the validation set.\n"}
{"snippet": "vector = np.arange(-5,5,0.1)\ndef relu(x):\n    return max(0.,x)\nrelu = np.vectorize(relu)\n", "intent": "- 28x28=728 elements in input layer\n- 10 elements in output layer\n- 3 hidden layers: 350, 200, 100\n"}
{"snippet": "tf_keras.activations.relu(inputs)\ntf_keras.activations.sigmoid(inputs)\ntf_keras.activations.softmax(inputs)\ntf_keras.activations.tanh(inputs)\ntf_keras.activations.elu(inputs)\ntf_keras.activations.hard_sigmoid(inputs)\ntf_keras.activations.softplus(inputs)\ntf_keras.activations.softsign(inputs)\ntf_keras.activations.linear(inputs)\n", "intent": "This is a layer of neurons that applies the non-saturating activation function. It increases the nonlinear properties of the decision function\n"}
{"snippet": "model.fit(x=X_train, y=Y_train, batch_size=64,\n          verbose=1, epochs=6, validation_data=(X_test,Y_test))\n", "intent": "To train our CNN, we simply need to call the fit function.\nLet's pass in both our train and test data and set the batch size to 64.\n"}
{"snippet": "def compile_model(model):\n    loss = tf_keras.losses.categorical_crossentropy\n    optimizer = tf_keras.optimizers.Adam(lr=hyper_params[\"learning_rate\"])\n    metrics = [tf_keras.metrics.categorical_accuracy,\n               tf_keras.metrics.top_k_categorical_accuracy]\n    model.compile(loss = loss,\n                  optimizer = optimizer,\n                  metrics = metrics)\n    print(model.summary())\n    return model\n", "intent": "Add the loss function, the optimizer, and the metrics to the model.\n"}
{"snippet": "def updateTargetGraph(tfVars,tau):\n    total_vars = len(tfVars)\n    op_holder = []\n    for idx,var in enumerate(tfVars[0:total_vars//2]):\n        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n    return op_holder\ndef updateTarget(op_holder,sess):\n    for op in op_holder:\n        sess.run(op)\n", "intent": "These functions allow us to update the parameters of our target network with those of the primary network.\n"}
{"snippet": "def update_target_graph(from_scope, to_scope):\n    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n    op_holder = []\n    for from_var,to_var in zip(from_vars,to_vars):\n        op_holder.append(to_var.assign(from_var))\n    return op_holder\n", "intent": "These functions allow us to update the parameters of our target network with those of the primary network.\n"}
{"snippet": "MLP_G = Sequential([\n    Dense(200, input_shape=(100,), activation='relu'),\n    Dense(400, activation='relu'),\n    Dense(784, activation='sigmoid'),\n])\n", "intent": "We'll keep thinks simple by making D & G plain ole' MLPs.\n"}
{"snippet": "X = iris[[\"petal_length\", \"setosa\", \"virginica\"]]\nX = sm.add_constant(X) \ny = iris[\"petal_width\"]\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n", "intent": "Now we perform a multilinear regression with the dummy variables added.\n"}
{"snippet": "accuracy = DecisionTreeClassifier(max_depth=5).fit(X_train, y_train).score(X_test, y_test)\nprint('Model\\'s classification accuracy on the test set: {0}%'.format(accuracy*100))\n", "intent": "**1.3** Report the model's classification accuracy on the test set.\n"}
{"snippet": "i = 5    \ns = 2    \nk = 3    \np = 0    \nx = torch.randn(1,1,i,i)\ncl = nn.Conv2d(1,1,k,s,padding=p)\nprint('Output size:                    ', cl(x).size())\nprint('Expected (spatial) output size: ', np.floor((i-k)/s).astype(np.int)+1)\n", "intent": "We expect (**Relationship 5**)\n$$ o = \\lfloor (i-k)/s \\rfloor +1$$\n"}
{"snippet": "i = 4    \ns = 1    \nk = 3    \np = 0    \ntcl = nn.ConvTranspose2d(1,1,k,s,p)\nprint(tcl(out).size())\n", "intent": "Lets implement the same using PyTorch's `nn.ConvTranspose2d` layer.\n"}
{"snippet": "i = 9   \ns = 2    \nk = 3    \np = 1    \nx = torch.randn(1,1,i,i)\ncl = nn.Conv2d(1,1,k,s,p)\nout = cl(x)\nprint(out.size())\ntcl = nn.ConvTranspose2d(1,1,k,s,p)\nprint(tcl(out).size())\n", "intent": "Here, we assume, $i-2p+k$ is a multiple of $s$ (otherwise, we have the same problem as mentioned above).\n"}
{"snippet": "simp_model = resnet18()                           \nsimp_model.load_state_dict(simp_model_state_dict) \nfreeze_model(simp_model)                          \nsimp_model.fc = nn.Linear(simp_model.fc.in_features, 2) \nprint(simp_model.fc.weight.requires_grad)\n", "intent": "Next, we replace the last layer of our model!\n"}
{"snippet": "a = ll.weight.data.numpy().flatten()\nb = x.data.numpy()\nprint('Dot product:', (a*b).sum())\nassert(np.abs(ll(x).item() -(a*b).sum())<1e-9)\n", "intent": "Lets quickly check the result, by computing the dot product $\\langle w,x\\rangle$:\n"}
{"snippet": "hl = nn.Linear(2,2, bias=False)\nx = torch.randn(2)\nprint(\"Output:\", hl(x))\nprint(hl.weight)\na = hl.weight.data.numpy()\nprint(\"Output 1: {:.4f}\".format((a[0,:]*x.data.numpy()).sum()))\nprint(\"Output 2: {:.4f}\".format((a[1,:]*x.data.numpy()).sum()))\n", "intent": "In fact, we have now one neuron that takes 2 inputs and produces 1 output. Now, lets try to have two 2 neurons, each producing one output:\n"}
{"snippet": "from exercise_code.model_savers import save_test_model\nsave_test_model(model)\n", "intent": "Now you need to save the model. We provide you with all the functionality, so you will only need to execute the next cell.\n"}
{"snippet": "CNN_D = Sequential([\n    Convolution2D(256, 5, 5, subsample=(2,2), border_mode='same', \n                  input_shape=(28, 28, 1), activation=LeakyReLU()),\n    Convolution2D(512, 5, 5, subsample=(2,2), border_mode='same', activation=LeakyReLU()),\n    Flatten(),\n    Dense(256, activation=LeakyReLU()),\n    Dense(1, activation = 'sigmoid')\n])\nCNN_D.compile(Adam(1e-3), \"binary_crossentropy\")\n", "intent": "The discriminator uses a few downsampling steps through strided convolutions.\n"}
{"snippet": "def get_gradient(X, y, w, mini_batch_indices, lmbda):\n    x0 = np.zeros((mini_batch_indices.shape[0], X.shape[1]))\n    y0 = np.zeros(mini_batch_indices.shape[0])\n    for i in range(len(mini_batch_indices)):\n        x0[i] = X[mini_batch_indices[i],:]\n        y0[i] = y[mini_batch_indices[i]]\n    dw = np.zeros(w.shape[0])\n    dw = x0.T.dot(sigmoid(x0.dot(w))-y0)/len(y0)+lmbda*w    \n    return dw\n", "intent": "Make sure that you compute the gradient of the loss function $\\mathcal{L}(\\mathbf{w})$ (not simply the NLL!)\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(256, input_dim=x_train.shape[1]))\nmodel.add(Activation('softmax'))\nmodel.add(Dropout(0.8))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=10, validation_data=(x_test,y_test), verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = KNeighborsClassifier(10)\nmodel.fit(X_train, Y_train)\ndata_tools.Decision_Surface(X, Y, model, cell_size=.05, surface=True, points=False)\n", "intent": "Now, let's try setting the number of neighbors to use, $k$, to a few different values and look at the results.\n"}
{"snippet": "W_conv2 = weight_variable([2, 2, 32, 64])  \nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n", "intent": "In order to build a deep network, we stack several layers of this type. The second layer will have 64 features for each 5x5 patch.\n"}
{"snippet": "W_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n", "intent": "Finally, we add a softmax layer, just like for the one layer softmax regression above.\n"}
{"snippet": "km = KMeans(n_clusters=4, random_state=1)\nkm.fit(X_scaled)\nbeer['cluster'] = km.labels_\nbeer.sort('cluster')\n", "intent": "Ignore past 6. Optimum number is 4. \n"}
{"snippet": "feature_cols = ['Pclass','Parch']\nX = titanic[feature_cols]\ny = titanic.Survived\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n", "intent": "Define **Pclass** and **Parch** as the features, and **Survived** as the response.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntreeclf = DecisionTreeClassifier(max_depth = 7, random_state=1)\ntreeclf.fit(X, y)\n", "intent": "Use 10-fold cross-validation to evaluate a decision tree model with those same features (fit to any \"max_depth\" you choose).\n"}
{"snippet": "PATH = 'data/cifar10/'\ndata = datasets.CIFAR10(root=PATH, download=True,\n   transform=transforms.Compose([\n       transforms.Scale(sz),\n       transforms.ToTensor(),\n       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n   ])\n)\n", "intent": "Pytorch has the handy [torch-vision](https://github.com/pytorch/vision) library which makes handling images fast and easy.\n"}
{"snippet": "km = KMeans(n_clusters=2, n_init=20)\nkm.fit(X_train)\n", "intent": "**Because the diabetes results are binary (either you have it or you dont) then we must use k=2. i.e. we are only looking for 2 populations**\n"}
{"snippet": "logreg = LogisticRegression(penalty='l1', C=10)\n", "intent": "penalty: 'l1' for lasso, 'l2' for ridge\n"}
{"snippet": "neigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(data, target) \n", "intent": "Set kNN's k value to 3 and fit the data\n"}
{"snippet": "classifier = LogisticRegression()\nclassifier.fit(train_arrays, train_labels)\n", "intent": "Now we train a logistic regression classifier using the training data.\n"}
{"snippet": "from keras.layers import Convolution2D, MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "from sklearn.svm import SVC \nmodel = SVC(kernel = 'linear', C = 1E10)\nmodel.fit(X, y)\n", "intent": "In SVMs the line that maximizis the margin the more is the one we will choose\n"}
{"snippet": "gmm = GMM(110, covariance_type='full', random_state=0)\ngmm.fit(data)\nprint(gmm.converged_)\n", "intent": "It appears that around 110 components minimizes the AIC\n"}
{"snippet": "sess = tf.Session()\nsess.run(init)\n", "intent": "This is the session where calculations encoded in the graph will\ntake place.\n"}
{"snippet": "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n", "intent": "We then convolve x_image with the weight tensor, add the bias, apply the\nReLU function, and finally max pool.\n"}
{"snippet": "from torch import FloatTensor as FT\ndef Var(*params): return Variable(FT(*params).cuda())\n", "intent": "Just some shortcuts to create tensors and variables.\n"}
{"snippet": "from sklearn.cluster import KMeans\nkmeans = KMeans(nb_clusters, random_state=8)\nY_hat = kmeans.fit(X).labels_\n", "intent": "Normally, you do not know the information in Y, however.\nYou could try to recover it from the data alone.\nThis is what the kMeans algorithm does.\n"}
{"snippet": "model.fit(x_train,y_train,epochs=10,batch_size=100)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform(dtype=tf.float32,shape=[vocab_size,embed_dim],minval=-.1,maxval=.1))\n    embed = tf.nn.embedding_lookup(params=embedding,ids=input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "get_predictions(index=0, \n                partition='train',\n                input_to_softmax=final_model(input_dim=161,units=200, recur_layers=2), \n                model_path='results/model_end.h5')\n", "intent": "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='validation',\n                input_to_softmax=final_model(input_dim=161,units=200, recur_layers=2), \n                model_path='results/model_end.h5')\n", "intent": "Use the next code cell to visualize the model's prediction for the first example in the validation dataset.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1000))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "history = model.fit(x_train, y_train, epochs=10, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = Sequential()\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "model = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n", "intent": "In the next two steps, we build the decision tree model from the training set and export it to a file for viewing in GraphViz.\n"}
{"snippet": "K.set_value(m_final.optimizer.lr, 1e-4)\nm_final.fit([arr_lr, arr_hr], targ, 16, 3, **parms)\n", "intent": "We use learning rate annealing to get a better fit.\n"}
{"snippet": "import statsmodels.formula.api as smf\nlm = smf.ols(formula='Sales ~ TV', data=data).fit()\nlm.summary()\n", "intent": "Let's use `Statsmodels` to estimate the associations between advertising efforts and sales. \n"}
{"snippet": "kmeans=KMeans(n_clusters=2,verbose=5)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "kmeans.fit(collage.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "lm=LinearRegression()\n", "intent": "Now its time to train our model on our training data!\n** Import LinearRegression from sklearn.linear_model **\n"}
{"snippet": "logmodel.fit(X_train,y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n", "intent": "Score and plot your predictions. What do these results tell us?\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\nmodel.add(Dropout(.75))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=200)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "top_model = Model(inp, outp)\n", "intent": "We are only interested in the trained part of the model, which does the actual upsampling.\n"}
{"snippet": "lm = smf.ols(formula='sales ~ TV + radio + newspaper', data=data).fit()\nprint lm.pvalues\n", "intent": "We could try a model with all features, and only keep features in the model if they have **small p-values**:\n"}
{"snippet": "lm = smf.ols(formula='sales ~ TV + radio', data=data).fit()\nlm.rsquared\n", "intent": "We could try models with different sets of features, and **compare their R-squared values**:\n"}
{"snippet": "import tensorflow as tf\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n", "intent": "** Import tensorflow.contrib.learn.python.learn as learn**\n"}
{"snippet": "data_dict_expanded = []\nfor transaction in data_dict:\n    for det in transaction['dets']:\n        data_dict_expanded.append(transaction.copy())\n        data_dict_expanded[-1].pop('dets', None)\n        data_dict_expanded[-1]['det'] = flatten(det, '.')\n        data_dict_expanded[-1]['num_items'] = len(transaction['dets'])\nprint_pretty_json(data_dict_expanded[0])\n", "intent": "__And expand the data dictionary over each det__\n"}
{"snippet": "df['ds'] = df.index\nmy_model.fit(df)\n", "intent": "and fit it with our dataframe df\n"}
{"snippet": "happyModel.fit(x=X_train,y=Y_train,epochs=5,batch_size=10)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "test_input = [[0.25, 0.15]]\ngraph=tf.Graph()\nwith tf.Session() as session:\n    tf.global_variables_initializer().run(session=session)\n    output = session.run([out],feed_dict=feed_dict) \n    print(output[0]) \n", "intent": "As with the previous example, we can use `session.run()` to execute this computation graph, and use a `feed_dict` to feed in our input:\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 2))\nnet.add(SoftMax())\nnet.add(Linear(2, 4))\nnet.add(ReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\ncriterion = MSECriterion()\nprint(net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "clz = DBSCAN(min_samples=4, eps=21.2)\nclz.fit(X)\n", "intent": "{'min_samples': 4, 'eps': 21.2}\n"}
{"snippet": "inp = Input((288,288,3))\nref_model = Model(inp, ReflectionPadding2D((40,2))(inp))\nref_model.compile('adam', 'mse')\n", "intent": "Testing the reflection padding layer:\n"}
{"snippet": "tf.reset_default_graph()\nbatch_size = 32\nwith tf.device('/cpu:0'):\n    real_images, one_hot_labels, _ = data_provider.provide_data(\n        'train', batch_size, MNIST_DATA_DIR)\ncheck_real_digits = tfgan.eval.image_reshaper(real_images[:20,...], num_cols=10)\nvisualize_digits(check_real_digits)\n", "intent": "<a id='conditional_input'></a>\n"}
{"snippet": "tf.reset_default_graph()\nbatch_size = 32\nwith tf.device('/cpu:0'):\n    real_images, _, _ = data_provider.provide_data(\n        'train', batch_size, MNIST_DATA_DIR)\ncheck_real_digits = tfgan.eval.image_reshaper(real_images[:20,...], num_cols=10)\nvisualize_digits(check_real_digits)\n", "intent": "<a id='infogan_input'></a>\nThis is the same as the unconditional case (we don't need labels).\n"}
{"snippet": "new_model = keras.models.load_model('my_model.h5')\nnew_model.summary()\n", "intent": "Now recreate the model from that file:\n"}
{"snippet": "model = models.Model(inputs=[inputs], outputs=[outputs])\n", "intent": "Using functional API, you must define your model by specifying the inputs and outputs associated with the model. \n"}
{"snippet": "history = model.fit(train_ds, \n                   steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))),\n                   epochs=epochs,\n                   validation_data=val_ds,\n                   validation_steps=int(np.ceil(num_val_examples / float(batch_size))),\n                   callbacks=[cp])\n", "intent": "Don't forget to specify our model callback in the `fit` function call. \n"}
{"snippet": "import tensorflow.contrib.eager as tfe\nw = tfe.Variable(3.0)\nwith tf.GradientTape() as tape:\n  loss = w ** 2\ndw, = tape.gradient(loss, [w])\nprint(\"\\nYou can use `tf.GradientTape` to compute the gradient of a \"\n      \"computation with respect to a list of `tf.contrib.eager.Variable`s;\\n\"\n      \"for example, `tape.gradient(loss, [w])`, where `loss` = w ** 2 and \"\n      \"`w` == 3.0, yields`\", dw,\"`.\")\n", "intent": "Create variables with `tf.contrib.eager.Variable`, and use `tf.GradientTape`\nto compute gradients with respect to them.\n"}
{"snippet": "from sklearn.dummy import DummyClassifier\nclf = DummyClassifier('most_frequent')\nclf.fit(train_data_finite, train_labels)\nprint(\"predcition accuracy: %f\" % clf.score(test_data_finite, test_labels))\n", "intent": "- most common dummy, if we predict all most_frequent we'd get this many predictions correct\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\n", "intent": "<img src=\"figures/supervised_workflow.svg\" width=\"100%\">\n"}
{"snippet": "model = svm.SVC(kernel='linear')\nmodel.fit(ingredients, type_label)\n", "intent": "__Step 4:__ Fit the Model\n"}
{"snippet": "class Evaluator(object):\n    def __init__(self, f, shp): self.f, self.shp = f, shp\n    def loss(self, x):\n        loss_, self.grad_values = self.f([x.reshape(self.shp)])\n        return loss_.astype(np.float64)\n    def grads(self, x): return self.grad_values.flatten().astype(np.float64)\n", "intent": "Since we use scipy bfgs for gradient descent, we need to send ti loss function and gradients seperately.\n"}
{"snippet": "from sklearn.neural_network import MLPRegressor\nmodelo_MLP=MLPRegressor(hidden_layer_sizes=(13),activation='relu',solver='adam',alpha=0.0001)\nmodelo_MLP.fit(X_train,Y_train)\n", "intent": "* Train a regression model using MLPRegressor in order to predict the output variable MEDV.\n"}
{"snippet": "centr_model = LinearRegression()\ncentr_model.fit(X_train_s,Y_train)\n", "intent": "1. Train a linear regression model using the scaled data.\n"}
{"snippet": "from sklearn.neural_network import MLPRegressor\nmodelo_MLP_cent=MLPRegressor(hidden_layer_sizes=(128,512),activation='relu',solver='adam',alpha=0.0001)\nmodelo_MLP_cent.fit(X_train_s,Y_train)\n", "intent": "2. Train a regression model using a 2-layer MultiLayer Perceptron (128 neurons in the first and 512 in the second) and with the scaled data.\n"}
{"snippet": "import tensorflow as tf\na = tf.constant(5, name='a')\nb = tf.constant(3, name='b')\nc = tf.add(a, b)\nwith tf.Session() as sess:\n    print(sess.run(c))\n    writer = tf.summary.FileWriter('tmp/', sess.graph)\nwriter.close()\n", "intent": "- Basic operations\n- Tensor types\n- Project speed dating\n- Placeholders and feeding inputs\n- Lazy loading\n"}
{"snippet": "random_normal = tf.random_normal([1]) \ntruncated_normal = tf.truncated_normal([1]) \nwith tf.Session() as sess:\n    print('random_normal = \\n', str(sess.run(random_normal)))\n    print('truncated_normal = \\n', str(sess.run(truncated_normal)))\n", "intent": "Randomly Generated Constants\n"}
{"snippet": "from tensorflow.python.framework import ops\nops.reset_default_graph()\ng=tf.get_default_graph()\n[op.name for op in g.get_operations()]\n", "intent": "- first clear default graph\n"}
{"snippet": "tf.reset_default_graph()\ndimensions = [512, 256, 128, 64]             \nn_features = mnist.train.images.shape[1]     \nX = tf.placeholder(tf.float32, [None, n_features])  \n", "intent": "<img src='pic/autoencoder_0.png',height=400, width = 900>\n"}
{"snippet": "from tensorflow.python.framework import ops\nops.reset_default_graph()\ng=tf.get_default_graph()\n[op.name for op in g.get_operations()]\n", "intent": "- using 1 layer first\n"}
{"snippet": "def init_weight(shape):\n    init_random_weight = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(init_random_weight)\ndef init_bais(shape):\n    init_random_bais = tf.constant(0.1, shape=shape)\n    return tf.Variable(init_random_bais)\n", "intent": "1. Helper\n2. INIT Weight\n3. INIT Bias\n4. CONV 2D\n5. Pooling\n"}
{"snippet": "K.set_value(m_sr.optimizer.lr, 1e-4)\nm_sr.fit([arr_lr, arr_hr], targ, 16, 1, **parms)\n", "intent": "We use learning rate annealing to get a better fit.\n"}
{"snippet": "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n", "intent": "<h4>5.3.1.2. Testing the model with best hyper paramters</h4>\n"}
{"snippet": "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n", "intent": "<h4>5.3.2.2. Testing model with best hyper parameters</h4>\n"}
{"snippet": "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42,class_weight='balanced')\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)\n", "intent": "<h3>5.4.2. Testing model with best hyper parameters</h3>\n"}
{"snippet": "clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)\n", "intent": "<h3>5.5.2. Testing model with best hyper parameters (One Hot Encoding)</h3>\n"}
{"snippet": "clf = RandomForestClassifier(max_depth=max_depth[int(best_alpha%4)], n_estimators=alpha[int(best_alpha/4)], criterion='gini', max_features='auto',random_state=42)\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y,cv_x_responseCoding,cv_y, clf)\n", "intent": "<h3>5.5.4. Testing model with best hyper parameters (Response Coding)</h3>\n"}
{"snippet": "rgrs = LinearRegression()\nrgrs.fit(x_tr, y_tr)\n", "intent": "d) Train model on training data, and make predictions on testing data\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable( tf.random_uniform([vocab_size, embed_dim], -1, 1) )\n    embeded = tf.nn.embedding_lookup(embedding, input_data)\n    return embeded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from distutils.version import LooseVersion\nimport warnings\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nassert LooseVersion(tf.__version__) == LooseVersion('1.1.0'), 'Please use TensorFlow version 1.1'\nprint('TensorFlow Version: {}'.format(tf.__version__))\nif not tf.test.gpu_device_name():\n    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\nelse:\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n", "intent": "This will check to make sure you have the correct version of TensorFlow and access to a GPU\n"}
{"snippet": "inp = Input((288,288,3))\nref_model = Model(inp, ReflectionPadding2D((40,10))(inp))\nref_model.compile('adam', 'mse')\n", "intent": "Testing the reflection padding layer:\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "Get Linear Regression model object\n"}
{"snippet": "lm.fit(x, y)\n", "intent": "Fit linear regression model on data\n"}
{"snippet": "def sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n", "intent": "Define sigmoid function. <br>\n           1. Input: An array.\n           2. Output: Sigmoid of Input.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1024, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=200, batch_size=100, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from distutils.version import LooseVersion\nimport tensorflow as tf\ntf.reset_default_graph()\nassert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\nprint('TensorFlow Version: {}'.format(tf.__version__))\n", "intent": "This will check to make sure you have the correct version of TensorFlow\n"}
{"snippet": "one_class_clf = OneClassSVM(nu=.00101001, gamma=0.000010, kernel='rbf')\none_class_clf.fit(X_resampled)\n", "intent": "Here I tried some inputs nu and gamma. <br/>\nThe following one was the highest auc that I achieved\n"}
{"snippet": "gauss_clf = GaussianMixture(3)\ngauss_clf.fit(X_resampled)\n", "intent": "Let's see a histogram of the classifier with the best AuC score.\n"}
{"snippet": "t_clf = t_mix(n_components=1, random_state=0)\nt_clf.fit(X_resampled)\n", "intent": "Let's try with the t distribution, and compare them\n"}
{"snippet": "import theano\nimport theano.tensor as T\ncurrent_states = T.matrix(\"states[batch,units]\")\nactions = T.ivector(\"action_ids[batch]\")\nrewards = T.vector(\"rewards[batch]\")\nnext_states = T.matrix(\"next states[batch,units]\")\nis_end = T.ivector(\"vector[batch] where 1 means that session just ended\")\n", "intent": "First step is initializing input variables\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embbeding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embbeding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0))\n    return tf.nn.embedding_lookup(embeddings, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embeddings, ids=input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = ARIMA(series, order=(5,1,0))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())\n", "intent": " we can see that there is a positive correlation with the first 10-to-12 lags that is perhaps significant for the first 5 lags.\n"}
{"snippet": "session = tf.Session()\n", "intent": "We now create a $TensorFlow$ session which is used to execute the graph.\n"}
{"snippet": "history_new_cont = model_new.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch-775,\n                                 verbose=0, validation_data=(X_test, Y_test), callbacks=callbacks_list_new)\nmodel_new.load_weights(checkpoints_filepath_new)\n", "intent": "After epoch 475 nothing will be saved because precision doesn't increase anymore.\n"}
{"snippet": "def discriminator(x, n_units=128, reuse=False, alpha=0.01):\n    with tf.variable_scope('discriminator', reuse=reuse):\n        h1 = tf.layers.dense(x, n_units, activation=None)\n        h1 = tf.maximum(alpha * h1, h1)\n        h2 = tf.layers.dense(h1, n_units / 2, activation=None)\n        h2 = tf.maximum(alpha * h2, h2)\n        logits = tf.layers.dense(h2, 1, activation=None)\n        out = tf.sigmoid(logits)\n        return out, logits\n", "intent": "The discriminator network is almost exactly the same as the generator network, except that we're using a sigmoid output layer.\n"}
{"snippet": "def discriminator(x, n_units=128, reuse=False, alpha=0.01):\n    with tf.variable_scope('discriminator', reuse=reuse):\n        h1 = tf.layers.dense(x, n_units, activation=None)\n        h1 = tf.maximum(alpha * h1, h1)\n        h2 = tf.layers.dense(h1, n_units, activation=None)\n        h2 = tf.maximum(alpha * h2, h2)\n        logits = tf.layers.dense(h1, 1, activation=None)\n        out = tf.sigmoid(logits)\n        return out, logits\n", "intent": "The discriminator network is almost exactly the same as the generator network, except that we're using a sigmoid output layer.\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingRegressor\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\ngbrt.fit(X, y)\n", "intent": "Now lets apply a Gradient Boosting Regressor and see how it functions\n"}
{"snippet": "import numpy as np\nimport tensorflow as tf\nm1 = np.array([[1., 2.], [3., 4.], [5., 6.], [7., 8.]], dtype=np.float32)\nm1_input = tf.placeholder(tf.float32, shape=[4, 2])\nm2 = tf.Variable(tf.random_uniform([2, 3], -1.0, 1.0))\nm3 = tf.matmul(m1_input, m2)\nm3 = tf.Print(m3, [m3], message=\"m3 is: \")\ninit = tf.global_variables_initializer()\n", "intent": "A jupyter notebook version of the simple 'starter' example.\nFirst, define a constant, and define the TensorFlow graph.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n", "intent": "The library is same as it was for Simple Linear Regression\n"}
{"snippet": "model = nn.Sequential()\nmodel.add_module('flatten', Flatten())\nmodel.add_module('dense1', nn.Linear(3 * 32 * 32, 64))\nmodel.add_module('dense1_relu', nn.ReLU())\nmodel.add_module('dense2_logits', nn.Linear(64, 10)) \n", "intent": "Let's start with a dense network for our baseline:\n"}
{"snippet": "input_image = Variable(content_img.clone().data, requires_grad=True)\noptimizer = torch.optim.LBFGS([input_image])\n", "intent": "We can now optimize both style and content loss over input image.\n"}
{"snippet": "regr.fit(X, Y);\nprint \"Aerogerador (Single):\", r_squared(regr.intercept_, regr.coef_, X, Y), \"\\n\\n\\n\";\n", "intent": "2. Execute o notebook usando agora o dataset aerogerador na pasta do github.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(units=512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units=256, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units=2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, \n          y_train, \n          epochs = 15,\n          batch_size = 64,\n          shuffle=True,\n          verbose = 2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model.fit(x_train, \n          y_train, \n          epochs = 10,\n          batch_size = 64,\n          shuffle=True,\n          verbose = 2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def random_forest_scores(n_iterations):\n    test_scores = []\n    for n in np.arange(n_iterations):\n        rf = RandomForestClassifier()\n        rf.fit(train_data, train_target)\n        test_scores.append(rf.score(test_data, test_target))\n    return pd.Series(test_scores)\n", "intent": "Let's try random forest next to get a comparison.\n"}
{"snippet": "data = import_data('train.csv')\ndata = preprocess_data(data)\nrf = RandomForestClassifier()\nrf.fit(data.drop(['Survived'], axis=1), data.Survived)\n", "intent": "Train the competition classifier.\n"}
{"snippet": "with tf.Session() as session:\n    init.run()\n    print(\"Initialized\")\n    print(\"m2: {}\".format(m2))\n    print(\"eval m2: {}\".format(m2.eval()))\n    feed_dict = {m1_input: m1}\n    result = session.run([m3], feed_dict=feed_dict)\n    print(\"\\nresult: {}\\n\".format(result))\n", "intent": "Then, run the graph in a `session`, specifying a value for the `m1_input` placeholder.\n"}
{"snippet": "housingModel = createCompileFitModel(X_train, y_train, learning_rate=0.01, epochs=500)\n", "intent": "Increasing the learning rate and number of epochs\n"}
{"snippet": "model.fit(X_train, y_train_cat, batch_size=128,\n          epochs=2, verbose=1, validation_split=0.3)\n", "intent": "Tons of parameters. That's gonna take sometime to train\n"}
{"snippet": "model.fit(X_train, y_train_cat, batch_size=128,\n          epochs=2, verbose=1, validation_split=0.3)\n", "intent": "As you can see there are fewer parameters\n"}
{"snippet": "model = Sequential()\nmodel.add(Embedding(input_dim=max_features, output_dim=100, embeddings_initializer=\"glorot_uniform\", input_length=maxlen))\nmodel.add(LSTM(20, return_sequences=False)) \nmodel.add(Dense(46))  \nmodel.add(Activation('softmax'))\nnp.random.seed(123)\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['accuracy'])\n", "intent": "<img src='../imgs/learning_a.png'/>\n"}
{"snippet": "model = Sequential()\nmodel.add(Embedding(input_dim=max_features, output_dim=500, embeddings_initializer=\"glorot_uniform\", input_length=maxlen))\nmodel.add(LSTM(256, return_sequences=False, dropout_W=0.2, dropout_U=0.2)) \nmodel.add(Dense(46))  \nmodel.add(Activation('softmax'))\nnp.random.seed(123)\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['accuracy'])\n", "intent": "<img src='../imgs/learning_b.png'/>\n"}
{"snippet": "dbscn = DBSCAN(eps = .6, min_samples = 5).fit(X)\n", "intent": "Let's use 5 min points (num dimensions + 1)\n"}
{"snippet": "def model( inputs, h, b ):\n    lastY = inputs\n    for i, (hi, bi) in enumerate( zip( h, b ) ):\n        y =  tf.add( tf.matmul( lastY, h[i]), b[i] )    \n        if i==len(h)-1:\n            return y\n        lastY =  tf.nn.sigmoid( y )\n        lastY =  tf.nn.dropout( lastY, dropout )\n", "intent": "<h2>Define the Layer operations as a Python funtion</h2>\n"}
{"snippet": "feature_columns = boston.feature_names\nregressor = learn.DNNRegressor( feature_columns=None,\n                                hidden_units=[10, 10] )\nregressor.fit( X_train, y_train, steps=5000, batch_size=1 )\n", "intent": "<h1>Building a [10,10] Neural Net</h1>\n<br />\n"}
{"snippet": "regressor = learn.DNNRegressor( feature_columns=None,\n                               hidden_units=[13, 13])\nregressor.fit(X_train, y_train, steps=5000, batch_size=10)\n", "intent": "<h1>Building a [13,13] Neural Net</h1>\n<br />\n"}
{"snippet": "graph = tf.Graph()\nnum_steps = 5000\nwith graph.as_default():\n  features = tf.placeholder(tf.float32, shape=[4, 2])\n  labels = tf.placeholder(tf.int32, shape=[4])\n  train_op, loss, gs = make_graph(features, labels)\n  init = tf.global_variables_initializer()\n", "intent": "Build the graph -- define the placeholders, and call make_graph().\nThen add an op to init the variables.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_mat = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding_mat, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embedded = tf.nn.embedding_lookup(embedding,input_data)\n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model.add(Dense(100, activation='relu', input_dim=X_train.shape[1]))\n", "intent": "In the first hidden layer, we must also specify the dimension of our input layer. This will simply be the number of elements (pixels) in each image.\n"}
{"snippet": "model.add(Dense(100, activation='relu'))\n", "intent": "A dense layer is when every node from the previous layer is connected to each node in the current layer.\n"}
{"snippet": "model.add(Dense(num_classes, activation='softmax'))\n", "intent": "We also need to specify the number of output classes. In this case, the number of digits that we wish to classify.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform(shape = [vocab_size, embed_dim], minval = -1, maxval = 1))\n    embedded_input = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model.fit(norm_data[:,0], norm_data[:,1],\n          epochs=10,\n          validation_split=0.2)\n", "intent": "Then, we can train the model.\n"}
{"snippet": "reset_graph()\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\ny = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n", "intent": "**Defining placeholders**\n"}
{"snippet": "reset_graph()\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\ny = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n", "intent": "**Using dense instead of neuron_layer function()**\n"}
{"snippet": "x = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.matmul(x, W) + b\ny_ = tf.placeholder(tf.float32, [None, 10])\n", "intent": "Next, create the model graph. It includes input 'placeholders'.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"model_ckps/my_model_final.ckpt\") \n    X_new_scaled = mnist.test.images[:20]\n    Z = logits.eval(feed_dict={X: X_new_scaled})\n    y_pred = np.argmax(Z, axis=1)\n", "intent": "**Using the Neural Network**\n"}
{"snippet": "from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(random_state=42, max_iter=10) \nsgd_clf.fit(X_train, y_train_5)\n", "intent": "Now, for binary classification '5' and 'Not 5', we train the SGD Classifier on the training dataset.\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n", "intent": "Training a classifier\n"}
{"snippet": "from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(random_state=42) \nsgd_clf.fit(X_train_scaled, y_train)\n", "intent": "Let us try <b>SGDClassifier</b> first\n"}
{"snippet": "from sklearn.ensemble import VotingClassifier\nlog_clf_ens = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=42)\nrnd_clf_ens = RandomForestClassifier(n_estimators=100, max_depth=50, random_state=42)\nvoting_clf = VotingClassifier(\n    estimators=[('lr', log_clf_ens), ('rf', rnd_clf_ens)],\n    voting='soft')\nvoting_clf.fit(X_train_scaled, y_train)\n", "intent": "Now, let us try <b>Ensemble</b> with <b>soft voting</b>\n"}
{"snippet": "from sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X, y)\nplot_decision_boundary(model, X, y)\n", "intent": "Try to find a model in scikit-learn that actually works on this datasets\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform(shape=[vocab_size, embed_dim], \n                                              minval=-1.0, maxval=1.0), \n                            name='embedding')\n    print('Embedding Table Shape: {0}'.format(embedding.get_shape()))\n    embed = tf.nn.embedding_lookup(embedding, ids=input_data, name='embed')\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Conv2D(32, (3,3), input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Activation('elu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('elu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform(shape=(vocab_size, embed_dim), \n                            minval=-1, maxval=1, dtype=tf.float32)) \n    embed = tf.nn.embedding_lookup(embedding, ids=input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "NUM_STEPS = 10000\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n", "intent": "Next, create a session and initialize the graph variables. We'll also set the number of steps we'll train with.\n"}
{"snippet": "import tensorflow as tf\nhello = tf.constant('It works!')\nsess = tf.Session()\nprint(sess.run(hello))\n", "intent": "This snippet of Python creates a simple graph.\n"}
{"snippet": "print(x4.todense()) \n", "intent": "Let's print it and see what it looks like \n"}
{"snippet": "mnb=MultinomialNB()\nprint_model_metrics(mnb,data_fill,features,label,20)\n", "intent": "As a reminder, here are the metrics for the data where we fill in missing Ages with the median and where we drop those rows.\n"}
{"snippet": "rf=RandomForestClassifier(random_state=1212, n_jobs=-1, max_depth=4,max_features=None,bootstrap=False)\nprint_model_metrics(rf,data_impute_rf,features_rf,label,20)\n", "intent": "That's much better than the resuls we got from our Naive Bayes imputation. \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.3))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=80, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "X = matutils.corpus2dense(lsi_corpus, num_terms=200).transpose()\nX.shape\n", "intent": "We have (very good, 300-dimensional) vectors for our documents now!  So we can do any machine learning we want on our documents!\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nX = body[['Weight', 'Height']] \ny = body.Male \nclf = LogisticRegression().fit(X, y)\npd.Series(clf.coef_[0], index=['Weight', 'Height'])\n", "intent": "*Do we already know any machine learning models?*\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import  KNeighborsClassifier\nX = body[['Weight', 'Height']]\ny = body.Male\nclf = KNeighborsClassifier(n_neighbors=50).fit(X, y)\n", "intent": "Another model: nearest neighbor\n"}
{"snippet": "def serving_input_receiver_fn():\n    feature_tensor = tf.placeholder(tf.float32, [None, 784])\n    return tf.estimator.export.ServingInputReceiver({'x': feature_tensor}, {'x': feature_tensor})\nexported_model_dir = mnist_classifier.export_savedmodel(MODEL_DIR, serving_input_receiver_fn)\ndecoded_model_dir = exported_model_dir.decode(\"utf-8\")\n", "intent": "Finally, export the model for later serving.\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid, verbose=3)\ngrid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "svc_rbf = SVC(kernel='rbf', random_state = 0, probability = True)\nsvc_rbf.fit(X_train_robust, y_train)\nvalidation_score = svc_rbf.score(X_val_robust, y_val)\nprint('Classification accuracy on preprocessed validation set: {:.6f}\\n'.format(validation_score))  \n", "intent": "<h2>SVC attempt with Radial Basis and RobustScaler</h2>\n"}
{"snippet": "svc_poly = SVC( kernel='poly', random_state = 0)\nsvc_poly.fit(X_train_robust, y_train)\nvalidation_score = svc_poly.score(X_val_robust, y_val)\nprint('Polynomial Classification accuracy on preprocessed validation set: {:.6f}\\n'.format(validation_score))  \n", "intent": "<h2>SVC attempt with Polynomials and RobustScaler</h2>\n"}
{"snippet": "kmeans = KMeans(n_clusters=5, random_state = 0).fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1)) \n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import KMeans\ncluster = KMeans(n_clusters=2)\ncluster.fit(distances.values)\n", "intent": "The clustering code is very simple: The code below will create two groups of stations.\n"}
{"snippet": "model_rnn=Sequential()\n", "intent": "The first thing is to create a model.\n"}
{"snippet": "model_rnn.add(SimpleRNN(1,input_shape=(1,1)))\n", "intent": "then we add the layer that we want\n"}
{"snippet": "model_LSTM=Sequential()\nmodel_LSTM.add(SimpleRNN(1,input_shape=(1,1)))\nmodel_LSTM.compile(loss='mean_squared_error',optimizer='Adam')\n", "intent": "and we can do the same for LSTM and GRU\n"}
{"snippet": "transfer_learning.maybe_download_and_extract(INCEPTION_MODEL_DIR)\ngraph, bottleneck_tensor, jpeg_data_tensor, resized_image_tensor = (\n    create_inception_graph())\nprint(\"Finished.\")\n", "intent": "Define the Inception-based graph we'll use to generate the 'bottleneck' values. Wait for this to print \"Finished\" before continuing.\n"}
{"snippet": "sess=tf.Session()\n", "intent": "Now, nothing has run yet, we need to create a session and run it to see how thinks change\n"}
{"snippet": "X=tf.placeholder(shape=(4,2),dtype=tf.float32,name='input')\ny=tf.placeholder(shape=(4,1),dtype=tf.float32,name='output')\nW=tf.Variable([[1],[1]],dtype=tf.float32,name='weights')\nb=tf.Variable([0],dtype=tf.float32,name='bias')\nglobal_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n", "intent": "and the objects were\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    writer = tf.summary.FileWriter('./cooler_graphs', sess.graph)\n    for i in range(10000):\n        _,_,summary=sess.run([loss,optimizer,summary_op],feed_dict={X:X_data,y:y_data})\n        writer.add_summary(summary,global_step=i)\n", "intent": "we are now ready for training\n"}
{"snippet": "X = tf.placeholder(shape=(4),dtype=tf.int32,name='INPUT')\ny_ = tf.placeholder(shape=(1),dtype=tf.int32,name='OUTPUT')\nU = tf.Variable(tf.random_normal([8238,128], stddev=0.5),name=\"U\",dtype=tf.float32)\nV = tf.Variable(tf.random_normal([128,8238], stddev=0.5),name=\"V\",dtype=tf.float32)\naveg_creator = tf.constant([[0.25,0.25,0.25,0.25]],name='average_creator')\n", "intent": "We first create the objects that will appear on the graphs\n"}
{"snippet": "u= tf.nn.embedding_lookup(U,X)\nu_aveg= tf.matmul(aveg_creator,u,name='average')\nv= tf.matmul(u_aveg,V)\ny_hat = tf.nn.softmax(v)\n", "intent": "Now, we create the graph\n"}
{"snippet": "sess=tf.Session()\nsess.run(tf.global_variables_initializer())\n", "intent": "We are ready to start the session\n"}
{"snippet": "sess=tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.global_variables())\n", "intent": "Let's run this in a Session.\n"}
{"snippet": "output,state= tf.contrib.rnn.static_rnn(cell,inputs,dtype=tf.float32,initial_state=initial_state )\n", "intent": "We now 'unroll' it to length 4.\n"}
{"snippet": "sess=tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(tf.global_variables())\n", "intent": "and we can run the session.\n"}
{"snippet": "age = tf.contrib.layers.real_valued_column(\"age\")\neducation_num = tf.contrib.layers.real_valued_column(\"education_num\")\ncapital_gain = tf.contrib.layers.real_valued_column(\"capital_gain\")\ncapital_loss = tf.contrib.layers.real_valued_column(\"capital_loss\")\nhours_per_week = tf.contrib.layers.real_valued_column(\"hours_per_week\")\nprint('continuous columns configured')\n", "intent": "Second, configure the real-valued columns using `real_valued_column()`. \n"}
{"snippet": "cell = tf.contrib.rnn.BasicRNNCell(1)\n", "intent": "and create the cell \n"}
{"snippet": "inputs=tf.unstack(X,axis=0)\ninitial_state=tf.constant([[0],[0],[0]],dtype=tf.float32)\noutput,state= tf.contrib.rnn.static_rnn(cell,inputs,dtype=tf.float32,initial_state=initial_state )\n", "intent": "next we create the network, but recall the signature takes a list not a tensor, so we need to use unpack\n"}
{"snippet": "sess=tf.Session()\nsess.run(tf.global_variables_initializer())\n", "intent": "and it is time to use the session\n"}
{"snippet": "output,_= tf.nn.dynamic_rnn(cell,X,dtype=tf.float32)\n", "intent": "and we create the rnn layer now\n"}
{"snippet": "sess=tf.Session()\nsess.run(tf.global_variables_initializer())\n", "intent": "we are ready to run a session\n"}
{"snippet": "with tf.name_scope(\"embedding\"):\n    W = tf.Variable(tf.random_uniform([vocab_size, emb_dim], -1.0, 1.0),name=\"W\")\n    embedded_chars = tf.nn.embedding_lookup(W, input_x)\n", "intent": "Next, we create the variables we are going to need.\n"}
{"snippet": "with tf.name_scope(\"hidden\"):\n    W_h= tf.Variable(tf.random_uniform([sentence_len*emb_dim, hidden_dim], -1.0, 1.0),name=\"w_hidden\")\n    b_h= tf.Variable(tf.zeros([hidden_dim],name=\"b_hidden\"))\n    hidden_output= tf.nn.relu(tf.matmul(emb_vec,W_h)+b_h)\n", "intent": "we can now go over the hidden dimension, but first we need a variable for this\n"}
{"snippet": "with tf.name_scope(\"output_layer\"):\n    W_o= tf.Variable(tf.random_uniform([hidden_dim,2], -1.0, 1.0),name=\"w_o\")\n    b_o= tf.Variable(tf.zeros([2],name=\"b_o\"))\n    score = tf.nn.relu(tf.matmul(hidden_output,W_o)+b_o)\n    predictions = tf.argmax(score, 1, name=\"predictions\")\n", "intent": "finally, the output layer\n"}
{"snippet": "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\noptimizer=tf.train.AdamOptimizer(1e-4).minimize(loss)\nloss_summary = tf.summary.scalar(\"loss\", loss)\nacc_summary = tf.summary.scalar(\"accuracy\", accuracy)\nsummary_op=tf.summary.merge([loss_summary,acc_summary])\n", "intent": "We are almost ready to start the session, we need the training operation\n"}
{"snippet": "age = layers.real_valued_column(\"age\")\neducation_num = layers.real_valued_column(\"education_num\")\ncapital_gain = layers.real_valued_column(\"capital_gain\")\ncapital_loss = layers.real_valued_column(\"capital_loss\")\nhours_per_week = layers.real_valued_column(\"hours_per_week\")\nprint('continuous columns configured')\n", "intent": "Second, configure the real-valued columns using `real_valued_column()`. \n"}
{"snippet": "with tf.name_scope(\"embedding\"):\n    W = tf.Variable(emb_matrix,name=\"W\",dtype=tf.float32)\n    embedded_chars = tf.nn.embedding_lookup(W, input_x)\n", "intent": "Next, we create the variables we are going to need.\n"}
{"snippet": "sess = tf.Session()\nsess.run(tf.global_variables_initializer())\ntrain_summary_writer = tf.summary.FileWriter('./summaries2/', sess.graph)\n", "intent": "Running the session\n"}
{"snippet": "embeddings=tf.Variable(emb_matrix)\nembeddings= tf.nn.embedding_lookup(embeddings,input_placeholder)\nembeddings= tf.reshape(embeddings,shape=(-1,embed_size*n_features))\n", "intent": "and the embedding layer\n"}
{"snippet": "W=tf.Variable(tf.random_normal(shape=(n_features*embed_size,hidden_size)))\nb1=tf.Variable(tf.zeros(hidden_size))\nU = tf.Variable(tf.random_normal(shape=(hidden_size,n_classes)))\nb2=tf.Variable(tf.zeros(n_classes))\npred = tf.nn.relu(tf.matmul(embeddings,W)+b1)\npred = tf.nn.dropout(pred,dropout)\npred = tf.matmul(pred,U)+b2\n", "intent": "and then the prediction layer\n"}
{"snippet": "sess= tf.Session()\nsess.run(tf.global_variables_initializer())\n", "intent": "We are ready to run the session.\n"}
{"snippet": "with tf.name_scope(\"Recurrent_layers\"):\n    lstms=[tf.contrib.rnn.LSTMCell(100) for i in range(3)]\n    staked_lstm = tf.contrib.rnn.MultiRNNCell(lstms)\n    initial_state=state=staked_lstm.zero_state(50,dtype=tf.float32)\n    output = tf.reshape(input_tokens, (-1,5,1),name=\"reshaped_input\")\n    output,_ = tf.nn.dynamic_rnn(staked_lstm,output,dtype=tf.float32)\n    output = tf.reshape(output,[-1,100])\n", "intent": "This will be feed to a RNN made of LSTM's, hence the first step \n<img src=\"lstmRNN.png\">\n"}
{"snippet": "with tf.name_scope(\"Softmax_layer\"):\n    W = tf.Variable(tf.random_normal(shape=(100,26)))\n    b= tf.Variable(tf.zeros(26))\n    output = tf.matmul(output,W)+b\n", "intent": "next we create the softmax layer\n"}
{"snippet": "with tf.name_scope(\"optimizer\"):\n    global_step=tf.Variable(0,dtype=tf.int32,trainable=False,name=\"global_step\")\n    optimizer=tf.train.AdamOptimizer().minimize(loss,global_step=global_step)\n", "intent": "let's use Adam optimizer for the gradient descend\n"}
{"snippet": "sess=tf.Session()\n", "intent": "we are ready for the training part\n"}
{"snippet": "kmeans_model = KMeans(n_clusters=2, random_state=42)\nkmeans_model.fit(X_scaled)\n", "intent": "From above, it appears that 2 would be a good number of clusters\n"}
{"snippet": "clustering = AgglomerativeClustering(linkage='ward', n_clusters=2)\nclustering.fit(data3_g)\ndata3_g.groupby(clustering.labels_).count()\n", "intent": "** From the result above, I will use 2 clusters**\n"}
{"snippet": "clustering = AgglomerativeClustering(linkage='complete', n_clusters=2)\nclustering.fit(data3_g)\ndata3_g.groupby(clustering.labels_).count()\n", "intent": "** From the result above, I will use 2 clusters**\n"}
{"snippet": "kernel = 1* RBF(length_scale=100.0, length_scale_bounds=(1e-3, 1e3)) \\\n    + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e10))\ngp = GaussianProcessRegressor(kernel=kernel,\n                              n_restarts_optimizer=10,random_state=1)\ngp.fit(X, Y)\n", "intent": "**1) The kernel after parameter optimization and fitting to the observed data.**\n**2) The log marginal likelihood of the training data.  **\n"}
{"snippet": "from sklearn.mixture import GaussianMixture\nGM=GaussianMixture(n_components=5,random_state=999)\nGM.fit(Data3.iloc[:,1:])\n", "intent": "**b. Cluster with Gaussian Mixture. Please repeat (2)a but use loglikelihood for each record.**\n"}
{"snippet": "from sklearn.ensemble import IsolationForest\nrng = np.random.RandomState(42)\nclf = IsolationForest(max_samples=100, random_state=rng)\nclf.fit(X)\n", "intent": "(3) Choose one more anomaly detection model you prefer and report the top 5 most anomalous counties by the model you choose. \n"}
{"snippet": "from pgmpy.models import BayesianModel\nfrom pgmpy.estimators import MaximumLikelihoodEstimator, BayesianEstimator\nmodel = BayesianModel(best_model.edges())\nmodel.fit(data, estimator=MaximumLikelihoodEstimator)\nfor cpd in model.get_cpds():\n    print(\"CPD of {variable}:\".format(variable=cpd.variable))\n    print(cpd)\nprint model.get_independencies()\n", "intent": "[('Crime', 'ChildrenPoverty'), ('Smokers', 'Obese'), ('Smokers', 'PM2.5'), ('ChildrenPoverty', 'Smokers'), ('ChildrenPoverty', 'Income')] \n"}
{"snippet": "reverse_site_dict = dict((v,k) for (k,v) in site_dict.items())\nunique, counts = np.unique(train_dataset[train_dataset['target'] == 1][site_cols].values.flatten(), return_counts=True)\ntop30 = [s[0] for s in sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)[0:31]]\ntop30.remove(0)\n[reverse_site_dict[site_id] for site_id in top30]\n", "intent": "Let's figure out top 30 popular sites for our train set:\n"}
{"snippet": "get_predictions(index=0, \n                partition='train',\n                input_to_softmax=final_model(input_dim=13, \n    filters=150,\n    kernel_size=11,\n    conv_stride=3,\n    conv_border_mode='same',\n    units=100), \n                model_path='results/model_end.h5')\n", "intent": "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='validation',\n                input_to_softmax=final_model(input_dim=13, \n    filters=150,\n    kernel_size=11,\n    conv_stride=3,\n    conv_border_mode='same',\n    units=100), \n                model_path='results/model_end.h5')\n", "intent": "Use the next code cell to visualize the model's prediction for the first example in the validation dataset.\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, save_file)\n    test_accuracy = sess.run(\n        accuracy,\n        feed_dict={features: mnist.test.images, labels: mnist.test.labels})\nprint('Test Accuracy: {}'.format(test_accuracy))\n", "intent": "Let's load the weights and bias from memory, then check the test accuracy.\n"}
{"snippet": "tf.reset_default_graph()\nweights = tf.Variable(tf.truncated_normal([2, 3]))\nbias = tf.Variable(tf.truncated_normal([3]))\nsaver = tf.train.Saver()\n", "intent": "Now that the Tensor Variables are saved, let's load them back into a new model.\n"}
{"snippet": "print(\"< operations >\")\nfor op in tf.get_default_graph().get_operations():\n    print(op.name)\nprint()\nprint(\"< variables >\")\nfor var in tf.global_variables():\n    print(var.name)\n", "intent": "It is also possible to automatically print all the ops and variables by iterating through the graph.\n"}
{"snippet": "loss_per_pixel = tf.square(tf.subtract(l_out, x_pl))\nloss = tf.reduce_mean(loss_per_pixel, name=\"mean_error\")\nreg_scale = 0.0005\nregularize = tf.contrib.layers.l2_regularizer(reg_scale)\nparams = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\nreg_term = tf.reduce_sum([regularize(param) for param in params])\nloss += reg_term\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.25)\ntrain_op = optimizer.minimize(loss)\n", "intent": "Following we define the TensorFlow functions for training and evaluation.\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 4))\nnet.add(LeakyReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\nprint(net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\n", "intent": "<a id='train_model'></a>\n"}
{"snippet": "log_mod_10 = linear_model.LogisticRegression(C = 10.0, class_weight = {0:0.45, 1:0.55}) \nlog_mod_10.fit(Comps_10, y_train)\n", "intent": "Execute the code in the cell below to define and fit a logistic regression model using the 10 components of the transformed features. \n"}
{"snippet": "nr.seed(3456)\nparam_grid = {\"C\": [0.1, 1, 10, 100, 1000]}\nlogistic_mod = linear_model.LogisticRegression(class_weight = {0:0.45, 1:0.55}) \nclf = ms.GridSearchCV(estimator = logistic_mod, param_grid = param_grid, \n                      cv = inside, \n                      scoring = 'roc_auc',\n                      return_train_score = True)\nclf.fit(Features_reduced, Labels)\nclf.best_estimator_.C\n", "intent": "The code in the cell below performs the grid search for the optimal model hyperparameter. As before, the scoring metric is AUC.   \n"}
{"snippet": "log_mod_10 = linear_model.LogisticRegression(C = 100) \nlog_mod_10.fit(Comps_10, y_train)\n", "intent": "Execute the code in the cell below to define and fit a logistic regression model using the 10 components of the transformed features. \n"}
{"snippet": "nr.seed(3456)\nparam_grid = {\"C\": [0.1, 1, 10, 100, 1000]}\nlogistic_mod = linear_model.LogisticRegression(class_weight = {0:0.1, 0:0.9}) \nclf = ms.GridSearchCV(estimator = logistic_mod, param_grid = param_grid, \n                      cv = inside, \n                      scoring = sklm.make_scorer(sklm.roc_auc_score),\n                      return_train_score = True)\nclf.fit(Features_reduced, Labels)\nclf.best_estimator_.C\n", "intent": "The code in the cell below performs the grid search for the optimal model hyperparameter. As before, the scoring metric is AUC.   \n"}
{"snippet": "alg = linear_model.LogisticRegression()\nalg.fit(xtrain, ytrain)\n", "intent": "Finally, lets try machine learning!\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n", "intent": "Scikit-Learn makes this shockingly easy.\n"}
{"snippet": "lm = LinearRegression()\nlm.fit(X_train, y_train)\n", "intent": "Create a linear regression model object and fit (train it with) the training data.\n"}
{"snippet": "logmodel = LogisticRegression()\nlogmodel.fit(X_train, y_train)\n", "intent": "Train the model with the training chunks.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier()\ntree.fit(X_train, y_train)\n", "intent": "Let's start with a single, simple decision tree, without any pruning or other bells and whistles.\n"}
{"snippet": "dtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\n", "intent": "Let's begin with a basic sort of decision tree.\n"}
{"snippet": "a = np.array([[5.0, 5.0]]) \nb = np.array([[2.0], [2.0]]) \nmat_a = tf.constant(a)\nmat_b = tf.constant(b)\nmatmul = tf.matmul(mat_a, mat_b)\nwith tf.Session() as sess:\n    result = sess.run(matmul)\n    print(result)\n", "intent": "Tensorflow does matrix operation very well, as you would expect.\n"}
{"snippet": "images, labels = next(iter(trainloader))\nimg = images[7].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "import sklearn.naive_bayes, sklearn.tree\nc2 = sklearn.naive_bayes.BernoulliNB()\ncv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True, random_state=123456)\n", "intent": "Not too bad, but can we do better?\n"}
{"snippet": "c = sklearn.ensemble.BaggingClassifier(base_estimator=sklearn.linear_model.LogisticRegression())\ncv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True, random_state=123456)\n", "intent": "Test your understanding: what happens if you Bag with Logistic Regression?\n"}
{"snippet": "lr2 = LinearRegression()\nlr2.fit(pca_features_train,y_train)\nplotModelResults(lr2,pca_features_train,y_train.reshape(-1,1),plot_intervals=True)\n", "intent": "Now train linear regression on pca features and plot its forecast.\n"}
{"snippet": "c = sklearn.ensemble.AdaBoostClassifier(base_estimator=sklearn.tree.DecisionTreeClassifier(max_depth=1))\ncv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True, random_state=123456)\n", "intent": "This approach was developed with things like decision stumps in mind:\n"}
{"snippet": "c21 = sklearn.ensemble.GradientBoostingClassifier()\ncv = sklearn.cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True, random_state=123456)\n", "intent": "Gradient Boosting is beloved by some, although I have not had life-changing success with it, myself:\n"}
{"snippet": "import sklearn.naive_bayes\nclf = sklearn.naive_bayes.BernoulliNB()\nclf.fit(__________________, __________________)\n", "intent": "Now, let us try a range of prediction methods:\n"}
{"snippet": "oos_accuracy(sklearn.linear_model.LinearRegression())  \n", "intent": "Bonus challenge: Figure out a way to make it work for the linear regression predictor\n(Hard because we had to round the numeric predictions)\n"}
{"snippet": "import sklearn.naive_bayes\nclf = sklearn.naive_bayes.BernoulliNB()\nclf.fit(X_train, y_train)\n", "intent": "Now, let us try a range of prediction methods:\n"}
{"snippet": "oos_accuracy(sklearn.naive_bayes.BernoulliNB())  \n", "intent": "Figure out a way to test it:\n"}
{"snippet": "clf = sklearn.linear_model.LogisticRegression(penalty='l2', C=10)\nclf.fit(X, y)\nnp.where(clf.coef_ > .1)[1]\n", "intent": "Well, that did not do anything interesting...\nBut I will not give up yet.\n"}
{"snippet": "clf = sklearn.linear_model.LogisticRegression(penalty='l2', C=10)\nclf.fit(X, y)\nfor i in np.where(clf.coef_ > .5)[1]:\n    print vectorizer.get_feature_names()[i],\nprint\nprint\nfor i in np.where(clf.coef_ < -.5)[1]:\n    print vectorizer.get_feature_names()[i],\n", "intent": "Try it out with an arbitrary value of C:\n"}
{"snippet": "model = Sequential()\nmodel.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1,28,28), dim_ordering='th'))\nprint(model.output_shape)\nmodel.add(Convolution2D(32, 3, 3, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n", "intent": "Example implementations in Keras:\nhttps://github.com/fchollet/keras/tree/master/examples\n"}
{"snippet": "logit = LogisticRegression(n_jobs=-1, random_state=7)\nlogit.fit(X_train, y_train)\n", "intent": "**The next step is to train Logistic Regression.**\n"}
{"snippet": "from sklearn import svm\nmodel = svm.SVC(kernel='linear', C=10.) \nmodel.fit(X, y)\nplot_decision_boundary(model, X, plot_hyperplanes=True)\nscatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)\nSV = model.support_vectors_\nscatter(SV[:, 0], SV[:, 1], c=y[model.support_],\n        cmap=cm_bright, s=300, marker='o', facecolors='none', linewidths=2.);\n", "intent": "E' necessario che le classi siano separate.\n"}
{"snippet": "classifier = svm.SVC(kernel='rbf', gamma=gs.best_params_['gamma'])\nclassifier.fit(U, y)\nX_test_norm = scaler.transform(X_test)\nU_test = pca.transform(X_test_norm)\nprint classifier.score(U_test, y_test)\n", "intent": "Check the score of the best model that uses PCA in the test set.\n"}
{"snippet": "test_input = test_orig[0:1]\ntest_output = test_refc[0:1]\ntest = Variable(torch.from_numpy(test_image), requires_grad=False).cuda()\n", "intent": "We can now start feeding data from our testing set and seeing how it does. To start, let's just grab a single image.\n"}
{"snippet": "result = model(test)\nloss = loss_fn(result, test_output)\nprint(\"test Loss\", loss.data[0])\n", "intent": "Now we can feed it through the network and compare the result.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport pydotplus\nfrom IPython.display import Image\ntreeclf=DecisionTreeClassifier(max_depth=5)\ntreeclf.fit(X_train,y_train)\ndot_data=export_graphviz(treeclf, out_file=None)\ngraph = pydotplus.graph_from_dot_data(dot_data)  \nImage(graph.create_png())\n", "intent": "7) Train a Decision Tree classifier with maximum depth 5 and plot the decision tree. How does performance compare?\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_dim=1000))\nmodel.add(Dropout(.2))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=100, validation_data=(x_test,y_test), verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "V_data = [1., 2., 3.]\nV = torch.tensor(V_data)\nprint(V)\nM_data = [[1., 2., 3.], [4., 5., 6]]\nM = torch.tensor(M_data)\nprint(M)\nT_data = [[[1., 2.], [3., 4.]],\n          [[5., 6.], [7., 8.]]]\nT = torch.tensor(T_data)\nprint(T)\n", "intent": "Creating Tensors\n~~~~~~~~~~~~~~~~\nTensors can be created from Python lists with the torch.Tensor()\nfunction.\n"}
{"snippet": "import sys\nsys.path.append('/Users/dmitrys/xgboost/python-package/')\nfrom xgboost import XGBRegressor \nxgb = XGBRegressor()\nxgb.fit(X_train_scaled, y_train)\n", "intent": "Why shouldn't we try XGBoost now?\n<img src=\"https://habrastorage.org/files/754/a9a/26e/754a9a26e59648de9fe2487241a27c43.jpg\"/>\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train.values.reshape(-1, 1), y_train)\n", "intent": "In `scikit-learn` the function to use (for almost all models) is `.fit()`\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train.values, y_train)\n", "intent": "Remember the `.values.reshape()` is only required if you have **only one** predictor\n"}
{"snippet": "lr_l1_penalty = LogisticRegression(penalty=\"l1\", C=0.1)\nlr_l2_penalty = LogisticRegression(penalty=\"l2\", C=0.1)\nlr_l1_penalty.fit(X_train, y_train)\nlr_l2_penalty.fit(X_train, y_train);\n", "intent": "Based on what we learned for linear regression, how can logistic regression be regularised?\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=3)\ndt.fit(df[iris.feature_names], df[\"target\"])\n", "intent": "- what are \"maximum tree depth\" and \"minimum samples per leaf\" examples of?\n"}
{"snippet": "from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(min_samples_leaf=5)\ndt.fit(X_train, y_train)\n", "intent": "- Make sure you choose the appropriate kind of tree (classification or regression?)\n- Choose an appropriate metric to go with your type of tree\n"}
{"snippet": "rf2 = RandomForestRegressor()\nrf2.fit(X_train, y_train)\nfor z in zip(X_train.columns, rf2.feature_importances_):\n    print(z)\n", "intent": "Remember: if you used `cross_val_score` above, you need to fit another random forest!\n"}
{"snippet": "from sklearn.tree import export_graphviz\ndt = DecisionTreeRegressor(min_samples_leaf=5, max_depth=4)\ndt.fit(X_train, y_train)\nexport_graphviz(dt, \"tree.dot\", feature_names=X_train.columns)\n", "intent": "When you view your tree, what does it tell you? What features are more/less important than others?\n"}
{"snippet": "rf = RandomForestRegressor(n_estimators=11, min_samples_leaf=5)\nrf.fit(X_train, y_train)\nfor z in zip(X_train.columns, rf.feature_importances_):\n    print(z)\n", "intent": "Remember: if you used `cross_val_score` above, you need to fit another random forest!\n"}
{"snippet": "kmeans = KMeans(n_clusters=len(X))\nkmeans.fit(X)\n", "intent": "As with machine learning in general, increasing complexity will increase the \"fit\"\n"}
{"snippet": "lm = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=data).fit()\nprint lm.pvalues\n", "intent": "We could try a model with all features, and only keep features in the model if they have **small p-values**:\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(estimator=KNeighborsClassifier(),\n                    param_grid={'n_neighbors': [3,5,7,9,11,13,15]},\n                    cv=10,\n                    scoring='f1',\n                    return_train_score=True)\ngrid.fit(X_train,y_train);\n", "intent": "- look at the best score of your grid search\n- look at the best hyperparameter\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(estimator=KNeighborsClassifier(),\n                    param_grid={\"n_neighbors\": [3, 5, 7, 9, 11, 13, 15, 17]},\n                    cv=10,\n                    scoring=\"f1\")\ngrid.fit(X_train, y_train)\n", "intent": "- look at the best score of your grid search\n- look at the best hyperparameter\n"}
{"snippet": "def plot_weights(W, img_shape, layout, cmp=cm.gray):\n    nrows, ncols = layout\n    fig, axes = subplots(nrows = nrows, ncols = ncols, \n                         figsize=(1.5*ncols, 1.5*nrows))\n    axes = axes.flatten()\n    for (i, im) in enumerate(W):\n        axes[i].imshow(im.reshape(img_shape), cmp)\n", "intent": "***Summary of SVM ***\n- Most of time we prefer to use an implemented SVC version in a library, e.g., SGDClassifier/LogisticRegression in sklearn.\n"}
{"snippet": "x = T.dmatrix('x')\ns = 1 / (1 + T.exp(-x))\nlogistic = function([x], s)\nprint logistic([[0, 1], [-1, -2]])\ns2 = (1 + T.tanh(x/2)) / 2\nlogistic2 = function([x], s2)\nprint logistic2([[0, 1], [-1, -2]])\n", "intent": "***Example of Logistic Regression***\n"}
{"snippet": "from theano.ifelse import ifelse\nimport time\na, b = T.scalars('a', 'b')\nx, y = T.matrices('x', 'y')\nz_switch = T.switch(T.lt(a, b), T.mean(x), T.mean(y)) \nz_lazy = ifelse(T.lt(a, b), T.mean(x), T.mean(y)) \nf_switch = theano.function([a, b, x, y], z_switch, mode = theano.Mode(linker = 'vm'))\nf_lazyifelse = theano.function([a, b, x, y], z_lazy, mode = theano.Mode(linker = 'vm'))\nval1, val2 = 0., 1.\nbig_mat1, big_mat2 = np.ones((10000, 1000)), np.ones((10000, 1000))\n", "intent": "***Flow of Control***\n"}
{"snippet": "import scipy.sparse as sp\nfrom theano import sparse\nprint sparse.all_dtypes\nx = sparse.csc_matrix('x')\nprint x.type\nprint sparse.dense_from_sparse(x).type\nprint sparse.csr_from_dense(sparse.dense_from_sparse(x)).type\n", "intent": "***Sparse Matrices in Theano***\n"}
{"snippet": "reload(theanoml.autoencoder)\nac = theanoml.autoencoder.ContractiveAutoEncoder(n_epochs=5)\nac.fit(X)\nprint ac.transform(X).shape\n", "intent": "***TEST Contractive AutoEncoder***\n"}
{"snippet": "reload(theanoml.autoencoder)\nda = theanoml.autoencoder.DenoisingAutoEncoder()\nda.fit(X)\nprint da.transform(X).shape\n", "intent": "***Test Denoising AutoEncoder***\n"}
{"snippet": "kmeans = MiniBatchKMeans(n_clusters = 1000, batch_size=10000, random_state=0)\nkmeans.fit(all_feats)\n", "intent": "** IT SEEMS SURF FEATURES behave quite well in terms of generating similiar-range features. And so normalization is not necessary before clustering**\n"}
{"snippet": "lm = smf.ols(formula='Sales ~ TV + Radio', data=data).fit()\nlm.rsquared\n", "intent": "We could try models with different sets of features, and **compare their R-squared values**:\n"}
{"snippet": "model.fit(X, y)\n", "intent": "**4\\. Fit the model to your data (i.e. learning)**\n"}
{"snippet": "from sklearn.svm import SVC \nmodel = SVC(kernel='linear', C=1E10)\nmodel.fit(X, y)\n", "intent": "SVMs requires Hyperparameter. We'll discuss them later.\n"}
{"snippet": "clf = SVC(kernel='rbf', C=1E6)\nclf.fit(X, y)\n", "intent": "* The kernel trick is build into SVM class of Scikit-Learn.\n* We can simply change the `kernel` parameter to use the RBF kernel\n"}
{"snippet": "model.fit(Xtrain, ytrain)\n", "intent": "Then we fit the model:\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier().fit(X, y)\n", "intent": "In Scikit-Learn a `DecisionTreeClassifier` estimator is used to construct Decision Trees.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100, random_state=0)\nvisualize_classifier(model, X, y);\n", "intent": "We've built a Random Forest by hand. But Scikit-Learn comes with a `RandomForestClassifier` estimator which is easier to handle:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data) \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nprint(knn)\n", "intent": "**Step1:** Build the Classifier\n"}
{"snippet": "knn.fit(X, y)\n", "intent": "**step2:** Train the Classifier on your train dataset\n- Model is learning the relationship between X and y\n- Occurs in-place (inside the same object)\n"}
{"snippet": "from statsmodels.tsa.arima_model import ARMA\nstore1_sales_data = store1_open_data[['Sales']].astype(float)\nmodel = ARMA(store1_sales_data, (1, 0)).fit()\nmodel.summary()\n", "intent": "Remember, an ARMA model is a combination of autoregressive and moving average models.\nWe can train an AR model by turning off the MA component (q=0).\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(6, input_dim=train_features.shape[1], kernel_initializer='uniform', activation='sigmoid'))\nmodel.add(Dense(train_targets.shape[1], activation='linear'))\n", "intent": "We create the keras model here.\n"}
{"snippet": "history = model.fit(X, Y, validation_split=0.33, epochs=epochs, batch_size=100, verbose=0)\n", "intent": "We are going to use the 33% of the data as validation data:\n"}
{"snippet": "clf_1 = DecisionTreeClassifier(max_depth = 2)\nclf_1.fit(X_train,y_train)\nclf_2 = DecisionTreeClassifier(max_depth = 5)\nclf_2.fit(X_train,y_train)\n", "intent": "***\nLucius chose **Depths of 2 and 5 respectively **and then compared the results to see which is better\n"}
{"snippet": "gNB = GaussianNB()\n", "intent": "$$P(x|c_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^c}} e^{-\\frac{{(x-\\mu_k)}^2}{2\\sigma_k^2}}$$\n"}
{"snippet": "wcss = {}\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters= i, init= 'k-means++', max_iter= 300)\n    kmeans.fit(rfm_scaled)\n    wcss[i] = kmeans.inertia_\n", "intent": "With the Elbow method, we can get the optimal number of clusters.  \n"}
{"snippet": "lr.fit(X, y)\n", "intent": "Now we will fit the model on the dataset\n"}
{"snippet": "lr = LogisticRegression(penalty=\"l2\")\ngrid = GridSearchCV(estimator=lr, param_grid=param_grid, cv=3, n_jobs=-1)\nstart_time = time.time()\ngrid_result = grid.fit(X, y)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nprint(\"Execution time: \" + str((time.time() - start_time)) + \" ms\")\n", "intent": "You have defined the grid. Let's run the grid search over them and see the results with execution time.\n"}
{"snippet": "def k_nearest_vectors(k, mtx, candidate_vector):\n    cos_similarities = cosine_similarity(mtx, candidate_vector).flatten()\n    k_sorted = np.flip(np.argsort(cos_similarities)[-k:], axis = 0)\n    cos_sorted = np.flip(np.sort(cos_similarities), axis = 0)[:k]\n    return k_sorted, cos_sorted\n", "intent": "We define the following helper function to reduce the search space. \n"}
{"snippet": "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n", "intent": "Now we'll copy the files to S3 for Amazon SageMaker training to pickup.\n"}
{"snippet": "tf.reset_default_graph()\nweights = tf.Variable(tf.truncated_normal([2, 3]))\nbias = tf.Variable(tf.truncated_normal([3]))\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, save_file)\n    print('Weight:')\n    print(sess.run(weights))\n    print('Bias:')\n    print(sess.run(bias))\n", "intent": "Now that the Tensor Variables are saved, let's load them back into a new model.\n"}
{"snippet": "drugTree.fit(X_trainset,y_trainset)\n", "intent": "Next, we will fit the data with the training feature matrix <b> X_trainset </b> and training  response vector <b> y_trainset </b>\n"}
{"snippet": "dtree = DecisionTreeClassifier(criterion='gini',max_depth=None)\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "k_means.fit(X)\n", "intent": "Now let's fit the KMeans model with the feature matrix we created above, <b> X </b>\n"}
{"snippet": "k = 2\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(data)\n", "intent": "From the plot, we can see that the data can be divided into 2 main groups. Therefore, we will try using `k = 2` for our *k*-means model.\n"}
{"snippet": "k = 5\nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh\n", "intent": "Lets start the algorithm with k=4 for now:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_shape=(784,)))\nmodel.add(Activation('relu')) \nmodel.add(Dropout(0.2))   \nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10))\nmodel.add(Activation('softmax')) \n", "intent": "Build the neural-network. Here we'll do a simple 3 layer fully connected network.\n<img src=\"figure.png\" />\n"}
{"snippet": "model.fit(X_train, Y_train,\n          batch_size=128, nb_epoch=4\n          , verbose=1,\n          validation_data=(X_test, Y_test))\n", "intent": "This is the fun part: you can feed the training data loaded in earlier into this model and it will learn to classify digits\n"}
{"snippet": "pipe = make_pipeline(PreProcessing(),\n                    RandomForestClassifier())\n", "intent": "We'll create a `pipeline` to make sure that all the preprocessing steps that we do are just a single `scikit-learn estimator`.\n"}
{"snippet": "grid.fit(X_train, y_train)\n", "intent": "- Fitting the training data on the `pipeline estimator`:\n"}
{"snippet": "k = 5\nclf =  DecisionTreeClassifier(criterion='gini', max_depth=15)\nKfoldPlot(X, y, clf, k)\n", "intent": "Run your a classifier for your choice and say couple of words about the results.\n"}
{"snippet": "regr = Lasso(alpha=alpha_best)\nregr.fit(X_train, y_train)\nprint('Coefficients: \\n', regr.coef_)\n", "intent": "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? \n"}
{"snippet": "X_train = train.ix[:,0:1].copy()\nX_test = test.ix[:,0:1].copy()\ny_train = train.ix[:,6].copy()\ny_test = test.ix[:,6].copy()\nreg_tree = tree.DecisionTreeRegressor()\nreg_tree.fit(X_train,y_train)\nresult = reg_tree.feature_importances_\nprint('The test score is '+ '%.4f'% reg_tree.score(X_test,y_test))\n", "intent": "Train with all data using outdoor temperature only\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding  = tf.Variable(tf.random_uniform((vocab_size, embed_dim),-1,1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf = svm.SVC(kernel='rbf', C=C_opt, gamma=gamma)\nclf.fit(X_tr2, np.ravel(Y_tr2))\npe_tst = 1.0 - clf.score(Xtest, Ytest)\nprint(\"The test error for the selected model is {0}\".format(pe_tst))\n", "intent": "Evaluate the classifier performance using the test data, for the selected hyperparameter values.\n"}
{"snippet": "num_topics = 50\nldag = gensim.models.ldamodel.LdaModel(corpus=corpus_bow, id2word=D, num_topics=num_topics)\n", "intent": "** Exercise 3**: Create an LDA model with the 50 topics using `corpus_bow` and the dictionary, `D`.\n"}
{"snippet": "def most_relevant_projects(ldag, topicid, corpus_bow, nprojects=10):\n    print('Computing most relevant projects for Topic', topicid)\n    print('Topic composition is:')\n    print(ldag.show_topic(topicid))\n    document_topic = [el for el in ldag[corpus_bow]]\n    document_topic = gensim.matutils.corpus2dense(document_topic, ldag.num_topics).T\n    return np.argsort(document_topic[:,topicid])[::-1][:nprojects].tolist()\nproject_id = most_relevant_projects(ldag, 17, corpus_bow[:10000])\nfor idproject in project_id:\n    print(NSF_data[idproject]['title'])\n", "intent": "**Exercise 6**: Build a function that returns the most relevant projects for a given topic\n"}
{"snippet": "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer_1']), biases['hidden_layer_1'])\nlayer_1 = tf.nn.relu(layer_1)\nlayer_1 = tf.nn.dropout(layer_1, keep_prob)\nlayer_2 = tf.add(tf.matmul(layer_1, weights['hidden_layer_2']), biases['hidden_layer_2'])\nlayer_2 = tf.nn.relu(layer_2)\nlayer_2 = tf.nn.dropout(layer_2, keep_prob)\nlayer_3 = tf.add(tf.matmul(layer_2, weights['hidden_layer_3']), biases['hidden_layer_3'])\nlayer_3 = tf.nn.relu(layer_3)\nlayer_3 = tf.nn.dropout(layer_3, keep_prob)\n", "intent": "Now that we have an input layer, we can add the first hidden layer!\n"}
{"snippet": "model = sm.OLS(y, X)\nresults = model.fit()\nresults.summary()\n", "intent": "This means that our best fit line is:\n$$y = a + b x$$\nwhere $a = -0.363075521319$ and $b = 0.41575542$.\nNext let's use `statsmodels`.\n"}
{"snippet": "food = linear_model.LinearRegression()\n", "intent": "- Create a LinearRegression instance and use `fit()` function to fit the data.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(250, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(150, activation='relu'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=600)\n", "intent": "Now its time to train our model!\n**Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.**\n"}
{"snippet": "model.fit(colleges.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "kmodel.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlmodel = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "y_tempe_train = df_tempe_since_1960['5 year mean'].values\nlr_tempe.fit(X_tempe_train, y_tempe_train)\n", "intent": "Linear Regression model is trained with features matrix (years vs indicators) and 5-year-mean global temperature anomaly vectors.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train, y_train)\n", "intent": "Now let's compare the decision tree model to a random forest.\n"}
{"snippet": "km = KMeans(2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "X = df.drop('Private', axis=1)\nkm.fit(X, df['Private'])\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "model.fit(x_train, y_train, epochs=2, batch_size=1500, verbose=0);\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "knn.fit(X, y)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "lm.fit(X, y)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "gs = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)\ngs.fit(X, y)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "line_reg.fit(train_data[['X']], train_data['Y'])\nnp.hstack([line_reg.coef_, line_reg.intercept_]) \n", "intent": "We can then fit the model in one line (this solves the normal equations.\n"}
{"snippet": "from sklearn import neighbors\nfrom sklearn import preprocessing\nX_scaled = preprocessing.scale(X) \nclf1 = neighbors.KNeighborsRegressor(10)\ntrain_X = X_scaled[:half]\ntest_X = X_scaled[half:]\nclf1.fit(train_X,train_Y)\n", "intent": "** *k*-Nearest Neighbor (KNN) Regression **\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nclf2 = RandomForestRegressor(n_estimators=100, \n                            criterion='mse', max_depth=None, \n                            min_samples_split=2, min_samples_leaf=1, \n                            max_features='auto', max_leaf_nodes=None, \n                            bootstrap=True, oob_score=False, n_jobs=1, \n                            random_state=None, verbose=0, warm_start=False)\nclf2.fit(train_X,train_Y)\n", "intent": "Pretty good intro\nhttp://blog.yhathq.com/posts/random-forests-in-python.html\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=200,oob_score=True)\nclf.fit(X,Y)\n", "intent": "Let's look at random forest\n"}
{"snippet": "def apply_threshold(heatmap, threshold):\n    heatmap[heatmap <= threshold] = 0\n    return heatmap\n", "intent": "This method is adopted from Udacity lesson \"Multiple Detections and False Positives\"\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB().fit(X_train_sc,y_train)\n", "intent": "Let's train our classifier by calling its `fit()` method:\n"}
{"snippet": "model = RandomForestClassifier(n_estimators=100)\n", "intent": "Try a random forest model by running the cell below. \n"}
{"snippet": "linreg.fit(sd_like.values, reading)\nfor coef, var in zip(linreg.coef_, sd_like.columns):\n    print(var, coef)\n", "intent": "- What is the mean cross val score?\n"}
{"snippet": "params = {'penalty':['l1', 'l2'], 'C': [1, 10], 'max_iter': [50, 100]}\nlr = LogisticRegression(fit_intercept = True, random_state = seed)\nclf = GridSearchCV(lr, params, scoring = 'neg_log_loss')\nresult = clf.fit(X_train, Y_train)\n", "intent": "Use GridSearch to find the best model\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform( (vocab_size, embed_dim), -1, 1 ))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "class FastText(nn.Module):\n    def __init__(self, vocab_size, emb_dim):\n        super(FastText, self).__init__()\n        self.dummy_layer = nn.Linear(5,1)\n    def forward(self, data, length):\n        return nn.functional.sigmoid(self.dummy_layer(data.float()).view(-1))\nmodel = FastText(vocab_size, emb_dim)\n", "intent": "Please refers to https://arxiv.org/abs/1607.01759 for Fast Text model (Joulin et al.)\n"}
{"snippet": "for epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        if (i+1) % 100 == 0:\n            print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n                   % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n", "intent": "Define the training procedure:\n"}
{"snippet": "lm = LinearRegression()\nlm.fit(feature, outcome)\n", "intent": "- Second, fit the linear regression model.\n"}
{"snippet": "lm.fit(Employee['Salary'].iloc[complete], outcome)\n", "intent": "Passing wrong type to the `lm.fit` doesn't work. For example, if we pass a `Series` as features:\n"}
{"snippet": "logit = linear_model.LogisticRegression(C=1e4)\n", "intent": "Build a logistic regression model **`logit`** with the data.\n- What's the score?\n"}
{"snippet": "from sklearn import naive_bayes\ngnb = naive_bayes.GaussianNB()\n", "intent": "<p><a name=\"gnb-sklearn\"></a></p>\n**Exercise**\nWe will work on the iris dat. Fit a Gaussian Naive Bayes and print out the accuracy:\n"}
{"snippet": "model = SVC()\n", "intent": "Try a Support Vector Machines model by running the cell below. \n"}
{"snippet": "model1=RandomForestRegressor()\n", "intent": "Random Forest Model\n---\n"}
{"snippet": "Xtrain_sm = sm.add_constant(Xtrain) \nlogreg_sm = sm.GLM(ytrain, Xtrain_sm, family=sm.families.Binomial()).fit()\nsummary_string = logreg_sm.summary().as_csv()\nlogreg_sm.summary()\n", "intent": "**Statsmodels logistic regression**\n"}
{"snippet": "svc = SVC(probability=True)\n", "intent": "**Support vector machine model (default parameters)**\n"}
{"snippet": "tree = DecisionTreeClassifier()\n", "intent": "**Decision tree and random forest classifiers**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\ntfidf_log_models = []\nfor i, ngram_model in enumerate(tfidf_learning_inputs): \n    lr = LogisticRegression(solver = 'saga', multi_class='multinomial', max_iter = 200)\n    lr.fit(ngram_model[0], ngram_model[2])\n    print('{}-gram TF-IDF:\\t Train Accuracy {}'.format(i+1,lr.score(ngram_model[0], ngram_model[2])))\n    print('{}-gram TF-IDF:\\t Test Accuracy {}'.format(i+1,lr.score(ngram_model[1], ngram_model[3])))\n    tfidf_log_models.append(lr)\n", "intent": "Basic logistic regression with one-versus-rest (OVR) multiclass scheme. \n"}
{"snippet": "lm_nointercept = LinearRegression(fit_intercept=False)\nlm_nointercept.fit(X,bos.PRICE)\n", "intent": "To not fit the intercept, we create a linear regression model with an additional argument: \n"}
{"snippet": "knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance') \nknn.fit(iris.data[:,2:], iris.target)\nprint knn.score(iris.data[:,2:], iris.target)\n", "intent": "Do we see a change in performance when using the distance weight?\n"}
{"snippet": "scaler.fit(data.drop(columns=['Class']))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\ndnn_classifier = learn.DNNClassifier(hidden_units=[10,20,10], n_classes=2, feature_columns=feature_columns)\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "model = GradientBoostingClassifier()\n", "intent": "Try a Gradient Boosting Classifier model by running the cell below. \n"}
{"snippet": "kmeans.fit(data.drop(columns=['Unnamed: 0', 'Private']))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "nb = MultinomialNB()\n", "intent": "Time to train a model!\n** Import MultinomialNB and create an instance of the estimator and call is nb **\n"}
{"snippet": "svc_model = SVC()\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "X_sparse_toy.todense()\n", "intent": "**Number of columns in `X_sparse_toy` should be 11, because in the toy example 3 users visited 11 unique websites.**\n"}
{"snippet": "model.fit(X_train, y_train,\n          validation_data=(X_val, y_val), epochs=20);\n", "intent": "Keras models follow __Scikit-learn__'s interface of fit/predict with some notable extensions. Let's take a tour.\n"}
{"snippet": "class ReLU(Layer):\n    def __init__(self):\n    def backward(self, input, grad_output):\n", "intent": "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\n"}
{"snippet": "weights = tf.Variable(tf.random_normal(shape=(X.shape[1],1), stddev=0.01, dtype=tf.float32))\nb = tf.Variable(0.0, dtype=tf.float32)\ns.run(tf.global_variables_initializer())\n", "intent": "__Your code goes here.__ For the training and testing scaffolding to work, please stick to the names in comments.\n"}
{"snippet": "autoencoder.fit(x=X_train,y=X_train,epochs=32, verbose=2,\n                validation_data=[X_test,X_test])\n", "intent": "Training may take some 20 minutes.\n"}
{"snippet": "autoencoder.fit(x=X_train,y=X_train,epochs=32,\n                validation_data=[X_test,X_test])\n", "intent": "Training may take some 20 minutes.\n"}
{"snippet": "model = KNeighborsClassifier(n_neighbors = 3)\n", "intent": "Try a k-nearest neighbors model by running the cell below. \n"}
{"snippet": "history = model.fit(X_train, y_train, batch_size = 50, validation_split = 0.3, epochs = 100, verbose = 1)\n", "intent": "- Training the model with training data provided\n"}
{"snippet": "model.add(Conv2D(32, (3, 3), input_shape=(50,65,3),activation='relu'))\nmodel.add(Conv2D(32,(3,3),activation='relu'))\n", "intent": "Declare 2 convolutional layers for the model\n"}
{"snippet": "a = mx.sym.Variable('a', shape=(1,))\nb_pl = mx.sym.Variable(\"b_pl\", shape=[0])\nc = ???\nd = ???\ne = ???(???, name='sum')\nf = ???(???, name='mean')\ng = ???\nmx.viz.plot_network(g)\n", "intent": "<img src=\"figs/fig3_new.png\",width=200>\n"}
{"snippet": "a = mx.sym.Variable('a', shape=(1,))\nb_pl = mx.sym.Variable(\"b_pl\", shape=[0])\nc = a * b_pl\nd = a + b_pl\ne = mx.sym.sum(c, name='sum')\nf = mx.sym.mean(d, name='mean')\ng = e - f\nmx.viz.plot_network(g)\n", "intent": "<img src=\"figs/fig3_new.png\",width=200>\n"}
{"snippet": "W = tf.Variable([.3], tf.float32)\nb = tf.Variable([-.3], tf.float32)\nx = tf.placeholder(tf.float32)\nlinear_model = W * x + b\n", "intent": "Has Variables, which Can be Updated Within the Graph \n"}
{"snippet": "fit_1 = fit = sm.GLS(df.wage, X1).fit()\nfit_2 = fit = sm.GLS(df.wage, X2).fit()\nfit_3 = fit = sm.GLS(df.wage, X3).fit()\nfit_4 = fit = sm.GLS(df.wage, X4).fit()\nfit_5 = fit = sm.GLS(df.wage, X5).fit()\nsm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1)\n", "intent": "Selecting a suitable degree for the polynomial of age.\n"}
{"snippet": "explainerAnchor.fit(dataset.train, dataset.labels_train, dataset.validation, dataset.labels_validation)\n", "intent": "The fit function inside anchor facilitate a discretization process if needed for ordinal features (in this dataset we use only categorical features)\n"}
{"snippet": "from xgboost import XGBRegressor\nmy_model = XGBRegressor()\nmy_model.fit(train_X, train_y, verbose=False)\n", "intent": "We build and fit a model just as we would in scikit-learn.\n"}
{"snippet": "from sklearn.svm import SVC \nclf = SVC(kernel='linear')\nclf.fit(X, y)\n", "intent": "**Let's Try**\nFit the model:\n"}
{"snippet": "model = GaussianNB()\n", "intent": "Try a Gaussian Naive Bayes model by running the cell below. \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, kernel_initializer='uniform',activation = 'relu', input_shape=x_train[0].shape))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nfrom keras import optimizers\nsgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nadadelta = keras.optimizers.Adadelta(lr=0.1, rho=0.95, decay=0.0)\nmodel.compile(loss='categorical_crossentropy', optimizer=adadelta,metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.summary()\nmodel.fit(x_train, y_train, epochs=600, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = Sequential()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "start = time.time()\nmodel.fit(x_train,y_train, batch_size=32, epochs=20,verbose=2)\nend = time.time()\nprint(\"time to train\",end - start)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, alpha=\"auto\")\nmodel\n", "intent": "Teraz wytrenujmy sam model LDA.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nfeature_cols = ['al']\nX = glass.loc[:, feature_cols]\ny = glass.loc[:, 'ri']\nlinreg = LinearRegression()\nlinreg.fit(X, y)\n", "intent": "- Instantiate and fit a linear regression model predicting `ri` from `al` (and an intercept).\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=64,\n          epochs=20,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, inputs_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "import tensorflow as tf\nx = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n", "intent": "This is the tensorflow way:\n"}
{"snippet": "model = LogisticRegression()\n", "intent": "Try a Logistic Regression model by running the cell below. \n"}
{"snippet": "W_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n", "intent": "$5\\times 5$ windows and $32$ features each\n"}
{"snippet": "batch_size=32\nEx1_history = Linear_model.fit(train_data,train_labels, batch_size=batch_size, epochs=5,\n                               validation_data =(valid_data,valid_labels))\n", "intent": "Training 10 epochs.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\n", "intent": "First Step, find the coefficients of your regression line\n"}
{"snippet": "for i in [X2,X4]:\n    lm1 = smf.ols(formula='y ~ i', data=BostonData).fit()\n    print(lm1.summary())\n    print \"\\n \\n ------------------------------------------ \\n \\n\"\n", "intent": "Answer: If the goal is interpretability, use either model 2 or 4. If all variables are significant, use model 4\n"}
{"snippet": "lm = LogisticRegression()\nlm.fit(X1,y)\n", "intent": "Run your regression line on X1 and interpret your MOMMY AND DADMY coefficients.\n"}
{"snippet": "def PredictThreshold(Predictprob,Threshhold):\n    y_predict = 0\n    if (Predictprob >= Threshhold):\n        y_predict = 1\n    return y_predict\n", "intent": "Answer: Lower the threshold for predicting Myopia. Under 2% would mean that <= 1 false negative exists in the \n"}
{"snippet": "treereg = DecisionTreeClassifier(max_depth=1,min_samples_leaf=5)\ntreereg.fit(X,y)\n", "intent": "Depth of 1 appears to be ideal for this tree\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nRF = RandomForestRegressor(n_estimators = 10000, \n                           max_features = 4,     \n                           min_samples_leaf = 5, \n                           oob_score = True)      \nRF.fit(X,y)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n"}
{"snippet": "rfc = RandomForestClassifier(n_estimators=1000)\n", "intent": "Now its time to train our model!\n**Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.**\n"}
{"snippet": "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n", "intent": "- **Kernel Ridge Regression** :\n"}
{"snippet": "model = SVC()\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "grid = GridSearchCV(estimator=SVC(), param_grid=param_grid, verbose=3, refit=True)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nX = mammals.loc[:, ['body']]\npf = PolynomialFeatures(degree=3, include_bias=False) \npf.fit(X)\nX = pf.transform(X)\n", "intent": "**sklearn has a \"transformer\" that generates polynomial terms**\n"}
{"snippet": "clf = tree.DecisionTreeClassifier(max_depth=3)\nclf.fit(X,y)\n", "intent": "* Now you get to build your decision tree classifier! First create such a model with `max_depth=3` and then fit it your data:\n"}
{"snippet": "images, labels = next(iter(trainloader))\nimg = images[63].view(1, 784)\nwith torch.no_grad():\n    logits = model.forward(img)\nprint(logits)\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n", "intent": "With the network trained, we can check out it's predictions.\n"}
{"snippet": "rf2 = RandomForestClassifier(n_estimators=500, \n                            criterion='entropy', \n                            max_features=1, \n                            max_depth=10, \n                            n_jobs=2)\nrf2.fit(X_train, y_train)\nfor i, column in enumerate(credit_clean.drop('Approve', axis = 1)):\n    print(column, rf2.feature_importances_[i])\n", "intent": "Compare the feature importances as estimated with the decision tree and random forest classifiers.\n"}
{"snippet": "lsvc = LinearSVC();\nlsvc.fit(X_train, y_train)\nprint('Train score: {:.3f}'.format(lsvc.score(X_train, y_train)))\nprint('Test score: {:.3f}'.format(lsvc.score(X_test, y_test)))\n", "intent": "Train a [`LinearSVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n"}
{"snippet": "rbf_svc = SVC(kernel=\"rbf\")\nrbf_svc.fit(X_train, y_train)\nprint('Train score: {:.3f}'.format(rbf_svc.score(X_train, y_train)))\nprint('Test score: {:.3f}'.format(rbf_svc.score(X_test, y_test)))\npoly_svc = SVC(kernel=\"poly\", degree=2)\npoly_svc.fit(X_train, y_train)\nprint('Train score: {:.3f}'.format(poly_svc.score(X_train, y_train)))\nprint('Test score: {:.3f}'.format(poly_svc.score(X_test, y_test)))\n", "intent": "By using the [`SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nC_vals = [0.0001, 0.001, 0.01, 0.1, .15, .25, .275, .33, 0.5, .66, 0.75, 1.0, 2.5, 5.0, 10.0, 100.0, 1000.0]\npenalties = ['l1','l2']\ngs = GridSearchCV(model, {'penalty': penalties, 'C': C_vals}, verbose=False, cv=15)\ngs.fit(X, y)\n", "intent": "For now, focus on giving Gridsearch different parameters and different penalities, where `C_vals =` something and `penalties = ` something\n"}
{"snippet": "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n", "intent": "- **Gradient Boosting Regression** :\nWith **huber**  loss that makes it robust to outliers\n"}
{"snippet": "from sklearn.cluster import DBSCAN\ndbscn = DBSCAN(eps = 3, min_samples = 3).fit(X)\n", "intent": "Remember to pass an epsilon and min_points of your choice.\n"}
{"snippet": "def relu(input):\n", "intent": "The Rectified Linear Activation Function\n"}
{"snippet": "best_param = param_range[np.argmin(test_errors)]\nprint 'our best cross-validated choice of parameter was: ' + param_name + ' = ' + str(best_param)\nclf = KernelRidge(kernel = 'poly',degree = best_param,alpha = 0)\ndata_x = data[:,0]\ndata_y = data[:,1]\nclf.fit(data_x[:, np.newaxis], data_y)  \nutils.plot_data(data,true_func)\nutils.plot_approx(clf,data)\n", "intent": "Now that we have computed the training and testing errors, lets choose the best one and then fit the corresponding regression model to our dataset.\n"}
{"snippet": "best_param = param_range[np.argmin(test_errors)]\nprint 'our best cross-validated choice of parameter was: ' + param_name + ' = ' + str(best_param)\nclf = MLPRegressor(solver = 'lbgfs',hidden_layer_sizes = best_param)\ndata_x = data[:,0]\ndata_y = data[:,1]\nclf.fit(data_x[:, np.newaxis], data_y)  \nutils.plot_data(data,true_func)\nutils.plot_approx(clf,data)\n", "intent": "Now lets fit a neural network model to the dataset with minimial testing error.\n"}
{"snippet": "best_param = param_range[np.argmin(test_errors)]\nprint 'our best cross-validated choice of parameter was: ' + param_name + ' = ' + str(best_param)\nclf = GradientBoostingRegressor(n_estimators = best_param, max_depth = 2)\ndata_x = data[:,0]\ndata_y = data[:,1]\nclf.fit(data_x[:, np.newaxis], data_y)  \nutils.plot_data(data,true_func)\nutils.plot_approx(clf,data)\n", "intent": "Now lets fit a gradient boosting model to the dataset with minimial testing error.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embedded_input = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.cluster import KMeans\nk = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(X_scaled)\n", "intent": "Cluster the Data to our our target groups.\n"}
{"snippet": "dbscn = DBSCAN(eps = 3, min_samples = 3).fit(X)  \n", "intent": "Remember to pass an epsilon and min_points of your choice.\n"}
{"snippet": "from statsmodels.tsa.arima_model import ARMA\nmodel = ARMA(store1_diff, (1, 0)).fit()\nmodel.summary()\n", "intent": "- Create an AR(1) model on the training data and compute the mean absolute error of the predictions. How effective is this model?\n"}
{"snippet": "rf_lv2=RandomForestClassifier(n_estimators=200, n_jobs=6, min_samples_split=5, max_depth=7,\n                          criterion='gini', random_state=0)\nrf_lv2_outcomes = cross_validate_sklearn(rf_lv2, lv1_train_df, y_train ,lv1_test_df, kf, \n                                            scale=True, verbose=True)\nrf_lv2_cv=rf_lv2_outcomes[0]\nrf_lv2_train_pred=rf_lv2_outcomes[1]\nrf_lv2_test_pred=rf_lv2_outcomes[2]\n", "intent": "Now let's try a few more algorithms on level 2, and let's revisit random forest again.\n"}
{"snippet": "X2=df1[['SibSp','Parch', 'Fare', 'first', 'second',\n       'third', 'embarked_C', 'embarked_Q', 'Child', 'Older Person',\n       'Male', 'unknownAge', 'unknownCabin']]\nlm2 = LogisticRegression()\nlm2.fit(X2, y)\nlm2.score(X2, y)\n", "intent": "Logistic model with all variables and using grid search cross validation \n"}
{"snippet": "with pm.Model() as model:\n    pm.glm.glm('default ~ SEX + EDUCATION + MARRIAGE + AGE', data, family=pm.glm.families.Binomial())\n    trace_logistic_model = pm.sample(2000, pm.NUTS(), progressbar=True)\n", "intent": "Note, this will take some time to run! It's fitting 2000 different samples and finding best parameters over time as it does so!\n"}
{"snippet": "benchmark_raw = LinearRegression()\n", "intent": "Instantiate a new `LinearRegression` model and save it as `benchmark_raw`.\n"}
{"snippet": "type(Ridge()) == Ridge\n", "intent": "These are your benchmark results. You will refer to these for analysis during the next phase.\n"}
{"snippet": "cross_validated_ridge = RidgeCV(alphas=np.logspace(-2,4,20))\ncross_validated_lasso = LassoCV(alphas=np.logspace(-2,4,20))\ncross_validated_elasticnet = ElasticNetCV(alphas=np.logspace(-2,4,20), l1_ratio=np.linspace(0,1,20))\ncross_validated_ridge_scaled = RidgeCV(alphas=np.logspace(-2,4,20))\ncross_validated_lasso_scaled = LassoCV(alphas=np.logspace(-2,4,20))\ncross_validated_elasticnet_scaled = ElasticNetCV(alphas=np.logspace(-2,4,20), l1_ratio=np.linspace(0,1,20))\n", "intent": "Perform the cross-validation using an `np.logspace(-2,4,7)`.\n"}
{"snippet": "OUTPUT_DIMENSION = 3\nNUMBER_MIXTURES = 5\nmodel = keras.Sequential()\nmodel.add(keras.layers.LSTM(HIDDEN_UNITS, batch_input_shape=(None,SEQ_LEN,OUTPUT_DIMENSION), return_sequences=True))\nmodel.add(keras.layers.LSTM(HIDDEN_UNITS))\nmodel.add(mdn.MDN(OUTPUT_DIMENSION, NUMBER_MIXTURES))\nmodel.compile(loss=mdn.get_mixture_loss_func(OUTPUT_DIMENSION,NUMBER_MIXTURES), optimizer=keras.optimizers.Adam())\nmodel.summary()\n", "intent": "Now let's set up the model:\n"}
{"snippet": "def cross_entropy(A, X_train2, Y_train2):\n    Y_pred = softmax(X_train2@A) \n    return -np.sum(Y_train2*np.log(Y_pred))/X_train2.shape[0]\n", "intent": "but accuracy function is not differentiable because it has jumps, hence we can minimize error function instead\n....Or minimize Error\n"}
{"snippet": "lasso = Lasso(alpha=0.01, normalize=True)\nlasso.fit(X_train_red, y_train_red)\n", "intent": "$\\lambda$ must be lowered because there is less noise to learn from.\n"}
{"snippet": "kmeans = KMeans(n_clusters=6, init='k-means++')\n", "intent": "  * Maak K-Means aan door middel van SK-Learn. Kies de instellingen `n_clusters=6`, en `init='k-means++'`\n"}
{"snippet": "logit_lv2=LogisticRegression(random_state=0, C=0.5)\nlogit_lv2_outcomes = cross_validate_sklearn(logit_lv2, lv1_train_df, y_train ,lv1_test_df, kf, \n                                            scale=True, verbose=True)\nlogit_lv2_cv=logit_lv2_outcomes[0]\nlogit_lv2_train_pred=logit_lv2_outcomes[1]\nlogit_lv2_test_pred=logit_lv2_outcomes[2]\n", "intent": "Logistic Regression, take 2\n"}
{"snippet": "kmeans = KMeans(n_clusters=6, init='k-means++').fit(X)\n", "intent": "  * Maak K-Means aan door middel van SK-Learn. Kies de instellingen `n_clusters=6`, en `init='k-means++'`\n"}
{"snippet": "plot_KMeans(X, centroids, labels, N=None)\n", "intent": "  * Plot de data, centroids en labels.\n  * Wat gebeurt er als je het algoritme meerdere keren draait? Worden altijd dezelfde zes clusters gevonden?\n"}
{"snippet": "perceptron.fit(X_pairs, y_and);\n", "intent": "  * Train de perceptron op `X_pairs` met `y_and` als target.\n"}
{"snippet": "perceptron.fit(X_pairs, y_or);\n", "intent": "  * Train de perceptron op `X_pairs` met `y_or` als target.\n"}
{"snippet": "perceptron.fit(X_pairs, y_xor);\n", "intent": "  * Train de perceptron op `X_pairs` met `y_xor` als target.\n"}
{"snippet": "mlp = MLPClassifier(hidden_layer_sizes=(2,),\n                    max_iter=2000,\n                    learning_rate_init=0.8,\n                    solver='sgd', \n                    activation='logistic',\n                    momentum=0,\n                    tol=0.00000001)\n", "intent": "Hieronder wordt een Multi-Layer Perceptron gedefinieerd met twee hidden units.\n"}
{"snippet": "mlp.fit(X_pairs, y_xor);\n", "intent": "  * Train de MLP op `X_pairs` met `y_xor` als target.\n"}
{"snippet": "mlp = MLPClassifier(solver='sgd', \n                    activation='logistic',\n                    momentum=0.2,\n                    tol=0.00001)\n", "intent": "Hieronder is een MLP gedefinieerd. Enkele parameters zijn al ingesteld. \n"}
{"snippet": "hiddenunits_params = [(1),(3),(10),(20),(30), (50), (100)]\nlearningrate_params = [0.01, 0.05, 0.1, 0.5, 1.0]\nparameters = {'hidden_layer_sizes':hiddenunits_params, 'learning_rate_init':learningrate_params}\nclf = GridSearchCV(mlp, parameters, cv=3).fit(X_train, y_train)\n", "intent": "  * Voer een 5-fold cross validated grid search uit om de optimale waarden te vinden voor het aantal hidden units en de learning rate\n"}
{"snippet": "logit_lv3=LogisticRegression(random_state=0, C=0.5)\nlogit_lv3_outcomes = cross_validate_sklearn(logit_lv3, lv2_train, y_train ,lv2_test, kf, \n                                            scale=True, verbose=True)\nlogit_lv3_cv=logit_lv3_outcomes[0]\nlogit_lv3_train_pred=logit_lv3_outcomes[1]\nlogit_lv3_test_pred=logit_lv3_outcomes[2]\n", "intent": "and of course that something linear is going to be Logistic Regression\n"}
{"snippet": "perceptron.fit(X_pairs,y_or)\n", "intent": "  * Train de perceptron op `X_pairs` met `y_or` als target.\n"}
{"snippet": "perceptron.fit(X_pairs,y_xor)\n", "intent": "  * Train de perceptron op `X_pairs` met `y_xor` als target.\n"}
{"snippet": "mlp = MLPClassifier(hidden_layer_sizes=(2),\n                    max_iter=2000,\n                    learning_rate_init=0.8,\n                    solver='sgd', \n                    activation='logistic',\n                    momentum=0,\n                    tol=0.00000001)\n", "intent": "Hieronder wordt een Multi-Layer Perceptron gedefinieerd met twee hidden units.\n"}
{"snippet": "mlp.fit(X_pairs,y_xor)\n", "intent": "  * Train de MLP op `X_pairs` met `y_xor` als target.\n"}
{"snippet": "c_params = [0.01, 0.03, 0.1, 0.3, 1, 3]\nhidden_layer_params = [(100,),(50,),(10,),(1,)]\nparameters = {'hidden_layer_sizes':hidden_layer_params, 'learning_rate_init':c_params}\nclf = GridSearchCV(mlp, parameters, cv=5).fit(X_train, y_train)\n", "intent": "  * Voer een 5-fold cross validated grid search uit om de optimale waarden te vinden voor het aantal hidden units en de learning rate\n"}
{"snippet": "c_params = [0.01, 0.03, 0.1, 0.3, 1, 3]\nkernel_params = ['linear', 'rbf']\nparameters = {'kernel':kernel_params, 'C':c_params}\nclfSVM = GridSearchCV(SVC(), parameters, cv=5).fit(xtc, y)\n", "intent": "We proberen SVM en Naive Bayes als learning algoritme om de voorspelling te doen welke data bij welke wandelaar hoort.\n"}
{"snippet": "alpha_params = [0.1, 0.3, 0.5, 0.7, 1.0]\nparameters = {'alpha':alpha_params}\nclfNB = GridSearchCV(MultinomialNB(), parameters, cv=5).fit(xtc,y)\n", "intent": "Score SVM op trainingsdata is bij k=30 is 0.72. Bij k=20 was SVM score 0.68. Meer clusters geeft hogere score.\nEens kijken wat Naive Bayes doet. \n"}
{"snippet": "mlp = MLPClassifier(solver='lbfgs', \n                    activation='logistic',\n                    momentum=0.2,\n                    tol=0.00001)\n", "intent": "Naive Bayes doet het slechter dan SVM. We proberen een decision tree.\n"}
{"snippet": "tree = DecisionTreeClassifier()\n", "intent": "Ook de multilayer classifier werkt niet beter als SVM. Als laatste een decision tree.\n"}
{"snippet": "KRR = KernelRidge(alpha=0.1, coef0=20, degree=2, gamma=100.0, kernel='polynomial',kernel_params=None)\n", "intent": "- **Kernel Ridge Regression** :\n"}
{"snippet": "mlp = MLPClassifier(solver='lbfgs', \n                    activation='logistic',\n                    momentum=0.2,\n                    tol=0.00001)\n", "intent": "Naive Bayes doet het slechter dan SVM. We proberen een neuraal net.\n"}
{"snippet": "max_K = 30\ndistortions =  np.zeros(max_K)\nfor K in range(max_K):\n    kmeans = KMeans(n_clusters=K+1)\n    kmeans.fit(np.vstack(X_train_windows))\n    distortions[K] = kmeans.inertia_\n", "intent": "Eerst bepalen wat een goed aantal clusters is.\n"}
{"snippet": "km = KMeans(n_clusters=10, init='k-means++', max_iter=100, n_init=1)\n", "intent": "Do the actual clustering\n"}
{"snippet": "model = Sequential()\nmodel.add(Embedding(10000, 32))\nmodel.add(SimpleRNN(32, return_sequences=True))\nmodel.add(SimpleRNN(32, return_sequences=True))\nmodel.add(SimpleRNN(32, return_sequences=True))\nmodel.add(SimpleRNN(32)) \nmodel.summary()\n", "intent": "Stack recurrent layers\n"}
{"snippet": "model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation=tf.sigmoid))\nmodel.summary()\n", "intent": "let's train the network\n"}
{"snippet": "advifit_2 = pm.ADVI( model=nn_mixture)\nadvifit_2.fit(n=15000, obj_optimizer=pm.adam())\n", "intent": "**D2**: Sample from the posterior predictive as you did in B4 and produce a diagram like C4 and B5 for this model.\n"}
{"snippet": "def make_pred(X_set):\n    output = model2.forward(Variable(torch.from_numpy(X_set).float()))\n    return output.data.numpy().argmax(axis=1)\n", "intent": "We can wrap this machinery in a function, and pass this function to `points_plot` to predict on a grid and thus give us a boundary viz\n"}
{"snippet": "import sklearn.linear_model\nols = sklearn.linear_model.LinearRegression()\ncolumns = ['dwelling_type_Condo','dwelling_type_Multi-Family','dwelling_type_Residential']\nols.fit(sacramento_with_dummies[columns],sacramento_with_dummies.price)\nzip(columns,ols.coef_)\n", "intent": "even ignoring one column gives us sensible,comprehensive results.\n"}
{"snippet": "knn = sklearn.neighbors.KNeighborsRegressor()\nknn.fit(table[['sq__ft']],table['price'])\n", "intent": "K Neighbors Regressor\n"}
{"snippet": "GBR = GradientBoostingRegressor(n_estimators=100, learning_rate=0.05,\n                                max_depth=4, max_features=0.1,\n                                min_samples_leaf=17, min_samples_split=10, \n                                loss='huber', random_state =2017)\n", "intent": "- **Gradient Boosting Regression** :\nWith **huber**  loss that makes it robust to outliers\n"}
{"snippet": "lr.fit(titanic_temp[['Pclass']], titanic_temp.Survived)\n", "intent": "Errm, predicting based on age doesn't work.\n"}
{"snippet": "base_model = ResNet50(input_tensor=Input((224, 224, 3)), weights='imagenet', include_top=False)\nfor layers in base_model.layers:\n    layers.trainable = False\nx = GlobalAveragePooling2D()(base_model.output)\nx = Dropout(0.25)(x)\nx = Dense(1, activation='sigmoid')(x)\nmodel = Model(base_model.input, x)\n", "intent": "https://github.com/fchollet/keras/blob/master/keras/applications/resnet50.py\n"}
{"snippet": "X = bikes.loc[:, ['temp_celsius', 'atemp_celsius']]\ny = bikes.loc[:, 'num_total_users']\nlr_temp_atemp = LinearRegression()\nlr_temp_atemp.fit(X,y)\n", "intent": "**Exercise.**\n- Create another `LinearRegression` instance that is fit using `temp_celsius` and `atemp_celsius`, and print the coefficients.\n"}
{"snippet": "X = bikes.loc[:, ['atemp_celsius']]\ny = bikes.loc[:, 'num_total_users']\nlr_atemp = LinearRegression()\nlr_atemp.fit(X,y)\n", "intent": "- Create another `LinearRegression` instance that is fit using `atemp_celsius` only, and print the coefficients.\n"}
{"snippet": "feature_cols = ['temp_celsius', 'atemp_celsius']\nX = bikes.loc[:, feature_cols]\ny = bikes.loc[:, 'num_total_users']\nlr_temp_atemp = LinearRegression()\nlr_temp_atemp.fit(X, y)\nlist(zip(X.columns, lr_temp_atemp.coef_))\n", "intent": "**Exercise.**\n- Create another `LinearRegression` instance that is fit using `temp_celsius` and `atemp_celsius`, and print the coefficients.\n"}
{"snippet": "feature_cols = ['atemp_celsius']\nX = bikes[feature_cols]\ny = bikes.num_total_users\nlr_atemp = LinearRegression()\nlr_atemp.fit(X, y)\nlist(zip(X.columns, lr_atemp.coef_))\n", "intent": "- Create another `LinearRegression` instance that is fit using `atemp_celsius` only, and print the coefficients.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nX = glass.loc[:, ['al']] \ny = glass.loc[:, 'ri']\nlinreg.fit(X, y)\n", "intent": "- Instantiate and fit a linear regression model predicting `ri` from `al` (and an intercept).\n"}
{"snippet": "from sklearn.preprocessing import PolynomialFeatures\nX = mammals.loc[:, ['body']]\npf = PolynomialFeatures(degree=3, include_bias=False)\npf.fit(X)\npf.transform(X)  \n", "intent": "**sklearn has a \"transformer\" that generates polynomial terms**\n"}
{"snippet": "feature_cols = ['DIS']\nX = boston.loc[:, feature_cols]\ny = boston.loc[:, 'MEDV']\nlr_boston1 = LinearRegression()\nlr_boston1.fit(X, y)\n", "intent": "- Create a linear regression model for MEDV against DIS with no higher-order polynomial terms.\n"}
{"snippet": "reg = xgb.XGBRegressor(objective='reg:linear',n_estimators=200)\nreg.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=10,eval_metric=rmsle_metric)\n", "intent": "Now with the same data we use objective='reg:linear', which is MSE. Although objective doesn't match our metric, the results are better.\n"}
{"snippet": "X_train_dtm.todense()\n", "intent": "Q: Why is it stored as a sparse matrix?\n"}
{"snippet": "inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "intent": "Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit.\n"}
{"snippet": "model.fit(X_t, y, batch_size=128, epochs=2, validation_split=0.35)\n", "intent": "Now we're ready to fit out model! Use `validation_split` when not submitting.\n"}
{"snippet": "def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r\n", "intent": "Fit a model for one dependent at a time:\n"}
{"snippet": "clf = DecisionTreeClassifier(random_state=3, max_depth=3)\nmodel = clf.fit(X, y)\n", "intent": "Here we create a Decision Tree classifier and fit the model to the training data.\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 4))\nnet.add(ReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "y = df[\"Hired\"]\nX = df[features]\nclf = tree.DecisionTreeClassifier(random_state=1)\nclf = clf.fit(X,y)\n", "intent": "Now actually construct the decision tree:\n"}
{"snippet": "model = LogisticRegression(penalty = 'l1', C = .1) \nmodel.fit(X, y)\nexamine_coefficients(model, X)\n", "intent": "- Change the `C` parameter\n    - how do the coefficients change? (use `examine_coeffcients`)\n    - how does the model perfomance change (using AUC)\n"}
{"snippet": "gs2 = GridSearchCV(KNeighborsClassifier(),\n                   {'n_neighbors': np.arange(1,50,5),\n                    'weights': ['uniform', 'distance'],\n                    'algorithm': ['ball_tree', 'kd_tree', 'brute']},\n                   cv=5)\ngs2.fit(X_train,y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "print(\"-- Grid Parameter Search via 10-fold CV\")\nparam_grid = {\"criterion\": [\"mse\"],\n              \"max_depth\": [None, 2, 5], \n              'n_estimators': [500, 700], \n              \"min_samples_leaf\": [2, 5], \n              'min_samples_split': [2, 4] \n             }\nrfr = RandomForestRegressor()\nrfr_grid = run_gridsearch(x_localtrain[col], y_localtrain, rfr, param_grid, cv=5)\n", "intent": "** Find Parameters **\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nparam_dict = dict(n_neighbors=range(1, 31),\\\n                  weights=['uniform', 'distance'])\ngsknncv = GridSearchCV(knn, param_dict, scoring='accuracy')\ngscv_model = gsknncv.fit(X_train, y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "gsap = GridSearchCV(lr_tts, logreg_parameters,\\\n                  verbose=False, cv=15, scoring='average_precision')\ngsap.fit(X_train, y_train)\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "X_train = X[:60000]\ny_train = y[:60000]\nX_test = X[60000:]\ny_test = y[60000:]\nlogistic = LogisticRegression(penalty = 'l2', C = 1.0, solver = 'newton-cg', multi_class = 'multinomial')\nlogistic.fit(X_train, y_train)\n", "intent": "The l2 model performed the best, followed by the l1 model, and then by the model with no/very little regularization.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding=tf.Variable(tf.random_normal((vocab_size,embed_dim),-1,1))\n    embed=tf.nn.embedding_lookup(embedding,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "tree = DecisionTreeClassifier().fit(np.squeeze(Xtrain), ytrain)\n", "intent": "Now we are going to fit the decision tree classifier to the entire (un-PCA'd) dataset (not just the first 2 features)\n"}
{"snippet": "from sklearn.svm import LinearSVC\nfrom sklearn.grid_search import GridSearchCV\ngrid = GridSearchCV(LinearSVC(), {'C': [0.125, 0.25, 0.5, 1.0]})\ngrid.fit(X_train, y_train)\ngrid.best_score_\n", "intent": "Let's try the support vector machine, with a grid search over a few choices of the C parameter:\n"}
{"snippet": "gnb = GaussianNB(priors=[0.5,0.5])\ngnb.fit(data_train, target)\n", "intent": "What if we adjust the priors? Say, that there is a 50% prior likelihood of sand pixels\n"}
{"snippet": "import  tensorflow  as tf\nx = tf.constant(35, name='x')\ny = tf.Variable(x + 5, name='y')\nprint(y)\n", "intent": "a TensorFlow equivalent:\n"}
{"snippet": "weights = {\n    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n    'out': tf.Variable(tf.random_normal([n_hidden_2, N]))\n}\nbiases = {\n    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n    'out': tf.Variable(tf.random_normal([N]))\n}\n", "intent": "Set up network weights and biases\n"}
{"snippet": "from sklearn.cluster import MiniBatchKMeans\nnum_clusters = 30 \nkmeans_model = MiniBatchKMeans(n_clusters=num_clusters,\n                               init='k-means++',\n                               n_init=1,\n                               init_size=1000, batch_size=1000, verbose=0, max_iter=1000)\n", "intent": "K-means clustering obejctive is to minimize the average squared Euclidean distance of the document / description from their cluster centroids. \n"}
{"snippet": "if decim>1:\n   Zx = Zx[::decim]\n   Zy = Zy[::decim]\ngraph = load_graph(classifier_file)\nw1 = []\nZ,ind = sliding_window(result, (tile,tile,3), (tile, tile,3))\nif decim>1:\n   Z = Z[::decim]\n", "intent": "Step 3: Load graph and partition image into tiles\n"}
{"snippet": "n = len(alabs)\ncm = np.zeros((n,n))\nfor amat, pmat in zip(a.flatten(), c2.flatten()):\n    cm[amat][pmat] += 1\n", "intent": "Let's create a confusion matrix to look at class-by-class comparisons\n"}
{"snippet": "def pool(T):\n    return tf.nn.max_pool(T, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n", "intent": "The pooling layer performs max. pooling with a 2x2 filter and stride of 2\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), minval=-1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "lmi = LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False)\nlmi\n", "intent": "**Your turn:** How would you change the model to not fit an intercept term? Would you recommend not having an intercept?\n"}
{"snippet": "def conv_block(x, filters, size, stride=(2,2), mode='same', act=True):\n    x = Conv2D(filters, (size, size), strides=stride, padding=mode)(x)\n    x = BatchNormalization()(x)\n    return Activation('relu')(x) if act else x\n", "intent": "conv with mode = 'same'\n"}
{"snippet": "def up_block(x, filters, size):\n    x = keras.layers.UpSampling2D()(x)\n    x = Conv2D(filters, (size, size), padding='same')(x)\n    x = BatchNormalization()(x)\n    return Activation('relu')(x)\n", "intent": "deconv (fractionaly strided conv with stride 1/2)\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    domain = (-0.1, 0.1)\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), domain[0], domain[1]))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def fully_connected(prev_layer, num_units, is_training):\n    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n    layer = tf.layers.batch_normalization(layer, training=is_training)\n    layer = tf.nn.relu(layer)\n    return layer\n", "intent": "Modified `fully_connected` to add batch normalization to the fully connected layers it creates. \n"}
{"snippet": "import sagemaker as sage\nsess = sage.Session()\n", "intent": "The session remembers our connection parameters to Amazon SageMaker. We'll use it to perform all of our SageMaker operations.\n"}
{"snippet": "class Variable():\n    def __init__(self, initial_value=None):\n        self.value = initial_value\n        self.output_nodes = []\n        _default_graph.variables.append(self)\n", "intent": "Changeable parameter of a Graph, mainly, _weights_.\n"}
{"snippet": "class Graph():\n    def __init__(self):\n        self.operations = []\n        self.placeholders = []\n        self.variables = []\n    def set_as_default(self):\n        global _default_graph\n        _default_graph = self\n", "intent": "Global variable joining variables, placeholders and operations.\n"}
{"snippet": "drop_layer = tf.nn.dropout(full_layer, hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "outputs, states = tf.nn.dynamic_rnn(rnn_cell, X, dtype = tf.float32)\n", "intent": "** Now pass in the cells variable into tf.nn.dynamic_rnn, along with your first placeholder (X)**\n"}
{"snippet": "    x = np.random.normal(0,1, (45, 28*28)).astype('float32') \n    model = lasagne.layers.get_output(l_out, sym_x)\n    out = model.eval({sym_x:x}) \n    print(\"l_out\", out.shape)\n", "intent": "Now we can use the power of Theano to implement the train, validation, and test functions.\n"}
{"snippet": "model =  Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax' ))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy', \n              optimizer='rmsprop', \n              metrics=['accuracy'] )\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "checkpoint = ModelCheckpoint(filepath='mnist.model.best.hdf5',\n                            verbose=1, save_best_only=True)\nhist = model.fit(x_train, y_train,\n          batch_size=32, epochs=10,  \n          validation_split=0.2, callbacks=[checkpoint],\n          verbose=2, shuffle=True)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "hold_1 = tf.nn.dropout(full_1, keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "data = bank_note.drop('Class', axis = 1)\nscaler.fit(data)\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "estimator.fit(inputs=inputs)\n", "intent": "The call to fit will launch the training job and regularly report on the different performance metrics related to the training. \n"}
{"snippet": "model.fit(x_train, y_train, epochs=6, batch_size=100, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=400)\nrfc.fit(X_train, y_train)\n", "intent": "Now let's compare the decision tree model to a random forest.\n"}
{"snippet": "scalar.fit(df.drop('TARGET CLASS',axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "gs = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\ngs.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "model = LogisticRegression()\nmodel = model.fit(X,y)\nmodel.score(X,y)\n", "intent": "Using Logistic Regression on the entire dataset lets observe the accuracy\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    genrule = np.sqrt(2/vocab_size)\n    embedding = tf.Variable(tf.random_uniform(shape=(vocab_size, embed_dim),\n                                              minval=-genrule, maxval=genrule))\n    embed = tf.nn.embedding_lookup(ids=input_data,\n                                   params=embedding,\n                                   name=\"embed\")\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, epochs=100, batch_size=128)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model.fit(X_train, Y_train, epochs=2, batch_size=32)\n", "intent": "Run the following cell to train your model on 2 epochs with a batch size of 32. On a CPU it should take you around 5min per epoch. \n"}
{"snippet": "inference_model = music_inference_model(\n    LSTM_cell, densor, n_values=78, n_a=64, Ty=50)\n", "intent": "Run the cell below to define your inference model. This model is hard coded to generate 50 values.\n"}
{"snippet": "from sagemaker import Model\nalgorithm_name = \"pytorch-fairseq-serve\"\nimage = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, algorithm_name)\nmodel = Model(model_data=trained_model_location,\n              role=role,\n              image=image,\n              predictor_cls=JSONPredictor,\n             )\n", "intent": "We can now use the Model class to deploy the model artificats (the pre-trained model), and deploy it on a CPU instance. Let's use a `ml.m5.xlarge`. \n"}
{"snippet": "A = tf.placeholder(tf.float32, shape = (None, 3))\nB = A + 5\nwith tf.Session() as s:\n    B1 = B.eval(feed_dict = { A: [[1, 2, 3]] })\n    B2 = B.eval(feed_dict = { A: [[4, 5, 6], \n                                  [7, 8, 9]] })\nprint(B1)\nprint(B2)\n", "intent": "Modify code to run as mini-batch GD\n"}
{"snippet": "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\nkeep_prob = 0.5\nX_drop = dropout(X, keep_prob, is_training=is_training)\nhidden1 = fully_connected(X_drop, n_hidden1, scope='hidden1')\nhidden1_drop = dropout(hidden1, keep_prob, is_training=is_training)\nhidden2 = fully_connected(hidden1_drop, n_hidden2, scope='hidden2')\nhidden2_drop = dropout(hidden2, keep_prob, is_training=is_training)\nlogits = fully_connected(\n    hidden2_drop, n_outputs, activation_fn=None, scope='outputs')\n", "intent": "Note: if overfitting, increase the dropout rate; if underfitting, decrease the dropout rate\n"}
{"snippet": "reset_graph()\nX0 = tf.placeholder(tf.float32, [None, n_inputs])\nX1 = tf.placeholder(tf.float32, [None, n_inputs])\nbasic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\noutput_seqs, states = tf.contrib.rnn.static_rnn(\n    basic_cell, [X0, X1], dtype=tf.float32)\nY0, Y1 = output_seqs\n", "intent": "Doing the same as above, using `static_rnn()`\n"}
{"snippet": "reset_graph()\nn_steps = 20\nn_inputs = 1\nn_outputs = 1\nn_neurons = 100\nETA = 0.001\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\ny = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n", "intent": "(Somewhat more complicated, but also more efficient)\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(\n        tf.truncated_normal([vocab_size, embed_dim], stddev=0.1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "normal_01_weights = [\n    tf.Variable(tf.random_normal(layer_1_weight_shape, stddev=0.1)),\n    tf.Variable(tf.random_normal(layer_2_weight_shape, stddev=0.1)),\n    tf.Variable(tf.random_normal(layer_3_weight_shape, stddev=0.1))]\nhelper.compare_init_weights(\n    mnist,\n    'Uniform [-0.1, 0.1) vs Normal stddev 0.1',\n    [(uniform_neg01to01_weights, 'Uniform [-0.1, 0.1)'),\n     (normal_01_weights, 'Normal stddev 0.1')])\n", "intent": "Let's compare the normal distribution against the previous uniform distribution.\n"}
{"snippet": "trunc_normal_01_weights = [\n    tf.Variable(tf.truncated_normal(layer_1_weight_shape, stddev=0.1)),\n    tf.Variable(tf.truncated_normal(layer_2_weight_shape, stddev=0.1)),\n    tf.Variable(tf.truncated_normal(layer_3_weight_shape, stddev=0.1))]\nhelper.compare_init_weights(\n    mnist,\n    'Normal vs Truncated Normal',\n    [(normal_01_weights, 'Normal'),\n     (trunc_normal_01_weights, 'Truncated Normal')])\n", "intent": "Again, let's compare the previous results with the previous distribution.\n"}
{"snippet": "from sklearn import tree\ntrain_X = train[[\"Sepal.Width\",\"Sepal.Length\", \"Petal.Length\", \"Petal.Width\"]]\ntrain_Y = train[\"Species\"]\ntest_X = test[[\"Sepal.Width\",\"Sepal.Length\", \"Petal.Length\", \"Petal.Width\"]]\ntest_Y = test[\"Species\"]\ntree_classifier = tree.DecisionTreeClassifier()\ntree_classifier.fit(train_X, train_Y)\n", "intent": "1) Fit a decision tree using all the features.\n"}
{"snippet": "def init_weights(shape):\n    return tf.Variable(tf.random_normal(shape, stddev=1))\n", "intent": "let's try another segment (i.e. 26160,10080,10200) and check the prediction results ..\n"}
{"snippet": "estimator.fit(inputs=inputs)\n", "intent": "The call to fit will launch the training job and regularly report on the different performance metrics such as losses. \n"}
{"snippet": "clf = tree.DecisionTreeClassifier()\nclf = clf.fit(training_data.values,training_data.TARGET)\nclf\n", "intent": "Let's try another algorithm which is \"Decision Tree - DT\" \n"}
{"snippet": "rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\n", "intent": "- http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"}
{"snippet": "clf.fit(train_X, train_y)\n", "intent": "from sklearn.externals import joblib\njoblib.dump(clf, '/home/jli/pr_xgb.pkl', compress=1)\n"}
{"snippet": "ada = AdaBoostClassifier(n_estimators=300)\n", "intent": "Compare and contrast Decision Trees and AdaBoost\n"}
{"snippet": "grid_dt = GridSearchCV(estimator = DecisionTreeClassifier(), \n                        param_grid = param_grid_dt, cv = 5, scoring = \"accuracy\")\ngrid_dt.fit(X, y)\n", "intent": "The CV in GridSearchCV stands for cross validation which means we have to set a cv and scoring value.\n"}
{"snippet": "grid_dt = GridSearchCV(estimator = DecisionTreeClassifier(), \n                        param_grid = param_grid_dt, cv = 5, scoring = \"accuracy\")\ngrid_dt.fit(X, y)\n", "intent": "It's going to cross-validate every combination between the criterion parameters and depth parameters.\n"}
{"snippet": "nn = NearestNeighbors(n_neighbors=5, metric=\"euclidean\")\nnn.fit(cities_ss)\n", "intent": "Time to fit the data on a NearestNeighbors object\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators = 50)\nrf.fit(X, y)\n", "intent": "Fit RF model on data and visualize it\n"}
{"snippet": "km4 = KMeans(n_clusters=4)\nkm4.fit(data)\nlabels = km4.labels_\nlabels\n", "intent": "Let's try it again with four clusters\n"}
{"snippet": "kmeans = sklearn.cluster.KMeans(n_clusters=10).fit(train_set[0])\n", "intent": "Again for simplicity, let's stick with the k-means algorithm.\n"}
{"snippet": "km5 = KMeans(n_clusters=5, random_state = 10)\nkm5.fit(X)\nlabs5 = km5.labels_\ndf2[\"labs5\"] = labs5\npd.value_counts(labs5)\n", "intent": "Exercise: Analyze the data using five clusters\n"}
{"snippet": "model = Sequential()\n", "intent": "Time to design the model.\nSetting up a Keras model takes more work than your a Sklearn model.\n"}
{"snippet": "n_cols = X.shape[1]\nmodel.add(Dense(10, activation=\"relu\", input_shape=(n_cols,)))\n", "intent": "Adding an input layer to our model using the Dense function\n"}
{"snippet": "model.fit(X, y_binary)\n", "intent": "Fitting time! Call .fit() like you would a sklearn model.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape=(n_cols,)))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(2, activation=\"softmax\"))\nmodel.compile(optimizer=\"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n", "intent": "This is a very simple model, it only has one shallow layer. Let's add some more layers.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape=(n_cols,)))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(2, activation=\"sigmoid\"))\nmodel.compile(optimizer=\"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\nmodel.fit(X, y_binary, epochs=40, validation_split = 0.25)\n", "intent": "We're trained a really good model, but principles of cross validation also to deep learning. Here's how we'll evaluate the model on a testing data.\n"}
{"snippet": "model.fit(Xr, yr, epochs = 20, validation_split=.25)\n", "intent": "Let's try it again but with train test split\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape=(n_cols,)))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer=\"adam\", loss = \"mean_squared_error\")\n", "intent": "**Back to the drawing board!**\nWe need more layers!!\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape=(n_cols,)))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer=\"adam\", loss = \"mean_squared_error\")\n", "intent": "Let's visualize the performance over the epochs, but first we have to reset the model.\n"}
{"snippet": "import sagemaker as sage\nsess = sage.Session()\n", "intent": "The session remembers our connection parameters to SageMaker. We use it to perform all of our SageMaker operations.\n"}
{"snippet": "knn3 = KNeighborsClassifier(n_neighbors=3)\nknn3.fit(X,y)\nscore3 = knn3.score(X,y)\nprint (\"The model accurately labelled {:.2f} percent of the data\".format(score3*100))\n", "intent": "Train a KNN model using 3 neighbors\n"}
{"snippet": "knn5 = KNeighborsClassifier(n_neighbors=5)\nknn5.fit(X,y)\nscore5 = float(knn5.score(X,y))\nprint (\"The model accurately labelled {:.2f} percent of the data\".format(score5*100))\n", "intent": "Now with 5 neighbors\n"}
{"snippet": "knn7 = KNeighborsClassifier(n_neighbors=7)\nknn7.fit(X, y)\nknn7.score(X, y)\n", "intent": "What about 7 neighbors?\n"}
{"snippet": "knn29 = KNeighborsClassifier(n_neighbors=29)\nknn29.fit(X, y)\nknn29.score(X, y)\n", "intent": "Let's try something much higher\n"}
{"snippet": "knn5 = KNeighborsClassifier(n_neighbors=5)\nknn5.fit(X_train, y_train)\nknn5.score(X_test, y_test)\n", "intent": "Fit model with 5 neighbors on training data and test model on testing data\n"}
{"snippet": "lm = smf.ols(formula='sales ~ TV + radio + newspaper', data=data).fit()\nlm.pvalues\n", "intent": "We could try a model with all features, and only keep features in the model if they have **small p-values**:\n"}
{"snippet": "lr = LinearRegression()\nX = df.drop(\"Y\", axis = 1)\ny = df.Y\nlr.fit(X, y)\nlr.score(X,y)\n", "intent": "Let's model this data using linear regresion in sklearn\n"}
{"snippet": "lr = LinearRegression()\nX = df[[\"X1\", \"X3\", \"X4\"]]\ny = df.Y\nlr.fit(X, y)\nlr.score(X,y)\n", "intent": "Run this again but only X1, X3, X4, the three most correlated variables with Y\n"}
{"snippet": "X = df.drop(\"target\", axis=1)\ny = df.target\nlr = LogisticRegression()\nlr.fit(X,y)\nscore = lr.score(X,y)\nprint (\"The model produces an accuracy score of {:.2f} percent\".format(score*100))\n", "intent": "Train a logistic regression model on the data to predict whether or not I will like a certain song\n"}
{"snippet": "import sagemaker as sage\nfrom time import gmtime, strftime\nsess = sage.Session()\n", "intent": "The session remembers our connection parameters to SageMaker. We'll use it to perform all of our SageMaker operations.\n"}
{"snippet": "dt = DecisionTreeClassifier(max_depth=4)\ndt.fit(X, y)\n", "intent": "Bonus!!\n<br><br>\nLet's see the most important features and visualize the decision tree\n"}
{"snippet": "def iris_model(x):\n    if x.petal_length < 2.5:\n        return \"setosa\"\n    else:\n        if x.petal_width > 1.75:\n            return \"virginica\"\n        else:\n            return \"versicolor\"\n", "intent": "x.feature_name allow us to apply function to data frame as opposed to a series.\n"}
{"snippet": "grid_dt = RandomizedSearchCV(estimator = DecisionTreeClassifier(), n_iter = 15,\n                        param_distributions = param_grid_dt, cv = 5, scoring = \"accuracy\")\nt = time()\ngrid_dt.fit(X, y)\nprint (time() - t)\n", "intent": "Obviously grid search takes a long time and in some case can cause memory errors. This is where RandomizedSearchCV comes in.\n"}
{"snippet": "fraud_ds_X = fraud_ds.drop(\"Class\", axis = 1)\nfraud_ds_y = fraud_ds.Class\nlr = LogisticRegression()\nlr.fit(fraud_ds_X, fraud_ds_y)\n", "intent": "Train Logistic Regression on downsampled data and evaluate it on testing data\n"}
{"snippet": "randsearch_cv = RandomizedSearchCV(pipe_cv, n_iter = 5,\n                        param_distributions = param_grid_cv, cv = 5, scoring = \"accuracy\")\nt = time()\nrandsearch_cv.fit(X, y)\nprint time() - t\n", "intent": "Randomized Search option\n"}
{"snippet": "randsearch_cv = RandomizedSearchCV(pipe_cv, n_iter = 10,\n                        param_distributions = param_grid_cv, cv = 5, scoring = \"accuracy\")\nt = time()\nrandsearch_cv.fit(X, y)\nprint (time() - t)\n", "intent": "Countvectorizer randomized search\n"}
{"snippet": "randsearch_tf = RandomizedSearchCV(pipe_tf, n_iter = 10,\n                        param_distributions = param_grid_tf, cv = 5, scoring = \"accuracy\")\nt = time()\nrandsearch_tf.fit(X, y)\nprint (time() - t)\n", "intent": "Tfidfvectorizer randomized search\n"}
{"snippet": "km4 = KMeans(n_clusters=4)\nkm4.fit(dtm)\n", "intent": "It is standard practice to cluster with tfidf data instead of the count vectorized data\n"}
{"snippet": "km4 = KMeans(n_clusters=4)\nkm4.fit(dist)\n", "intent": "Let's try this exercise again but this time we'll cluster the cosine distances.\n"}
{"snippet": "model_file_name = \"DEMO-local-xgboost-model\"\nbt._Booster.save_model(model_file_name)\n", "intent": "Note that the model file name must satisfy the regular expression pattern: `^[a-zA-Z0-9](-*[a-zA-Z0-9])*;`. The model file also need to tar-zipped. \n"}
{"snippet": "def iris_model(x):\n", "intent": "x.feature_name allow us to apply function to data frame as opposed to a series.\n"}
{"snippet": "randsearch_cv = RandomizedSearchCV(pipe_cv, n_iter = 5,\n                        param_distributions = param_grid_cv, cv = 5, scoring = \"accuracy\")\nt = time()\nrandsearch_cv.fit(X, y)\nprint (time() - t)\n", "intent": "Randomized Search option\n"}
{"snippet": "sm = pystan.StanModel(model_code=simple_STAN)\n", "intent": "Let's ellaborate a bit more with the input/outputs.\n"}
{"snippet": "sm = pystan.StanModel(model_code=simple_vector_STAN)\n", "intent": "With one data point at a time, we won't go that far, so it's time to work with vectors...  \n"}
{"snippet": "sm = pystan.StanModel(model_code=GMM_STAN)\n", "intent": "Study the code carefully\n"}
{"snippet": "sm = pystan.StanModel(model_code=model_definition)\nfit = sm.sampling(data=data, iter=1000, chains=4, algorithm=\"NUTS\", seed=42, verbose=True)\nprint(fit)\n", "intent": "Please run the above model, with the corresponding data\n"}
{"snippet": "alpha = pystan_utils.vb_extract_variable(fit, \"alpha\", var_type=\"real\")\nprint(\"alpha:\", alpha)\nbeta = pystan_utils.vb_extract_variable(fit, \"beta\", var_type=\"vector\")\nprint(\"beta:\", beta)\n", "intent": "We can also use pystan_utils to extract the mean values of the posteriors:\n"}
{"snippet": "sm = pystan.StanModel(model_code=model_definition)\nfit = sm.vb(data=data, iter=10000, algorithm=\"meanfield\", grad_samples=10, seed=42, verbose=True)\n", "intent": "Compile the model and run inference using ADVI:\n"}
{"snippet": "sm = pystan.StanModel(model_code=model_definition)\n", "intent": "Compile STAN program:\n"}
{"snippet": "from sagemaker.amazon.amazon_estimator import get_image_uri\ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')\n", "intent": "This involves creating a SageMaker model from the model file previously uploaded to S3.\n"}
{"snippet": "model.fit(X_train, y_train,\n          validation_data=(X_val, y_val), callbacks =[es], epochs=10);\n", "intent": "Keras models follow __Scikit-learn__'s interface of fit/predict with some notable extensions. Let's take a tour.\n"}
{"snippet": "class ReLU(Layer):\n    def __init__(self):\n        return np.maximum(input, 0)\n    def backward(self, input, grad_output):\n", "intent": "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\n"}
{"snippet": "class ReLU(Layer):\n    def __init__(self):\n        return np.maximum(0,input)\n    def backward(self, input, grad_output):\n", "intent": "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\n"}
{"snippet": "model(X_train, y_train, X_val, y_val, num_of_neurons = [784,256,128,128,128,10], init = 'Xavier')\n", "intent": "**Running the models with different initialization methods on a deep (5 layers) network:**\n"}
{"snippet": "weights = tf.Variable(initial_value=np.random.randn(X.shape[1], 1)*0.01,\n                              name=\"weights\", dtype=\"float32\")\nb = tf.Variable(initial_value=0, name=\"bias\", dtype=\"float32\")\n", "intent": "__Your code goes here.__ For the training and testing scaffolding to work, please stick to the names in comments.\n"}
{"snippet": "autoencoder.fit(x=X_train,y=X_train,epochs=80,\n                validation_data=[X_test,X_test])\n", "intent": "Training may take some 20 minutes.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=32,\n          epochs=10,\n          validation_data=(x_test,y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.ensemble import RandomForestRegressor\nclr = RandomForestRegressor(min_samples_leaf=20, n_estimators=50, min_weight_fraction_leaf=0.01, min_samples_split=10)\nclr.fit(X_train, y_train)\n", "intent": "[RandomForestRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingRegressor\nest = GradientBoostingRegressor(min_samples_leaf=20, n_estimators=50, min_weight_fraction_leaf=0.01, min_samples_split=10)\nest.fit(X_train, y_train)\n", "intent": "[GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n"}
{"snippet": "bucket='<<bucket-name>>' \nprefix = 'object2vec-movie-genre-prediction'\ncontainer = get_image_uri(boto3.Session().region_name, 'object2vec')\ntrain_s3_path = \"s3://{}/{}/data/train/\".format(bucket, prefix)\nvalidation_s3_path = \"s3://{}/{}/data/validation/\".format(bucket, prefix)\ntest_s3_path = \"s3://{}/{}/data/test/\".format(bucket, prefix)\nauxiliary_s3_path = \"s3://{}/{}/data/auxiliary/\".format(bucket, prefix)\nprediction_s3_path = \"s3://{}/{}/predictions/\".format(bucket, prefix)\n", "intent": "Let us start with defining some configurations \n"}
{"snippet": "np.sum(np.isnan(X.flatten()))\n", "intent": "**All digit got almost the same counts**\n"}
{"snippet": "model.fit(x_train, y_train, epochs=25, batch_size=200, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "X = df.drop('Private', axis=1)\nkmeans.fit(X)\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "df_features = df.drop('TARGET CLASS', axis=1)\nscaler.fit(df_features)\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlog = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "grid_model = GridSearchCV(SVC(), param_grid, verbose=4)\ngrid_model.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, batch_size=16, epochs=2)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "W = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n", "intent": "Here x and y_ aren't specific values. Rather, they are each a placeholder -- a value that we'll input when we ask TensorFlow to run a computation.\n"}
{"snippet": "def weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n", "intent": "Assign random numbers of weights and biases.\n"}
{"snippet": "kmeans.fit(kmeans.record_set(train_data))\n", "intent": "Then we train the model on our training data.\n"}
{"snippet": "W_fc2 = weight_variable([100, 10])\nb_fc2 = bias_variable([10])\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n", "intent": "Finally, we add a layer, just like for the one layer softmax regression above.\n"}
{"snippet": "predictors = [x for x in train.columns if x not in [target] + IDcol]\nalg6 = RandomForestRegressor(n_estimators=400,\n                             max_depth=6, \n                             min_samples_leaf=100,\n                             n_jobs=4)\nmodelfit(alg6, train, test, predictors, target, IDcol, 'alg6.csv')\ncoef6 = pd.Series(alg6.feature_importances_, predictors).sort_values(ascending=False)\ncoef6.plot(kind='bar', title='Feature Importances')\n", "intent": "- Try making another random forest with max_depth of 6 and 400 trees. \n"}
{"snippet": "clf = OneVsRestClassifier(RandomForestClassifier(n_jobs=-1, n_estimators=100, max_depth=10, random_state=42))\n", "intent": "This model was included as a example for the activity, we integrated this to our solution as a benchmark for our own model.\n"}
{"snippet": "model.fit(Xsmall, ysmall, batch_size=100, epochs=20,verbose = 1)\n", "intent": "Now lets fit our model!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), name='embedding')\n    embed = tf.identity(tf.nn.embedding_lookup(embedding, input_data), name='embed')\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nX = np.array([[xi**2, xi, 1] for xi in x])\nlin = LinearRegression(fit_intercept=False)\nlin.fit(X, y)\nprint(lin.coef_)\n", "intent": "We looked at a chart of our data and decided to describe it with:\n* A quadratic term  \n* A linear term\n* An offset\n"}
{"snippet": "import pulp\nb = pulp.LpVariable('b')\nw1 = pulp.LpVariable('w1')\nw2 = pulp.LpVariable('w2')\n", "intent": "Let's get hacking. First make variables for the separating hyperplane.\nNote these are _unrestricted in sign_.\n"}
{"snippet": "from sklearn.cluster import KMeans\nclust = KMeans(n_clusters=3)\nclust.fit(X)\n", "intent": "Assume we know a prioi how many clusters there are.\n"}
{"snippet": "w = []\nfor j in range(len(centers)):\n    x10 = pulp.LpVariable('x_10%d' % j, lowBound=0, upBound=1)\n    x20 = pulp.LpVariable('x_20%d' % j, lowBound=0, upBound=1)\n    w.append([x10, x20])\n", "intent": "Primary decision variables = locations of cluster centers\n"}
{"snippet": "uncorr_data = UncorrelationMethod(sensitive_feature_gender, 0.0)\nuncorr_data.fit(training_data_matrix) \nnew_training_data_matrix = np.hstack([uncorr_data.new_representation(training_data_matrix[:, :-1]),\n                                     training_data_matrix[:, -1:-2:-1]])\nnew_test_data_matrix = np.hstack([uncorr_data.new_representation(test_data_matrix[:, :-1]),\n                                  test_data_matrix[:, -1:-2:-1]])\n", "intent": "At this point we are ready to apply this algorithm to our data.\n"}
{"snippet": "x = []\nfor r in range(len(routes)):\n    x.append(pulp.LpVariable('x%d' % r, cat=pulp.LpBinary))\n", "intent": "Binary variables indicate which routes to use.\n"}
{"snippet": "z = pulp.LpVariable('z', lowBound=0)\nprm += z == sum(route_costs[r] * x[r] for r in range(len(routes)))\nprm.setObjective(z)\nassert prm.solve() == pulp.LpStatusOptimal\nbest_total_cost = z.value()\nprint('total cost:', best_total_cost)\n", "intent": "Objective = minimize total distance.\n"}
{"snippet": "z2 = pulp.LpVariable('z2', lowBound=0)\nfor r in range(len(routes)):\n    prm += z2 >= route_costs[r] * x[r]\nprm.setObjective(z2)\nassert prm.solve() == pulp.LpStatusOptimal\nprint('total cost:', z.value())\nprint('max route length:', z2.value())\n", "intent": "Some of our pedicabs complain that they have to travel much farther than others. Let's fix that by minimizing the maximum trip length.\n"}
{"snippet": "model.fit( map( list, ('CAGCATCAGT', 'C', 'ATATAGAGATAAGCT', 'GCGCAAGT', 'GCATTGC', 'CACATCACGACTAATGATAAT') ) )\nprint model.log_probability( list('CAGCATCAGT') ) \nprint model.log_probability( list('C') )\nprint model.log_probability( list('CACATCACGACTAATGATAAT') )\n", "intent": "We can fit the model to sequences which we pass in, and as expected, see that these sequences subsequently have a higher likelihood.\n"}
{"snippet": "full_1_drop=tf.nn.dropout(x=normal_full_1, keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\nencoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n", "intent": "Setup embeddings (see tutorial 1)\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embeded_layer = tf.nn.embedding_lookup(embedding, input_data)\n    return embeded_layer\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "ml_model(LogisticRegression(), 'Logistic Regression')\nml_model(GaussianNB(), 'Naive Bayes')\n", "intent": "We can now call our function for the two models that we've created and compare the algorithms:\n"}
{"snippet": "from sklearn.svm import SVC\nml_model(SVC(), 'SVM')\n", "intent": "It's also really easy for us to import and run a new model, in this case a Support Vector Machines (SVM) classifier:\n"}
{"snippet": "sage_session = sagemaker.session.Session()\ns3_bucket = sage_session.default_bucket()  \ns3_output_path = 's3://{}/'.format(s3_bucket) \n", "intent": "Set up the linkage and authentication to the S3 bucket that we want to use for checkpoint and metadata.\n"}
{"snippet": "global session\nsession = tf.Session()\n", "intent": "Once the TensorFlow graph has been created, we have to create a TensorFlow session which is used to execute the graph.\n"}
{"snippet": "import tensorflow as tf\nx = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.nn.softmax(tf.matmul(x, W) + b) \ny\n", "intent": "Define a neural network\n"}
{"snippet": "happyModel.fit(x=X_train,y=Y_train,batch_size=32,epochs=10,validation_split=0.1)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "import os\nos.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\nplot_model(model, to_file='model.png')\nSVG(model_to_dot(model).create(prog='dot', format='svg'))\n", "intent": "Finally, run the code below to visualize your ResNet50. You can also download a .png picture of your model by going to \"File -> Open...-> model.png\".\n"}
{"snippet": "G = nx.Graph()\nG.add_nodes_from(A, bipartite=0)\nG.add_nodes_from(B, bipartite=1)\nfor i in A:\n    for j in B:\n        G.add_edge(i,j, weight= - abs(score_treated.loc[i].prop_1 - score_untreated.loc[j].prop_1))\n", "intent": "We now construct the vertices by mapping every element of A to every element of B\n"}
{"snippet": "_, p = train.shape\nX = tf.placeholder('float', shape=[None, p], name=\"X\")\ny = tf.placeholder('float', shape=[None, 1], name=\"y\")\nV = tf.Variable(tf.random_normal([k, p], stddev=0.01), name=\"V\")\nw0 = tf.Variable(tf.zeros([1]), name=\"w0\")\nw = tf.Variable(tf.zeros([p]), name=\"w\")\n", "intent": "First, define the placeholders and variables needed for the model.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(50, activation='sigmoid', input_shape=(1000,)))\nmodel.add(Dropout(.15))\nmodel.add(Dense(20, activation='relu', input_shape=(100,)))\nmodel.add(Dropout(.15))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "import tensorflow as tf\nsoftmax_data = [0.7, 0.2, 0.1]\none_hot_data = [1.0, 0.0, 0.0]\nsoftmax = tf.placeholder(tf.float32)\none_hot = tf.placeholder(tf.float32)\ncross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\nwith tf.Session() as sess:\n    print(sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data}))\n", "intent": "to calculate the cross-entropy loss function use the below\n"}
{"snippet": "images, labels = next(iter(trainloader))\nimg = images[0].view(1,784)\nwith torch.no_grad():\n    logits = model.forward(img)\nps = F.softmax(logits,dim=1)\n", "intent": "with the network trained, we can check out it's predictions.\n"}
{"snippet": "sage_session = sagemaker.session.Session()\ns3_bucket = sage_session.default_bucket()  \ns3_output_path = 's3://{}/'.format(s3_bucket)\nprint(\"S3 bucket path: {}\".format(s3_output_path))\n", "intent": "Set up the linkage and authentication to the S3 bucket that you want to use for checkpoint and the metadata. \n"}
{"snippet": "def conv2d(x,W,b,strides=1):\n    x = tf.nn.conv2d(x,W,strides = [1,strides,strides,1],padding =\"same\")\n    x = tf.nn.bias_add(x,b)\n    return tf.nn.relu(x)\n", "intent": "* **tf.nn.conv2d()**\n* **tf.nn.bias_add()**\n* **tf.nn.relu()**\n"}
{"snippet": "def maxpool2d(x,k=2):\n    return tf.nn.max_pool(x,ksize=[1,k,k,1],strides=[1,k,k,1],\n                         padding = \"same\")\n", "intent": "* **tf.nn.max_pool()**\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers.core import Dense,Dropout,Activation\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils\nmodel = Sequential()\nmodel.add(Dense(18,activation =\"tanh\",input_shape=(1000,)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2,activation=\"softmax\"))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=1000, batch_size=124, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size,embed_dim],-1,1))\n    embed = tf.nn.embedding_lookup(embedding,input_data)\n    print(embed.get_shape())\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn import svm\nclf = svm.SVC(C=10)\nclf.fit(X_train, y_train)\n", "intent": "Train a SVM classifier\n"}
{"snippet": "neigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X, y);\n", "intent": "What we have above are one dimensional points. Logically, a decision boundary should exist at x = 1.5. We will verify if it holds\n"}
{"snippet": "k_means = KMeans(n_clusters=3, init='random')\nk_means.fit(X)\nlabels = k_means.labels_\n", "intent": "What is the difference between calling KMeans and any classifier using sklearn?\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu',input_shape=(1000,)))    \nmodel.add(Dropout(.4)) \nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(.4))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(optimizer='adam',\n             loss='categorical_crossentropy',\n             metrics=['accuracy']) \n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "sage_session = sagemaker.session.Session()\ns3_bucket = sage_session.default_bucket()  \ns3_output_path = 's3://{}/'.format(s3_bucket) \nprint(\"S3 bucket path: {}\".format(s3_output_path))\n", "intent": "Set up the linkage and authentication to the S3 bucket that you want to use for checkpoint and the metadata. \n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embed_var = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embedding = tf.nn.embedding_lookup(embed_var, input_data)\n    return embedding\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "discriminator_probabilities = tf.nn.softmax(discriminator_samples_logits)\ngenerator_loss = - tf.log(discriminator_probabilities[:, 1])\ngenerator_loss = tf.reduce_mean(generator_loss)\n", "intent": "Consider what you have seen above to define the generator loss.\n"}
{"snippet": "discriminator_samples_logits = ...     \ndiscriminator_probabilities = tf.nn.softmax(discriminator_samples_logits)\ngenerator_loss = ...\n", "intent": "Consider what you have seen above to define the generator loss.\n"}
{"snippet": "mean = tf.Variable(tf.zeros(shape=(), dtype=tf.float32), name=\"mean\")\nstd = tf.Variable(tf.ones(shape=(), dtype=tf.float32), name=\"std\")\nmodel = tfd.Normal(mean, std)\n", "intent": "We start with a model that is initialized to be Gaussian with 0 mean and standard deviation of 1.\n"}
{"snippet": "tf.reset_default_graph()\na = tf.constant(5, name='a')\nb = tf.constant(-1, name='b')\nc = tf.add(a, b, name='c')\nshow_graph(tf.get_default_graph())\n", "intent": "To solve this issue, tensorflow has `tf.reset_default_graph()`, which clears everything from the default graph.\n"}
{"snippet": "with tf.Session() as session:\n  print('a:', session.run(a))  \n  print('[b, c]:', session.run([b, c]))\n  print(session.run({'a': a, 'c': c}))\n", "intent": "You can run any node from your graph, or a combination of them.\n"}
{"snippet": "shape = tf.shape(inputs)\nnum_total_elements = tf.reduce_prod(shape)\nwith tf.Session() as session:\n  print(session.run([shape, num_total_elements], feed_dict={\n      inputs: np.array(np.random.random((3, 2, 2)))\n  }))\n", "intent": "The **dynamic shape itself is a tensor** and may (only) be evaluated or computed with once the graph is run in a session.\n"}
{"snippet": "linear = snt.Linear(output_size=5)\nlinear\n", "intent": "Start by creating a Linear module (dense layer).\n"}
{"snippet": "pre_activations = linear(inputs_placeholder)\n", "intent": "As in tensorflow, we \"call\" the module on the tensor that we want it to compute on. This yields a tensor, the output of the calculation.\n"}
{"snippet": "from sagemaker.tensorflow.serving import Model\nmodel = Model(model_data=estimator.model_data,\n              role=role)\npredictor = model.deploy(initial_instance_count=1, instance_type=instance_type)\n", "intent": "Now let us deploy the RL policy so that we can get the optimal action, given an environment observation.\n"}
{"snippet": "show_graph()   \n", "intent": "Let's see the graph we built.\n"}
{"snippet": "show_graph()\n", "intent": "We can verify on the graph that everything is as expected. The `name_scope` and `name` instructions make the graph easier to interpret.\n"}
{"snippet": "show_graph()\n", "intent": "**It is worth noting here the effect of this call on the graph.**\n"}
{"snippet": "tf.reset_default_graph()\nmodel = MySimpleModule(num_hidden=5)\n", "intent": "We can make a particular instance of the module we defined like so:\n"}
{"snippet": "show_graph()\n", "intent": "The connection triggered the `_build()` function and we can see the graph corresponding to the model is built.\n"}
{"snippet": "test_inputs, test_labels = test_dataset.make_one_shot_iterator().get_next()\ntest_outputs = model(test_inputs)\n", "intent": "The beauty of sonnet is that we can **connect the same `model` instance to the test data tensor and it will automatically share variables**.\n"}
{"snippet": "model.fit(x_train,y_cat_train,epochs=10)\n", "intent": "**TASK 6: Train/Fit the model to the x_train set. Amount of epochs is up to you.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size , embed_dim) , stddev=0.1) )\n    embed = tf.nn.embedding_lookup(embedding , input_data )\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model.fit(X, y)\nprint(model)\n", "intent": "Next, we fit the model to our data using the fit method. \n"}
{"snippet": "job_name_prefix = 'rl-roboschool-distributed-' + roboschool_problem\naws_region = boto3.Session().region_name\n", "intent": "We define variables such as the job prefix for the training jobs *and the image path for the container (only when this is BYOC).*\n"}
{"snippet": "model.add(\n    Dense(100, activation='relu', input_dim=X_train.shape[1])\n)\n", "intent": "In the first hidden layer, we must also specify the dimension of our input layer. This will simply be the number of elements (pixels) in each image.\n"}
{"snippet": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nn_folds = 5\nparameters = {'max_features': [2,3]}\nrf = RandomForestClassifier(max_depth=4)\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)\n", "intent": "Let's see how the model performance varies with ```max_features```, which is the maximum numbre of features considered for splitting at a node.\n"}
{"snippet": "param_grid = {\n    'max_depth': [4,8,10],\n    'min_samples_leaf': range(100, 400, 200),\n    'min_samples_split': range(200, 500, 200),\n    'n_estimators': [100,200, 300], \n    'max_features': [2,4]\n}\nrf = RandomForestClassifier()\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1,verbose = 1)\n", "intent": "We can now find the optimal hyperparameters using GridSearchCV.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=10,\n                             min_samples_leaf=100, \n                             min_samples_split=200,\n                             max_features=4,\n                             n_estimators=100)\n", "intent": "**Fitting the final model with the best parameters obtained from grid search.**\n"}
{"snippet": "lm.fit(X, bos.PRICE)\nprint 'Score: %f' % lm.score(X, bos.PRICE)\nprint 'Coefficients: \\n' + str(lm.coef_)\nprint 'Intercept: %f' % lm.intercept_\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "import tensorflow as tf\ntf.reset_default_graph()\nlearning_rate = 0.001\ntraining_epochs = 10\nbatch_size = 128\ndisplay_step = 1\nn_input = 784     \nn_classes = 10    \n", "intent": "We define some global parameters\n"}
{"snippet": "n_hidden_layer = 256 \nweights = {'hidden_layer' : tf.Variable(tf.random_normal([n_input, n_hidden_layer], name = 'W_0')),\n           'out' : tf.Variable(tf.random_normal([n_hidden_layer, n_classes]), name = 'W_1')}\nbiases = {'hidden_layer' : tf.Variable(tf.random_normal([n_hidden_layer]), name = 'b_0'),\n          'out' : tf.Variable(tf.random_normal([n_classes]), name = 'b_1')}\nx = tf.placeholder('float', [None, 28,28,1], name = 'input_x')\ny = tf.placeholder('float', [None, n_classes], name = 'target_y')\nx_flat = tf.reshape(x,[-1, n_input], name = 'input_x_flat')\n", "intent": "We define some parameter/variables for our neural networks\n"}
{"snippet": "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\nlayer_1 = tf.nn.relu(layer_1)\nlogits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n", "intent": "We define our layer with ReLU as following\n"}
{"snippet": "s3_bucket = '< ENTER BUCKET NAME HERE >'\nprefix = 'Scikit-LinearLearner-pipeline-abalone-example'\nimport sagemaker\nfrom sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n", "intent": "Let's first create our Sagemaker session and role, and create a S3 prefix to use for the notebook example.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogistic = LogisticRegression()\nlogistic.fit(X, y)\n", "intent": "(c) Fit a logistic regression model to the data, using X1 and X2 as predictors.\n"}
{"snippet": "clust = KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "clust.fit(cdf.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "km.fit(colleges.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "y = df_raw['TARGET CLASS']\nscaler.fit(df_raw.drop('TARGET CLASS',axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "scalar.fit(df.drop('TARGET CLASS', axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "model1 = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "model1.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogm = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "prefix = 'Scikit-iris'\nimport sagemaker\nfrom sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n", "intent": "First, lets create our Sagemaker session and role, and create a S3 prefix to use for the notebook example.\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid=param_grid, verbose=3)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "history = model.fit(x_train, y_train, epochs=2500, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\n", "intent": "We'll start just by training a single decision tree.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ndt = DecisionTreeRegressor()\nparameter_grid = {'min_impurity_split': 2. ** np.arange(-5, 5)}\nparam_searcher = GridSearchCV(dt, parameter_grid, cv=10)\nparam_searcher.fit(train_df[predictors], train_df[target])\n", "intent": "Sklearn doesn't allow _post-pruning_, but we can try some pre-pruning parameters\n"}
{"snippet": "W_l = tf.Variable(tf.zeros([5,1], name=\"weights\"))\nb_l = tf.Variable(0., name=\"bias\")\ndef combine_inputs(X):\n    return tf.matmul(X,W_l) + b\ndef inference_l(X):\n    return tf.sigmoid(combine_inputs(X))\n", "intent": "Logistic Regression\n"}
{"snippet": "model = MultinomialNB()\nmodel.fit(train_sparse, train_label)\n", "intent": "We have used sklearn library which implements 'Naive Bayes classifier for multinomial models'\n"}
{"snippet": "from sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nDT = DecisionTreeClassifier(random_state=0)\n", "intent": "Simple Decision Tree Classifier\n"}
{"snippet": "from sklearn.naive_bayes import BernoulliNB\nBNV = BernoulliNB()\nBNV.fit(X_train[features], y_train)\n", "intent": "Bernoulli Naive Bayes\n"}
{"snippet": "for i in [1,2,3,4]:\n    svcs = SVC(C=i)\n    svcs.fit(X_train[features], y_train)\n    print 'Accuracy for SVC with C=', str(i)\n    print svcs.score(X_test[features],y_test)\n", "intent": "Gains in Accuracy with greater C\n"}
{"snippet": "import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.session import Session\nsagemaker_session = sagemaker.Session()\nregion = sagemaker_session.boto_session.region_name\nrole = get_execution_role()\n", "intent": "First, we'll just set up a few things needed for this example\n"}
{"snippet": "ll = linear_model.LassoLars(alpha=.1)\nll.fit(X, y)\n", "intent": "Lasso Lars Regression\n"}
{"snippet": "br = linear_model.BayesianRidge()\n", "intent": "Bayesian Ridge Regression\n"}
{"snippet": "sv_reg = SVR()\n", "intent": "Using Nested GridSearch CV with more regressors\n"}
{"snippet": "neigh_reg = KNeighborsRegressor()\n", "intent": "K Neighbors Regressors\n"}
{"snippet": "linear_regression = linear_model.LinearRegression(normalize=False, fit_intercept=True)\n", "intent": "Now Rerunning the Linear Regression after the label encoding and binarization have been made. \n"}
{"snippet": "def r2_gbd_est_two(X,y, X_new, y_new, loss_, _rate):\n    gbd = GradientBoostingRegressor(loss = loss_, learning_rate = _rate)\n    gbd.fit(X,y)\n    return gbd.score(X_new, y_new)\ndef r2_gbd_est_two_huber(X,y, X_new, y_new):\n    return r2_gbd_est_two(X,y, X_new, y_new, 'huber', .1)\n", "intent": "Evaluating for impact of each feature\n"}
{"snippet": "from yellowbrick.features import Rank2D\nvisualizer = Rank2D(features=X_num[features].columns, algorithm='pearson')\nvisualizer.fit(X_num[features], y)                \nvisualizer.transform(X_num[features])             \nvisualizer.poof()  \n", "intent": "Feature Importance with Pearson Correlation\n"}
{"snippet": "from yellowbrick.regressor import ManualAlphaSelection\nfrom sklearn.linear_model import Ridge\nmodel = ManualAlphaSelection(Ridge(), cv=12, \n                             scoring='neg_mean_squared_error')\nmodel.fit(X_train, y_train)\nmodel.poof()\n", "intent": "ALpha Selection with Ridge CV but throws weird error \n"}
{"snippet": "import yellowbrick as yb\nfrom yellowbrick.features.pcoords import ParallelCoordinates\nfrom yellowbrick.base import VisualizerGrid\nvisualizers = [\n               ResidualsPlot(ridge),\n               ResidualsPlot(ridge),\n              ]\nvg = VisualizerGrid(visualizers, ncols=2)\nvg.fit(X,y)\nvg.poof()\n", "intent": "Grid Visualizations - Not working\n"}
{"snippet": "sagemaker.Session().delete_endpoint(predictor.endpoint)\n", "intent": "To avoid incurring charges to your AWS account for the resources used in this tutorial you need to delete the SageMaker Endpoint:\n"}
{"snippet": "chosen_model_ridge = linear_model.Ridge(alpha=0.1)\n", "intent": "We still have a seemingly acceptable RMSE with an alpha of 10^-1, let's test it and see how it performs\n"}
{"snippet": "gs.fit(X_train, y_train)\nprint(\"model score: %.3f\" % gs.score(X_test, y_test))\n", "intent": "The pipeline transformations are fit to the training set and applied to the test set to obtain a score and thus preventing data leakage.\n"}
{"snippet": "clf = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n", "intent": "Same tree with Max Depth parameter - higher depth, higher complexity\n"}
{"snippet": "model = load_model('bitcoin_lstm_v0.h5')\n", "intent": "Let's start by loading our previously trained model. \n"}
{"snippet": "model_v0 = load_model('bitcoin_lstm_v0.h5')\n", "intent": "For reference, let's load data for `v0` of our model and train it alongside future modifications.\n"}
{"snippet": "models = [model_v0, model_v1, model_v2, model_v3]\nfor i, M in enumerate(models):\n    predicted_days = evaluate_model(M, kind='other')\n    plot_weekly_predictions(predicted_days, 'model_v{}'.format(i), display_plot=False)\n", "intent": "Finally, let's evaluate each one the models trained in this activity in sequence. \n"}
{"snippet": "model.fit(x, y)\n", "intent": "Let us train this on the very simple data set from above.\n"}
{"snippet": "m = svm.SVR(kernel='rbf', C=1, gamma=0.1)\nm.fit(xtrain, ytrain)\n", "intent": "Now, train again the SVR only on the training data. I have chosen an rbf kernel now, just to see how that performs.\n"}
{"snippet": "knn_new = KNeighborsClassifier(n_neighbors = 3)\n", "intent": "Now as a function of m (training instances)\n"}
{"snippet": "import boto3\nregion = boto3.Session().region_name\ntrain_data = 's3://sagemaker-sample-data-{}/tensorflow/pipe-mode/train'.format(region)\neval_data = 's3://sagemaker-sample-data-{}/tensorflow/pipe-mode/eval'.format(region)\ntensorflow.fit({'train':train_data, 'eval':eval_data})\n", "intent": "After we've created the SageMaker Python SDK TensorFlow object, we can call fit to launch TensorFlow training:\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors = 15)\nknn.fit(np.array(X_train), np.ravel(y_train))\nknn.score(np.array(X_test), y_test)\n", "intent": "Now we redo this with a higher K:\n"}
{"snippet": "regressor_reg = Ridge().fit(X_train, y_train)\n", "intent": "Frequently, polynomial features are used in conjunction with regularization to avoid overfitting\n"}
{"snippet": "clf_svm = svm.SVC()\n", "intent": "Let's try GridSearchCV with our SVM Classifier\n"}
{"snippet": "history = model.fit(X_train_centered, y_train_onehot, batch_size=100, epochs=50, verbose=1, validation_split=0.1)\n", "intent": "The categorical cross entropy is the cost function for logistic regression generalized for multiclass problems\n"}
{"snippet": "history = model.fit(X_train_centered, y_train_onehot, batch_size=50, epochs=60, verbose=1, validation_split=0.1)\n", "intent": "The model is converging to a training loss of 0.07 and a validation loss of 0.11. Let's see if we can do better with different hyperparameters\n"}
{"snippet": "p = tf.Graph()\n", "intent": "Next, let's use Tensorflow methods to calculate sum and mean of arrays\n"}
{"snippet": "ridge = Ridge(alpha = 0.01)\n", "intent": "Higher alpha implies higher degree of regularization and lower model complexity i.e. sensitivity to overfitting\n"}
{"snippet": "ridge = Ridge(alpha = 10)\nridge.fit(X_train_scaled, y_train)\n", "intent": "Now let's rerun with a higher alpha:\n"}
{"snippet": "estimator = SGDClassifier()\n", "intent": "Nested CV with Time Series Split\n"}
{"snippet": "sagemaker.Session().delete_endpoint(predictor.endpoint)\n", "intent": "To avoid incurring charges to your AWS account for the resources used in this tutorial you need to delete the **SageMaker Endpoint:**\n"}
{"snippet": "best_ada = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n          learning_rate=0.5, n_estimators=50, random_state=1)\n", "intent": "Recreating the three classifiers with their best hyperparameters\n"}
{"snippet": "clf = LogisticRegression(C=1, max_iter=100, class_weight = 'balanced')\n", "intent": "Tune According to Campaign ROI as defined by differenced between avg loan and deposit interest rates and fixed/var campaign costs\n"}
{"snippet": "clf = LogisticRegressionCV(cv=5, Cs = [10**i for i in range(-2,2)],\n                           penalty = 'l2', random_state=0, \n                           scoring = 'accuracy',\n                           multi_class='ovr').fit(X_train, y_train)\nprint(clf.get_params)\nprint('\\n')\nprint(\"Prediction %s on Test Set: %.6f\" %(clf.scoring, clf.score(X_test, y_test)))\n", "intent": "Baseline model with Cross Validation\n"}
{"snippet": "def create_new_model(optimizer='rmsprop', init='glorot_uniform'):\n    model = Sequential()\n    model.add(Dense(72, input_dim=72,kernel_initializer=init, activation='relu'))\n    model.add(Dense(1, kernel_initializer=init,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n", "intent": "Testing Model with Hyperparameter Tuning\n"}
{"snippet": "def gridSearch_clf(clf, param_grid, X_train, y_train):\n    gs = GridSearchCV(clf, param_grid).fit(X_train, y_train)\n    print(\"Best Parameters\")\n    print(gs.best_params_)\n    return gs.best_estimator_\n", "intent": "We will implement a stacking algorithm that uses all three and makes a prediction based on their predictions\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB().fit(xTrainStandard,ytrain)\n", "intent": "Let's train our classifier by calling its `fit()` method:\n"}
{"snippet": "mu.save_predictions_from_model(best_model, 'vw_{}'.format(best_model['group']))\n", "intent": "Save the model for bagging\n"}
{"snippet": "svc = SVC(C=1.0, cache_size=500, coef0=0.0, degree=3,\n gamma=0.0, kernel='poly', max_iter=-1, probability=False,\nrandom_state=None, shrinking=True, tol=0.001, verbose=True)\nsvc.fit(X_train, Y_train)\nprint(svc)\n", "intent": "Now that the vector are ready, its time to apply the machine learning algorithms\n"}
{"snippet": "model1 = load_model(save_model_name, custom_objects={'my_iou_metric': my_iou_metric})\ninput_x = model1.layers[0].input\noutput_layer = model1.layers[-1].input\nmodel = Model(input_x, output_layer)\nc = optimizers.RMSprop(lr=0.01)\nmodel.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric_2])\n", "intent": "Then the default threshod for pixel prediction is 0 instead of 0.5, as in my_iou_metric_2.\n"}
{"snippet": "estimator_local.fit({\"train\":s3_train_path, \"test\":s3_test_path})\n", "intent": "Call `fit()` to start the local training \n"}
{"snippet": "model1 = load_model(save_model_name, custom_objects={'my_iou_metric': my_iou_metric})\ninput_x = model1.layers[0].input\noutput_layer = model1.layers[-1].input\nmodel = Model(input_x, output_layer)\nc = optimizers.Adam(lr = 0.001)\nmodel.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric_2])\n", "intent": "Then the default threshod for pixel prediction is 0 instead of 0.5, as in my_iou_metric_2.\n"}
{"snippet": "def conv_block_simple(prevlayer, filters, prefix, strides=(1, 1)):\n    conv = Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", strides=strides, name=prefix + \"_conv\")(prevlayer)\n    conv = BatchNormalization(name=prefix + \"_bn\")(conv)\n    conv = Activation('relu', name=prefix + \"_activation\")(conv)\n    return conv\ndef conv_block_simple_no_bn(prevlayer, filters, prefix, strides=(1, 1)):\n    conv = Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", strides=strides, name=prefix + \"_conv\")(prevlayer)\n    conv = Activation('relu', name=prefix + \"_activation\")(conv)\n    return conv\n", "intent": "https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py\n"}
{"snippet": "from keras import models\nfrom keras import layers\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\nnetwork.add(layers.Dense(10, activation='softmax'))\n", "intent": "Let's create the network by using the `Sequential` API of Keras\nDocumentation : https://keras.io/getting-started/sequential-model-guide/\n"}
{"snippet": "smaller_model_hist = smaller_model.fit(x_train, y_train,\n                                       epochs=20,\n                                       batch_size=512,\n                                       validation_data=(x_test, y_test))\n", "intent": "Train the model with the same parameters as the previous one\nDocumentation : https://keras.io/models/sequential/\n"}
{"snippet": "l2_model_hist = l2_model.fit(x_train, y_train,\n                             epochs=20,\n                             batch_size=512,\n                             validation_data=(x_test, y_test))\n", "intent": "Train the model with the same parameters as the previous one\nDocumentation : https://keras.io/models/sequential/\n"}
{"snippet": "dpt_model_hist = dpt_model.fit(x_train, y_train,\n                               epochs=20,\n                               batch_size=512,\n                               validation_data=(x_test, y_test))\n", "intent": "Train the model with the same parameters as the previous one\nDocumentation : https://keras.io/models/sequential/\n"}
{"snippet": "from keras.layers import Dense\nmodel = Sequential()\n", "intent": "> TODO : URL SimpleRNN\n"}
{"snippet": "res = []\nfor k in xrange(1,121):\n    res.append(cross_validate(X, y, neighbors.KNeighborsClassifier(k).fit, 5))\n", "intent": "To find the optimal k we accumulate the scores of a 5 folds cross validation in a list of results. Then we find the max score in that list.\n"}
{"snippet": "model = tree.DecisionTreeClassifier(random_state=0)\n", "intent": "Create a new Decision Tree classifier using default parameters:\n"}
{"snippet": "estimator.fit({\"train\":s3_train_path, \"test\":s3_test_path})\n", "intent": "Call `fit()` to start the training\n"}
{"snippet": "model = tree.DecisionTreeClassifier(random_state=0).fit\ncross_validate(X_train, y_train,model,10)\n", "intent": "What does cross-validation look like if we use only our Train set?\n"}
{"snippet": "model = tree.DecisionTreeClassifier(max_depth=3, random_state=0).fit\ncross_validate(iris.data, iris.target,model,10)\n", "intent": "model.feature_importances_\nDoes limiting the max depth affect our average model performance with this training data?\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation=\"relu\", input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation=\"softmax\"))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=30, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from pgmpy.models import BayesianModel\nmodel = BayesianModel([('fruit', 'tasty'), ('size', 'tasty')])  \n", "intent": "We know that the variables relate as follows:\n"}
{"snippet": "from sklearn.cluster import KMeans\nn=2 \ndd=X \ntar=y \nkm=KMeans(random_state=324,n_clusters=n)\nres=km.fit(dd)\nprint(res.labels_)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier as RFC\nrf = RFC(n_estimators=30, n_jobs=-1,max_leaf_nodes=10)\nrf.fit(X_train, Y_train)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"}
{"snippet": "from sklearn import svm\nclf = svm.SVC(kernel='linear',C=10**100)  \nclf.fit(X, Y)\n", "intent": "http://scikit-learn.org/stable/modules/svm.html\nhttp://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"}
{"snippet": "import boto3\nregion = boto3.Session().region_name\ntrain_data = 's3://sagemaker-sample-data-{}/tensorflow/pipe-mode/train'.format(region)\neval_data = 's3://sagemaker-sample-data-{}/tensorflow/pipe-mode/eval'.format(region)\ntensorflow.fit({'train':train_data, 'eval':eval_data})\n", "intent": "After we've created the SageMaker Python SDK TensorFlow object, we can call ``fit()`` to launch TensorFlow training:\n"}
{"snippet": "kmeans = KMeans(n_clusters=5, random_state=1337)  \nkmeans.fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "model.fit(x_train, y_train, epochs=1000, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n", "intent": "Next we will create the simplest possible neural network. It has 1 layer, and that layer has 1 neuron, and the input shape to it is just 1 value.\n"}
{"snippet": "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n", "intent": "Let's now design the model. There's quite a few new concepts here, but don't worry, you'll get the hang of them. \n"}
{"snippet": "test_accuracy = tfe.metrics.Accuracy()\nfor (x, y) in tfe.Iterator(test_dataset):\n  prediction = tf.argmax(model(x), axis=1, output_type=tf.int32)\n  test_accuracy(prediction, y)\nprint(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))\n", "intent": "Unlike the training stage, the model only evaluates a single [epoch](https://developers.google.com/machine-learning/glossary/\n"}
{"snippet": "x = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dropout(0.2)(x)\nx = layers.Dense(1, activation='sigmoid')(x)\nmodel = Model(pre_trained_model.input, x)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=RMSprop(lr=0.0001),\n              metrics=['acc'])\n", "intent": "Now let's stick a fully connected classifier on top of `last_output`:\n"}
{"snippet": "model = Sequential()\nmodel2 = Dense()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_prime(x):\n    return sigmoid(x) * (1-sigmoid(x))\ndef error_formula(y, output):\n    return - y*np.log(output) - (1 - y) * np.log(1-output)\n", "intent": "The following function trains the 2-layer neural network. First, we'll write some helper functions.\n"}
{"snippet": "def sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n", "intent": "$g(z) = \\frac{1}{1+e^{-z}}$ expressed as:\n"}
{"snippet": "import os\nimport sagemaker\nfrom sagemaker import get_execution_role\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nregion = sagemaker_session.boto_session.region_name\n", "intent": "Let's start by setting up the environment:\n"}
{"snippet": "model = LogisticRegression(tol = .00000000000000000000001)\nmodel = model.fit(X, y)\nmodel.score(X, y)\n", "intent": "Now let's try fitting and scoring the model again:\n"}
{"snippet": "with tf.Session() as sess:\n    print(\"a: %i\" % sess.run(a), \"b: %i\" % sess.run(b))\n    print(\"Addition with constants: %i\" % sess.run(a+b))\n    print(\"Multiplication with constants: %i\" % sess.run(a*b))\n", "intent": "Let us launch the default graph.\n"}
{"snippet": "with tf.Session() as sess:\n    print(\"Addition with variables: %i\" % sess.run(add, feed_dict={a: 2, b: 3}))\n    print(\"Multiplication with variables: %i\" % sess.run(mul, feed_dict={a: 2, b: 3}))\n", "intent": "Let us launch the default graph.\n"}
{"snippet": "X = tf.placeholder(\"float\")\nY = tf.placeholder(\"float\")\nW = tf.Variable(rng.randn(), name=\"weight\")\nb = tf.Variable(rng.randn(), name=\"bias\")\n", "intent": "The input graph is created and weights for the model are set.\n"}
{"snippet": "def neural_net(x_dict):\n    x = x_dict['images']\n    layer_1 = tf.layers.dense(x, n_hidden_1)\n    layer_2 = tf.layers.dense(layer_1, num_classes)\n    out_layer = tf.layers.dense(layer_2, n_hidden_2)\n    return out_layer\n", "intent": "Define the neural network.\n"}
{"snippet": "def neural_net(x_dict):\n    x = x_dict['images']\n    layer_1 = tf.layers.dense(x, n_hidden_1)\n    layer_2 = tf.layers.dense(layer_1, n_hidden_2)\n    out_layer = tf.layers.dense(layer_2, num_classes)\n    return out_layer\n", "intent": "Define the neural network.\n"}
{"snippet": "G = nx.barabasi_albert_graph(1000000, 1)\ndegrees = G.degree()\ndegree_values = sorted(set(degrees.values()))\nhistogram = [list(degrees.values()).count(i)/float(nx.number_of_nodes(G)) for i in degree_values]\n", "intent": "To illustrate this, we create a graph with 1000000 nodes starting from 1 node.\n"}
{"snippet": "lm2 = LinearRegression(fit_intercept=False)\nlm2.fit(X, bos.PRICE)\n", "intent": "**Your turn:** How would you change the model to not fit an intercept term? Would you recommend not having an intercept?\n"}
{"snippet": "kmn.fit(df.drop(columns=['Private'],axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "sagemaker.Session().delete_endpoint(predictor.endpoint)\n", "intent": "Let's delete the endpoint we just created to prevent incurring any extra costs.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**KNN model instance with n_neighbors=1**\n"}
{"snippet": "log_reg.fit(X_train,y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nrfreg = RandomForestClassifier()\nrfreg\nimport numpy as np\n", "intent": "Build Random Forest Model\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Flatten, Dense\nfrom keras.layers import Embedding\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=200))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n", "intent": "We will be using the same model architecture as before  with an additional Dense layer using `relu` activation:\n"}
{"snippet": "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\nmodel.save_weights('pre_trained_spacy_model.h5')\n", "intent": "Let's compile our model and train it:\n"}
{"snippet": "from keras.layers import Dense\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n                    epochs=10,\n                    batch_size=128,\n                    validation_data=(x_val, y_val))\n", "intent": "Let's train a simple recurrent network using an `Embedding` layer and a `SimpleRNN` layer:\n"}
{"snippet": "from datetime import timedelta\nltg30_blobs = goesio.get_ltg_blobs(irdt + timedelta(minutes=30), timespan_minutes=15)\nltg30 = goesio.create_ltg_grid(ltg30_blobs, griddef, influence_km)\nmodel = LogisticRegression()\ndf = create_prediction_df(ref, ltg30, griddef)\nx = df.drop(['lat', 'lon', 'ltg'], axis=1)\nmodel = model.fit(x, df['ltg'])\nprint(model.coef_, model.intercept_)\nprint('Model accuracy={}%'.format(100*model.score(x, df['ltg'])))\n", "intent": "How about if we try to predict lightning 30 minutes into the future?\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], minval=-1, maxval=1), name='embedding')\n    embed = tf.nn.embedding_lookup(embedding, input_data) \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def fit_and_plot_dt(x, y, depth, title, ax):\n    dt = tree.DecisionTreeClassifier(max_depth = depth)\n    dt.fit(x, y)\n    ax = plot_tree_boundary(x, y, dt, title, ax)\n    return ax\n", "intent": "And a Decision Tree classification for our two new images are:\n"}
{"snippet": "estimator.fit(training_data)\n", "intent": "The `estimator.fit` call bellow starts training and creates a data channel named `training` with the contents of the\n S3 location `training_data`.\n"}
{"snippet": "kmeans = KMeans(n_clusters=2)\n", "intent": "**Erstelle eine Instanz von einem K Means Modell mit 2 Clustern.**\n"}
{"snippet": "kmeans.fit(df.drop('Private',axis=1))\n", "intent": "**Fitte das Modell auf alle Daten (ohne die \"Private\" Spalte).**\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS',axis=1))\n", "intent": "**Wende den Scaler auf die Eigenschaften an.**\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Erstelle ein KNN Modell mit n_neighbors=1.**\n"}
{"snippet": "knn.fit(X_train,y_train)\n", "intent": "**Fitte dieses Modell zum Trainingsset.**\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "**Erstelle eine Instanz von LinearRegression() namens lm.**\n"}
{"snippet": "lm.fit(X_train,y_train)\n", "intent": "**Trainiere lm mit den Trainingsdaten.**\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\ngrid.fit(X_train,y_train)\n", "intent": "**Erstelle ein GridSearchCV Objekt und fitte es auf die Trainingsdaten.**\n"}
{"snippet": "model.fit(X_train, y_train)\n", "intent": "Train the model using the training data\n"}
{"snippet": "from sagemaker.tensorflow.serving import Model\nenv = {'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'mobilenet_v2_140_224'}\nmodel = Model(model_data=model_data, role=sagemaker_role, framework_version=1.12, env=env)\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge')\n", "intent": "Now that the model archive is in S3, we can create a Model and deploy it to an \nEndpoint with a few lines of python code:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform(shape=(vocab_size,embed_dim),minval=-1,maxval=1))\n    embed = tf.nn.embedding_lookup(embedding,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "for k in range(1,30)[::-1]:\n    knn = KneighborsClassifier(n_neighbors=k)\n    knn.fit(wine_df,y)\n    print (k, knn.score(wine_df,y))\n", "intent": "Using the best features, explore what the best value of `k` is for this dataset. \n"}
{"snippet": "for k in range(1,30)[::-1]:\n    knn = KneighborsClassifier(n_neighbors=k)\n    knn.fit(x_train,y_train)\n    print (k, knn.score(x_test,y_test)    \n", "intent": "Are our choice of hyper-parameters and features the same as they were when we were validating based on a training set alone?\n"}
{"snippet": "depth_range = np.arange(1, 25)\ntrain_scores, test_scores = validation_curve(DecisionTreeClassifier(), \n                                             X, \n                                             y, \n                                             param_name=\"max_depth\", \n                                             param_range=depth_range,\n                                             cv=3, \n                                             scoring=\"accuracy\")\n", "intent": "Code adapted from [www.chrisalbon.com](www.chrisalbon.com)\n"}
{"snippet": "ada = AdaBoostClassifier(n_estimators=300) \n", "intent": "Compare and contrast Decision Trees and AdaBoost\n"}
{"snippet": "km = KMeans(n_clusters=4)\nkm.fit(data)\nlabels = km.labels_\ncents = km.cluster_centers_\npd.value_counts(labels)\n", "intent": "What cluster value should we set?\n"}
{"snippet": "dt = DecisionTreeClassifier(max_depth=4)\ndt.fit(X,y)\n", "intent": "Bonus!!\n<br><br>\nLet's see the most important features and visualize the decision tree\n"}
{"snippet": "model = Sequential()\nmodel.compile(optimizer=\"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\nmodel.fit(X, y_binary, epochs=40, validation_split = 0.25)\n", "intent": "We're trained a really good model, but principles of cross validation also to deep learning. Here's how we'll evaluate the model on a testing data.\n"}
{"snippet": "model = Sequential()\nmodel.compile(optimizer=\"adam\", loss = \"mean_squared_error\")\n", "intent": "**Back to the drawing board!**\nWe need more layers!!\n"}
{"snippet": "ntm.fit({'train': s3_train, 'validation': s3_val, 'auxiliary': s3_aux, 'test': s3_test})\n", "intent": "We are ready to run the training job. Again, we will notice in the log that the top words are printed together with the WETC and TU scores.\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3, input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten(input_shape=(32, 32, 3)))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "X_train_new = X_train.iloc[:,40:60]\nX_test_new = X_test.iloc[:,40:60]\nlr = LinearRegression()\nlr.fit(X_train_new, y_train)\nprint 'Train R-squared: {:.3}'.format(lr.score(X_train_new, y_train))\nprint 'Test R-squared: {:.3}'.format(lr.score(X_test_new, y_test))\n", "intent": "This is the most naive approach to dimensionality reduction; just pick $k$ original features at random.\n"}
{"snippet": "X, actual, y = generate_sample_data(20, true_function)\nlr = LinearRegression(fit_intercept=True)\nlr.fit(X, y)\ncompare_fitted_function(X, y, actual, lr)\n", "intent": "Generate some data, fit a linear regression model, and plot the results.\n"}
{"snippet": "degree = 2\npoly = PolynomialFeatures(degree = degree, include_bias=False)\nlr = LinearRegression(fit_intercept=True)\npipeline = Pipeline([(\"polynomial_features\", poly),\n                     (\"linear_regression\", lr)])\npipeline.fit(X, y)\ncompare_fitted_function(X, y, true_function(X), pipeline)\n", "intent": "We can use sklearn's Pipeline framework to connect the polynomial expansion and the linear regression into one 'model'\n"}
{"snippet": "dt.fit(train_df[predictors], train_df[target])\n", "intent": "Fit to the training data\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ndt = DecisionTreeRegressor(max_depth = 6)\nparameter_grid = {'max_depth': [2,3,4,5,6]}\nsearcher = GridSearchCV(dt, param_grid = parameter_grid, cv = 10)\nsearcher.fit(train_df[predictors], train_df[target])\ndt.fit(train_df[predictors], train_df[target])\n", "intent": "Sklearn doesn't allow _post-pruning_ of decision trees, but we can try some pre-pruning parameters\n"}
{"snippet": "categorical_pipe = Pipeline(\n)\n", "intent": "**Exercise**: Combine the categorical steps into a single pipeline\n"}
{"snippet": "degree = 6\npoly = PolynomialFeatures(degree = degree, include_bias=False)\nlr = LinearRegression(fit_intercept=True)\npipeline = Pipeline([(\"polynomial_features\", poly),\n                     (\"linear_regression\", lr)])\npipeline.fit(X, y)\ncompare_fitted_function(X, y, true_function(X), pipeline)\n", "intent": "We can use sklearn's Pipeline framework to connect the polynomial expansion and the linear regression into one 'model'\n"}
{"snippet": "def random_features(nfeatures):\n    cols = np.random.choice(X_train.columns, nfeatures, replace=False)\n    lr = LinearRegression()\n    lr.fit(X_train.loc[:, cols], y_train)\n    return lr.score(X_test.loc[:, cols], y_test)\nout = [random_features(i) for i in range(1, 120)]\n", "intent": "This is the most naive approach to dimensionality reduction; just pick $k$ features at random.\n"}
{"snippet": "f = io.BytesIO()\ntorch.onnx.export(pytorch_model, inputs, f, verbose=True)\nonnx_model = onnx.ModelProto.FromString(f.getvalue())\nprint(\"Check the ONNX model.\")\nonnx.checker.check_model(onnx_model)\n", "intent": "Run the PyTorch exporter to generate an ONNX model.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ndt = DecisionTreeRegressor()\nparameter_grid = {'max_depth': [2, 3, 4, 5, 6]}\nsearcher = GridSearchCV(dt, param_grid=parameter_grid, cv=10)\nsearcher.fit(train_df[predictors], train_df[target])\n", "intent": "Sklearn doesn't allow _post-pruning_ of decision trees, but we can try some pre-pruning parameters\n"}
{"snippet": "x = tf.placeholder(tf.float32)\nW = tf.Variable([3.], tf.float32)\nb = tf.Variable([-3.], tf.float32)\nlinear_model = W * x + b\n", "intent": "Think about the linear equation\n$$\ny = 3 x - 3\n$$\n"}
{"snippet": "scaler.fit(bank_data.drop('Class', axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=1)]\nlearn = learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[20, 20, 20], n_classes=2)\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "kmeans.fit(college.drop(['Unnamed: 0', 'Private'], axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "standardScaler.fit(df.drop('TARGET CLASS', axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "naive_bayes.fit(X_train, y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid= param_grid, verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nlgreg_cv_model = LogisticRegressionCV(n_jobs=-1, random_state=42, Cs = [1], cv = 10, \\\n                                      class_weight = 'balanced', refit=False, scoring = 'accuracy',\\\n                                     penalty = 'l2') \nlgreg_cv_model.fit(X_train, Y_train.ravel()) \nlgreg_cv_model.scores_\nprint(np.mean(lgreg_cv_model.scores_['Male']))\n", "intent": "<div class=\"span5 alert alert-info\">\n> \n"}
{"snippet": "pytorch_results = pytorch_model(*inputs)\n_, caffe2_results = c2_native_run_net(init_net, predict_net, caffe2_inputs)\n", "intent": "Run PyTorch and Caffe2 models separately, and get the results.\n"}
{"snippet": "idf = np.array([np.log(3), np.log(3), np.log(3./2), np.log(3), np.log(3./2), np.log(3./2)])\nXtfidf = np.dot(X.todense(), np.diag(idf))\n", "intent": "**Q**: Compute the td-ifd matrix for the training set \n"}
{"snippet": "regrAll = LinearRegression()\nregrAll.fit(X, y)\nprint \"sales = \", regrAll.intercept_, \" + \", regrAll.coef_[0], \" x TV + \", \\\n       regrAll.coef_[1], \" x Radio + \", \\\n       regrAll.coef_[2], \" x Newspaper\"\n", "intent": "<br>\nNow we'll fit a multiple linear regression model for the three features simultaneously. \n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nnames = [\"Nearest Neighbors\"]\ncl = [KNeighborsClassifier(3)] \n", "intent": "**Activity! Get up!**\n"}
{"snippet": "from sklearn import linear_model\nobj = linear_model.LinearRegression()\nobj.fit(np.array(data.height.values.reshape(-1,1)), data.stories )\nprint( obj.coef_, obj.intercept_ )\n", "intent": "We fit a linear regression model below. We try to use height to predict the number of stories in a building.\n"}
{"snippet": "obj2 = linear_model.LinearRegression()\nX = np.array( (data.height.values, data.year.values))\nobj2.fit(X.transpose() , data.stories)\nprint(obj2.coef_, obj2.intercept_)\n", "intent": "Now we will do multiple linear regression. This means we will use more than one predictor when we fit a model and predict our response variable \n"}
{"snippet": "print x4.todense()\n", "intent": "Let's print it and see what it looks like \n"}
{"snippet": "def convolutional_layer(input_x,shape):\n    W = init_weights(shape)\n    b = init_bias([shape[3]])\n    return tf.nn.relu(conv2d(input_x,W)+b)\n", "intent": "Convolutional layer\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(20, input_dim = 2, activation = 'sigmoid'))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam', metrics= ['accuracy'])\n", "intent": "Can you draw a graph and see how it looks?\n"}
{"snippet": "kernels = tf.get_default_graph().get_operation_by_name('convolution/kernel').values()[0]\nshape = kernels.shape.as_list()  \nw = tf.reshape(kernels, [shape[3], shape[0], shape[1], shape[2]])\ntf.summary.image('kernel_weights', w, max_outputs=shape[3], collections=None)\n", "intent": "This will add the kernels to the TensorBoard output, so that we can visualize them.\n"}
{"snippet": "pytorch_time = benchmark_pytorch_model(pytorch_model, inputs)\ncaffe2_time = benchmark_caffe2_model(init_net, predict_net)\nprint(\"PyTorch model's execution time is {} milliseconds/ iteration, {} iterations per second.\".format(\n    pytorch_time, 1000 / pytorch_time))\nprint(\"Caffe2 model's execution time is {} milliseconds / iteration, {} iterations per second\".format(\n    caffe2_time, 1000 / caffe2_time))\n", "intent": "The following code measures the performance of PyTorch and Caffe2 models.\nWe report:\n- Execution time per iteration\n- Iterations per second\n"}
{"snippet": "merged = tf.summary.merge_all()\nreset_vars()\ntrain_writer = tf.summary.FileWriter(logs_path + '/train', graph=tf.get_default_graph())\ntest_writer = tf.summary.FileWriter(logs_path + '/test', graph=tf.get_default_graph())\n", "intent": "We will then initialize our variables, launch our graph, and train our model. \n"}
{"snippet": "W1 = tf.Variable(initializer([N_PIXELS, hidden_size]), name=\"weights\")\nb1 = tf.Variable(tf.zeros([hidden_size]), name=\"biases\")\nhidden = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\nW2 = tf.Variable(initializer([hidden_size, N_CLASSES]), name=\"weights2\")\nb2 = tf.Variable(tf.zeros([N_CLASSES]), name=\"biases2\")\ny = tf.matmul(hidden, W2) + b2\n", "intent": "This training placeholder will be used later to indicate whether we are in the process of training or prediction.  Right now, it is extraneous.\n"}
{"snippet": "hidden = tf.layers.dense(x, hidden_size, activation=tf.nn.sigmoid, use_bias=True,\n    kernel_initializer=tf.truncated_normal_initializer(stddev=N_PIXELS**-0.5))\n", "intent": "We have been using **dense** layers.  That is, each neuron is connected to all of the inputs to the layer.  First we create thie hidden layer:\n"}
{"snippet": "draw_graph(\"exercise\")\n", "intent": "Implement the following graph in TensorFlow.\n"}
{"snippet": "with tf.Session(graph=g2) as s2:\n    print s2.run(sum_g2)\n", "intent": "Alternatively, you can use sessions as context managers.  They close themselves on exit, so you don't have to do this explicitly.\n"}
{"snippet": "sess2 = tf.Session()\nsess2.run(tf.global_variables_initializer())\nprint \"New session:\", sess2.run(var)\nprint \"Old session:\", sess.run(var)\n", "intent": "The value of each variable exists only within a given session.  If you start with another session, variables will have a separate value in that one.\n"}
{"snippet": "p_insc = tf.Variable(4 * np.sqrt(2), name=\"Inscribed_Perimeter\", dtype=np.float64)\np_circ = tf.Variable(8.0, name=\"Circumscribed_Perimeter\", dtype=np.float64)\n", "intent": "Now we can re-run our estimation for $2\\pi$ using TensorFlow variables.  We'll start them off with the values for $n = 4$.\n"}
{"snippet": "reset_tf()\nF = tf.Variable([0,1])\nupdate = F.assign([F[1], F[0] + F[1]])\n", "intent": "Note that using int64 is important.  The default int dtype (on my system, at least) is int32, which overflows.\n"}
{"snippet": "reset_tf()\nF_low = tf.Variable(1)\nF_high = tf.Variable(1)\nF_next = F_low + F_high\nupdate1 = F_low.assign(F_high)\nupdate2 = F_high.assign(F_next)\n", "intent": "This doesn't work, because the definition of F_next depends on F_high, which is already updated.\n"}
{"snippet": "from torch.autograd import Variable\nbatch_size = 1    \nx = Variable(torch.randn(batch_size, 3, 224, 224), requires_grad=True)\ntorch_out = torch.onnx._export(torch_model,             \n                               x,                       \n                               \"squeezenet.onnx\",       \n                               export_params=True)      \n", "intent": "and export the pytorch model as onnx model:\n"}
{"snippet": "reset_tf()\nW = tf.Variable(tf.zeros((13, 1)), name=\"weight\")\nb = tf.Variable(tf.zeros(1), name=\"bias\")\nx = tf.placeholder(shape=[None, 13], dtype=tf.float32, name='x')\ny_label = tf.placeholder(shape=[None, 1], dtype=tf.float32, name='y_label')\ny = tf.matmul(x, W) + b\nloss = tf.reduce_mean(\n    tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=y_label))\n", "intent": "The setup is nearly identical to the case of linear regression.\n"}
{"snippet": "y_pred, out_shape, lstm_new_state = make_lstm(x_enc, lstm_init_value, n_chars, lstm_size, n_layers)\n", "intent": "These values get fed into our net.\n"}
{"snippet": "final_out = tf.reshape(tf.nn.softmax(y_pred), \n                       (out_shape[0], out_shape[1], n_chars))\n", "intent": "The predictions come out flattened, so if we want an array in the same shape as the input, we need to reshape it.\n"}
{"snippet": "merged = tf.summary.merge_all()\ntrain_writer = tf.summary.FileWriter(logs_path + '/train', graph=tf.get_default_graph())\n", "intent": "As we have done in the previous model we built, we then merge our summaries and initialize and lunch the graph.\n"}
{"snippet": "y = df.imdb\nones = (df.director*0+1)\nones = ones.rename('bias')\nX = pd.concat([ones, df.likes, df.year], axis=1)\nmodel = sm.OLS(y, X) \nfit = model.fit()\nfit.summary()\n", "intent": "I'll use the movie release year as the categorical variable, because it is the only categorical variable I have in the dataset.\n"}
{"snippet": "lr=LogisticRegression()\nfit = lr.fit(X,y)\nfit.coef_\n", "intent": "The KNN and Logistic Regression accuracy scores are not any better than the \"predict PG-13 for every movie\" accuracy score.\n"}
{"snippet": "lr = LogisticRegression()\nfit = lr.fit(X,y)\nfit.coef_\n", "intent": "What are the coefficients of logistic regression? Which features affect the outcome how?\n"}
{"snippet": "fit = LogisticRegression().fit(X,y)\nfit.coef_\n", "intent": "What are the coefficients of logistic regression? Which features affect the outcome how?\n"}
{"snippet": "m, train_err, test_err = learning_curve(LogisticRegression(),X,y,cv=5)\ntrain_cv_err = np.mean(train_err,axis=1)\ntest_cv_err = np.mean(test_err,axis=1) \n", "intent": "Draw the learning curve for logistic regression in this case.\n"}
{"snippet": "from torch.autograd import Variable\nimport torch.onnx\nimport torchvision\ndummy_input = Variable(torch.randn(1, 3, 224, 224))\nmodel = torchvision.models.alexnet(pretrained=True)\ntorch.onnx.export(model, dummy_input, \"alexnet.onnx\")\n", "intent": "If you already have your model built, it's just a few lines:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1,1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "num_examples = len(X) \nnn_input_dim = 2 \nnn_output_dim = 2 \nepsilon = 0.01 \nreg_lambda = 0.01 \ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef dsigmoid(y):\n    return y * (1.0 - y)    \n", "intent": "Now we are ready for our implementation. We start by defining some useful variables and parameters for gradient descent:\n"}
{"snippet": "lm1 = smf.ols(formula='Sales ~ TV', data=data).fit()\nlm1.params\n", "intent": "Let's estimate the model coefficients for the advertising data:\n"}
{"snippet": "feature_cols = ['TV', 'Radio', 'Newspaper', 'Size_large']\nX = data[feature_cols]\ny = data.Sales\nlm2 = LinearRegression()\nlm2.fit(X, y)\nzip(feature_cols, lm2.coef_)\n", "intent": "Let's redo the multiple linear regression and include the **Size_large** feature:\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nfeature_cols = ['al']\nX = glass[feature_cols]\ny = glass.ri\nlinreg.fit(X, y)\n", "intent": "Let's create a linear regression model.  What are our 6 steps for creating a model?\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=50)\n", "intent": "Instead of treating this as a regression problem, treat it as a classification problem and see what testing accuracy you can achieve with KNN.\n"}
{"snippet": "km = KMeans(n_clusters=4, random_state=1)\nkm.fit(X_scaled)\nbeer['cluster'] = km.labels_\nbeer.sort_values('cluster')\n", "intent": "7) Which k gives the best Silhouette score? use this value instead of 'x' below.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation = 'relu', input_shape = (1000,)))\nmodel.add(Dropout(.4))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=5000, verbose=1, validation_data = (x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "import onnx\nmodel = onnx.load(\"alexnet.onnx\")\nonnx.checker.check_model(model)\nprint(onnx.helper.printable_graph(model.graph))\n", "intent": "**That's it!**\nYou can also use ONNX tooling to check the validity of the resulting model or inspect the details\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1000, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.4))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss ='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=1000, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "baseline=DummyClassifier()\nbaseline.fit(X_val,y_val)\nprint(\"the classification accuracy score for the baseline classifier is:\", baseline.score(X_val,y_val))\n", "intent": "In my opinion, I will use DummyClassifier as my baseline classifier, because this classifier makes predictions using simple rules.\n"}
{"snippet": "do_dbscan(data, 10, 0.25)\ndo_dbscan(data, 10, 0.35)\ndo_dbscan(data, 10, 0.45)\n", "intent": "Now compare three different max. dist values\n"}
{"snippet": "test_accuracy = tfe.metrics.Accuracy()\nfor (x, y) in test_dataset:\n  prediction = tf.argmax(model(x), axis=1, output_type=tf.int32)\n  test_accuracy(prediction, y)\nprint(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))\n", "intent": "Unlike the training stage, the model only evaluates a single [epoch](https://developers.google.com/machine-learning/glossary/\n"}
{"snippet": "import numpy\nimport theano\nimport theano.tensor as T\nclass MLP(object):\n     def __init__(self, rng, input, n_in, n_hidden, n_out):\n", "intent": "Our Multi-Layer-Perceptron now plugs everything together, i.e. one hidden layer and the softmax layer.\n"}
{"snippet": "def build_mlp(n_in, n_hidden, n_out, input_var=None):\n    l_in = lasagne.layers.InputLayer(shape=(None, n_in), input_var=input_var)\n    l_hid1 = lasagne.layers.DenseLayer(incoming=l_in,\n                num_units=n_hidden, nonlinearity=lasagne.nonlinearities.tanh,\n                W=lasagne.init.GlorotUniform())\n    l_out = lasagne.layers.DenseLayer(incoming=l_hid1, \n            num_units=n_out, nonlinearity=lasagne.nonlinearities.softmax)\n    return l_out\n", "intent": "Now we use the provided layers from Lasagne to build our MLP\n"}
{"snippet": "X_2 = news_A.drop('class',axis=1)\ny_2 = news_A['class']\nclf2 = GaussianNB()\nclf2.fit(X_2,y_2)\nprint('The classification accuracy on the original training dataset is:',format(clf2.score(X_2,y_2)))\n", "intent": "Fit a Gaussian Naive Bayes model to the original dataset A (including the outliers). Display the classification accuracy on the training dataset.\n"}
{"snippet": "kmeans = KMeans(n_clusters = 5, random_state = 1234)\nkmeans.fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "p_y_mean_test = sigmoid(beta_coef(x_test, beta_mean))\np_y_post_test = sigmoid(beta_coef(x_test, trace['betas'].T))\npost_pred_test = np.random.binomial(1, p_y_post_test)\np_y_pp = post_pred_test.mean(axis=1)\np_y_post_thresh = p_y_post_test[:, :]\np_y_post_thresh[p_y_post_thresh > 0.5] = 1\np_y_post_thresh[p_y_post_thresh <= 0.5] = 0\np_y_cdf = p_y_post_thresh.mean(axis=1)\n", "intent": "We perform the same analysis on the testing set.\n"}
{"snippet": "def fully_connected(x, dim_in, dim_out, name):\n    with tf.variable_scope(name) as scope:\n        w = tf.get_variable('w', shape=[dim_in, dim_out], \n                            initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n        b = tf.get_variable('b', shape=[dim_out])\n        out = tf.matmul(x, w) + b\n        return out    \n", "intent": "To implement a fully-connected layer, we can simply use [tf.matmul](https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops.html\n"}
{"snippet": "kmeans_cluster = KMeans(2).fit(zipcode_shape.iloc[:, range(5,26)])\ncenters = kmeans_cluster.cluster_centers_\nzipcode_shape['km_labels'] = kmeans_cluster.labels_\n", "intent": "- It can also be seen how the data starts to segregate in the best way possible for 2 clusters.\n"}
{"snippet": "kmeans_cluster_3 = KMeans(3).fit(zipcode_shape.iloc[:, range(5,26)])\ncenters_3 = kmeans_cluster_3.cluster_centers_\nzipcode_shape['km_labels'] = kmeans_cluster_3.labels_\n", "intent": "- It can also be seen how the data starts to segregate in the best way possible for 3 clusters.\n"}
{"snippet": "kmeans_cluster_4 = KMeans(4).fit(zipcode_shape.iloc[:, range(5,26)])\ncenters_4 = kmeans_cluster_4.cluster_centers_\nzipcode_shape['km_labels'] = kmeans_cluster_4.labels_\n", "intent": "- It can also be seen how the data starts to segregate in the best way possible for 4 clusters.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded_input = tf.contrib.layers.embed_sequence(input_data,\n                                                      vocab_size=vocab_size,\n                                                      embed_dim=embed_dim)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "import statsmodels.api as sm\nfrom statsmodels.stats.gof import chisquare\nmodels = []\nselectedIndeces = [17,16,15]\nfor i in selectedIndeces:\n    models.append(sm.OLS(mtabyride[i], \n                         sm.add_constant(np.arange(mta.shape[2]))).fit())\nfor i,m in enumerate(models):\n    print (selectedIndeces[i], \"R^2 line fit %.2f\"%m.rsquared)\n", "intent": "Figure 4: the steepest smooth increase in ridership type is for Senior passes. Above is the average over all station of the senior pass popularity\n"}
{"snippet": "from sklearn.cluster import KMeans\nnc = 9\ndata = np.load(\"MTA_Fare.npy\")\ntots = data.transpose(2,0,1).reshape(data.shape[2], \n                                     data.shape[1]*data.shape[0]).T\ntots = tots[tots.std(1)>0]\nkm = KMeans(n_clusters=nc)\nvals = ((tots.T - tots.mean(1))/tots.std(1)).T\nkm.fit(vals)\n", "intent": "Figure 8: The 4 station that shows the most clear periodicity at 1-year\n"}
{"snippet": "test_models = [LinearRegression(), Ridge(alpha=10), Lasso(alpha=10)]\nscores = [analyze_performance(my_model) for my_model in test_models] \n", "intent": "Let's try a few degrees with a regularized model.\n"}
{"snippet": "from eml.net import describe as ndescribe\nfrom eml.net.reader import keras_reader\nnet = {}\ndef convert_keras_net(knet, net):\n    for target in sim_out:\n        net[target] = keras_reader.read_keras_sequential(knet[target])\n    net['all'] = keras_reader.read_keras_sequential(knet['all'])\nconvert_keras_net(knet, net)\n", "intent": "Now, the EMLlib allows us to convert the keras networks into an internal format with one line of code:\n"}
{"snippet": "from sklearn import mixture\ngmm = mixture.GaussianMixture(n_components=2,covariance_type='full').fit(train_x)\n", "intent": "<font color = \"blue\">\n"}
{"snippet": "target = train_df['is_iceberg'].values\nIcebergModel.fit(x=X_band,y=target,epochs=20,batch_size=128)\n", "intent": "Now we fit the model with the training data having epochs = 20 and random batch_size = 128 . \n"}
{"snippet": "IcebergModel = Iceberg_model((75,75,3))\n", "intent": "For  creating the model we provide the shape of each training data (75,75,2) .\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "X = df.drop(['Unnamed: 0','Private'], axis=1)\ny = df['Private']\nkmeans.fit(X)\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid, refit=True, verbose=2)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=50)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.cluster import DBSCAN\ndbscan = DBSCAN(eps = 3, min_samples = 3)\ndbscan.fit(Xs)\n", "intent": "Remember to pass an `eps` and `min_samples` of your choice.\n"}
{"snippet": "model = dt_classifier.fit(X_train, y_train)\n", "intent": "Then we use the `fit` method on the train data to fit our model.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=128,\n         validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = rf_classifier.fit(X_train, y_train)\n", "intent": "Now we fit the model on our training data.\n"}
{"snippet": "param_grid = {'min_samples_split': range(2,10),\n              'min_samples_leaf': range(1,10)}\nmodel_r = GridSearchCV(ensemble.RandomForestClassifier(), param_grid)\nmodel_r.fit(X_train, y_train)\nbest_index = np.argmax(model_r.cv_results_[\"mean_test_score\"])\nprint(\"Best index:\", model_r.cv_results_[\"params\"][best_index])\nprint(\"Mean test score:\", max(model_r.cv_results_[\"mean_test_score\"]))\nprint(\"Held-out:\", model_r.score(X_test, y_test))\n", "intent": "Let's do another grid search to determine the best hyperparameters:\n"}
{"snippet": "from sklearn import linear_model\nlin_reg = linear_model.LinearRegression(n_jobs=1)  \nmodel = lin_reg.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\n", "intent": "We'll start with a basic OLS linear regression model:\n"}
{"snippet": "from tpot import TPOTRegressor\ntpot = TPOTRegressor(generations=5, population_size=20, verbosity=2)  \ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_boston_pipeline.py')\n", "intent": "Now TPOT will make the model.\n"}
{"snippet": "kmeans = KMeans(n_clusters=3,\n               max_iter=300 \n               ).fit(X)\n", "intent": "Now we can create the model:\n"}
{"snippet": "from sklearn.cluster import AgglomerativeClustering\nward = AgglomerativeClustering(n_clusters=2,\n                               linkage='ward', \n                               affinity='euclidean') \n", "intent": "We'll use two clusters this time, and use ward linkage.\n"}
{"snippet": "ward.fit(noisy_moons)\n", "intent": "Now we'll fit the clustering model on the dataset.\n"}
{"snippet": "from IPython.display import display\nfor i in range(25):\n    run_iteration('IR_input_data.csv')\n    if (i + 1) in [1, 2, 3, 4, 5, 15, 25]:\n        print 'Iteration', i + 1\n        display(check_ir_model())\n", "intent": "The remaining iterations work with the model that was generated.\n"}
{"snippet": "with tf.Session() as sess:\n    saver = tf.train.Saver()\n    saver.restore(sess, './model/model.checkpoint')\n    dev_predicted = sess.run(predict, feed_dict=test_feed_dict)\n    dev_accuracy = nn.calculate_accuracy(dev_orders, dev_predicted)\ndev_accuracy\n", "intent": "The following code loads your model, computes accuracy, and exports the result. **DO NOT** change this code.\n"}
{"snippet": "from sklearn.svm import SVC\nir = SVC()\n", "intent": "Now its time to train a Support Vector Machine Classifier. \n**Call the SVC() model from sklearn and fit the model to the training data.**\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\nmnb.fit(train_dtm, y_train)\n", "intent": "Use `sklearn` to build a `MultinomialNB` classifier against your training data.\n"}
{"snippet": "km = KMeans(n_clusters=4, init='k-means++', n_init=10, max_iter=300, random_state=1)\n", "intent": "Initialize a new kmeans classifier\n"}
{"snippet": "clf = DecisionTreeClassifier(max_depth=2,random_state=0)\nvisualize_tree(clf,X,y)\n", "intent": "**Let's plot out a Decision Tree boundary with a max depth of two branches**\n"}
{"snippet": "clf = DecisionTreeClassifier(max_depth=4,random_state=0)\nvisualize_tree(clf,X,y)\n", "intent": "**How about 4 levels deep?**\n"}
{"snippet": "model.add(Dense(16, input_shape=(4,)))\nmodel.add(Activation('sigmoid'))\n", "intent": "The next two lines define the size input layer (input_shape=(4,), and the size and activation function of the hidden layer\n"}
{"snippet": "model.add(Dense(3))\nmodel.add(Activation('softmax'))\n", "intent": "... and the next line defines the size and activation function of the ouput layer.\n"}
{"snippet": "print \"Hit Rates:\"\nmodels = [(\"LR\", LogisticRegression()), (\"LDA\", LDA()), (\"QDA\", QDA())]\nfor m in models:\n    fit_model(m[0], m[1], X_train, y_train, X_test, pred)\nprint pred.head()\n", "intent": "Create and fit three basic models:\n"}
{"snippet": "def build_linear_regression_model(X, y):\n    linear_mdl = linear_model.LinearRegression()  \n    X = np.reshape(X, (X.shape[0], 1))\n    y = np.reshape(y, (y.shape[0], 1))\n    linear_mdl.fit(X, y)  \n    return linear_mdl\n", "intent": "   **Design the Benchmark Regression Model **\n"}
{"snippet": "regression_model = build_linear_regression_model(X_train,y_train)\n", "intent": "   **Compile the model and fit the data **\n"}
{"snippet": "param_grid = {'C':[0.1,1,10,100],'gamma':[1,0.1,0.01,0.001]}\nir1 = GridSearchCV(SVC(),param_grid,verbose=3)\n", "intent": "**Create a dictionary called param_grid and fill out some parameters for C and gamma.**\n"}
{"snippet": "lstm_basic_model = build_basic_model(shape,neurons)\n", "intent": "**Build and compile basic LSTM model**\n"}
{"snippet": "epochs = 1\nlstm_basic_model.fit(X_train, y_train,epochs=epochs,validation_split=0.2)\n", "intent": "**Train basic LSTM model**\n"}
{"snippet": "def build_optimized_lstm_model(shape, neurons, dropout):\n    model = Sequential()\n    model.add(LSTM(neurons[0], input_shape=(shape[0], shape[1]), return_sequences=True))\n    model.add(Dropout(dropout))\n    model.add(LSTM(neurons[1], input_shape=(shape[0], shape[1]), return_sequences=False))\n    model.add(Dropout(dropout))\n    model.add(Dense(neurons[2],activation='linear'))\n    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n    model.summary()\n    return model\n", "intent": " **Step : 1 - Build the enhanced Model**\n"}
{"snippet": "optimized_lstm_model.fit(\n                        X_train,\n                        y_train,\n                        batch_size=batch_size,\n                        epochs=epochs,\n                        validation_split=validation_split,\n                        verbose=1)\n", "intent": "**Train using Optimized LSTM model**\n"}
{"snippet": "def build_basic_gru_model(layers):\n        model = Sequential()\n        model.add(GRU(2048, input_shape=(layers[0]+1, layers[1]-1), return_sequences=True))\n        model.add(GRU(1024, return_sequences=True))\n        model.add(GRU(512, return_sequences=False))       \n        model.add(Dense(1, activation='linear'))\n        model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n        return model\n", "intent": "**Define the basic GRU model**\n"}
{"snippet": "gru_basic_model = build_basic_gru_model([seq_len, no_of_features])\n", "intent": "**Compile the basic GRU model**\n"}
{"snippet": "gru_basic_model.fit(X_train,\n                    y_train,\n                    epochs=epochs, \n                    validation_split=0.2)\n", "intent": "**Train basic GRU model**\n"}
{"snippet": "gru_df = df\nno_of_features = len(gru_df.columns)\nseq_len= 21\nsplit = 0.8\nlayers = [seq_len, 10]\nepochs = 1\nbatch_size = 100\ngru_opt_model = build_optimized_gru_model([seq_len, no_of_features],0.2)\n", "intent": "**Compile the enhanced GRU model**\n"}
{"snippet": "gru_opt_model.fit(\n                X_train,\n                y_train,\n                batch_size=batch_size,\n                epochs=epochs, \n                validation_split=0.2)\n", "intent": "**Train the enhanced GRU model**\n"}
{"snippet": "ir1.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "gru_df = df\nno_of_features = len(gru_df.columns)\nseq_len= 21\nsplit = 0.8\nlayers = [seq_len, 10]\nepochs = 5\nbatch_size = 100\ngru_opt_model = build_optimized_gru_model([seq_len, no_of_features],0.2)\n", "intent": "**Compile the enhanced GRU model**\n"}
{"snippet": "from sklearn import neighbors\nmodel_knn = neighbors.KNeighborsClassifier(10,weights='uniform')\npredictor_var = ['ApplicantIncome']\nclassification_model(model_knn, df,predictor_var,outcome_var)\n", "intent": "* K Nearest neighbor\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddingMatrix = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    return tf.nn.embedding_lookup(embeddingMatrix, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "drop_out_layer = tf.nn.dropout(flat_x,keep_prob=hold_prob )\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "lda = LdaModel(corpus, id2word=id2word, num_topics=5, passes=10)\n", "intent": "Using the standard model\n"}
{"snippet": "lda_jit = LdaJitModel(corpus, id2word=id2word, num_topics=5, passes=10)\n", "intent": "Faster training time with ``LdaJitModel``, an optimized version of ``LdaModel`` by speeding up critical components of training procedure using Numba.\n"}
{"snippet": "lda_jit = LdaJitModel(corpus, id2word=id2word, num_topics=5, passes=50)\n", "intent": "Train for real (more passes)\n"}
{"snippet": "f ='home_win ~ home_strength + away_strength + home_rest + away_rest'\nres = (sm\n         .Logit\n         .from_formula(f, df)\n         .fit()\n)\n", "intent": "using formulas from ``patsy`` to describe our regression structure\n"}
{"snippet": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2) \nprint(num_ftrs)\nprint(model_ft.parameters())\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "intent": "Loading a pretrained model and reseting final fully connected layer.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(\n        tf.random_uniform(shape=[vocab_size, embed_dim], minval=-1, maxval=1,\n                          name='embedding'))\n    embed = tf.nn.embedding_lookup(\n        embedding, input_data, name='embed')\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "lle_data, _ = manifold.locally_linear_embedding(X, n_neighbors=12,\n                                             n_components=2)\nplot_clustering(lle_data[indices], X[indices], y_num, title='LLE')\n", "intent": "**Excerise 4:** Apply LLE on the data\n"}
{"snippet": "from sklearn import manifold\niso = manifold.Isomap(n_neighbors=6, n_components=2)\niso.fit(X)\nmanifold_data = iso.transform(X)\nplot_clustering(manifold_data, X, y, title='ISOMAP')\n", "intent": "**Excerise 2:** Apply ISOMAP on the data\n"}
{"snippet": "lle_data, _ = manifold.locally_linear_embedding(X, n_neighbors=12,\n                                             n_components=2)\nplot_clustering(lle_data, X, y, title='LLE')\n", "intent": "**Excerise 3:** Apply LLE on the data\n"}
{"snippet": "seq_len = 50 + 1\nsequences = torch.FloatTensor([data[t:t+seq_len] for t in range(len(data)-seq_len)])\nprint('Dimensions: {} x {}'.format(*sequences.shape))\n", "intent": "We first divide the raw dataset into 4950 series with seq_len of 51 (with overlaps). \n"}
{"snippet": "lr.fit(X_train, y_train)\n", "intent": "Now, we can fit the model on the training data:\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(max_depth=2)\ntree.fit(X_train, y_train)\n", "intent": "Let's start by building a very small tree (``max_depth=2``) and visualizing it.\nThe model fitting shouldn't be anything new:\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid=param_grid,\n                    cv=10., return_train_score=True)\ngrid.fit(X_train, y_train)\n", "intent": "Tune the ``max_leaf_nodes`` parameter using ``GridSearchCV``:\n"}
{"snippet": "model.fit(X_train, y_train, epochs=10, verbose=1, validation_split=.1)\n", "intent": "In keras, the ``fit`` method takes options like the number of epochs (iterations) to run, and whether to use a validation set for early stopping.\n"}
{"snippet": "history = model.fit(x_train, y_train, epochs=50, batch_size=100, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "print \"Dimensions of teams DataFrame:\", teams.shape\nprint \"Dimensions of players DataFrame:\", players.shape\nprint \"Dimensions of salaries DataFrame:\", salaries.shape\nprint \"Dimensions of fielding DataFrame:\", fielding.shape\nprint \"Dimensions of master DataFrame:\", master.shape\n", "intent": "We can print the dimensions (i.e. number of rows and columns) for each of the DataFrames.  \n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nlogreg = LogisticRegression()\nCs = np.logspace(-5,1.0,50)\nlogreg_parameters = {\n    'penalty':['l1','l2'],\n    'C': Cs,\n    'solver':['liblinear']\n}\ngrid_log = GridSearchCV(logreg, logreg_parameters, cv =5)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nlogreg = LogisticRegression()\nCs = np.logspace(-5,1.0,50)\nlogreg_parameters = {\n    'penalty':['l1','l2'],\n    'C': Cs,\n    'solver':['liblinear']\n}\ngrid_log_precision = GridSearchCV(logreg, logreg_parameters, scoring='average_precision', cv =5)\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "et.fit(X,y)\n", "intent": "Now it's your turn: repeat the investigation for the extra trees model.\n"}
{"snippet": "k=3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(X,y)\n", "intent": "Run the clustering analysis using scikit-learn.\n*Hint*: Estimate the number of clusters, k, based on your visual examination of the distributions.\n"}
{"snippet": "k = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(Xs)\n", "intent": "Cluster the Data to our our target groups. **BONUS**: Perform multiple clustering tests on various indicator variables\n"}
{"snippet": "kmeans = KMeans(n_clusters=2)\nclusters = kmeans.fit(Xs)\n", "intent": "Set up the k-means clustering analysis. Use the graph from above to derive \"k\"\n"}
{"snippet": "knn = KNeighborsClassifier()\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "model = LinearRegression()\n", "intent": "With the data being split, we'll now create the LinearRegression module. Write the line in the cell below:\n`model = LinearRegression()`\n"}
{"snippet": "lm.fit(X, bos.PRICE)\n", "intent": "The `lm.fit()` function estimates the cofficients the linear regression using least squares. \n"}
{"snippet": "def churn_graph(data, feature):\n    crosstab=pd.crosstab(data[feature],data_all['is_churn'])\n    crosstab.plot(kind='bar', stacked=True)\n    crosstab[\"Ratio\"] =  crosstab[1] / (crosstab[0]+ crosstab[1])\n    print (crosstab)\n", "intent": "Data exploration on training data\n"}
{"snippet": "churn_graph(data_all, feature='last_trans_to_expire_day')\n", "intent": "Looks like give 100% discount is not a good idea.\n"}
{"snippet": "churn_graph(data_all, feature='membership_length')\n", "intent": "Only 3% of churn customers have negative value, so I am going to exclude negative last_trans_to_expire_day\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators =100,  max_features='log2', max_depth=5)\n", "intent": "Train the model with the best params\n"}
{"snippet": "lda = LDA(n_components=2)\nlabels = digits.target\nlda.fit(flat, labels)\nreduced_data_lda = lda.transform(flat)\n", "intent": "**Now do the same as for PCA but now project onto 2 LDA components to get the following:**\n**Marks: 0**\n"}
{"snippet": "model = KMeans(n_clusters=n_digits, init='k-means++')\nmeans = model.fit(lda_digit_train)\nconfusion_matrix_stats(means, lda_digit_test, labels_test, ['N/A']*10)\n", "intent": "Then we fit a k-means model to the training data and have a look at the models accuracy.\n"}
{"snippet": "km = KM(codes=10,itr=2)\nkm.fit(X_train_flat)\nkm_cluster_centers = km.get_means\n", "intent": "**Unsupervised k-means:**\n"}
{"snippet": "from keras.callbacks import ModelCheckpoint   \ncheckpointer = ModelCheckpoint(filepath='imdb.model.best.hdf5', \n                               verbose=1, save_best_only=True)\nhist = model.fit(x_train, y_train, batch_size=100, epochs=50,\n          validation_split=0.2, callbacks=[checkpointer],\n          verbose=1, shuffle=True)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "scaler.fit(project_data.drop('TARGET CLASS', axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "lm = LinearRegression()\nlm.fit(X[['PTRATIO']], bos.PRICE)\n", "intent": "Try fitting a linear regression model using only the 'PTRATIO' (pupil-teacher ratio by town)\nCalculate the mean squared error. \n"}
{"snippet": "RF_clf = RandomForestClassifier(random_state=seed)\nRF_clf\n", "intent": "Train and evaluate the Random Forest Classifier with Cross Validation\n"}
{"snippet": "RF_clf.fit(X_train, y_train)\n", "intent": "Fitting the Random Forest Classifier on training set\n"}
{"snippet": "import tensorflow as tf\ndef get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = build_model(40, verbose = True)\n", "intent": "By trial and error it was found that a hidden layer with 3 nodes produced a reasonable model.\n"}
{"snippet": "model = build_model(3, verbose = True)\n", "intent": "By trial and error it was found that a hidden layer with 3 nodes produced a reasonable model.\n"}
{"snippet": "Cs = np.logspace(-6, -1, 10)\nclf = model_selection.GridSearchCV(estimator=svm.LinearSVC(), param_grid=dict(C=Cs), n_jobs=-1)\nclf.fit(X_train, Y_train)\nprint(clf.best_score_)\nprint(clf.best_estimator_.C)\nprint(\"Score: %s\" % clf.score(X_test, Y_test))\n", "intent": "retrain the model by tuning the parameters\n"}
{"snippet": "rfc = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_features='sqrt', oob_score=True)\nparam_grid = { \n    'n_estimators': [200, 700],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\nclf = model_selection.GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nclf.fit(X_train, Y_train)\nprint(clf.best_params_)\nclf.fit(X_train,Y_train)\nprint(\"Score: %s\" % clf.score(X_test, Y_test))\n", "intent": "train a model using random forest classifier\n"}
{"snippet": "from sklearn.cluster import KMeans\nnum_clusters =4\nkmeans_pca = KMeans(num_clusters).fit(stats_PCA)\n", "intent": "Finally, let's apply clustering once more, this time using the sklearn library.\n"}
{"snippet": "from sklearn.cluster import KMeans\nnum_clusters =3\nkmeans_pca = KMeans(num_clusters).fit(stats_PCA)\n", "intent": "Finally, let's apply clustering once more, this time using the sklearn library.\n"}
{"snippet": "modelW0 = sm.OLS(y, X)\nresultsW0 = modelW0.fit()\nprint resultsW0.summary()\n", "intent": "Now let's fit a linear regression model with an intercept. \n"}
{"snippet": "class Sigmoid(Activation):\n    def __init__(self):\n        super().__init__()\n    def function(self,z):\n        denominator = (1 + np.exp(-z))\n        self.x = 1/denominator\n        return self.x\n    def derivative(self,z = None):\n        self.dx = self.x * (1 - self.x)\n        return np.multiply(z,self.dx)\n", "intent": "Let's define at least three activation functions. Let's start with sigmoid, which we already know by heart.\n"}
{"snippet": "class TanH(Activation):\n    def __init__(self):\n        super().__init__()\n    def function(self,z):\n        denominator =  1 + np.exp(-2 * z)\n        self.x = (2 / denominator) - 1\n        return self.x\n    def derivative(self,z):\n        self.dx = 1 - self.x ** 2\n        return np.multiply(z,self.dx)\n", "intent": "Now let's go with another familiar one, tanh.\n"}
{"snippet": "import tensorflow as tf\nhello_constant = tf.constant('Hello World!')\nwith tf.Session() as sess:\n    output = sess.run(hello_constant)\n    print(output)\n", "intent": "TensorFlow hello world program\n"}
{"snippet": "if gpus <= 1:\n    model = create_conv_model()\nelse:\n    with tf.device(\"/cpu:0\"):\n        model = create_conv_model()\n    model = multi_gpu_model(model, gpus=gpus)\n", "intent": "if more than one GPU is present on the machine we need to create a copy of the model on each GPU and sync them on the CPU\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "from pyspark.ml.classification import GBTClassifier\ngb = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\npipeline = Pipeline(stages=[gb])\ngbModel = pipeline.fit(trainingData)\n", "intent": "1. Train a GBTClassifier on the training data, call the trained model 'gbModel'\n"}
{"snippet": "from sklearn.svm import SVC\nclf = SVC(kernel='linear')\nclf.fit(X, y)\n", "intent": "**Let's Try**\nFit the model:\n"}
{"snippet": "Param_grid_Lr={\"Scaler__with_std\":[True,False],\"LR__C\":np.logspace(-5,5,11),\"LR__penalty\":['l2'],\"LR__class_weight\":[\"balanced\"]}\nbest_Lr_recall=GridSearchCV(Pipe_Lr,Param_grid_Lr,cv=4,verbose=1,n_jobs=1,scoring='recall')\nbest_Lr_recall.fit(Train_X,Train_Y)\n", "intent": "Lets Now run the same procedure with recall as the optimization parameter\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(500, input_dim=x_train.shape[1]))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "import statsmodels.formula.api as smf\npollster_anova = smf.ols(formula=\"bias ~ pollster - 1\", data=tmppolls).fit()\npollster_anova.summary()\n", "intent": "We can also explore these biases using a linear regression to explain `bias in terms of the categorical variable `pollster`.\n"}
{"snippet": "nodes = [2**n for n in range(6,12)]\nlosses = run_model(nodes)\n", "intent": "Varying the amount of nodes in each of the hidden layers\n"}
{"snippet": "hidden_scores = [i for i in range(1, 6)]\nlayer_losses = run_model(hidden_layers)\n", "intent": "Varying the amount of hidden layers\n"}
{"snippet": "dropout_score = mlp_model(1, dropout=False)\nprint('Score without dropout: {}'.format(dropout_score))\n", "intent": "Removing dropout layers\n"}
{"snippet": "activation_score = mlp_model(1, activation=False)\nprint('Score without dropout: {}'.format(activation_score))\n", "intent": "Score is slightly lower without drop out\n"}
{"snippet": "batch_size = [2**n for n in range(5, 12)]\nbatch_scores = run_model(batch_size)\nplot_scores(batch_scores['nodes'], batch_scores['scores'], 'batch_size', 'scores')\n", "intent": "WOOH now that's a huge drop\n"}
{"snippet": "best_model = mlp_model(1)\nprint('Final score: {}'.format(best_model))\n", "intent": "Running the model with all the best parameters\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, input_dim=x_train[-1].shape[0], activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metric=[\"accuracy\"])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, batch_size=512, epochs = 125, validation_split=0.2, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model.fit(Xsmall, ysmall, batch_size=256, epochs=40,verbose = 1)\nmodel.save_weights('model_weights/best_RNN_small_textdata_weights.hdf5')\n", "intent": "Now lets fit our model!\n"}
{"snippet": "k = 1 \nclassifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors=k)\nclassifier.fit(x,y)\n", "intent": "\"train\" a KNN Classifier\n========================\n*** Why is this not really training? ***\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state=0)\nmodel\n", "intent": "Let's use a Decision tree model\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(random_state=0)\nmodel.fit(X_train, y_train)\nmy_score = model.score(X_test, y_test)\nprint(\"Classification Score: %0.3f\" % my_score)\nprint(\"Benchmark Score: %0.3f\" % benchmark_accuracy)\n", "intent": "Let's try with a Random Forest Classifier\n"}
{"snippet": "model = RandomForestClassifier(random_state=0)\nmodel.fit(X_train, y_train)\nmy_score = model.score(X_test, y_test)\nprint(\"Classification Score: %0.3f\" % my_score)\nprint(\"Benchmark Score: %0.3f\" % benchmark_accuracy)\n", "intent": "Random Forest Classifier\n"}
{"snippet": "scaler.fit(bnd.drop('Class',axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "scaler.fit(knn_data.drop('TARGET CLASS',axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "mimetic_digraph = build_graph(mimetic_digraph, user_id, mimetic_following_set, mimetic_followers_set, level=0)\nstore_objects(mimetic_digraph=mimetic_digraph)\n", "intent": "And store the graph as a pickle so I don't have to keep on recreating it.\n"}
{"snippet": "def polynomial_ridge_regression(data, deg, l2_penalty):\n    feature_cols = []\n    for i in xrange(deg):\n        feature_cols.append('X' + str(i+1))\n    poly_data = polynomial_features(data,deg)\n    X = poly_data[feature_cols]\n    y = poly_data.Y\n    model = Ridge(alpha = l2_penalty)\n    model.fit(X, y) \n    return model\n", "intent": "Function to solve the ridge objective for a polynomial regression model of any degree:\n"}
{"snippet": "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price') \nmy_weights = np.array([1., 10.])\ntest_predictions = predict_output(example_features, my_weights) \nerrors = test_predictions - example_output \nprint feature_derivative_ridge(errors, example_features[:,1], my_weights[1], 1, False)\nprint np.sum(errors*example_features[:,1])*2+20.\nprint ''\nprint feature_derivative_ridge(errors, example_features[:,0], my_weights[0], 1, True)\nprint np.sum(errors)*2.\n", "intent": "To test your feature derivartive run the following:\n"}
{"snippet": "best_model = linear_model.Lasso(alpha=10)\nX = training[all_features]\ny = training['price']\nbest_model.fit(X, y)\nprint best_model.coef_\nprint best_model.intercept_\n", "intent": "Notice best l1_penalty = 10\nGet the coefficents and intercepts of the corresponding model.\n"}
{"snippet": "from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn. ensemble import VotingClassifier\nlr = LogisticRegression()\ndt = DecisionTreeClassifier()\nsvm = SVC(kernel = 'poly', degree = 2 )\nevc = VotingClassifier( estimators= [('lr',lr),('dt',dt),('svm',svm)], voting = 'hard')\nevc.fit(X_train,y_train)\nevc.score(X_test, y_test)\n", "intent": "voting for classification \n"}
{"snippet": "clf = cluster.KMeans(init='k-means++', n_clusters=3, random_state=33)\nclf.fit(X_train4)\n", "intent": "Now, calculate the clusters, using the four attributes\n"}
{"snippet": "def regression_model():\n    model = Sequential()\n    model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(50, activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model\n", "intent": "Let's define a function that defines our regression model for us so that we can conveniently call it to create our model.\n"}
{"snippet": "tensor_example = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\nprint(\"What is the value on 1st-row first two columns? \", tensor_example[0, 0:2])\nprint(\"What is the value on 1st-row first two columns? \", tensor_example[0][0:2])\n", "intent": "Let us see how  we use slicing with 2D tensors to get the values in the above picture.\n"}
{"snippet": "tensor_example = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\nprint(tensor_example)\nprint(\"What is the value on 3rd-column last two rows? \", tensor_example[1:3, 2])\n", "intent": "Let's see the code below.\n"}
{"snippet": "X = torch.tensor([[1, 0],[0, 1]]) \nY = torch.tensor([[2, 1],[1, 2]])\nX_plus_Y = X + Y\nprint(\"The result of X + Y: \", X_plus_Y)\n", "intent": "Let us see how tensor addition works with <code>X</code> and <code>Y</code>.\n"}
{"snippet": "Y = torch.tensor([[2, 1], [1, 2]]) \ntwo_Y = 2 * Y\nprint(\"The result of 2Y: \", two_Y)\n", "intent": "Let us try to calculate the product of <b>2Y</b>.\n"}
{"snippet": "X = torch.tensor([[1, 0], [0, 1]])\nY = torch.tensor([[2, 1], [1, 2]]) \nX_times_Y = X * Y\nprint(\"The result of X * Y: \", X_times_Y)\n", "intent": "The code below calculates the element-wise product of the tensor <strong>X</strong> and <strong>Y</strong>:\n"}
{"snippet": "A = torch.tensor([[0, 1, 1], [1, 0, 1]])\nB = torch.tensor([[1, 1], [1, 1], [-1, 1]])\nA_times_B = torch.mm(A,B)\nprint(\"The result of A * B: \", A_times_B)\n", "intent": "We use <code>torch.mm()</code> for calculating the multiplication between tensors with different sizes.\n"}
{"snippet": "print(\"Type of the first element: \", type(dataset[0]))\nprint(\"The length of the tuple: \", len(dataset[0]))\nprint(\"The shape of the first element in the tuple: \", dataset[0][0].shape)\nprint(\"The type of the first element in the tuple\", type(dataset[0][0]))\nprint(\"The second element in the tuple: \", dataset[0][1])\nprint(\"The type of the second element in the tuple: \", type(dataset[0][1]))\nprint(\"As the result, the structure of the first element in the dataset is (tensor([1, 28, 28]), tensor(7)).\")\n", "intent": "Each element of the dataset object contains a tuple. Let us see whether the first element in the dataset is a tuple and what is in it.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -.1, .1), name='embedding')\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "rf_model = rf_classifier.fit(X_train, y_train)\n", "intent": "Now we fit the model on our training data.\n"}
{"snippet": "param_grid = {'min_samples_split': range(2,10),\n              'min_samples_leaf': range(1,10)}\nmodel_rf = GridSearchCV(ensemble.RandomForestClassifier(n_estimators=10), param_grid, cv=3, iid=False)\nmodel_rf.fit(X_train, y_train)\nbest_index = np.argmax(model_rf.cv_results_[\"mean_test_score\"])\nprint(\"Best parameter values:\", model_rf.cv_results_[\"params\"][best_index])\nprint(\"Best Mean cross-validated test accuracy:\", model_rf.cv_results_[\"mean_test_score\"][best_index])\nprint(\"Overall Mean test accuracy:\", model_rf.score(X_test, y_test))\n", "intent": "Let's do another grid search to determine the best parameters:\n"}
{"snippet": "from sklearn import linear_model\nlin_reg = linear_model.LinearRegression(n_jobs=1)  \n", "intent": "We'll start with a basic [OLS linear regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n"}
{"snippet": "ridge_cv = linear_model.RidgeCV(alphas=alphas,\n                               normalize=False,\n                               store_cv_values=True)\nridge_cv.fit(X_train, y_train);\nprint('Selected Alpha:', ridge_cv.alpha_)\n", "intent": "By default the `RidgeCV` uses \"Leave One Out Cross Validation\" (LOOCV). Let's fit the Ridge model\n"}
{"snippet": "kmeans = KMeans(n_clusters=3,\n               max_iter=300 \n               ).fit(X)\n", "intent": "Now we can create the model. Notice how we are chaining the 'fit' function onto the model instantiation. \n"}
{"snippet": "from sklearn.cluster import AgglomerativeClustering\nward = AgglomerativeClustering(n_clusters=3,\n                               linkage='ward', \n                               affinity='euclidean') \n", "intent": "We'll use two clusters this time, and use ward linkage.\n"}
{"snippet": "dbscan = DBSCAN(eps=0.2)\ndbscan.fit(blobs)\n", "intent": "Now let's fit another DBSCAN model to the blobs data.\n"}
{"snippet": "from tpot import TPOTRegressor\ntpot = TPOTRegressor(generations=5, population_size=20, verbosity=1, scoring='r2')  \ntpot.fit(X_train, y_train.ravel())\nprint(tpot.score(X_test, y_test.ravel()))\ntpot.export('tpot_heart_pipeline.py')\n", "intent": "Now TPOT will make the model.\n"}
{"snippet": "input_dim = x_train.shape[1]\nmodel = Sequential()\nmodel.add(Dense(2, input_dim=input_dim))\nmodel.add(Activation(\"sigmoid\"))\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "test_input = [[0.25, 0.15]]\ngraph=tf.Graph()\nwith tf.Session() as session:\n    tf.global_variables_initializer().run(session=session)\n    output = session.run([out],feed_dict=feed_dict)\n    print(output[0]) \n", "intent": "As with the previous example, we can use `session.run()` to execute this computation graph, and use a `feed_dict` to feed in our input:\n"}
{"snippet": "x = torch.randn((2, 2))\ny = torch.randn((2, 2))\nz = x + y  \nvar_x = autograd.Variable(x)\nvar_y = autograd.Variable(y)\nvar_z = var_x + var_y\nprint(var_z.grad_fn)\nvar_z_data = var_z.data  \nnew_var_z = autograd.Variable(var_z_data)\nprint(new_var_z.grad_fn)\n", "intent": "Understanding what is going on in the block below is crucial for being a\nsuccessful programmer in deep learning.\n"}
{"snippet": "happyModel.fit(x=X_train, y=Y_train, epochs=20, batch_size=32, validation_data=(X_test, Y_test))\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "clf = RandomForestClassifier(n_estimators = 100, max_features = 'auto', n_jobs=-1, max_depth = 3, class_weight = 'balanced')\nclf.fit(features_train, target_train)\n", "intent": "Train the random forest classifier\n"}
{"snippet": "from sklearn.svm import SVC\nclf = SVC(kernel='linear', C=1000000000)\nclf.fit(X, y)\n", "intent": "By reducing C, we are penalizing the errors less\n"}
{"snippet": "logreg = LogisticRegression(C=gs.best_params_['C'],\\\n                            penalty=gs.best_params_['penalty'])\ncv_model = logreg.fit(X_train, y_train)\n", "intent": "Compared to my score for the vanilla logistic regression of .78 the gridsearchcv is 2% better (.80).\n"}
{"snippet": "z_sample = T.matrix()\ngen_fn = theano.function([z_sample], generated_x)\n", "intent": "This task requires deeper Lasagne knowledge. You need to perform inference from $z$, reconstruct an image given some random $z$ representations.\n"}
{"snippet": "n_steps = T.scalar(dtype='int32')\nfeedback_loop = Recurrence(\n    state_variables={step.h_new:step.h_prev,\n                     step.next_token:step.inp},\n    tracked_outputs=[step.next_token_probas,],\n    batch_size=theano.shared(1),\n    n_steps=n_steps,\n    unroll_scan=False,\n)\n", "intent": "here we re-wire the recurrent network so that it's output is fed back to it's input\n"}
{"snippet": "n_steps = T.scalar(dtype='int32')\nfeedback_loop = Recurrence(\n    state_variables=<...>,\n    tracked_outputs=<...>,\n    batch_size=theano.shared(1),\n    n_steps=n_steps,\n    unroll_scan=False,\n)\n", "intent": "here we re-wire the recurrent network so that it's output is fed back to it's input\n"}
{"snippet": "model.fit(X_train, X_train, nb_epoch=5, batch_size=258)\n", "intent": "Note that 784 dimension is reduced through encoding to 144 in the hidden layer and again in layer 3 constructed back to 784 using decoder.\n"}
{"snippet": "k_values = [1,2,5,10,20] \nknn_performance = [] \nfor kk in k_values:\n    knn_model = neighbors.KNeighborsClassifier(n_neighbors = kk)\n", "intent": "Plot the performance of the kNN classifier for different values of `k`. For which `k` is best performance achieved?\n"}
{"snippet": "basic_pipe = Pipeline([('bt', bt), ('ridge', Ridge())])\nbasic_pipe.fit(train, y)\nbasic_pipe.score(train, y)\n", "intent": "Our transformer can be part of pipeline.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1000))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(256))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(2))\nmodel.add(Activation(\"sigmoid\"))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "history = model.fit(x_train, y_train,\n                    epochs=20,\n                    verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "history = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from keras.callbacks import ModelCheckpoint  \ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best_adamax.ResNet50.hdf5', \n                               verbose=1, save_best_only=True)\nepochs = 30\nbatch_size = 64\nResNet_model.fit(train_ResNet50, train_targets, \n          validation_data=(valid_ResNet50, valid_targets),\n          epochs=epochs, batch_size=batch_size, callbacks=[checkpointer], verbose=1)\n", "intent": "Here we train the model in the code cell below.\n"}
{"snippet": "print(x_train.shape)\nprint(y_train.shape)\nmodel.fit(x_train, y_train, epochs=20, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "LR = LogisticRegression(C=0.01)\nLR.fit(X_train,y_train)\n", "intent": "Lets build our model:\n"}
{"snippet": "lr.fit(X_train[[\"PROF\"]],y_train)\nlr.score(X_test[[\"PROF\"]], y_test)\n", "intent": "We can fit the model and calculate the R^2:\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=10)\nmodel.fit(X, y)\nmodel.score(X, y)\n", "intent": "- Initialize kNN, fit the model on all data, and compute the accuracy of your model.\n"}
{"snippet": "lr.fit(X_train, y_train)\nlr.score(X_test, y_test)\n", "intent": " We can also use multiple linear regression, the R^2 is approximately 0.155.\n"}
{"snippet": "lr.fit(Z_train, y_train)\nlr.score(Z_test, y_test)\n", "intent": " We can fit the model and calculate the R^2:\n"}
{"snippet": "rr = Ridge(alpha=0.1,normalize=True)\nrr.fit(X, y)\n", "intent": " We create a Ridge Regression object and fit the model:\n"}
{"snippet": "Alpha=[0.001, 0.001,0.01,0.1,1]\nRsq_ridge=[]\nfor a in Alpha:\n    rr = Ridge(alpha=a,normalize=True)\n    rr.fit(X_train, y_train)\n    print(rr.score(X_test, y_test))\n    Rsq_ridge.append(rr.score(X_test, y_test))\n", "intent": "We test the R^2  for different Alpha: \n"}
{"snippet": "Rsq_ridge_poly=[]\nfor a in Alpha:\n    clf = Ridge(alpha=a,normalize=True)\n    clf.fit(Z_train, y_train)\n    Rsq_ridge_poly.append(clf.score(Z_test, y_test))\n", "intent": " We can perform the same set of operations with polynomial features:  \n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedded = tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "X, actual, y = generate_sample_data(20, true_function)\nlr = LinearRegression()\nlr.fit(X,y)\ncompare_fitted_function(X , y, actual, lr)\n", "intent": "Generate some data, fit a linear regression model, and plot the results.\n"}
{"snippet": "degree = 4\npoly = PolynomialFeatures(degree = degree, include_bias=False)\nlr = LinearRegression(fit_intercept=True)\npipeline = Pipeline([(\"polynomial_features\", poly),\n                     (\"linear_regression\", lr)])\npipeline.fit(X, y)\ncompare_fitted_function(X, y, true_function(X), pipeline)\n", "intent": "We can use sklearn's Pipeline framework to connect the polynomial expansion and the linear regression into one 'model'\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\ndt = DecisionTreeRegressor()\nparameter_grid = {\n    'min_samples_leaf': [1, 2, 5, 10, 20, 30, 40, 50, 60, 70, 80],\n    'max_depth': [3, 4, 5]}\nparam_searcher = GridSearchCV(dt, parameter_grid, cv=10)\n", "intent": "Sklearn doesn't allow _post-pruning_ of decision trees, but we can try some pre-pruning parameters\n"}
{"snippet": "model = LinearRegression(fit_intercept=False)\nprint model.fit(X, y).score(X, y)\nprint model.intercept_\nprint model.coef_\n", "intent": "Note that the first coefficient is zero, since we already fitted an intercept. \nAlternatively, we can do:\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=3)\nkm.fit(X)\n", "intent": "Use KMeans to segment the iris data into two clusters\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=2)\nkm.fit(X)\n", "intent": "Use KMeans to segment the iris data into two clusters\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Let's see how to train a model and save its weights.\nFirst start with a model:\n"}
{"snippet": "tf.reset_default_graph() \nx = tf.placeholder(tf.float32, (None, 32, 32, 3))\ny = tf.placeholder(tf.int32, (None))\none_hot_y = tf.one_hot(y, 43)\n", "intent": "x is a placeholder for a batch of input images. y is a placeholder for a batch of output labels.\nYou do not need to modify this section.\n"}
{"snippet": "model = xgb.XGBClassifier(max_depth=19,\n                        subsample=0.8325428224507427,\n                          min_child = 6,\n                        objective='binary:logistic',\n                        n_estimators=50,\n                        learning_rate = 0.1)\neval_set = [(train_X, train_Y), (test_X, test_Y)]\nmodel.fit(train_X, train_Y.values.ravel(), early_stopping_rounds=15, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)\n", "intent": "x_max_depth': 19, 'x_min_child': 6, 'x_subsample': 0.8325428224507427\n"}
{"snippet": "model.add(Dense(num_classes, activation='softmax',kernel_initializer='normal'))\n", "intent": "Create and add in the output layer to our model. Use 'softmax' activation and a normal kernel_initializer.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train,\n          batch_size=32,\n          epochs=10,\n          validation_data=(x_test, y_test), \n          verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "kmeans = KMeans(n_clusters=5,random_state=0).fit(X)\nkmeans.labels_\nkmeans\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "model = sm.ols(formula=\"sl ~ sx + yr\", data=data).fit()\nmodel.summary()\n", "intent": "We can use categorical features as well, and `statsmodels` will use `patsy` for that just as we have been before.\n"}
{"snippet": "kmeans = KMeans(n_clusters=5,random_state=0).fit(X)\nkmeans.labels_\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n    embedded = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "outputs, states = tf.nn.dynamic_rnn(cell,x,dtype=tf.float32)\n", "intent": "** Now pass in the cells variable into tf.nn.dynamic_rnn, along with your first placeholder (X)**\n"}
{"snippet": "from sklearn import svm\nfrom sklearn.metrics import accuracy_score\nclf=svm.SVC(gamma=0.001, C=100)\n", "intent": "I followed this [scikit-learn example](http://scikit-learn.org/stable/tutorial/basic/tutorial.html\n"}
{"snippet": "from sklearn import datasets, neighbors, linear_model\ndef train_models(train_size, train_vector, train_labels):\n    train_vector_small=train_vector[:train_size,:]\n    train_labels_small=train_labels[:train_size]\n    clf=svm.SVC(gamma=0.001, C=100)\n    logistic = linear_model.LogisticRegression()\n    clf.fit(train_vector_small, train_labels_small)\n    logistic.fit(train_vector_small, train_labels_small)\n    return clf, logistic\n", "intent": "Now define a function that trains the model for a given training size\n"}
{"snippet": "def softmax(x):\n    return np.exp(x)/np.sum(np.exp(x), axis=0)\n", "intent": "Define the softmax function (generalization of logistic)\n$$\\sigma(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$$\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n                                            hidden_units=[10, 20, 10],\n                                            n_classes=2,\n                                            model_dir=\"./project_output\")\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.1))\n    embed = tf.nn.embedding_lookup(embedding,  input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nprint(x_train.shape)\nmodel.add(Dense(512,activation = 'relu', input_shape = (1000,)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation  = 'softmax'))\nmodel.compile(loss= 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "print model.fit(X, y).score(X, y)\nprint model.fit(X_scaled, y).score(X_scaled, y)\n", "intent": "Note that this does not affect the modeling performance at all\n"}
{"snippet": "kmeans.fit(data.ix[:,2:])\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "data_features = data.drop('TARGET CLASS', axis=1)\nscaler.fit(data_features)\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "knn.fit(x_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "def soft_threshold(rho,lamda):\n    if rho < - lamda:\n        return rho + lamda\n    elif rho > lamda:\n        return rho - lamda\n    else: \n        return 0\nlamda = 3    \ny_st = [soft_threshold(xii,lamda) for xii in x1]\n", "intent": "$ S(\\rho_j, \\lambda)$\n"}
{"snippet": "lambda_range = np.logspace(0,2,num = 100)/1000\ntheta_0_list_reg_l1 = []\ntheta_1_list_reg_l1 = []\nfor l in lambda_range:\n    model_sk_reg = linear_model.Lasso(alpha=l, fit_intercept=False)\n    model_sk_reg.fit(X,y_noise)\n    t0, t1 = model_sk_reg.coef_\n    theta_0_list_reg_l1.append(t0)\n    theta_1_list_reg_l1.append(t1)\n", "intent": "Note we are using Sklearn as there is no closed form solution\n"}
{"snippet": "happyModel.fit(x = X_train, y = Y_train, epochs = 20, batch_size = 16)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size,embed_dim],-1,1))\n    embedded = tf.nn.embedding_lookup(embedding,input_data)\n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "dropout_layer = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "model.fit(X, data.Insult)  \ncoef = pd.Series(model.coef_[0], index=cv.get_feature_names())  \ncoef.sort()  \n", "intent": "Let's look at the most important features.\n"}
{"snippet": "import tensorflow as tf\nhello = tf.constant('It works!')\nname = tf.constant(\"My name is .\")\nsess = tf.Session()\nprint(sess.run(hello))\nprint(sess.run(name))\n", "intent": "This snippet of Python creates a simple graph.\n"}
{"snippet": "lm1 = smf.ols(formula='sales ~ TV', data=data).fit()\nlm1.params\n", "intent": "Let's estimate the model coefficients for the advertising data:\n"}
{"snippet": "model.fit(X_train, newsgroups_train.target)\n", "intent": "Let's fit the model and analyze the coeficients.\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\nX = df.drop('class', axis =1)\ny = df['class']\ngnb.fit(X,y)\nprint(gnb.score(X,y))\n", "intent": "Fit a Gaussian Naive Bayes model to the original dataset A (including the outliers). Display the classification accuracy on the training dataset.\n"}
{"snippet": "kmeans = KMeans(n_clusters=5, random_state=1).fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "slr = LinearRegression() \nslr.fit(X,y)\nprint('Intercept:', slr.intercept_) \nprint('Slope:', slr.coef_)\n", "intent": "We will now use Scikit-Learn's `LinearRegression` module to create the slr model. \n"}
{"snippet": "X = data.iloc[:,0:2] \ny = data.iloc[:,2] \nmlr = LinearRegression() \nmlr.fit(X,y)\nprint(mlr.coef_)\nprint(mlr.intercept_)\nprint(mlr.score(X,y))\n", "intent": "We can use the `LinearRegression` class from Scikit-Learn to create multiple regression models.\n"}
{"snippet": "X = mpg.iloc[:,1:6]\nprint(X.head())\ny = mpg.iloc[:,0]\nprint(X.shape)\nmpg_lm = LinearRegression()\nmpg_lm.fit(X,y)\nprint(mpg_lm.intercept_)\nprint(mpg_lm.coef_)\nprint(mpg_lm.score(X,y))\n", "intent": "We will create a regression model using columns 1 through 5 as features. \n"}
{"snippet": "X = x.reshape(len(x),1) \nlinearMod = LinearRegression() \nlinearMod.fit(X,y)\nprint(\"Intercept: \", linearMod.intercept_)\nprint(\"Coefficeint: \", linearMod.coef_)\nprint(\"Training r^2: \", linearMod.score(X,y))\n", "intent": "We will start by using Scikit-Learn to produce a linear model.\n"}
{"snippet": "knn_mod = KNeighborsRegressor(8)\nknn_mod.fit(sX_train, y_train)\nprint('--- KNN Model (K=8) ---')\nprint('Training r2:', knn_mod.score(sX_train, y_train))\nprint('Testing r2: ', knn_mod.score(sX_test, y_test))\n", "intent": "It appears that any *K* in the range from about 8 to 20 would be good. \n"}
{"snippet": "lr_mod = LogisticRegression()\nlr_mod.fit(X_train, y_train)\nprint('Training Accuracy:', lr_mod.score(X_train, y_train))\nprint('Testing Accuracy:', lr_mod.score(X_test, y_test))\n", "intent": "When creating a Logistic Regression mode, there will be no hyper-parameters that you are asked to specify. \n"}
{"snippet": "knn_mod = KNeighborsClassifier(n_neighbors=6)\nknn_mod.fit(X_train, y_train)\nprint('Training Accuracy:', knn_mod.score(X_train, y_train))\nprint('Testing Accuracy:', knn_mod.score(X_test, y_test))\n", "intent": "When creating a KNN model, you will be provided with the number of neighbors to use in the algorithm. No other hyper-parameters will be used. \n"}
{"snippet": "model = DecisionTreeClassifier()\n", "intent": "We'll start training one decision tree.\n"}
{"snippet": "ols_mod = LinearRegression()\nols_mod.fit(X_train, y_train)\nprint('Training r-squared:', ols_mod.score(X_train, y_train))\nprint('Testing r-squared:', ols_mod.score(X_test, y_test))\n", "intent": "With ordinary least squares regression (i.e. regular linear regression), there are no hyper-parameters to specify. \n"}
{"snippet": "knn_mod = KNeighborsRegressor(n_neighbors=10)\nknn_mod.fit(X_train, y_train)\nprint('Training r-squared:', knn_mod.score(X_train, y_train))\nprint('Testing r-squared:', knn_mod.score(X_test, y_test))\n", "intent": "When creating a KNN model, you will be provided with the number of neighbors to use in the algorithm. No other hyper-parameters will be used. \n"}
{"snippet": "rtree_mod = DecisionTreeRegressor(max_depth=8, min_samples_leaf=5)\nrtree_mod.fit(X_train, y_train)\nprint('Training r-squared:', rtree_mod.score(X_train, y_train))\nprint('Testing r-squared:', rtree_mod.score(X_test, y_test))\n", "intent": "You will be asked to specifty two parameters for the decision tree model: `max_depth` and `min_samples_leaf`. These values will be provided to you.\n"}
{"snippet": "Xorig, y_orig = getFeaturesAndOutput(partA, 'class')\nprint Xorig.shape\nprint y_orig.shape\nclsOrig = MultinomialNB()\nclsOrig.fit(Xorig, y_orig)\n", "intent": "Fit a Gaussian Naive Bayes model to the original dataset A (including the outliers). Display the classification accuracy on the training dataset.\n"}
{"snippet": "dt = DecisionTreeClassifier(criterion='entropy', max_depth=2)\ndt\n", "intent": "MAKE SURE YOU NAME your tree classifier as \"dt\"\n"}
{"snippet": "linearSVC = LinearSVC()\n", "intent": "Train a [`LinearSVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n"}
{"snippet": "rbfSVC = SVC(kernel='rbf')\n", "intent": "By using the [`SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"}
{"snippet": "def scorePolySVC(degree=2):\n    polySVC = SVC(kernel='poly',degree=degree).fit(Xtrain, yTr)\n    return polySVC.score(Xtrain, yTr), polySVC.score(Xvalid, yVal)\n", "intent": "The transformation of the data using RBF has improved the classification accuracy\n"}
{"snippet": "Xorig, y_orig = getFeaturesAndOutput(partA, 'class')\nprint Xorig.shape\nprint y_orig.shape\nclsOrig = MultinomialNB()\nclsOrig.fit(Xorig, y_orig)\n", "intent": "<span style=\"color:red\">wrong, see my comment for 2.3\n"}
{"snippet": "model = RandomForestClassifier(n_estimators=20)\n", "intent": "Let's try a small forest with a number of trees.\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 4))\nnet.add(ReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\nprint(net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "a = 3\ndef testGlobalVariable():\n    print(a)\ntestGlobalVariable()\n", "intent": "* An instance variable is visible to all functions, constructors and blocks, within the file that it is declared in.\n"}
{"snippet": "history = model.fit(x_train, y_train, epochs=epocs, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "import tensorflow as tf\ninput = tf.placeholder(tf.float32, (None, 32, 32, 3))\nfilter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) \nfilter_bias = tf.Variable(tf.zeros(20))\nstrides = [1, 2, 2, 1] \npadding = 'SAME'\nconv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias\n", "intent": "Usando TensorFlow ficaria:\n"}
{"snippet": "ss.fit(bank_data.drop('Class', axis = 1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "km.fit(data.drop('Private', axis = 1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "features = ['XVPM', 'GWYH', 'TRAT', 'TLLZ', 'IGGA', 'HYKR', 'EDFS', 'GUUB', 'MGJM','JHZC']\nscaler.fit(df[features])\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "knn = KNeighborsClassifier()\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "knn.fit(train_x, train_y)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "C = 1.0  \nmodels = [\n    ['SVC with linear kernel', svm.SVC(kernel='linear', C=C).fit(X, y)],\n    ['LinearSVC (linear kernel)', svm.LinearSVC(C=C).fit(X, y)],\n    ['SVC with RBF kernel', svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)],\n    ['SVC with polynomial (degree 3) kernel', svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)]]\n", "intent": "Train models. We create an instance of SVM and fit out data. We do not scale our data since we want to plot the support vectors.\n"}
{"snippet": "nb.fit(train_x, train_y)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "clf1 = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_split=2)\nclf2 = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_split=90)\nclf1.fit(X_train, y_train)\nclf2.fit(X_train, y_train)\nacc_min_samples_split_2 = clf1.score(X_test, y_test)\nacc_min_samples_split_50 = clf2.score(X_test, y_test)\nprint(\"Accuracy with min split = 2:\", acc_min_samples_split_2)\nprint(\"Accuracy with min split = 100:\", acc_min_samples_split_50)\n", "intent": "Try out a different min samples split and see the difference\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dropout(0.2, input_shape = (1000,)))\nmodel.add(Dense(100, activation = 'sigmoid'))\nmodel.add(Dense(50, activation = 'sigmoid', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dense(2, activation = 'sigmoid'))\nsgd = SGD(lr=0.05, momentum=0.9, decay=0.0, nesterov=False)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=128,\n          epochs=20,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def conv2d(input):\n    F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3)))\n    F_b = tf.Variable(tf.zeros(3))\n    strides = [1, 2, 2, 1]\n    padding = 'VALID'\n    return tf.nn.conv2d(input, F_W, strides, padding) + F_b\nconv_out = conv2d(X)\n", "intent": "```\nnew_height = (input_height - filter_height + 2 * P)/S + 1\nnew_width = (input_width - filter_width + 2 * P)/S + 1\n```\n"}
{"snippet": "model.fit(X_train,y_train)\n", "intent": "Train a logistic regression model on the train set.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), dtype=tf.float32)\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "graph_model = DecisionTreeClassifier(criterion='entropy', random_state=42)\ngraph_X = X[:200]\ngraph_y = y[:200]\n", "intent": "`When entropy is 1, the classes are balanced and when entropy is 0 everything is the same class.` \n"}
{"snippet": "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1, verbose=False)\nkm.fit(X)\n", "intent": "Do the actual clustering.\n"}
{"snippet": "from sklearn.pipeline import Pipeline\npipeline = Pipeline([\n        ('features', vectorizer),\n        ('model', model)   \n    ])\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: tst_x,\n            labels_: tst_y,\n            keep_prob_: 1.0}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(\n        tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0),\n        name=\"embedding_matrix\")\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./rnn_time_series_model\")\n    zero_seq_seed = [0. for i in range(num_time_steps)]\n    for iteration in range(len(ts_data.x_data) - num_time_steps):\n        X_batch = np.array(zero_seq_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n        y_pred = sess.run(outputs, feed_dict={X: X_batch})\n        zero_seq_seed.append(y_pred[0, -1, 0])\n        print(X_batch)\n        print(\"\\n\")\n", "intent": "** Note: Can give wacky results sometimes, like exponential growth**\n"}
{"snippet": "word_embedding = td.Embedding(\n    *weight_matrix.shape, initializer=weight_matrix, name='word_embedding')\n", "intent": "Create the word embedding using [`td.Embedding`](https://github.com/tensorflow/fold/blob/master/tensorflow_fold/g3doc/py/td.md\n"}
{"snippet": "kk = 6\nimg_kmeans = KMeans(n_clusters= kk).fit(img_new)\n", "intent": "** 2. ** Fit kmeans on the model and predict what cluster each pixel is in.\n"}
{"snippet": "def kmeans(X,k,max_iters =100):\n    centroids = random_centroids(X,k)\n    labels = old_centroids = None\n    iteration = 1\n    while not should_stop(old_centroids,centroids,iteration,max_iters):\n        labels = closest_centroids(X,centroids)\n        old_centroids = centroids\n        centroids = calc_new_centroids(X,labels,k)\n        iteration += 1\n    return centroids, labels\n", "intent": "We've broken down the algo it small parts now lets piece it altogether for our final implementation. \n"}
{"snippet": "x_split = tf.split(0, len(char_dic), x_data) \noutputs, state = tf.nn.rnn(cell = rnn_cell, inputs = x_split, initial_state = initial_state)\nprint outputs, '\\n'\nprint state\n", "intent": "x_split output \n```python\n[[1,0,0,0]] \n[[0,1,0,0]] \n[[0,0,1,0]] \n[[0,0,0,1]] \n```\n"}
{"snippet": "X4_train = np.c_[X2_train.iloc[:,0:2], X2_train.iloc[:, 0] * X2_train.iloc[:, 1]]\nX4_test = np.c_[X2_test.iloc[:,0:2], X2_test.iloc[:, 0] * X2_test.iloc[:, 1]]\nregr4 = LinearRegression()\nregr4.fit(X4_train, y2_train)\nprint('Variance score: %.4f' % regr4.score(X4_test, y2_test))\n", "intent": "The hyperplane plot suggests that we should consider adding the interaction term \"TV*radio\". \n"}
{"snippet": "model.fit(X_train, y_train)\n", "intent": "Training RBM-Logistic Pipeline.\n"}
{"snippet": "X_over, Y_over = oversample(X_train, Y_train, minority_weight=.5)\nlr = LogisticRegression(penalty='l1')\ncv = cross_validation(lr, X_over, Y_over , n_splits=5,init_chunk_size = 100, chunk_spacings = 50, average = \"binary\")\ncv.validate_for_holdout_set(X_holdout, Y_holdout)\ncv.plot_learning_curve()\n", "intent": "Regularization: Lasso \n"}
{"snippet": "X_over, Y_over = oversample(X_train, Y_train, minority_weight=.5)\nlr = LogisticRegression(penalty='l2')\ncv = cross_validation(lr, X_over, Y_over , n_splits=5,init_chunk_size = 100, chunk_spacings = 50, average = \"binary\")\ncv.validate_for_holdout_set(X_holdout, Y_holdout)\ncv.plot_learning_curve()\n", "intent": "Regularization: Ridge \n"}
{"snippet": "sm = SMOTE(kind='regular')\nX_smote, Y_smote= sm.fit_sample(X_train, Y_train)\nlr = LogisticRegression()\ncv = cross_validation(lr, X_smote, Y_smote , n_splits=5, init_chunk_size = 100, chunk_spacings = 10, average = \"binary\")\ncv.validate_for_holdout_set(X_holdout, Y_holdout)\ncv.plot_learning_curve()\n", "intent": "Regularization: It dosen't matter (you'll see).\n"}
{"snippet": "X_under, Y_under = undersample(X_train, Y_train, majority_weight=.5)\nlr = LogisticRegression(penalty='l2')\ncv = cross_validation(lr, X_under, Y_under , n_splits=5,init_chunk_size = 100, chunk_spacings = 50, average = \"binary\")\ncv.validate_for_holdout_set(X_holdout, Y_holdout)\ncv.plot_learning_curve()\n", "intent": "Now that we've feature engineered two statistically sound features, let's see how Logistical Regression responds. \n"}
{"snippet": "X_under, Y_under = undersample(X_train, Y_train, majority_weight=.5)\nlr = LogisticRegression(penalty='l2')\ncv = cross_validation(lr, X_under, Y_under , n_splits=5,init_chunk_size = 100, chunk_spacings = 10, average = \"binary\")\ncv.validate_for_holdout_set(X_holdout, Y_holdout)\ncv.plot_learning_curve()\n", "intent": "Regularization: Rigde \n"}
{"snippet": "sm = SMOTE(kind='regular')\nX_smote, Y_smote = sm.fit_sample(X_train, Y_train)\nlr = LogisticRegression()\ncv = cross_validation(lr, X_smote, Y_smote , n_splits=5, init_chunk_size = 100, chunk_spacings = 10, average = \"binary\")\ncv.validate_for_holdout_set(X_holdout, Y_holdout)\ncv.plot_learning_curve()\n", "intent": "Regularization: Ridge\n"}
{"snippet": "lr = LogisticRegression(penalty='l2')\ncv = cross_validation(lr, \n                      X_train_scaled, \n                      Y_train_scaled , \n                      n_splits=10,\n                      init_chunk_size = 100, \n                      chunk_spacings = 25, \n                      average = \"binary\")\ncv.train_for_learning_curve()\ncv.plot_learning_curve()\n", "intent": "Next we are going to simply pass in a scaled version of the data set. \n"}
{"snippet": "lr = LogisticRegression(penalty='l2')\ncv = cross_validation(lr, \n                      X_train_scaled, \n                      Y_train_scaled , \n                      n_splits=10,\n                      init_chunk_size = 100, \n                      chunk_spacings = 25, \n                      average = \"binary\")\ncv.validate_for_holdout_set(X_test_scaled, Y_test_scaled)\ncv.plot_learning_curve()\n", "intent": "----\nAs a reminder, let's get visualize LR's learning curves as a reminder.\n"}
{"snippet": "svc = SVC(C = 100.0, \n          gamma = 1.0, \n          kernel = \"poly\",\n          probability = True)\n", "intent": "Optimized Hyper-Parameters\n     (F1 = 0.91093502317158193, \n     {'C': 100.0, \n     'gamma': 1.0, \n     'kernel': 'poly'})\n"}
{"snippet": "logistic_classifier = linear_model.LogisticRegression(C=100.0)\nlogistic_classifier.fit(X_train, y_train)\n", "intent": "Just a plain logistic regression.\n"}
{"snippet": "svc = SVC(C = 100.0, \n          gamma = 1.0, \n          kernel = \"poly\",\n         probability = True)\n", "intent": "Optimized Hyper-Parameters\n     (F1 = 0.91093502317158193, {'C': 100.0, 'gamma': 1.0, 'kernel': 'poly'})\n"}
{"snippet": "lb_view = client.load_balanced_view()\nmodel = RandomForestClassifier()\nrfc_params = {'max_depth': [None, 2, 6, 10],\n              'min_samples_split': [2, 4, 6],\n              'min_samples_leaf': [1, 3, 5]}\n", "intent": "Now we repeat the same process for a another classifier\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), name='embedding')\n    embed_vec = tf.nn.embedding_lookup(embedding, input_data, name='embed_vec')\n    return embed_vec\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "lm2 = LogisticRegression()\nlm2.fit(analyst[['Austin', 'Chicago', 'New York', 'San Francisco', 'Seattle']], analyst['parsed_salary'])\n", "intent": "I chose to take a look at the model with the \"analyst\" variable as it was the most robust variable out of the data set.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn_parameters = {\n    'n_neighbors': range(1,11),\n    'weights': ['uniform','distance'],\n    'algorithm':['auto']\n}\nknn = KNeighborsClassifier()\ngs = GridSearchCV(estimator=knn, param_grid=knn_parameters)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "lr = ols(\"price ~ age + gender + delay + delay1 + weekday\", data=feat3)\nres = lr.fit()\nres.summary()\n", "intent": "On apprend avec la variable *weekday* :\n"}
{"snippet": "lr = ols(\"price ~ age + gender + delay + delay1 + price_label\", data=feat3)\nres = lr.fit()\nres.summary()\n", "intent": "Et on compare avec la variable *price_label* :\n"}
{"snippet": "from tpot import TPOTRegressor\ntpot = TPOTRegressor(generations=2, population_size=50, verbosity=2)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_boston_pipeline.py')\n", "intent": "[TPOT](https://github.com/rhiever/tpot) est un module d'apprentissage automatique.\n"}
{"snippet": "a = np.array([4, 3, 6])\nb = np.array([3, -1, 2])\nc = np.add(a, b)\nprint(c)\na = tf.constant([4, 3, 6])\nb = tf.constant([3, -1, 2])\nc = tf.add(a, b)\nwith tf.Session() as sess:\n  result = sess.run(c)\n  print(result)\n", "intent": "*Step 2: Perform Basic Matrix Operations Using TensorFlow*\n"}
{"snippet": "from sklearn import linear_model\nclf = linear_model.LinearRegression()\n", "intent": "** Linear Regression **\nhttp://scikit-learn.org/stable/modules/linear_model.html\n"}
{"snippet": "def runLogisticRegression(a,b,c,d):\n", "intent": "*Step 5: Compare Classification Algorithms*\n"}
{"snippet": "estimator.fit(bodyfat_features, bodyfat['body.fat'])\nexpected_coef = pd.Series(regressor.coef_, bodyfat_features.columns)\nprint(regressor.intercept_)\nprint(expected_coef)\n", "intent": "To confirm that our results are valud, we'll compare them with the results of the scikit-learn estimator.\n"}
{"snippet": "tf.reset_default_graph()\nsess = tf.Session()\n", "intent": "Create a new ``tf.Session``.\n"}
{"snippet": "with tf.name_scope('model'):\n    slope = tf.Variable(0.0, name='slope')\n    intercept = tf.Variable(0.0, name='intercept')\n    y = slope * x_placeholder + intercept\n", "intent": "We do the same to define the linear model in the ``model`` scope:\n"}
{"snippet": "tf.reset_default_graph()\nsess = tf.Session()\n", "intent": "Initialize a new ``tf.Session``.\n"}
{"snippet": "LOGDIR = 'graphs'\nwriter = tf.summary.FileWriter(join(LOGDIR, '02_logistic_regression'))\nwriter.add_graph(sess.graph)\n", "intent": "Before starting the traning, we setup summary statistics to be visualized in TensorBoard.\n"}
{"snippet": "history = model.fit(train_data,labels,epochs=150,callbacks=[PlotLossesKeras(dynamic_x_axis=False)])\n", "intent": "Let's go ahead and train the model, and look at how the loss and accuracy change with time.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_matrix = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n    return tf.nn.embedding_lookup(embedding_matrix, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "hidden_layer = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\nhidden_layer = tf.nn.relu(hidden_layer)\noutput = tf.add(tf.matmul(hidden_layer, weights['out']), biases['out'])\n", "intent": "A ReLu activation function will be used on the hidden layer before it is connected to the output. Each layer will implement a linear function `wx+b`\n"}
{"snippet": "from sklearn import neighbors\nfrom sklearn import preprocessing\nX_scaled = preprocessing.scale(X) \nclf1 = neighbors.KNeighborsRegressor(5)\ntrain_X = X_scaled[:half]\ntest_X = X_scaled[half:]\nclf1.fit(train_X,train_Y)\n", "intent": "** *k*-Nearest Neighbor (KNN) Regression **\n"}
{"snippet": "from sklearn.tree import DecisionTreeRegressor\ntree_carseats = DecisionTreeRegressor(min_samples_split = 10, min_samples_leaf = 5)\ntree_carseats.fit(train_x, train_y)\n", "intent": "b) Fit a regression tree to the training set. Plot the tree, and interpret the results. What test error rate do you obtain ?\n"}
{"snippet": "with pm.Model() as logistic_model:\n    pm.glm.GLM.from_formula('income_more_50K ~ age + educ', data, family=pm.glm.families.Binomial())\n    map_estimate = pm.find_MAP()\n    print(map_estimate)\n", "intent": "Sumbit MAP estimations of corresponding coefficients:\n"}
{"snippet": "xgb_model.save_model('Pipeline.model')\n", "intent": "RMSE for XGBoost is 0.857923\nTrain R-squared for XGBoost is 0.507406\n"}
{"snippet": "model = 'Logistic Regression A'\nclf_lgra = LogisticRegression(random_state=123)\ngs_params = {'C': [.01, 0.1, 1.0, 10], 'solver': ['liblinear', 'lbfgs']}\ngs_score = 'roc_auc'\nclf_lgra, pred_lgra = bin_classify(model, clf_lgra, features_extr, params=gs_params, score=gs_score)\nprint('\\nBest Parameters:\\n',clf_lgra)\nmetrics_lgra, roc_lgra, prc_lgra = bin_class_metrics(model, y_test, pred_lgra.y_pred, pred_lgra.y_score, print_out=True, plot_out=True)\n", "intent": "Engines in the above charts represent the queue or number of engines to be maintain per period, i.e. maintenance capacity.\n"}
{"snippet": "clf = GradientBoostingClassifier(verbose=True)\nclf.fit(X_train, y_train)\nprint('Train Score:', round(clf.score(X_train,y_train),3))\nprint('Test Score:', round(clf.score(X_test,y_test),3))\n", "intent": "Train and tune a single model.\n"}
{"snippet": "param_test1 = {'n_estimators':range(20,81,10)}\ngsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, \n                                                                min_samples_split=500,\n                                                               min_samples_leaf=50,\n                                                               max_depth=8,\n                                                               max_features='sqrt',\n                                                               subsample=0.8,\n                                                               random_state=2007), \nparam_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch1.fit(X_train,y_train)\n", "intent": "now lets try some hyperparameter tunning.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1024, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=500, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_normal((vocab_size, embed_dim), -1, 1), name=\"embedding\")\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from nltk.classify.scikitlearn import SklearnClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nbernoulli = SklearnClassifier(BernoulliNB())\nbernoulli.train(trainData)\nprint nltk.classify.accuracy(bernoulli, testData)\n", "intent": "Nltk also has functions that allow us to call other machine learning libraries, including scikit-learn, using wrapper classes.\n"}
{"snippet": "param_grid = {'alpha': np.logspace(-3, 3, 13)}\ngrid = GridSearchCV(lm.Ridge(), param_grid, cv=10,return_train_score=True)\ngrid.fit(x_train, y_train)\n", "intent": "**Ridge regression with Gridsearch**\n"}
{"snippet": "param_grid = {'alpha': np.logspace(-3, 3, 13)}\ngrid = GridSearchCV(lm.Ridge(), param_grid, cv=10,return_train_score=True)\ngrid.fit(x_train, y_train)\n", "intent": "Tuning parameters for Ridge Regression:\n"}
{"snippet": "param_grid = {'alpha': np.logspace(-3, 3, 13)}\ngrid = GridSearchCV(lm.Lasso(), param_grid, cv=10,return_train_score=True)\ngrid.fit(x_train, y_train)\n", "intent": "Tuning parameters for Lasso Regression:\n"}
{"snippet": "param_grid = {'alpha': np.logspace(-3, 3, 13)}\ngrid = GridSearchCV(lm.ElasticNet(), param_grid, cv=10,return_train_score=True)\ngrid.fit(x_train, y_train)\n", "intent": "Tuning parameters for Elastic Nets:\n"}
{"snippet": "logit_1 = LogisticRegression()\nlogit_1.fit(X_train_1, y_train_1)\n", "intent": "**default model settings:**\n"}
{"snippet": "train_score = []\nvalid_score = []\noob_score = []\nfor trees in tqdm(tree_sizes, desc='Training Models', unit='Model'):\n    clf = bagging_classifier(trees, 0.2, 1.0, clf_parameters)\n    clf.fit(X_train, y_train)\n    train_score.append(clf.score(X_train, y_train.values))\n    valid_score.append(clf.score(X_valid, y_valid.values))\n    oob_score.append(clf.oob_score_)\n", "intent": "With the bagging classifier built, lets train a new model and look at the results.\n"}
{"snippet": "from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(4,4,4),max_iter=50000)\n", "intent": "- Finally, we will build an MLP!\n"}
{"snippet": "mlp2 = MLPClassifier(hidden_layer_sizes=(8,8,8),max_iter=50000)\n", "intent": "- Let's try a different configuration.\n"}
{"snippet": "from sklearn import ensemble\ntask = oml.tasks.get_task(3954)\nclf = ensemble.RandomForestClassifier()\nflow = oml.flows.sklearn_to_flow(clf)\nrun = oml.runs.run_flow_on_task(task, flow)\n", "intent": "We can run (many) scikit-learn algorithms on (many) OpenML tasks.\n"}
{"snippet": "from sklearn.svm import LinearSVC\nsvm = SklearnClassifier(LinearSVC())\nsvm.train(trainData)\nprint nltk.classify.accuracy(svm, testData)\n", "intent": "We can use any of the learning algorithms implemented by scikit-learn ([decision trees](http://scikit-learn.org/stable/modules/tree.html\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\nnames = [\"Gradient Boosting 1 tree\", \"Gradient Boosting 3 trees\", \"Gradient Boosting 100 trees\"]\nclassifiers = [\n    GradientBoostingClassifier(n_estimators=1, random_state=0, learning_rate=0.5),\n    GradientBoostingClassifier(n_estimators=3, random_state=0, learning_rate=0.5),\n    GradientBoostingClassifier(n_estimators=100, random_state=0, learning_rate=0.5)\n    ]\nmorelearn.plots.plot_classifiers(names, classifiers, figuresize=(20,8))\n", "intent": "Each tree provides good predictions on part of the data, use voting for final prediction\n* Soft voting for classification, mean values for regression\n"}
{"snippet": "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\ngbrt.fit(X_train, y_train)\nplot_feature_importances_cancer(gbrt)\n", "intent": "Gradient boosting machines use much simpler trees\n- Hence, tends to completely ignore some of the features\n"}
{"snippet": "mglearn.plots.plot_svm_linear()\n", "intent": "SVM result. The circled samples are support vectors, together with their coefficients.\n"}
{"snippet": "svc = SVC()\nsvc.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n        svc.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\n", "intent": "Much better results, but they can still be tuned further\n"}
{"snippet": "param_grid = {'polynomialfeatures__degree': [1, 2, 3],\n              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=1)\ngrid.fit(X_train, y_train)\n", "intent": "* We don't know the optimal polynomial degree or alpha value, so we use a grid search (or random search) to find the optimal values\n"}
{"snippet": "score = Ridge().fit(X_train_log, y_train).score(X_test_log, y_test)\nprint(\"Test score: {:.3f}\".format(score))\n", "intent": "And our Ridge regressor now performs a lot better.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nX_test_selected = select.transform(X_test)\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nprint(\"Score with all features: {:.3f}\".format(lr.score(X_test, y_test)))\nlr.fit(X_train_selected, y_train)\nprint(\"Score with only selected features: {:.3f}\".format(\n        lr.score(X_test_selected, y_test)))\n", "intent": "As usual, we need to check how the transformation affects the performance of our learning algorithms.\n"}
{"snippet": "tf.reset_default_graph()\n", "intent": "Please note that the graph visualizations only work in Chrome :(\n"}
{"snippet": "def reset_model():\n    net = Net()\n    net = net.cuda()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n    return net,criterion,optimizer\n", "intent": "<b>The below function is called to reinitialize the weights of the network and define the required loss criterion and the optimizer.</b> \n"}
{"snippet": "X = iris[[\"petal_length\"]]\ny = iris[\"petal_width\"]\nmodel = linear_model.LinearRegression()\nresults = model.fit(X, y)\nprint results.intercept_, results.coef_\n", "intent": "Now let's use scikit-learn to find the best fit line.\n"}
{"snippet": "def reset_model(is_teacher = True):\n    if is_teacher == True:\n        net = Teacher()\n    else:\n        net = Student()\n    net = net.cuda()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n    return net,criterion,optimizer\n", "intent": "<b>The below function is called to reinitialize the weights of the network and define the required loss criterion and the optimizer.</b> \n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 1)\n", "intent": "1. Import class you plan to use \n2. make an instance of the model in scikit learn \n   tune the model parameters \n3. Fit the model with the data \n"}
{"snippet": "odd_ratio_lm = lmodel.fit().params.apply(np.exp)\nprint (odd_ratio_lm)\n", "intent": "hint 1: np.exp(X)\nhint 2: conf['OR'] = params\n           conf.columns = ['2.5%', '97.5%', 'OR']\n"}
{"snippet": "model = ExtraTreesClassifier()\n", "intent": "Imported library, now let's create our model making a call in the library.\n"}
{"snippet": "model = DecisionTreeClassifier()\n", "intent": "After that, let's instance out model calling the method Classifier.\n"}
{"snippet": "middle = 30\nw_1 = tf.Variable(tf.truncated_normal([784, middle]))\nb_1 = tf.Variable(tf.truncated_normal([1, middle]))\nw_2 = tf.Variable(tf.truncated_normal([middle, 10]))\nb_2 = tf.Variable(tf.truncated_normal([1, 10]))\n", "intent": "Now we fill the weights and biases. \n"}
{"snippet": "rfc = RandomForestClassifier(n_estimators=100, min_samples_leaf=10, random_state=1, n_jobs=2)\ngbc = GradientBoostingClassifier()\netc = ExtraTreesClassifier()\nabc = AdaBoostClassifier()\nsvc = SVC()\nknc = KNeighborsClassifier()\ndtc = DecisionTreeClassifier()\nptc = Perceptron()\nlrc = LogisticRegression()\n", "intent": "Now, we'll instance our objects with the classifiers. \n"}
{"snippet": "rfc.fit(X_train, Y_train)\ngbc.fit(X_train, Y_train)\netc.fit(X_train, Y_train)\nabc.fit(X_train, Y_train)\nsvc.fit(X_train, Y_train)\nknc.fit(X_train, Y_train)\ndtc.fit(X_train, Y_train)\nptc.fit(X_train, Y_train)\nlrc.fit(X_train, Y_train)\n", "intent": "With the training sets, we'll fit all models for each classifier.\n"}
{"snippet": "lasso_baseline = get_baseline_results(X_train, X_test, Y_train, Y_test, Lasso(), 'Lasso')\nridge_baseline = get_baseline_results(X_train, X_test, Y_train, Y_test, Ridge(), 'Ridge')\nelastic_net_baseline = get_baseline_results(X_train, X_test, Y_train, Y_test, ElasticNet(), 'ElasticNet')\nxgboost_baseline = get_baseline_results(X_train, X_test, Y_train, Y_test, XGBRegressor(), 'XGBoost')\n", "intent": "First at all lets get the results of our baseline models. \n"}
{"snippet": "model = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n", "intent": "This means that our best fit line is:\n$$y = a + b x$$\nwhere $a = -0.363075521319$ and $b = 0.41575542$.\nNext let's use `statsmodels`.\n"}
{"snippet": "import eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())\n", "intent": "Now use the following cell to create the materials for the doctors.\n"}
{"snippet": "import eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())\n", "intent": "Here is how to calculate and show importances with the [eli5](https://eli5.readthedocs.io/en/latest/) library:\n"}
{"snippet": "from tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\nfashion_model  = Sequential()\nq_1.check()\n", "intent": "Create a `Sequential` model called `fashion_model`. Don't add layers yet.\n"}
{"snippet": "import statsmodels.formula.api as smf\nlm = smf.ols(formula='Sales ~ TV', data=data).fit()\nlm2 = smf.ols(formula='Sales ~ Radio', data=data).fit()\nlm.params\nlm2.params\nlm2.summary()\n", "intent": "Let's use **Statsmodels** to estimate the model coefficients for the advertising data:\n"}
{"snippet": "aa = linear_model.Ridge( copy_X=True, fit_intercept=False, max_iter=None,\n      normalize=False, random_state=0, solver='auto',alpha=6.0,tol=1.0\n                                   ).fit(df_train_data, df_train_target)\n", "intent": "df2_train = df2_train.values\nresult_last_var = svc_last.predict(df2_train)\nresult_last = pd.DataFrame({'Id':df2['Id'],'SalePrice':result_last_var})\n"}
{"snippet": "def create_baseline_model():\n    model = Sequential()\n    model.add(Dense(60, input_dim=60, init='normal', activation='relu'))\n    model.add(Dense(1, init='normal', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\nestimator = KerasClassifier(build_fn=create_baseline_model, nb_epoch=100, batch_size=5, verbose=0)\n", "intent": "Create baseline model\n"}
{"snippet": "def create_nn():\n    model = Sequential()\n    model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n    model.add(Dense(8, init='uniform', activation='relu'))\n    model.add(Dense(1, init='uniform', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n", "intent": "Define a neural network model\n"}
{"snippet": "model = create_nn()\nmodel.fit(X, Y, nb_epoch=150, batch_size=10, verbose=0)\n", "intent": "Fit and evaluate this model\n"}
{"snippet": "model = create_nn()\nfile_path = './models/c14/nn-best-model.h5'  \ncheckpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nmodel.fit(X_train, Y_train, validation_data=(X_val, Y_val), nb_epoch=150, batch_size=10, callbacks=[checkpoint], verbose=0)\n", "intent": "Create a model and a checkpoint instance. When the val_acc is improved, the weights will be saved.\n"}
{"snippet": "X = iris[[\"petal_length\", \"setosa\", \"versicolor\", \"virginica\"]]\nX = sm.add_constant(X) \ny = iris[\"petal_width\"]\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n", "intent": "Now we perform a multilinear regression with the dummy variables added.\n"}
{"snippet": "def get_dense_layers():\n    return [\n        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n        Flatten(),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(1000, activation='relu')\n        ]\n", "intent": "This is our usual Vgg network just covering the dense layers:\n"}
{"snippet": "def fit_model(model, batches, val_batches, nb_epoch=1):\n    model.fit_generator(batches, steps_per_epoch=steps_per_epoch, epochs=nb_epoch, \n                        validation_data=val_batches, validation_steps=validation_steps)\n", "intent": "We'll define a simple function for fitting models, just to save a little typing...\n"}
{"snippet": "fc_model.fit(trn_features, trn_labels, epochs=8, \n             batch_size=batch_size, validation_data=(val_features, val_labels))\n", "intent": "And fit the model in the usual way:\n"}
{"snippet": "user_in = Input(shape=(1,), dtype='int64', name='user_in')\nu = Embedding(input_dim=n_users, output_dim=n_factors, input_length=1, embeddings_regularizer=l2(1e-4))(user_in)\nmovie_in = Input(shape=(1,), dtype='int64', name='movie_in')\nm = Embedding(input_dim=n_movies, output_dim=n_factors, input_length=1, embeddings_regularizer=l2(1e-4))(movie_in)\n", "intent": "The most basic model is a dot product of a movie embedding and a user embedding. Let's see how well that works:\n"}
{"snippet": "graph_in = Input ((vocab_size, 50))\nconvs = [ ] \nfor fsz in range (3, 6): \n    x = Conv1D(64, fsz, padding='same', activation=\"relu\")(graph_in)\n    x = MaxPooling1D()(x) \n    x = Flatten()(x) \n    convs.append(x)\nout = Concatenate(axis=-1)(convs) \ngraph = Model(graph_in, out) \n", "intent": "We use the functional API to create multiple conv layers of different sizes, and then concatenate them.\n"}
{"snippet": "model = Sequential ([\n    Embedding(vocab_size, 50, input_length=seq_len, weights=[emb]),\n    SpatialDropout1D(0.2),\n    Dropout (0.2),\n    graph,\n    Dropout (0.5),\n    Dense (100, activation=\"relu\"),\n    Dropout (0.7),\n    Dense (1, activation='sigmoid')\n    ])\n", "intent": "We then replace the conv/max-pool layer in our original CNN with the concatenated conv layers.\n"}
{"snippet": "model = Sequential([\n    Embedding(vocab_size, 32, input_length=seq_len, mask_zero=True,\n              embeddings_regularizer=l2(1e-6)),\n    SpatialDropout1D(0.2),\n    LSTM(100, implementation=2),\n    Dense(1, activation='sigmoid')])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "We haven't covered this bit yet!\n"}
{"snippet": "model=Sequential([\n        Embedding(vocab_size, n_fac, input_length=cs),\n        SimpleRNN(n_hidden, activation='relu', recurrent_initializer='identity'),\n        Dense(vocab_size, activation='softmax')\n    ])\n", "intent": "This is nearly exactly equivalent to the RNN we built ourselves in the previous section.\n"}
{"snippet": "model=Sequential([\n        SimpleRNN(n_hidden, return_sequences=True, input_shape=(cs, vocab_size),\n                  activation='relu', recurrent_initializer='identity'),\n        TimeDistributed(Dense(vocab_size, activation='softmax')),\n    ])\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam())\n", "intent": "This is the keras version of the theano model that we're about to create.\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\ntype(knn)\n", "intent": "**Step 2:** \"Instantiate\" the \"estimator\"\n- \"Estimator\" is scikit-learn's term for \"model\"\n- \"Instantiate\" means \"make an instance of\"\n"}
{"snippet": "model.fit([conv_feat, trn_sizes], trn_labels, batch_size=batch_size, epochs=3, \n             validation_data=([conv_val_feat, val_sizes], val_labels))\n", "intent": "And when we train the model, we have to provide all the input layers' data in an array.\n"}
{"snippet": "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, epochs=1, \n             validation_data=(conv_val_feat, val_labels))\n", "intent": "Now we can train the model as usual, with pre-computed augmented data.\n"}
{"snippet": "K.set_value(answer.optimizer.lr, 1e-2)\nhist=answer.fit(inps, answers_train, **parms, epochs=4, batch_size=32,\n           validation_data=(val_inps, answers_test))\n", "intent": "And it works extremely well\n"}
{"snippet": "K.set_value(answer.optimizer.lr, 5e-3)\nhist=answer.fit(inps, answers_train, **parms, epochs=8, batch_size=32,\n           validation_data=(val_inps, answers_test))\n", "intent": "Fitting this model can be tricky.\n"}
{"snippet": "def relu(x): return Activation('relu')(x)\ndef dropout(x, p): return Dropout(p)(x) if p else x\ndef bn(x): return BatchNormalization()(x)  \ndef relu_bn(x): return relu(bn(x))\n", "intent": "These components should all be familiar to you:\n* Relu activation\n* Dropout regularization\n* Batch-normalization\n"}
{"snippet": "def conv(x, nf, sz, wd, p):\n    x = Conv2D(nf, (sz, sz), kernel_initializer='he_uniform', padding='same',  \n                          kernel_regularizer=l2(wd))(x)\n    return dropout(x,p)\n", "intent": "Convolutional layer:\n* L2 Regularization\n* 'same' border mode returns same width/height\n* Pass output through Dropout\n"}
{"snippet": "rn_top = Model(model.input, mid_out.output)\nrn_top_avg = Sequential([rn_top, AveragePooling2D((7,7))])\n", "intent": "We put an average pooling layer on top to make it a more managable size.\n"}
{"snippet": "rn_bot_inp = Input(shp[1:])\nx = rn_bot_inp\nx = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\nx = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\nx = Flatten()(x)\nrn_bot = Model(rn_bot_inp, x)\nrn_bot.output_shape\n", "intent": "Our final layers match the original resnet, although we add on extra resnet block at the top as well.\n"}
{"snippet": "lm_inp = Input(shape=(2048,))\nlm = Model(lm_inp, Dense(ndim)(lm_inp))\n", "intent": "We add a linear model on top to predict our word vectors.\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train_dtm, y_train)\n", "intent": "Use Naive Bayes to predict the star rating for reviews in the testing set, and calculate the accuracy.\n"}
{"snippet": "img_nn = NearestNeighbors(3, metric='cosine', algorithm='brute').fit(pred_wv)\n", "intent": "Something very nice about this kind of model is we can go in the other direction as well - find images similar to a word or phrase!\n"}
{"snippet": "ft_model = Sequential([rn_top_avg, rn_bot_seq])\n", "intent": "Since that worked so well, let's try to find images with similar content to another image...\n"}
{"snippet": "def meanshift(data):\n    X = torch.FloatTensor(np.copy(data))\n    for it in range(5):\n        for i, x in enumerate(X):\n            dist = torch.sqrt((sub(x, X)**2).sum(1))\n            weight = gaussian(dist, 3).unsqueeze(1)  \n            num = mul(weight, X).sum(0).unsqueeze(0)  \n            X[i] = num / weight.sum()\n    return X\n", "intent": "And the implementation of meanshift is nearly identical too!\n"}
{"snippet": "K.set_value(m_sr.optimizer.lr, 1e-4)\nm_sr.fit([arr_lr, arr_hr], targ, 16, 100, **parms)\n", "intent": "We use learning rate annealing to get a better fit.\n"}
{"snippet": "inp = Input((288,288,3))\nref_model = Model(inp, ReflectionPadding2D((1,288,288,3), padding=(40,10))(inp))\nref_model.compile('adam', 'mse')\n", "intent": "Testing the reflection padding layer:\n"}
{"snippet": "outputs = net(Variable(images).cuda())\n_, predicted = torch.max(outputs.data, 1)\n' '.join('{}'.format([classes[predicted[j]] for j in range(4)]))\n", "intent": "Okay, now let us see what the neural network thinks these examples above are:\n"}
{"snippet": "correct,total = 0,0\nfor data in testloader:\n    images, labels = data\n    outputs = net(Variable(images).cuda())\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels.cuda()).sum()\nprint('Accuracy of the network on the 10000 test images: {} %%'.format(100 * correct / total))\n", "intent": "The results seem pretty good. Let us look at how the network performs on the whole dataset.\n"}
{"snippet": "def get_contin(feat):\n    name = feat[0][0]\n    inp = Input((1,), name=name+'_in')\n    return inp, Dense(1, name=name+'_d', kernel_initializer=my_init(1.))(inp)  \n", "intent": "Helper function for continuous inputs.\n"}
{"snippet": "def relu(x): return Activation('relu')(x)\ndef dropout(x, p): return Dropout(p)(x) if p else x\ndef bn(x): return BatchNormalization()(x)  \ndef relu_bn(x): return relu(bn(x))\ndef concat(xs): return concatenate(xs)  \n", "intent": "This should all be familiar.\n"}
{"snippet": "x = dataset['sex'].values\nn = dataset['total'].values\ny_obs = dataset['>50'].values\nwith pm.Model() as edu_A1:\n    sigma = pm.HalfCauchy('sigma',beta=2)\n    alpha = pm.Normal('alpha',mu=0,sd=10)\n    beta_m = pm.Normal('beta_m',mu=0,sd=1)\n    alpha_edu = pm.Normal('alpha_edu', mu=alpha, sd=sigma, shape=7)\n    p = pm.Deterministic('p', pm.math.invlogit(alpha_edu[np.arange(14)//2] + beta_m * x))\n    y = pm.Binomial('y_obs', p=p, n=n, observed=y_obs)\n", "intent": "**Answer:** Let's follow the HW10 first and build a similar model (edu_A1).\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=24, verbose=1)  \n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "scaler.fit(df.drop('Class', axis=1), df['Class'])\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "lm.fit(X_train, y_train) \n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlm = LogisticRegression()\nlm.fit(X_train, y_train)\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "gsc = GridSearchCV(SVC(), param_grid, refit=True,verbose=3)\ngsc.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "gbc_params = {'n_estimators':[100,500,1000],\n             'subsample':[0.3,0.7,1.0],\n             'min_samples_split':[2,4,6,8],\n             'max_depth':[3,7,10],\n             'max_features':['auto','sqrt','log2',None]\n             }\ngcv = GridSearchCV(gbc,gbc_params,cv=5)\ngcv.fit(Xtrain,ytrain)\nprint(gcv.score(Xtrain, ytrain))\nprint(gcv.best_params_)\n", "intent": "Gradient boosting seems to perform the best so I then tune the hyperparameters to increase performance even more.\n"}
{"snippet": "best_model.save_model(\"pubg_model.model\")\n", "intent": "The mean absolute error for the model tuned in xgboost is almost half versus the model made in scikit-learn.\n"}
{"snippet": "def softmax(sess, x):\n    kh = tf.cast(x, tf.float32)\n    return sess.run(tf.exp(kh) / tf.reduce_sum(tf.exp(kh)))\n", "intent": "$$\\text{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$$\n"}
{"snippet": "def softmax_matrix(sess, x):\n    kh = tf.cast(x, tf.float32)\n    return sess.run(tf.nn.softmax(kh))\n", "intent": "$$\\text{softmax}(X)_{ij} = \\frac{\\exp(X_{ij})}{\\sum_j \\exp(X_{ij})}$$\n"}
{"snippet": "x_shared = shared(x_train)\ny_shared = shared(y_train)\nlogistic_model = pm.Model()\nwith logistic_model:\n    betas = pm.Normal('betas', 0, tau=1/100 , shape=(D, ))\n    p = pm.math.invlogit(t.dot(x_shared, betas))\n    y_obs = pm.Bernoulli('y_obs', p=p, observed=y_shared)\n", "intent": "**Setting Up the Model in Pymc3**\n"}
{"snippet": "sess.close()\ntf.reset_default_graph()\nsess = tf.Session(config=config)\nwriter = tf.summary.FileWriter('log/')\n", "intent": "`tf.gradients(ys, xs)` Constructs symbolic partial derivatives of sum of ys w.r.t. x in xs.\n"}
{"snippet": "criterion = ClassNLLCriterion()\nnet = Sequential()\nnet.add(Linear(2, 4))\nnet.add(ReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\nprint(net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "input_X_reshaped = tf.reshape(input_X, shape=[-1, 1*28*28], \n                              name=\"reshape_X\")\nl1 = tf.layers.dense(input_X_reshaped, units=50, \n                     activation=tf.nn.sigmoid)\nl2 = tf.layers.dense(l1, units=10, activation=None)\nl_out = tf.nn.softmax(l2)\ny_predicted = tf.argmax(l2, axis=-1)\n", "intent": "Defining network architecture\n"}
{"snippet": "def softmax(sess, x):\n    <student.implement_softmax()>\n", "intent": "$$\\text{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$$\n"}
{"snippet": "def softmax_matrix(sess, x):\n    <student.implement_softmax()>\n", "intent": "$$\\text{softmax}(X)_{ij} = \\frac{\\exp(X_{ij})}{\\sum_j \\exp(X_{ij})}$$\n"}
{"snippet": "kmeans = KMeans(n_clusters=3, random_state=0)\nkmeans.fit(iris.data)\n", "intent": "Most algorithms have a `fit` method for training and then either a `transform` or a `predict` method that acts on new data.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n                batch_size=32,\n                epochs=10,\n                validation_data=(x_test, y_test),\n                verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "inputs = tf.placeholder(tf.float32, [None, n_states])\nweights = tf.Variable(tf.zeros([n_states,n_actions]))\noutputs = tf.matmul(inputs, weights)\ntargets = tf.placeholder(tf.float32, [None, n_actions])\nselected_actions = tf.argmax(outputs,1)\n", "intent": "Define Policy Network\n"}
{"snippet": "p_y_mean_test = sigmoid(beta_coef(x_test, beta_mean))\np_y_post_test = sigmoid(beta_coef(x_test, trace['betas'].T))\npost_pred_test = np.random.binomial(1, p_y_post_test)\np_y_pp = post_pred_test.mean(axis=1)\np_y_post_thresh = p_y_post_test[:, :]\np_y_post_thresh[p_y_post_thresh > 0.5] = 1\np_y_post_thresh[p_y_post_thresh <= 0.5] = 0\np_y_cdf = p_y_post_thresh.mean(axis=1)\n", "intent": "**II. Testing Data**\nWe perform the same analysis on the testing set.\n"}
{"snippet": "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\nhidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\nhidden_2 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\nhidden_3 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\noutput_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)\n", "intent": "Now that we've built a framework allowing us to define networks as a series of layers, we can easily build deeper networks.\n"}
{"snippet": "def sigmoid(x):\n    return 1.0/(1.0+np.exp(-x))\n", "intent": "$$ B = \\sigma(A) $$ or\n$$ b_1 = \\sigma(a_1) $$\n$$ b_2 = \\sigma(a_2) $$\n$$ b_3 = \\sigma(a_3) $$\n$$ b_4 = \\sigma(a_4) $$\n"}
{"snippet": "P = sigmoid(C)\narray_print(P)\n", "intent": "So we can simply code this up as:\n"}
{"snippet": "dPdC = sigmoid(C) * (1-sigmoid(C))\narray_print(dPdC)\n", "intent": "So, coding this up is simply:\n"}
{"snippet": "def setup_layers(hidden_neurons, outputs):\n    layers = []\n    for i in range(len(hidden_neurons)):\n        layer = FullyConnected(neurons=hidden_neurons[i], activation_function=sigmoid)\n        layers.append(layer)\n    output_layer = FullyConnected(neurons=outputs, activation_function=sigmoid)\n    layers.append(output_layer)\n    return layers\n", "intent": "First, a helper function to set up the layers of the neural net:\n"}
{"snippet": "def sigmoid(x, bprop=False):\n    if bprop:\n        return sigmoid(x) * (1-sigmoid(x))\n    else:\n        return 1.0/(1.0+np.exp(-x))\n", "intent": "We'll need to redefine our functions to have `bprop` option:\n"}
{"snippet": "def setup_layers(hidden_neurons, outputs, learning_rate=1.0):\n    layers = []\n    for i in range(len(hidden_neurons)):\n        layer = FullyConnectedLR(neurons=hidden_neurons[i], activation_function=sigmoid)\n        setattr(layer, \"learning_rate\", learning_rate)\n        layers.append(layer)\n    output_layer = FullyConnectedLR(neurons=outputs, activation_function=sigmoid)\n    setattr(output_layer, \"learning_rate\", learning_rate)\n    layers.append(output_layer)\n    return layers   \n", "intent": "We'll modify the a new `setup_layers` function to give each layer a learning rate: \n"}
{"snippet": "def _sigmoid(x):\n    return 1.0/(1.0+np.exp(-x))\n", "intent": "Refresher on the sigmoid\n"}
{"snippet": "B = _sigmoid(A)\n", "intent": "$$ B = \\sigma(A) $$ or\n$$ b_1 = \\sigma(a_1) $$\n$$ b_2 = \\sigma(a_2) $$\n$$ b_3 = \\sigma(a_3) $$\n$$ b_4 = \\sigma(a_4) $$\n"}
{"snippet": "MLR.fit()\n", "intent": "We defined our multinomial logistic model with Cross Entropy Loss above.  Let's train it now.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\nprint(model)\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "from sklearn import linear_model\nmodel_lr = linear_model.LogisticRegression()\n", "intent": "$$ \\frac{1}{2m} \\sum_i (h(x_i) - y_i)^2 \\text{ mit } h(x) = m*x + t$$\n$$  $$\n"}
{"snippet": "scaler.fit(df.drop(\"TARGET CLASS\", axis = 1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "                knn.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "logreg = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from sklearn.linear_model import Ridge\nmodel = make_pipeline(GaussianFeatures(30), \n                      Ridge(alpha=.05))\nbasis_plot(model, title='Ridge Regression')\n", "intent": "- Penalizes sum of squares (2-norms) of the model coefficients\n"}
{"snippet": "from sklearn.linear_model import Lasso\nmodel = make_pipeline(GaussianFeatures(30),\n                      Lasso(alpha=0.001))\nbasis_plot(model, title='Lasso Regression')\n", "intent": "- Penalizes sum of absolute values of regression coefficients\n- Favors sparse models where possible\n"}
{"snippet": "toy_sess = tf.Session()\n", "intent": "In order to evaluate `c` we have to provide values for the inputs ``a`` and ``b`` using feed_dict.\n"}
{"snippet": "x = tf.placeholder(\"float\")\ny = tf.placeholder(\"float\")\nop1 = tf.add(x, y)\nop2 = tf.multiply(x, y)\nuseless = tf.multiply(x, op1)\nop3 = tf.pow(op2, op1)\nwith tf.Session() as sess:\n    op3_value, not_useless_value = sess.run([op3, useless], feed_dict={x:2, y:3})\n    print(\"opt3 value: \",op3_value,  \"\\nnot_useless value\", not_useless_value)\n", "intent": "We can use symbolic inputs to define a single time the graph which we can use for as many values as we want\n"}
{"snippet": "mlrs = [MLR(lr=0.1, weight_decay=0.01, max_epoch=15).fit(trainloader, cvloader)]\n", "intent": "We construct a validation set by spliting 10% of data from training set.\n"}
{"snippet": "aux = tf.linspace(start=1., stop=10., num=5, name=None) \nwith tf.Session() as sess:\n    print(\"value in x: \", aux.eval())  \n", "intent": "- `tf.linspace`\n- `tf.range`\n"}
{"snippet": "x = tf.Variable(3, name=\"x\")\ny = tf.Variable(4, name=\"y\")\nf = x * y + y + 1\nsess = tf.Session()\nsess.run(x.initializer)\nsess.run(y.initializer)\nresult = sess.run(f)\nprint(result)\n", "intent": "If we initialize the variables the code will work as expected\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    result = sess.run(f)\n    print(result)\n", "intent": "Initializing all variables with `tf.global_variables_initializer()`\n"}
{"snippet": "W = tf.Variable(tf.truncated_normal([700, 10]))\nwith tf.Session() as sess:\n    sess.run(W.initializer)\n    print(W.eval())\n", "intent": "Recall that `var.eval()` is the same as `sess.run(W)`\n"}
{"snippet": "W = tf.Variable(3.14)\nW.assign(10)\nwith tf.Session() as sess:\n    sess.run(W.initializer)   \n    print(W.eval())\n", "intent": "This function allows to initialize and assign a value to a variable.\n"}
{"snippet": "with tf.Session() as sess:\n    s= saver.restore(sess, \"./saved_tests/my_save_test_begin.cpkt\")\n    sess.graph\n    print(sess.run(x))\n    print(sess.run(y))\n", "intent": "Restoring a model is as easy as calling saver.restore()\nAfter a session is closed, we can open a new session and use `save.restore\nis executed\n"}
{"snippet": "import tensorflow as tf\nimport numpy as np\nph = tf.placeholder(shape=[None,None], dtype=tf.int32)\nx = tf.slice(ph, [0, 0], [-1, 2])\ninput_ = np.array([[ 1, 2, 3, 4, 5, 6, 7, 8],\n                   [11,21,31,41,51,61,71,81],\n                   [11,21,31,41,51,61,71,81]])\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(x, feed_dict={ph: input_}))\n", "intent": "- `tf.slice` allow us to get an slice (or subset) of a variable.\nWe can use `tf.slice` on a placeholder\n"}
{"snippet": "tf.reset_default_graph()\na = tf.Variable(1, name='a')\npprint(tf.get_default_graph().get_operations())\nprint(len(tf.get_default_graph().get_operations()))\n", "intent": "This example adds nodes in the graph (see it ends up with 19 nodes)\n"}
{"snippet": "tf.reset_default_graph()\na = tf.Variable(1, name='a')\nb = tf.Variable(2, name='b')\ncompute_addition_a_b = tf.add(a, b)\n", "intent": "This example does not add nodes in the graph (well, only one, see it ends up with 10 nodes).\n"}
{"snippet": "lrs = [0.1, 0.01]\nbatch_sizes = [20, 50, 100, 200]\nhidden_dims = [25, 50, 100]\nmlps = [[[MLPClassifier(lr=lr, batch_size=b, hidden_dim=h).fit(trainset, verbose=False) \\\n         for h in hidden_dims] for b in batch_sizes] for lr in lrs]\n", "intent": "After a few trials, we decide to set `max_epoch` to 12.\n"}
{"snippet": "logistic = LogisticRegression(random_state=seed)\n", "intent": "instantiate the random search and fit it\n"}
{"snippet": "rf = RandomForestClassifier()\n", "intent": "instantiate the random search and fit it\n"}
{"snippet": "svm =svm.SVC(probability=True)\n", "intent": "instantiate the random search and fit it\n"}
{"snippet": "criterion = ClassNLLCriterion()\nnet = Sequential()\nnet.add(Linear(2, 4))\nnet.add(ReLU())\nnet.add(Dropout(p=0.95))\nnet.add(Linear(4, 4))\nnet.add(ReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\nprint(net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "test_scores = []\nfor feature in dataset_2.columns:\n    results = run_model(Lasso(alpha=100), 'variable ranking', 50, \n                        dataset_2[[feature]], target_2)\n    test_score = results['r2_test_score']\n    if test_score > 0.2:\n        test_scores.append({'feature': feature, 'score' : test_score})\n", "intent": "    run_model(model, model_name, n_pcnt, data, labels)\n"}
{"snippet": "results = run_model(Lasso(alpha=100), 'lasso', 100, dataset_2, target_2)\n", "intent": "Fit the full model in order to find the largest coefficients.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr_model = LogisticRegression()\n", "intent": "We import the `LogisticRegression` class from the `sklearn.linear_model` library and instantiate the `lr_model` object.\n"}
{"snippet": "cluster_model = KMeans(n_clusters=3)\n", "intent": "After importing the model, you instantiate `cluster_model`, an object of class `KMeans` with `n_clusters` set to 3.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)    \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "lams = [0.01, 0.001, 0.0001, 0]\nmlps2 = [MLPClassifier(lr=mlp_best.lr, batch_size=mlp_best.batch_size, \\\n                       hidden_dim=mlp_best.hidden_dim, weight_decay=l)\\\n         .fit(trainset, verbose=False) for l in lams]\n", "intent": "We use the best parameters chosen from the previous part and tune $\\lambda$ only.\n"}
{"snippet": "from distutils.version import LooseVersion\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nassert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\nprint('TensorFlow Version: {}'.format(tf.__version__))\n", "intent": "This will check to make sure that you have correct version of TensorFlow\n"}
{"snippet": "rf = RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=17) \nrf.fit(X_train, y_train)\n", "intent": "Train a random forest **(RandomForestClassifier)**. Set the number of trees to 100 and use **random_state = 17**.\n"}
{"snippet": "forest_params = {'max_depth': range(10, 21),\n                'max_features': range(5, 105, 20)}\nlocally_best_forest = GridSearchCV(rf,forest_params,n_jobs=-1,cv=2,verbose=True)\nlocally_best_forest.fit(X_train, y_train) \n", "intent": "Train a random forest **(RandomForestClassifier)**. Tune the maximum depth and maximum number of features for each tree using **GridSearchCV**. \n"}
{"snippet": "alice_top_sites = pd.Series(train_df[train_df['target'] == 1][sites].values.flatten()).value_counts().sort_values(ascending = False).head(5)\nprint(alice_top_sites)\nsites_dict.loc[alice_top_sites.index.tolist()]\n", "intent": "- videohostings\n- social networks\n- torrent trackers\n- news\n"}
{"snippet": "lr = linear_model.LinearRegression()\nsales = sales.dropna(how='any') \ndf = sales[[\"2015 Volume Sold (Liters)\", \"2015 Sales Q1\", \"2015 Margin mean\"]]\ny = sales['2016 Sales Q1'][df['2015 Sales Q1'] > 8000]\n", "intent": "Make some plots, look at correlations, etc.\n"}
{"snippet": "mean_knn_n1 = KNeighborsClassifier(n_neighbors=1,\n                              weights='uniform',\n                              p=2,\n                              metric='minkowski')\naccuracy_crossvalidator(X, Y, mean_knn_n1, cv_indices)\n", "intent": "As you can see the mean cross-validated accuracy is very high with 5 neighbors. \nLet's see what it's like when we use only 1 neighbor:\n"}
{"snippet": "from sklearn.manifold import TSNE\ntsne = TSNE()\ntsne.fit(X)\n", "intent": "TSNE is good to preserve local structure, but not global structure\n"}
{"snippet": "from sklearn.svm import SVC \nmodelSVMLinear = SVC(kernel=\"linear\")\nmodelSVMLinear.fit(XTrain,yTrain)\n", "intent": "<div class=\"exo\"> <b>Question:</b> Use Scikit-Learn to perform a first classification and evaluate the obtained performance.\n</div>\n"}
{"snippet": "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)\n", "intent": "Let's now evaluate the model using coherence score.\n"}
{"snippet": "mnist_MLP= Artificial_Neural_Network(MLP,reg_rate=0.001,learning_rate=0.1,batch_size=100, hidden=100)\nmnist_MLP.fit()\n", "intent": "**It looks like a learning rate of 0.1 is pretty clearly optimal.  As noted earlier, batch size has less effect, but let's use a size of 100 anyway**\n"}
{"snippet": "param_dist = {\"max_depth\": ,\n              \"max_features\":,\n             }\nclf = RandomForestClassifier(n_jobs=-1, n_estimators = 10)\nn_iter_search = 7\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   cv=4,\n                                   n_iter=n_iter_search)\nrandom_search.fit(X, y)\n", "intent": "Your turn: Define feature space\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nlogreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\nlr = LogisticRegression()\nclf = GridSearchCV(lr, logreg_parameters, cv = 5)\nclf.fit(X_train,y_train)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nkn_parameters = {\n    'n_neighbors':list(range(1,51)),\n    'weights':['uniform', 'distance'],\n    'leaf_size':list(range(20,51))\n}\nclf_k = GridSearchCV(KNeighborsClassifier(), kn_parameters, cv = 5)\nclf_k.fit(X_train,y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "logreg_parameters = {\n    'penalty':['l1','l2'],\n    'C':np.logspace(-5,1,50),\n    'solver':['liblinear']\n}\nlr = LogisticRegression()\nclf_pr = GridSearchCV(lr, logreg_parameters, cv = 5, scoring='average_precision')\nclf_pr.fit(X_train, y_train)\n", "intent": "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy.\n"}
{"snippet": "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\n", "intent": "Best results out of the three simple classifiers I tried.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=100, verbose=2, validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train , y_train)\n", "intent": "Standard Linear Regression Model\n"}
{"snippet": "model.fit(X_train , y_train,\n          cat_features=categorical_features,\n          eval_set = (X_test , y_test),\n          plot = True)\n", "intent": "now let's fit our model to our training data and plot the results to visualise the increase of accuracy\n"}
{"snippet": "hist = model.fit(x_train, y_train, epochs=20, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "mlps = []\naccuracies = []\nlambdas = [0.001, 0.01, .1, .5]\nfor lambda_i in lambdas: \n    print(\"MNIST MLP -- Lambda {}\".format(lambda_i))\n    mnist_MLP= Artificial_Neural_Network(MLP,reg_rate=lambda_i,learning_rate=0.1,batch_size=100, hidden=100)\n    mnist_MLP.fit(show_validation=False)\n    accuracies.append(mnist_MLP.get_params(\"validation_scores\"))\n    mnist_MLP.score(dataset=\"Validation\", save_misclassified=True) \n    mlps.append(mnist_MLP)\n", "intent": "**Great.  Test set accuracy of ~96%.  Let's try some different regularization rates.**\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dropout,Dense,Flatten\nmodel = Sequential()\nmodel.add(Dense(512,activation='relu',input_shape=(1000,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train,y_train,epochs=10,batch_size=10,verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def sigmoid(v):\n    a=np.exp(-v)\n    b=1+a\n    yhat=1/b\n    return yhat\n", "intent": "This function takes input as vector and outputs sigmoid of every element of that vector. Here v is vector.\n"}
{"snippet": "def gradient_ascent(Xs,y,learning_rate,beta,m,num_iters):\n    for i in range(num_iters):\n        yhat=sigmoid(np.dot(Xs,beta))\n        temp = beta + (learning_rate/m) * -(np.dot(Xs.transpose(), yhat-y))\n        beta = temp\n        if(i%100==0):\n            cost= compute_cost(Xs,y,beta,m)\n            print(\"Cost\")\n            print(cost)\n    return beta      \n", "intent": "Write the code to perform Gradient Descent in the function below.\n"}
{"snippet": "W1 = tf.get_variable(\"W1\", [30, 30], initializer = tf.random_normal_initializer)\n", "intent": "Remember that the parameters/weights have to be Tensorflow Variables.<br>\nWe should not initialize W as zeros. We can initialize b as 0.\n"}
{"snippet": "theta = tf.Variable([a_init, b_init])\nsess.run(tf.global_variables_initializer())\n", "intent": "$$\\theta = \\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}$$\n"}
{"snippet": "def inception(x, reuse):\n    preprocessed = tf.multiply(tf.subtract(x, 0.5), 2.0)\n    arg_scope = nets.inception.inception_v3_arg_scope(weight_decay=0.0)\n    with slim.arg_scope(arg_scope):\n        logits, _ = nets.inception.inception_v3(\n            preprocessed, 1001, is_training=False, reuse=reuse)\n        logits = logits[:,1:] \n        probs = tf.nn.softmax(logits) \n    return logits, probs\nlogits, probs = inception(tf.expand_dims(x, 0), reuse=False)\n", "intent": "Next, we load the Inception v3 model.\n"}
{"snippet": "utils.plot_model(x1, y_hat)\n", "intent": "<font color=\"1874CD\"> **A line.**\n"}
{"snippet": "utils.plot_model(x2, y_hat-w[1]*x1)\n", "intent": "Using the cell below, plot `y_hat-w[1]*x1` as a function of `x2`.\n"}
{"snippet": "m1 = Model(dfs[1]).run()\n", "intent": "We can test the model on one of the restaurant we chose.\n"}
{"snippet": "utils.plot_model(x1, y_hat)\n", "intent": "The cell below will plot the input values against the output values for your model.\n"}
{"snippet": "x = torch.Tensor(5,3)\nx\n", "intent": "Construct a 5x3 matrix, uninitialized:\n"}
{"snippet": "for epoch in range(epochs):\n  hidden_activation = sigmoid(np.dot(X, Theta_hidden))\n  y_hat = np.dot(hidden_activation, Theta_output)\n  loss = criterion(y_hat, y)\n  delta_output = learning_rate * loss\n  Theta_output += hidden_activation.T.dot(delta_output)\n  delta_hidden = delta_output.dot(Theta_output.T) * sigmoid_prime(hidden_activation)\n  Theta_hidden += X.T.dot(delta_hidden)\n  if epoch % 5000 == 0:\n    print(f'epoch {epoch}: error {np.mean(loss)}')\n", "intent": "We can now train our model via the Backpropagation algorihm. It is time to use those derivates:\n"}
{"snippet": "X = Variable(torch.Tensor([[1.0], [2.0], [3.0]]))\nY = Variable(torch.Tensor([[2.0], [4.0], [6.0]]))\n", "intent": "We'll have to wrap the data in PyTorch variables & Tensors:\n"}
{"snippet": "for epoch in range(10):\n    y_pred = model(X)\n    loss = criterion(y_pred, Y)\n    print(\"epoch:\", epoch, loss.data[0])\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n", "intent": "Training using PyTorch is much simpler:\n"}
{"snippet": "hour_var = Variable(torch.Tensor([[4.0]]))\ny_pred = model(hour_var)\nprint(\"predict (after training)\",  4, model(hour_var).data[0][0])\n", "intent": "Let's see what model predicts:\n"}
{"snippet": "with pm.Model() as hierarchical_model:\n    m = pm.Categorical('m', np.asarray([.5, .5]))\n    kappa = 12\n    omega = pm.math.switch(eq(m, 0), .25, .75)\n    theta = pm.Beta('theta', omega*(kappa)+1, (1-omega)*(kappa)+1)\n    y = pm.Bernoulli('y', theta, observed=[1,1,1,1,1,1,0,0,0])    \n", "intent": "Coin is flipped nine times, resulting in six heads.\n"}
{"snippet": "with pm.Model() as hierarchical_model2:\n    m = pm.Categorical('m', np.asarray([.5, .5]))\n    omega_0 = .25\n    kappa_0 = 12\n    theta_0 = pm.Beta('theta_0', omega_0*(kappa_0)+1, (1-omega_0)*(kappa_0)+1)\n    omega_1 = .75\n    kappa_1 = 12\n    theta_1 = pm.Beta('theta_1', omega_1*(kappa_1)+1, (1-omega_1)*(kappa_1)+1)\n    theta = pm.math.switch(eq(m, 0), theta_0, theta_1)\n    y2 = pm.Bernoulli('y2', theta, observed=[1,1,1,1,1,0,0,0])    \n", "intent": "Coin is flipped nine times, resulting in six heads.\n"}
{"snippet": "lda = LDA(n_components=2)\nlda.fit(wine_train, wine_train_labels)\nwine_train_lda = lda.transform(wine_train)\nwine_test_lda = lda.transform(wine_test)\n", "intent": "Fit an LDA model to the training data, and transform the test data onto 2 LDA components\n"}
{"snippet": "model = neighbors.KNeighborsClassifier(n_neighbors = 1).\\\n    fit(X, c)\n", "intent": "- (http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed=tf.nn.embedding_lookup(embedding,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "mod=clf.fit(X,y)\n", "intent": "Regularized Cost Function, l1 norm = $-\\sum_{1}^{n}[ y_i log(p_i)+(1-y_i)log(1-p_i)]+\\frac{1}{C}|\\beta|$, here $\\frac{1}{C}=\\alpha$\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n", "intent": "Now that we have a nice vector representation of all sample documents, lets apply a classifier. \n"}
{"snippet": "input_bias = tf.Variable(tf.zeros(shape=(*input_nodes,)), name='input_bias')\n", "intent": "We use the same pattern for the biases.\n"}
{"snippet": "input_layer = tf.nn.relu(pre_activation, name='input_layer_output')\n", "intent": "Finally we can form the layer by squeezing the output through a rectified linear unit.\n"}
{"snippet": "h_w = tf.Variable(tf.random_normal((*input_nodes, *hidden_nodes)), name='hidden_weights')\nh_b = tf.Variable(tf.zeros((*hidden_nodes, ), name='hidden_bias'))\nhidden_layer = tf.nn.relu(tf.add(tf.matmul(input_layer, h_w), h_b))\n", "intent": "We create a hidden layer using the same logic.\n"}
{"snippet": "o_w = tf.Variable(tf.random_normal((*hidden_nodes, *output_shape)), name='output_weights')\no_b = tf.Variable(tf.zeros((*output_shape, ), name='output_bias'))\noutput = tf.add(tf.matmul(hidden_layer, o_w), o_b)\n", "intent": "The output layer has no activation function (aka a linear activation function).  This allows the network to output negative values.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n", "intent": "Logistic regresssion\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg_bal = LogisticRegression(class_weight='balanced')\nlogreg_bal.fit(X_train, y_train)\n", "intent": "balanced logistic regresion\n"}
{"snippet": "X = df[ ['Income', 'Cards', 'Age', 'Education', 'Gender_Male', 'Ethnicity_Asian', 'Ethnicity_Caucasian'] ]\ny = df.Balance\nmodel = linear_model.LinearRegression()\nmodel.fit(X,y)\nprint model.intercept_\nprint model.coef_\n", "intent": "First, find the coefficients of your regression line.\n"}
{"snippet": "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\nparam_grid = dict(optimizer=optimizer)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_log_loss')\ngrid_result = grid.fit(X, y)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n", "intent": "build fn create model is called via an optimizer parameter. \n"}
{"snippet": "model =create_model(1000,100,30)\n", "intent": "create  the model and train and validate on valiation data save the best weights \nby watching the validation logarthmic loss ,monitoring\n"}
{"snippet": "model = ks.models.Sequential()\nmodel.add(ks.layers.Conv2D(32, (3,3), activation = \"relu\", input_shape=(150,150,3)))\nmodel.add(ks.layers.MaxPooling2D(2,2))\nmodel.add(ks.layers.Conv2D(64, (3,3), activation = \"relu\"))\nmodel.add(ks.layers.MaxPooling2D(2,2))\nmodel.add(ks.layers.Flatten()) \nmodel.add(ks.layers.Dropout(0.5))\nmodel.add(ks.layers.Dense(512,activation=\"relu\"))\nmodel.add(ks.layers.Dense(1,activation=\"sigmoid\"))\n", "intent": "<h4>Define the network</h4>\n"}
{"snippet": "scaler.fit(X_train)\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "with tf.Session() as sess:\n    saver = tf.train.Saver()   \n    saver.restore(sess, './model/model.checkpoint')\n    test_predicted = sess.run(predict, feed_dict=test_feed_dict)\n    test_accuracy = nn.calculate_accuracy(test_orders, test_predicted)\ntest_accuracy\n", "intent": "The following code loads your model, computes accuracy, and exports the result. **DO NOT** change this code.\n"}
{"snippet": "with tf.Graph().as_default():\n    x = tf.get_variable(\"param\", [])  \n    loss = -tf.log(tf.sigmoid(x))  \n    optim = tf.train.AdamOptimizer(learning_rate=0.1)\n    min_op = optim.minimize(loss)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(x.assign(1.5))\n        for i in range(10):\n            print(sess.run([min_op, loss], {})[1])\n", "intent": "- Adadelta\n- **Adam**\n"}
{"snippet": "with tf.Graph().as_default():\n    x = tf.placeholder(tf.float32, [None], \"input\")\n    x_dropped = tf.nn.dropout(x, 0.7)  \n    with tf.Session() as sess:        \n        print(sess.run(x_dropped, {x: np.random.rand(6)}))\n", "intent": "<img src=\"http://everglory99.github.io/Intro_DL_TCC/intro_dl_images/dropout1.png\" width=1000></img>\nFor RNNs: `tf.nn.rnn_cell.DropoutWrapper`\n"}
{"snippet": "def plot_top_k(alpha, label='pos', k=10):\n    positive_words = [w for (w,y) in alpha.keys() if y == label]\n    sorted_positive_words = sorted(positive_words, key=lambda w:-alpha[w,label])[:k]\n    util.plot_bar_graph([alpha[w,label] for w in sorted_positive_words],sorted_positive_words,rotation=45)\nplot_top_k(alpha)\n", "intent": "Let us look at the $\\balpha$ parameters that correspond to the probability of generating a word given a class.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nimport numpy as np\nlr = LogisticRegression(C=0.25)\nlr.fit(train_X, train_Y)\n", "intent": "Finally, we can train the logistic regression model using a regularisation parameter $C=0.25$.\n"}
{"snippet": "logr = LogisticRegression()\nlogr.fit(X,y);\n", "intent": "Let's fit a logistic regression model on the data above and plot the predicted labels and the probabilities\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nimport numpy as np\nlr = LogisticRegression(C=0.25, class_weight='balanced')\nlr.fit(train_X, train_Y)\n", "intent": "Train the logistic regression with l2 regularisation $C=0.25$\n"}
{"snippet": "dotprod_pos, dotprod_neg, diff_dotprod, placeholders = ie.create_model_f_reader(max_lens_rel, max_lens_ents, repr_dim, vocab_size_rels,\n                          vocab_size_ents)\nloss = tf.reduce_sum(tf.nn.softplus(-dotprod_pos)+tf.nn.softplus(dotprod_neg))\n", "intent": "Now that we have read in the data, vectorised it and created the universal schema relation extraction model, let's start training\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    W = tf.Variable(\n    tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0), name='W')\n    embedding_layer = tf.nn.embedding_lookup(W, input_data)\n    return embedding_layer\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "ridge = Ridge(alpha=0.5)\nridge.fit(X_train, y_train)\n", "intent": "We're really just doing this to make sure that we don't have any issues in our data that will break our model fitting.\n"}
{"snippet": "gd = GradientDescent(lrf.cost, lrf.gradient, lrf.predict, \n                     fit_intercept=False, num_iterations=100)\ngd.fit(X, y)\n", "intent": "Let's test this on our fake data.\n"}
{"snippet": "gd_standardized = GradientDescent(lrf.cost, lrf.gradient, lrf.predict, \n                                  fit_intercept=True, standardize=True,\n                                  num_iterations=100)\ngd_standardized.fit(X, y)\n", "intent": "Let's fit to our simulated data.\n"}
{"snippet": "gd_regularized = GradientDescent(lrf.cost, lrf.gradient, lrf.predict, \n                                 fit_intercept=True, standardize=True,\n                                 num_iterations=100,\n                                 cost_func_parameters={'lam': 5})\ngd_regularized.fit(X, y)\n", "intent": "Let's test this out by adding some simple ridge shrinkage to our test model.\n"}
{"snippet": "lambdas = [0.0, 0.5, 1, 2, 5, 10, 25, 50]\nmodels = OrderedDict()\nfor lam in lambdas:\n    gd = GradientDescent(lrf.cost, lrf.gradient, lrf.predict,\n                         fit_intercept=True, standardize=True,\n                         num_iterations=100,\n                         cost_func_parameters={'lam': lam})\n    gd.fit(X, y)\n    models[lam] = gd\n", "intent": "It's fun to see what happens to our regularized decision boundary as we vary the parameter:\n"}
{"snippet": "gd.fit(X, y)\n", "intent": "We can also use some ipython magic to time the two fit methods.\n"}
{"snippet": "V = df[[\"valence\"]]\nlr_V = LogisticRegression()\nlr_V.fit(V, y);\n", "intent": "<b>Train model using one feature: \"danceability\"</b>\n"}
{"snippet": "clf = LogisticRegression(intercept_scaling=100).fit(X, y)\n", "intent": "2\\. Fit `LogisticRegression` with `gym_hours` and `email_hours` as features and `data_scientist` as the response.\n"}
{"snippet": "preds = ['2015 Avg Price Per Liter']\nx = model_copy[preds]\ny = model_copy['2015 Sales']\nlmodel = Lasso(alpha = 1)\nlmodel.fit(x ,y)\n", "intent": "Using scikit-learn or statsmodels, build the necessary models for your scenario. Evaluate model fit.\n"}
{"snippet": "formula = 'label ~ ' + ' + '.join(numeric_cols) + ' -1'\neg_lr = smf.logit(formula, data=su)\neg_results = eg_lr.fit()\nprint eg_results.summary()\n", "intent": "And print out the results as shown in the example above.\n---\n"}
{"snippet": "lr_biz = LogisticRegression()\nlr_biz_parameters = {\n    'penalty':['l1','l2'],\n    'solver':['liblinear'],\n    'C':np.logspace(-4,0,20),\n}\nlr_biz_gs = GridSearchCV(lr_biz, lr_biz_parameters, cv=5, verbose=1)\nlr_biz_gs.fit(Xinter_n, business)\n", "intent": "Include Ridge and Lasso.\n---\n"}
{"snippet": "lr_biz_pr = LogisticRegression()\nlr_biz_pr_parameters = {\n    'penalty':['l1','l2'],\n    'solver':['liblinear'],\n    'C':np.logspace(-4,0,20)\n}\nlr_biz_pr_gs = GridSearchCV(lr_biz_pr, lr_biz_pr_parameters, \n                               cv=5, verbose=2, scoring='precision')\nlr_biz_pr_gs.fit(Xinter_n, business)\n", "intent": "Look at the documentation.\n---\n"}
{"snippet": "best_title_model = LogisticRegression(penalty = 'l2', C = best_c_title, class_weight= None, solver = 'liblinear')\n", "intent": "Print out the top 20 or 25 word features as ranked by their coefficients.\n---\n"}
{"snippet": "ind = np.unravel_index(np.argmax(np.mean(neg_mses, axis=-1)), dims=neg_mses.shape[:-1])\nregression_tree = sklearn.tree.DecisionTreeRegressor(max_depth=depth_params[ind[1]],\n                                                     max_leaf_nodes=leaf_params[ind[0]]).fit(X, y_obs)\n", "intent": "We can now use the hyperparameters selected by cross validation to retrain the model and compare it tto the generative model.\n"}
{"snippet": "Cs = np.exp(np.linspace(-5, 8, 20))\nnfold = 5\nlrCV = skl.LogisticRegressionCV(Cs=Cs,\n                                cv=nfold, penalty=\"l1\",\n                                solver='saga',\n                                multi_class=\"multinomial\",\n                                scoring=\"neg_log_loss\",\n                                refit=True)\nlrCV.fit(x_train, y_train)\n", "intent": "We use cross validation to select the tuning parameter.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(\n    tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\nembed = get_embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "logr = LogisticRegression();\n", "intent": "Let's fit a logistic regression model on the data above and plot the predicted labels and the probabilities\n"}
{"snippet": "model = LeNetOneShot()\ndef eval_func(input_data, output, loss):\n    anchor, positive, negative = output\n    positive_distance = F.pairwise_distance(anchor, positive)\n    negative_distance = F.pairwise_distance(anchor, negative)\n    return (positive_distance < negative_distance).sum().item()\nlearner = Learner(model, data_loader_train, data_loader_test)\nlearner.fit(model_loss_func, 0.002, num_epochs=100, eval_func=eval_func, early_stop='loss')\n", "intent": "***BE CAREFUL***\nThe accuracy depends on threshold. Right now it's unreliable. We will find the best threshold later\n"}
{"snippet": "model = FaceNetModel(embedding_size=128, num_classes=10000)\ndef eval_func(input_data, output, loss):\n    anchor, positive, negative = output\n    positive_distance = F.pairwise_distance(anchor, positive)\n    negative_distance = F.pairwise_distance(anchor, negative)\n    return (positive_distance < negative_distance).sum().item()\nlearner = Learner(model, data_loader_train, data_loader_test)\nlearner.fit(model_loss_func, 0.002, num_epochs=100, eval_func=eval_func, early_stop='loss')\n", "intent": "***BE CAREFUL***\nThe accuracy depends on threshold. Right now it's unreliable. We will find the best threshold later\n"}
{"snippet": "lr = LinearRegression()\ndtr = DecisionTreeRegressor()\nrfr = RandomForestRegressor()\nabr = AdaBoostRegressor()\n", "intent": "> **Step 3:** Now that you have imported the 4 models that can be used for regression problems, instantate each below.\n"}
{"snippet": "lr.fit(X_train, y_train)\ndtr.fit(X_train, y_train)\nrfr.fit(X_train, y_train)\nabr.fit(X_train, y_train)\n", "intent": "> **Step 4:** Fit each of your instantiated models on the training data.\n"}
{"snippet": "model.fit(x=x_train, y=y_train, epochs=5, batch_size=128)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state=0)\n", "intent": "Initialize a decision tree model\n"}
{"snippet": "model = KNeighborsClassifier(5).fit(f_train, t_train)\n", "intent": "Train the KNN classifier\n"}
{"snippet": "print crossValidate(features, target, KNeighborsClassifier(3),10, 0)\nprint crossValidate(features, target, KNeighborsClassifier(4),10, 0)\nprint crossValidate(features, target, KNeighborsClassifier(5),10, 0)\n", "intent": "Test the cross-validation function on different numbers of neighbors\n"}
{"snippet": "model = RandomForestClassifier(random_state=0)\n", "intent": "Create an instance of a random forest classifier.  Random state is used to set random number generator for reproducible results\n"}
{"snippet": "model = tree.DecisionTreeRegressor(random_state = 0).\\\n    fit(train_X, train_y)\n", "intent": "- (http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor)\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_matrix = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    word_embeddings = tf.nn.embedding_lookup(embedding_matrix, input_data)\n    return word_embeddings\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "rf_clf = RandomForestClassifier(n_estimators=200, oob_score=True, n_jobs=-1,\n                                random_state=42, verbose=1, class_weight='balanced')\nrf_clf.fit(X, y)\nprint('Out-of-bag generalization accuracy estimate: {0:.2f} %'.format(rf_clf.oob_score_))\n", "intent": "Now the data is ready for modeling, with the adopted_user column serving as our target variable. Random forest will be used for the predictions here.\n"}
{"snippet": "happyModel.fit(x = X_train, y = Y_train , epochs = 40, batch_size = 16)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model.fit(X_train, Y_train, epochs = 100, batch_size = 32)\n", "intent": "Run the following cell to train your model on 2 epochs with a batch size of 32. On a CPU it should take you around 5min per epoch. \n"}
{"snippet": "from sklearn.kernel_ridge import KernelRidge\nparam_grid={'alpha':[0.2,0.3,0.4], 'kernel':[\"polynomial\"], 'degree':[3],'coef0':[0.8,1]}\ngrid(KernelRidge()).grid_get(train_sel_60,target_df,param_grid)\n", "intent": "    3. Kernel Ridge\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.25))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.16))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=300, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "regr_1 = DecisionTreeRegressor(max_depth=1)\nregr_1.fit(X, y)\n", "intent": "-What does the data look like before the last line?\n-Why did I put the pi in the definition of y and what is the effect? \n"}
{"snippet": "mod = smf.ols(formula='log_marketcap ~ log_revenue + log_employees  + log_assets', data=df_companies)\nres = mod.fit()\nprint(res.summary())\n", "intent": "Revenue, employees and assets are highly correlated.\nLet's imagine wwe want to explain the market capitalization in terms of the other variables. \n"}
{"snippet": "model = tree.DecisionTreeClassifier(max_depth = 2,\n    min_samples_leaf = 5,\n    random_state = 0).\\\n        fit(train_X, train_y)\n", "intent": "- (http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier)\n"}
{"snippet": "from sklearn import linear_model\nreg = linear_model.Lasso(alpha = 0.01)\nreg.fit(X_train,y_train)\nprint(\"log_revenue \tlog_employees \tlog_assets \")\nprint(reg.coef_)\n", "intent": "**Lasso**\n- Have a penalty\n- Discards the variables with low weights.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    word_embeddings = tf.get_variable(\"word_embeddings\",[vocab_size, embed_dim])\n    embedded_word_ids = tf.gather(word_embeddings, input_data)\n    print (embedded_word_ids)\n    return embedded_word_ids\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "rfcl = RandomForestClassifier(random_state=seed)\nrfcl.fit(xTrain, yTrain)\n", "intent": "We can see that maximum score can be achieved using regularization of max_dept = 3.\n"}
{"snippet": "rfcl = RandomForestClassifier(n_estimators=3, max_depth=5, random_state=seed)\nrfcl.fit(xTrain, yTrain)\nrfcl.score(xTest, yTest)\n", "intent": "Random Forest classifier has given us an accuracy of **77.05%** which is more than Decision tree classifier.\n"}
{"snippet": "tss = TimeSeriesSplit(n_splits=10)\nlogit_cv2 = LogisticRegressionCV(Cs = [3], n_jobs=-1, scoring ='roc_auc', cv=tss)\nlogit_cv2.fit(X_train, y_train)\n", "intent": "Perform cross-validation with logistic regression.\n"}
{"snippet": "tss = TimeSeriesSplit(n_splits=7)\nlogit_cv = LogisticRegressionCV(Cs = [1], n_jobs=-1, scoring ='roc_auc', cv=tss)\nlogit_cv.fit(X_train, y_train)\n", "intent": "Perform cross-validation with logistic regression.\n"}
{"snippet": "print(\"C \\t train \\t test\")\nfor C in [0.1,0.5,0.7,1,3,10,100,1000]:\n    svm=SVC(C=C)\n    svm.fit(Xtrain_scaled,ytrain)\n    print(\"{} \\t {:.3f} \\t {:.3f}\".format(C,svm.score(Xtrain_scaled,ytrain), svm.score(Xtest_scaled,ytest)))\n", "intent": "1. Scaling made a huge difference!\n1. We can alter the fit parameters 'C',gamma for a better model score.\n"}
{"snippet": "sales_reduced_columns = [u'2015 Sales mean', u'Price per Liter mean', u'2015 Volume Sold (Liters)',\n                         u'2015 Volume Sold (Liters) mean', u'2015 Margin mean']\nfor x in sales_reduced_columns:\n    model = sm.OLS(sales[\"2015 Sales\"], sales[x]).fit()\n    print x, \":\", model.mse_model\n", "intent": "Using scikit-learn or statsmodels, build the necessary models for your scenario. Evaluate model fit.\n"}
{"snippet": "k=2\nkmeans=cluster.KMeans(n_clusters=k)\nkmeans.fit(dn)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "model = ensemble.RandomForestClassifier(n_estimators = 1000,\n        max_features = 4,\n        min_samples_leaf = 5,\n        oob_score = True,\n        random_state = 0).\\\n    fit(train_X, train_y)\n", "intent": "- (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier)\n"}
{"snippet": "lm = smf.ols(formula='speed ~ wind_speed', data = df).fit()\nprint (lm.summary())\n", "intent": "The R-squared is 0.026 which is small, so it seems 'Visibility' has no correlation to 'speed'.\n"}
{"snippet": "from sklearn import ensemble\nclf =  ensemble.RandomForestClassifier(max_depth=3, criterion=\"gini\", random_state=222).fit(titanic_short.values, titanic[\"Survived\"])\nclf.score(titanic_short.values, titanic[\"Survived\"])\n", "intent": "changing model to Random Forest\n"}
{"snippet": "clf =  ensemble.GradientBoostingClassifier(max_depth=4, random_state=222).fit(titanic_short.values, titanic[\"Survived\"])\nclf.score(titanic_short.values, titanic[\"Survived\"])\n", "intent": "changing model to Gradient Boosted Trees\n"}
{"snippet": "lr = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "rfc_grid2 = RandomForestClassifier(n_jobs=-1, random_state = 42,bootstrap= False, criterion = 'entropy' )\nparam_grid2 = {\n              \"max_features\": [ 10,'auto' ],\n              \"min_samples_split\": [2, 10, 15],\n              \"min_samples_leaf\": [1, 10, 15] }\nCV_rfc2 = GridSearchCV(estimator=rfc_grid2, param_grid=param_grid2, cv= 3, scoring= \"f1\" , n_jobs=-1)\nCV_rfc2.fit(X_train_stem, y_train_stem)\nprint (CV_rfc2.best_params_)\n", "intent": "Keep from the last grid search:  bootstrap= False, criterion = 'entropy'.\n"}
{"snippet": "rfc = RandomForestClassifier(n_jobs=-1, random_state = 36) \nparam_grid = { \n           \"n_estimators\" : [24,26,28], \n           \"max_depth\" : [30,35,40], \n           \"min_samples_leaf\" : [8,10,12]}\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 3, scoring= \"f1\" , n_jobs=-1)\nCV_rfc.fit(x_train_res, y_train_res)\nprint (CV_rfc.best_params_)\n", "intent": "For Grid Search CV : scoring can be  =  f1, accuracy, recall, or precision    \n"}
{"snippet": "est = LogisticRegression(dual=False)\n", "intent": "We create an instance of a Logistic Regression Classifier ...\n"}
{"snippet": "est.fit(X, y)\n", "intent": "... and fit it to the training set (i.e., we learn the patterns required to predict digits).\n"}
{"snippet": "scaler_model.fit(data)\n", "intent": "just convert int32 to float64\n"}
{"snippet": "mlp = MLPClassifier(hidden_layer_sizes=(20,10), max_iter=1000, activation = \"logistic\", solver = \"lbfgs\", learning_rate_init=0.9, learning_rate = \"constant\", verbose = True, early_stopping = False, tol=0.001, random_state = 10000)\n", "intent": "accuracy of the model could vary depending on which rows were selected for the training and test sets.\n"}
{"snippet": "model.fit(x_train,y_train,epochs=10,batch_size=100,validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "dtree =  DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\ngrid.fit(X_train, y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "LogReg = LogisticRegression()\nLogReg.fit(X_train, y_train)\n", "intent": "<h2> Logistic regression </h2>\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\nrf.fit(X_train, y_train)\n", "intent": "<h3> Random Forest </h3>\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(train[['Gr Liv Area']], train['SalePrice'])\nprint(lr.coef_)\nprint(lr.intercept_)\na0 = lr.intercept_\na1 = lr.coef_\n", "intent": "lr.coef_ : W <br>\nlr.intercept: b\n"}
{"snippet": "test_input = [[0.25, 0.15]]\ngraph=tf.Graph()\nwith tf.Session() as session:\n    tf.global_variables_initializer().run(session=session)\n    output = session.run([out], feed_dict = feed_dict)\n    print(output[0]) \n", "intent": "As with the previous example, we can use `session.run()` to execute this computation graph, and use a `feed_dict` to feed in our input:\n"}
{"snippet": "params = {'max_depth':(5, 10, 15),\n         'min_samples_split':(.05, .075, .1),\n         'min_samples_leaf':(.05, .075, .1)}\ndtr2 = DecisionTreeRegressor(random_state=0,\n                            criterion='mae',\n                            splitter='best',\n                            max_features='auto')\nclf2 = GridSearchCV(dtr2, params)\nclf2.fit(train[columns], train['cnt'])\n", "intent": "* Based on the first result from GridSearchCV, re-arrange the parameters and execute the second GridSearchCV.\n"}
{"snippet": "with train_graph.as_default():\n    saver = tf.train.Saver()\nwith tf.Session(graph=train_graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    embed_mat = sess.run(embedding)\n", "intent": "Restore the trained network :\n"}
{"snippet": "RF = RandomForestRegressor(n_estimators = 10000, \n                           max_features = 4,     \n                           min_samples_leaf = 10, \n                           oob_score = True,    \n                           random_state = 1)      \nRF.fit(X,y)\n", "intent": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n"}
{"snippet": "lm.fit(df_train,train_target)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "lg=LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from keras.models import Sequential\nfrom keras.layers import Dense, Activation, LSTM\nfrom keras.optimizers import RMSprop\nmodel = Sequential()\nmodel.add(LSTM(128, input_shape=(MAXLEN, len(chars)), dropout=0.5))\nmodel.add(Dense(len(chars)))\nmodel.add(Activation('softmax'))\noptimizer = RMSprop(lr=0.001)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\n", "intent": "Build an LSTM language model\n"}
{"snippet": "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=17)\nrf.fit(X_train, y_train)\n", "intent": "Train a random forest **(RandomForestClassifier)**. Set the number of trees to 100 and use **random_state = 17**.\n"}
{"snippet": "forest_params = {'max_depth': range(10, 21),\n                'max_features': range(5, 105, 20)}\nlocally_best_forest = GridSearchCV(rf, forest_params, cv=5, n_jobs=-1, verbose=True) \nlocally_best_forest.fit(X_train, y_train)\n", "intent": "Train a random forest **(RandomForestClassifier)**. Tune the maximum depth and maximum number of features for each tree using **GridSearchCV**. \n"}
{"snippet": "scaler.fit(bn.drop('Class', axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "model_l1_cv.fit(X_train, y_train)\n", "intent": "Now we can run the cross-validated optimization on our training set.\n"}
{"snippet": "model_l1.fit(X_train, y_train)\n", "intent": "We train the model on the training set with the standard 'fit' method:\n"}
{"snippet": "nclasso = ncLasso(transposed_incidence=tim, lambda1=0.001, lambda2=0.001)\nnclasso.fit(X_2W_tr, y_2W_tr)\n", "intent": "__Use the network-constrained Lasso on the data. What do you observe?__\n"}
{"snippet": "X = CreditData[['Education','Race_Asian','Race_Caucasian','Gender_Female','Age','Cards','Income']]\nX.head(2)\ny = CreditData['Balance']\nfrom sklearn.linear_model import LinearRegression  \nlinreg = LinearRegression() \nlinreg.fit(X,y)     \nprint(linreg.intercept_)\nprint(linreg.coef_)\n", "intent": "First Step, find the coefficients of your regression line\n"}
{"snippet": "input_ques = Input(shape=(None,), dtype='int32', name='ques')\nembeded_ques = layers.Embedding(32, ques_vocab_size)(input_ques)\nencoded_ques = layers.LSTM(16)(embeded_ques)\n", "intent": "**Defining input for question**\n"}
{"snippet": "input_ = layers.concatenate([encoded_text, encoded_ques], axis=-1)\n", "intent": "**Concatanating the input into one**\n"}
{"snippet": "answer = layers.Dense(ans_vocab_size, activation='softmax')(input_)\n", "intent": "**Defining Output**\n"}
{"snippet": "hist = model.fit([input_text, input_ques], answer, epochs=10, batch_size=128)\n", "intent": "**Fitting the model**\n"}
{"snippet": "clf = RandomForestRegressor(n_jobs=-1)\nclf.fit(raw.drop(['SalePrice'], axis=1), raw.SalePrice)\n", "intent": "**Using RandomForestRegressor algorithm as baseline algorithm**\n"}
{"snippet": "x = tf.Variable(5)\n", "intent": "Whn a tensor needs to be modified `tf.placeholder()` or `tf.constant()` can't serve the purpose. For this purpose `tf.Variable()` is used.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_matrix = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    return tf.nn.embedding_lookup(embedding_matrix, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "e, fs = anomaly.select_threshold(X, Xval, yval)\nprint('Best epsilon: {}\\nBest F-score on validation data: {}'.format(e, fs))\n", "intent": "<img style=\"float: left;\" src=\"../img/f1_score.png\">\n"}
{"snippet": "clf = linear_model.LogisticRegression(C=1)\nclf.fit(xtrain,ytrain)\n", "intent": "Let's try this special case of a SVM and see if drawing a line through the data to decide on the outcome is any better.\n"}
{"snippet": "lm = smf.ols(formula='y ~ X', data = CreditData).fit()\nlm.summary()\nprint(\"P-Vales: \", zip(['Education','Race_Asian','Race_Caucasian','Gender_Female','Age','Cards','Income'], \n                      lm.pvalues[1:8]))\n", "intent": "Second Step, find the p-values of your estimates. You have a few variables try to show your p-values along side the names of the variables.\n"}
{"snippet": "_x = np.array([1, 2, 3])\n_y = np.array([-1, -2, -3])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nwith tf.Session() as sess:\n    print(sess.run(x + y))\n", "intent": "Q1. Add x and y element-wise.\n"}
{"snippet": "_x = np.array([3, 4, 5])\n_y = np.array(3)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nwith tf.Session() as sess:\n    print(sess.run(x-y))\n", "intent": "Q2. Subtract y from x element-wise.\n"}
{"snippet": "_x = np.array([3, 4, 5])\n_y = np.array([1, 0, -1])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nwith tf.Session() as sess:\n    print(sess.run(x * y))\n", "intent": "Q3. Multiply x by y element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3])\nx = tf.convert_to_tensor(_x)\nwith tf.Session() as sess:\n    print(sess.run(x * 5))\n", "intent": "Q4. Multiply x by 5 element-wise.\n"}
{"snippet": "_x = np.array([10, 20, 30], np.int32)\n_y = np.array([2, 3, 7], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout1 = tf.mod(x,y)\nprint(out1.eval())\n", "intent": "Q6. Get the remainder of x / y element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3], np.int32)\n_y = np.array([4, 5, 6], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\ntf.cross(x,y).eval()\n", "intent": "Q7. Compute the pairwise cross product of x and y.\n"}
{"snippet": "_x = np.array([1, 2, 3], np.int32)\n_y = np.array([4, 5, 6], np.int32)\n_z = np.array([7, 8, 9], np.int32)\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nz = tf.convert_to_tensor(_y)\n(x + y + z).eval()\n", "intent": "Q8. Add x, y, and z element-wise.\n"}
{"snippet": "_X = np.array([[1, -1], [3, -3]])\nX = tf.convert_to_tensor(_X)\ntf.abs(X).eval()\n", "intent": "Q9. Compute the absolute value of X element-wise.\n"}
{"snippet": "_x = np.array([1, -1])\nx = tf.convert_to_tensor(_x)\n(-x).eval()\n", "intent": "Q10. Compute numerical negative value of x, elemet-wise.\n"}
{"snippet": "for i in [X2,X4]:\n    lm1 = smf.ols(formula='y ~ i', data=BostonData).fit()\n    print(lm1.pvalues)\n", "intent": "Answer: we will most likely use model 2 or 4. If both of these models are significant, then we use model 4. \n"}
{"snippet": "_x = np.array([1, 2, 2/10])\nx = tf.convert_to_tensor(_x)\n(1/x).eval()\n", "intent": "Q12. Compute the reciprocal of x, element-wise.\n"}
{"snippet": "_x = np.array([1, 2, -1])\nx = tf.convert_to_tensor(_x)\n(x**2).eval()\n", "intent": "Q13. Compute the square of x, element-wise.\n"}
{"snippet": "_x = np.array([1, 4, 9], dtype=np.float32)\nx = tf.convert_to_tensor(_x)\ntf.floor(x**1/2).eval()\n", "intent": "Q15. Compute square root of x element-wise.\n"}
{"snippet": "_x = np.array([1., 4., 9.])\nx = tf.convert_to_tensor(_x)\ntf.reciprocal((x**1/2)).eval()\n", "intent": "Q16. Compute the reciprocal of square root of x element-wise.\n"}
{"snippet": "_x = np.array([[1, 2], [3, 4]])\n_y = np.array([[1, 2], [1, 2]])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n(x**y).eval()\n", "intent": "Q17. Compute $x^y$, element-wise.\n"}
{"snippet": "_x = np.array([1., 2., 3.], np.float32)\nx = tf.convert_to_tensor(_x)\n(np.e ** x).eval()\n", "intent": "Q17. Compute $e^x$, element-wise.\n"}
{"snippet": "_x = np.array([1, np.e, np.e**2])\nx = tf.convert_to_tensor(_x)\ntf.log(x).eval()\n", "intent": "Q18. Compute natural logarithm of x element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 2])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\ntf.maximum(x,y).eval()\n", "intent": "Q19. Compute the max of x and y element-wise.\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 2])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\ntf.minimum(x,y).eval()\n", "intent": "Q20. Compute the min of x and y element-wise.\n"}
{"snippet": "for i in [X12,X13,X14,X15]:\n    lm1 = smf.ols(formula='y ~ i', data=BostonData).fit()\n    print(lm1.pvalues)\n", "intent": "Answer: if our goal is prediction, we will most like use models 12, 13, 14 or 15. Let's test to see which one is the best!\n"}
{"snippet": "_x = np.array([2, 3, 4])\n_y = np.array([1, 5, 1])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\n((x-y)*(x-y)).eval()\n", "intent": "Q22. Compute (x - y)(x - y) element-wise.\n"}
{"snippet": "_x = np.array([1, 2, 3, 4])\nx = tf.convert_to_tensor(_x)\ntf.diag(x).eval()\n", "intent": "Q1. Create a diagonal tensor with the diagonal values of x.\n"}
{"snippet": "_X = np.array(\n[[1, 0, 0, 0],\n [0, 2, 0, 0],\n [0, 0, 3, 0],\n [0, 0, 0, 4]])\nX = tf.convert_to_tensor(_X)\ntf.diag_part(X).eval()\n", "intent": "Q2. Extract the diagonal of X.\n"}
{"snippet": "_X = np.random.rand(2,3,4)\nX = tf.convert_to_tensor(_X)\nout = tf.transpose(X, [1, 2, 0])\nprint(\"X\")\nprint(_X)\nprint(\"Transpose\")\nprint(out.eval())\n", "intent": "Q3. Permutate the dimensions of x such that the new tensor has shape (3, 4, 2).\n"}
{"snippet": "_X= np.random.rand(1, 2, 3, 4)\nX = tf.convert_to_tensor(_X)\nprint(\"X\")\nprint(_X)\nprint(\"transpose\")\nprint(tf.transpose(X,[0,1,3,2]).eval())\n", "intent": "Q6. Transpose the last two dimensions of X.\n"}
{"snippet": "_X = np.array([[1, 2, 3], [4, 5, 6]])\n_Y = np.array([[1, 1], [2, 2], [3, 3]])\nX = tf.convert_to_tensor(_X,dtype=np.int32)\nY = tf.convert_to_tensor(_Y,dtype=np.int32)\nout = tf.matmul(X, Y)\nprint(out.eval())\n_out = np.dot(_X, _Y)\nassert np.array_equal(out.eval(), _out)\n", "intent": "Q7. Multiply X by Y.\n"}
{"snippet": "_X = np.arange(1, 5, dtype=np.float32).reshape((2, 2))\nX = tf.convert_to_tensor(_X)\ntf.matrix_determinant(X).eval()\n", "intent": "Q9. Compute the determinant of X.\n"}
{"snippet": "_X = np.arange(1, 5, dtype=np.float64).reshape((2, 2))\nX = tf.convert_to_tensor(_X)\ntf.matrix_inverse(X).eval()\n", "intent": "Q10. Compute the inverse of X.\n"}
{"snippet": "_X = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]], np.float32)\nX = tf.convert_to_tensor(_X)\ntf.cholesky(X).eval()\n", "intent": "Q11. Get the lower-trianglular in the Cholesky decomposition of X.\n"}
{"snippet": "lm = LogisticRegression(solver = 'newton-cg', max_iter = 10000)\nlm.fit(X1, y)\nprint(zip(X1.columns.values, lm.coef_[0, :]))\nprint(lm.intercept_)\n", "intent": "Run your regression line on X1 and interpret your MOMMY AND DADMY coefficients. Assume variables are significant. \n"}
{"snippet": "_X = np.array(\n[[1, 0, 0, 0, 2], \n [0, 0, 3, 0, 0], \n [0, 0, 0, 0, 0], \n [0, 2, 0, 0, 0]], dtype=np.float32)\nX = tf.convert_to_tensor(_X)\ntf.svd(X)[0].eval()\n", "intent": "Q13. Compute the singular values of X.\n"}
{"snippet": "_X = np.arange(1, 7).reshape((2, 3))\n_Y = np.arange(1, 7).reshape((3, 2))\nX = tf.convert_to_tensor(_X, dtype=np.int32)\nY = tf.convert_to_tensor(_Y, dtype=np.int32)\n", "intent": "Q17. Complete the einsum function that would yield the same result as the given function.\n"}
{"snippet": "_X = np.array([[1,2,3], [4,5,6]])\nX = tf.convert_to_tensor(_X)\ntf.cumsum(X).eval()\n", "intent": "Q1. Compute the cumulative sum of X along the second axis.\n"}
{"snippet": "_X = np.array([[1,2,3], [4,5,6]])\nX = tf.convert_to_tensor(_X)\ntf.cumprod(X).eval()\n", "intent": "Q2. Compute the cumulative product of X along the second axis.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [-1,-2,-3,-4], \n     [-10,-20,-30,-40],\n     [10,20,30,40]])\nX = tf.convert_to_tensor(_X)\ntf.segment_sum(X,[0,0,1,1]).eval()\n", "intent": "Q3. Compute the sum along the first two elements and \nthe last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [1,1/2,1/3,1/4], \n     [1,2,3,4],\n     [-1,-1,-1,-1]])\nX = tf.convert_to_tensor(_X)\ntf.segment_prod(X,[0,0,1,1]).eval()\n", "intent": "Q4. Compute the product along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,4,5,7], \n     [2,3,6,8], \n     [1,2,3,4],\n     [-1,-2,-3,-4]])\nX = tf.convert_to_tensor(_X)\ntf.segment_min(X,[0,0,1,1]).eval()\n", "intent": "Q5. Compute the minimum along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,4,5,7], \n     [2,3,6,8], \n     [1,2,3,4],\n     [-1,-2,-3,-4]])\nX = tf.convert_to_tensor(_X)\ntf.segment_max(X,[0,0,1,1]).eval()\n", "intent": "Q6. Compute the maximum along the first two elements and the last two elements of X separately.\n"}
{"snippet": "_X = np.array(\n    [[1,2,3,4], \n     [5,6,7,8], \n     [-1,-2,-3,-4],\n     [-5,-6,-7,-8]])\nX = tf.convert_to_tensor(_X)\ntf.segment_mean(X,[0,0,1,1]).eval()\n", "intent": "Q7. Compute the mean along the first two elements and the last two elements of X separately.\n"}
{"snippet": "iris_pca_fit = pca.fit(iris)\nprint(iris_pca_fit.explained_variance_ratio_)\nsns.heatmap(iris_pca_fit.components_, cmap='viridis', xticklabels=iris.columns.tolist(),\n            yticklabels=['PCA1','PCA2'], linewidth=1, linecolor='k');\n", "intent": "We can also plot the mappings of each variable onto principal components.\n"}
{"snippet": "_X = np.random.permutation(10).reshape((2, 5))\nprint(\"_X =\", _X)\nX = tf.convert_to_tensor(_X)\ntf.argmax(X,axis=1).eval()\ntf.argmin(X,axis=1).eval()\n", "intent": "Q9. Get the indices of maximum and minimum values of X along the second axis.\n"}
{"snippet": "_x = np.array([0, 1, 2, 5, 0])\n_y = np.array([0, 1, 4])\nx = tf.convert_to_tensor(_x)\ny = tf.convert_to_tensor(_y)\nout = tf.setdiff1d(x, y)[0]\nout.eval()\n", "intent": "Q10. Find the unique elements of x that are not present in y.\n"}
{"snippet": "_X = np.arange(1, 10).reshape(3, 3)\nX = tf.convert_to_tensor(_X)\nout = tf.where(X < 4, X, X*10)\nout.eval()\n", "intent": "Q11. Return the elements of X, if X < 4, otherwise X*10.\n"}
{"snippet": "_x = np.array([1, 2, 6, 4, 2, 3, 2])\nx = tf.convert_to_tensor(_x)\nout, indices = tf.unique(x)\nindices.eval()\n", "intent": "Q12. Get unique elements and their indices from x.\n"}
{"snippet": "hypothesis = tf.SparseTensor(\n    [[0, 0],[0, 1],[0, 2],[0, 4]],\n    [\"a\", \"b\", \"c\", \"a\"],\n    (1, 5)) \ntruth = tf.SparseTensor(\n    [[0, 0],[0, 2],[0, 4]],\n    [\"a\", \"c\", \"b\"],\n    (1, 6))\ntf.edit_distance(hypothesis,truth, normalize=True).eval()\ntf.edit_distance(hypothesis,truth, normalize=False).eval()\n", "intent": "Q13. Compute the edit distance between hypothesis and truth.\n"}
{"snippet": "_x = np.array([[1, 2, 4, 8], [2, 4, 6, 8]], dtype=np.float32)\nx = tf.convert_to_tensor(_x)\nout = tf.nn.softmax(x)\nwith tf.Session() as sess:\n    _out = sess.run(out)\n    print(_out)    \n    assert np.allclose(np.sum(_out, axis=-1), 1)\n", "intent": "Q3. Apply `softmax` to x.\n"}
{"snippet": "_x = np.array([[1, 2, 4, 8], [2, 4, 6, 8]], dtype=np.float32)\nprint(\"_x =\\n\" , _x)\nx = tf.convert_to_tensor(_x)\nout = tf.nn.dropout(x, keep_prob=.5)\nwith tf.Session() as sess:\n    _out = sess.run(out)\n    print(\"_out =\\n\", _out) \n", "intent": "Q4. Apply `dropout` with keep_prob=.5 to x.\n"}
{"snippet": "x = tf.random_normal([8, 10])\nout = tf.contrib.layers.fully_connected(x, num_outputs=2, activation_fn=tf.nn.sigmoid)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(out))\n", "intent": "Q5. Apply a fully connected layer to x with 2 outputs and then an sigmoid function.\n"}
{"snippet": "x = tf.constant([[1, 0, 0, 0],\n                 [0, 0, 2, 0],\n                 [0, 0, 0, 0]], dtype=tf.int32)\nsp = tf.SparseTensor([[0,0],[1,2]], values= [1,2], dense_shape=[3,4])\nprint(sp.eval())\n", "intent": "Q1. Convert tensor x into a `SparseTensor`.\n"}
{"snippet": "import statsmodels.api as sm\nX = np.array(xs).transpose()\nX = sm.add_constant(X)\nmod = sm.OLS(ys, X)\nres = mod.fit()\nprint res.summary()\n", "intent": "This generated a fit of $y = 3 x + 1$. Let's see what a linear regression yields.\n"}
{"snippet": "output = tf.sparse_tensor_to_dense(sp)\nprint(output.eval())\nprint(\"Check if this is identical with x:\\n\", x.eval())\n", "intent": "Q5. Convert the SparseTensor `sp` to a Tensor using `tf.sparse_tensor_to_dense`.\n"}
{"snippet": "imputer.fit(housing_num)\n", "intent": "Now you can fit the imputer instance to the training data using the fit() method:\n"}
{"snippet": "x1 = tf.Variable(1)\nx1.graph is tf.get_default_graph()\n", "intent": "Any node you create is automatically added to the default graph:\n"}
{"snippet": "test_files=test_files[:10000]\ntest_id=test_id[:10000]\nfinal_test_tensors = paths_to_tensor(test_files).astype('float32')/255\n", "intent": "Reduce test set size\n"}
{"snippet": "model = read_model()\nmodel.load_weights('cache/weights_single_model.hdf5')\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\nprint('read and compiled model')\n", "intent": "Section that loads the model and its weights\n"}
{"snippet": "lm = LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=True)\nlm.fit(X, bos.PRICE)\n", "intent": "**Your turn:** How would you change the model to not fit an intercept term? Would you recommend not having an intercept?\n"}
{"snippet": "knn_clf = neighbors.KNeighborsClassifier(n_neighbors=3, weights='uniform')\n", "intent": "-Which model is more accurate?\n"}
{"snippet": "from keras.layers import Dense, Dropout, Activation, PReLU\nmodel = Sequential()\nmodel.add(Dense(128, input_shape=(1000,)))\nmodel.add(PReLU())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=200, verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "rcv = linear_model.RidgeCV(alphas=\n                           (.001, .001, .01, .1, .5, 1, 5, 10),\n                           store_cv_values=True,\n                          )\n", "intent": "RidgeCV implements cross validation on a ridge regression with various alphas\n"}
{"snippet": "x1 = tf.placeholder(tf.float32, [2])\nx2 = tf.placeholder(tf.float32, [2])\nz = tf.add(x1,x2)\nfeed_dict = {x1: [1,5], x2:[1,1]}\nsess = tf.Session()\nsess.run(z, feed_dict)\n", "intent": "Now we can also use vectors as input, to show that we don't need to deal only with scalars\n"}
{"snippet": "x1 = tf.constant(1)\nx2 = tf.constant(2)\nz = tf.add(x1, x2)\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\nsess.run(z)\n", "intent": "Consider the following code\n"}
{"snippet": "c = tf.constant(5)\nx = c + 1\ny = x + 1\nz = x + 2\nsess = tf.Session()\nprint('y is',sess.run(y))\nprint('z is',sess.run(z))\nsess.close()\n", "intent": "Sometime tensorflow may evaluate some nodes multiple times. Pay attention. Consider the following code\n"}
{"snippet": "c = tf.constant(5)\nx = c + 1\ny = x + 1\nz = x + 2\nsess = tf.Session()\nprint('y,z are',sess.run([y,z]))\nsess.close()\n", "intent": "here ```x``` is evaluated twice! You can do it like this\n"}
{"snippet": "sess = tf.Session()\nsess.close()\n", "intent": "You can close a session in this way\n"}
{"snippet": "with tf.Session() as sess:\n", "intent": "You can also do it in this way\n"}
{"snippet": "c = tf.constant(5)\nx = c + 1\ny = x + 1\nz = x + 2\nsess = tf.Session()\nprint('y is',sess.run(y))\nprint('z is',sess.run(z))\nsess.close()\n", "intent": "you need some code to avoid the error. Consider the following code\n"}
{"snippet": "c = tf.constant(5)\nx = c + 1\ny = x + 1\nz = x + 2\nwith tf.Session() as sess:\n    print('y is',sess.run(y))\n    print('z is',sess.run(z))\n", "intent": "this can also be written as the following\n"}
{"snippet": "sess = tf.Session()\nprint(sess.run(result, feed_dict = {x1: 3, x2: 4}))\nprint(sess.run(result, feed_dict = {x1: 5, x2: 7}))\nsess.close()\n", "intent": "To run the same graph twice in the same session you can do the following\n"}
{"snippet": "logreg = LogisticRegression(solver='liblinear')\nC_vals = [0.0001, 0.001, 0.01, 0.1, .15, .25, .275, .33, 0.5, .66, 0.75, 1.0, 2.5, 5.0, 10.0, 100.0, 1000.0]\npenalties = ['l1','l2']\ngs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals}, verbose=False, cv=15)\ngs.fit(X, y)\n", "intent": "Then will pass those to GridSearchCV\n"}
{"snippet": "x = np.arange(-5,5,0.1)\nidentity = x\nsigmoid = 1.0 / (1.0 + np.exp(-x))\narctan = np.tanh(x)\nrelu = np.maximum(x, 0)\nleakyrelu = relu - 0.05 * np.maximum(-x, 0)\n", "intent": "First let's create the data that we need to plot the different activation functions\n"}
{"snippet": "tf.reset_default_graph()\nX = tf.placeholder(tf.float32, [n_dim, None])\nY = tf.placeholder(tf.float32, [1, None])\nlearning_rate = tf.placeholder(tf.float32, shape=())\nW = tf.Variable(tf.ones([n_dim, 1]))\nb = tf.Variable(tf.zeros(1))\ninit = tf.global_variables_initializer()\n", "intent": "First we define the variables and placeholders we need to build the network above.\n"}
{"snippet": "sess, cost_history = run_logistic_model(learning_r = 0.05, \n                                training_epochs = 750, \n                                train_obs = Xtrain, \n                                train_labels = ytrain, \n                                debug = True)\n", "intent": "**CAREFUL** it will take some time to run!\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogistic = LogisticRegression()\n", "intent": "As a side note, here is the code to see how easy it is in comparison to do the same with the sklearn library... \n"}
{"snippet": "tf.reset_default_graph()\nX = tf.placeholder(tf.float32, [1, None]) \nY = tf.placeholder(tf.float32, [1, None]) \nlearning_rate = tf.placeholder(tf.float32, shape=())\nW = tf.Variable(tf.ones([1, 1])) \nb = tf.Variable(tf.zeros(1)) \ninit = tf.global_variables_initializer()\ny_ = tf.matmul(tf.transpose(W),X)+b \nmse = tf.reduce_mean(tf.square(y_ - Y)) \n", "intent": "Let's build our neural network. That would be one single neuron, with an identity activation function\n"}
{"snippet": "sess, ch = run_linear_model(1e-3, 1000, x, y, True)\n", "intent": "Interesting... Let's try a smaller learning rate\n"}
{"snippet": "sess3, ch3 = run_linear_model(1e-2, 5000, x, y, True)\n", "intent": "Note that the learning rate is small, and so the convergence is slow... Let's try something faster...\n"}
{"snippet": "sess5, ch5 = run_linear_model(0.03, 5000, x, y, True)\n", "intent": "For the sake of trying to find the best parameters\n"}
{"snippet": "xt = x.reshape(7,-1)\nyt = y.reshape(7,-1)\nreg = LinearRegression().fit(xt,yt)\nreg.score(xt,yt)\n", "intent": "Now let's compare the results with a classical linear regression as we can do with ```sklearn```\n"}
{"snippet": "model = LogisticRegression(penalty = 'l1', C = 10.0) \nmodel.fit(X, y)\nexamine_coefficients(model, X)\n", "intent": "- Change the `C` parameter\n    - how do the coefficients change? (use `examine_coeffcients`)\n    - how does the model perfomance change (using AUC)\n"}
{"snippet": "def get_random_element_with_label (data, lbls, lbl):\n    tmp = lbls == lbl\n    subset = data[:,tmp.flatten()]\n    return subset[:,randint(1,subset.shape[1])]\n", "intent": "THe following function will return one numpy array (one column) with an example of a choosen label.\n"}
{"snippet": "sess1, cost_history1 = model(minibatch_size = 1, \n                              training_epochs = 50, \n                              features = train_red, \n                              classes = labels_red, \n                              logging_step = 10,\n                              learning_r = 1e-3)\n", "intent": "The dataset is pretty balanced so we are good to go.\n"}
{"snippet": "sessb, cost_history500_2 = model(minibatch_size = 60000, \n                              training_epochs = 1000, \n                              features = train, \n                              classes = labels_, \n                              logging_step = 50,\n                              learning_r = 0.01)\n", "intent": "Let's try now batch-gradient descent\n"}
{"snippet": "sessb, cost_history500 = model(minibatch_size = 50, \n                              training_epochs = 1000, \n                              features = train, \n                              classes = labels_, \n                              logging_step = 100,\n                              learning_r = 0.001)\n", "intent": "The `corr_pred` array contains `True` when the prediction is right and `False` when it is not.\n"}
{"snippet": "sessad, cost_ad, Woutput, boutput = run_linear_model(400, 1, X_, Y_, \n                                                   debug = True, learning_r = 0.05)\nprint(Woutput[-1])\nprint(boutput[-1])\n", "intent": "**Check the speed (in seconds) difference between mini_batchsize = 1 and 30.**\n"}
{"snippet": "def create_layer (X, n, activation):\n    ndim = int(X.shape[0])\n    stddev = 2.0 / np.sqrt(ndim)\n    initialization = tf.truncated_normal((n, ndim), stddev = stddev)\n    W = tf.Variable(initialization)\n    b = tf.Variable(tf.zeros([n,1]))\n    Z = tf.matmul(W,X)+b\n    return activation(Z), W, b\n", "intent": "Now let's look at what happens when we try to do linear regression with a network with 4 layers and each 20 neurons.\n"}
{"snippet": "sess, cost_history = run_logistic_model(learning_r = 5e-4, \n                                training_epochs = 600, \n                                train_obs = X_train[1], \n                                train_labels = y_train[1], \n                                debug = True)\n", "intent": "Now let's train our network on Fold with index ```1```\n"}
{"snippet": "lr = LinearRegression()\nlr.fit(X_train, y_train)\n", "intent": "We can know run linear LS on the training set to find the parameter values, as above. \n"}
{"snippet": "lr = LinearRegression()\n", "intent": "In the context of scikit-learn, we have encountered and used examples of *classes*. Recall the notation, e.g. for the **LinearRegression()** class:\n"}
{"snippet": "model = LogisticRegression(penalty = 'l2', C = .01) \nmodel.fit(X, y)\nexamine_coefficients(model, X) \n", "intent": "- Change the `C` parameter - how do the coefficients change? (use `examine_coeffcients`)\n"}
{"snippet": "model.fit(x_train, y_train, epochs=2, batch_size=100, verbose = 2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr = LinearRegression().fit(X, y)\nplot(lr, \"OLS\", \"green\")\n", "intent": "We can see there's some random perturbations in the begenning of the scatter plot. We'll see how OLS fits the data. \n"}
{"snippet": "ada_model = AdaBoostClassifier(\n                    DecisionTreeClassifier(random_state = 17),\n                    random_state = 17, learning_rate = 0.1\n                )\nstart = time.time()\nada_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(ada_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"ada_model\"></a>\n"}
{"snippet": "dtc_model = DecisionTreeClassifier(random_state = 17)\nstart = time.time()\ndtc_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(dtc_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"dtc_model\"></a>\n"}
{"snippet": "etc_model = ExtraTreesClassifier(random_state = 17)\nstart = time.time()\netc_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(etc_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"etc_model\"></a>\n"}
{"snippet": "gnb_model = GaussianNB()\nstart = time.time()\ngnb_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(gnb_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"gnb_model\"></a>\n"}
{"snippet": "gbc_model = GradientBoostingClassifier(random_state=17)\nstart = time.time()\ngbc_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(gbc_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"gbc_model\"></a>\n"}
{"snippet": "knn_model = KNeighborsClassifier()\nstart = time.time()\nknn_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(knn_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"knn_model\"></a>\n"}
{"snippet": "lin_model = LinearDiscriminantAnalysis()\nstart = time.time()\nlin_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(lin_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"lin_model\"></a>\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nrf = RandomForestClassifier()\nall_models['rf'] = {'model': rf,\n                    'score': evaluate_model(rf)}\net = ExtraTreesClassifier()\nall_models['et'] = {'model': et,\n                    'score': evaluate_model(et)}\n", "intent": "Let's see if Random Forest and Extra Trees perform better\n1. Initialize RF and ET and test on Train/Test set\n- Find optimal params with Grid Search\n"}
{"snippet": "mlp_model = MLPClassifier(random_state = 17)\nstart = time.time()\nmlp_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nmlp_model(lin_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"mlp_model\"></a>\n"}
{"snippet": "rfc_model = RandomForestClassifier(random_state = 17)\nstart = time.time()\nrfc_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(rfc_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"rfc_model\"></a>\n"}
{"snippet": "svc_model = SVC()\nstart = time.time()\nsvc_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(svc_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"svc_model\"></a>\n"}
{"snippet": "rf_model = RandomForestRegressor()\nstart = time.time()\nrf_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\n", "intent": "WARNING: Takes a lot of time\n"}
{"snippet": "xgb_model = XGBRegressor()\nstart = time.time()\nxgb_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\n", "intent": "(Training takes 544 seconds)\n(Scoring takes 453 seconds)\n"}
{"snippet": "xgb_model = XGBRegressor()\nstart = time.time()\nxgb_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\n", "intent": "(Training takes 544, 533 seconds)\n(Scoring takes 453, 487 seconds)\n"}
{"snippet": "rf_model = RandomForestRegressor()\nstart = time.time()\nrf_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"random_forest\"></a>\nWARNING: Takes a lot of time\n"}
{"snippet": "xgb_model = XGBRegressor()\nstart = time.time()\nxgb_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"xgb\"></a>\n(Training takes 544, 533 seconds)\n(Scoring takes 453, 487 seconds)\n"}
{"snippet": "num_classes = 5\nresnet_weights_path = '../data/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\nmy_new_model = Sequential()\nmy_new_model.add(ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path))\nmy_new_model.add(Dense(num_classes, activation='softmax'))\nmy_new_model.layers[0].trainable = False\nmy_new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "- Using adam instead of sgd\n- model is trained from resnet 50\n- data is augmented\n"}
{"snippet": "Cat_Naive_Bayes = naive_bayes.MultinomialNB();\nCat_Naive_Bayes.fit(feature_train, target_train)\n", "intent": "Instantiate the Naive Bayes predictor from scikit-learn with the training data. \n"}
{"snippet": "lgb_model = LGBMClassifier()\nstart = time.time()\nlgb_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(lgb_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"lgb_model\"></a>\n"}
{"snippet": "xgb_model = XGBClassifier()\nstart = time.time()\nxgb_model.fit(train_x, train_y)\nprint(\"Training took {} seconds\".format(round(time.time() - start)))\nstart = time.time()\nscore_model(xgb_model)\nprint(\"\\nScoring took {} seconds\".format(round(time.time() - start)))\n", "intent": "<a id=\"xgb_model\"></a>\n"}
{"snippet": "vgg.fit(batches, val_batches, nb_epoch=1)\nvgg.save()\n", "intent": "This is where we fit the dogs and cats to the existing model.  This takes a while even on a GPU.\n"}
{"snippet": "def myCrossEntropyLoss(outputs, labels):\n    batch_size = outputs.size()[0]\n    tmp_outputs = F.softmax(outputs, dim=1)\n    print(tmp_outputs)\n    outputs = F.log_softmax(outputs, dim=1)\n    print(outputs)\n    outputs = outputs[range(batch_size), labels] \n    return -torch.sum(outputs)/len(labels)\n", "intent": "\"def softmax(x):\\n\",\n    \"    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)\\n\"\n"}
{"snippet": "activation( torch.mm( activation( torch.mm( features, W1) + B1 ), W2 ) + B2 )\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "regressor = Sequential()\n", "intent": "We call our recurrent neural network a regressor since we are predicting using regression, not classification.\n"}
{"snippet": "regressor.add(Dense(units = 1))\n", "intent": "Our output layer will return the predicted stock price for the next day.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\n", "intent": "Select and train a model\n"}
{"snippet": "model.fit(x_train, y_train)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "import sklearn.linear_model as linear_model\nlogistic_class = linear_model.LogisticRegression()\nlogit = logistic_class.fit(feature_train, target_train)\n", "intent": "Define a logistic regression and train it with the feature and target set\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nparameters = {\n    'vec__max_df': [0.8, 1.0],\n    'vec__ngram_range': [(1, 1), (1, 2)],\n    'clf__alpha': np.logspace(-5, 0, 6)\n}\ngs = GridSearchCV(pipeline, parameters, verbose=2, refit=False, n_jobs=3)\n_ = gs.fit(twenty_train_small.data, twenty_train_small.target)\n", "intent": "For the grid search, the parameters names are prefixed with the name of the pipeline step using \"__\" as a separator:\n"}
{"snippet": "X = iris_train.values\ny = class_train.values\nclf = tree.DecisionTreeClassifier(max_depth=3)\nclf.fit(X, y)\n", "intent": "* `max-depth` is the max depth of dicision tree plus the root node\n* Default `tree.DecisionTreeClassifier` use criterion of gini instead of entropy\n"}
{"snippet": "regr = LogisticRegression()\nregr.fit(df2_reduced, target_variable)\nneigh = KNeighborsClassifier(n_neighbors=5)\nneigh.fit(df2_reduced, target_variable)\ntree = DecisionTreeClassifier()\ntree.fit(df2_reduced, target_variable)\n", "intent": "<i>Educate ML algorithms for classification:\n- without explicitly stating a \"random_state\"\n- on the whole train data</i>\n"}
{"snippet": "k_values = range(1,50,1)\nparameters = {'n_neighbors':k_values}\nknn = sklearn.neighbors.KNeighborsClassifier()\nsearch = sklearn.grid_search.GridSearchCV(knn, param_grid=parameters, cv=5)\nsearch.fit(x_2d, Y_train)\nsearch.best_estimator_  \nsearch.best_params_  \n", "intent": "Verify that the grid search has indeed chosen the right parameter value for $k$.\n"}
{"snippet": "k_values = range(1,50,1)\nparameters = {'n_neighbors':k_values}\nknn = sklearn.neighbors.KNeighborsClassifier()\nF10 = sklearn.grid_search.GridSearchCV(knn, param_grid=parameters, cv=10)\nF10.fit(X_train, Y_train)\nF10.best_params_\n", "intent": "Verify that the grid search has indeed chosen the right parameter value for $k$.\n"}
{"snippet": "class_less = sklearn.svm.SVC(C=1, gamma=1, class_weight=None)\nclass_less.fit(key_features, target)\nplot_decision_surface(class_less, key_features, target)\n", "intent": "**(d)** The <a href='http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"}
{"snippet": "model = Sequential()\n", "intent": "Model achitecture: ConvDNA (which is Conv1D) --> GlobalMaxPooling1D  -->  Dense --> Sigmoid\n"}
{"snippet": "model = Sequential()\nmodel.add(cl.ConvDNA(filters=1,\n                    kernel_size=15,\n                    activation='relu',\n                    input_shape=(500, 4), \n                    name='convDNA'))\nmodel.add(kl.GlobalMaxPooling1D())\nmodel.add(kl.Dense(1, name='dense'))\nmodel.add(kl.Activation('sigmoid'))\n", "intent": "Model achitecture: ConvDNA (which is Conv1D) --> GlobalMaxPooling1D  -->  Dense --> Sigmoid\n"}
{"snippet": "distortions = []\nfor i in range(1,11):\n    km = KMeans(n_clusters=i)\n    km.fit(df.drop('Private',axis=1))\n    distortions.append(km.inertia_)\n", "intent": "**using the elbow method to find the optimal number of clusters **\n"}
{"snippet": "logreg = LogisticRegression(solver='liblinear')\nC_vals = np.logspace(-5,1,50)\npenalties = ['l1','l2']\ngs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals}, cv=5)\ngs.fit(X3, y)\n", "intent": "Now that I had a model I wanted to see if I could improve it at all. My first attempt at doing this was with a GridSearchCV\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nclf_knn = KNeighborsClassifier()\nk_range = list(range(1, 21))\nprint(k_range)\n", "intent": "<a href=\"https://www.ritchieng.com/machine-learning-efficiently-search-tuning-param/\">knn tuning</a>\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nclf_knn = KNeighborsClassifier()\nk_range = list(range(1, 15))\nprint(k_range)\n", "intent": "<a href=\"https://www.ritchieng.com/machine-learning-efficiently-search-tuning-param/\">knn tuning</a>\n"}
{"snippet": "with tf.Session() as session:\n    result = session.run(c)\n    print (\"c =: {}\".format(result))\n", "intent": "<div align=\"right\">\n<a href=\"\n</div>\n<div id=\"operations\" class=\"collapse\">\n```\nc=tf.sin(a)\n```\n</div>\n"}
{"snippet": "a = tf.Variable(1.0)\nb = tf.Variable(0.2)\ny = a * x_data + b\n", "intent": "First, we initialize the variables __a__ and __b__, with any random guess, and then we define the linear function:\n"}
{"snippet": "init = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n", "intent": "Don't forget to initialize the variables before executing a graph:\n"}
{"snippet": "act = tf.sigmoid(tf.matmul(i, w) + b)\nact.eval(session=sess)\n", "intent": "3D sigmoid plot. The x-axis is the weight, the y-axis is the bias.\n"}
{"snippet": "act = tf.tanh(tf.matmul(i, w) + b)\nact.eval(session=sess)\n", "intent": "3D tanh plot. The x-axis is the weight, the y-axis is the bias.\n"}
{"snippet": "act = tf.nn.relu(tf.matmul(i, w) + b)\nact.eval(session=sess)\n", "intent": "3D relu plot. The x-axis is the weight, the y-axis is the bias.\nTensorFlow has ReLU and some other variants of this function. Take a look:\n"}
{"snippet": "weights = {\n    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n}\nbiases = {\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n", "intent": "Lets create the weight and biases for the read out layer\n"}
{"snippet": "chosen_k = 3\nkmean = KMeans(n_clusters=chosen_k, random_state=43)\nclusters = kmean.fit(PCAdf)\nlabel = pd.Series(kmean.labels_, name = 'label')\nkmean_df = pd.concat([PCAdf,label], axis = 1)\nkmean_df.head()\n", "intent": "K Means also seems to prefer 2 clusters because of stronger inertia, but I think having 3 clusters would be more useful so I'm going with those.\n"}
{"snippet": "outputs, states = tf.nn.dynamic_rnn(lstm_cell, inputs=x, dtype=tf.float32)\n", "intent": "__dynamic_rnn__ creates a recurrent neural network specified from __lstm_cell__:\n"}
{"snippet": "lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_size, forget_bias=0.0)\nstacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell] * num_layers)\n", "intent": "In this step, we create the stacked LSTM, which is a 2 layer LSTM network:\n"}
{"snippet": "embedding = tf.get_variable(\"embedding\", [vocab_size, hidden_size])  \n", "intent": "We create the embeddings for our input data. embedding is dictionary of [10000x200] for all 10000 unique words.\n"}
{"snippet": "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n", "intent": "__BasicRNNCell__ is the most basic RNN cell.\n"}
{"snippet": "session = tf.Session()\nfeed_dict={input_data:x, targets:y}\nsession.run(input_data, feed_dict)\n", "intent": "Lets check the value of the input_data again:\n"}
{"snippet": "with tf.variable_scope(\"rnn\"):\n    model = LSTMModel()\n", "intent": "Now we create a LSTM model:\n"}
{"snippet": "with  tf.Session() as sess:\n    a= tf.constant([0.7, 0.1, 0.8, 0.2])\n    print sess.run(a)\n    b=sess.run(tf.random_uniform(tf.shape(a)))\n    print b\n    print sess.run(a-b)\n    print sess.run(tf.sign( a - b))\n    print sess.run(tf.nn.relu(tf.sign( a - b)))\n", "intent": "Before we go further, let's look at an example of sampling:\n"}
{"snippet": "cur_w = np.zeros([784, 500], np.float32)\ncur_vb = np.zeros([784], np.float32)\ncur_hb = np.zeros([500], np.float32)\nprv_w = np.zeros([784, 500], np.float32)\nprv_vb = np.zeros([784], np.float32)\nprv_hb = np.zeros([500], np.float32)\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n", "intent": "Let's start a session and initialize the variables:\n"}
{"snippet": "hh0 = tf.nn.sigmoid(tf.matmul(X, W) + hb)\nvv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\nfeed = sess.run(hh0, feed_dict={ X: sample_case, W: prv_w, hb: prv_hb})\nrec = sess.run(vv1, feed_dict={ hh0: feed, W: prv_w, vb: prv_vb})\n", "intent": "Now let's pass this image through the net:\n"}
{"snippet": "logreg = LogisticRegression(solver='liblinear')\nC_vals = np.logspace(-5,1,50)\npenalties = ['l1','l2']\ngs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals}, cv=5)\ngs.fit(X, y)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "def decoder(x):\n    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n                                   biases['decoder_b1']))\n    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n                                   biases['decoder_b2']))\n    return layer_2\n", "intent": "And the decoder:\nYou can see that the layer_1 in the encoder is the layer_2 in the decoder and vice-versa.\n"}
{"snippet": "epochs = 5\nbatch_size = 50\nmodel.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def discriminator(images, reuse=False, alpha=0.1):\n    with tf.variable_scope(discriminator_name, reuse=reuse):\n        x = discriminator_conv2d(images, 128, 5, alpha=alpha)\n        x = discriminator_conv2d(x, 256, 5, alpha=alpha)       \n        x = discriminator_conv2d(x, 512, 5, alpha=alpha)\n        flat = tf.reshape(x, (-1, 4*4*512))\n        logits = tf.layers.dense(flat, 1)\n        out = tf.sigmoid(logits)\n    return out, logits        \ntests.test_discriminator(discriminator, tf)\n", "intent": "I doubled the depth at each convolution layer compared to previous MNIST of SVHN examples in hope of better capturing the information in faces.\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid'))\nmodel.add(Activation('relu'))\nmodel.add(Flatten(input_shape=(32, 32, 3)))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(256, input_dim = 1000, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss='categorical_crossentropy',\n                            optimizer='adadelta',\n                            metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train,\n                        batch_size=50,\n                        epochs=10,\n                        verbose=0,\n                        validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "mod_lin = LinearRegression()\nmod_dec = DecisionTreeRegressor()\nmod_ada = AdaBoostRegressor()\nmod_rf = RandomForestRegressor()\n", "intent": "> **Step 3:** Now that you have imported the 4 models that can be used for regression problems, instantate each below.\n"}
{"snippet": "mod_lin.fit(X_train, y_train)\nmod_dec.fit(X_train, y_train)\nmod_ada.fit(X_train, y_train)\nmod_rf.fit(X_train, y_train)\n", "intent": "> **Step 4:** Fit each of your instantiated models on the training data.\n"}
{"snippet": "ag = AgglomerativeClustering(n_clusters=3)\nag.fit(X)\npredicted_labels = ag.labels_\n", "intent": "Next we'll ask `AgglomerativeClustering` to return back two clusters for Iris:\n"}
{"snippet": "GK_cv = KNeighborsClassifier(n_neighbors=8, weights='distance', n_jobs=5,)\ncv_model = GK_cv.fit(X_train, y_train)\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "clf = DecisionTreeClassifier(max_depth=4)\nclf.fit(data_missing_drop_train.iloc[:,:-1], data_missing_drop_train.iloc[:,-1])\nprint('Optimal Decision Tree Depth: 4')\nprint ('Train score:', clf.score(X_drop_train, y_drop_train))\nprint ('Test score:', clf.score(X_drop_test, y_drop_test))\n", "intent": "**From CV, it looks like best choice for Decision Tree depth is 4 **\n"}
{"snippet": "clf = RandomForestClassifier(max_depth=4)\nclf.fit(X_drop_train, y_drop_train)\nprint('Optimal Random Forest Tree Depth: 4')\nprint ('Train score:', clf.score(X_drop_train, y_drop_train))\nprint ('Test score:', clf.score(X_drop_test, y_drop_test))\n", "intent": "**From CV, it looks like best choice for Random Forest Tree depth is 4 **\n"}
{"snippet": "clf = DecisionTreeClassifier(max_depth=4)\nclf.fit(X_mean_train, y_mean_train)\nprint('Optimal Tree Depth: 4')\nprint ('Train score:', clf.score(X_mean_train,y_mean_train))\nprint ('Test score:', clf.score(X_mean_test, y_mean_test))\n", "intent": "**From CV on Mean Imputation it seems like the best choice for Decision Tree depth is 4**\n"}
{"snippet": "clf = RandomForestClassifier(max_depth=7)\nclf.fit(X_mean_train, y_mean_train)\nprint('Optimal Random Forest Tree Depth: 7')\nprint ('Train score:', clf.score(X_mean_train, y_mean_train))\nprint ('Test score:', clf.score(X_mean_test, y_mean_test))\n", "intent": "**From CV it looks like the optimal max_tree_depth for Random Forest in mean Imputation is 7**\n"}
{"snippet": "clf_cv = LogisticRegressionCV(cv = 5)\nclf_cv.fit(x_train_pca, y_train)\n", "intent": "testing accuracy lower than before. \n"}
{"snippet": "dt_single = DecisionTreeClassifier(max_depth=md_best)\ndt_single.fit(X_train, y_train).score(X_test, y_test)\n", "intent": "Pavlos' Note: One can use md = 4 even though md_best = 5 since md = 4 is consistent with of best md but a simpler tree. \n"}
{"snippet": "clf_cv = LogisticRegressionCV()\nclf_cv.fit(x_train_pca, y_train)\n", "intent": "testing accuracy lower than before. \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "kmc = KMeans(n_clusters = num_clus, init= 'random')\nkmc.fit(response_array)\n", "intent": "**Your turn (extra credit):** Play with the following: \n* Different initializations for `KMeans`\n* Other clustering algorithms in scikit-learn\n"}
{"snippet": "chosen_k = 3\nkmean = KMeans(n_clusters=chosen_k, random_state=43)\nclusters = kmean.fit(PCAdf3)\nlabel = pd.Series(kmean.labels_, name = 'label')\ncentroids = kmean.cluster_centers_\nkmean_df = pd.concat([PCAdf3,label], axis = 1)\nkmean_df.head()\n", "intent": "well that looks pretty sweet!\n"}
{"snippet": "predictors = [x for x in train.columns if x not in [target, IDcol]]\ngbm_tuned_1 = GradientBoostingClassifier(learning_rate=0.05, n_estimators=120,max_depth=9, min_samples_split=1200, \n                                         min_samples_leaf=60, subsample=0.85, random_state=10, max_features=7)\nmodelfit(gbm_tuned_1, train, test, predictors)\n", "intent": "With all tuned lets try reducing the learning rate and proportionally increasing the number of estimators to get more robust results:\n"}
{"snippet": "predictors = [x for x in train.columns if x not in [target, IDcol]]\ngbm_tuned_2 = GradientBoostingClassifier(learning_rate=0.01, n_estimators=600,max_depth=9, min_samples_split=1200, \n                                         min_samples_leaf=60, subsample=0.85, random_state=10, max_features=7)\nmodelfit(gbm_tuned_2, train, test, predictors)\n", "intent": "1/10th learning rate\n"}
{"snippet": "predictors = [x for x in train.columns if x not in [target, IDcol]]\ngbm_tuned_3 = GradientBoostingClassifier(learning_rate=0.005, n_estimators=1200,max_depth=9, min_samples_split=1200, \n                                         min_samples_leaf=60, subsample=0.85, random_state=10, max_features=7,\n                                         warm_start=True)\nmodelfit(gbm_tuned_3, train, test, predictors, performCV=False)\n", "intent": "1/50th learning rate\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\nprint(net)\nnet = Sequential()\nnet.add(Linear(2, 4))\nnet.add(ReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "net = Sequential()\nnet.add(Linear(X_train.shape[1], X_train.shape[1] / 4))\nnet.add(LeakyReLU())\nnet.add(Linear(X_train.shape[1] / 4, X_train.shape[1] / 8))\nnet.add(LeakyReLU())\nnet.add(Linear(X_train.shape[1] / 8, 10))\nnet.add(SoftMax())\n", "intent": "Print here your accuracy. It should be around 90%.\n"}
{"snippet": "with graph.as_default():\n    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n", "intent": "with graph.as_default():\n    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs_, initial_state=initial_state)\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1.0,1.0))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf.fit(digits.data[:-1], digits.target[:-1])\n", "intent": "Next, we can train the SVM using all but the last sample.\n"}
{"snippet": "ptrat_mod = ols('PRICE ~ PTRATIO', data=bos).fit()\nprint(ptrat_mod.summary())\n", "intent": "**Try fitting a linear regression model using only the 'PTRATIO' (pupil-teacher ratio by town) and interpret the intercept and the coefficients.**\n"}
{"snippet": "chosen_k = 4\nkmean = KMeans(n_clusters=chosen_k, random_state=43)\nkmean.fit(PCAdf3)\nlabel = pd.Series(kmean.labels_, name = 'label')\nkmean_df = pd.concat([PCAdf3,label], axis = 1)\nkmean_df.head()\n", "intent": "even better! let's see 4 and be done for the day'\n"}
{"snippet": "my_tree = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200)\nmy_tree = my_tree.fit(X_train,y_train)\n", "intent": "Train a decision tree, setting min samples per leaf to a sensible value\n"}
{"snippet": "my_model = ensemble.RandomForestClassifier(n_estimators=300, \\\n                                           max_features = 3,\\\n                                           min_samples_split=200)\nmy_model.fit(X_train,y_train)\n", "intent": "Train and evaluate a simple model \n"}
{"snippet": "param_grid = [\n {'n_estimators': list(range(100, 501, 50)), 'max_features': list(range(2, 10, 2)), 'min_samples_split': [200] }\n]\nmy_tuned_model = GridSearchCV(ensemble.RandomForestClassifier(), param_grid, cv=cv_folds, verbose = 2)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nmodel_tuned_params_list[\"Tuned Random Forest\"] = my_tuned_model.best_params_\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "my_model = ensemble.BaggingClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 50), \\\n                                      n_estimators=10)\nmy_model.fit(X_train,y_train)\n", "intent": "Train and evaluate a simple model \n"}
{"snippet": "param_grid = [\n {'n_estimators': list(range(50, 501, 50)),\n  'base_estimator': [tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth = 6, min_samples_leaf = 200)]}\n]\nmy_tuned_model = GridSearchCV(ensemble.BaggingClassifier(), param_grid, cv=cv_folds, verbose = 2)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nmodel_tuned_params_list[\"Tuned Bagging\"] = my_tuned_model.best_params_\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "my_model = ensemble.AdaBoostClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 200), \\\n                                       n_estimators=10)\nmy_model.fit(X_train,y_train)\n", "intent": "Train and evaluate a simple model \n"}
{"snippet": "param_grid = [\n {'n_estimators': list(range(50, 501, 50)),\n 'base_estimator': [tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth = 6, min_samples_leaf = 200)]}\n]\nmy_tuned_model = GridSearchCV(ensemble.AdaBoostClassifier(), param_grid, cv=cv_folds, verbose = 2)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nmodel_tuned_params_list[\"Tuned AdaBoost\"] = my_tuned_model.best_params_\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "my_model = linear_model.LogisticRegression()\nmy_model.fit(X_train,y_train)\n", "intent": "Train and evaluate a simple model \n"}
{"snippet": "my_model = neighbors.KNeighborsClassifier()\nmy_model = my_model.fit(X_train,y_train)\n", "intent": "Train and evaluate a simple model \n"}
{"snippet": "chosen_k = 3\nkmean = KMeans(n_clusters=chosen_k, random_state=43)\nclusters = kmean.fit(PCAdf3)\nlabel = pd.Series(kmean.labels_, name = 'label')\ncentroids = kmean.cluster_centers_\nkmean_df = pd.concat([PCAdf3,label], axis = 1)\nkmean_df.head()\n", "intent": "Let's go with 3--that fourth one is bleh (technically speaking)\nDid you catch how I used different variable names for the 3rd cluster? Foreshadowing!\n"}
{"snippet": "my_model = neural_network.MLPClassifier(hidden_layer_sizes=(300, 100))\nmy_model = my_model.fit(X_train,y_train)\n", "intent": "Train and evaluate a simple model \n"}
{"snippet": "param_grid = [\n               {'hidden_layer_sizes': [(400), (400, 200), (400, 200, 100)], \n               'alpha': list(10.0 ** -np.arange(1, 7))}\n]\nmy_tuned_model = GridSearchCV(neural_network.MLPClassifier(), param_grid, cv=cv_folds, verbose = 2)\nmy_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\nprint(\"Best parameters set found on development set:\")\nprint(my_tuned_model.best_params_)\nmodel_tuned_params_list[\"Tuned MLP\"] = my_tuned_model.best_params_\nprint(my_tuned_model.best_score_)\n", "intent": "Choose parameters using a grid search\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_shape=(784,)))\nmodel.add(Activation(\"sigmoid\"))\nmodel.add(Dense(units=207))\nmodel.add(Activation(\"sigmoid\"))\nmodel.add(Dense(units=102))\nmodel.add(Activation(\"sigmoid\"))\nmodel.add(Dense(units=10))\nmodel.add(Activation(\"softmax\"))\n", "intent": "Specfiy the structure of the neural network model\n"}
{"snippet": "from sklearn.cluster import KMeans\nmodel = KMeans(4)  \nmodel.fit(X)\nclustering = model.labels_\n", "intent": "We will apply the k-means algorithm to automatically split the data into *k=4* clusters.\n"}
{"snippet": "ndatapoints = 20\nxis_true = np.random.uniform(0, 1, ndatapoints)\nx_grid = np.linspace(0, 1, 100)\ndef model_linear(xs, slope, intercept): return xs * slope + intercept\nyis_true = model_linear(xis_true, slope_true, intercept_true)\n", "intent": "Let's generate some data drawn from that model:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)      \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    with tf.name_scope(\"embeddings\"):\n        embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), name=\"embeddigs\")\n        emb = tf.nn.embedding_lookup(embeddings, input_data)\n    tf.summary.histogram(\"embeddings\", embeddings)\n    return emb\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    with tf.name_scope(\"embeddings\"):\n        embeddings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), name=\"embeddigs\")\n        emb = tf.nn.embedding_lookup(embeddings, input_data)\n    return emb\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn import linear_model\nlr = linear_model.LinearRegression()\nlr.fit(X_train, Y_train)\n", "intent": "You can import a Linear Regression Classifier by using the following codes:\n"}
{"snippet": "chosen_k = 3\nkmean = KMeans(n_clusters=chosen_k, random_state=43)\nclusters = kmean.fit(PCAdf3)\nlabel = pd.Series(kmean.labels_, name = 'label')\ncentroids = kmean.cluster_centers_\nkmean_df = pd.concat([PCAdf3,label], axis = 1)\nkmean_df.head()\n", "intent": "Let's go with 3--that fourth one is bleh (technically speaking)\n"}
{"snippet": "from sklearn import linear_model\nlr = linear_model.LogisticRegression()\nlr.fit(X_train, y_train)\n", "intent": "You can import a Logistic Regression Classifier by using the following codes:\n"}
{"snippet": "from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nclf_grad = GradientBoostingClassifier(random_state=0)\nfitted = clf_grad.fit(X_train, y_train)\nclf_tree = DecisionTreeClassifier(random_state=0)\nfitted_tree = clf_tree.fit(X_train, y_train)\nclf_random = RandomForestClassifier(random_state=0)\nfitted_random = clf_random.fit(X_train, y_train)\n", "intent": "You can import a Decision Tree Classifier by using the following codes:\n"}
{"snippet": "ft_map = base_model.get_layer(index=-2).output\nx = Conv2D(128, (3,3), padding='same')(ft_map)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Conv2D(1, (3,3), activation='sigmoid', padding='same')(x)\nx = GlobalAveragePooling2D()(x)\nmodel = Model(base_model.input, x)\n", "intent": "Add new classification head. Can use max or average pooling.\n"}
{"snippet": "import tensorflow as tf\ntf.reset_default_graph()\nX_train, y_train           = current_data_dict['X_train'], current_data_dict['y_train']\nX_validation, y_validation = current_data_dict['X_valid'], current_data_dict['y_valid']\nX_test, y_test             = current_data_dict['X_test'], current_data_dict['y_test']\nX_my_test, y_my_test       = current_data_dict['X_my_test'], current_data_dict['y_my_test']\nassert(len(X_train) == len(y_train))\nassert(len(X_validation) == len(y_validation))\nassert(len(X_test) == len(y_test))\nassert(len(X_my_test) == len(y_my_test))\n", "intent": "Load the MNIST data, which comes pre-loaded with TensorFlow.\nYou do not need to modify this section.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform(shape = [vocab_size, embed_dim], minval = -1, maxval = 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\ny = spam.is_spam.values\nX = spam.iloc[:, 1:100]\nknn = KNeighborsClassifier()\n", "intent": "---\nIt's up to you what predictors you want to use and how you want to parameterize the model.\n"}
{"snippet": "mean_knn_n2 = KNeighborsClassifier(n_neighbors=1,\n                              weights='uniform')\naccs, mean_acc = accuracy_crossvalidator(X, y, mean_knn_n2)\naccs, mean_acc\n", "intent": "---\nAs you can see the mean cross-validated accuracy is very high with 5 neighbors. \nLet's see what it's like when we use only 1 neighbor:\n"}
{"snippet": "gbt = GradientBoostingClassifier()\ngbt_params = {'max_depth': [None, 4, 8, 10],\n              'min_samples_split': [2,6,10],\n             'max_leaf_nodes': [None, 4, 8, 12],\n              \"learning_rate\" : [0.01, 0.1]}\n", "intent": "The print out shows that the parameters in the top row lead to the hightest F1 score for Random Forest\n"}
{"snippet": "from sklearn.cluster import KMeans\ncluster_model = KMeans(n_clusters = 2)\ncluster_model.fit(X)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "import tensorflow as tf\nx = tf.Variable(3, name='x')\ny = tf.Variable(4, name='y')\nf = x*x*y + y + 2\n", "intent": "The following code creates the graph represented above. \n"}
{"snippet": "def sigmoid(z):\n    s = 1 / (1 + np.exp(-z))\n    return s\n", "intent": "**Sigmoid**: $sigmoid( w^T x + b) = \\cfrac{1}{1 + e^{-(w^T x + b)}}$\n"}
{"snippet": "lm.fit(X, bos.PRICE )\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "from  sklearn.naive_bayes  import MultinomialNB\nclf = MultinomialNB()\n", "intent": "> Using `scikit-learn`'s `MultinomialNB()` classifier with default parameters.\n"}
{"snippet": "model.fit(X_train,y_train)\n", "intent": "6.train the model on the training set and check its accuracy on training and test set,\nhow's your model doing? Is the loss growing smaller?\n"}
{"snippet": "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\nc = tf.matmul(a, b)\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nprint(sess.run(c))\n", "intent": "Check TensorFlow uses GPU\n"}
{"snippet": "sparse_matrix = tf.SparseTensor()\nsparse_matrix.values\n", "intent": "** tf.SparseTensor ** - A sparse representation Tensor\nFor sparse Tensors, a more efficient representation is `tf.SparseTensor`.\n"}
{"snippet": "sparse_matrix = tf.sparse.SparseTensor(dense_shape=[3,3], indices=[[0,0],[1,1],[2,2]], values=[1,1,1]) \nsparse_matrix.values\n", "intent": "** tf.SparseTensor ** - A sparse representation Tensor\nFor sparse Tensors, a more efficient representation is `tf.SparseTensor`.\n"}
{"snippet": "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n", "intent": "Run the next cell to train your model and learn the softmax parameters (W,b). \n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(64, activation='softmax', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='softmax'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "with tf.Session() as sess:\n    x.initializer.run()\n    y.initializer.run()\n    result = f.eval()\nprint(result)\n", "intent": "One downside of the above code is that we keep repeating `sess.run()`. There is a better way as shown below:  \n"}
{"snippet": "model.fit(x_train, y_train, epochs=200, batch_size=100, verbose=0)\nmodel2.fit(x_train, y_train, epochs=200, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "hidden = activation(torch.mm(features, W1) + B1)\noutput = activation(torch.mm(hidden, W2) + B2)\nprint(output)\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "gm = GaussianMixture(n_components=5, random_state=229)\ngm.fit(X)\n", "intent": "**Gaussian Mixture**\n"}
{"snippet": "clf = IsolationForest(random_state=229)\nclf.fit(X)\nData3[\"isf_score\"] = clf.decision_function(X)\nData3.sort_values(by=\"isf_score\").County[:5]\n", "intent": "**Isolation Forest**\n"}
{"snippet": "outputs = net(Variable(images_copy))\n", "intent": "Now, let's forward these through our trained network and see what our predicted labels are:\n"}
{"snippet": "correct = 0\ntotal = 0\nfor i, data in enumerate(testloader, 0):\n    images, labels = data\n    outputs = net(Variable(images))\n    _, predicted = torch.max(outputs.data, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum()\nprint('Accuracy on all 10000 test images: %d %%' % (100 * correct/total))\n", "intent": "Not bad I guess. Let's compute how we did over the entire test set:\n"}
{"snippet": "variables = [\"Diesel\", \"Petrol\"]\nfor i in range(len(variables)):\n    X = priceDaily[\"Crude Oil\"]\n    y = priceDaily[[variables[i]]]\n    X = sm.add_constant(X)\n    model = sm.OLS(y, X, missing='drop').fit()\n    print('Dependent: ', variables[i])\n    print('      R2: ', model.rsquared)\n", "intent": "We can see a major improvement, with the major peaks lining up. We will now check the R2 values from Linear Regression Summaries.\n"}
{"snippet": "logreg.fit(training_x, training_y)\n", "intent": "<a name=\"20180525_01\"></a>The following won't work, because our $x$ variable has to be a column vector, whereas currently it's a 1d array.\n"}
{"snippet": "logreg.fit(training_x1, training_y)\n", "intent": "And now the logistic regression function works. (Note that we leave $y$ as a 1d-array, and do <i>not</i> transform it into a column vector.)\n"}
{"snippet": "x1 = tf.Variable(1)\nx1.graph is tf.get_default_graph()\n", "intent": "Any node you create is automatically added to the default graph: \n"}
{"snippet": "W = tf.Variable(tf.zeros([flatSize, numberOfClasses]), name=\"Weights\")\nb = tf.Variable(tf.zeros([10]), name=\"Biases\")\n", "intent": "Definim weights si biases:\n"}
{"snippet": "Y_Pred_Prob = tf.nn.softmax(tf.matmul(X_vec, W) + b, name=\"softmax\")\n", "intent": "Activarea pentru layerul FC:\n"}
{"snippet": "with tf.name_scope(\"Layer_1\"):\n    W1 = tf.Variable(tf.truncated_normal([flatSize, sizeLayerOne], stddev=0.1), name=\"Weights\")\n    b1 = tf.Variable(tf.zeros([sizeLayerOne]), name=\"Biases\")\n    Y1 = tf.nn.relu(tf.matmul(X_vec, W1) + b1, name=\"Activation\")\n    Y1_Dropout = tf.nn.dropout(Y1, probKeep)\n", "intent": "Definim arhitectura modelului : \n"}
{"snippet": "with tf.name_scope(\"Output_Layer\"):\n    W5 = tf.Variable(tf.truncated_normal([sizeLayerFour, sizeLayerFive], stddev=0.1), name=\"Weights\")\n    b5 = tf.Variable(tf.zeros([sizeLayerFive]), name=\"Biases\")\n    Y_logits = tf.matmul(Y4_Dropout, W5) + b5\n    Y_Pred = tf.nn.softmax(Y_logits, name=\"Activation\")\n", "intent": "Clasificator / Output layer : \n"}
{"snippet": "with tf.name_scope(\"Layer_1\"):\n    W1 = tf.Variable(tf.truncated_normal([flatSize, sizeLayerOne], stddev=0.1), name=\"Weights\")\n    b1 = tf.Variable(tf.zeros([sizeLayerOne]), name=\"Biases\")\n    Y1 = tf.nn.sigmoid(tf.matmul(X_vec, W1) + b1, name=\"Activation\")\n", "intent": "Definim arhitectura, in termeni de dimensiuni si conexiuni intre W si b\n"}
{"snippet": "with tf.name_scope(\"Output_Layer\"):\n    W5 = tf.Variable(tf.truncated_normal([sizeLayerFour, sizeLayerFive], stddev=0.1), name=\"Weights\")\n    b5 = tf.Variable(tf.zeros([sizeLayerFive]), name=\"Biases\")\n    Y_logits = tf.matmul(Y4, W5) + b5\n    Y_Pred = tf.nn.softmax(Y_logits, name=\"Activation\")\n", "intent": "Clasificatorul final, de output:\n"}
{"snippet": "m = SequenceModel(sequence_extraction, 10, name='lstm38', cell_size=128, layers=1)\nm.setup()\nprint 'built model'\nfor i in xrange(300):\n    m.train(1)\n    m.show_example()\n", "intent": "Epoch 167: accuracy: 98.09375%\n"}
{"snippet": "input = layers.Input(shape=(SIZE, SIZE, 3))\nmodel = applications.InceptionV3(weights='imagenet', include_top=False, input_tensor=input)\n", "intent": "Pretrained `VGG-F` networks aren't available for Keras, so use `InceptionV3` instead.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embedded_seq = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded_seq\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "A = tf.placeholder(tf.float32, shape=(None, 3))\nB = A + 5\nwith tf.Session() as sess:\n    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n", "intent": "Here is another example: \n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier(7)\nknn_clf.fit(X,y)\n", "intent": "**Problem 3b**\nTrain a $k=7$ nearest neighbors machine learning model on the Titanic training set.\n"}
{"snippet": "sess = tf.Session()\n", "intent": "A session object encapsulates the environment in which operation object are executed. Tensorflow objects are evaluated in those operations.\n"}
{"snippet": "predictor.fit(x_train, y_train,\n              batch_size=128,\n              epochs=10000,\n              callbacks=callbacks,\n              validation_data=(x_validation, y_validation))\npredictor.load_weights('best_model')  \n", "intent": "Finally, we fit our model:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1.0, 1.0))\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -0.1, 0.1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model.fit(data.drop(columns='Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "features = df.drop('TARGET CLASS', axis=1)\nscaler.fit(features)\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "lm.fit(X=X_train, y = y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid=param_grid, refit=True, verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "for v in [0., 1e-4, 1e-3, 1e-1, 1.]:\n    print \"Setting reg_lambda to \", v\n    reg_lambda = v\n    tf_non_linear_model = TFNonLinearModel()\n    train_tf_model(tf_non_linear_model, 10000, 1000)\n    wrapper = TFModelWrapper(tf_non_linear_model)\n    plot_decision_boundary(X, wrapper)\n", "intent": "Change the `l2_lambda` parameter and observe the effect it has on the decision boundary. \n**QUESTION**: Can you explain why this happens?\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\nclassifier = tf_learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[10,20,10], n_classes=2)\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "lm.fit(X=x_train,y=y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "history = model.fit(x_train, y_train, epochs=10, batch_size=50)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "dec_tree_classifier = DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1000))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(128))\nmodel.add(Dropout(0.4))\nmodel.add(Activation('relu'))\nmodel.add(Dense(2))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=12, batch_size=50, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "clf = RandomForestClassifier(\n    n_estimators=240,\n    max_depth=120,\n    random_state=0,\n    n_jobs=-1)\nclf.fit(x_train, y_train)\n", "intent": "The best result is achieved with `n_estimators=240` and `max_depth=120`. The accuracy obtained with these parameters is 0.838538.\n"}
{"snippet": "rf_ff = ensemble.RandomForestRegressor()\nrf_ff.fit(X_ff_train, y_ff_train)\nrf_ff.score(X_ff_test, y_ff_test)\n", "intent": "Try using another model (RandomForestRegressor or SVM)\n"}
{"snippet": "for v in [0., 1e-4, 1e-3, 1e-1, 1.]:\n    print(\"Setting reg_lambda to :\" + str(v))\n    reg_lambda = v\n    tf_non_linear_model = TFNonLinearModel()\n    train_tf_model(tf_non_linear_model, 10000, 1000)\n    wrapper = TFModelWrapper(tf_non_linear_model)\n    plot_decision_boundary(X, wrapper)\n", "intent": "Change the `l2_lambda` parameter and observe the effect it has on the decision boundary. \n**QUESTION**: Can you explain why this happens?\n"}
{"snippet": "plot_learning_curve(ensemble.RandomForestClassifier(), \n                    'Random Forest', seed_X, seed_y, fig_opts={'figsize':(14,10)})\n", "intent": "* Run a learning curve against the seed data? How much data do we need to train on?\n"}
{"snippet": "hold_prob = tf.placeholder(tf.float32)\nfull_one_dropot = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)\n", "intent": "** Now create the dropout layer with tf.nn.dropout, remember to pass in your hold_prob placeholder. **\n"}
{"snippet": "x = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.nn.softmax(tf.matmul(x, W) + b)\n", "intent": "<img src=\"https://www.tensorflow.org/images/softmax-regression-scalargraph.png\" width=\"800\">\n"}
{"snippet": "dtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "model.fit(df.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS',axis=1))\n", "intent": "** Create a StandardScaler() object called scaler.**\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Import KNeighborsClassifier from scikit learn.**\n"}
{"snippet": "knn.fit(X_train,y_train)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "lm.fit(X = x_train, y = y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "def cell_fn_forward(X, h, model, train=True):\n    Wxh, Whh, Why = model['Wxh'], model['Whh'], model['Why']\n    bh, by = model['bh'], model['by']\n    hprev = h.copy()\n    h, h_cache = tanh_forward(np.dot(hprev, Whh) + np.dot(X, Wxh) + bh)\n    y, y_cache = fc_forward(h, Why, by)\n    cache = (X, Whh, h, hprev, y, h_cache, y_cache)\n    if not train:\n            y = softmax(y)\n    return y, h, cache\n", "intent": "Now we can implement the equation $h_t = \\sigma(W_{hh}h_{t-1} + W_{xh}x_t)$ as follows:\n"}
{"snippet": "clf_s = SVC(C = 100, class_weight = None, gamma = 1.0, probability=True)\nplot_decision_surface(clf_s, subset, y)\nclf_s = SVC(C = 100, class_weight = 'auto', gamma = 1.0, probability=True)\nplot_decision_surface(clf_s, subset, y)\n", "intent": "**(d)** The <a href='http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"}
{"snippet": "layer2_input = layer1.output.flatten(2)\nlayer2 = HiddenLayer(rng,\n                     input=layer2_input,\n                     n_in=num_kernels[1] * edge1 * edge1,\n                     n_out=sigmoidal_output_size,\n                     activation=T.tanh)\n", "intent": "The sigmoidal layer takes a vector as input.\nWe flatten all but the first two dimensions, to get an input of size **`(batch_size, 30 * 4 * 4)`**.\n"}
{"snippet": "layer3 = LogisticRegression(input=layer2.output,\n                            n_in=sigmoidal_output_size,\n                            n_out=10)\n", "intent": "A fully connected logistic regression layer converts the sigmoid's layer output to a class label.\n"}
{"snippet": "validate_model = theano.function(\n        [index],\n        layer3.errors(y),\n        givens={\n            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n        }\n    )\n", "intent": "To track progress on a held-out set, we count the number of misclassified examples in the validation set.\n"}
{"snippet": "test_model = theano.function(\n    [index],\n    layer3.errors(y),\n    givens={\n        x: test_set_x[index * batch_size: (index + 1) * batch_size],\n        y: test_set_y[index * batch_size: (index + 1) * batch_size]\n    }\n)\n", "intent": "After training, we check the number of misclassified examples in the test set.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n    embed_out = tf.nn.embedding_lookup(embedding,input_data)\n    return embed_out\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(nb_filter=32, nb_row=3, nb_col=3, border_mode='valid', input_shape=(32, 32,3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "model = LogisticRegression()\n", "intent": "---\nProblem 6\n---------\nTrain a logistic regressor on the image data using 50, 100, 1000 and 5000 training samples. \n"}
{"snippet": "regr = LinearRegression() \nregr.fit(X_train, y_train) \n", "intent": "d) Train model on training data, and make predictions on testing data\n"}
{"snippet": "def hidden_model(x):\n    result = x[:, 5] + x[:, 10]\n    result += np.random.normal(0, .005, result.shape)\n    return result\ndef make_x(nobs):\n    return np.random.uniform(0, 3, (nobs, 10 ** 6))\nx = make_x(20)\ny = hidden_model(x)\nprint(x.shape)\n", "intent": "Let's make the dataset, and compute the y's with a \"hidden\" model that we are trying to recover\n"}
{"snippet": "clf=DecisionTreeClassifier()\nclf.fit(feature,label)\n", "intent": "Fitting the data set to DECISION TREE CLASSIFIER\n"}
{"snippet": "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']),\\\n    biases['hidden_layer'])\nlayer_1 = tf.nn.relu(layer_1)\nlogits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])\n", "intent": "<img src=\"images/multi-layer.png\" style=\"height: 50%;width: 50%; position: relative; right: 5%\">\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform(shape=(vocab_size, embed_dim), minval= -0.1, maxval=0.1, dtype=tf.float32, \n                                              name='embeddings'))\n    embed = tf.nn.embedding_lookup(embeddings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "class PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self,epoch,logs):\n    if epoch % 100 == 0: print('')\n    print('.', end='')\nEPOCHS = 500\nhistory = model.fit(train_data, train_labels, epochs=EPOCHS,\n                    validation_split=0.2, verbose=0,\n                    callbacks=[PrintDot()])\n", "intent": "The model is trained for 500 epochs, and record the training and validation accuracy in the `history` object.\n"}
{"snippet": "smaller_model = keras.Sequential([\n    keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(10000,)),\n    keras.layers.Dense(4, activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\nsmaller_model.compile(optimizer='adam',\n                loss='binary_crossentropy',\n                metrics=['accuracy', 'binary_crossentropy'])\nsmaller_model.summary()\n", "intent": "Let's create a model with less hidden units to compare against the baseline model that we just created:\n"}
{"snippet": "smaller_history = smaller_model.fit(train_data,\n                                    train_labels,\n                                    epochs=20,\n                                    batch_size=512,\n                                    validation_data=(test_data, test_labels),\n                                    verbose=2)\n", "intent": "And train the model using the same data:\n"}
{"snippet": "bigger_history = bigger_model.fit(train_data, train_labels,\n                                  epochs=20,\n                                  batch_size=512,\n                                  validation_data=(test_data, test_labels),\n                                  verbose=2)\n", "intent": "And, again, train the model using the same data:\n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=x_train.shape[1]))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.35))\nmodel.add(Dense(256))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dense(num_classes))\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "xt = x[:, best_features]\nclf = LinearRegression().fit(xt, y)\nprint(\"Score is \", clf.score(xt, y))\n", "intent": "A linear regression on the full data looks good. The \"score\" here is the $R^2$ score -- scores close to 1 imply a good fit.\n"}
{"snippet": "net = sklearn.linear_model.Perceptron(n_iter=1, warm_start=True)\n", "intent": "Create a [perceptron object](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html).\n"}
{"snippet": "def conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n", "intent": "Define the two handy functions to convolve and pool features:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev=0.25))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf = GridSearchCV(SVC('rbf'), \n                   param_grid=parameters_grid, \n                   cv=cv,      \n                   n_jobs=-1)  \nclf.fit(X, y)\n", "intent": "Then give `GridSearchCV` the ML model and the grid of its hyperparameters and fit it on the data.\n"}
{"snippet": "db = DBSCAN(eps=3, min_samples=3, n_jobs=-1)\ndb.fit(Xs)\n", "intent": "Remember to pass an `eps` and `min_samples` of your choice.\n"}
{"snippet": "lr = LinearRegression()\nmodel1 = lr.fit(X_train_ss, y_train)\ntrain_score = model1.score(X_train_ss, y_train)\nprint('model score on training data: ', train_score)\n", "intent": "---\nCross-validate the $R^2$ of an ordinary linear regression model with 10 cross-validation folds.\nHow does it perform?\n"}
{"snippet": "rdge = Ridge(alpha=ridge_alpha)\nrdge.fit(X_train_ss, y_train)\nrdge_train = rdge.score(X_train_ss, y_train)\nrdge_test = rdge.score(X_test_ss, y_test)\nprint('model score for training dataset is ', rdge_train)\nprint('model score for test dataset is ', rdge_test)\n", "intent": "---\nIs it better than the Linear regression? If so, why might this be?\n"}
{"snippet": "lasso = Lasso(lasso_alpha)\nlasso.fit(X_train_ss, y_train)\nlasso_train = lasso.score(X_train_ss, y_train)\nlasso_test = lasso.score(X_test_ss, y_test)\nprint('lasso model score for training is ', lasso_train)\nprint('lasso model score for test is ', lasso_test)\n", "intent": "---\nIs it better than the Linear regression? Is it better than Ridge? What do the differences in results imply about the issues with the dataset?\n"}
{"snippet": "df['MDEV'] = y\nmodel = sm.formula.ols(formula=\"MDEV ~ LSTAT + NOX + CHAS -1\", data=df)\nresults = model.fit()\nresults.summary()\n", "intent": "We will walk through this after practice.  (ie: formula='Lottery ~ Literacy + Wealth + Region', data=df)\n"}
{"snippet": "smaller_frame=census_data[['educ_coll', 'average_income', 'per_vote']]\nfrom pandas.tools.plotting import scatter_matrix\naxeslist=scatter_matrix(smaller_frame, alpha=0.8, figsize=(12, 12), diagonal=\"kde\")\nfor ax in axeslist.flatten():\n    ax.grid(False)\n", "intent": "We use a SPLOM to visualize some columns of this dataset. In Panda's the SPLOM is a one-liner.\n"}
{"snippet": "enet.fit(Xn,y)\n", "intent": "---\nHow does it compare to the other regularized regressions?\n"}
{"snippet": "from sklearn.cross_validation import cross_val_score, cross_val_predict\nfrom sklearn import metrics\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport seaborn as sns\nfrom sklearn import datasets, linear_model\nlm = linear_model.LinearRegression()\n", "intent": "---\nCross-validate the $R^2$ of a linear regression model with 10 cross-validation folds.\nHow does it perform?\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nX = sample[['number_project']].values\ny = sample['satisfaction_level'].values\nlr.fit(X, y)\norig_int = lr.intercept_\norig_coef = lr.coef_[0]\nprint lr.intercept_, lr.coef_\n", "intent": "**Build a regression predicting satisfaction level from number of projects using the sample.**\nPrint out the intercept and coefficient.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(400, activation='sigmoid', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='sigmoid'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          epochs=10,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from keras.regularizers import l2\nmodel = Sequential()\nmodel.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.), input_dim=x_train.shape[1]))\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=x_train.shape[0],\n          epochs=100,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\npipeline = build_pipeline(DecisionTreeClassifier(criterion=\"entropy\"), x_train, y_train)\nclassifier= pipeline.get_params()['classifier']\ncount_vectorizer = pipeline.get_params()['count_vectorizer']\ndot_data = export_graphviz(classifier, out_file=None,max_depth=3,\n                           feature_names=count_vectorizer.get_feature_names(),class_names=class_names) \ngraph = graphviz.Source(dot_data) \ngraph\n", "intent": "Now we're ready to build and visualize the decision tree.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)  \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf = KNeighborsClassifier(20, warn_on_equidistant=False).fit(Xtrain, ytrain)\npoints_plot(Xtr, Xtrain, Xtest, ytrain, ytest, clf)\n", "intent": "We do kNN with 20 neighbors. kNN \n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 50)\nrf.fit(X, y)\n", "intent": "Fit RF model on data and visualize it\n"}
{"snippet": "ds = DecisionTreeClassifier(max_depth=1)\nada = AdaBoostClassifier(n_estimators=300)\n", "intent": "Compare and contrast Decision Trees and AdaBoost\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape=(n_cols,)))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(2, activation = \"softmax\"))\nmodel.compile(optimizer=\"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n", "intent": "This is a very simple model, it only has one shallow layer. Let's add some more layers.\n"}
{"snippet": "model.fit(X, y_binary, epochs=40, validation_split = 0.25)\n", "intent": "We're trained a really good model, but principles of cross validation also to deep learning. Here's how we'll evaluate the model on a testing data.\n"}
{"snippet": "knn3 = KNeighborsClassifier(n_neighbors=3)\nknn3.fit(X,y)\nscore3 = float(knn3.score(X,y))\nprint \"The model accurately labelled {:.2f} percent of the data\".format(score3*100)\n", "intent": "Train a KNN model using 3 neighbors\n"}
{"snippet": "knn5 = KNeighborsClassifier(n_neighbors=5)\nknn5.fit(X,y)\nscore5 = float(knn5.score(X,y))\nprint \"The model accurately labelled {:.2f} percent of the data\".format(score5*100)\n", "intent": "Now with 5 neighbors\n"}
{"snippet": "randsearch_cv = RandomizedSearchCV(pipe_cv, n_iter = 5,\n                        param_distributions = param_grid_cv, cv = 5, scoring = \"accuracy\")\nt = time()\nrandsearch_cv.fit(X, y)\nprint time() - t\n", "intent": "Countvectorizer randomized search\n"}
{"snippet": "randsearch_tf = RandomizedSearchCV(pipe_tf, n_iter = 10,\n                        param_distributions = param_grid_tf, cv = 5, scoring = \"accuracy\")\nt = time()\nrandsearch_tf.fit(X, y)\nprint time() - t\n", "intent": "Tfidfvectorizer randomized search\n"}
{"snippet": "lm.fit(X_train, Y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "clf = KNeighborsClassifier(1, warn_on_equidistant=False).fit(Xtrain, ytrain)\npoints_plot(Xtr, Xtrain, Xtest, ytrain, ytest, clf)\n", "intent": "What if we decide to get ultra local. We get high variance and Jagged islands.\n"}
{"snippet": "model = Sequential([\n    Dense(512, input_dim=num_pixels, activation='relu'),\n    Dropout(0.5), \n    Dense(512, activation='relu'),\n    Dropout(0.2), \n    Dense(num_classes, kernel_initializer='normal', activation='softmax')\n])\nmodel.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\nmodel.summary()\n", "intent": "W tym przypadku dwa razy.\n"}
{"snippet": "model = Sequential([\n    LSTM(32, input_shape=(1, 1)),\n    Dense(1)\n])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n", "intent": "Wracamy do naszej sieci.\n"}
{"snippet": "model.fit(x_train, y_train, \n          batch_size=32,\n          epochs=10, \n          validation_data=(x_test, y_test),\n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "kmeans_2 = KMeans(n_clusters=2, random_state=0)\nmodel_2 = kmeans_2.fit(data)\nlabels_2 = model_2.labels_\nh.plot_data(data, labels_2)\n", "intent": "`4.` Now try again, but this time fit kmeans using 2 clusters instead of 4 to your data.\n"}
{"snippet": "kmeans_7 = KMeans(n_clusters=7, random_state=0)\nmodel_7 = kmeans_7.fit(data)\nlabels_7 = model_7.labels_\nh.plot_data(data, labels_7)\n", "intent": "`5.` Now try one more time, but with the number of clusters in kmeans to 7.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X, y)\n", "intent": "Now we will fit a model to figure out which variables drive user retention. We will use the random forest model.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    with tf.name_scope(name='embedding'):\n        embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1, name='embeding_w'))\n        embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "happyModel.fit(x = X_train,y = Y_train,epochs=40,batch_size=50)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "W_fc2 = weight_variable([1024, labels_count])\nb_fc2 = bias_variable([labels_count])\ny = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n", "intent": "Softmax layer, the same one if we use just a simple softmax regression.\n"}
{"snippet": "clf = KNeighborsClassifier(35, warn_on_equidistant=False).fit(Xtrain, ytrain)\npoints_plot(Xtr, Xtrain, Xtest, ytrain, ytest, clf)\n", "intent": "You do it for 35 now..see what happens?\n"}
{"snippet": "svc = LinearSVC(C = 1)\nt=time.time()\nsvc.fit(X_train, y_train)\nt2 = time.time()\nprint(round(t2-t, 2), 'Seconds to train SVC...')\nprint('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n", "intent": "The best C parameter was found to be 4 since it resulted in the fastest training time.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    return tf.nn.embedding_lookup(embeding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "weights = lasagne.layers.get_all_params(nn,trainable=True)\n", "intent": "* The standard way:\n * prediction\n * loss\n * updates\n * training and evaluation functions\n"}
{"snippet": "model = KMeans(n_clusters=5,random_state=0).fit(X)\nmodel.cluster_centers_.shape\n", "intent": "In case I need to revisit later!\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nparam_grid = [{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n              {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n             ]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\nscoring='neg_mean_squared_error')\ngrid_search.fit(housing_prepared, housing_labels)\ngrid_search.best_params_\n", "intent": "The RandomForestRegressor looks promosing. Lets do some grid searching to fine tune it a little!\n"}
{"snippet": "reset_graph()\nn_inputs = 2\nX = tf.placeholder(tf.float32, shape=(None, n_inputs +1), name='X')\ny = tf.placeholder(tf.float32, shape=(None, 1), name='y')\ntheta = tf.Variable(tf.random_uniform([n_inputs + 1,1],-1.0,1.0,seed=42),name='theta')\nlogits = tf.matmul(X,theta,name='logits')\ny_proba = tf.sigmoid(logits)\n", "intent": "Try building the sigmoid function from scratch, and using the TF built in function:\n"}
{"snippet": "init = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    test2 = y_proba.eval(feed_dict={X:X_train,y:y_train})\nprint(test2.shape)\nprint(test2[:5])\n", "intent": "Good start, lets test this out to see if its working so far:\n"}
{"snippet": "K.clear_session()\nnp.random.seed(1)\nsess = K.get_session()\nyolo = load_model(\"model_data/yolo.h5\")\n", "intent": "Build `yolo_head` func, which takes in model output and converts it to...\n"}
{"snippet": "model.fit([Xoh, s0, c0], outputs, epochs=10, batch_size=100)\n", "intent": "Let's now fit the model and run it for one epoch.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ndc = DecisionTreeClassifier()\ndc.fit(features_train_std, labels_train)\n", "intent": "We'll use Random Forest classification algorithm\n"}
{"snippet": "from sklearn.datasets import fetch_mldata\nmnist = fetch_mldata('MNIST original')\nrnd_clf = RandomForestClassifier(random_state=42)\nrnd_clf.fit(mnist[\"data\"], mnist[\"target\"])\n", "intent": "**Let's try to calculate feature importances for the MNIST dataset!**\n"}
{"snippet": "from sklearn.svm import SVC\nclf_svc = SVC(kernel='rbf', C=3, gamma=0.5, probability=True)\nclf_svc.fit(X_imbal,y_imbal)\nplot_decision_boundary(clf_svc, X_imbal, y_imbal, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True)\n", "intent": "**Build a somewhat regularized model (low C and gamma params) so that we misclassify many of the 1s (the unbalanced class)**\n"}
{"snippet": "import tensorflow as tf\nx = tf.Variable(3, name='x')\ny = tf.Variable(4, name='y')\nf=x*x*y +y + 2\n", "intent": "The following code creates the graph represented below:\n"}
{"snippet": "x1 = tf.Variable(1)\nx1.graph is tf.get_default_graph()\n", "intent": "Any node you create is automatically added to the default graph.\n"}
{"snippet": "w = tf.constant(3)\nx = w + 2\ny = x + 5\nz = x * 3\nwith tf.Session() as sess:\n    print(y.eval()) \n    print(z.eval()) \n", "intent": "When you evaluate a node, TF determines set of nodes that it depends on and it evaluates these first.\n"}
{"snippet": "model = w2v_model(h_size=10, v_size=V, context_size=5)\nmodel.fit(sentences, num_neg_samples=10, learning_rate=1e-4, momentum=0.99, reg=0.1, epochs=10, plot_data=True)\n", "intent": "Word embeddings will be trash, but we want to make sure it is atleast running!\n"}
{"snippet": "df_1 = df.drop('Private',axis=1)\nkmeans.fit(df_1)\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "sc.fit(df.drop('TARGET CLASS',axis=1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "from sklearn.svm import SVC\nclf = SVC(kernel='linear')\nclf.fit(X,y)\n", "intent": "**Let's Try**\nFit the model:\n"}
{"snippet": "clf = GaussianNB()\nclf.fit(features_train_std, labels_train)\n", "intent": "Fit the training data and target/label data.  Again, the training data is the part \n"}
{"snippet": "model_RAND   = compile_keras_sequential_model(model_layers_RAND, \"RAND\")\nmodel_LAST   = compile_keras_sequential_model(model_layers_LAST, \"LAST\")\nmodel_LAST2  = compile_keras_sequential_model(model_layers_LAST2, \"LAST2\")\nmodel_LINEAR = compile_keras_sequential_model(model_layers_LINEAR, \"LINEAR\")\nmodel_DNN    = compile_keras_sequential_model(model_layers_DNN, \"DNN\")\nmodel_CNN    = compile_keras_sequential_model(model_layers_CNN, \"CNN\")\nmodel_RNN    = compile_keras_sequential_model(model_layers_RNN, \"RNN\")\nmodel_RNN_N  = compile_keras_sequential_model(model_layers_RNN_N, 'RNN_N')\n", "intent": "<a name=\"benchmark\"></a>\nBenchmark all the algorithms. This takes a while (approx. 10 min).\n"}
{"snippet": "Linear_Model = LinearRegression()\n", "intent": "Age(y) = B0 + B1NO + B2Nt +B3pH + B4PetalCount\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\ngsm = GridSearchCV(LogisticRegressionCV(solver='liblinear'), {\n                                                                'Cs':np.arange(2,12,1), \n                                                                'cv':np.arange(2,17,1), \n                                                                'penalty':['l1','l2']\n                                                             }, verbose=1, n_jobs=-1)\ngsm.fit(X, y_ravel)\ngsm.best_estimator_\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\ngsm = GridSearchCV(LogisticRegressionCV(solver='liblinear'), {'Cs':np.arange(2,12,1), 'cv':np.arange(2,17,1), 'penalty':['l1','l2']}, verbose=1, n_jobs=-1)\ngsm.fit(X, y_ravel)\ngsm.best_estimator_\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "from sklearn.grid_search import GridSearchCV\nmodel2 = GridSearchCV(LogisticRegressionCV(solver='liblinear'), {'Cs':np.arange(2,12,1), 'cv':np.arange(2,17,1), 'penalty':['l1','l2']}, verbose=True)\nmodel2.fit(Xtr, ytr)\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embed = tf.contrib.layers.embed_sequence(input_data, vocab_size=vocab_size, embed_dim=embed_dim)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "tf.logging.set_verbosity(tf.logging.INFO)\nclassifier.fit(\n    input_fn = train_input_fn,\n    steps = 100,\n    monitors = [logging_hook, validation_monitor]\n)\n", "intent": "Let its trainnnn !!!\n"}
{"snippet": "model = Sequential()\nmodel.add(Conv2D(filters=10, kernel_size=3, strides=1, padding=\"same\", activation='relu', input_shape=(28,28,1)))\nmodel.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))\nmodel.add(Conv2D(filters=10, kernel_size=3, strides=1, padding=\"same\", activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))\nmodel.add(Flatten())\nmodel.add(Dense(units=10, activation=\"softmax\"))\nadam = Adam(lr=1e-3)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n", "intent": "Similar model as Tensorflow\nCONV3 - MAX POOL - CONV3 - MAX POOL - FC\n"}
{"snippet": "sess, weights = train_perceptron(X, y)\n", "intent": "Obtain weights and the TensorFLow session variable\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_jobs=1)\nrfc.fit(features_train_std, labels_train)\n", "intent": "We'll use Random Forest classification algorithm\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1) \n", "intent": " - ESTIMATOR is scikits term for model (estimate unknown quantities)\n- \"INSTANTIATE means make an instance of\"\n"}
{"snippet": "knn.fit(X,y)\n", "intent": "    -  model is learning the relationship btwn X and Y \n      - occurs inplace \n"}
{"snippet": "import sklearn.ensemble as sk\nclf = sk.RandomForestClassifier(n_estimators=100, oob_score=True,min_samples_split=5, min_samples_leaf= 2)\nclf = clf.fit(x_train, y_train.income)\n", "intent": "<strong>Random Forest</strong>\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_normal([vocab_size, embed_dim]))\n    embeddings = tf.nn.embedding_lookup(embeddings, input_data)\n    return embeddings\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform([vocab_size,embed_dim], -1.0,1.0))\n    embed = tf.nn.embedding_lookup(embeddings,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embeddings = tf.Variable(tf.random_uniform([vocab_size,embed_dim], -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embeddings,input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf1 = svm.SVC(gamma=0.1,C =1)\nclf2 = svm.SVC(gamma=0.1,C =1)\n", "intent": "defining the function\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable( tf.random_uniform((vocab_size, embed_dim), -1, 1) )\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "gmm = GaussianMixture(n_components=5,covariance_type='full',init_params='kmeans') \ngmm.fit(X_sv)\nX_grid = np.array([x.flatten(),y.flatten()]).T\nz = np.exp(gmm.score_samples(X_grid).reshape(x.shape)) \n", "intent": "---\nWhat do we get if we change the number of components to 5?\n"}
{"snippet": "clf = GaussianNB()\nclf.fit(features, labels)\n", "intent": "Fit the training data and target/label data.  Again, the training data is the part \n"}
{"snippet": "a = np.asarray([[1, 2, 3], [4, 5, 6]], dtype=np.float64)\nb = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nprint(a + b)\nprint(a - b)\nprint(a * b)\nprint(a / b)\nprint(a % b)\nprint(a**b)\n", "intent": "But this is where it get crazy...\n"}
{"snippet": "a = torch.rand(3)\nb = torch.autograd.Variable(a)\nprint(a)\nprint(b)\n", "intent": "To make `pytorch` track which operations are performed on a `torch.Tensor`, we have to make it into a **variable**:\n"}
{"snippet": "sess = tf.Session()\n", "intent": "First create a new session\n"}
{"snippet": "model2 = Sequential()\nmodel2.add(Dense(units=256,input_shape=X_dtm.shape[1:]))\nmodel2.add(Dropout(0.5))\nmodel2.add(Activation('relu'))\nmodel2.add(BatchNormalization())\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(units=24))\nmodel2.add(Activation('sigmoid'))\n", "intent": "The input shape for the neural network will be the X_dtm shape\nWe will use a relu activation. The output will be the categories\n"}
{"snippet": "clf = OneVsRestClassifier(RandomForestClassifier(n_jobs=-1, n_estimators=260, max_depth=10,random_state=42,bootstrap=False,max_features=10))\n", "intent": "After RandomForest tunning, the number of estimators that maximizes the AUC are 260.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\nclassifier_DTC = DecisionTreeClassifier(criterion='gini', random_state = 0)\nclassifier_DTC.fit(X_train, y_train)\n", "intent": "<a id=\"decision_tree_classifier\"></a>\n"}
{"snippet": "def new_model(d_in=X.shape[1], d_hidden=20, d_out=(y.max() + 1)):\n    print(d_in, d_hidden, d_out)\n    model = {}\n    model[\"W1\"] = np.random.rand(d_in, d_hidden) \n    model[\"b1\"] = np.zeros(d_hidden) \n    model[\"W2\"] = np.random.rand(d_hidden, d_out) \n    model[\"b2\"] = np.zeros(d_out) \n    return model\n", "intent": "Now let's define a model. It will be a two-layer fully connected neural network with ReLU activation.\n"}
{"snippet": "def relu(activation):\n    return activation * (activation > 0)\ndef accuracy(y_pred, y_actual):\n    accurate = (np.argmax(y_pred, axis=1) == np.argmax(y_actual, axis=1)).sum()\n    return accurate / y_pred.shape[0]\ndef to_onehot(y_labels):\n    onehot = np.zeros((y_labels.shape[0], y_labels.max() + 1))\n    onehot[np.arange(y_labels.shape[0]), y_labels] = 1\n    return onehot\n", "intent": "We'll also define some utility functions:\n"}
{"snippet": "neural_regressor = MLPRegressor(verbose=True,\n                                hidden_layer_sizes=(10, 10),\n                               learning_rate_init=0.01,\n                               max_iter=100,\n                               tol=-100)\nneural_regressor.fit(X_train, y_train)\n", "intent": "And a neural network:\n"}
{"snippet": "feature_cols = ['TV', 'Radio', 'Newspaper']\nX = data[feature_cols]\ny = data.Sales\nlinreg = LinearRegression()\nlinreg.fit(X, y)\nprint(linreg.intercept_)\nprint(linreg.coef_)\n", "intent": "Let's redo some of the Statsmodels code above in scikit-learn:\n"}
{"snippet": "hidden_dim = 256\nlstm_layers_no = 3\nmodel = CharacterModel(hidden_dim, lstm_layers_no=lstm_layers_no).type(dtype)\nloss_fun = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\nmodel\n", "intent": "Having defined the model, we can initialize it. Feel free to play with the hyperparameters!\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\nlinear_reg.fit(X_train, y_train)\n", "intent": "It's a bunch of ones and zeros! Wouldn't it make sense to just train a linear regressor on the data?\n"}
{"snippet": "def relu(activation):\n    return activation * tf.cast((activation > 0), dtype=tf.float32)\n", "intent": "A computational graph is made of:\n* placeholders (inputs to the graph)\n* variables\n* operations on them and their results\n"}
{"snippet": "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\ntrain_dict = {X: X_train, y: y_train}\nnum_iterations = 500\nwith tf.Session() as sess:\n    with tf.device(\"/gpu:0\"): \n        tf.global_variables_initializer().run()\n        for i in range(num_iterations):\n            train_step.run(feed_dict=train_dict)\n            loss_val = loss.eval(feed_dict=train_dict)\n            if i % 50 == 0: print(loss_val)\n", "intent": "Let's train again, but without perfoming Gradient Descent manually\n"}
{"snippet": "def conv_block(out_channels: int):\n  return k.models.Sequential([\n      k.layers.Conv2D(out_channels, 3, padding='same', activation='relu'),\n      k.layers.BatchNormalization(),\n      k.layers.Dropout(0.3)\n  ])\n", "intent": "Firts, let's define our own layer:\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nrid = Ridge()\nparams = {'alpha':[0.1, 0.5, 1,2,5, 100]}\ncv_rid = GridSearchCV(rid,params)\ncv_rid.fit(X_train,y_train)\ncv_rid.best_params_\n", "intent": "Ridge model has parameter *alpha* we can tune, which controls the strength of the regularization; use GridSearchCV to try to optimize:\n"}
{"snippet": "from sklearn.linear_model import Lasso\nlas = Lasso(max_iter=4000)\nparams = {'alpha':[0.1, 0.5, 1,2,10, 100]}\ncv_las = GridSearchCV(las,params)\ncv_las.fit(X_train,y_train)\ncv_las.best_params_\n", "intent": "In Lasso, some of the coefficients can actually be set to zero.\n"}
{"snippet": "from keras.callbacks import ModelCheckpoint\nh=network.fit(train_images_1d, \n              train_labels, \n              epochs=5, \n              batch_size=128, \n              shuffle=True, \n              callbacks=[ModelCheckpoint('tutorial_MNIST.h5',save_best_only=True)])\n", "intent": "Training the network does not take long and can easily be done quickly on a CPU. Validation accuracy quickly rises to  about 99%.\n"}
{"snippet": "params_0 = {\n    'scale_pos_weight': [x for x in range(1,11)]\n}\ngs_0 = GridSearchCV(\n    estimator=model_v0, \n    param_grid=params_0,\n    scoring=scoring, \n    cv=cv_sets, \n    refit='Accuracy')\ngs_0.fit(X_train, y_train)\n", "intent": "Tuning scale_pos_weight parameter.\n"}
{"snippet": "lm = smf.ols(formula='Sales ~ TV + Radio + Newspaper + IsLarge', data=data).fit()\nlm.params\n", "intent": "Let's redo the multiple linear regression and include the **IsLarge** predictor:\n"}
{"snippet": "sm.OLS(y_2015, X_2015).fit().summary()\n", "intent": "The 2015 model actually applies pretty well for every year. I feel confident in applying it to my 2016 data.\n"}
{"snippet": "y, X = patsy.dmatrices('high_paid ~ rating_cat', data=jobs_w_salary)\nsm.Logit(y, X).fit().summary()\n", "intent": "In fact, the pseudo-r2 is even better when it's a categorized variable!\n"}
{"snippet": "sm.Logit(y, X).fit().summary()\n", "intent": "All but parent_child and fare are statistically significant. I will remove these to see how it compares.\n"}
{"snippet": "logreg_parameters = {\n                     'penalty' : ['l1','l2'],\n                     'C' : [.00001, .0001, .001, .01, .1, 1, 5, 10, 100, 1000, 10000]              \n                    }\nbest_log_reg = model_selection.GridSearchCV(log_reg, logreg_parameters)\nbest_log_reg.fit(X, y)\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "neigh = best_knn.best_estimator_\nneigh.fit(X, y)\n", "intent": "*af*\n*You probably want to use logistic regression when you have dummy variables*\n"}
{"snippet": "logreg_parameters = {\n    'penalty' : ['l1','l2'],\n    'C' : [.00001, .0001, .001, .01, .1, 1, 5, 10, 100, 1000, 10000]              }\nbest_log_reg = model_selection.GridSearchCV(log_reg, logreg_parameters)\nbest_log_reg.fit(X, y)\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "model_sgdrA = linear_model.SGDRegressor()\nmodel_sgdrB = linear_model.SGDRegressor()\nmodel_sgdrC = linear_model.SGDRegressor()\nmodel_sgdrD = linear_model.SGDRegressor()\nmodel_sgdrE = linear_model.SGDRegressor()\n", "intent": "- Stochastic Gradient Descent Regressor\n"}
{"snippet": "model_lrA = linear_model.LinearRegression()\nmodel_lrB = linear_model.LinearRegression()\nmodel_lrC = linear_model.LinearRegression()\nmodel_lrD = linear_model.LinearRegression()\nmodel_lrE = linear_model.LinearRegression()\n", "intent": "- Linear Regression\n"}
{"snippet": "model_rfrA = ensemble.RandomForestRegressor()\nmodel_rfrB = ensemble.RandomForestRegressor()\nmodel_rfrC = ensemble.RandomForestRegressor()\nmodel_rfrD = ensemble.RandomForestRegressor()\nmodel_rfrE = ensemble.RandomForestRegressor()\n", "intent": "- Random Forest Regressor\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Step 2:** \"Instantiate\" the \"estimator\"\n- \"Estimator\" is scikit-learn's term for \"model\"\n- \"Instantiate\" means \"make an instance of\"\n"}
{"snippet": "model_final = ensemble.RandomForestRegressor()\n", "intent": "**Random Forest Regressor**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, input_dim=1000))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "kmeans = KMeans(n_clusters=12)\nmodel = kmeans.fit(X)\nprint(\"Model\\n\", model)\ntype(kmeans)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nUse k-Means Clustering\n<br><br></p>\n"}
{"snippet": "kmeans.fit(College_Data.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "sigma_e_a = np.linspace(1.0, 30, num=100)\nbeta0_hat_a =np.zeros(np.size(sigma_e_a))\nbeta1_hat_a =np.zeros(np.size(sigma_e_a))\nN = 1000\nfor i, sigma_e in enumerate(sigma_e_a):\n    X, Y = GenerateDataLinearFun(N, sigma=sigma_e)\n    beta0_hat_a[i], beta1_hat_a[i] = FitLinearModel(X,Y)\n", "intent": "Obviously there is a $1/n$ relationship \nNow let's look at the relationship wrt to size of the error ($\\epsilon$)\n"}
{"snippet": "dt = DecisionTreeRegressor(max_depth=3)\n", "intent": "Let's start with a very shallow tree which we can visualize. Again we are using the exact same sklearn API.\n"}
{"snippet": "dt = DecisionTreeRegressor()\n", "intent": "Now let's actually build a complicated tree.\n"}
{"snippet": "dt = DecisionTreeRegressor(min_samples_leaf=200)\n", "intent": "To combat this we can set the minimum number of samples in each final leaf to a higher value.\n"}
{"snippet": "rf = RandomForestRegressor(n_estimators=10, n_jobs=-1)\n", "intent": "Now we are fitting several decision trees. This can be done in parallel by several processors. `n_jobs = -1` uses all available processors.\n"}
{"snippet": "from pyspark.ml.classification import GBTClassifier\ngb = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\ngbModel = gb.fit(trainingData)\n", "intent": "1. Train a GBTClassifier on the training data, call the trained model 'gbModel'\n"}
{"snippet": "rf = RandomForestRegressor(n_estimators=40, n_jobs=-1, min_samples_leaf=10)\n", "intent": "The number of trees. \n"}
{"snippet": "lr = LinearRegression()\nlr.fit(X_train, y_train)\n", "intent": "Now we proceed as before.\n"}
{"snippet": "rf = RandomForestRegressor(n_estimators=40, n_jobs=-1, min_samples_leaf=10)\n", "intent": "We already know that we can probably do better with a random forest. Se let's simply try our setting from the previous notebook.\n"}
{"snippet": "cnn = Sequential([\n    Conv2D(32, kernel_size=5, strides=2, activation='relu', padding='same', \n           input_shape=(28, 28, 1)),\n    Conv2D(64, kernel_size=3, strides=2, activation='relu', padding='same'),\n    Conv2D(128, kernel_size=3, strides=2, activation='relu', padding='same'),\n    Flatten(),\n    Dense(10, activation='softmax'),\n])\n", "intent": "To take advantage of the spatial information in images we can use convolutions rather than fully connected layers.\nhttp://setosa.io/ev/image-kernels/\n"}
{"snippet": "model = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same', input_shape=(CHANNELS, ROWS, COLS), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation = 'softmax'))\nmodel.summary()\n", "intent": "A simple convolutional network to control the lander.\n"}
{"snippet": "model1 = Sequential()\nmodel1.add(Conv2D(32, (3, 3), padding='same', input_shape=(CHANNELS, ROWS, COLS), activation='relu'))\nmodel1.add(MaxPooling2D(pool_size=(2, 2)))\nmodel1.add(Conv2D(48, (3, 3), padding='same', activation='relu'))\nmodel1.add(MaxPooling2D(pool_size=(2, 2)))\nmodel1.add(Flatten())\nmodel1.add(Dense(128, activation='relu'))\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(num_classes, activation = 'softmax'))\nmodel1.summary()\n", "intent": "CNN model - 2 layers\n"}
{"snippet": "clf1 = SuperLearnerClassifier()\nsuperlearner1 = clf1.fit(X_train_plus_valid, y_train_plus_valid)\n", "intent": "Case -1 label based training data, Stack layer classifier is Decision Tree\n"}
{"snippet": "my_model = SuperLearnerClassifier()\nmy_model.fit(X_train, y_train)\n", "intent": "Train a Super Learner Classifier using the prepared dataset\n"}
{"snippet": "scaler.fit(bank.drop('Class', axis = 1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "gbCv = CrossValidator(estimator=gb, estimatorParamMaps=gbParamGrid, evaluator=evaluator, numFolds=2)\ngbCvModel = gbCv.fit(trainingData)\n", "intent": "1. Perform cross validation of params on your 'gb' model.\n1. Print out the best params you found.\n"}
{"snippet": "kmeans = KMeans(n_clusters = 2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "kmeans.fit(df.drop('Private', axis = 1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedings = tf.Variable(tf.random_uniform((vocab_size, embed_dim), name='embedings'))\n    embed = tf.nn.embedding_lookup(embedings, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "gs_cv = GridSearchCV(pipe, params, n_jobs=-1,\n                    cv=ShuffleSplit(n_splits=20, test_size=0.2, train_size=0.8))\ngs_cv.fit(x, y, groups)\n", "intent": "**Note:** We first use `ShuffleSplit`, which is supposed to used on i.i.d samples, to show how super accuracy is acquired.\n"}
{"snippet": "model_conv2 = tf.keras.models.Model(inputs=images, outputs=output)\nsgd_optimizer = tf.keras.optimizers.SGD(lr=0.01, momentum=0.99, decay=0.005, nesterov=True)\nmodel_conv2.compile(loss='sparse_categorical_crossentropy', optimizer=sgd_optimizer, metrics=['accuracy'])\n", "intent": "    - Define a batch generator\n    - Use it in the train process\n"}
{"snippet": "lrmodel = LinearRegression()\nlrmodel.fit(x, y)\n", "intent": "Using scikit-learn or statsmodels, build the necessary models for your scenario. Evaluate model fit.\n"}
{"snippet": "logreg = smf.logit(formula, data=su)\nlogreg_results = logreg.fit()\nprint logreg_results.summary()\n", "intent": "And print out the results as shown in the example.\n---\n"}
{"snippet": "x = tf.constant(5) + tf.constant(2)\ny = tf.constant(10) - tf.constant(4)\nz = tf.constant(2) * tf.constant(5)\nwith tf.Session() as sess:\n    print(sess.run([x, y, z]))\n", "intent": "You can also use the overloaded \\_\\_add\\_\\_(), \\_\\_sub\\_\\_(), etc functions to implement the operations.\n"}
{"snippet": "n_labels = 5\nbias = tf.Variable(tf.zeros(n_labels))\n", "intent": "The tf.zeros() function returns a tensor with all zeros.\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\ntype(knn)\n", "intent": "**Step 3:** \"Instantiate\" the \"estimator\"\n- \"Instantiate\" means \"make an instance of\"\n"}
{"snippet": "w = \nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n", "intent": "Q0. Create a variable w with an initial value of 1.0 and name weight. Then, print out the value of w.\n"}
{"snippet": "with tf.Session() as sess:\n    zeros = \n    print(sess.run(zeros))\n    assert np.allclose(sess.run(zeros), np.zeros([2, 3]))\n", "intent": "Q1. Create a tensor with shape [2, 3] with all elements set to zero.\n"}
{"snippet": "with tf.Session() as sess:\n    _X = np.array([[1,2,3], [4,5,6]])\n    X = \n    zeros = \n    print(sess.run(zeros))\n    assert np.allclose(sess.run(zeros), np.zeros_like(_X))\n", "intent": "Q2. Let X be a tensor of [[1,2,3], [4,5,6]]. \nCreate a tensor of the same shape and dtype as X with all elements set to one.\n"}
{"snippet": "with tf.Session() as sess:\n    out = \n    print(sess.run(out))\n    assert np.allclose(sess.run(out), np.array([[1, 3, 5], [4, 6, 8]], dtype=np.float32))\n", "intent": "Q3. Create a constant tensor of [[1, 3, 5], [4, 6, 8]].\n"}
{"snippet": "with tf.Session() as sess:\n    _X = np.array([[1, 2], [3, 4], [5, 6], [7,8]])\n    X = \n    out = \n    print(sess.run(out))\n", "intent": "Q6. Randomly shuffle the data in matrix X.\n"}
{"snippet": "with tf.Session() as sess:\n    x = tf.random_uniform([], -1, 1)\n    y = tf.random_uniform([], -1, 1)\n    out = \n    print(sess.run([x, y, out]))\n", "intent": "Q7. Let x and y be random 0-D tensors. Return x + y if x < y and x - y otherwise.\n"}
{"snippet": "sess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nwriter = tf.summary.FileWriter('logs/train', graph=tf.get_default_graph())\n", "intent": "Create a new interactive session and initialize all variables.\n"}
{"snippet": "y_pred = tf.nn.softmax(...)\n", "intent": "Apply softmax to the final fully connected layer.\n"}
{"snippet": "with tf.Session() as sess:\n    ??\n    best_theta_restored = ??\n", "intent": "Restoring the final model\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1e9)\nlogreg.fit(X_train, y_train)\nzip(feature_cols, logreg.coef_[0])\n", "intent": "Confirm that the coefficients make intuitive sense.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, \"./tmp/my_model_final.ckpt\")\n    best_theta_restored = theta.eval() \n", "intent": "Restoring the final model\n"}
{"snippet": "reset_graph()\nsaver = tf.train.import_meta_graph(\"./tmp/my_model_final.ckpt.meta\")  \ntheta = tf.get_default_graph().get_tensor_by_name(\"theta:0\") \nwith tf.Session() as sess:\n    saver.restore(sess, \"./tmp/my_model_final.ckpt\")  \n    best_theta_restored = theta.eval() \nbest_theta_restored\n", "intent": "Restoring a model without the graph\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=64,\n          epochs=10,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nllinreg = LinearRegression()\n", "intent": "Valitset mallin sovittamiseen `sklearn.linear_model`-moduulin lineaarisen regression funktion. Luot oman funktion taso- ja log-malleille.\n"}
{"snippet": "logreg.fit(Xtrain, Ytrain)\n", "intent": "Sovitetaan ensin malli harjoitusdatalla eli estimoidaan mallin (1) $ \\beta $-kertoimien arvot. \n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\ntree_clf.fit(X, y)\n", "intent": "Train DecisionTreeClassifier\n"}
{"snippet": "y3 = ?? \ntree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\ntree_reg3.??\n", "intent": "Fit a third DecisionTreeRegressor on the residual error\n"}
{"snippet": "gbrt_best = GradientBoostingRegressor(??)\ngbrt_best.fit(X_train, y_train)\n", "intent": "Train another GBRT using n_estimator as the `best_n_estimator`\n"}
{"snippet": "feature_cols = ['al']\nX = glass[feature_cols]\ny = glass.ri\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X, y)\n", "intent": "Let's create a linear regression model.  What are our 6 steps for creating a model?\n"}
{"snippet": "lin1 = linear_model.LinearRegression()\nlin2 = linear_model.Ridge(alpha=10**9.5)\n", "intent": "Select linear and ridge models\n"}
{"snippet": "from sklearn.linear_model import SGDRegressor\nsgd_reg = SGDRegressor(max_iter =50, penalty=None, eta0=0.1, random_state=42)\nsgd_reg.fit(X, y.ravel())\n", "intent": "Use SGDRegressor for stochastic gd\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg.intercept_, lin_reg.coef_\n", "intent": "Use LinearRegression from sklearn.linear_model\n"}
{"snippet": "lin_reg = LinearRegression()\nlin_reg.fit(X_poly_scaled, y)\n", "intent": "sklearn Linear Regression\n"}
{"snippet": "from sklearn.svm import SVC\nsvm_clf = SVC(kernel=\"linear\", C=float('inf'))\nsvm_clf.??(X, y)\n", "intent": "Find $w_0$, $w_1$ and $b$ in $w_0x_0 + w_1x_1+b=0$\n"}
{"snippet": "from sklearn.svm import SVC\nsvm_clf = SVC(kernel=\"linear\", C=float('inf'))\nsvm_clf.fit(X, y)\n", "intent": "Find $w_0$, $w_1$ and $b$ in $w_0x_0 + w_1x_1+b=0$\n"}
{"snippet": "tree_reg1 = ??\ntree_reg1.fit(X, y)\n", "intent": "Fit a first DecisionTreeRegressor to label(y)\n"}
{"snippet": "y2 = ?? \ntree_reg2 = ??\ntree_reg2.fit(X, ??)\n", "intent": "Train a second DecisionTreeRegressor on the residual error made by tree_reg1\n"}
{"snippet": "y3 = ?? \ntree_reg3 = ??\ntree_reg3.fit(X, ??)\n", "intent": "Train a third DecisionTreeRegressor on the residual error made by tree_reg2\n"}
{"snippet": "rbm = BernoulliRBM(random_state=0)\nrbm.fit(digits_X, digits_y,)\nbiases_for_visible = rbm.intercept_visible_\nprint biases_for_visible.shape\nprint biases_for_visible\n", "intent": "Manual page for scikit-learn neural network models (unsupervised): http://scikit-learn.org/stable/modules/neural_networks.html\n"}
{"snippet": "model = ARMA(store1_sales_data, (1, 1)).fit()\nprint model.summary()\n", "intent": "Becuase of the errors, it doesn't look like an AR model is good enough -- the data isn't stationary. So let's expand to an `ARMA` model.\n"}
{"snippet": "rf_best = RandomForestClassifier(criterion='gini', class_weight='balanced', n_jobs=-1, \n                                 n_estimators=rf_cv.best_params_['n_estimators'],\n                                 min_samples_split=rf_cv.best_params_['min_samples_split'], \n                                 max_depth=rf_cv.best_params_['max_depth'])\nrf_best.fit(x_train, y_train)\n", "intent": "With the best parameters, we can re-fit another RF object with those values. \n"}
{"snippet": "model.fit(x_train, y_train, epochs=10, batch_size=30, verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model.fit(x_train, \n          y_train, \n          epochs=50, \n          batch_size=50, \n          validation_data=(x_test, y_test),\n          verbose=2);\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "x = affair[['child','sex','religious','education','rate']].values\ny = affair['target'].values\nknn_n3 = KNeighborsClassifier(n_neighbors=3,weights='uniform')\nknn_n3.fit(x,y)\nprint 'Accuracy: ', knn_n3.score(x,y)\n", "intent": "---\nYou should choose **2 predictor variables** to predict had affair vs. not\n"}
{"snippet": "from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier()\nbest_ada = ada.fit(Xn,y)\n", "intent": "---\n1. Train a AdaBoost classifier on your chosen classification problem.\n- Evaluate the classifier performance with a 5-fold cross-validation.\n"}
{"snippet": "g2 = tf.Graph()\n", "intent": "By default, it grabs the default graph.  But we could have created a new graph like so:\n"}
{"snippet": "convolved = tf.nn.conv2d(img_4d, z_4d, strides=[1, 1, 1, 1], padding='SAME')\nres = convolved.eval()\nprint(res.shape)\n", "intent": "<a name=\"convolvefilter-an-image-using-a-gaussian-kernel\"></a>\nWe can now use our previous Gaussian Kernel to convolve our image:\n"}
{"snippet": "Y_pred = tf.Variable(tf.random_normal([1]), name='bias')\nfor pow_i in range(0, 5):\n    W = tf.Variable(\n        tf.random_normal([1], stddev=0.1), name='weight_%d' % pow_i)\n    Y_pred = tf.add(tf.mul(tf.pow(X, pow_i), W), Y_pred)\ntrain(X, Y, Y_pred)\n", "intent": "But we really don't want to add *too many* powers.  If we add just 1 more power:\n"}
{"snippet": "C = 1.0  \nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(iris_X, iris_y)  \nsvc = svm.SVC(kernel='linear', C=C).fit(iris_X, iris_y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(iris_X, iris_y)\n", "intent": "* Radial Bias Function (RBF)\n* Linear\n* Poly of degree 3\n"}
{"snippet": "h, W = utils.linear(\n    x=X, n_output=20, name='linear', activation=tf.nn.relu)\n", "intent": "You can now write the same process as the above steps by simply calling:\n"}
{"snippet": "sess = tf.Session()\nsess.run(tf.initialize_all_variables())\n", "intent": "Now we'll create a session to manage the training in minibatches:\n"}
{"snippet": "g = tf.get_default_graph()\n[op.name for op in g.get_operations()]\n", "intent": "And just to confirm, let's see what's in our graph:\n"}
{"snippet": "b_1 = tf.get_variable(\n    name='b',\n    shape=[n_filters_out],\n    initializer=tf.constant_initializer())\n", "intent": "Bias is always `[output_channels]` in size.\n"}
{"snippet": "h_3, W = utils.linear(h_2_flat, 128, activation=tf.nn.relu, name='fc_1')\n", "intent": "Create a fully-connected layer:\n"}
{"snippet": "sess = tf.Session()\nsess.run(tf.initialize_all_variables())\n", "intent": "And create a new session to actually perform the initialization of all the variables:\n"}
{"snippet": "[op.name for op in tf.get_default_graph().get_operations()]\n", "intent": "Let's take a look at the graph:\n"}
{"snippet": "def decode(z, dimensions, Ws, activation=tf.nn.tanh):\n    current_input = z\n    for layer_i, n_output in enumerate(dimensions):\n        with tf.variable_scope(\"decoder/layer/{}\".format(layer_i)):\n            W = tf.transpose(Ws[layer_i])\n            h = tf.matmul(current_input, W)\n            current_input = activation(h)\n            n_input = n_output\n    Y = current_input\n    return Y\n", "intent": "Now we'll build the decoder.  I've shown you how to do this.  Read through the code to fully understand what it is doing:\n"}
{"snippet": "g = tf.get_default_graph()\nnames = [op.name for op in g.get_operations()]\nprint(names)\n", "intent": "<TODO: visual of graph>\nLet's have a look at the graph:\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, Y_train)\n", "intent": "Train the model using LogisticRegression\n"}
{"snippet": "with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n    res = softmax.eval(feed_dict={x: img_4d})[0]\nprint([(res[idx], net['labels'][idx])\n       for idx in res.argsort()[-5:][::-1]])\n", "intent": "<a name=\"dropout\"></a>\nIf I run this again, I get a different result:\n"}
{"snippet": "with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n    res = softmax.eval(feed_dict={\n        x: img_4d,\n        'vgg/dropout_1/random_uniform:0': [[1.0]],\n        'vgg/dropout/random_uniform:0': [[1.0]]})[0]\nprint([(res[idx], net['labels'][idx])\n       for idx in res.argsort()[-5:][::-1]])\n", "intent": "Looking at the network, it looks like there are 2 dropout layers.  Let's set these values to 1 by telling the `feed_dict` parameter.\n"}
{"snippet": "with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n    res = softmax.eval(feed_dict={\n        x: img_4d,\n        'vgg/dropout_1/random_uniform:0': [[1.0]],\n        'vgg/dropout/random_uniform:0': [[1.0]]})[0]\nprint([(res[idx], net['labels'][idx])\n       for idx in res.argsort()[-5:][::-1]])\n", "intent": "Let's try again to be sure:\n"}
{"snippet": "with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n    res = softmax.eval(\n        feed_dict={\n            x: style_img_4d,\n            'vgg/dropout_1/random_uniform:0': [[1.0]],\n            'vgg/dropout/random_uniform:0': [[1.0]]})[0]\nprint([(res[idx], net['labels'][idx])\n       for idx in res.argsort()[-5:][::-1]])\n", "intent": "And for fun let's see what VGG thinks of it:\n"}
{"snippet": "with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n    loss = 0.1 * content_loss + 5.0 * style_loss + 0.01 * tv_loss\n    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n", "intent": "<a name=\"training\"></a>\nWith both content and style losses, we can combine the two, optimizing our loss function, and creating a stylized coffee cup.\n"}
{"snippet": "def plot_gradient(img, x, feature, g, device='/cpu:0'):\n    with tf.Session(graph=g) as sess, g.device(device):\n        saliency = tf.gradients(tf.reduce_mean(feature), x)\n        this_res = sess.run(saliency[0], feed_dict={x: img})\n        grad = this_res[0] / np.max(np.abs(this_res))\n        return grad\n", "intent": "We'll now try to find the gradient activation that maximizes a layer with respect to the input layer `x`.\n"}
{"snippet": "for feature_i in range(len(features)):\n    with tf.Session(graph=g) as sess, g.device(device):\n        layer = ...\n        gradient = ...\n        dream(...)\n", "intent": "We'll do the same thing as before, now w/ our noise image:\n<h3><font color='red'>TODO! COMPLETE THIS SECTION!</font></h3>\n"}
{"snippet": "with tf.Session(graph=g) as sess, g.device(device):\n    tf.import_graph_def(net['graph_def'], name='net')\n", "intent": "Let's now import the graph definition into our newly created Graph using a context manager and specifying that we want to use the CPU.\n"}
{"snippet": "x = ...\nsoftmax = ...\nfor img in [content_img, style_img]:\n    with tf.Session(graph=g) as sess, g.device('/cpu:0'):\n        res = softmax.eval(feed_dict={x: img,\n                    'net/dropout_1/random_uniform:0': [[1.0]],\n                    'net/dropout/random_uniform:0': [[1.0]]})[0]\n        print([(res[idx], net['labels'][idx])\n               for idx in res.argsort()[-5:][::-1]])\n", "intent": "Let's see what the network classifies these images as just for fun:\n<h3><font color='red'>TODO! COMPLETE THIS SECTION!</font></h3>\n"}
{"snippet": "clf = tree.DecisionTreeClassifier(max_depth=3)\nclf.fit(X, y)\n", "intent": "* Now you get to build your decision tree classifier! First create such a model with `max_depth=3` and then fit it your data:\n"}
{"snippet": "graph = tf.get_default_graph()\nnb_utils.show_graph(graph.as_graph_def())\n", "intent": "And we can see what the network looks like now:\n"}
{"snippet": "d_reg = tf.contrib.layers.apply_regularization(\n    tf.contrib.layers.l2_regularizer(1e-6), vars_d)\ng_reg = tf.contrib.layers.apply_regularization(\n    tf.contrib.layers.l2_regularizer(1e-6), vars_g)\n", "intent": "We can also apply regularization to our network.  This will penalize weights in the network for growing too large.\n"}
{"snippet": "sess = tf.Session()\ninit_op = tf.initialize_all_variables()\nsaver = tf.train.Saver()\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\nsess.run(init_op)\n", "intent": "Now create a session and create a coordinator to manage our queues for fetching data from the input pipeline and start our queue runners:\n"}
{"snippet": "nb_utils.show_graph(tf.get_default_graph().as_graph_def())\n", "intent": "Let's take a look at the graph:\n"}
{"snippet": "nb_utils.show_graph(g.as_graph_def())\n", "intent": "Let's now take a look at the model:\n"}
{"snippet": "k = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(dn)\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\nprint labels\nprint  \nprint centroids\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "db = DBSCAN(eps=0.5, min_samples=5,random_state=5).fit(X)\n", "intent": "Let's set up the DBSCAN from scikit using an epsilon of .5 and a minimum sample number of 5\n"}
{"snippet": "regr = LogisticRegressionCV(n_jobs=-1)\nregr.fit(rez, y)\n", "intent": "Perform cross-validation with logistic regression.\n"}
{"snippet": "a, b = 1.5, 2.5\ne_out = graph(a,b)\nprint e_out\n", "intent": "Now, we can call this function to execute the computation graph given some inputs `a,b`:\n"}
{"snippet": "def make_nn_layer(input, output_size):\n  input_size = input.get_shape().as_list()[1]\n  weights = tf.Variable(tf.truncated_normal(\n      [input_size, output_size],\n      stddev=1.0 / math.sqrt(float(input_size))))\n  biases = tf.Variable(tf.zeros([output_size]))\n  return tf.matmul(input, weights) + biases\n", "intent": "We start with a helper function to build a simple TensorFlow neural network layer.\n"}
{"snippet": "vae = tf.keras.Model(inputs, reconstructed_inputs)\n", "intent": "Finally we can construct our network end-to-end.\n"}
{"snippet": "util.display_model(encoder)\n", "intent": "Let's visualize the architecture of the encoder to get a more concrete understanding of this network,\n"}
{"snippet": "def train_step(model, optimizer, observations, actions, discounted_rewards):\n  with tf.GradientTape() as tape:\n      observations = tf.convert_to_tensor(observations, dtype=tf.float32)\n      logits = model(observations)\n", "intent": "Now let's use the loss function to define a backpropogation step of our learning algorithm.\n"}
{"snippet": "def create_pong_model():\n  model = tf.keras.models.Sequential([\n      tf.keras.layers.InputLayer(input_shape=(6400,), dtype=tf.float32),\n      tf.keras.layers.Reshape((80, 80, 1)),\n      tf.keras.layers.Conv2D(filters=16, kernel_size=(8,8), strides=(4,4), activation='relu', padding='same'),\n  ])\n  return model\npong_model = create_pong_model()\n", "intent": "We'll define our agent again, but this time, we'll add convolutional layers to the network to increase the learning capacity of our network.\n"}
{"snippet": "save_video_of_model(pong_model, \"Pong-v0\", filename='pong_agent.mp4')  \n", "intent": "We can now save the video of our model learning:\n"}
{"snippet": "def train_step(model, optimizer, observations, actions, discounted_rewards):\n  with tf.GradientTape() as tape:\n      observations = tf.convert_to_tensor(observations, dtype=tf.float32)\n      logits = model(observations)\n  grads = tape.gradient(loss, model.variables) \n  optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n", "intent": "Now let's use the loss function to define a backpropogation step of our learning algorithm.\n"}
{"snippet": "module = LogisticRegression()\nvector = torch.randn(10)\noutput = module(vector)\n", "intent": "We can now create a random vector and pass it through the module:\n"}
{"snippet": "from sklearn import svm, grid_search\ndef svc_param_selection(X, y, nfolds):\n    Cs = [...]\n    gammas = [...]\n    param_grid = {'C': Cs, 'gamma' : gammas}\n    search = grid_search.GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n    search.fit(X, y)\n    search.best_params_\n    return search.best_params_\n", "intent": "TODO: Fill in the following function to use grid search to find the optimal value of C and gamma (hyperparameter of the RBF kernel):\n"}
{"snippet": "svc = svm.SVC(C=..., gamma=..., probability=True)\nsvc\n", "intent": "TODO: Use your optimal values of C and gamma to train the SVM:\n"}
{"snippet": "history = model.fit(X_train, y_train, epochs=10,\n                    validation_data=(X_valid, y_valid))\n", "intent": "1.7) Try running `model.fit()` again, and notice that training continues where it left off.\n"}
{"snippet": "class EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        features = self.resnet(images)\n        features = features.reshape(features.size(0), -1)\n        features = self.bn(self.linear(features))\n        return features\n", "intent": "Now we can implement the encoder and decoder networks!\n"}
{"snippet": "reg = linear_model.LinearRegression()\nreg.normalize=True\nreg.fit (x_train_sc, y_train_sc)\ntr_coeffs = reg.coef_\ntr_resi = reg.residues_\ntr_intr = reg.intercept_\nprint(\"Coeffs: \", tr_coeffs)\nprint(\"Residuals: \", tr_resi)\nprint(\"Intercept: \", tr_intr)\n", "intent": "FINDING MODEL COEFFICIENTS\n"}
{"snippet": "reg = linear_model.LinearRegression()\nreg = reg.fit(x_train_sc, y_train_sc)\ntr_coeffs = reg.coef_\ntr_resi = reg.residues_\ntr_intr = reg.intercept_\nprint(\"Coeffs: \", tr_coeffs)\nprint(\"Residuals: \", tr_resi)\nprint(\"Intercept: \", tr_intr)\n", "intent": "FINDING MODEL COEFFICIENTS\n"}
{"snippet": "clf.fit(x,y)\n", "intent": "With all that established, all we have to do is train the classifer,\n"}
{"snippet": "i,j = np.where(M==0)\nx=np.vstack([i,j]).T\ny = j.reshape(-1,1)*0\ni,j = np.where(M==1)\nx=np.vstack([np.vstack([i,j]).T,x])\ny = np.vstack([j.reshape(-1,1)*0+1,y])\nclf.fit(x,y)\n", "intent": "Now we have a `1` entry in the previously\npure first column's second  row.\n"}
{"snippet": "lr = LogisticRegression()\n_=lr.fit(X[:,:-1],c)\n", "intent": "This establishes the training data.  The next block\ncreates the logistic regression object and fits the data.\n"}
{"snippet": "x = np.linspace(-1,1,30)\nX = np.c_[x,x+1,x+2] \npca.fit(X)\nprint(pca.explained_variance_ratio_)\n", "intent": "Let's create some very simple data and apply PCA.\n"}
{"snippet": "from sklearn.neighbors import KNeighborsRegressor\nknr=KNeighborsRegressor(2) \nknr.fit(xin,y)\n", "intent": " We can use this data to construct a simple nearest neighbor\nestimator using Scikit-learn,\n"}
{"snippet": "dtr = DecisionTreeRegressor(max_depth=4,\n                            min_samples_split=5,\n                            max_leaf_nodes=10)\ndtr.fit(X,y)\n", "intent": "<h1> Sample Decision Tree Regressor\n"}
{"snippet": "model = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n    keras.layers.Dense(1)\n])\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n", "intent": "5.4) Now build, train and evaluate a neural network to tackle this problem. Then use it to make predictions on the test set.\n"}
{"snippet": "def show_kdtree(kdtree,depth):\n    if kdtree is None:\n        print 'None'\n        return None\n    if depth == 0:\n        print 'nodeid=',kdtree[0],':',kdtree[1]\n        return None\n    for j in [2,3]:\n        show_kdtree(kdtree[j],depth-1)        \n", "intent": "I implemented the function ``show_kdtree``, that allows us to have a look at the nodes of the generated k-d tree for a given depth.\n"}
{"snippet": "show_kdtree(kdtree,4)\n", "intent": "Going further down the tree...\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1000)) \nmodel.add(Activation(\"relu\")) \nmodel.add(Dropout(0.5)) \nmodel.add(Dense(num_classes))\nmodel.add(Activation(\"softmax\")) \nmodel.summary()\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) \n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "linear_svc = svm.SVC(kernel='linear')\nlinear_svc.kernel\n", "intent": "<h2> Kernel functions\n"}
{"snippet": "from glove import Corpus, Glove\ncorpus = Corpus()\ncorpus.fit(sentences_list, window=5)\nglove = Glove(no_components=100, learning_rate=0.05)\nglove.fit(corpus.matrix, epochs=100, no_threads=4, verbose=False)\nglove.add_dictionary(corpus.dictionary)\n", "intent": "pip install glove-python\n"}
{"snippet": "from keras import models\nfrom keras import layers\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n", "intent": "Here's what our network looks like:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    print(\"vocab_size: \", vocab_size)\n    print(\"embed_dim: \", embed_dim)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(penalty = 'l1')\n", "intent": "Then we used those features to build a classification model\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1)) \n    embed = tf.nn.embedding_lookup(embedding, input_data) \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n    keras.layers.Dense(1),\n])\n", "intent": "2) Compile the following model, passing it your custom loss function, then train it and evaluate it. **Tip**: don't forget to use the scaled sets.\n"}
{"snippet": "happyModel.fit(X_train, Y_train, epochs=40, batch_size=64)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=100, validation_split=0.2, batch_size=128, verbose=0,\n         callbacks=[EarlyStopping(patience=10)])\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    embed_layer = tf.nn.embedding_lookup(embedding, input_data)\n    return embed_layer\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "lam = 3.0; x_train = np.random.exponential(1/lam, size=10) \n", "intent": "**Part D**: Use the formula you found in **Part C** to estimate the rate parameter $\\lambda$ for the following training data.  \n"}
{"snippet": "knn = KNeighborsClassifier(25).fit(X_train, y_train)\nplot_knn_boundary(X_train, y_train, knn)\n", "intent": "**Part C**: Play with the value of $K$ above.  How does the character of the decision boundary change with $K$? \n"}
{"snippet": "new_perc = Perceptron(max_iter=5, alpha=0.0, shuffle=True)\nnew_perc.fit(Xnew, ynew)\nxplot = np.linspace(-1.5,1.5,20)\nw, b = new_perc.coef_[0], perc.intercept_[0]\nyplot = -(b + w[0]*xplot)/w[1] \nfig, ax = data_plot(scatter=[(Xnew, ynew)])\nax.plot(xplot, yplot, lw=3, color=\"black\");\n", "intent": "Shuffling the data gives a much better decision boundary, likely just because we didn't encounter the misclassified points at the end of the epoch. \n"}
{"snippet": "bagofwords_pipe.fit(text_train, labels_train)\n", "intent": "We can fit our model to the training data using the `.fit()` method. \n"}
{"snippet": "clf.fit(vectorized_count_train_data, train_target)\n", "intent": "Fit it to the vectorized training data and training labels\n"}
{"snippet": "lm = LinearRegression(normalize=True)\nlm.fit(X_train, y_train)\n", "intent": "<font color='red'>TASK MARK: 2</font>\n<br>\n<font color='green'>COMMENT:  - </font>\n"}
{"snippet": "model = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n    keras.layers.Dense(1),\n])\n", "intent": "2) Compile your model, passing it your custom loss function, then train it and evaluate it. **Tip**: don't forget to use the scaled sets.\n"}
{"snippet": "from sklearn import linear_model\nmodels = {}\nfor k in Phis_tr:\n    model = linear_model.LinearRegression()\n    model.fit(Phis_tr[k], Y_tr)\n    models[k] = model\n", "intent": "Train a model for each number of feature\n"}
{"snippet": "one_hot_features.todense()[:5,:]\n", "intent": "Examining a few rows we see there are multiple one hot encodings (one for flavor and one for toppings).\n"}
{"snippet": "Phi.todense()[:5,:]\n", "intent": "Again let's look at a few examples (in practice you would want to avoid the `todense()` call\n"}
{"snippet": "line_reg = linear_model.LinearRegression(fit_intercept=True)\nline_reg.fit(train_data[['X']], train_data['Y'])\n", "intent": "The following block of code creates an instance of the Least Squares Linear Regression model and the fits that model to the training data.  \n"}
{"snippet": "sin_reg = linear_model.LinearRegression(fit_intercept=False)\nsin_reg.fit(Phi, train_data['Y'])\n", "intent": "We can again use the scikit-learn package to fit a linear model on the transformed space.\n"}
{"snippet": "(theta0,theta1) = np.meshgrid(np.linspace(0,3,20), np.linspace(1,5,20))\ntheta_values = np.vstack((theta0.flatten(), theta1.flatten())).T\n", "intent": "Generate all combinations of $\\theta_0$ and $\\theta_1$ on a 20 by 20 grid.\n"}
{"snippet": "import sklearn.linear_model as linear_model\nleast_squares_model = linear_model.LinearRegression()\nleast_squares_model.fit(X,Y)\n", "intent": "Fit a least squares regression model.\n"}
{"snippet": "def neg_gradL(theta, X, Y):\n    return -(X.T @ (Y - sigmoid(X @ theta)))\n", "intent": "In the following we implement the gradient function for a dataset X and Y.\n"}
{"snippet": "lr = linear_model.LogisticRegressionCV(100)\nlr.fit(X,Y)\n", "intent": "We will use the built in cross-validation support for logistic regression in scikit-learn.\n"}
{"snippet": "def scaled_elu(z, scale=1.0, alpha=1.0):\n    is_positive = tf.greater_equal(z, 0.0)\n    return scale * tf.where(is_positive, z, alpha * tf.nn.elu(z))\n", "intent": "1) Examine and run the following code examples.\n"}
{"snippet": "lam_values = np.logspace(-1.3,2.5,20)\nmodels = []\nfor lam in lam_values:\n    model = linear_model.Lasso(alpha = lam, max_iter=100000)\n    model.fit(Phi, Y_tr)\n    models.append(model)\n", "intent": "In the following we use the scikit-learn Lasso package.  As before we will try a range of values for the regularization parameter. \n"}
{"snippet": "from keras import optimizers\nmodel = Sequential()\nmodel.add(Dense(512,activation='relu',input_shape=(num_words,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes,activation='sigmoid'))          \nmodel.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, batch_size=100, verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "classifier = MLPClassifier(random_state=seed)\n", "intent": "Read the [Docs](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB \nmnb = MultinomialNB()\nmnb.fit(X_train,Y_train)\nmnb.score(X_validation,Y_validation)\n", "intent": "1. Multinomial Naive Bayes\n2. SVM\n3. XgBoost\n4. Keras Neural Net\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim),-1,1))\n    embed_input = tf.nn.embedding_lookup(embedding, input_data)\n    return embed_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nmodel.add(Flatten(input_shape=(28,28)))\nmodel.add(Dense(10, activation='softmax') )\nsgd=SGD(lr=0.2, momentum=0.0, decay=0.0)\nmodel.compile(optimizer='sgd',\n      loss='categorical_crossentropy',\n      metrics=['accuracy'])\n", "intent": "Construct the model:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation= 'relu',input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train_e, y_train, batch_size=32, epochs=20)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = keras.models.Sequential([\n    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n    keras.layers.Dense(1),\n])\n", "intent": "8) Build and compile a Keras model for this regression task, and use your datasets to train it, evaluate it and make predictions for the test set.\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: test_x,\n            labels_: test_y}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc ))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], minval = -1, maxval=1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(1024, activation='sigmoid', input_shape=(1000,)))\nmodel.add(Dropout(.4))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal([vocab_size, embed_dim], stddev=0.01))\n    embedded = tf.nn.embedding_lookup(embedding, input_data) \n    return embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n", "intent": "Build the `tf.keras` model by stacking layers. Select an optimizer and loss function used for training:\n"}
{"snippet": "x = tf.ones((2, 2))\nwith tf.GradientTape() as t:\n  t.watch(x)\n  y = tf.reduce_sum(x)\n  z = tf.multiply(y, y)\ndz_dy = t.gradient(z, y)\nassert dz_dy.numpy() == 8.0\n", "intent": "You can also request gradients of the output with respect to intermediate values computed during a \"recorded\" `tf.GradientTape` context.\n"}
{"snippet": "class Model(object):\n  def __init__(self):\n    self.W = tf.Variable(5.0)\n    self.b = tf.Variable(0.0)\n  def __call__(self, x):\n    return self.W * x + self.b\nmodel = Model()\nassert model(3.0).numpy() == 15.0\n", "intent": "Let's define a simple class to encapsulate the variables and the computation.\n"}
{"snippet": "predictions = model(features)\npredictions[:5]\n", "intent": "Let's have a quick look at what this model does to a batch of features:\n"}
{"snippet": "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\nglobal_step = tf.Variable(0)\n", "intent": "Let's setup the optimizer and the `global_step` counter:\n"}
{"snippet": "tf.reset_default_graph()\nX = tf.placeholder(tf.float32, [None, N_TIME_STEPS, N_FEATURES], name=\"input\")\nY = tf.placeholder(tf.float32, [None, N_CLASSES])\n", "intent": "Now, let create placeholders for our model:\n"}
{"snippet": "test_accuracy = tfe.metrics.Accuracy()\nfor (x, y) in test_dataset:\n  logits = model(x)\n  prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n  test_accuracy(prediction, y)\nprint(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))\n", "intent": "Unlike the training stage, the model only evaluates a single [epoch](https://developers.google.com/machine-learning/glossary/\n"}
{"snippet": "mobilenet_layer = layers.Lambda(hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/2\"))\nmobilenet_classifier = tf.keras.Sequential([mobilenet_layer])\n", "intent": "Use `hub.module` to load a mobilenet, and `tf.keras.layers.Lambda` to wrap it up as a keras layer.\n"}
{"snippet": "mobilenet_features_module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/2\")\nmobilenet_features = layers.Lambda(mobilenet_features_module)\nmobilenet_features.trainable = False\n", "intent": "TensorFlow Hub also distributes models without the top classification layer. These can be used to easily do transfer learning.\n"}
{"snippet": "model = tf.keras.Sequential([\n  mobilenet_features,\n  layers.Dense(image_data.num_classes, activation='softmax')\n])\n", "intent": "Now wrap the hub layer in a `tf.keras.Sequential` model, and add a new classification layer.\n"}
{"snippet": "class PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs):\n    if epoch % 100 == 0: print('')\n    print('.', end='')\nEPOCHS = 1000\nhistory = model.fit(\n  normed_train_data, train_labels,\n  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n  callbacks=[PrintDot()])\n", "intent": "Train the model for 1000 epochs, and record the training and validation accuracy in the `history` object.\n"}
{"snippet": "smaller_model = keras.Sequential([\n    keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n    keras.layers.Dense(4, activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\nsmaller_model.compile(optimizer='adam',\n                loss='binary_crossentropy',\n                metrics=['accuracy', 'binary_crossentropy'])\nsmaller_model.summary()\n", "intent": "Let's create a model with less hidden units to compare against the baseline model that we just created:\n"}
{"snippet": "model = create_model()\nmodel.fit(train_images, train_labels, epochs=5)\n", "intent": "Build a fresh model:\n"}
{"snippet": "saved_model_path = tf.contrib.saved_model.save_keras_model(model, \"./saved_models\")\n", "intent": "Create a `saved_model`: \n"}
{"snippet": "new_model = tf.contrib.saved_model.load_keras_model(saved_model_path)\nnew_model\n", "intent": "Reload a fresh keras model from the saved model.\n"}
{"snippet": "pred_Y = create_LSTM_model(X)\npred_softmax = tf.nn.softmax(pred_Y, name=\"y_\")\n", "intent": "Note that we named the input tensor, that will be useful when using the model from Android. Creating the model:\n"}
{"snippet": "if tf.test.is_gpu_available():\n  rnn = tf.keras.layers.CuDNNGRU\nelse:\n  import functools\n  rnn = functools.partial(\n    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n", "intent": "Next define a function to build the model.\nUse `CuDNNGRU` if running on GPU.  \n"}
{"snippet": "for input_example_batch, target_example_batch in dataset.take(1): \n  example_batch_predictions = model(input_example_batch)\n  print(example_batch_predictions.shape, \"\n", "intent": "Now run the model to see that it behaves as expected.\nFirst check the shape of the output:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(500, input_dim=1000, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\nmodel.summary()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test), verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "feature = one_hot_train.drop('Survived', axis=1)\ntarget = one_hot_train['Survived']\nrf = RandomForestClassifier(random_state=1, criterion='gini', max_depth=10, n_estimators=50, n_jobs=-1)\nrf.fit(feature, target)\n", "intent": "We are going to split the data into features and targer, create the model and verify the the score\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(8, activation='sigmoid', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(4, activation='sigmoid'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\noptimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.001)\nmodel.compile(loss = 'categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=100, batch_size=100, verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "lm = sm.OLS(y, X).fit()\nlm.summary()\n", "intent": "Fit a simple linear regression model without validating the assumptions just in order to get some inights on the assumptions that we need to check.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    Embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    Embedded = tf.nn.embedding_lookup(Embedding, input_data)\n    return Embedded\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "pickle.dump(predictions, open(\"predictions.p\", \"wb\"))\npickle.dump(history, open(\"history.p\", \"wb\"))\ntf.train.write_graph(sess.graph_def, '.', './checkpoint/har.pbtxt')  \nsaver.save(sess, save_path = \"./checkpoint/har.ckpt\")\nsess.close()\n", "intent": "Whew, that was a lot of training. Do you feel thirsty? Let's store our precious model to disk:\n"}
{"snippet": "LogisticRegression().fit(X_train, y_train).score(X_test, y_test)\n", "intent": "Shorter, maybe less readible.\n"}
{"snippet": "pca.fit(X)\n", "intent": "2) Fit to training data\n"}
{"snippet": "param_grid = {'n_neighbors': np.arange(1, 10),\n             'weights': ['uniform', 'distance']}\nfrom sklearn.neighbors import KNeighborsClassifier\ngrid_search = GridSearchCV(KNeighborsClassifier(), param_grid, verbose=3, cv=5, n_jobs=23)\ngrid_search.fit(X_train, y=y_train)\n", "intent": "Use GridSearchCV to adjust n_neighbors of KNeighborsClassifier.\n"}
{"snippet": "keepList = dfFeatureImportances.index.tolist()\nforest = RandomForestClassifier(n_estimators=10)\nxTrain = dfInput[keepList].iloc[0:-500] \nyTrain = dfOutput.iloc[0:-500]\nxTest = dfInput[keepList].iloc[-500:].copy()\nyTest = dfOutput.iloc[-500:].copy()\nforest.fit(xTrain,yTrain)\n", "intent": "Retrain our model using the fetures which we determine were most relevent.\nThis time, we'll train on fewer features to prevent over-fitting.\n"}
{"snippet": "X = iris[[\"petal_length\"]] \ny = iris[\"petal_width\"] \nmodel = lm.LinearRegression() \nresults = model.fit(X, y) \nprint (results.intercept_, results.coef_)\n", "intent": "Now let's use scikit-learn to find the best fit line.\n"}
{"snippet": "model = smf.ols(formula = 'petal_width ~ petal_length + versicolor + virginica', data = iris)\nresults = model.fit()\nresults.summary()\n", "intent": "Now we perform a multilinear regression with the dummy variables added.\n"}
{"snippet": "lm = smf.ols(formula='sales ~ TV + radio + newspaper', data=data).fit()\nlm.summary()\n", "intent": "syntax can be found here: http://statsmodels.sourceforge.net/devel/example_formulas.html\n"}
{"snippet": "import statsmodels.formula.api as sm\nfrom scipy import stats\nstats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\nimport statsmodels.formula.api as smf\nresult = smf.logit('label ~ is_news', data=df)\nresult = result.fit()\nresult.summary()\n", "intent": "The `sm.logit` function from `statsmodels.formula.api` will perform a logistic regression using a formula string.\n"}
{"snippet": "vr_by_f1_features_to_test = []\nvr_by_f1_features_test_results = {}\nfor feature in tqdm(vr_by_f1_performant_features):\n    vr_by_f1_features_to_test.append(feature)\n    vr_by_f1_features_test_results[feature] = run_model(LogisticRegression(), 'logit', 100,\n                                                        adult_train_df[vr_by_f1_features_to_test],\n                                                        adult_train_target)\n", "intent": "Add one feature at a time.\n"}
{"snippet": "from xgboost import XGBClassifier\nmodel = XGBClassifier()\nmodel.fit(train, y_train)\nget_model_results(model, train, test, y_train, y_test)\n", "intent": "https://github.com/dmlc/xgboost\n"}
{"snippet": "SVC_linear = LinearSVC()\nSVC_linear.fit(X_train,y_train)\nprint(\"train acc:\",SVC_linear.score(X_train,y_train))\nprint(\"test acc:\",SVC_linear.score(X_test,y_test))\n", "intent": "Train a [`LinearSVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n"}
{"snippet": "mask = (df[\"dg\"] == \"doctorate\")\nXdg = df[mask][[u'yr', u'yd',\n       u'rk_assistant', u'rk_associate', u'rk_full', u'dg_doctorate',\n       u'dg_masters',u'sx_male',u'sx_female']]\nydg = df[mask][u'sl']\nlm = linear_model.LinearRegression()\nmodel = lm.fit(Xdg,ydg)\nprint \"Doctorates\", lm.score(Xdg,ydg)\n", "intent": "Did regularization improve the second fit?\nNow let's move on to the next category, \"dg\" (degree).\n"}
{"snippet": "neigh = KNeighborsClassifier(n_neighbors=10)\nneigh.fit(X,y)\n", "intent": "Build a logit model and fit\n"}
{"snippet": "k = 3\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(matrix)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "k = 2\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(matrix)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "k  = 2\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(df.loc[:,[\"age\",\"income\"]])\n", "intent": "Cluster the Data to our our target groups. **BONUS**: Perform multiple clustering tests on various indicator variables\n"}
{"snippet": "k = 3\nfirstDimension = iris.feature_names[0]\nsecondDimension = iris.feature_names[1]\nmatrix = df.loc[:,[firstDimension,secondDimension]]\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(matrix)\n", "intent": "Run the clustering analysis using scikit-learn.\n*Hint*: Estimate the number of clusters, k, based on your visual examination of the distributions.\n"}
{"snippet": "dbscan = DBSCAN(eps=.5,min_samples=5)\ndb = dbscan.fit(X)\ndb\n", "intent": "Let's set up the DBSCAN from scikit using an epsilon of .5 and a minimum sample number of 5\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(784,)))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])\n", "intent": "We build simple model with two hidden layers to classify digits.\n"}
{"snippet": "transformations = transforms.Compose([transforms.Scale(32),transforms.ToTensor()])\n", "intent": "- Converts: a PIL.Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n"}
{"snippet": "m1 = ols('PRICE ~ CRIM + RM + PTRATIO',bos).fit()\nprint(m1.summary())\n", "intent": "1. 'CRIM' (per capita crime rate by town)\n2. 'RM' (average number of rooms per dwelling)\n3. 'PTRATIO' (pupil-teacher ratio by town)\n"}
{"snippet": "model = Sequential()\nmodel.add(Embedding(len(word2num), 50)) \nmodel.add(GRU(64))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nmodel.summary()\n", "intent": "This method is no different to the method utilised in the sentiment analysis lesson.\n"}
{"snippet": "batch_size = 128\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=5, validation_data=(X_test, y_test))\n", "intent": "I may heave cheated and run the following block 3 times. Good thing about Keras is that it remembers the last learning rate and goes from there.\n"}
{"snippet": "m_lin = \nX = \nm_lin.fit(...)\n", "intent": "The first thing we would like to do is to fit a linear model. As dependent variable, we take charges, as independent variables bmi, age and children.\n"}
{"snippet": "m_tree.fit(...)\n", "intent": "Now fit the same model on the train set.\n"}
{"snippet": "m_lin = linear_model.LinearRegression()\nX = sample[['bmi', 'age', 'children']]\nm_lin.fit(X=X, y=sample['charges'])\n", "intent": "The first thing we would like to do is to fit a linear model. As dependent variable, we take charges, as independent variables bmi, age and children.\n"}
{"snippet": "m_lin = linear_model.LinearRegression()\nm_lin.fit(X=X_train, y=y_train)\n", "intent": "Fit a linear model for charges on bmi, age and smoker, using the training set.\n"}
{"snippet": "m_tree = DecisionTreeRegressor(max_leaf_nodes = 3, random_state = 2017)\nm_tree.fit(X=X, y=sample['charges'])\n", "intent": "Fit a CART regression tree for charges on bmi, age and smoker on the whole dataset. To keep the tree parsimonious, limit the number of leafs to 3.\n"}
{"snippet": "m_tree.fit(X=X_train, y=y_train)\n", "intent": "Now fit the same model on the train set.\n"}
{"snippet": "def dsigmoid(y):\n    return y - y**2\n", "intent": "Note: We need this when we run the backpropagation algorithm\n"}
{"snippet": "m_svm.fit(X=X_train, y=y_train)\n", "intent": "Train on the train data.\n"}
{"snippet": "m_nn = MLPRegressor(hidden_layer_sizes = (5,2), max_iter = 10000, random_state = 2017)\nm_nn.fit(X=X, y=sample['charges'])\n", "intent": "Fit a 2-layer - with 5 nodes in first and 2 nodes is second layer - perceptron with _relu_ as the activation function for hidden layers.\n"}
{"snippet": "m_nn.fit(X=X_train, y=y_train)\n", "intent": "Train on the train data.\n"}
{"snippet": "m_rf.fit(X=X_train, y=y_train)\n", "intent": "Fit the model on the train set.\n"}
{"snippet": "m_km = KNeighborsClassifier(n_neighbors=5, weights='distance')\nm_km.fit(X=X_train, y=y_train)\n", "intent": "Predict the diagnosis from the test set based on 5 points using euclidean distance metric.\n"}
{"snippet": "m_nb = GaussianNB()\nm_nb.fit(X=X_train, y=y_train)\n", "intent": "Train a model based on Naive Bayes with the train data, and show the model parameters.\n"}
{"snippet": "m_logit = LogisticRegression()\nm_logit.fit(X=X_train, y=y_train)\n", "intent": "Train a logistic model with the train data.\n"}
{"snippet": "m_tree = DecisionTreeClassifier(max_leaf_nodes = 5, random_state = 2017)\nm_tree.fit(X=X_train, y=y_train)\n", "intent": "Train a logistic model with the train data. Limit the number of leafs to 5.\n"}
{"snippet": "m_nn = MLPClassifier(hidden_layer_sizes = (5,2), max_iter = 10000, random_state = 2017)\nm_nn.fit(X=X_train, y=y_train)\n", "intent": "Fit a 2-layer - with 5 nodes in first and 2 nodes is second layer - perceptron with relu as the activation function for hidden layers.\n"}
{"snippet": "x = tf.placeholder(tf.float32)\ny = tf.log(x)   \nvar_grad = tf.gradients(y, x)\nwith tf.Session() as session:\n    var_grad_val = session.run(var_grad, feed_dict={x:2})\n    print(var_grad_val)\n", "intent": "- Gradients are free!\n"}
{"snippet": "m_dbs = DBSCAN(eps=0.25, min_samples=5).fit(dfn)\n", "intent": "Another approach is density based clustering. Perform dbscan on the normalized data, with max distance 0.25.\n"}
{"snippet": "m_ar = SARIMAX(tslog, trend='ct', order=[1,0,0], seasonal_order=[1,0,1,12],\n               enforce_stationarity=False, enforce_invertibility=False\n              ).fit(maxiter=5000, method='nm')\nprint(m_ar.summary())\n", "intent": "Fit the best model and display its summary.\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\naccuracy = classifier.score(x_test, y_test)\nprint (\"Naive Bayes Classifier accuracy: \", (100* accuracy), \"%.\")\n", "intent": "Now we build and test the classifier.\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\naccuracy = classifier.score(x_test, y_test)\nprint (\"Naive Bayes Classifier accuracy: \", (100* accuracy), \"%.\")\n", "intent": "Now train and test as before.\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\naccuracy = classifier.score(x_test, y_test)\nprint (\"Naive Bayes Classifier accuracy: \", (100* accuracy), \"%.\")\n", "intent": "Now with the data sets created, lets train then test as before.\n"}
{"snippet": "knn = KNeighborsClassifier()\nknn.fit(train_data,train_labels)\n", "intent": "* Scikit KNN\nhttp://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n"}
{"snippet": "nb = GaussianNB()\nnb.fit(train_data,train_labels)\n", "intent": "* Scikit Naive Bayes\nhttp://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n"}
{"snippet": "mlp =  MLPClassifier(solver='sgd',hidden_layer_sizes=(5, 1))\nmlp.fit(train_data,train_labels)\n", "intent": "* Scikit MLP\nhttps://en.wikipedia.org/wiki/Multilayer_perceptron\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\naccuracy = classifier.score(x_test, y_test)\nprint (\"Naive Bayes Classifier accuracy: \", (100* accuracy), \"%.\")\n", "intent": "Now we build and test the classifier. We'll just use a basic calssfier here to keep things simple.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\nmodel.summary()\n", "intent": "So, how hard can it be to build a Multi-Layer percepton with keras?\nIt is baiscly the same, just add more layers!\n"}
{"snippet": "clf.fit(X_train, y_train)\n", "intent": "This is the function that actually trains the classifier with our training data. \n"}
{"snippet": "clf = SVC(kernel='rbf', probability=True)\nclf.fit(X_train, y_train)\n", "intent": "Scikit-learn is designed to let you easily swap algorithms out\n"}
{"snippet": "svm = LinearSVC()\n", "intent": "1) Instantiate an object and set the parameters\n"}
{"snippet": "model = SVC(kernel='linear')\nmodel.fit(X, y)\n", "intent": "Scikit-learn implements support vector machine models in the svm package.\n"}
{"snippet": "net = Sequential()\nnet.add(Linear(2, 4))\nnet.add(LeakyReLU())\nnet.add(Linear(4, 2))\nnet.add(SoftMax())\ncriterion = ClassNLLCriterion()\nprint (net)\n", "intent": "Define a **logistic regression** for debugging. \n"}
{"snippet": "model = Sequential() \nmodel.add(Dense(2, input_dim=2, name='wx_b'))\nmodel.add(Activation('softmax', name='softmax'))\nbatch_size = 128\nnb_epoch = 100\n", "intent": "Define the model and train it.\n"}
{"snippet": "from distutils.version import LooseVersion\nimport warnings\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nprint('TensorFlow Version: {}'.format(tf.__version__))\nassert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\nif not tf.test.gpu_device_name():\n    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\nelse:\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n", "intent": "This will check to make sure you have the correct version of TensorFlow and access to a GPU\n"}
{"snippet": "from sklearn.neighbors import KNeighborsClassifier\nprint KNeighborsClassifier()\n", "intent": "<center>\n<img src=\"https://static1.squarespace.com/static/55ff6aece4b0ad2d251b3fee/t/5752540b8a65e246000a2cf9/1465017829684/\">\n</center>\n"}
{"snippet": "from sklearn.svm import SVC\nprint SVC()\n", "intent": "..and kernel application\n<center>\n<img src=\"http://i2.wp.com/blog.hackerearth.com/wp-content/uploads/2017/02/kernel.png\" width=\"500\">\n</center>\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\nmodel.fit(X_train, Y_train, validation_data = (X_val, Y_val), epochs=20, \n          batch_size=128, verbose=True)\n", "intent": "Take couple of minutes and Try and optimize the number of layers and the number of parameters in the layers to get the best results. \n"}
{"snippet": "linear_regression_model = LinearRegression()\n", "intent": "No we define the model we want to use and herein lies one of the main advantages of using this lib\n"}
{"snippet": "linear_regression_model.fit(housing_data.data, housing_data.target)\n", "intent": "Next, we can fit out Linear Regression model on our feature set to make predictions for out labels (the price of the houses). \n"}
{"snippet": "from keras import optimizers\nmodel = Sequential()\nmodel.add(Dense(1000, input_dim=1000))\nmodel.add(Dense(2000))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss= \"mean_squared_error\",  optimizer=\"adam\",  metrics=[\"accuracy\"])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nkfold = StratifiedKFold(n_splits=6, random_state=42, \n                        shuffle=True)\ngrid_searcher = GridSearchCV(\n    RandomForestClassifier(random_state=42, n_estimators=20), \n                 {'criterion': ['gini', 'entropy'],\n                  'max_depth': [1, 3, 5, 7, 10, None]},\n                  cv=kfold, n_jobs=3)\ngrid_searcher.fit(X, y)\nprint('best score:', grid_searcher.best_score_, grid_searcher.best_params_)\n", "intent": "<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n    return tf.nn.embedding_lookup(embedding, input_data)    \ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "clf = tree.DecisionTreeRegressor()\nclf = clf.fit(x_train, y_train)\n", "intent": "Finally, we will use Decision Tree Regression model as a training model and fit it with our training data.\n"}
{"snippet": "from tensorflow.python.framework import graph_io\ngraph_io.write_graph(sess.graph, \n                     \"/root/models/optimize_me/\", \n                     \"unoptimized_gpu.pb\")\n", "intent": "We will use this later.\n"}
{"snippet": "from tensorflow.python.framework import graph_io\ngraph_io.write_graph(sess.graph, \"/root/models/optimize_me/\", \"unoptimized_cpu.pb\")\n", "intent": "We will use this later.\n"}
{"snippet": "from tensorflow.python.framework import graph_io\ngraph_io.write_graph(sess.graph, \"/root/models/optimize_me/\", \"unoptimized_gpu.pb\")\n", "intent": "We will use this later.\n"}
{"snippet": "def build_model():\n", "intent": "The above code has been written as a function. \nChange some of the **hyperparameters** and see what happens. \n"}
{"snippet": "kM.fit(df.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "**Instanstiate instance of model**\n"}
{"snippet": "lm = LinearRegression()\n", "intent": "**Instaiate an Instance of the model**\n"}
{"snippet": "lm.fit(X_train,y_train)\n", "intent": "**Fit model on the training data**\n"}
{"snippet": "grid = GridSearchCV(SVC(),param_grid,verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "A = tf.Variable(tf.zeros([2, 2]), dtype=tf.float32)\nB = tf.Variable(tf.zeros([2, 1]), dtype=tf.float32)\nX = tf.matmul(tf.matrix_inverse(A), B)\nwith tf.Session() as sess:\n    print(sess.run(X, feed_dict={A:[[3, 2], [4, -1]], B:[[15], [10]]}))\n", "intent": "$$\n    \\begin{array}{c} \n    a_{11}x + a_{12}y = b_{11} \\\\ \n    a_{21}x + a_{22}y = b_{21} \\\\ \n    \\end{array} \n$$\n"}
{"snippet": "x = tf.Variable(2.0, dtype=tf.float32, name='x') \ny = tf.pow(x, 2)\n", "intent": "Hint : tf.train.GradientDescentOptimizer(learning_rate).minimize(function)\n"}
{"snippet": "m3 = ols('PRICE ~ CRIM + RM + PTRATIO + LSTAT',bos).fit()\nprint(m3.summary())\n", "intent": "Let us add LSTAT to the set of independent variables:\n"}
{"snippet": "depth = 2\nxTrain = train.iloc[:, :-1].values\nyTrain = train.iloc[:,  -1].values\nxTest = test.iloc[:, :-1].values\nyTest = test.iloc[:,  -1].values\ntree2a = tree.DecisionTreeClassifier(max_depth = depth)\nmod2a  = tree2a.fit(xTrain , yTrain)\n", "intent": "<br>\n** Fit training data:**\n<br>\n"}
{"snippet": "from keras.layers import concatenate\nauxiliary_input = Input(shape=(5,), name='aux_input')\nx = concatenate([lstm_out, auxiliary_input])\nx = Dense(64, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\nmain_output = Dense(1, activation='sigmoid', name='main_output')(x)\n", "intent": "At this point, we feed into the model our auxiliary input data by concatenating it with the LSTM output:\n"}
{"snippet": "lambda_range = np.array(range(-7, 8, 1))\ntrain_r_sq = np.zeros(lambda_range.shape)\ntest_r_sq = np.zeros(lambda_range.shape)\nfor i , lmb in enumerate(lambda_range):\n    ridge_2 = Ridge_Reg(alpha = 10**lmb)\n    ridge_2.fit(x_2_train, y_2_train)\n    train_r_sq[i] = ridge_2.score(x_2_train, y_2_train)\n    test_r_sq[i]  = ridge_2.score(x_2_test, y_2_test)\n", "intent": "**2. Perform the ridge regression for different lambda values:**\n<br>\n"}
{"snippet": "import numpy as np\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\ninputs = np.array([0.7, -0.3])\nweights = np.array([0.1, 0.8])\nbias = -0.1\nx = np.dot(inputs,np.transpose(weights)) + bias\noutput = sigmoid(x)\nprint('Output:')\nprint(output)\n", "intent": "actually as i understand, this is logistic regression.\n"}
{"snippet": "h     = activation(torch.mm(features, W1) + B1)\ny_hat = activation(torch.mm(h, W2) + B2)\ny_hat\n", "intent": "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. \n"}
{"snippet": "model1.fit(X, y)\n", "intent": "The model 1 displays higher score.\n"}
{"snippet": "model = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n", "intent": "This means that our best fit line is:\n$$y = a + b x$$\nwhere\n$$a = -0.363075521319, b = 0.41575542$$\nNext let's use `statsmodels`.\n"}
{"snippet": "test_models = [LinearRegression(), Ridge(alpha = 10), Lasso(alpha = 10)]\nscores = [analyze_performance(my_model) for my_model in test_models]\n", "intent": "Let's try a few degrees with a regularized model.\n"}
{"snippet": "import tensorflow as tf\nx = tf.placeholder(tf.float32)\ny = tf.placeholder(tf.float32)\nc = tf.placeholder(tf.float32)\nz = tf.subtract(tf.divide(x,y),c)\nwith tf.Session() as sess:\n    output = sess.run(z,feed_dict={x:10,y:2,c:1})\n    print(output)\n", "intent": "Use tf constants and tf functions to do basic math operations. Also use feed_dict to allow user input.\n"}
{"snippet": "n_features = 120\nn_labels = 5\nweights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\nbias = tf.Variable(tf.zeros(n_labels))\n", "intent": "It is good practice to initialize variables from a normal distribution to prevent any one variable from overwhelming the others.\n"}
{"snippet": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, save_file)\n    test_accuracy = sess.run(\n        accuracy,\n        feed_dict={features: mnist.test.images, labels: mnist.test.labels})\nprint('Test Accuracy: {}'.format(test_accuracy))\n", "intent": "We don't need to spend all that time retraining the model since we can just reload the parameters.\n"}
{"snippet": "autoencoder.fit(x_train_noisy, x_train,\n                epochs=100,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test_noisy, x_test),\n                callbacks=[TensorBoard(log_dir='/tmp/autoencoder_denoise', \n                                       histogram_freq=0, write_graph=False)])\n", "intent": "Let's train the AutoEncoder for `100` epochs\n"}
{"snippet": "model.fit(x_train, y_train, batch_size=10, nb_epoch=10, validation_data=(x_test, y_test), verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "scaler.fit(bank_notes.drop('Class',axis=1))\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "classifier.fit(X_train, y_train, steps=2000, batch_size=20)\n", "intent": "** Now fit classifier to the training data. Use steps=200 with a batch_size of 20. You can play around with these values if you want!**\n"}
{"snippet": "decTree = DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "kmeans.fit(colleges.drop('Private', axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "scalar.fit(classified_data.drop('TARGET CLASS', axis = 1))\n", "intent": "** Fit scaler to the features.**\n"}
{"snippet": "linear_model = LinearRegression()\n", "intent": "**Create an instance of a LinearRegression() model named lm.**\n"}
{"snippet": "linear_model.fit(X_train, Y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "mnb = MultinomialNB()\nmnb.fit(X_train, y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "decoder_h = Dense(intermediate_dim, activation='relu')\ndecoder_mean = Dense(original_dim, activation='sigmoid')\nh_decoded = decoder_h(z)\nx_decoded_mean = decoder_mean(h_decoded)\n", "intent": "Finally, we can map these sampled latent points back to reconstructed inputs:\n"}
{"snippet": "df = KNeighborsClassifier(n_neighbors=3)\ndf.fit(features, target)\n", "intent": "Set kNN's k value to 3 and fit data\n"}
{"snippet": "iris = st.datasets.get_rdataset('iris','datasets')\ny = iris.data.Species \nX = iris.data.ix[:, 0:4, le]\nx = X.as_matrix(columns=None)\ny = y.as_matrix(columns=None)\nmdl = sm.MNLogit(y, x)\nmdl_fit = mdl.fit()\nmdl_fit.summary()\n", "intent": "Setup the Logistic Regression \n"}
{"snippet": "plot_graph(10,11,len(all_pickles)-1,len(all_pickles),'Training Loss min {0:.2f}'.format(min(train_loss[10])),\n          'Validation Loss min {0:.2f}'.format(min(valid_loss[10])))\n", "intent": "Great! I see, that the model_10 gave me the best result. The model_12 is in the second place. Let's plot them for the further consideration.\n"}
{"snippet": "get_predictions(index=0, \n                partition='train',\n                input_to_softmax=deep_bdrnn_model(input_dim=161,\n                            units=200,\n                            recur_layers=2),\n                model_path='./results/model_12.h5')\n", "intent": "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='validation',\n                input_to_softmax=deep_bdrnn_model(input_dim=161,\n                            units=200,\n                            recur_layers=2),\n                model_path='./results/model_12.h5')\n", "intent": "Use the next code cell to visualize the model's prediction for the first example in the validation dataset.\n"}
{"snippet": "def weight_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial, name='bias')\nx = tf.placeholder(tf.float32, [None, 784], name='x')\ny_ = tf.placeholder(tf.float32, [None, 10], name='labels')\n", "intent": "The same functions above, rewritten and redfined in order to include name labels.\n"}
{"snippet": "def weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial, name='weight')\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial, name='bias')\nx = tf.placeholder(tf.float32, [None, 784], name='x')\ny_ = tf.placeholder(tf.float32, [None, 10], name='labels')\n", "intent": "The same functions above, rewritten and redfined in order to include name labels.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=22,\n          epochs=20,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "f = theano.function([x], y)\n", "intent": "Or compile a function\n"}
{"snippet": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    feed = {inputs_: test_x,\n            labels_: test_y, \n            keep_prob: 1}\n    test_acc = sess.run(accuracy, feed_dict=feed)\n    print(\"Test accuracy: {:.4f}\".format(test_acc))\n", "intent": "Below you see the test accuracy. You can also see the predictions returned for images.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    word_embedding = tf.Variable(initial_value=tf.random_uniform((vocab_size, embed_dim),-1,1),name='word_embedding')\n    embedded_input = tf.nn.embedding_lookup(word_embedding, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def conv_layer(prev_layer, layer_depth, is_training):\n    strides = 2 if layer_depth % 3 == 0 else 1\n    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', use_bias=False)\n    conv_layer = tf.nn.relu(\n        tf.layers.batch_normalization(conv_layer, training=is_training))\n    return conv_layer\n", "intent": "Modify `conv_layer` to add batch normalization to the convolutional layers it creates. Feel free to change the function's parameters if it helps.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform(shape=(vocab_size,embed_dim),minval=-1,maxval=1,dtype=tf.float32))\n    embed = tf.nn.embedding_lookup(embedding,input_data) \n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=600)\nrfc.fit(X_train, y_train)\n", "intent": "Now its time to train our model!\n**Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.**\n"}
{"snippet": "kmeans.fit(college.drop('Private',axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "nb = MultinomialNB()\nnb.fit(X_train,y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "lda_5_topics_raw = utils.create_lda_model(corpus_raw, id2word_raw, 5)\nlda_5_topics_raw.print_topics()\n", "intent": "Let's create LDA models with different number of topics and see if we observe significant differences.\n"}
{"snippet": "G = nx.Graph()\nfor node in nodes:\n    name = df_persons.loc[1]['Name']\n    G.add_node(node, id=node, name=name)\nG.add_edges_from(list_links)\n", "intent": "We can now build the first graph with all links and all nodes:\n"}
{"snippet": "X = T.vector()\nX = T.matrix()\nX = T.tensor3()\nX = T.tensor4()\n", "intent": "Other tensor types\n==========\n"}
{"snippet": "param_grid = {'C':[0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\ngrid = GridSearchCV(SVC(), param_grid, verbose=3)\ngrid.fit(X_train, y_train)\n", "intent": "The model was very good, still do grid search for pratice\n"}
{"snippet": "pred =multilayer_perceptron(x, weights, biases)\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n", "intent": "Prepare model and train\n"}
{"snippet": "def bag_of_words_kernel(fitted_vectoriser, X,Y):\n    X_feats = fitted_vectoriser.transform(X)\n    if X is Y :\n        Y_feats = X_feats\n    else:\n        Y_feats = fitted_vectoriser.transform(Y)\n    K = X_feats.dot(Y_feats.T).todense() / len(vectorizer.get_feature_names())\n    return K\n", "intent": "Next, let's define the bag of words kernel, as a product of embedded documents.\n"}
{"snippet": "inputs = digits.data\ntargets = digits.target\nhidden_weights = np.random.random([64,10])\noutput_weights = np.random.random(10)\nlearning_rate = 0.01\nepochs = 10\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n", "intent": "So now to build out a nerual net to classify this.\n"}
{"snippet": "hidden_input = np.dot(inputs[0], hidden_weights)\nhidden_output = sigmoid(hidden_input)\nhidden_output.shape\n", "intent": "So the dot product of each input vector and the weights gives us 10 outputs:\n"}
{"snippet": "hidden_outputs = sigmoid(hidden_inputs)\nprint(hidden_outputs.shape)\nhidden_outputs\n", "intent": "Using the sigmoid function to calculate the output of the hidden layer:\n"}
{"snippet": "input = tf.placeholder(tf.float32, (None, 32, 32, 3))\nfilter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) \nfilter_bias = tf.Variable(tf.zeros(20))\nstrides = [1, 2, 2, 1] \npadding = 'VALID'\nconv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias\n", "intent": "How to construct a simple convo network:\n"}
{"snippet": "x = tf.add(5, 2)  \nwith tf.Session() as s:\n    output = s.run(x)\nprint(x) \noutput \n", "intent": "to print the value of a tensorflow variable, you have to run it in a session:\n"}
{"snippet": "def run():\n    output = None\n    logit_data = [2.0, 1.0, 0.1]\n    logits = tf.placeholder(tf.float32)\n    softmax = tf.nn.softmax([2.0, 1.0, 0.1])     \n    with tf.Session() as sess:\n        output = sess.run(softmax)\n    return output\nrun()\n", "intent": "[softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) returns an array of prob values\n"}
{"snippet": "import numpy as np\nx = theano.shared(np.zeros((2, 3), dtype=theano.config.floatX))\n", "intent": "- Symbolic + Storage\n"}
{"snippet": "import tensorflow as tf\nx = tf.constant(8)\ny = tf.constant(9)\nz = tf.multiply(x,y)\nsess = tf.Session()\nsess.run(z)\n", "intent": "testing if tensorflow works, using [this example](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/tensorflow.html).\n"}
{"snippet": "simple_model_coef_table = list(zip(significant_words, simple_model.coef_.flatten()))\nprint(simple_model_coef_table)\n", "intent": "After following same steps as with previous, full model, we are reaching table of coefficients based on our smaller list of words.\n"}
{"snippet": "k = 2\nkmeans = cluster.KMeans(n_clusters=k)\nkmeans.fit(dn)\n", "intent": "Cluster two of the variables of your choice. Choose K based on your plots and the behavior of the data\n"}
{"snippet": "from sklearn.neighbors import NearestNeighbors\nnn = NearestNeighbors(n_neighbors = len(X)).fit(X)\ndistances, indices = nn.kneighbors(X)\n", "intent": "*average taxi out time* and *average airport departure delay*\n"}
{"snippet": "model1 = naive_bayes.GaussianNB()\nmodel1.fit(Xtrain,ytrain)\nprint 'Score =',model1.score(Xtest,ytest)\nmodel2 = naive_bayes.MultinomialNB()\nmodel2.fit(Xtrain,ytrain)\nprint 'Score =',model2.score(Xtest,ytest)\nmodel3 = naive_bayes.BernoulliNB()\nmodel3.fit(Xtrain,ytrain)\nprint 'Score =',model3.score(Xtest,ytest)\n", "intent": "Instantiate the Naive Bayes predictor from scikit-learn with the training data. \n"}
{"snippet": "logreg_parameters = {'penalty':['l2'], 'solver':['liblinear','lbfgs'],\n         'cv':[2,3,4,5,6,7,8,9,10]}\ngsm = GridSearchCV(LogisticRegressionCV(),logreg_parameters, scoring='mean_squared_error',verbose = True, n_jobs = -1, error_score= 0)\ngsm.fit(Xtrain,ytrain)\n", "intent": "Use GridSearchCV with logistic regression to search for optimal parameters.\n"}
{"snippet": "clf = RandomForestClassifier()\n", "intent": "Here, we will be using the *country* column as the target and (*city*, *region*) as the input for the random forest.\n"}
{"snippet": "arch=resnet34\ndata = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))\nlearn = ConvLearner.pretrained(arch, data, precompute=True)\nlearn.fit(0.01, 2)\n", "intent": "I am going to use a <b>pre-trained</b> [resnet model](https://github.com/KaimingHe/deep-residual-networks).\n"}
{"snippet": "def relu(x): \n    return (x+np.abs(x))/2\ndef calc_auc_nl(rv, rm, gv, gm, bv, bm):\n    filter_size = 45\n    gray_image = (rv*relu(rgb_img[:,:,0]/255.0-rm)+gv*relu(rgb_img[:,:,1]/255.0-gm)+\n                  bv*relu(rgb_img[:,:,2]/255.0-bm))/(rv+gv+bv)\n    score_value = gray_image.astype(np.float32).flatten()\n    fpr2, tpr2, _ = roc_curve(ground_truth_labels,score_value)\n    return {'fpr': fpr2, 'tpr': tpr2, 'auc':auc(fpr2,tpr2), 'gimg': gray_image, 'fimg': filtered_image}\n", "intent": "Here we use non-linear approaches to improve the quality of the results\n"}
{"snippet": "count = theano.shared(0)\nnew_count = count + 1\nupdates = {count: new_count}\nf = theano.function([], count, updates=updates)\n", "intent": "- Store results of function evalution\n- `dict` mapping shared variables to new values\n"}
{"snippet": "CONV_SIZE = (10,10,3)\ndef calc_auc_conv2d(rcoefs):\n    coefs = rcoefs.reshape(CONV_SIZE)/rcoefs.sum()\n    score_image = relu(convolve(grey_img,coefs))\n    score_value = score_image.flatten()\n    fpr2, tpr2, _ = roc_curve(ground_truth_labels,score_value)\n    return {'fpr': fpr2, 'tpr': tpr2, 'auc': auc(fpr2,tpr2), 'gimg': score_image}\n", "intent": "Using the RGB instead of the gray value for the CNN\n"}
{"snippet": "iris = load()\nX = iris.data\ny = iris.target\nf = X.shape[1]\nrnd_d3 = DecisionTreeClassifier(max_features=int(f ** 0.5)) \nd3 = DecisionTreeClassifier() \n", "intent": "<img src='pics/bagging.png'>\n"}
{"snippet": "scaler.fit(df.drop('TARGET CLASS',axis=1))\nscaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))\n", "intent": "**Fit the features data only to this estimator (leaving the TARGET CLASS column) and transform**\n"}
{"snippet": "kmeans = KMeans(n_clusters=2,verbose=0,tol=1e-3,max_iter=300,n_init=20)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "import numpy as np\nn = 1000\nboot_samples = np.empty((n, len(lrmod.coef_[0])))\nfor i in np.arange(n):\n    boot_ind = np.random.randint(0, len(X0), len(X0))\n    y_i, X_i = y.values[boot_ind], X0.values[boot_ind]\n    lrmod_i = LogisticRegression(C=1000)\n    lrmod_i.fit(X_i, y_i)\n    boot_samples[i] = lrmod_i.coef_[0]\n", "intent": "`scikit-learn` does not calculate confidence intervals for the model coefficients, but we can bootstrap some in just a few lines of Python:\n"}
{"snippet": "est = GradientBoostingRegressor(n_estimators=1000, max_depth=3, learning_rate=0.04,\n                                loss='huber', random_state=0)\nest.fit(X_train, y_train)\n", "intent": "Lets fit a gradient boosteed regression tree (GBRT) model to this dataset and inspect the model:\n"}
{"snippet": "from pymc3.glm import GLM\nwith pm.Model() as model:\n    GLM.from_formula('y ~ x', data)\n    glm_samples = pm.sample(1000)\n", "intent": "The model can then be very concisely specified in one line of code.\n"}
{"snippet": "rand_array = np.random.rand(4, 4)\ny = tf.identity(x)\nprint(sess.run(y, feed_dict={x: rand_array}))\n", "intent": "When the session is run, the `feed_dict` argument is used to feed in the value of x into the computational graph, via the placeholders.\n"}
{"snippet": "tf.reset_default_graph()\nsess = tf.Session()\n", "intent": "There are various ways to create matrices and manipulate them in TensorFlow\n"}
{"snippet": "Y_pred = tf.nn.softmax(tf.matmul(X, W) + b)\n", "intent": "The output of the logic regression is  $softmax(Wx+b)$\nNote that the dimension of *Y_pred* is *(nBatch, dimY)*\n"}
{"snippet": "data = np.random.random((1000, 32))\nlabels = np.random.random((1000, 10))\nmodel.fit(data, labels, epochs=10, batch_size=32)\n", "intent": "For small datasets, use in-memory NumPy arrays to train and evaluate a model. The model is \"fit\" to the training data using the `fit` method:\n"}
{"snippet": "lreg = LinearRegression()\n", "intent": "Next, we create a LinearRegression object.\n"}
{"snippet": "grid.fit(X, y);\n", "intent": "- Fit the model at each grid point & keep track of the scores.\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier().fit(X, y)\n", "intent": "- Scikit-Learn uses the ``DecisionTreeClassifier`` estimator.\n"}
{"snippet": "gmm = GaussianMixture(n_components=4, random_state=42)\nplot_gmm(gmm, X)\n", "intent": "- View the initial four-component GMM results.\n"}
{"snippet": "gmm2 = GaussianMixture(n_components=2, covariance_type='full', random_state=0)\nplot_gmm(gmm2, Xmoon)\n", "intent": "- A two-component GMM as a clustering model doesn't really help.\n"}
{"snippet": "gmm16 = GaussianMixture(n_components=16, covariance_type='full', random_state=0)\nplot_gmm(gmm16, Xmoon, label=False)\n", "intent": "- If we use many more components and ignore the cluster labels, the result is much better.\n"}
{"snippet": "gmm = GaussianMixture(120, covariance_type='full', random_state=0)\ngmm.fit(data)\nprint(gmm.converged_)\n", "intent": "- It looks like ~120 components minimizes the AIC. Fit this to the data and confirm that it has converged.\n"}
{"snippet": "import torch.nn.functional as F\ndef binary_accuracy(preds, y):\n    rounded_preds = torch.round(F.sigmoid(preds))\n    correct = (rounded_preds == y).float() \n    acc = correct.sum()/len(correct)\n    return acc\n", "intent": "We implement the function to calculate accuracy...\n"}
{"snippet": "import keras.utils.visualize_util as vutil\nfrom IPython.display import SVG\nSVG(vutil.to_graph(autoencoder, recursive=True, show_shape=True).create(prog='dot', format=\"svg\"))\n", "intent": "We display a graphical representation of the autoencoder. \nYou can see that the last layer has an output shape of 10.\n"}
{"snippet": "kmeans.fit(df.drop(['Private'],axis=1))\n", "intent": "**Fit the model to all the data except for the Private label.**\n"}
{"snippet": "regressor = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "from keras.models import Model, Sequential\nfrom keras.layers.convolutional import Convolution2D\nfrom keras.layers import Dense, Activation, MaxPooling2D, Flatten, Dropout\nfrom keras.callbacks import History \nhistory = History()\n", "intent": "Now we can train a conv net that will hopefully learn how the image should look\n"}
{"snippet": "model.fit(x= inputs, y = targets, nb_epoch = 1, batch_size = 1,callbacks=[history])\n", "intent": "Train the model now\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embed_input = tf.Variable(tf.truncated_normal((vocab_size, embed_dim), stddev = 0.01))\n    embed_input = tf.nn.embedding_lookup(embed_input, input_data)\n    return embed_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "logmodel  = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "dct = DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "class ReLU(Layer):\n    def __init__(self):\n        relu_forward = np.maximum(0,input)\n        return relu_forward\n    def backward(self, input, grad_output):\n", "intent": "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network.\n"}
{"snippet": "decoder = models.Sequential()\ndecoder.add(Layer(input_shape=(10,)))\ndecoding_layers = create_decoding_layers()\nfor i, l in enumerate(decoding_layers):\n    decoder.add(l)\ndecode = K.function([decoder.get_input(train=False)], [decoder.get_output(train=False)])\ntrained_weights = [l.get_weights() for l in autoencoder.decoding_layers]\nfor i, l in enumerate(decoding_layers):\n    l.set_weights(trained_weights[i])\n", "intent": "By activating a single representation neuron at a time, we can get our decoder to generate parts of digits\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=1)\nknn\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "dtree = DecisionTreeClassifier() \n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "gscv = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "from sklearn.cluster import DBSCAN, AgglomerativeClustering\nagg = AgglomerativeClustering(n_clusters = 7, compute_full_tree=True).fit(finaldt)\n", "intent": "* Agglomerative clustering\n"}
{"snippet": "ols.fit(X_train, y_train)\nprint(\"R^2 for train set: %f\" %ols.score(X_train, y_train))\nprint('-'*50)\nprint(\"R^2 for test  set: %f\" %ols.score(X_test, y_test))\n", "intent": "- Do multiple linear regression with new data set.\n- Report the coefficient of determination from the training and testing sets.\n"}
{"snippet": "sentence = [\"Literally just 8 really really cute dogs\"]\nvectorizer.fit(sentence)\nprint(vectorizer.vocabulary_) \n", "intent": "Fitting the `CountVectorizer` learns the vocabulary\n"}
{"snippet": "svc = svc.fit(bag_of_words, labels)\n", "intent": "Now we learn the boundary!\n"}
{"snippet": "pipeline = pipeline.fit(training.title, training.label)\n", "intent": "Now we're ready to train!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0))\n    embedded_input = tf.nn.embedding_lookup(embedding, input_data)\n    return embedded_input\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "x = T.vector('x', dtype='float32')\nsigmoid_x = T.nnet.sigmoid(x)\nf_x = theano.function(inputs=[x], outputs=sigmoid_x)\nyvals = f_x(xvals)\npl.plot(xvals, yvals)\n", "intent": "Using a vector for x lets us compute the sigmoid of a vector of values instead of doing it\none-by-one\n"}
{"snippet": "import tensorflow as tf\nhidden_layer = tf.add(tf.matmul(features, hidden_weights), hidden_biases)\nhidden_layer = tf.nn.relu(hidden_layer)\noutput = tf.add(tf.matmul(hidden_layer, output_weights), output_biases)\n", "intent": "TensorFlow provides the ReLU function as `tf.nn.relu()`, as shown below:\n"}
{"snippet": "keep_prob = tf.placeholder(tf.float32) \nhidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\nhidden_layer = tf.nn.relu(hidden_layer)\nhidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\nlogits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n", "intent": "TensorFlow provides the `tf.nn.dropout()` function, which you can use to implement dropout.\nLet's look at an example of how to use `tf.nn.dropout()`.\n"}
{"snippet": "import tensorflow as tf\nsoftmax_data = [0.7, 0.2, 0.1]\none_hot_data = [1.0, 0.0, 0.0]\nsoftmax = tf.placeholder(tf.float32)\none_hot = tf.placeholder(tf.float32)\ncross_entropy = -tf.reduce_sum(tf.multiply(one_hot_data, tf.log(softmax_data)))\nwith tf.Session() as session:\n    output = session.run(cross_entropy, feed_dict={one_hot: one_hot_data, softmax: softmax_data})\n    print(output)\n", "intent": "Print the cross entropy using `softmax_data` and `one_hot_encod_label`.\n"}
{"snippet": "svc_pipe.fit(X_train,Y_train)\nsvc_pipe.score(X_test,Y_test)\n", "intent": "The average cross validation accuracy looks promising. Let's calcualte the accuracy on the test set which was split from the training set.\n"}
{"snippet": "happyModel.fit(x = X_train, y = Y_train, epochs = 40, batch_size = 100)\n", "intent": "**Exercise**: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.\n"}
{"snippet": "def init_weights(shape):\n    init_rand_dist= tf.truncated_normal(shape,stddev=0.1)\n    return tf.Variable(init_rand_dist)\n", "intent": "1. Weight Initialization\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform([vocab_size, embed_dim]))\n    return tf.nn.embedding_lookup(embedding, input_data)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "body_regression.fit(x_values, y_values)\n", "intent": "[SciKit-Learn](http://scikit-learn.org/stable/)'s built-in model has this (straight) curve fitting for us, easily.\n"}
{"snippet": "from pyspark.ml.regression import LinearRegression\nlr = LinearRegression(maxIter=10,solver=\"l-bfgs\")\nlrModel = lr.fit(Z_DF)\nprint(\"Coefficients: %s\" % str(lrModel.coefficients))\nprint(\"Intercept: %s\" % str(lrModel.intercept))\ntrainingSummary = lrModel.summary\nprint(\"numIterations: %d\" % trainingSummary.totalIterations)\nprint(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\nprint(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n", "intent": "**Exercise:**\n* Fit the model with the normal and l-bgfs algorithms. \n* Check the Spark UI and compare the different stages\n"}
{"snippet": "input1 = tf.constant(3.0)\ninput2 = tf.constant(2.0)\ninput3 = tf.constant(5.0)\nintermed =tf.add(input2,input3)\nmul = tf.mul(input1,intermed)\nwith tf.Session() as sess:\n    result = sess.run([mul,intermed])\n    print(result)\n", "intent": "Calling sess.run(var) on a tf.Session() object retrieves its value. Can retrieve multiple variables simultaneously with sess.run([var1,var2])\n"}
{"snippet": "from keras.layers import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3,input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "B = 100 \nboot_coefs = np.zeros((X_train3m.shape[1],B)) \nfor i in range(B):\n    sample_index = np.random.choice(range(len(y_train3m)), size=len(y_train3m), replace=True)\n    X_train_samples = X_train3m[sample_index]\n    y_train_samples = y_train3m[sample_index]\n    logistic_mod_boot = LogisticRegression(C=10, fit_intercept=False)\n    logistic_mod_boot.fit(X_train_samples, y_train_samples)\n    boot_coefs[:,i] = logistic_mod_boot.coef_\nboot_coefs.shape\n", "intent": "- Use bootstrap to get the significant coefficients at level of 95%:\n"}
{"snippet": "ols = LinearRegression(fit_intercept=True)\nols.fit(X_train2, y_train2)\nprint('OLS Train Score', ols.score(X_train2, y_train2))\nprint('OLS Test Score', ols.score(X_test2, y_test2))\n", "intent": "- Fit a linear regression model:\n"}
{"snippet": "X = df[ ['Income', 'Cards', 'Age', 'Education', 'Gender_Male', 'Race_Asian', 'Race_Caucasian'] ]\ny = df['Balance']\nmodel = linear_model.LinearRegression()\nmodel.fit(X,y)\nprint model.intercept_\nprint model.coef_\n", "intent": "First, find the coefficients of your regression line.\n"}
{"snippet": "train_X = train_df[ ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth'] ]\ntrain_y = train_df['Species']\nmodel = neighbors.KNeighborsClassifier(n_neighbors = 7, weights = 'uniform')\nmodel.fit(train_X, train_y)\nprint 'train = ', model.score(train_X, train_y)\ntest_X = test_df[ ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth'] ]\ntest_y = test_df['Species']\nprint 'test  = ', model.score(test_X, test_y)\n", "intent": "The error in the training set is less than the error is the test set\n"}
{"snippet": "train_X = train_df[ ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth'] ]\ntrain_y = train_df['Species']\nmodel = neighbors.KNeighborsClassifier(n_neighbors = 5, weights = 'uniform')\nmodel.fit(train_X, train_y)\nprint 'train = ', model.score(train_X, train_y)\ntest_X = test_df[ ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth'] ]\ntest_y = test_df['Species']\nprint 'test  = ', model.score(test_X, test_y)\n", "intent": "The error in the training set is less than the error is the test set\n"}
{"snippet": "model = tree.DecisionTreeRegressor()\nmodel.fit(train_X, train_y)\n", "intent": "(Check http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html as needed)\n"}
{"snippet": "model = ensemble.RandomForestClassifier(n_estimators = 1000, max_features = 4, min_samples_leaf = 5, oob_score = True)\nmodel.fit(train_X, train_y)\n", "intent": "(Check http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html as needed)\n"}
{"snippet": "model=models.ldamodel.LdaModel(corpus = corpus, num_topics = 10, id2word = id2word, passes = 10)\n", "intent": "(Check https://radimrehurek.com/gensim/models/ldamodel as needed)\n"}
{"snippet": "a = np.zeros((3,3))\nta = tf.convert_to_tensor(a)\nwith tf.Session() as sess:\n    print(sess.run(ta))\n", "intent": "* All previous examples have manually defined tensors. How can we input external data into TensorFlow?\n* Simple solution: Import from Numpy:\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_w = tf.Variable(tf.random_uniform([vocab_size, embed_dim], -1, 1))\n    embed = tf.nn.embedding_lookup(embedding_w, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "x = tf.placeholder(tf.string)\ny = tf.placeholder(tf.int32)\nz = tf.placeholder(tf.float32)\nwith tf.Session() as sess:\n    output = sess.run(x, feed_dict={x: \"Test String\", y: 123, z: 123.4})\n", "intent": "It's also possible to set **more than one tensor** using feed_dict\n"}
{"snippet": "x = tf.reduce_sum([1,2,3,4,5])  \nwith tf.Session() as sess:\n    output = sess.run(x)\n    print (output)\n", "intent": "The **tf.reduce_sum()** function takes an array of numbers and sums them together\n"}
{"snippet": "x = tf.log(100.0)  \nwith tf.Session() as sess:\n    output = sess.run(x)\n    print (output)\n", "intent": "**tf.log()** takes the natural log of a number.\n"}
{"snippet": "save_file = \"./notMNIST_model.ckpt\"\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, save_file)\n    test_accuracy = sess.run(\n        accuracy,\n        feed_dict={features: test_features, labels: test_labels})\nprint('Test Accuracy: {}'.format(test_accuracy))\n", "intent": "** Load the trained model:**\n"}
{"snippet": "save_file = \"./MNIST_model/mnist_model.ckpt\"\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, save_file)\n    test_accuracy = sess.run(\n        accuracy,\n        feed_dict={x: test_features, y: test_labels})\nprint('Test Accuracy: {}'.format(test_accuracy))\n", "intent": "** Load Up the Trained Model:**\n"}
{"snippet": "def build_lstm(lstm_size, n_lstm_layers, keep_prob, batch_size):\n    lstm_basic = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    lstm_drop = tf.contrib.rnn.DropoutWrapper(lstm_basic, output_keep_prob=keep_prob)\n    multi_layers_lstm = tf.contrib.rnn.MultiRNNCell([lstm_drop] * n_lstm_layers)\n    initial_state = multi_layers_lstm.zero_state(batch_size, tf.float32)\n    return multi_layers_lstm, initial_state\n", "intent": "** Suppose n_lstm_layers=2, lstm_size=512, Then:**  \n** - there are 2 LSTM layers**  \n** - each LSTM cell has 512 layers**\n"}
{"snippet": "def build_lstm(lstm_size, n_lstm_layers, keep_prob, batch_size):\n    lstm_basic = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    lstm_drop = tf.contrib.rnn.DropoutWrapper(lstm_basic, output_keep_prob=keep_prob)\n    multi_layers_lstm = tf.contrib.rnn.MultiRNNCell([lstm_drop] * n_lstm_layers)\n    initial_state = cell.zero_state(batch_size, tf.float32)\n    return multi_layers_lstm, initial_state\n", "intent": "** Suppose n_lstm_layers=2, lstm_size=512, Then:**  \n** - there are 2 LSTM layers**  \n** - each LSTM cell has 512 layers**\n"}
{"snippet": "model.fit(x_train, y_train, batch_size=32, epochs=100, validation_data=(x_test, y_test), verbose=0)\nmodel2.fit(x_train, y_train, batch_size=32, epochs=100, validation_data=(x_test, y_test), verbose=0)\nmodel3.fit(x_train, y_train, batch_size=32, epochs=100, validation_data=(x_test, y_test), verbose=0)\nmodel4.fit(x_train, y_train, batch_size=512, epochs=100, validation_data=(x_test, y_test), verbose=0)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "with tf.variable_scope(\"fooo\"):\n    v = tf.get_variable(\"v\",[1])\nassert v.name == \"fooo/v:0\"\n", "intent": "* Behavior depends on whether variable reuse enabled   \n* Case 1: reuse set to false\n    * Create and return new variable\n"}
{"snippet": "W_fc1 = weight_variable([250 * 1 * 16, 32])\nb_fc1 = bias_variable([32])\nh_pool2_flat = tf.reshape(h_pool1, [-1, 250 * 1 * 16])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n", "intent": "print(h_conv2.get_shape())\nh_pool2.get_shape()\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n", "intent": "- Use entropy (deviance) as the impurity measure, $i(\\mathcal{N})=-\\sum_j  p_j \\log_2 p_j$\n- Limit the tree depth to three (to prevent overfitting).\n"}
{"snippet": "tree.fit(X_train, y_train)\n", "intent": "Fit decision tree using training set.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=1000.0, random_state=0)\nlr.fit(X_bin[:,0].reshape(-1,1), y_bin)\n", "intent": "Train a logistic regression model with scikit-learn\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr1 = LogisticRegression(C=1000.0, random_state=0)\nlr1.fit(glass['al'].reshape(-1,1), glass['household'].reshape(-1,1))\n", "intent": "Train a logistic regression model with scikit-learn\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(criterion = 'entropy', n_estimators = 100, random_state = 1, n_jobs = 2)\nX_train = dataset['train']['image']\ny_train = dataset['train']['label']\nforest.fit(X_train, y_train)\n", "intent": "- http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlr3 = LogisticRegression(C=0.1, random_state=0)\nlr3.fit(X_std, y)\nplot_decision_regions(X_std, y, lr3)\n", "intent": "Question: What is the test (prediction) results when $C$ is very small?\n"}
{"snippet": "from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(max_depth=3, criterion = 'entropy', n_estimators = 100, random_state = 1, n_jobs = 2)\nX_train = dataset['train']['image']\ny_train = dataset['train']['label']\nforest.fit(X_train, y_train)\n", "intent": "- http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "with tf.variable_scope(\"foooo\"):\n    v = tf.get_variable(\"v\",[1])\nwith tf.variable_scope(\"foooo\",reuse=True):\n    v1 = tf.get_variable(\"v\",[1])\nassert v1 == v\n", "intent": "* Case 2: Variable reuse set to true\n    * Search for existing variable with given name. Raise ValueError if none found\n"}
{"snippet": "self.embedding(input).shape\n", "intent": "`embedded = self.embedding(input).view(1, 1, -1)`\n"}
{"snippet": "pred = torch.sigmoid(x)\nloss = F.binary_cross_entropy(pred, y)\nloss\n", "intent": "The above but in pytorch.\n"}
{"snippet": "def softmax(x): return x.exp() / (x.exp().sum(-1)).unsqueeze(-1)\ndef nl(input, target): return -input[range(target.shape[0]), target].log().mean()\npred = softmax(x)\nloss=nl(pred, target)\nloss\n", "intent": "This version is most similar to the math formula, but not numerically stable.\n"}
{"snippet": "p = 0.4\nm = torch.nn.Dropout(p)\n", "intent": "Create a dropout layer `m` with a dropout rate `p=0.4`: \n"}
{"snippet": "train_model(sz, bs, lr)\n", "intent": "We see a validation accuracy of 94%. \n"}
{"snippet": "train_model(sz, bs, lr)\n", "intent": "We see a validation accuracy of 86%. \n"}
{"snippet": "train_model(sz, bs, lr)\n", "intent": "This gives us a 10-year average prediction error.\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression()\nlinear_model.fit(X_train,Y_train)\nCoeff = linear_model.coef_\nprint('Coefficients: \\n ', Coeff)\n", "intent": "Estimate the coefficients for each input feature. Construct and display a dataframe with coefficients and X.columns as columns\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\n", "intent": "Check: coef_ and intercept_ functions can help you get coefficients & intercept\n"}
{"snippet": "import tensorflow as tf\nx = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.nn.softmax(tf.matmul(x, W) + b) \ny\n", "intent": "blue: positive weights, red: negative weights\n"}
{"snippet": "W = tf.Variable(tf.zeros(shape=[13,1]), name=\"Weights\")\nb = tf.Variable(tf.zeros(shape=[1]), name=\"Bias\")\n", "intent": "Define Weights and Bias\n"}
{"snippet": "x = tf.placeholder(shape=[None,4],dtype=tf.float32, name='x-input')\nx_n = tf.layers.batch_normalization(x, training=True)\ny_ = tf.placeholder(shape=[None],dtype=tf.float32, name='y-input')\n", "intent": "1.Define input data placeholders\n"}
{"snippet": "W = tf.Variable(tf.zeros(shape=[4,1]), name=\"Weights\")\nb = tf.Variable(tf.zeros(shape=[1]), name=\"Bias\")\n", "intent": "2.Define Weights and Bias\n"}
{"snippet": "def con2d(x,W):\n    return tf.nn.conv2d(input=x, filter=W, strides=[1,1,1,1], padding=\"SAME\")\ndef ang_pool_2x2(x):\n    return tf.nn.avg_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n", "intent": "input ( 28 * 28 * 1)\noutput : single scalar numer - if it is fake or not\n"}
{"snippet": "param_grid = [\n        {'preparation__num_pipeline__imputer__strategy': ['mean', 'median', 'most_frequent'],\n         'feature_selection__k': [3, 4, 5, 6, 7]}\n]\ngrid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n                                scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\ngrid_search_prep.fit(housing, housing_labels)\n", "intent": "Question: Automatically explore some preparation options using `GridSearchCV`.\n"}
{"snippet": "reset_graph()\n", "intent": "Let's load the data:\n"}
{"snippet": "learning_rate = 0.01\nX = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\ny = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\ntheta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\ny_pred = tf.matmul(X, theta, name=\"predictions\")\nerror = y_pred - y\nmse = tf.reduce_mean(tf.square(error), name=\"mse\")\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ntraining_op = optimizer.minimize(mse)\ninit = tf.global_variables_initializer()\n", "intent": "To implement manin batch Gradient Descent.\nchange the definition of X and y in the construction phase to make them placeholder nodes\n"}
{"snippet": "reset_graph()\nbatch_norm_momentum = 0.9\n", "intent": "ELU activation and Batch norm -> each layer\n"}
{"snippet": "with tf.Session() as sess:\n    init.run()\n    saver.restore(sess, \"./my_model_final.ckpt\")\n    for epoch in range(n_epochs):\n        for iteration in range(mnist.train.num_examples//batch_size):\n            X_batch, y_batch = mnist.train.next_batch(batch_size)\n            sess.run(training_op, feed_dict = {X:X_batch, y:y_batch})\n        accuracy_val =accuracy.eval(feed_dict = {X: mnist.test.images, y:mnist.test.labels})\n        print(epoch, \"Test accuracy : \", accuracy_val)\n    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")\n", "intent": "We can train the new model\n"}
{"snippet": "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n", "intent": "We then convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool.\n"}
{"snippet": "from sklearn import tree\nclf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,min_samples_leaf=5)\nclf = clf.fit(X_train,y_train)\n", "intent": "Now, we can create a new DecisionTreeClassifier and use the fit method of the classifier to do the learning job.\n"}
{"snippet": "gnb_classifier_dirty = GaussianNB()\nx2 = news_A.drop({'class'}, axis=1)\ny2 = news_A['class']\ngnb_classifier_dirty.fit(x2, y2)\ngnb_classifier_dirty.score(x2, y2)\nnews_A.describe()\n", "intent": "Fit a Gaussian Naive Bayes model to the original dataset A (including the outliers). Display the classification accuracy on the training dataset.\n"}
{"snippet": "em_3 = evaluate_model(y_3, lr_f5_pred_3)\nprint('RMSE: {0}, MAE: {1}, CC: {2}'.format(em_3[1], em_3[2], em_3[3]))\n", "intent": "Display the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Correlation Coefficient (CC).\n"}
{"snippet": "kmeans = KMeans(n_clusters=5, random_state=30)\n_= kmeans.fit(X)\n", "intent": "Initialise a [k-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\ngnb_classifier = GaussianNB()\nx = news_A_clean.drop({'class'}, axis=1)\ny = news_A_clean['class']\ngnb_classifier.fit(x, y)\ngnb_classifier.score(x, y)\n", "intent": "<span style=\"color:red\">OK\n"}
{"snippet": "gnb_classifier_dirty = GaussianNB()\nx2 = news_A.drop({'class'}, axis=1)\ny2 = news_A['class']\ngnb_classifier_dirty.fit(x2, y2)\ngnb_classifier_dirty.score(x2, y2)\nnews_A.describe()\n", "intent": "<span style=\"color:red\">OK\n"}
{"snippet": "for bin in [x / 1.0 for x in range(1, 10)]:\n    bern = BernoulliNB(binarize=bin)\n    bern.fit(train_data, train_labels)\n    print 'binarize:', bin, 'bernoulli accuracy: %3.2f' %bern.score(test_data, test_labels)\n", "intent": "What choices did you make with manipulating the features above? Try tuning these choices, can you improve the accuracy?\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_normal([vocab_size, embed_dim], stddev=0.1), name='embeddings')\n    embed = tf.nn.embedding_lookup(embedding, input_data, name='embed')\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='ADAM', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "W_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n", "intent": "In order to build a deep network, we stack several layers of this type. The second layer will have 64 features for each 5x5 patch.\n"}
{"snippet": "stride=2\nprint('Input size: ', x.shape)\nconv1=nn.Conv2d(in_channels=1, out_channels=128, kernel_size=(3,3), stride=stride, padding=(1,1))\nprint('After convolution size: ', conv1(x).shape)\npool1 = nn.MaxPool2d(kernel_size=(2,2),stride=stride)\nprint('After pool size: ', pool1(conv1(x)).shape)\nprint('Output size is: 10')\n", "intent": "Clearly mention the sizes for your input, kernel, pooling, and output at each step until you get final output vector with 10 probabilities\n"}
{"snippet": "print('Results for Adam Optimizer')\nif use_gpu:\n    model_Q3p6.cuda()\n    optimizer = torch.optim.Adam(model_Q3p6.parameters(), lr = 0.0001)\nmodel_Q3p6.train()    \ninit_weights(model_Q3p6)\nmodel_Q3p8b = train_model(model_Q3p6, criterion, optimizer,\n                       num_epochs=40, trainVal=['train','val'],verbose=False)  \ny_s, y_t=inference(model_Q3p8b,validation_loader)\nr_AUCAdam = get_AUC(y_s, y_t)\n", "intent": "We choose SGD with momentum as an optimizer. Investigate the effect of at least two other optimizers in model performance.\n"}
{"snippet": "model_conv.train()   \nmodel_conv_ft = train_model(model_conv, criterion, optimizer,\n                       num_epochs=100, trainVal=['train','val'])\n", "intent": "Train the model for 100 epochs and save the weights of the best model using the validation loss. Plot train and validation loss curves\n"}
{"snippet": "model_conv_fine_tune.train()   \nmodel_conv_fine_tune_ft = train_model(model_conv_fine_tune, criterion, optimizer,\n                       num_epochs=100, trainVal=['train','val']) \n", "intent": "Train the model for 100 epochs and save the weights of the best model using the validation loss. Plot train and validation loss curves\n"}
{"snippet": "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=5)\n", "intent": "Train and evaluate\n^^^^^^^^^^^^^^^^^^\nIt should take around 15-25 min on CPU. On GPU though, it takes less than a\nminute.\n"}
{"snippet": "def test_sentence(sentence, model):\n    model.eval()\n    test_tensor = torch.LongTensor(sentences_to_padded_index_sequences(vocab_indices, [sentence]).astype(int))\n    score = model(Variable(test_tensor)).data.numpy()[0][0][0]\n    return (\"positive\" if score > 0.5 else \"negative\", score)\n", "intent": "Play around with trained model:\n"}
{"snippet": "get_predictions(index=0, \n                partition='train',\n                input_to_softmax=final_model(input_dim=13, \n                        filters=200,\n                        kernel_size=11, \n                        conv_stride=2,\n                        conv_border_mode='valid',\n                        dropout= 0.2,\n                        units=200), \n                model_path='results/model_end.h5')\n", "intent": "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='validation',\n                input_to_softmax=final_model(input_dim=13, \n                        filters=200,\n                        kernel_size=11, \n                        conv_stride=2,\n                        conv_border_mode='valid',\n                        dropout= 0.2,\n                        units=200),\n                model_path='results/model_end.h5')\n", "intent": "Use the next code cell to visualize the model's prediction for the first example in the validation dataset.\n"}
{"snippet": "text_clf_lsvc.fit(X_train, y_train)\n", "intent": "Next we'll run Linear SVC\n"}
{"snippet": "W_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n", "intent": "Finally, we add a softmax layer, just like for the one layer softmax regression.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(256, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(32, activation='softmax'))\nmodel.add(Dropout(.2))\nmodel.add(Dense(num_classes))\nsgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss = 'mean_squared_error', optimizer=sgd, metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "lm.fit(X_train, y_train)\nlmRidgeCV.fit(X_train, y_train)\nlmLassoCV.fit(X_train, y_train)\n", "intent": "* Luego, hacemos el fit de los tres estimadores, lo cual nos lleva a...\n"}
{"snippet": "train_cols = ['gre','gpa','prestige_2','prestige_3','prestige_4','intercept']\nlogit = sm.Logit(data[\"admit\"], data[train_cols])\nresult = logit.fit()\nprint(result.summary2())\n", "intent": "**Se descarta prestige_1 para evitar la [multicolinealidad](https://en.wikipedia.org/wiki/Multicollinearity)**\n"}
{"snippet": "from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X, y);\n", "intent": "Vamos a aplicar a este conjunto de datos un modelo de tipo Gaussian Naive Bayes.\n"}
{"snippet": "model_pca = pca.fit(Xtrain)\nX_train_pca = model_pca.transform(Xtrain)\n", "intent": "Vamos a reducir las dimensiones del conjunto de entrenamiento con PCA\n"}
{"snippet": "model_svc = SVC(kernel='rbf', class_weight='balanced', C=1, gamma = 0.001)\n", "intent": "```\nkernel: linear\nC: 0.015\ngamma: auto\n```\n"}
{"snippet": "grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\ngrid.fit(X_train, y_train)\n", "intent": "Ajustamos los modelos\n"}
{"snippet": "from IPython.display import Image\nfrom sklearn.tree import export_graphviz\nimport pydotplus\nbdtr = BaggingRegressor(DecisionTreeRegressor(max_depth=2),n_estimators=50)\nbdtr.fit(X,y)\n", "intent": "Para poder correr estas visualiaciones, abrir una terminal y ejecutar:\npip install pydotplus <br />\nconda install graphviz\n"}
{"snippet": "dbscn = DBSCAN(eps = 0.8, min_samples = 7).fit(X)\n", "intent": "Implementamos `DBSCAN`\n"}
{"snippet": "model = Sequential()\nmodel.add(LSTM(64, return_sequences=True, stateful=True, \n               batch_input_shape=(batch_size, 1, X_train.shape[-1])))\nmodel.add(LSTM(64, return_sequences=True, stateful=True))\nmodel.add(LSTM(64, stateful=True))\nmodel.add(Dense(num_classes, activation='softmax'))\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n", "intent": "We'll use a three-layer stacked LSTM with 64 cells and Adam optimizer with initial learning rate of 0.001\n"}
{"snippet": "hide_code\ndef model():\n    model = Sequential()\n    return model\nmodel = model()\n", "intent": "Define a model architecture and compile the model.\n"}
{"snippet": "hide_code\ndef rnn_model():\n    model = Sequential()\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])    \n    return model \nrnn_model = rnn_model()\n", "intent": "Define a model architecture and compile the model.\n"}
{"snippet": "hide_code\ndef rnn_model():\n    model = Sequential()\n    model.add(LSTM(52, return_sequences=True, input_shape=(1, 13)))\n    model.add(LSTM(52, return_sequences=True))\n    model.add(LSTM(52, return_sequences=False))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse', metrics=['mae'])       \n    return model \nrnn_model = rnn_model()\n", "intent": "Define a model architecture and compile the model.\n"}
{"snippet": "hide_code\ndef model():\n    model = Sequential()\n    return model\nmodel = model()\n", "intent": "Define a model architecture and compile the model for color images.\n"}
{"snippet": "hide_code\ny_train_c = np.array([np.argmax(y) for y in y_train])\ny_test_c = np.array([np.argmax(y) for y in y_test])\nclf = GradientBoostingClassifier().fit(x_train.reshape(-1, 32*32*3), y_train_c)\nclf.score(x_test.reshape(-1, 32*32*3), y_test_c)\n", "intent": "Let's compare the results with classifying algorithms.\n"}
{"snippet": "hide_code\ny_train2_c = np.array([np.argmax(y) for y in y_train2])\ny_test2_c = np.array([np.argmax(y) for y in y_test2])\nclf = GradientBoostingClassifier().fit(x_train2.reshape(-1, 32*32), y_train2_c)\nclf.score(x_test2.reshape(-1, 32*32), y_test2_c)\n", "intent": "Let's compare the results with classifying algorithms.\n"}
{"snippet": "hide_code\nbrands = data['brand_label'].values\nproducts = data['product_label'].values\nimages = data_to_tensor(data['file']);\n", "intent": "Let's create tensors of variables and display some examples of images.\n"}
{"snippet": "hide_code\ndef cb_model():\n    model = Sequential()\n    return model\ncb_model = cb_model()\n", "intent": "We should have an accuracy \n- greater than 14.3% for the first target (`brand`) and \n- greater than 10% for the second target (`product`).\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(3, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=32, verbose=1)\ncallbacks_list = [early_stopping]\nhist = model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.2, shuffle=True, verbose=2)\n", "intent": "Let's train the model with early stopping:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=3, batch_size=100, verbose=2, validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, input_dim=1500, activation='elu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "hist = model.fit(x_train, y_train, validation_split=0.2 ,epochs=15, batch_size=120)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "model.fit(x_train, y_train, epochs=15, batch_size=120, verbose=2, validation_data=(x_test, y_test))\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "reg = sk_lm.LinearRegression();\nreg.fit(X,y);\n", "intent": "Now we instantiate the *LinearRegression* class and call the function *fit(X,y)* in order to estimate the linear regression parameters.\n"}
{"snippet": "reg = sk_lm.LinearRegression();\nreg.fit(np.array(X_all),y);\nreg_fs = sk_lm.LinearRegression();\nreg_fs.fit(np.array(X_all)[:,fs_indices],y);\nprint(\"Mean squared error on training data, full input variables: \", mse(reg,X_all,y));\nprint(\"Mean squared error on training data, selected input variables: \", mse(reg_fs,np.array(X_all)[:,fs_indices],y));\n", "intent": "Forward selection will make our regression fit worse on our training data, but better on test data. We can show this looking at the prediction errow:\n"}
{"snippet": "logreg_simple = skl.linear_model.LogisticRegression();\nX_house_log_area = [X_house_log[\"area\"]];\nX_house_log_area_zip = [list(i) for i in zip(*X_house_log_area)];\nlogreg_simple.fit(X_house_log_area_zip,y_house_log);\n", "intent": "A more intuitive way to show how logistic regression works is to show the output for a single input variable.\n"}
{"snippet": "model = smf.ols(formula = 'MEDV ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT', data = training_df)\nfit = model.fit()\nfit.summary()\n", "intent": "Seems comparable to before -- let's see how it performs using all predictors:\n"}
{"snippet": "model = pm.Model([theta, phi, z, eta, y_tau, y, w])\nmcmc = pm.MCMC(model)\nmcmc.sample(iter=1000, burn=100, thin=2)\n", "intent": "Let's infer the latent variables using the Metropolis sampler:\n"}
{"snippet": "xgbr_all = []\nfor target, setting in zip(targets, settings):\n    xgb2 = XGBRegressor(**setting)\n    xgb2.fit(df0[predictors], df0[target], eval_metric=[\"rmse\"], verbose=False)\n    xgbr_all.append(xgb2)\nfName = 'xgbr_' + strftime(\"%Y_%m_%d_%H_%M_%S\")\npath_to_file = 'fitted_models/'+fName+'.model'\njoblib.dump(xgbr_all, path_to_file)\nprint('\\nExtreme gradient boosting regression model saved to {0}'.format(path_to_file))\n", "intent": "<a id='Cell16'></a>\n"}
{"snippet": "xgbc_all = []\nfor target, setting in zip(targets, settings):\n    xgbc = XGBClassifier(**setting)\n    %time xgbc.fit(df0[predictors], df0[target], eval_metric=[\"logloss\"], verbose=False)\n    xgbc_all.append(xgbc)\nfName = 'xgbc_' + strftime(\"%Y_%m_%d_%H_%M_%S\")\npath_to_file = 'fitted_models/'+fName+'.model'\njoblib.dump(xgbc_all, path_to_file)\nprint('\\nExtreme gradient boosting classifier model saved to {0}'.format(path_to_file))\n", "intent": "<a id='Cell18'></a>\n"}
{"snippet": "def leastsq_reg(x,y):\n    reg = sm.OLS(y, sm.add_constant(x)).fit()\n    print reg.summary()\n    return reg\n", "intent": "With Statsmodels for good measure\n"}
{"snippet": "hist = model.fit(x_train, y_train,\n          batch_size=64,\n          epochs=50,\n          validation_data=(x_test, y_test), \n          verbose=2)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "config = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.9\nwith tf.Session(config=config) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('./checkpoints/'))\n    new_predicted = sess.run( tf.argmax(logits, 1), feed_dict={x:X_new })\n    print('new_predicted values:', new_predicted)\n", "intent": "After prerpoced the new input data, then the pretrained model is loaded and tested with new mini dataset of 10 images.\n"}
{"snippet": "def get_top_five(in_image):\n    with tf.Session(config=config) as sess:\n        saver.restore(sess, tf.train.latest_checkpoint('./checkpoints/'))\n        softmax = tf.nn.softmax(logits)\n        top_prob = tf.nn.top_k(softmax, k =5 )\n        top_five = sess.run( top_prob, feed_dict={x: in_image })\n    return top_five\n", "intent": "A new function to get top 5 probabilities was defined.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(2000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x_train, y_train, epochs=5, batch_size=100, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "logit = sm.Logit(data['admit'], data[train_cols])\nresult = logit.fit()\nresult\nresult.summary()\n", "intent": "hint 1: np.exp(X)\nhint 2: conf['OR'] = params\n           conf.columns = ['2.5%', '97.5%', 'OR']\n"}
{"snippet": "model_test = pm.Model([theta_t, z_t, zbar_t, y_t, w_t])\nmcmc_test = pm.MCMC(model_test)\nmcmc_test.sample(iter=1000, burn=100, thin=2)\n", "intent": "Let's run the Metropolis sampler to obtain posterior distribution over our latent variables.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_initializer = tf.random_normal_initializer(mean=0.0, stddev=1.0/ np.sqrt(embed_dim)) \n    embedding = tf.get_variable('X', shape=[vocab_size, embed_dim],  \n                                     initializer=embedding_initializer,\n                                     trainable=True,\n                                     dtype=tf.float32)\n    input_embedding = tf.nn.embedding_lookup(embedding, input_data) \n    return input_embedding\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def logModel(W, b, x, y):\n    h = np.maximum(np.dot(x, W[:, :50]) + b[:, :50], 0.)\n    y_hat = np.dot(h, W[:, 50:].T) + b[:, 50:]\n    return np.sum( .5 * -(y - y_hat)**2 )\ndef fprop(W, b, x):\n    h = np.maximum(np.dot(x, W[:, :50]) + b[:, :50], 0.)\n    return np.dot(h, W[:, 50:].T) + b[:, 50:]\ndLogModel_dW = grad(logModel)\ndLogModel_db = grad(lambda b, W, x, y: logModel(W, b, x, y))\n", "intent": "Next let's define the model: a one-hidden-layer neural network with 50 units...\n"}
{"snippet": "def init_regression_model(in_size, std=.1):\n    return {'w':tf.Variable(tf.random_normal([in_size, 1], stddev=std)), 'b':tf.Variable(tf.zeros([1,]))}\ndef linear_regressor(X, params):\n    return tf.matmul(X, params['w']) + params['b']\n", "intent": "Next we need to create functions to initialize and run the regression model...\n"}
{"snippet": "model_params = init_regression_model(input_d)\nlinear_model_out = linear_regressor(X, model_params)\nce_cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(linear_model_out, Y))\n", "intent": "Let's use those functions and create a symbolic cost...\n"}
{"snippet": "model = keras.models.load_model('../data/keras_emotion_model.h5')\nwith open(\"../data/keras_emotion_history.p\", \"rb\") as f:\n    history= pickle.load(f)\n", "intent": "Load model and history:\n"}
{"snippet": "img_rows = 28 \nimg_cols = 28\nchannels = 1\nnpixels = img_rows * img_cols * channels\nimg_shape = (img_rows, img_cols, channels)\noptimizer = keras.optimizers.Adam(0.0002, 0.5)\n", "intent": "We will first build the discriminitive model, aka the discriminator.\nWe start with some definitions.\n"}
{"snippet": "combined = keras.models.Model(noise, valid)\ncombined.compile(\n    loss='binary_crossentropy', \n    optimizer=optimizer\n)\n", "intent": "The combined model  (stacked generator and discriminator) takes noise as input => generates images => determines validity.\n"}
{"snippet": "generator = keras.models.load_model('../data/gan_generator.h5')\ndiscriminator = keras.models.load_model('../data/gan_discriminator.h5')\ncombined = keras.models.load_model('../data/gan_combined.h5')\n", "intent": "You can see that the generator very quickly becomes pretty good at generating nice digits, and then bounces around.\n"}
{"snippet": "model.add(\n    keras.layers.Dense(ncats, activation='softmax'))\n", "intent": "We add a Softmax regression layer, similar to the previous, simpler example, by adding a dense layer with softmax activation.\n"}
{"snippet": "x = Input(shape=(original_dim,))\nh = Dense(intermediate_dim, activation='relu')(x)\nz_mean = Dense(latent_dim)(h)\nz_log_sigma = Dense(latent_dim)(h)\nz = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\ndecoder_h = Dense(intermediate_dim, activation='relu')\ndecoder_mean = Dense(original_dim, activation='sigmoid')\nh_decoded = decoder_h(z)\nx_decoded_mean = decoder_mean(h_decoded)\n", "intent": "Let's define the variational autoencoder architecture:\n"}
{"snippet": "def softmax(x):\n    x = x.squeeze()\n    expx = np.exp(x - x.sum())\n    return expx / expx.sum()\ndef cross_entropy(predictions, targets):\n    return sum([-np.log(p[t]) for p, t in zip(predictions, targets)])\n", "intent": "Let's get some of the functions we already know out of the way:\n"}
{"snippet": "W_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv=tf.nn.softmax(h_fc1_drop @ W_fc2 + b_fc2)\n", "intent": "We add a Softmax regression layer, similar to the previous, simpler example, including the regression coefficients, bias, and the softmax function:\n"}
{"snippet": "model = keras.models.load_model('../data/keras_esc50_model.h5')\nwith open(\"../data/keras_esc50_history.p\", \"rb\") as f:\n    history= pickle.load(f)\n", "intent": "Load pre-saved model and history:\n"}
{"snippet": "plot_model(model, to_file='tmp.png')\nimage.load_img('tmp.png')\n", "intent": "Now, let's see the ResNet50 architecture.\n"}
{"snippet": "model = keras.models.load_model('../data/keras_cnn_model.h5') \n", "intent": "Let's load the model we trained on MNIST data in the [CNN session](K_CNN.ipynb).\n"}
{"snippet": "history = model.fit(\n    x_train, y_train,\n    batch_size=50,\n    epochs=5,\n    validation_data=(x_test, y_test)\n).history\n", "intent": "Finally, we re-train the model on the new data.\n"}
{"snippet": "knn = nb.KNeighborsClassifier()\nknn.fit(X_train, y_train) \n", "intent": "Import the nearest-neighbor classifier, then create and fit it:\n"}
{"snippet": "reg = linear_model.LinearRegression()\nreg.fit(X_train, y_train)\nscore = reg.score(X_test, y_test)\nprint('Test score:', score)\n", "intent": "We want to see if we can remove any one feature without reducing our accuracy, or even improving it.\nFirst, let's fit the full linear model:\n"}
{"snippet": "scores = np.empty(n_features)\nfor j in range(n_features):\n    reg = linear_model.LinearRegression()\n    reg.fit(X_train[:,columns[j]], y_train)\n    scores[j] = reg.score(X_test[:,columns[j]], y_test)\n", "intent": "and calculate the score without each of the columns:\n"}
{"snippet": "y = CustomVariationalLayer()([x, x_decoded_mean])\nvae = Model(x, y)\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nvae.compile(loss=None, optimizer=adam, metrics=['accuracy'])\nvae.summary()\n", "intent": "We can finally compile our variational autoencoder model using Adam optimizer:\n"}
{"snippet": "model.compile(\n    loss=keras.losses.categorical_crossentropy,\n    optimizer=keras.optimizers.Adam(), \n    metrics=['accuracy']\n)\n", "intent": "**Compile the model.**\n"}
{"snippet": "hidden_size = 100\nmodel = keras.models.Sequential()\nmodel.add(\n    keras.layers.Dense(hidden_size, input_shape=(size**2,), activation='relu'))\nmodel.add(\n    keras.layers.Dense(hidden_size, activation='relu'))\nmodel.add(\n    keras.layers.Dense(3, activation='softmax'))\nmodel.compile(keras.optimizers.sgd(lr=.2), \"mse\")\n", "intent": "Now everything is ready. \n**Create your model, train it, and score it.**\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), minval = -1, maxval = 1), dtype=tf.float32)\n    embed = tf.nn.embedding_lookup(params = embedding, ids = input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "model=LogisticRegression(C=1.)\nmodel.fit(X_train_transform, y_train)\n", "intent": "Our matrice is only with one-gram words and still have a voculary of 586147 words\n"}
{"snippet": "parameters = {\"n_estimators\" : [10, 50, 100, 150], \"max_depth\": [None, 5, 10], \"bootstrap\" : [True, False]}\nmodel = RandomForestClassifier()\nclf = GridSearchCV(model, parameters, cv=5, scoring=\"accuracy\")\nclf.fit(X, y)\ncvres = clf.cv_results_\nfor score, param in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(score, param)\n", "intent": "Now we have our 2 best models <b>RandomForestClassifier</b> and <b>BaggingClassifier</b>. We can fine tune them usign a GridSearch\n"}
{"snippet": "model = load_model(\"model\")\n", "intent": "Now let's look at result but still with a critic eye as we still use the test set which was used for the early stop...\n"}
{"snippet": "clf = SVC()\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\nprint(clf.score(X_test, y_test))\n", "intent": "We can now try some models\n"}
{"snippet": "X_ph = tf.placeholder(tf.float32, shape=(None, m), name=\"X\")\nY_ph = tf.placeholder(tf.float32, shape=(None, 1), name=\"Y\")\ntheta = tf.Variable(tf.random_uniform([m, 1], -1, 1), name=\"theta\")\n", "intent": "Let's now create the tensorflow graph\n"}
{"snippet": "acc_summary = tf.summary.scalar(\"Accuracy\", accuracy)\nfile_writter = tf.summary.FileWriter(\"/saves/summary/BN_elu-{}-{}/\".format(size_hidden_layer_1, size_hidden_layer_2), tf.get_default_graph())\n", "intent": "For the result presented below, I changed the name of the save based on network topology and activation function.\n"}
{"snippet": "model = Sequential()\nmodel.add(LSTM(32, input_dim=1)) \nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(X_train, y_train, nb_epoch=100, batch_size=5, verbose=2)\n", "intent": "Next, we define our model:\n"}
{"snippet": "newModel = Sequential()\nnewModel.add(LSTM(units=7, stateful=True, batch_input_shape=(1,1,7), return_sequences=True))\nnewModel.set_weights(model.get_weights())\n", "intent": "Now let's create both model as one-to-many and evaluate them 20 times on 100 words generated.\n"}
{"snippet": "reference = out[-1].flatten()\nentropies = [entropy(x.flatten(), reference) for x in out]\ntexts = [\"{} comps\".format(i+1) for i in range(6)] + [\"Perfect filter\"]\n", "intent": "As we did previously, we can check the entropy vs the perfect filtered image. We can after that, plot it vs the time spet to render the time.\n"}
{"snippet": "def multilayer_perceptron(x, weights, biases):\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n    layer_1 = tf.nn.relu(layer_1)\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n    layer_2 = tf.nn.relu(layer_2)\n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n    return out_layer\n", "intent": "We are going to use Adam Optimizer to get the model right. See [here](https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218)\n"}
{"snippet": "scaler.fit(bank_note)\n", "intent": "**Fit scaler to the features.**\n"}
{"snippet": "import tensorflow as tf\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\nclassifier = learn.DNNClassifier(feature_columns=feature_columns,\n                                hidden_units=[10,20,10],\n                                n_classes=3,\n                                model_dir=\"./ouput\")\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "dnn_keras_model = models.Sequential()\n", "intent": "See more of Keras [here](https://keras.io/)\n"}
{"snippet": "with tf.Session() as sess:\n    writer = tf.summary.FileWriter(\"./output\", sess.graph)\n    print(sess.run(c))\n    writer.close()\n", "intent": "Visualization with TensorBoard\n"}
{"snippet": "graph_one = tf.get_default_graph()\n", "intent": "See, a different graph (different place in memory)\n"}
{"snippet": "graph_two = tf.Graph()\n", "intent": "The same as the default graph that we saw at the beginning\n"}
{"snippet": "from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n", "intent": "Build a Linear Regression model\n"}
{"snippet": "g = Graph()\ng.set_as_default()\n", "intent": "Negative means that (2, -10) falls under the line - **blue**\n"}
{"snippet": "knn.fit(X_test, y_test)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "nb_fit = nb.fit(X_train, y_train)\n", "intent": "**Now fit nb using the training data.**\n"}
{"snippet": "loss_func(model(xb), yb), accuracy(model(xb), yb)\n", "intent": "Note that we no longer call ``log_softmax`` in the ``model`` function. Let's\nconfirm that our loss and accuracy are the same as before:\n"}
{"snippet": "fit()\nloss_func(model(xb), yb)\n", "intent": "We are still able to use our same ``fit`` method as before.\n"}
{"snippet": "target = torch.tensor([3], dtype=torch.long)\nloss_fn = nn.CrossEntropyLoss()  \nerr = loss_fn(out, target)\nerr.backward()\nprint(err)\n", "intent": "Define a dummy target label and compute error using a loss function.\n"}
{"snippet": "model = build_model()\n", "intent": "Model will use the streams I created earlier to train and test.\n"}
{"snippet": "batch_size = 256\nepochs = 2\nfilepath_3 =\"./models/textcnn_v2_embeddings/weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\ncheckpoint_3 = ModelCheckpoint(filepath_3, monitor='val_acc', save_best_only=True, mode='max', verbose=1)\nhistory_textcnn_2 = textcnn_model_v2.fit(x=padded_train_seq, y=train_labels, validation_split=0.1, \n                                batch_size=batch_size, epochs=epochs, callbacks=[checkpoint_3], verbose=1)\n", "intent": "<img src='./model_images/text_cnn_model_v3.png'/>\n"}
{"snippet": "validation_split = 0.2\nbatch_size = 512\nepochs = 10\nfile_name = \"./models/cnn_model/weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\ncheckpoint = ModelCheckpoint(file_name, monitor='val_acc', \n                             save_best_only=True, mode='max', verbose=1)\nr = model.fit(train_data, train_targets, batch_size=batch_size, \n                      callbacks=[checkpoint],\n                      epochs = epochs, validation_split=validation_split)\n", "intent": "<img src='./model_images/cnn_model.png'/>\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression().fit(X_train, y_train)\nprint(\"Training set accuracy: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(logreg.score(X_test, y_test)))\n", "intent": "Logistic regression is one of the most common classification algorithms.\n"}
{"snippet": "get_predictions(index=0, \n                partition='train',\n                input_to_softmax=final_model(input_dim=161,\n                        filters=200,\n                        kernel_size=11, \n                        conv_stride=2,\n                        conv_border_mode='valid',\n                        units=128,\n                        recur_layers=2), \n                model_path='results/'+'model_end.h5')\n", "intent": "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset.\n"}
{"snippet": "get_predictions(index=0, \n                partition='validation',\n                input_to_softmax=final_model(input_dim=161, \n                        filters=200,\n                        kernel_size=11, \n                        conv_stride=2,\n                        conv_border_mode='valid',\n                        units=128,\n                        recur_layers=2), \n                model_path='results/'+'model_end.h5')\n", "intent": "Use the next code cell to visualize the model's prediction for the first example in the validation dataset.\n"}
{"snippet": "clf = AdaBoostClassifier(n_estimators=300)\n", "intent": "best score so far with 300 trees\n"}
{"snippet": "w = theano.shared(np.random.randn(len(features[0]), len(np.unique(species))),\n                name='w',borrow=True)\n", "intent": "We create shared variables that contain the matrix of weights and the bias terms and initialize it with normally distributed values:\n"}
{"snippet": "w = theano.shared(np.random.randn(len(features[0]),len(np.unique(species))), name='w', borrow=True)\n", "intent": "We create shared variables that contain the matrix of weights and the bias terms and initialize it with normally distributed values:\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf.fit(X_train, Y_train)\n", "intent": "Train a logistic regression using the 70% set\n---\nEntrenar una regresion logistica usando la particion del 70%\n"}
{"snippet": "feature_cols = ['temp', 'season', 'weather', 'humidity']\nX = bikes[feature_cols]\ny = bikes.total\nlinreg = LinearRegression()\nlinreg.fit(X, y)\nprint(linreg.intercept_)\nprint(linreg.coef_)\n", "intent": "Estimate a regression using more features ['temp', 'season', 'weather', 'humidity'].\nHow is the performance compared to using only the temperature?\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1e9)\nlogreg.fit(X_train, y_train)\nprint(feature_cols, logreg.coef_[0])\n", "intent": "Fit a logistic regression model and examine the coefficients\nConfirm that the coefficients make intuitive sense.\n"}
{"snippet": "np.identity(4)\n", "intent": "There's also an **identity** command that behaves as you'd expect:\n"}
{"snippet": "logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)\nprint(\"Training set accuracy: {:.3f}\".format(logreg001.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(logreg001.score(X_test, y_test)))\n", "intent": "The default value of C=1 provides with 78% accuracy on training and 77% accuracy on test set.\n"}
{"snippet": "np.identity(2) + np.array([[1,1],[1,2]])\n", "intent": "as well as when you add two matrices together. (However, the matrices have to be the same shape.)\n"}
{"snippet": "np.identity(2)*np.ones((2,2))\n", "intent": "Something that confuses Matlab users is that the times (*) operator give element-wise multiplication rather than matrix multiplication:\n"}
{"snippet": "np.dot(np.identity(2),np.ones((2,2)))\n", "intent": "To get matrix multiplication, you need the **dot** command:\n"}
{"snippet": "from sklearn.linear_model import SGDClassifier\nclf = SGDClassifier(loss=\"hinge\", alpha=0.01, n_iter=200, fit_intercept=True)\nclf.fit(X, Y)\n", "intent": "A classification algorithm may be used to draw a dividing boundary\nbetween the two clusters of points:\n"}
{"snippet": "clf1.fit(data[['area', 'area2']], data[' price'])\n", "intent": "When using sklearn there is no need to create the intercept\nAlso sklearn works with pandas\n"}
{"snippet": "from sklearn.neighbors import NearestNeighbors\nnn = NearestNeighbors(n_neighbors=2)\nnn.fit(x_vis)\nnns = nn.kneighbors(x_vis, return_distance=False)[:, 1]\n", "intent": "Find the nearest neighbour of every point\n"}
{"snippet": "from sklearn.tree import DecisionTreeClassifier\ntrees = []\ntrees.append(DecisionTreeClassifier(max_depth=1))\ntrees[t].fit(X_train, y_train, sample_weight=weights[t].values)\n", "intent": "Train the classifier\n"}
{"snippet": "W2=tf.Variable(tf.random_normal([EMB_DIM,vocab_size]),name='W2')\nb2=tf.Variable(tf.random_normal([vocab_size]),name='b2')\nprediction_layer = tf.nn.softmax(tf.add(tf.matmul(hidden_layer,W2),b2))\n", "intent": "The following layer is for predictions output/target layer\n"}
{"snippet": "with open('train.txt') as f:\n    X_train = [parseFeature(l) for l in f.readlines()]\nx_train, y_train = getTrainingData(X_train, 100)\nprint 'Training data size: %d' %len(y_train)\nclf = svm.SVC(kernel='linear', C=.1)\nclf.fit(x_train, y_train)\n", "intent": "- due to resource limitation, we only use data from a limited number of query for training\n- train SVM with linear kernel\n"}
{"snippet": "logreg100 = LogisticRegression(C=100).fit(X_train, y_train)\nprint(\"Training set accuracy: {:.3f}\".format(logreg100.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(logreg100.score(X_test, y_test)))\n", "intent": "Using C=0.01 results in lower accuracy on both the training and the test sets.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    return tf.contrib.layers.embed_sequence(input_data, vocab_size=vocab_size, embed_dim=embed_dim)\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(initial_value=tf.random_uniform(shape=[vocab_size, embed_dim], minval=-1.0, maxval=1.0))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "lm.fit(X, bos.PRICE)\nprint(lm.coef_)\n", "intent": "***\nThe `lm.fit()` function estimates the coefficients the linear regression using least squares. \n"}
{"snippet": "lm_no_intercept = LinearRegression(fit_intercept = False)\nlm_no_intercept.fit(X, bos.PRICE)\n", "intent": "**Your turn:** How would you change the model to not fit an intercept term? Would you recommend not having an intercept?\nprint(lm.coef_)\n"}
{"snippet": "history = model.fit(x_train, y_train_hot, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=1)\n", "intent": "Same as in earlier exercises ...\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "param_grid = {'penalty':['l1', 'l2'], \n              'C': [0.001, 0.01, 1, 10, 100, 1000]} \nlogisticModel = tuningPara_crossValid(LogisticRegression(), param_grid, X_train, X_test, y_train, y_test)\n", "intent": "<h4> Problem \n- Repeat **Problem \n"}
{"snippet": "MLPmodel=MLPClassifier(solver='sgd', activation='relu', random_state=10, hidden_layer_sizes=[10,5], alpha=0.5)\nMLPmodel.fit(X_train_transformed, y_train)\nmlp_accuracy = MLPmodel.score(X_test_transformed, y_test)\nprint('The accuracy of MLPClassifier is', mlp_accuracy)\n", "intent": "<h4> Problem \n- Repeat **Problem \n- Report accuracy, confusion matrix, precision, and recall\n"}
{"snippet": "MLPmodel=MLPClassifier(solver='lbfgs', activation='relu', random_state=10, hidden_layer_sizes=[10,5], alpha=0.5)\nMLPmodel.fit(X_train_transformed, y_train)\nmlp_accuracy = MLPmodel.score(X_test_transformed, y_test)\nprint('The accuracy of MLPClassifier is', mlp_accuracy)\n", "intent": "<h4> Problem \n- Repeat **Problem \n- Report accuracy, confusion matrix, precision, and recall\n- Comment on results\n"}
{"snippet": "mlp = MLPClassifier(max_iter=1000, random_state=0)\nmlp.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    mlp.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n", "intent": "The results are much better after scaling. As at matter of fact, we have obtained the highest test accuracy so far.\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.truncated_normal([vocab_size, embed_dim], -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "knn_1 = KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "knn_1.fit(X_train, y_train)\n", "intent": "**Fit this KNN model to the training data.**\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "grid = GridSearchCV(SVC(), param_grid, verbose=1)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "import theano\ntrain = theano.function([net_input, true_output], loss, updates=updates)\nget_output = theano.function([net_input], net_output)\n", "intent": "Finally, we create Theano-based functions that perform this training and also obtain the output of the network for testing\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1 ,1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "from sklearn.model_selection import GridSearchCV\nlogreg_parameters = {'penalty':['l1','l2'],\n                     'C':np.logspace(-5,1,50),\n                     'solver':['liblinear']}\ngrid_search = GridSearchCV(model1, param_grid=logreg_parameters, scoring=\"accuracy\")\ngrid_search.fit(X_train, y_train)\n", "intent": "- Use the provided parameter grid. Feel free to add if you like (such as n_jobs).\n- Use 5-fold cross-validation.\n"}
{"snippet": "from sklearn.neighbors import  KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nknn_parameters = {'n_neighbors' : list(range(1,20,3)), \n                  'weights' : ['uniform', 'distance']}\nknn = KNeighborsClassifier()\n", "intent": "At least have number of neighbors and weights in your parameters dictionary.\n"}
{"snippet": "from sklearn import ensemble\nfrom sklearn.ensemble import GradientBoostingRegressor\nmodel = ensemble.GradientBoostingRegressor()\nmodel.fit(X_train, y_train)\n", "intent": "Much better! Let's try one more.\n"}
{"snippet": "from sklearn.cluster import KMeans\nn_clusters=2\nkmeans = KMeans(n_clusters,random_state=1)\nkmeans.fit(adult)\n", "intent": "Cluster the Data to our our target groups. **BONUS**: Perform multiple clustering tests on various indicator variables\n"}
{"snippet": "iris.feature_names\nn_clusters=3\nkmeans=cluster.KMeans(n_clusters=n_clusters)\n", "intent": "Run the clustering analysis using scikit-learn.\n*Hint*: Estimate the number of clusters, k, based on your visual examination of the distributions.\n"}
{"snippet": "kmeans.fit(X)\nkmeans.cluster_centers_\n", "intent": "Compute the labels and centroids\n"}
{"snippet": "from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=2)\nkm.fit(Z)\n", "intent": "Now, conduct the PCA using scikit learn\n"}
{"snippet": "nb = BernoulliNB()\n", "intent": "The model should only be built (and cross-validated) on the training data.\nCross-validate the score and compare it to baseline.\n"}
{"snippet": "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n", "intent": "** Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=2000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='sigmoid'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "lm.fit(x_train,y_train)\n", "intent": "** Train/fit lm on the training data.**\n"}
{"snippet": "log_model=LogisticRegression()\n", "intent": "** Train and fit a logistic regression model on the training set.**\n"}
{"snippet": "humidity_classifier = DecisionTreeClassifier(max_leaf_nodes=10, random_state=0)\nhumidity_classifier.fit(X_train, y_train)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nFit on Train Set\n<br><br></p>\n"}
{"snippet": "model=KNeighborsClassifier(n_neighbors=1)\n", "intent": "**Create a KNN model instance with n_neighbors=1**\n"}
{"snippet": "def sigmoid(z):\n    return (1.0/(1+np.exp(-z)))\n", "intent": "Define sigmoid function. <br>\n           1. Input: An array.\n           2. Output: Sigmoid of Input.\n"}
{"snippet": "def sigmoid(z):\n    return (1/(1 + np.exp(-z)))\n", "intent": "Define sigmoid function. <br>\n           1. Input: An array.\n           2. Output: Sigmoid of Input.\n"}
{"snippet": "def sigmoid(z):\n    return (1/(1+np.exp((-1)*z)))\n", "intent": "Define sigmoid function. <br>\n           1. Input: An array.\n           2. Output: Sigmoid of Input.\n"}
{"snippet": "dtree=DecisionTreeClassifier(criterion='entropy')\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "model=KMeans(n_clusters=2)\n", "intent": "** Create an instance of a K Means model with 2 clusters.**\n"}
{"snippet": "from sklearn.naive_bayes import MultinomialNB\nnb=MultinomialNB()\n", "intent": "Time to train a model!\n** Import MultinomialNB and create an instance of the estimator and call is nb **\n"}
{"snippet": "meta_model = LinearRegression(n_jobs=16)\nstart = datetime.now()\nmeta_lr = meta_model.fit(X_train_level2, y_train_level2)\nstop = datetime.now()\nprint(\"Ellapsed time = \", stop - start)\n", "intent": "We will use linear regression to fit the meta-features, using the same parameters as in the model above.\n"}
{"snippet": "x = tf.placeholder(tf.string)\ny = tf.placeholder(tf.int32)\nz = tf.placeholder(tf.float32)\nwith tf.Session() as sess:\n    outX = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})\n    outY = sess.run(y, feed_dict={x: 'Test String', y: 123, z: 45.67})\n    outZ = sess.run(z, feed_dict={x: 'Test String', y: 123, z: 45.67})\n    print(outX, \" \", outY, \" \", outZ)\n", "intent": "x = 'Test String'  \ny = 123  \nz = 45.67  \n"}
{"snippet": "kmeans = KMeans(n_clusters=12)\nmodel = kmeans.fit(X)\nprint(\"model\\n\", model)\n", "intent": "<p style=\"font-family: Arial; font-size:1.75em;color:purple; font-style:bold\"><br>\nUse k-Means Clustering\n<br><br></p>\n"}
{"snippet": "history = model.fit(x=X_train_array, y=y_train, batch_size=64, epochs=5,\n                    verbose=1, validation_data=(X_test_array, y_test))\n", "intent": "Let's go ahead and train this for a few epochs and see what we get.\n"}
{"snippet": "model = Char3Model(vocab_size, n_factors, n_hidden)\nhistory = model.fit(x=[x1, x2, x3], y=y_cat, batch_size=512, epochs=3, verbose=1)\n", "intent": "Train the model for a few iterations.\n"}
{"snippet": "model = CharLoopModel(vocab_size, cs, n_factors, n_hidden)\nhistory = model.fit(x=X_array, y=y_cat, batch_size=512, epochs=5, verbose=1)\n", "intent": "Train the model a bit and generate some predictions.\n"}
{"snippet": "model = CharRnn(vocab_size, cs, n_factors, n_hidden)\nmodel.summary()\n", "intent": "Let's look at a summary of the model.  Notice the array shapes have a third dimension to them until we get on the other side of the RNN.\n"}
{"snippet": "K.set_value(model.optimizer.lr, 0.0001)\nhistory = model.fit(x=X, y=y_cat, batch_size=512, epochs=3, verbose=1)\n", "intent": "We can train it a bit longer at a lower learning rate to reduce the loss further.\n"}
{"snippet": "model.fit(x=X, y=y_cat, batch_size=bs, epochs=8, verbose=1, callbacks=[reset_state], shuffle=False)\n", "intent": "Train the model for a while as before, with the addition of the callback to reset state between epochs.\n"}
{"snippet": "model = CharStatefulLSTM(vocab_size, cs, n_factors, n_hidden, bs)\nmodel.fit(x=X, y=y_cat, batch_size=bs, epochs=20, verbose=1, callbacks=[reset_state], shuffle=False)\n", "intent": "LSTMs need to train for a bit longer.  We'll do 20 epochs at each learning rate.\n"}
{"snippet": "svm = SVC(kernel = 'poly', degree = 3, C = 0.8)\nsvm.fit(X_train, y_train)\ntest_scores = svm.score(X_test, y_test)\nprint(\"The score of the nested CV is %.02f and when using test data %.02f\" %(best_scores, test_scores))\n", "intent": "At last an svm is trained with the optimal parameters. This is then trained with the original training data and tested with the test data.\n"}
{"snippet": "svm = SVC(kernel = 'poly', degree = 3, C = 0.8)\nsvm.fit(X_train, y_train)\ntest_scores = svm.score(X_test, y_test)\nbest_scores, test_scores\nprint(\"The score of the nested CV is %.02f and when using test data %.02f\" %(np.mean(best_scores), test_scores))\n", "intent": "At last an svm is trained with the optimal parameters. This is then trained with the original training data and tested with the test data.\n"}
{"snippet": "knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\nprint('Accuracy of K-NN classifier on training set: {:.2f}'.format(knn.score(X_train, y_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'.format(knn.score(X_test, y_test)))\n", "intent": "The above plot suggests that we should shoose n_neighbors=3. Here we are:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(1000,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "knn5=KNeighborsClassifier(n_neighbors=5,weights='uniform')\nscores=accuracy_crossvalidator(Xs,y,knn5,cv_indices)\n", "intent": "<a id='cv-knn5'></a>\n"}
{"snippet": "import statsmodels.formula.api as sm\nmodel = sm.ols(formula='dep_delay ~ temp', data=flight_weather_df)\nresult = model.fit()\n", "intent": "In sum, temperature, wind speed, visibility, pressure and precipitation tend to affect the delays in flight departures.\n"}
{"snippet": "model = sm.ols(formula='dep_delay ~ wind_speed', data=flight_weather_df)\nresult = model.fit()\nprint(result.summary())\n", "intent": "For every 1 farenheit increase in temperature, the departure delay increases by 0.18 minutes.\n"}
{"snippet": "model = sm.ols(formula='dep_delay ~ visib', data=flight_weather_df)\nresult = model.fit()\nprint(result.summary())\n", "intent": "For every increase in wind speed by one unit, the departure delay increases by 0.06\n"}
{"snippet": "model = sm.ols(formula='dep_delay ~ pressure', data=flight_weather_df)\nresult = model.fit()\nprint(result.summary())\n", "intent": "for every 1 unit increase in visibility the departure delay decreases by 2.2 mins.\n"}
{"snippet": "model = sm.ols(formula='dep_delay ~ precip', data=flight_weather_df)\nresult = model.fit()\nprint(result.summary())\n", "intent": "for every one unit increase in pressure the departure delay decreses by 0.5\n"}
{"snippet": "clf_2 = LogisticRegression()\nclf_2.fit(X,y)\n", "intent": "Repeat the above process for logistic regression\n"}
{"snippet": "with tf.Session() as sess:\n    sess.run(init)\n    tfdata= {x:data[4:,1],\n             y:data[4:,2],\n             sigma:data[4:,3]}\n    for i in range(3000):\n        sess.run(train,tfdata)\n    print(\"m=%.2f, b=%.2f\" % (sess.run([m])[0],\n                              sess.run([b])[0]),\n          \"loss=%.2f\" % sess.run(loss,tfdata))\n", "intent": "and then we optimize:\n"}
{"snippet": "gb = GradientBoostingClassifier(random_state=0, max_depth=1)\ngb.fit(X_train, y_train)\nplot_feature_importances_cancer(gb1)\n", "intent": "Still, we can visualize the feature importances to get more insight into our model even though we are not really happy with the model.\n"}
{"snippet": "from keras.layers.pooling import MaxPooling2D\nmodel = Sequential()\nmodel.add(Convolution2D(32, 3, 3, input_shape=(32, 32, 3)))\nmodel.add(MaxPooling2D())\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(43))\nmodel.add(Activation('softmax'))\n", "intent": "1. Re-construct the network\n2. Add a 2x2 [max pooling layer](https://keras.io/layers/pooling/\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding_shape = (vocab_size, embed_dim)\n    embedding = tf.Variable(tf.random_uniform(embedding_shape, -1, 1, dtype=tf.float32), name=\"embedding\")\n    embed = tf.nn.embedding_lookup(embedding, input_data, name=\"embedding_lookup\")\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "opt = tf.keras.optimizers.RMSprop(lr=learning_rate)\nmodel.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=['acc'])\n", "intent": "1. Take a look at [keras optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) and choose the models optimizer.\n"}
{"snippet": "model.fit(x_train, y_train, \n          batch_size=batch_size, \n          epochs=epochs,\n          validation_data=(x_val, y_val))\n", "intent": "1. Complete the **fit()** function bellow and let's train the model.\n"}
{"snippet": "def create_embedding(name, n_in, n_out):\n    inp = Input(name=name + '_in', dtype='int64', shape=[1])\n    return inp, Flatten()(Embedding(n_in, n_out, input_length=1, name=name)(inp))\n", "intent": "Embedding function that creates inputs and embeddings\n"}
{"snippet": "def fit_model(epochs=1, lr=1e-3):\n    model.optimizer.lr = lr\n    model.fit([x1, x2, x3], y, batch_size=64, epochs=epochs,\n              verbose=0, callbacks=[TQDMNotebookCallback()], shuffle=False)\n", "intent": "Helper function to fit the model\n"}
{"snippet": "def fit_model(epochs=1, lr=1e-3):\n    model.optimizer.lr = lr\n    model.fit(x, y, batch_size=64, epochs=epochs,\n              verbose=0, callbacks=[TQDMNotebookCallback()])\n", "intent": "Model fitting utility function\n"}
{"snippet": "clear_session()\nmodel = Sequential([Embedding(vocab_size, n_factors, input_length=cs),\n                    SimpleRNN(n_hidden, activation='relu',\n                              recurrent_initializer=identity()),\n                    Dense(vocab_size, activation='softmax')])\nmodel.compile('adam', loss='sparse_categorical_crossentropy')\nmodel.summary()\n", "intent": "Create Sequential Model\n"}
{"snippet": "def fit_model(epochs=1, lr=1e-3):\n    model.optimizer.lr = lr\n    return model.fit(x, y, batch_size=batch_size, epochs=epochs, verbose=0, callbacks=[TQDMNotebookCallback()], shuffle=False, validation_data=(x_val, y_val))\n", "intent": "Helper Function for training\n"}
{"snippet": "mlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)\nmlp.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    mlp.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test, y_test)))\n", "intent": "The results are much better after scaling, and already quite competitive.\n"}
{"snippet": "def fit_model(epochs=1, lr=1e-3):\n    if exists(DIR_OUTPUT + '/resnet.h5'):\n        model.load_weights(DIR_OUTPUT + '/resnet.h5')\n        print('Loaded weights from previous fit.\\nDelete/Rename for new fit.')\n        return None\n    model.optimizer.lr = lr\n    return model.fit(train_features, train_classes, batch_size=64, epochs=epochs, validation_data=(\n        val_features, val_classes), verbose=0, callbacks=[TQDMNotebookCallback()])\n", "intent": "Helper function to fit data\n"}
{"snippet": "model = get_model()\nfit_model(2, 1e-5, save=False)\n", "intent": "No improvement once more ;(\nLower more!\n"}
{"snippet": "model = get_model()\nfit_model(20, 1e-6, save=False)\n", "intent": "Frustrated, we can try one thing - lower the learning rate more and fit more epochs\n"}
{"snippet": "model = get_model()\nfit_model( 2, 1e-5)\n", "intent": "The predictions are not uniform either. \nIn this case, there's just one thing left to try -\nLower the learning rate!\n"}
{"snippet": "fit_model(4, 1e-4)\n", "intent": "Excellent. It seems that we are ready to increase the learning rate slightly now.\n"}
{"snippet": "model = get_model(reg=1)\nfit_model([2, 4], [1e-5, 1e-4])\n", "intent": "Clearly, this is not working. \nPerhaps the regularization is too small.\nLet's ramp it up\n"}
{"snippet": "model = get_model(reg=10)\nfit_model([2, 4], [1e-5, 1e-4])\n", "intent": "Still not enough.\nGo higher! \n"}
{"snippet": "fit_model(4)\n", "intent": "Shows promise. Let's continue training\n"}
{"snippet": "import statsmodels.api as sm\nlr = sm.Logit(target, features_array)\nlr.fit()\n", "intent": "Let's try our `Logit` model from `statsmodels`:\n"}
{"snippet": "from sagemaker.tensorflow.serving import Model\nenv = {'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'mobilenet_v2_140_224'}\nmodel = Model(model_data=model_data, role=sagemaker_role, framework_version=1.11, env=env)\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge')\n", "intent": "Now that the model archive is in S3, we can create a Model and deploy it to an \nEndpoint with a few lines of python code:\n"}
{"snippet": "number_of_clusters = 3\nkmeans = KMeans(n_clusters=number_of_clusters)\nkmeans.fit(coords)\n", "intent": "Here we'll review the idea of k-means clustering you discussed last week and see how it applies to our crime data. We'll start with three clusters.\n"}
{"snippet": "from sklearn.linear_model import LogisticRegression\nfor C in np.arange(5., 5.51, 0.1):\n    print(C, cross_validation(LogisticRegression(random_state=501, n_jobs=3, C=C, dual=True), \n                              raw_data,\n                              ngram_range=(1, 6),\n                              n_features=2 ** 24))\n", "intent": "0.8603 -- full\n0.8251 -- train\n"}
{"snippet": "with pm.Model() as m2c_nopc:\n    betap = pm.Normal(\"betap\", 0, 1)\n    betac = pm.Normal(\"betac\", 0, 1)\n    alpha = pm.Normal(\"alpha\", 0, 100)\n    loglam = alpha + betap*df.logpop_c + betac*df.clevel\n    y = pm.Poisson(\"ntools\", mu=t.exp(loglam), observed=df.total_tools)\n    trace2c_nopc = pm.sample(5000, tune=1000)\n", "intent": "**(A)** Our complete model\n**(B)** A model with no interaction\n"}
{"snippet": "with pm.Model() as m2c_onlyp:\n    betap = pm.Normal(\"betap\", 0, 1)\n    alpha = pm.Normal(\"alpha\", 0, 100)\n    loglam = alpha + betap*df.logpop_c\n    y = pm.Poisson(\"ntools\", mu=t.exp(loglam), observed=df.total_tools)\n    trace2c_onlyp = pm.sample(5000, tune=1000)\n", "intent": "**(C)** A model with no contact term\n"}
{"snippet": "with pm.Model() as m2c_onlyc:\n    betac = pm.Normal(\"betac\", 0, 1)\n    alpha = pm.Normal(\"alpha\", 0, 100)\n    loglam = alpha +  betac*df.clevel\n    y = pm.Poisson(\"ntools\", mu=t.exp(loglam), observed=df.total_tools)\n    trace2c_onlyc = pm.sample(5000, tune=1000)\n", "intent": "**(D)** A model with only the contact term\n"}
{"snippet": "with pm.Model() as m2c_onlyic:\n    alpha = pm.Normal(\"alpha\", 0, 100)\n    loglam = alpha\n    y = pm.Poisson(\"ntools\", mu=t.exp(loglam), observed=df.total_tools)\n    trace2c_onlyic = pm.sample(5000, tune=1000)\n", "intent": "**(E)** A model with only the intercept.\n"}
{"snippet": "with pm.Model() as m1:\n    betap = pm.Normal(\"betap\", 0, 1)\n    betac = pm.Normal(\"betac\", 0, 1)\n    betapc = pm.Normal(\"betapc\", 0, 1)\n    alpha = pm.Normal(\"alpha\", 0, 100)\n    loglam = alpha + betap*df.logpop + betac*df.clevel + betapc*df.clevel*df.logpop\n    y = pm.Poisson(\"ntools\", mu=t.exp(loglam), observed=df.total_tools)\n    trace1 = pm.sample(10000, tune=2000)\n", "intent": "We can redo the coparison for non-centered models\n"}
{"snippet": "import theano.tensor as tt\ncov=np.array([[0,0.8],[0.8,0]], dtype=np.float64)\nwith pm.Model() as mdensity:\n    density = pm.MvNormal('density', mu=[0,0], cov=tt.fill_diagonal(cov,1), shape=2)\n", "intent": "Ok, so we just set up a simple sampler with no observed data\n"}
{"snippet": "mdvar = pm.ADVI(model=mdensity)\nmdvar.fit(n=40000)\n", "intent": "But when we sample using ADVI, the mean-field approximation means that we lose our correlation:\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\nmodel.fit(X, Y)\n", "intent": "So, how hard can it be to build a Multi-Layer percepton with keras?\nIt is baiscly the same, just add more layers!\n"}
{"snippet": "with pm.Model() as model:\n    mus = [MvNormal('mu_%d' % i, mu=np.zeros(2), tau=np.eye(2), shape=(2,))\n           for i in range(2)]\n    pi = Dirichlet('pi', a=0.1 * np.ones(2), shape=(2,))\n    xs = DensityDist('x', logp_gmix(mus, pi, np.eye(2)), observed=data)\n    start = find_MAP()\n    step = Metropolis()\n    trace = sample(1000, step, start=start, chains=2, njobs=1)\n", "intent": "MCMC takes of ther order of a minute in time, which is 50 times more than on the small dataset.\n"}
{"snippet": "def f():\n    approx = pm.fit(n=1500, obj_optimizer=pm.adagrad(learning_rate=1e-1), model=model)\n    means = approx.bij.rmap(approx.mean.eval())\n    sds = approx.bij.rmap(approx.std.eval())\n    return means, sds, -approx.hist\nmeans, sds, elbos = f()\n", "intent": "Run ADVI. It's much faster than MCMC, though the problem here is simple and it's not a fair comparison. \n"}
{"snippet": "with pm.Model() as m3c:\n    betap = pm.Normal(\"betap\", 0, 1)\n    alpha = pm.Normal(\"alpha\", 0, 10)\n    sigmasoc = pm.HalfCauchy(\"sigmasoc\", 1)\n    alphasoc = pm.Normal(\"alphasoc\", 0, sigmasoc, shape=df.shape[0])\n    loglam = alpha + alphasoc + betap*df.logpop_c \n    y = pm.Poisson(\"ntools\", mu=t.exp(loglam), observed=df.total_tools)\nwith m3c:\n    trace3 = pm.sample(6000, tune=1000, nuts_kwargs=dict(target_accept=.95))\n", "intent": "Notice that $\\beta_P$ has a value around 0.24\nWe also implement the varying intercepts per society model from before\n"}
{"snippet": "x = first_variable\ny = (x ** x) * (x - 2) \nz = F.tanh(y) \nz.backward()\nprint(\"y.data: \", y.data)\nprint(\"y.grad: \", y.grad)\nprint(\"z.data: \", z.data)\nprint(\"z.grad: \", z.grad)\nprint(\"x.grad:\", x.grad)\n", "intent": "Now let's create some new variables. We can do so implicitly just by creating other variables with functional relationships to our variable.\n"}
{"snippet": "import theano.tensor as t\nwith pm.Model() as model1:\n    alpha=pm.Normal(\"alpha\", 0,100)\n    beta=pm.Normal(\"beta\", 0,1)\n    logmu = t.log(df.days)+alpha+beta*df.monastery\n    y = pm.Poisson(\"obsv\", mu=t.exp(logmu), observed=df.y)\n    lambda0 = pm.Deterministic(\"lambda0\", t.exp(alpha))\n    lambda1 = pm.Deterministic(\"lambda1\", t.exp(alpha + beta))\n", "intent": "Now we set up the model in pymc, including two deterministics to capture the rates\n"}
{"snippet": "class ObserverModel(pm.Model):\n    def wrapped_f(**observed):\n        try:\n            with ObserverModel(observed) as model:\n                f(**observed)\n        except TypeError:\n            with ObserverModel(observed) as model:\n                f()\n        return model\n    return wrapped_f\n", "intent": "This should be way enough!, So lets go again:\nFirst we create a decorator using code from https://github.com/ColCarroll/sampled\n"}
{"snippet": "with pm.Model() as hm1dumb:\n    mu = pm.Normal('mu', mu=178, sd=0.1)\n    sigma = pm.Uniform('sigma', lower=0, upper=50)\n    height = pm.Normal('height', mu=mu, sd=sigma, observed=df2.height)\n", "intent": "Above we had used a very diffuse value on the prior. But suppose we tamp it down instead, as in the model below.\n"}
{"snippet": "import theano.tensor as T\nwith pm.Model() as model1:\n    alpha = pm.Normal('intercept', 0, 100)\n    beta = pm.DensityDist('beta', lambda value: -1.5 * T.log(1 + value**2), testval=10)\n    eps = pm.DensityDist('eps', lambda value: -T.log(T.abs_(value)), testval=10)\n    like = pm.Normal('y_est', mu=alpha + beta * (xdata_shared - xdata_shared.mean()), sd=eps, observed=ydata)\n", "intent": "We will need to write custom densities for this. Theano provides us a way:\n"}
{"snippet": "import pymc3 as pm\nfrom pymc3.math import switch\nwith pm.Model() as coaldis1:\n    early_mean = pm.Exponential('early_mean', 1)\n    late_mean = pm.Exponential('late_mean', 1)\n    switchpoint = pm.DiscreteUniform('switchpoint', lower=0, upper=n_years)\n    rate = switch(switchpoint >= np.arange(n_years), early_mean, late_mean)\n    disasters = pm.Poisson('disasters', mu=rate, observed=disasters_data)\n", "intent": "The rate parameter varies before and after the switchpoint, which itseld has a discrete-uniform prior on it. Rate parameters get exponential priors.\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(100, input_shape=(dims,)))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\nmodel.fit(X, Y)\n", "intent": "Take couple of minutes and Try and optimize the number of layers and the number of parameters in the layers to get the best results. \n"}
{"snippet": "LR_model = models['LR'].fit(X_train,y_train)\nprint(\"Train score: \", LR_model.score(X_train,y_train))\nprint(\"Test score: \", LR_model.score(X_test, y_test))\n", "intent": "After chossing the best scoring CV model we fit it on the entire training set and then evaluate it on the test set.\n"}
{"snippet": "dTree = DecisionTreeClassifier()\n", "intent": "**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**\n"}
{"snippet": "paramGrid = {'C':[0.1,1,10,1000],'gamma':[1,0.1,0.01,0.001,0.0001]}\ngrid = GridSearchCV(SVC(), paramGrid,verbose=3)\n", "intent": "**Create a dictionary called param_grid and fill out some parameters for C and gamma.**\n"}
{"snippet": "grid.fit(X_train,y_train)\n", "intent": "** Create a GridSearchCV object and fit it to the training data.**\n"}
{"snippet": "model = Sequential()\nmodel.add(Dense(32, input_dim=x_train.shape[1], activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\nmodel.summary()\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n", "intent": "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting.\n"}
{"snippet": "model.fit(x=x_train, y=y_train, batch_size=1000, epochs=100, validation_split=0.1, verbose=1)\n", "intent": "Run the model here. Experiment with different batch_size, and number of epochs!\n"}
{"snippet": "def get_embed(input_data, vocab_size, embed_dim):\n    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), minval=-1, maxval=1))\n    embed = tf.nn.embedding_lookup(embedding, input_data)\n    return embed\ntests.test_get_embed(get_embed)\n", "intent": "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence.\n"}
{"snippet": "keras.layers.recurrent.LSTM(output_dim, init='glorot_uniform', inner_init='orthogonal', \n                            forget_bias_init='one', activation='tanh', \n                            inner_activation='hard_sigmoid', \n                            W_regularizer=None, U_regularizer=None, b_regularizer=None, \n                            dropout_W=0.0, dropout_U=0.0)\n", "intent": "<img src =\"imgs/gru.png\" width=\"60%\">\n"}
